{"text": " Hi everyone, I'll get started. Okay, so we're now back to the second week of CS224N on Natural Language Processing with Deep Learning. Okay, so for today's lecture, what we're going to be looking at is all the math details of during neural net learning. First of all, looking at how we can work out by hand, gradients for training neural networks, and then looking at how it's done more algorithmically, which is known as the back propagation algorithm. And correspondingly for you guys, well, I hope you remembered that one minute ago was when assignment one was due and everyone has handed that in. If I sometimes who haven't handed in, really should have been as soon as possible best to preserve those late days for the harder assignments. So I mean, I actually forgot to mention, we actually did make one change for this year to make it a bit easier when occasionally people join the class a week wait. If you want to this year and the grading assignment one can be discounted and will just use your other four assignments. But if you've been in the class so far for that 98% of people, well since assignment one is the easiest assignment, again, it's silly not to do it and have it as part of your grade. Okay, so starting today, we've put out assignment two and assignment two is all about making sure you really understand the math of neural networks and then the software that we use to do that math. So this is going to be a bit of a tough week for some. So for some people who are great on all their math and backgrounds, they'll feel like this is stuff they know. Well, nothing very difficult, but I know there are quite a few of you who this lecture and week is the biggest struggle of the course. We really do want people to actually have an understanding of what goes on in your network learning rather than viewing it as some kind of deep magic. And I hope that some of the material we give today and that you read up on and use in the assignment will really give you more of a sense of what these neural networks are doing and how it is just math that's applied in a systematic large scale that works out the answers and that this will be valuable and give you a deeper sense of what's going on. But if this material seems very scary and difficult, you can take some refuge in the fact that there's fast light at the end of the tunnel since this is really the only lecture that's heavily going through the math details of neural networks. After that, we'll be kind of popping back up to a higher level and by and large, after this week, we'll be making use of software to do a lot of the complicated math for us. But nevertheless, I hope this is valuable. I'll go through everything quickly today, but if this isn't stuff that you know backwards, I really do encourage you to work through it and get help as you need it. So do come along to our office hours. There are also a number of pieces of tutorial material given in the syllabus. So there's both the lecture notes. There's some materials from CS231. In the list of readings, the very top reading is some material put together by Kevin Clark a couple of years ago. And actually, that one's my favorite. The presentation there fairly closely follows the presentation in this lecture of going through matrix calculus. So, you know, personally, I'd recommend starting with that one, but there are four different ones you can choose from. If one of them seems more helpful to you. Two other things on what's coming up. Actually, for Thursday's lecture, we make a big change. And Thursday's lecture is probably the most linguistic lecture of the whole class where we go through the details of dependency grammar and dependency parsing. Some people find that tough as well, but at least there'll be tough in a different way. And then one other really good opportunity is this Friday, we have our second tutorial at 10am, which is an introduction to PyTorch, which is the deep learning framework that will be using for the rest of the class. Once we've gone through these first two assignments where you do things by yourself. So this is a great chance, again, to intro to PyTorch. It will be really useful for later in the class. Okay. Today's material is really all about sort of the math of neural networks, but just to sort of introduce a setting where we can work through this, I'm going to introduce a simple NLP task and a simple form of classifier that we can use for it. So the task of named entity recognition is a very common basic NLP task. And the goal of this is you're looking through pieces of text and you're wanting to label by labeling the words, which words belong to entity categories like persons, locations, products, dates, times, et cetera. So for this piece of text, last night Paris Hilton, wowed in the sequin gown, Samuel Quinn was arrest in the Hilton Hotel in Paris in April 1989. The some words are being labeled as named entities as shown. These two sentences don't actually belong together in the same article, but I chose those two sentences to illustrate the basic point that is not that you can just do this task by using a dictionary. Yes, a dictionary is helpful to know that Paris can possibly be a location, but Paris can also be a person name. So you have to use context to get named entity recognition rights. Okay, well, how might we do that with the neural network? There are much more advanced ways of doing this, but a simple yet already pretty good way of doing named entity recognition with a simple neural net is to say, well, what we're going to do is use the word vectors that we've learned about and we're going to build up a context window of word vectors. And then we're going to put those through a neural network layer and then feed it through a softmax classifier of the kind that we, sorry, I said that wrong. And then we're going to feed it through a logistic classifier of the kind that we saw when looking at negative sampling, which is going to say for a particular entity type such as location, is it high probability location or is it not a high probability location. So for a sentence like the museums and Paris are amazing to see what we're going to do is for each word say we're doing the word Paris, we're going to form a window around it say a plus or minus two word window. And so for those five words, we're going to get word vectors for them from the kind of word to vac or glove word vectors we've learned. And we're going to make a long vector out of the concatenation of those five word vectors. So the word of interest is in the middle. And then we're going to feed this vector to a classifier, which is that the end going to have a probability of the word being a location. And then we could have another classifier that says the probability of the word being a person name. And so once we've done that, we're then going to run it at the next position. So we then say, well, is the word are a location and we'd feed a window of five words as then in Paris are amazing to and put it through the same kind of classifier. And so this is the classifier that we'll use. So its import will be this word window. So if we have de-dimensional word vectors, this will be a five de vector. And then we're going to put it through a layer of a neural network. So the layer of the neural network is going to multiply this vector by a matrix, add on a bias vector, and then put that through a non-linearity such as the softmax transformation that we've seen before. And that will give us a hidden vector, which might be of a smaller dimensionality such as this one here. And so then with that hidden vector, we're then going to take the dot product of it with an extra vector here, here's you. So we take you dot product h. And so when we do that, we're getting out a single number. And that number can be any real number. And so then finally, we're going to put that number through a logistic transform of the same kind that we saw when doing negative sampling. And the logistic transform will take any real number and it'll transform it into a probability that that word is a location. So its output is the predicted probability of the word belonging to a particular class. And so this could be our location classifier, which could classify each word in a window as to what the probability is that it's a location word. And so this little neural network here is the neural network I'm going to use today when going through some of the math. But actually, I'm going to make it even easier on myself. I'm going to throw away the logistic function at the top. And I'm really just going to work through the math of the bottom three quarters of this. If you look at Kevin Clark's handout that I just mentioned, he includes when he works through it also working through the logistic function. And we also saw working through a softmax in the first lecture when I was working through some of the word detect model. Okay. So the overall question we want to be able to answer is, so here's our stochastic gradient descent equation that we have existing parameters of our model. And we want to update them based on our current loss, which is the j of theta. So for getting our loss here, that the true answer is to whether a word is a location or not will be either one, if it is a location or zero, if it isn't, our logistic class to file returns some number like 0.9. And we'll use the distance away from what it should have been squared as our loss. So we work out a loss. And then we're moving a little distance in the negative of the gradient, which will be changing our parameter estimates in such a way that they reduce the loss. And so this is already being written in terms of a whole vector of parameters, which is being updated as to a new vector of parameters. But you can also think about it that for each individual parameter theta j that we're working out the partial derivative of the loss with respect to that parameter. And then we're moving a little bit in the negative direction of that. That's going to give us a new value for parameter theta j. And we're going to update all of the parameters of our model as we learn. I mean, in particular, in contrast to what commonly happens in statistics, we also, we update not only the sort of parameters of our model that are sort of weights in the classifier, but we also will update our data representation. So we'll also be changing our word vectors as we learn. Okay, so to build neural nets, I use to train neural nets based on data, what we need is to be able to compute this gradient of the parameters so that we can then iteratively update the weights of the model and efficiently train a model that has good weights, i.e. that has high accuracy. And so how can we do that? Well, what I'm going to talk about today is first of all how you can do it by hand. And so for doing it by hand, this is basically a review of matrix calculus. And that'll take quite a bit of the lecture. And then after we've talked about that for a while, I'll then shift gears and introduce the back propagation algorithm, which is the central technology for neural networks. And that technology is essentially the efficient application of calculus on a large scale as we'll come to talking about soon. So for computing gradients by hand, what we're doing is matrix calculus. So we're working with vectors and matrices and working out gradients. And this can seem like pretty scary stuff. And well, to extent that you're kind of scared and don't know what's going on, one choice is to work out a non-vectorized gradient by just working out what the partial derivative is for one parameter at a time. And I showed a little example of that in the first lecture. But it's much, much faster and more useful to actually be able to work with vectorized gradients. And in some sense, if you're not very confident, this is kind of almost a leap of faith. But it really is the case that multivariable calculus is just like single variable calculus, except you're using vectors and matrices. So providing you to remember some basics of single variable calculus, you really should be able to do this stuff and get it to work out. Lots of other sources, I've mentioned the notes. You can also look at the textbook from A51, which also has quite a lot of material on this. I know some of you have bad memories of A51. OK, so let's go through this and see how it works from ramping up from the beginning. So the beginning of calculus is, you know, we have a function with one input and one output, f of x equals x cubed. And so then its gradient is its slope, right? So that's its derivative. So its derivative is 3x squared. And the way to think about this is how much will the output change if we change the input a little bit, right? So what we're wanting to do in our neural net models is change what they output so that they do a better job of predicting the correct answers when we're doing supervised learning. And so what we want to know is if we fiddle different parameters of the model, how much will they have on the output? Because then we can choose how to fiddle them in the right way to move things down, right? So, you know, when we're saying that the derivative here is 3x squared, well, what we're saying is that if you're at x equals 1, if you fiddle the input a little bit, the output will change 3 times as much, 3 times 1 squared. And it does. So if I say what's the value at 1.01, it's about 1.03, it's changed 3 times as much and that's its slope. But it x equals 4, the derivative is 16 times 3, 48. So if we fiddle the input a little, it'll change 48 times as much and that's roughly what happens, 4.01 cubed is 64.48. Now, of course, you know, this is just sort of showing it for a small fiddle, but, you know, that's an approximation to the actual truth. Okay, so then we sort of ramp up to the more complex cases, which are more reflective of what we do with neural networks. So if we have a function with one output and n inputs, then we have a gradient. So a gradient is a vector of partial derivatives with respect to each input. So we've got n inputs x1 to xn and we're working out the partial derivative f with respect to x1, the partial derivative f with respect to x2, et cetera. And we then get a vector of partial derivatives, where each element of this vector is just like a simple derivative with respect to one variable. Okay, so from that point, we just keep on ramping up for what we do with neural networks. So commonly, when we have something like a layer in a neural network, we'll have a function with n inputs that will be like our word vectors, then we do something like multiply by a matrix and then we'll have m outputs. So we have a function now, which is taking n inputs and is producing m outputs. So at this point, what we're calculating for the gradient is what's called a Jacobian matrix. So for m inputs and n outputs, the Jacobian is an m by n matrix of every combination of partial derivatives. So function f splits up into these different sub functions f1 through m, fm, which generate each of the m outputs. And so then we're taking the partial derivative f1 with respect to x1 through the partial derivative f1 with respect to xn, then heading down, you know, we make it up to the partial derivative of fm with respect to x1, et cetera. So we have every possible partial derivative of an output variable with respect to one of the input variables. Okay. So in simple calculus, when you have a composition of one variable functions, so that if you have y equals x squared and then z equals 3y, then z is a composition of two functions of, or you're composing two functions, z is a function of x. Then you can work out the derivative of z with respect to x. And the way you do that is with the chain rule. And so in the chain rule, you multiply derivatives. So dzdx equals dzdy times dydx. So dzdy is just three, and dydx is 2x. So we get three times 2x. So that overall, the derivative here is 6x. And since if we multiply this together, we're really saying that z equals 3x squared, you should trivially be able to see again, aha, it's derivative is 6x. So that works. Okay. So once we move into vectors and matrices and Jacobians, it's actually the same game. So when we're working with those, we can compose functions and work out their derivatives by simply multiplying Jacobians. So if we have start with an input x and then put it through the simplest form of neural network layer and say that z equals wx plus b. So we multiply the expected by matrix w and then add on a bias vector b. And then typically we'd put things through a nonlinearity f. So f could be a sigmoid function. We'll then say 8 equals f of z. So this is the composition of two functions in terms of vectors and matrices. So we can use Jacobians and we can say the partial of h with respect to x is going to be the product of the partial of h with respect to z and the partial of z with respect to x. And this all does work out. So let's start going through some examples of how these things work slightly more concretely. First, just particular Jacobians and then composing them together. So one case we look at is the nonlinearities that we put a vector through. So this is something like putting a vector through the sigmoid function f. And so if we have an intermediate vector z and we're turning into vector h by putting it through a logistic function, we can say what is dhdz. Well, for this, formally, this is a function that has n inputs and n outputs. So at the end of the day, we're computing an n by n Jacobian. And so what that's meaning is the elements of this n by n Jacobian are going to take the partial derivative of each output with respect to each input. And well, what is that going to be in this case? Well, in this case, because we're actually just computing element-wise a transformation such as a logistic transform of each element zi, like the second equation here. If i equals j, we've got something to compute. Whereas if i doesn't equal j, there's just the input has no influence on the output and so the derivative is zero. So if i doesn't equal j, we're going to get a zero. And if i does equal j, then we're going to get the regular one variable derivative of the logistic function, which if I remember correctly, you were asked to compute, now I can't remember what's the assignment one or assignment two, but one of the two asks you to compute it. So our Jacobian for this case looks like this. We have a diagonal matrix with the derivatives of each element along the diagonal and everything else is zero. Okay, so let's look at a couple of other Jacobians. So if we're asking, if we've got this wx plus b basic neural network layer and we're asking for the gradient with respect to x, then what we're going to have coming out is that's actually going to be the matrix w. So this is where what I hope you can do is look at the notes at home and work through this exactly and see that this is actually the right answer. But this is the way in which if you just have faith and think this is just like single variable calculus, except I've now got vectors and matrices, the answer you get is actually what you expected to get because this is just like the derivative of ax plus b with respect to x where it's a. So similarly, if we take the partial derivative with respect to b of wx plus b, we get out the identity matrix. Okay, then one other Jacobian that we mentioned while in the first lecture while working through word to veck is if you have the dot product of two vectors, i, that's a number, that what you get coming out of that, so the the partial derivative of uth with respect to u is h transpose. And at this point, there's some fine print that I'm going to come back to in a minute. So this is the correct Jacobian, right? Because in this case, we have the dimension of h inputs and we have one output. And so we want to have a row vector. But there's a little bit more to say on that that I'll come back to in about 20 slides. But this is the correct Jacobian. Okay, so if you aren't not familiar with these kind of Jacobians, do please look at some of the notes that are available and try and compute these in more detail element wise and convince yourself that they really are right. But I'm going to assume these now and show you what happens when we actually then work out gradients for at least a mini little neural net. Okay, so here is most of this neural net. I mean, as I commented that, you know, really we'd be working out the partial derivative of the loss j with respect to these variables. But for the example I'm doing here, I just I've locked that off to keep it a little simpler and more manageable for the lecture. And so we're going to just work out the partial derivative of the score s, which is a real number with respect to the different parameters of this model where the parameters of this model are going to be the w and the b and the u and also the input because we can update the weight vectors of the word vectors of different words based on tuning them to better predict the classification outputs that we desire. So let's start off with a fairly easy one where we want to update the bias vector b to have our system classify better. So to be able to do that, what we want to work out is the partial derivatives of s with respect to b. So we know how to put that into our stochastic gradient update for the b parameters. Okay, so how do we go about doing these things? So the first step is we want to sort of break things up into different functions of minimal complexity that compose together. So in particular, this neural net layer, a equals f of wx plus b, it's still a little bit complex. So let's decompose that one further step. So we have the input x, we then calculate the linear transformation z equals wx plus b. And then we put things through the sort of element wise non-linearity, a equals f of z, and then we do the dot product with u. And it's useful for working these things out to split into pieces like this, have straight what your different variables are, and to know what the dimensionality of each of these variables is, it's well worth just writing out the dimensionality of every variable and making sure that the answers that you're computing are of the right dimensionality. So at this point though, what we can see is that calculating s is the product of three, sorry, is the composition of three functions around x. So for working out the partials of s with respect to b, it's the composition of the three functions shown on the left. And so therefore, the gradient of s with respect to b, we're going to take the product of these three partial derivatives. Okay, so how do we, so we've got the s equals uth, so that's sort of the top corresponding partial derivative, partial derivative of h with respect to z, partial derivative of z with respect to b, which is the first one that we're working out. Okay, so we want to work this out, and if we're lucky, we remember those Jacobians I showed previously about the Jacobian for a vector dot product, the Jacobian for the nonlinearity and the Jacobian for the simple linear transformation. And so we can use those. So for the partials of s with respect to h, well, that's going to be ut using the first one. The partials of h with respect to z, okay, so that's the nonlinearity. And so that's going to be the matrix that's the diagonal matrix with the element-wise derivative of f prime of z and zero elsewhere. And then for the wx plus b, when we're taking the partials with respect to b, that's just the identity matrix. So we can simplify that down a little, the identity matrix disappears. And since ut is a vector, and this is a diagonal matrix, we can rewrite this as ut, Hadamard product of f prime of z. I think this is the first time I've used this little circle for Hadamard product, but it's something that you'll see quite a bit in your network work since it's often used. So when we have two vectors, ut and this vector here, sometimes you want to do an element-wise product. So the output of this will be a vector where you've taken the first element of each and multiply them, the second element of each and multiply them, etc. downwards. And so that's called the Hadamard product, and it's what we're calculating to calculate a vector, which is the gradient of s with respect to b. Okay, so that's good. So we now have a gradient of s with respect to b, and we could use that in our stochastic gradient. But we don't stop there. We also want to work out the gradient with respect to others of our parameters. So we might want to next go on and work out the gradient of s with respect to w. Well, we can use the chain rule just like we did before. So we've got the same product of functions, and everything is going to be the same, apart from now taking the derivatives with respect to w rather than b. So it's now going to be the partial of s with respect to h, h with respect to z, and z with respect to w. And the important thing to notice here, and this leads into what we do with the back propagation algorithm, is wait a minute, that this is very similar to what we've already done. So when we're all working out the gradients of s with respect to b, the first two terms were exactly the same. It's only the last one that differs. So to be able to build or to train neural networks efficiently, this is what happens all the time, and it's absolutely essential that we use an algorithm that avoids repeated computation. And so the idea we're going to develop is when we have this equation stack that there's sort of stuff that's above where we compute z, and we're going to be sort of that'll be the same each time, and we want to compute something from that that we can then sort of feed downwards when working out the gradients with respect to w x or b. And so we do that by defining delta, which is delta is the partial's composed that are above the linear transform, and that's referred to as the local error signal. It's what's being passed in from above to the linear transform. And we've already computed the gradient of that in the preceding slides. And so the final form of the partial s with respect to b will be delta times the remaining part. And well, we'd seen that, you know, for partial s with respect to b, the partial z with respect to b is just the identity. So the end result was delta. But in this time, we then go and have to work out the partial of z with respect to w and multiply that by delta. So that's the part that we still haven't yet done. So and this is where things get in some sense a little bit hairier. And so there's something that's important to explain. So, you know, what should we have for the Jacobian of dsdw? Well, that's a function that has one output, the output is just a score of real number. And then it has n by m inputs. So the Jacobian is a 1 by m matrix. I a very long row vector. But that's correct math. But it turns out that that's kind of bad for our neural networks. Because remember, what we want to do with our neural networks is do stochastic gradient descent. And we want to say theta new equals theta old minus a small multiplier times the gradient. And well, actually, the w matrix is an n by m matrix. And so we couldn't actually do the subtraction if this gradient we calculate is just a huge row vector. We'd like to have it as the same shape as the w matrix. In neural network land, when we do this, we depart from pure math that this point. And we use what we call the shape convention. So what we're going to say is, and you're meant to use this for answers in the assignment, that the shape of the gradient we're always going to make to be the shape of the parameters. And so therefore, the st w we're also going to represent as an n by m matrix just like w. And we're going to reshape the Jacobian, deplace it into this matrix shape. Okay, so if we want to place it into this matrix shape, what do we, what are we going to want to get for the st w? Well, we know that it's going to involve delta our local error signal. And then we have to work out something for d z w. Well, since c equals w x plus b, you'd kind of expect that the answer should be x. And that's right. So the answer to d s d w is going to be delta transpose times x transpose. And so the form that we're getting for this derivative is going to be the product of the local error signal that comes from above versus what we calculate from the local input x. So that shouldn't yet be obvious why that is true. So let me just go through in a bit more detail why that's true. So when we want to work out d s d w, right, it's sort of delta times d z w, where what that's computing for z is w x plus b. So let's just consider for a moment what the derivative is with respect to a single weight w ij. So w ij might be w two three that's shown in my little neural network here. And so the first thing to notice is the w ij only contributes to z i. So it's going into z two, which then computes h two. And it has no effect whatsoever on h one. Okay, so when we're working out d z i d w ij, it's going to be d w i x that sort of row that that row of the matrix plus bi, which means that for we've got a kind of a sum of w i k times x k. And then for this sum, this is like one variable calculus that when we're taking the derivative of this with respect to w ij, every term in this sum is going to be zero. The derivative is going to be zero except for the one that involves w ij. And then the derivative of that is just like a x with respect to a, it's going to be x. So you get x j out as the answer. And so the end result of that is that when we're working out, what we want as the answer is that we're going to get that these columns where x one is all that's left x two is all that's left through x m is all that's left. And then that's multiplied by the vectors of the local error signal from above. And what we want to compute is this outer product matrix, we're getting the different combinations of the delta and the x. And so we can get the n by m matrix that we'd like to have by our shape convention by taking delta transpose, which is n by one times x transpose, which is n one by m. And then we get this outer product matrix. So like that's a kind of a hacky argument that I've made. It's certainly a way of doing things that the dimensions work out and it sort of makes sense. There's a more detailed run through this that appears in election notes. And I encourage you to sort of also look at the more Matthew version of that. Here's a little bit more information about the shape convention. So well, first of all, one more example of this. So when you're working out DSDB that that comes out as it's Jacobian is a row vector. But similarly, you know, according to shape convention, we want our gradient to be the same shape as B and B as a column vector. So that's sort of again, they're different shapes and you have to transpose one to get the other. And so effectively, what we have is a disagreement between the Jacobian form. So the Jacobian form makes sense for you know, calculus and math. Because if you want to have it like I claimed that matrix calculus is just like single variable calculus apart from using vectors and matrices, you can just multiply together the particles. That only works out if you're using Jacobians. But on the other hand, if you want to do stochastic gradient descent and be able to sort of subtract off a piece of the gradient, that only works if you have the same shape matrix for the gradient as you do for the original matrix. And so this is a bit confusing, but that's just the reality. There are both of these two things. So the Jacobian form is useful in doing the calculus. But for the answers in the assignment, we want the answers to be presented using the shape convention so that the gradient is shown in the same shape as the parameters. And therefore, you'll be able to, it's the right shape for doing a gradient update by just subtracting a small amount of the gradient. So for working through things, there are then basically two choices. One choice is to work through all the math using Jacobians and then write at the end to reshape following the shape convention to give the answer. So that's what I did when I worked out DSDB. We worked through it using Jacobians. We got an answer, but it turned out to be a row vector. And so, well, then we have to transpose it at the end to get into the right shape for the shape convention. The alternative is to always follow the shape convention. And that's kind of what I did when I was then working out DSDW. I didn't faultfully use Jacobians. I said, oh, well, when we work out, whatever was DZDW, let's work out what shape we want it to be and what to fill in the cells with. And if you're sort of trying to do it immediately with the shape convention, it's a little bit more hacky in a way since you know, you have to look at the dimensions for what you want and figure out when to transpose or to reshape the matrix to be it the right shape. But the kind of informal reasoning that I gave is what you do and what works. And you know, one way of, and there are sort of hints that you can use, right? That you know that your gradient should always be the same shape as your parameters. And you know that the error message coming in will always have the same dimensionality as that hidden layer. And you can sort of work it out always following the shape convention. Okay. So that is, hey, doing this is all matrix calculus. So after pausing for breath for a second, the rest of the lecture is then, okay, let's look at how our software trains neural networks using what's referred to as the back propagation algorithm. So the short answer is, you know, basically we've already done it, the rest of the lecture is easy. So, you know, essentially I've just shown you what the back propagation algorithm does. So the back propagation algorithm is judiciously taking and propagating derivatives using the matrix chain rule. The rest of the back propagation algorithm is to say, okay, when we have these neural networks, we have a lot of shared structure and shared derivatives. So what we want to do is maximally, efficiently reuse derivatives of higher layers when we're computing derivatives for lower layers so that we minimize computation. And I already pointed that out in the first half, but we want to systematically exploit that. And so the way we do that in our computational systems is they construct computation graphs. So this maybe looks a little bit like what you saw in a compiler's class if you did one, right, that you're creating, I call it here computation graph, but it's really a tree, right. So you're creating here this tree of computations in this case, but in more general case, it's some kind of directed graph of computations, which has source nodes, which are imports either inputs like x or input parameters like w and b. And it's interior nodes or operations. And so then once we've constructed a graph, and so this graph corresponds to exactly the example I did before, right, that this was our little neural net that's in the top right. And here's the corresponding computation graph of computing wx plus b, put it through the sigmoid nonlinearity f, multiply the resulting dot product that the resulting vector with you gives us our output score s. Okay, so what we do to compute this is we pass along the edges the results of operations. So this is wx, then z, then h, and then our output is s. And so the first thing we want to be able to do to compute with neural networks is to be able to compute for different inputs what the output is. And so that's referred to as forward propagation. And so we simply run this expression much like you just standardly do in a compiler to compute the value of s. And that's the forward propagation phase. But the essential additional element of neural networks is that we then also want to be able to send back gradients, which will tell us how to update the parameters of the model. And so it's this ability to send back gradients, which gives us the ability for these models to learn once we have a loss function at the end, we can work out how to change the parameters of the model so that they more accurately produce the desired output, i.e. they minimize the loss. And so it's doing that part that then is called back propagation. So we then once we forward propagated a value with our current parameters, we then head backwards reversing the direction of the arrows and pass along gradients down to the different parameters like B and W and U that we can use to change using stochastic gradient descent what the value of B is or what the value of W is. So we start off with ds ds, which is just one. And then we run our back propagation. And we're using the sort of same kind of composition of Jacobian. So we have ds dh here and ds dz and we progressively pass back those gradients. So we just need to work out how to efficiently and cleanly do this in a computational system. And so let's sort of work through again a few of these cases. So the general situation is we have a particular node. So a node is where some kind of operation like multiplication or a nonlinearity happens. And so the simplest case is that we've got one output and one input. So we'll do that first. So that's like h equals f of z. So what we have is an upstream gradient ds dh. And what we want to do is compute the downstream gradient of ds dz. And the way we're going to do that is say, well, for this function f, it's a function, it's got a derivative for gradient. So what we want to do is work out that local gradient dhd. And then that gives us everything that we need to work out ds dz. Because that's precisely we're going to use the chain rule. We're going to say that ds dz equals the product of ds dh times dhd where this is again using Jacobians. Okay, so the general principle that we're going to use is that downstream gradient equals the upstream gradient times the local gradient. Okay, sometimes it gets a little bit more complicated. So we might have multiple inputs to a function. So this is the matrix vector multiply. So z equals wx. Okay, when there are multiple inputs, we still have an upstream gradient ds dz. But what we're going to do is work out a local gradient with respect to each input. So we have dz dw and dz dx. And so then at that point, it's exactly the same for each piece of it. We're going to work out the downstream gradients ds dw and ds dx by using the chain rule with respect to the particular local gradient. So let's go through an example of this. I mean, this is kind of a silly example. It's not really an example that looks like a typical neural net. But it's sort of a simple example where we can show some of the components of what we do. So what we're going to do is want to calculate f of xyz, which is being calculated as x plus y times the max of y and z. And we've got, you know, particular values that we're starting off with x equals one y equals two and z equals zero. So these are the current values of our parameters. And so we can say, okay, well, we want to build an expression tree for that. Here's our expression tree. We're taking x plus y. We're taking the max of y and z. And then we're multiplying them. And so our forward propagation phase is just to run this. So we take the values of our parameters. And we simply start to compute with them, right? So we have one, two, two, zero. And we add them as three, the max is two. We multiply them. And that gives us six. Okay. So then at that point, we then want to go and work out how to do things for back propagation and how these back propagation steps work. And so the first part of that is sort of working out what our local gradients are going to be. So, so this is a here. And this is x and y. So dADX, since a equals x plus y is just going to be one. And dADY is also going to be one. Then for b equals the max of y z. So this is this max node. So the local gradients for that is it's going to depend on y, where the y is greater than z. So dBDY is going to be one, if and only if y is greater than z, which it is at our particular point here. So that's one. And dBdz is going to be one only if z is greater than y. So for our particular values here, that one is going to be zero. And then finally, here, we're calculating the product f equals a b. So for that, we're going to, sorry, that slides all in perfect. Okay, so for the product, the derivative f with respect to a is equal to b, which is two. And the derivative f with respect to b is a equals three. So that gives us all of the local gradients at each node. And so then to run back propagation, we start with dF dF, which is just one. And then we're going to work out the downstream equals the upstream times the local. Okay, so the local, so when you have a product like this, note the sort of the gradients flip. So we take upstream times the local, which is two. Oops. So the downstream is two on this side. DFDB is three. So we're taking upstream times local. That gives us three. And so that gives us back propagates values to the plus and max nodes. And so then we continue along. So for the max node, the local gradient dBDY equals one. So we're going to take upstream as three. So it's going to take three times one. And that gives us three. DBDC is zero because of the fact that Z's value is not the max. So we're taking three times zero and saying that the gradient there is zero. So finally, doing the plus node, the local gradients for both x and y, there are one. So we're just getting two times one in both cases. And we're saying the gradients there are two. Okay. And so again, at the end of the day, the interpretation here is that this is giving us this information as to if we wiggle the values of x, y and z, how much of a difference does it make to the output? What is the slope, the gradient, with respect to the variable? So what we've seen is that since Z isn't the max of y and z, if I change the value of z a little, like I find, z.1 or minus.1, it makes no difference at all to what I compute as the output. So therefore, the gradient there is zero. If I change the value of x a little, then that is going to have an effect. And it's going to affect the output by twice as much as the amount I change it. Right. So, and that's because the df dz equals two. So interestingly, so I mean, we can basically work that out. So if we imagine making sort of x 2.1, well, then what we'd calculate the max is two. Oh, sorry, sorry, if we make x 1.1, we then get the max here is two, and we get 1.1 plus two is 3.1. So we get 3.1 times two. So that'd be about 6.2. So changing x by 0.1 has added 0.2 to the value of f. Conversely, for the value of y, we find that the df dy equals five. So what we do when we've got two things coming out here, as I'll go through again in a moment, is with summing the gradient. So again, three plus two equals five. And empirically, that's what happens. So if we consider fiddling the value of y a little, let's say we make it a value of 2.1, then the prediction is they'll have five times as big an effect on the output value we compute. And well, what do we compute? So we compute 1 plus 2.1. So that's 3.1. And we compute the max of 2.1 and 0 is 2.1. So we'll take the product of 2.1 and 3.1. And I calculate that in advance, as I can't really do this arithmetic in my head. And the product of those two is 6.51. So it has gone up about by 0.5. So we've multiplied my fiddly at by 0.1 by five times to work out the magnitude of the effect of the output. Okay. So for this start, you know, before I did the case of, you know, when we had one one in and one out here and multiple inns and one out here, the case that I had actually dealt with is the case of when you have multiple outward branches, but that then turned up in the computation of y. So once you have multiple outward branches, what you're doing is your summing. So that when you want to work out the dfdy, you've got a local gradient, you've got two upstream gradients. And you're working it out with respect to each of them as in the chain rule, and then you're summing them together to work out the impact at the end. Right. So we also saw some of the other node intuitions, which it's useful to have doing this. So when you have an addition, that distributes the upstream gradient to each of the things the lowered. When you have max, it's like a routing node. So when you have max, you have the upstream gradient, and it goes to one of the branches below it and the rest of them get no gradient. When you then have a multiplication, it has this effect of switching the gradient. So if you're taking three by two, the gradient on the two side is three, and on the three side is two. And if you think about in terms of how much effect you get from when you're doing this sort of wiggling, that totally makes sense, right? Because if you're multiplying another number by three, then any change here is going to be multiplied by three and vice versa. Okay. So this is the kind of computation graph that we want to use to work out derivatives in an automated computational fashion, which is the basis of the back propagation algorithm. But at that point, this is what we're doing, but there's still one mistake that we can make. It would be wrong for us to sort of say, okay, well, first of all, we want to work out DSDB. So look, we can start up here. We can propagate our upstream errors, work out local gradients, upstream error, local gradient, and keep all the way down and get the DSDB down here. Okay, next we want to do it for DSDW. Let's just run it all over again. Because if we do that, we'd be doing repeated computation, as I showed in the first half, that this term is the same both times, this term is the same both times, this term is the same both times, that only the bits at the end differ. So what we want to do is avoid duplicated computation and compute all the gradients that we're going to need, successively, so that we only do them once. And so that was analogous to when I introduced this delta variable when we computed gradients by hand. So starting off here from DSD, we're starting off here with DSDS is one. We then want to one time compute gradients in the green here, one time compute the gradient and green here, that's all common work. Then we're going to take the local gradient for DZDB and multiply that by the upstream gradient to have worked out DSDB. And then we're going to take the same upstream gradient and then work out the local gradient here and then propagate that down to give us DSDW. So the end result is we want to systematically work to forward computation forward in the graph and backward computation, back propagation, backward in the graph in a way that we do things efficiently. So this is the general form of the algorithm which works for an arbitrary computation graph. So at the end of the day, we've got a single scalar output Z and then we have inputs and parameters which compute Z. And so once we have this computation graph and I added in this funky extra arrow here to make it a more general computation graph, well we can always say that we can work out a starting point, something that doesn't depend on anything. So in this case both of these bottom two nodes don't depend on anything else. So we can start with them and we can start to compute forward. We can compute values for all of these sort of second row from the bottom nodes and then we're able to compute the third lens up. So we can have a top logical sort of the nodes based on the dependencies in this directed graph and we can compute the value of each node given some subset of its pre-decesses which it depends on. And so doing that as referred to as the forward propagation phase and gives us a computation of the scalar output Z using our current parameters and our current inputs. And so then after that we run back propagation. So for back propagation we initialize the output gradient, DZ, DZ as one and then we visit nodes in the reverse order of the top logical sort and we compute the gradients downward. And so our recipe is that for each node as we head down, we're going to compute the gradient of the node with respect to its successes, the things that it feeds into. And how we compute that gradient is using this chain rule that we've looked at. So this is sort of the generalized form of the chain rule where we have multiple outputs. And so we're summing over the different outputs. And then for each output we're computing the product of the upstream gradient and the local gradient with respect to that node. And so we head downwards. And we continue down the reverse top logical sort order and we work out the gradient with respect to each variable in this graph. And so it hopefully looks kind of intuitive looking at this picture that if you think of it like this, the big oak complexity of forward propagation and backward propagation is the same. Right. In both cases you're doing a linear pass through all of these nodes and calculating values given predecessors and then values given successors. I mean you have to do a little bit more work is for working out the gradients sort of as shown by this chain rule that it's the same big oak complexity. So if somehow you're implementing stuff for yourself rather than relying on the software and you're calculating the gradients of a different order of complexity of forward propagation, it means that you're doing something wrong. You're doing repeated work that you shouldn't have to do. Okay. So this algorithm works for a completely arbitrary computation graph, any directed a cyclic graph. You can apply this algorithm. In general, what we find is that we build neural networks that have a regular layer structure. So we have things like a vector of inputs and then that's multiplied by matrix. It's transformed into another vector which might be multiplied by another matrix or some with another matrix or something. Right. So once we're using that kind of regular layer structure, we can then parallelize the computation by working out the gradients in terms of Jacobians of vectors and matrices and do things in parallel much more efficiently. Okay. So doing this is then referred to as automatic differentiation. And so essentially if you know the computation graph, you should be able to have your computer, clever computer system work out what the derivatives of everything is and then apply back propagation to work out how to update the parameters and learn. And there's actually a sort of an interesting sort of thing of how history has gone backwards here, which I'll just note. So some of you might be familiar with symbolic computation packages. So those are things like mathematical. So mathematical, you can give it a symbolic form of a computation and then it can work out derivatives for you. So it should be the case that if you give a complete symbolic form of a computation graph, then it should be able to work out all the derivatives for you and you never have to work out a derivative by hand whatsoever. And that was actually attempted in the famous deep learning library called Fiano, which came out of Joshua Bendios group at the University of Montreal that had a compiler that did that kind of symbolic manipulation. But you know somehow that sort of proved a little bit too hard a road to follow. I imagine it actually might come back again in the future. And so for modern deep learning frameworks, which includes both TensorFlow or PyTorch, they do 90% of this computation of automatic differentiation for you, but they don't actually symbolically compute derivatives. So for each particular node or layer of your deep learning system, somebody, either you or the person who wrote that layer, has handwritten the local derivatives. But then everything from that point on, the sort of the taking, doing the chain rule of combining upstream gradients with local gradients to work out downstream gradients, that's then all being done automatically for back propagation on the computation graph. And so that what that means is for a whole neural network, you have a computation graph, and it's going to have a forward pass and a backward pass. And so for the forward pass, you're topologically sorting the nodes based on their dependencies and the computation graph. And then for each node, you're running forward, the forward computation on that node. And then for backward propagation, you're reversing the topological sort of the graph. And then for each node in the graph, you're running the backward propagation, which is a little bit of back crop, the chain rule at that node. And then the result of doing that is you have gradients for your inputs and parameters. And so this is the overall software runs this for you. And so what you want to do is then actually have stuff for particular nodes or layers in the graph. So if I have a multiply gate, it's going to have a forward algorithm, which just computes that the output is x times y in terms of the two inputs. And then I'm going to want to compute, to tell it also how to calculate the local derivative. So I want to say, what is the local derivative? So dL dx and dL dy in terms of the upstream gradient, dL dz. And so I will then manually work out how to calculate that. And normally, what I have to do is I assume the forward pass is being run first. And I'm going to shove into some local variables for my class, the values that we used in the forward computation. So as well as computing z equals x times y, I'm going to sort of remember what x and y were. So then when I'm asked to compute the backward pass, I'm then going to have implemented here what we saw earlier of that when it's xy, you're going to sort of swap the y and the x to work out the local gradients. And so then I'm going to multiply those by the upstream gradient. And I'm going to return, I've just written it here as a sort of a little list, but really it's going to be a numpy vector of the gradients. Okay, so that's 98% of what I wanted to cover today, just a couple of quick comments left. So that can and should all be automated. Sometimes you want to just check if you're computing the right gradients. And so the standard way of checking that you're computing the right gradients is to manually work out the gradient by doing a numeric calculation of the gradient. And so you can do that. So you can work out what the derivative of x of f with respect to x should be by choosing some sort of small number like 10 to the minus 4, adding it to x, subtracting it from x. And then so the difference between these numbers is 2h, dividing it through by 2h. And you're simply working out the rise over the run, which is the slope at that point with respect to x. And that's an approximation of the gradient of f with respect to x at that value of x. So this is so simple, you can't make a mistake implementing it. And so therefore you can use this to check where your gradient values are correct or not. This isn't something that you'd want to use much because not only is it approximate that it's extremely slow. Because to work this out you have to run the forward computation for every parameter of the model. So if you have a model with a million parameters, you're now doing a million times as much work to run backprop as you would do if you're actually using calculus. So calculus is a good thing to know. But it can be really useful to check that the right values are being calculated. And the old days when we hand wrote everything, this was kind of the key unit test that people used everywhere. These days most of the time you're reusing layers that are built into PyTorch or some other deep learning framework. So it's much less needed. But sometimes you're implementing your own layer and you really do want to check the things are implemented correctly. There's a fine point in the way this has written. If you saw this in sort of high school calculus class, you will have seen rise over run of f of x plus h minus f of x divided by h. It turns out that doing this two-sided estimate like this is much, much more accurate than doing a one-sided estimate. And so you're really much encouraged to use this approximation. Okay, so at that point, we've mastered the core technology of neural nets. Back propagation is recursively and hence efficiently applying the chain rule along the computation graph with this sort of key step that downstream gradient equals upstream gradient times local gradient. And so for calculating with neural nets, we do the forward pass to work out values with current parameters, then run back propagation to work out the gradient of the loss currently computer loss with respect to those parameters. Now to some extent, you know, with modern deep learning frameworks, you don't actually have to know how to do any of this, right? It's the same as you don't have to know how to implement a C compiler. You can just write C code and say GCC and it'll compile it and it'll run the right stuff for you. And that's the kind of functionality you get from the PyTorch framework. So do come along to the PyTorch tutorial this Friday and get a sense about how easy it is to write neural networks using a framework like PyTorch or TensorFlow. And you know, it's so easy. That's why high school students across the nation are now doing their science projects, training deep learning systems because you don't actually have to understand very much to bung a few neural network layers together and set it computing on some data. But you know, we hope in this class that you actually are also learning how these things that implemented. So you have a deeper understanding of than that. And you know, it turns out that sometimes you need to have a deeper understanding. So back propagation doesn't always work carefully, perfectly. And so understanding what it's really doing can be crucial to debugging things. And so we'll actually see an example of that fairly soon when we start looking at recurrent models and some of the problems that they have, which will require us to think a bit more deeply about what's happening in our gradient computations. Okay, that's it for the day.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.200000000000001, "text": " Hi everyone, I'll get started. Okay, so we're now back to the second week of CS224N on", "tokens": [2421, 1518, 11, 286, 603, 483, 1409, 13, 1033, 11, 370, 321, 434, 586, 646, 281, 264, 1150, 1243, 295, 9460, 17, 7911, 45, 322], "temperature": 0.0, "avg_logprob": -0.27950365883963446, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.08092334121465683}, {"id": 1, "seek": 0, "start": 14.200000000000001, "end": 19.84, "text": " Natural Language Processing with Deep Learning. Okay, so for today's lecture, what we're", "tokens": [20137, 24445, 31093, 278, 365, 14895, 15205, 13, 1033, 11, 370, 337, 965, 311, 7991, 11, 437, 321, 434], "temperature": 0.0, "avg_logprob": -0.27950365883963446, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.08092334121465683}, {"id": 2, "seek": 0, "start": 19.84, "end": 27.0, "text": " going to be looking at is all the math details of during neural net learning. First of", "tokens": [516, 281, 312, 1237, 412, 307, 439, 264, 5221, 4365, 295, 1830, 18161, 2533, 2539, 13, 2386, 295], "temperature": 0.0, "avg_logprob": -0.27950365883963446, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.08092334121465683}, {"id": 3, "seek": 2700, "start": 27.0, "end": 33.88, "text": " all, looking at how we can work out by hand, gradients for training neural networks, and", "tokens": [439, 11, 1237, 412, 577, 321, 393, 589, 484, 538, 1011, 11, 2771, 2448, 337, 3097, 18161, 9590, 11, 293], "temperature": 0.0, "avg_logprob": -0.21169000384451328, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0002902389387600124}, {"id": 4, "seek": 2700, "start": 33.88, "end": 40.6, "text": " then looking at how it's done more algorithmically, which is known as the back propagation algorithm.", "tokens": [550, 1237, 412, 577, 309, 311, 1096, 544, 9284, 984, 11, 597, 307, 2570, 382, 264, 646, 38377, 9284, 13], "temperature": 0.0, "avg_logprob": -0.21169000384451328, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0002902389387600124}, {"id": 5, "seek": 2700, "start": 40.6, "end": 47.36, "text": " And correspondingly for you guys, well, I hope you remembered that one minute ago was when", "tokens": [400, 11760, 356, 337, 291, 1074, 11, 731, 11, 286, 1454, 291, 13745, 300, 472, 3456, 2057, 390, 562], "temperature": 0.0, "avg_logprob": -0.21169000384451328, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0002902389387600124}, {"id": 6, "seek": 2700, "start": 47.36, "end": 52.96, "text": " assignment one was due and everyone has handed that in. If I sometimes who haven't handed", "tokens": [15187, 472, 390, 3462, 293, 1518, 575, 16013, 300, 294, 13, 759, 286, 2171, 567, 2378, 380, 16013], "temperature": 0.0, "avg_logprob": -0.21169000384451328, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.0002902389387600124}, {"id": 7, "seek": 5296, "start": 52.96, "end": 58.4, "text": " in, really should have been as soon as possible best to preserve those late days for the", "tokens": [294, 11, 534, 820, 362, 668, 382, 2321, 382, 1944, 1151, 281, 15665, 729, 3469, 1708, 337, 264], "temperature": 0.0, "avg_logprob": -0.19598251918576798, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.0006620605709031224}, {"id": 8, "seek": 5296, "start": 58.4, "end": 64.8, "text": " harder assignments. So I mean, I actually forgot to mention, we actually did make one change", "tokens": [6081, 22546, 13, 407, 286, 914, 11, 286, 767, 5298, 281, 2152, 11, 321, 767, 630, 652, 472, 1319], "temperature": 0.0, "avg_logprob": -0.19598251918576798, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.0006620605709031224}, {"id": 9, "seek": 5296, "start": 64.8, "end": 69.64, "text": " for this year to make it a bit easier when occasionally people join the class a week", "tokens": [337, 341, 1064, 281, 652, 309, 257, 857, 3571, 562, 16895, 561, 3917, 264, 1508, 257, 1243], "temperature": 0.0, "avg_logprob": -0.19598251918576798, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.0006620605709031224}, {"id": 10, "seek": 5296, "start": 69.64, "end": 76.0, "text": " wait. If you want to this year and the grading assignment one can be discounted and will", "tokens": [1699, 13, 759, 291, 528, 281, 341, 1064, 293, 264, 35540, 15187, 472, 393, 312, 11635, 292, 293, 486], "temperature": 0.0, "avg_logprob": -0.19598251918576798, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.0006620605709031224}, {"id": 11, "seek": 5296, "start": 76.0, "end": 81.88, "text": " just use your other four assignments. But if you've been in the class so far for that 98%", "tokens": [445, 764, 428, 661, 1451, 22546, 13, 583, 498, 291, 600, 668, 294, 264, 1508, 370, 1400, 337, 300, 20860, 4], "temperature": 0.0, "avg_logprob": -0.19598251918576798, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.0006620605709031224}, {"id": 12, "seek": 8188, "start": 81.88, "end": 86.83999999999999, "text": " of people, well since assignment one is the easiest assignment, again, it's silly not", "tokens": [295, 561, 11, 731, 1670, 15187, 472, 307, 264, 12889, 15187, 11, 797, 11, 309, 311, 11774, 406], "temperature": 0.0, "avg_logprob": -0.19042793361619972, "compression_ratio": 1.680952380952381, "no_speech_prob": 8.204759797081351e-05}, {"id": 13, "seek": 8188, "start": 86.83999999999999, "end": 94.24, "text": " to do it and have it as part of your grade. Okay, so starting today, we've put out assignment", "tokens": [281, 360, 309, 293, 362, 309, 382, 644, 295, 428, 7204, 13, 1033, 11, 370, 2891, 965, 11, 321, 600, 829, 484, 15187], "temperature": 0.0, "avg_logprob": -0.19042793361619972, "compression_ratio": 1.680952380952381, "no_speech_prob": 8.204759797081351e-05}, {"id": 14, "seek": 8188, "start": 94.24, "end": 99.91999999999999, "text": " two and assignment two is all about making sure you really understand the math of neural", "tokens": [732, 293, 15187, 732, 307, 439, 466, 1455, 988, 291, 534, 1223, 264, 5221, 295, 18161], "temperature": 0.0, "avg_logprob": -0.19042793361619972, "compression_ratio": 1.680952380952381, "no_speech_prob": 8.204759797081351e-05}, {"id": 15, "seek": 8188, "start": 99.91999999999999, "end": 107.44, "text": " networks and then the software that we use to do that math. So this is going to be a", "tokens": [9590, 293, 550, 264, 4722, 300, 321, 764, 281, 360, 300, 5221, 13, 407, 341, 307, 516, 281, 312, 257], "temperature": 0.0, "avg_logprob": -0.19042793361619972, "compression_ratio": 1.680952380952381, "no_speech_prob": 8.204759797081351e-05}, {"id": 16, "seek": 10744, "start": 107.44, "end": 114.16, "text": " bit of a tough week for some. So for some people who are great on all their math and backgrounds,", "tokens": [857, 295, 257, 4930, 1243, 337, 512, 13, 407, 337, 512, 561, 567, 366, 869, 322, 439, 641, 5221, 293, 17336, 11], "temperature": 0.0, "avg_logprob": -0.12885008034882722, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00017523161659482867}, {"id": 17, "seek": 10744, "start": 114.16, "end": 119.12, "text": " they'll feel like this is stuff they know. Well, nothing very difficult, but I know there", "tokens": [436, 603, 841, 411, 341, 307, 1507, 436, 458, 13, 1042, 11, 1825, 588, 2252, 11, 457, 286, 458, 456], "temperature": 0.0, "avg_logprob": -0.12885008034882722, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00017523161659482867}, {"id": 18, "seek": 10744, "start": 119.12, "end": 126.44, "text": " are quite a few of you who this lecture and week is the biggest struggle of the course.", "tokens": [366, 1596, 257, 1326, 295, 291, 567, 341, 7991, 293, 1243, 307, 264, 3880, 7799, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.12885008034882722, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00017523161659482867}, {"id": 19, "seek": 10744, "start": 126.44, "end": 131.68, "text": " We really do want people to actually have an understanding of what goes on in your network", "tokens": [492, 534, 360, 528, 561, 281, 767, 362, 364, 3701, 295, 437, 1709, 322, 294, 428, 3209], "temperature": 0.0, "avg_logprob": -0.12885008034882722, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00017523161659482867}, {"id": 20, "seek": 10744, "start": 131.68, "end": 137.32, "text": " learning rather than viewing it as some kind of deep magic. And I hope that some of", "tokens": [2539, 2831, 813, 17480, 309, 382, 512, 733, 295, 2452, 5585, 13, 400, 286, 1454, 300, 512, 295], "temperature": 0.0, "avg_logprob": -0.12885008034882722, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00017523161659482867}, {"id": 21, "seek": 13732, "start": 137.32, "end": 142.92, "text": " the material we give today and that you read up on and use in the assignment will really", "tokens": [264, 2527, 321, 976, 965, 293, 300, 291, 1401, 493, 322, 293, 764, 294, 264, 15187, 486, 534], "temperature": 0.0, "avg_logprob": -0.11353734027908509, "compression_ratio": 1.7303921568627452, "no_speech_prob": 3.994594226242043e-05}, {"id": 22, "seek": 13732, "start": 142.92, "end": 149.4, "text": " give you more of a sense of what these neural networks are doing and how it is just math", "tokens": [976, 291, 544, 295, 257, 2020, 295, 437, 613, 18161, 9590, 366, 884, 293, 577, 309, 307, 445, 5221], "temperature": 0.0, "avg_logprob": -0.11353734027908509, "compression_ratio": 1.7303921568627452, "no_speech_prob": 3.994594226242043e-05}, {"id": 23, "seek": 13732, "start": 149.4, "end": 155.0, "text": " that's applied in a systematic large scale that works out the answers and that this will", "tokens": [300, 311, 6456, 294, 257, 27249, 2416, 4373, 300, 1985, 484, 264, 6338, 293, 300, 341, 486], "temperature": 0.0, "avg_logprob": -0.11353734027908509, "compression_ratio": 1.7303921568627452, "no_speech_prob": 3.994594226242043e-05}, {"id": 24, "seek": 13732, "start": 155.0, "end": 160.35999999999999, "text": " be valuable and give you a deeper sense of what's going on. But if this material seems", "tokens": [312, 8263, 293, 976, 291, 257, 7731, 2020, 295, 437, 311, 516, 322, 13, 583, 498, 341, 2527, 2544], "temperature": 0.0, "avg_logprob": -0.11353734027908509, "compression_ratio": 1.7303921568627452, "no_speech_prob": 3.994594226242043e-05}, {"id": 25, "seek": 16036, "start": 160.36, "end": 168.12, "text": " very scary and difficult, you can take some refuge in the fact that there's fast light", "tokens": [588, 6958, 293, 2252, 11, 291, 393, 747, 512, 10991, 294, 264, 1186, 300, 456, 311, 2370, 1442], "temperature": 0.0, "avg_logprob": -0.1319796134685648, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.497294553904794e-05}, {"id": 26, "seek": 16036, "start": 168.12, "end": 173.60000000000002, "text": " at the end of the tunnel since this is really the only lecture that's heavily going through", "tokens": [412, 264, 917, 295, 264, 13186, 1670, 341, 307, 534, 264, 787, 7991, 300, 311, 10950, 516, 807], "temperature": 0.0, "avg_logprob": -0.1319796134685648, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.497294553904794e-05}, {"id": 27, "seek": 16036, "start": 173.60000000000002, "end": 178.8, "text": " the math details of neural networks. After that, we'll be kind of popping back up to a", "tokens": [264, 5221, 4365, 295, 18161, 9590, 13, 2381, 300, 11, 321, 603, 312, 733, 295, 18374, 646, 493, 281, 257], "temperature": 0.0, "avg_logprob": -0.1319796134685648, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.497294553904794e-05}, {"id": 28, "seek": 16036, "start": 178.8, "end": 185.60000000000002, "text": " higher level and by and large, after this week, we'll be making use of software to do a", "tokens": [2946, 1496, 293, 538, 293, 2416, 11, 934, 341, 1243, 11, 321, 603, 312, 1455, 764, 295, 4722, 281, 360, 257], "temperature": 0.0, "avg_logprob": -0.1319796134685648, "compression_ratio": 1.6118721461187215, "no_speech_prob": 2.497294553904794e-05}, {"id": 29, "seek": 18560, "start": 185.6, "end": 193.28, "text": " lot of the complicated math for us. But nevertheless, I hope this is valuable. I'll go through everything", "tokens": [688, 295, 264, 6179, 5221, 337, 505, 13, 583, 26924, 11, 286, 1454, 341, 307, 8263, 13, 286, 603, 352, 807, 1203], "temperature": 0.0, "avg_logprob": -0.13671366373697916, "compression_ratio": 1.5481171548117154, "no_speech_prob": 5.214670454734005e-05}, {"id": 30, "seek": 18560, "start": 193.28, "end": 198.79999999999998, "text": " quickly today, but if this isn't stuff that you know backwards, I really do encourage", "tokens": [2661, 965, 11, 457, 498, 341, 1943, 380, 1507, 300, 291, 458, 12204, 11, 286, 534, 360, 5373], "temperature": 0.0, "avg_logprob": -0.13671366373697916, "compression_ratio": 1.5481171548117154, "no_speech_prob": 5.214670454734005e-05}, {"id": 31, "seek": 18560, "start": 198.79999999999998, "end": 207.07999999999998, "text": " you to work through it and get help as you need it. So do come along to our office hours.", "tokens": [291, 281, 589, 807, 309, 293, 483, 854, 382, 291, 643, 309, 13, 407, 360, 808, 2051, 281, 527, 3398, 2496, 13], "temperature": 0.0, "avg_logprob": -0.13671366373697916, "compression_ratio": 1.5481171548117154, "no_speech_prob": 5.214670454734005e-05}, {"id": 32, "seek": 18560, "start": 207.07999999999998, "end": 212.2, "text": " There are also a number of pieces of tutorial material given in the syllabus. So there's", "tokens": [821, 366, 611, 257, 1230, 295, 3755, 295, 7073, 2527, 2212, 294, 264, 48077, 13, 407, 456, 311], "temperature": 0.0, "avg_logprob": -0.13671366373697916, "compression_ratio": 1.5481171548117154, "no_speech_prob": 5.214670454734005e-05}, {"id": 33, "seek": 21220, "start": 212.2, "end": 218.44, "text": " both the lecture notes. There's some materials from CS231. In the list of readings, the", "tokens": [1293, 264, 7991, 5570, 13, 821, 311, 512, 5319, 490, 9460, 9356, 16, 13, 682, 264, 1329, 295, 27319, 11, 264], "temperature": 0.0, "avg_logprob": -0.13776782842782828, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.00011213996185688302}, {"id": 34, "seek": 21220, "start": 218.44, "end": 226.04, "text": " very top reading is some material put together by Kevin Clark a couple of years ago. And", "tokens": [588, 1192, 3760, 307, 512, 2527, 829, 1214, 538, 9954, 18572, 257, 1916, 295, 924, 2057, 13, 400], "temperature": 0.0, "avg_logprob": -0.13776782842782828, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.00011213996185688302}, {"id": 35, "seek": 21220, "start": 226.04, "end": 232.2, "text": " actually, that one's my favorite. The presentation there fairly closely follows the presentation", "tokens": [767, 11, 300, 472, 311, 452, 2954, 13, 440, 5860, 456, 6457, 8185, 10002, 264, 5860], "temperature": 0.0, "avg_logprob": -0.13776782842782828, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.00011213996185688302}, {"id": 36, "seek": 21220, "start": 232.2, "end": 237.28, "text": " in this lecture of going through matrix calculus. So, you know, personally, I'd recommend", "tokens": [294, 341, 7991, 295, 516, 807, 8141, 33400, 13, 407, 11, 291, 458, 11, 5665, 11, 286, 1116, 2748], "temperature": 0.0, "avg_logprob": -0.13776782842782828, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.00011213996185688302}, {"id": 37, "seek": 21220, "start": 237.28, "end": 241.35999999999999, "text": " starting with that one, but there are four different ones you can choose from. If one", "tokens": [2891, 365, 300, 472, 11, 457, 456, 366, 1451, 819, 2306, 291, 393, 2826, 490, 13, 759, 472], "temperature": 0.0, "avg_logprob": -0.13776782842782828, "compression_ratio": 1.6327272727272728, "no_speech_prob": 0.00011213996185688302}, {"id": 38, "seek": 24136, "start": 241.36, "end": 249.36, "text": " of them seems more helpful to you. Two other things on what's coming up. Actually, for Thursday's", "tokens": [295, 552, 2544, 544, 4961, 281, 291, 13, 4453, 661, 721, 322, 437, 311, 1348, 493, 13, 5135, 11, 337, 10383, 311], "temperature": 0.0, "avg_logprob": -0.13710130835479162, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.0002862504043150693}, {"id": 39, "seek": 24136, "start": 249.36, "end": 254.92000000000002, "text": " lecture, we make a big change. And Thursday's lecture is probably the most linguistic", "tokens": [7991, 11, 321, 652, 257, 955, 1319, 13, 400, 10383, 311, 7991, 307, 1391, 264, 881, 43002], "temperature": 0.0, "avg_logprob": -0.13710130835479162, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.0002862504043150693}, {"id": 40, "seek": 24136, "start": 254.92000000000002, "end": 260.0, "text": " lecture of the whole class where we go through the details of dependency grammar and dependency", "tokens": [7991, 295, 264, 1379, 1508, 689, 321, 352, 807, 264, 4365, 295, 33621, 22317, 293, 33621], "temperature": 0.0, "avg_logprob": -0.13710130835479162, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.0002862504043150693}, {"id": 41, "seek": 24136, "start": 260.0, "end": 264.04, "text": " parsing. Some people find that tough as well, but at least there'll be tough in a different", "tokens": [21156, 278, 13, 2188, 561, 915, 300, 4930, 382, 731, 11, 457, 412, 1935, 456, 603, 312, 4930, 294, 257, 819], "temperature": 0.0, "avg_logprob": -0.13710130835479162, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.0002862504043150693}, {"id": 42, "seek": 24136, "start": 264.04, "end": 270.92, "text": " way. And then one other really good opportunity is this Friday, we have our second tutorial", "tokens": [636, 13, 400, 550, 472, 661, 534, 665, 2650, 307, 341, 6984, 11, 321, 362, 527, 1150, 7073], "temperature": 0.0, "avg_logprob": -0.13710130835479162, "compression_ratio": 1.6476868327402134, "no_speech_prob": 0.0002862504043150693}, {"id": 43, "seek": 27092, "start": 270.92, "end": 276.2, "text": " at 10am, which is an introduction to PyTorch, which is the deep learning framework that", "tokens": [412, 1266, 335, 11, 597, 307, 364, 9339, 281, 9953, 51, 284, 339, 11, 597, 307, 264, 2452, 2539, 8388, 300], "temperature": 0.0, "avg_logprob": -0.2023090606040143, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0001667842734605074}, {"id": 44, "seek": 27092, "start": 276.2, "end": 281.44, "text": " will be using for the rest of the class. Once we've gone through these first two assignments", "tokens": [486, 312, 1228, 337, 264, 1472, 295, 264, 1508, 13, 3443, 321, 600, 2780, 807, 613, 700, 732, 22546], "temperature": 0.0, "avg_logprob": -0.2023090606040143, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0001667842734605074}, {"id": 45, "seek": 27092, "start": 281.44, "end": 286.96000000000004, "text": " where you do things by yourself. So this is a great chance, again, to intro to PyTorch.", "tokens": [689, 291, 360, 721, 538, 1803, 13, 407, 341, 307, 257, 869, 2931, 11, 797, 11, 281, 12897, 281, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.2023090606040143, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0001667842734605074}, {"id": 46, "seek": 27092, "start": 286.96000000000004, "end": 295.68, "text": " It will be really useful for later in the class. Okay. Today's material is really all about", "tokens": [467, 486, 312, 534, 4420, 337, 1780, 294, 264, 1508, 13, 1033, 13, 2692, 311, 2527, 307, 534, 439, 466], "temperature": 0.0, "avg_logprob": -0.2023090606040143, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0001667842734605074}, {"id": 47, "seek": 29568, "start": 295.68, "end": 301.76, "text": " sort of the math of neural networks, but just to sort of introduce a setting where we can", "tokens": [1333, 295, 264, 5221, 295, 18161, 9590, 11, 457, 445, 281, 1333, 295, 5366, 257, 3287, 689, 321, 393], "temperature": 0.0, "avg_logprob": -0.13532467948065863, "compression_ratio": 1.6318181818181818, "no_speech_prob": 6.096429206081666e-05}, {"id": 48, "seek": 29568, "start": 301.76, "end": 308.84000000000003, "text": " work through this, I'm going to introduce a simple NLP task and a simple form of classifier", "tokens": [589, 807, 341, 11, 286, 478, 516, 281, 5366, 257, 2199, 426, 45196, 5633, 293, 257, 2199, 1254, 295, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.13532467948065863, "compression_ratio": 1.6318181818181818, "no_speech_prob": 6.096429206081666e-05}, {"id": 49, "seek": 29568, "start": 308.84000000000003, "end": 314.64, "text": " that we can use for it. So the task of named entity recognition is a very common basic", "tokens": [300, 321, 393, 764, 337, 309, 13, 407, 264, 5633, 295, 4926, 13977, 11150, 307, 257, 588, 2689, 3875], "temperature": 0.0, "avg_logprob": -0.13532467948065863, "compression_ratio": 1.6318181818181818, "no_speech_prob": 6.096429206081666e-05}, {"id": 50, "seek": 29568, "start": 314.64, "end": 320.64, "text": " NLP task. And the goal of this is you're looking through pieces of text and you're wanting", "tokens": [426, 45196, 5633, 13, 400, 264, 3387, 295, 341, 307, 291, 434, 1237, 807, 3755, 295, 2487, 293, 291, 434, 7935], "temperature": 0.0, "avg_logprob": -0.13532467948065863, "compression_ratio": 1.6318181818181818, "no_speech_prob": 6.096429206081666e-05}, {"id": 51, "seek": 32064, "start": 320.64, "end": 326.8, "text": " to label by labeling the words, which words belong to entity categories like persons,", "tokens": [281, 7645, 538, 40244, 264, 2283, 11, 597, 2283, 5784, 281, 13977, 10479, 411, 14453, 11], "temperature": 0.0, "avg_logprob": -0.19531143795360217, "compression_ratio": 1.584070796460177, "no_speech_prob": 6.089487942517735e-05}, {"id": 52, "seek": 32064, "start": 326.8, "end": 333.71999999999997, "text": " locations, products, dates, times, et cetera. So for this piece of text, last night Paris", "tokens": [9253, 11, 3383, 11, 11691, 11, 1413, 11, 1030, 11458, 13, 407, 337, 341, 2522, 295, 2487, 11, 1036, 1818, 8380], "temperature": 0.0, "avg_logprob": -0.19531143795360217, "compression_ratio": 1.584070796460177, "no_speech_prob": 6.089487942517735e-05}, {"id": 53, "seek": 32064, "start": 333.71999999999997, "end": 339.52, "text": " Hilton, wowed in the sequin gown, Samuel Quinn was arrest in the Hilton Hotel in Paris", "tokens": [389, 15200, 11, 6076, 292, 294, 264, 5123, 259, 34428, 11, 23036, 36723, 390, 7823, 294, 264, 389, 15200, 20354, 294, 8380], "temperature": 0.0, "avg_logprob": -0.19531143795360217, "compression_ratio": 1.584070796460177, "no_speech_prob": 6.089487942517735e-05}, {"id": 54, "seek": 32064, "start": 339.52, "end": 347.64, "text": " in April 1989. The some words are being labeled as named entities as shown. These two sentences", "tokens": [294, 6929, 22427, 13, 440, 512, 2283, 366, 885, 21335, 382, 4926, 16667, 382, 4898, 13, 1981, 732, 16579], "temperature": 0.0, "avg_logprob": -0.19531143795360217, "compression_ratio": 1.584070796460177, "no_speech_prob": 6.089487942517735e-05}, {"id": 55, "seek": 34764, "start": 347.64, "end": 353.96, "text": " don't actually belong together in the same article, but I chose those two sentences to illustrate", "tokens": [500, 380, 767, 5784, 1214, 294, 264, 912, 7222, 11, 457, 286, 5111, 729, 732, 16579, 281, 23221], "temperature": 0.0, "avg_logprob": -0.1446951911562965, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.00016155086632352322}, {"id": 56, "seek": 34764, "start": 353.96, "end": 359.52, "text": " the basic point that is not that you can just do this task by using a dictionary. Yes,", "tokens": [264, 3875, 935, 300, 307, 406, 300, 291, 393, 445, 360, 341, 5633, 538, 1228, 257, 25890, 13, 1079, 11], "temperature": 0.0, "avg_logprob": -0.1446951911562965, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.00016155086632352322}, {"id": 57, "seek": 34764, "start": 359.52, "end": 366.15999999999997, "text": " a dictionary is helpful to know that Paris can possibly be a location, but Paris can also", "tokens": [257, 25890, 307, 4961, 281, 458, 300, 8380, 393, 6264, 312, 257, 4914, 11, 457, 8380, 393, 611], "temperature": 0.0, "avg_logprob": -0.1446951911562965, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.00016155086632352322}, {"id": 58, "seek": 34764, "start": 366.15999999999997, "end": 372.59999999999997, "text": " be a person name. So you have to use context to get named entity recognition rights.", "tokens": [312, 257, 954, 1315, 13, 407, 291, 362, 281, 764, 4319, 281, 483, 4926, 13977, 11150, 4601, 13], "temperature": 0.0, "avg_logprob": -0.1446951911562965, "compression_ratio": 1.6244343891402715, "no_speech_prob": 0.00016155086632352322}, {"id": 59, "seek": 37260, "start": 372.6, "end": 380.88, "text": " Okay, well, how might we do that with the neural network? There are much more advanced", "tokens": [1033, 11, 731, 11, 577, 1062, 321, 360, 300, 365, 264, 18161, 3209, 30, 821, 366, 709, 544, 7339], "temperature": 0.0, "avg_logprob": -0.1084305136951048, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.00015583996719215065}, {"id": 60, "seek": 37260, "start": 380.88, "end": 389.28000000000003, "text": " ways of doing this, but a simple yet already pretty good way of doing named entity recognition", "tokens": [2098, 295, 884, 341, 11, 457, 257, 2199, 1939, 1217, 1238, 665, 636, 295, 884, 4926, 13977, 11150], "temperature": 0.0, "avg_logprob": -0.1084305136951048, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.00015583996719215065}, {"id": 61, "seek": 37260, "start": 389.28000000000003, "end": 396.6, "text": " with a simple neural net is to say, well, what we're going to do is use the word vectors", "tokens": [365, 257, 2199, 18161, 2533, 307, 281, 584, 11, 731, 11, 437, 321, 434, 516, 281, 360, 307, 764, 264, 1349, 18875], "temperature": 0.0, "avg_logprob": -0.1084305136951048, "compression_ratio": 1.569767441860465, "no_speech_prob": 0.00015583996719215065}, {"id": 62, "seek": 39660, "start": 396.6, "end": 403.96000000000004, "text": " that we've learned about and we're going to build up a context window of word vectors.", "tokens": [300, 321, 600, 3264, 466, 293, 321, 434, 516, 281, 1322, 493, 257, 4319, 4910, 295, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1217169708080506, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.130698213586584e-05}, {"id": 63, "seek": 39660, "start": 403.96000000000004, "end": 409.56, "text": " And then we're going to put those through a neural network layer and then feed it through", "tokens": [400, 550, 321, 434, 516, 281, 829, 729, 807, 257, 18161, 3209, 4583, 293, 550, 3154, 309, 807], "temperature": 0.0, "avg_logprob": -0.1217169708080506, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.130698213586584e-05}, {"id": 64, "seek": 39660, "start": 409.56, "end": 415.8, "text": " a softmax classifier of the kind that we, sorry, I said that wrong. And then we're going", "tokens": [257, 2787, 41167, 1508, 9902, 295, 264, 733, 300, 321, 11, 2597, 11, 286, 848, 300, 2085, 13, 400, 550, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.1217169708080506, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.130698213586584e-05}, {"id": 65, "seek": 39660, "start": 415.8, "end": 420.8, "text": " to feed it through a logistic classifier of the kind that we saw when looking at negative", "tokens": [281, 3154, 309, 807, 257, 3565, 3142, 1508, 9902, 295, 264, 733, 300, 321, 1866, 562, 1237, 412, 3671], "temperature": 0.0, "avg_logprob": -0.1217169708080506, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.130698213586584e-05}, {"id": 66, "seek": 42080, "start": 420.8, "end": 429.6, "text": " sampling, which is going to say for a particular entity type such as location, is it high probability", "tokens": [21179, 11, 597, 307, 516, 281, 584, 337, 257, 1729, 13977, 2010, 1270, 382, 4914, 11, 307, 309, 1090, 8482], "temperature": 0.0, "avg_logprob": -0.16259814916032084, "compression_ratio": 1.766990291262136, "no_speech_prob": 6.797830428695306e-05}, {"id": 67, "seek": 42080, "start": 429.6, "end": 435.88, "text": " location or is it not a high probability location. So for a sentence like the museums and", "tokens": [4914, 420, 307, 309, 406, 257, 1090, 8482, 4914, 13, 407, 337, 257, 8174, 411, 264, 23248, 293], "temperature": 0.0, "avg_logprob": -0.16259814916032084, "compression_ratio": 1.766990291262136, "no_speech_prob": 6.797830428695306e-05}, {"id": 68, "seek": 42080, "start": 435.88, "end": 440.88, "text": " Paris are amazing to see what we're going to do is for each word say we're doing the", "tokens": [8380, 366, 2243, 281, 536, 437, 321, 434, 516, 281, 360, 307, 337, 1184, 1349, 584, 321, 434, 884, 264], "temperature": 0.0, "avg_logprob": -0.16259814916032084, "compression_ratio": 1.766990291262136, "no_speech_prob": 6.797830428695306e-05}, {"id": 69, "seek": 42080, "start": 440.88, "end": 447.04, "text": " word Paris, we're going to form a window around it say a plus or minus two word window.", "tokens": [1349, 8380, 11, 321, 434, 516, 281, 1254, 257, 4910, 926, 309, 584, 257, 1804, 420, 3175, 732, 1349, 4910, 13], "temperature": 0.0, "avg_logprob": -0.16259814916032084, "compression_ratio": 1.766990291262136, "no_speech_prob": 6.797830428695306e-05}, {"id": 70, "seek": 44704, "start": 447.04, "end": 453.76000000000005, "text": " And so for those five words, we're going to get word vectors for them from the kind of word", "tokens": [400, 370, 337, 729, 1732, 2283, 11, 321, 434, 516, 281, 483, 1349, 18875, 337, 552, 490, 264, 733, 295, 1349], "temperature": 0.0, "avg_logprob": -0.1395478674343654, "compression_ratio": 1.9515418502202644, "no_speech_prob": 5.2932726248400286e-05}, {"id": 71, "seek": 44704, "start": 453.76000000000005, "end": 459.04, "text": " to vac or glove word vectors we've learned. And we're going to make a long vector out of", "tokens": [281, 2842, 420, 26928, 1349, 18875, 321, 600, 3264, 13, 400, 321, 434, 516, 281, 652, 257, 938, 8062, 484, 295], "temperature": 0.0, "avg_logprob": -0.1395478674343654, "compression_ratio": 1.9515418502202644, "no_speech_prob": 5.2932726248400286e-05}, {"id": 72, "seek": 44704, "start": 459.04, "end": 464.04, "text": " the concatenation of those five word vectors. So the word of interest is in the middle.", "tokens": [264, 1588, 7186, 399, 295, 729, 1732, 1349, 18875, 13, 407, 264, 1349, 295, 1179, 307, 294, 264, 2808, 13], "temperature": 0.0, "avg_logprob": -0.1395478674343654, "compression_ratio": 1.9515418502202644, "no_speech_prob": 5.2932726248400286e-05}, {"id": 73, "seek": 44704, "start": 464.04, "end": 470.8, "text": " And then we're going to feed this vector to a classifier, which is that the end going", "tokens": [400, 550, 321, 434, 516, 281, 3154, 341, 8062, 281, 257, 1508, 9902, 11, 597, 307, 300, 264, 917, 516], "temperature": 0.0, "avg_logprob": -0.1395478674343654, "compression_ratio": 1.9515418502202644, "no_speech_prob": 5.2932726248400286e-05}, {"id": 74, "seek": 44704, "start": 470.8, "end": 476.8, "text": " to have a probability of the word being a location. And then we could have another class", "tokens": [281, 362, 257, 8482, 295, 264, 1349, 885, 257, 4914, 13, 400, 550, 321, 727, 362, 1071, 1508], "temperature": 0.0, "avg_logprob": -0.1395478674343654, "compression_ratio": 1.9515418502202644, "no_speech_prob": 5.2932726248400286e-05}, {"id": 75, "seek": 47680, "start": 476.8, "end": 481.72, "text": "ifier that says the probability of the word being a person name. And so once we've done", "tokens": [9902, 300, 1619, 264, 8482, 295, 264, 1349, 885, 257, 954, 1315, 13, 400, 370, 1564, 321, 600, 1096], "temperature": 0.0, "avg_logprob": -0.18593956058860844, "compression_ratio": 1.753968253968254, "no_speech_prob": 6.006229523336515e-05}, {"id": 76, "seek": 47680, "start": 481.72, "end": 485.48, "text": " that, we're then going to run it at the next position. So we then say, well, is the word", "tokens": [300, 11, 321, 434, 550, 516, 281, 1190, 309, 412, 264, 958, 2535, 13, 407, 321, 550, 584, 11, 731, 11, 307, 264, 1349], "temperature": 0.0, "avg_logprob": -0.18593956058860844, "compression_ratio": 1.753968253968254, "no_speech_prob": 6.006229523336515e-05}, {"id": 77, "seek": 47680, "start": 485.48, "end": 492.40000000000003, "text": " are a location and we'd feed a window of five words as then in Paris are amazing to", "tokens": [366, 257, 4914, 293, 321, 1116, 3154, 257, 4910, 295, 1732, 2283, 382, 550, 294, 8380, 366, 2243, 281], "temperature": 0.0, "avg_logprob": -0.18593956058860844, "compression_ratio": 1.753968253968254, "no_speech_prob": 6.006229523336515e-05}, {"id": 78, "seek": 47680, "start": 492.40000000000003, "end": 498.8, "text": " and put it through the same kind of classifier. And so this is the classifier that we'll use.", "tokens": [293, 829, 309, 807, 264, 912, 733, 295, 1508, 9902, 13, 400, 370, 341, 307, 264, 1508, 9902, 300, 321, 603, 764, 13], "temperature": 0.0, "avg_logprob": -0.18593956058860844, "compression_ratio": 1.753968253968254, "no_speech_prob": 6.006229523336515e-05}, {"id": 79, "seek": 47680, "start": 498.8, "end": 505.52, "text": " So its import will be this word window. So if we have de-dimensional word vectors, this", "tokens": [407, 1080, 974, 486, 312, 341, 1349, 4910, 13, 407, 498, 321, 362, 368, 12, 18759, 1349, 18875, 11, 341], "temperature": 0.0, "avg_logprob": -0.18593956058860844, "compression_ratio": 1.753968253968254, "no_speech_prob": 6.006229523336515e-05}, {"id": 80, "seek": 50552, "start": 505.52, "end": 512.0, "text": " will be a five de vector. And then we're going to put it through a layer of a neural network.", "tokens": [486, 312, 257, 1732, 368, 8062, 13, 400, 550, 321, 434, 516, 281, 829, 309, 807, 257, 4583, 295, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1466099626274519, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.599257903872058e-05}, {"id": 81, "seek": 50552, "start": 512.0, "end": 519.3199999999999, "text": " So the layer of the neural network is going to multiply this vector by a matrix, add", "tokens": [407, 264, 4583, 295, 264, 18161, 3209, 307, 516, 281, 12972, 341, 8062, 538, 257, 8141, 11, 909], "temperature": 0.0, "avg_logprob": -0.1466099626274519, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.599257903872058e-05}, {"id": 82, "seek": 50552, "start": 519.3199999999999, "end": 529.3199999999999, "text": " on a bias vector, and then put that through a non-linearity such as the softmax transformation", "tokens": [322, 257, 12577, 8062, 11, 293, 550, 829, 300, 807, 257, 2107, 12, 1889, 17409, 1270, 382, 264, 2787, 41167, 9887], "temperature": 0.0, "avg_logprob": -0.1466099626274519, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.599257903872058e-05}, {"id": 83, "seek": 50552, "start": 529.3199999999999, "end": 535.36, "text": " that we've seen before. And that will give us a hidden vector, which might be of a smaller", "tokens": [300, 321, 600, 1612, 949, 13, 400, 300, 486, 976, 505, 257, 7633, 8062, 11, 597, 1062, 312, 295, 257, 4356], "temperature": 0.0, "avg_logprob": -0.1466099626274519, "compression_ratio": 1.7089201877934272, "no_speech_prob": 6.599257903872058e-05}, {"id": 84, "seek": 53536, "start": 535.36, "end": 542.96, "text": " dimensionality such as this one here. And so then with that hidden vector, we're then", "tokens": [10139, 1860, 1270, 382, 341, 472, 510, 13, 400, 370, 550, 365, 300, 7633, 8062, 11, 321, 434, 550], "temperature": 0.0, "avg_logprob": -0.13596178137737772, "compression_ratio": 1.8082901554404145, "no_speech_prob": 6.703395047225058e-05}, {"id": 85, "seek": 53536, "start": 542.96, "end": 550.92, "text": " going to take the dot product of it with an extra vector here, here's you. So we take", "tokens": [516, 281, 747, 264, 5893, 1674, 295, 309, 365, 364, 2857, 8062, 510, 11, 510, 311, 291, 13, 407, 321, 747], "temperature": 0.0, "avg_logprob": -0.13596178137737772, "compression_ratio": 1.8082901554404145, "no_speech_prob": 6.703395047225058e-05}, {"id": 86, "seek": 53536, "start": 550.92, "end": 557.5600000000001, "text": " you dot product h. And so when we do that, we're getting out a single number. And that", "tokens": [291, 5893, 1674, 276, 13, 400, 370, 562, 321, 360, 300, 11, 321, 434, 1242, 484, 257, 2167, 1230, 13, 400, 300], "temperature": 0.0, "avg_logprob": -0.13596178137737772, "compression_ratio": 1.8082901554404145, "no_speech_prob": 6.703395047225058e-05}, {"id": 87, "seek": 53536, "start": 557.5600000000001, "end": 564.64, "text": " number can be any real number. And so then finally, we're going to put that number through", "tokens": [1230, 393, 312, 604, 957, 1230, 13, 400, 370, 550, 2721, 11, 321, 434, 516, 281, 829, 300, 1230, 807], "temperature": 0.0, "avg_logprob": -0.13596178137737772, "compression_ratio": 1.8082901554404145, "no_speech_prob": 6.703395047225058e-05}, {"id": 88, "seek": 56464, "start": 564.64, "end": 571.48, "text": " a logistic transform of the same kind that we saw when doing negative sampling. And the", "tokens": [257, 3565, 3142, 4088, 295, 264, 912, 733, 300, 321, 1866, 562, 884, 3671, 21179, 13, 400, 264], "temperature": 0.0, "avg_logprob": -0.10819617295876527, "compression_ratio": 1.728643216080402, "no_speech_prob": 7.242478022817522e-05}, {"id": 89, "seek": 56464, "start": 571.48, "end": 578.28, "text": " logistic transform will take any real number and it'll transform it into a probability", "tokens": [3565, 3142, 4088, 486, 747, 604, 957, 1230, 293, 309, 603, 4088, 309, 666, 257, 8482], "temperature": 0.0, "avg_logprob": -0.10819617295876527, "compression_ratio": 1.728643216080402, "no_speech_prob": 7.242478022817522e-05}, {"id": 90, "seek": 56464, "start": 578.28, "end": 584.96, "text": " that that word is a location. So its output is the predicted probability of the word", "tokens": [300, 300, 1349, 307, 257, 4914, 13, 407, 1080, 5598, 307, 264, 19147, 8482, 295, 264, 1349], "temperature": 0.0, "avg_logprob": -0.10819617295876527, "compression_ratio": 1.728643216080402, "no_speech_prob": 7.242478022817522e-05}, {"id": 91, "seek": 56464, "start": 584.96, "end": 590.72, "text": " belonging to a particular class. And so this could be our location classifier, which", "tokens": [22957, 281, 257, 1729, 1508, 13, 400, 370, 341, 727, 312, 527, 4914, 1508, 9902, 11, 597], "temperature": 0.0, "avg_logprob": -0.10819617295876527, "compression_ratio": 1.728643216080402, "no_speech_prob": 7.242478022817522e-05}, {"id": 92, "seek": 59072, "start": 590.72, "end": 596.76, "text": " could classify each word in a window as to what the probability is that it's a location", "tokens": [727, 33872, 1184, 1349, 294, 257, 4910, 382, 281, 437, 264, 8482, 307, 300, 309, 311, 257, 4914], "temperature": 0.0, "avg_logprob": -0.11458117167154948, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00010216272494290024}, {"id": 93, "seek": 59072, "start": 596.76, "end": 603.88, "text": " word. And so this little neural network here is the neural network I'm going to use today", "tokens": [1349, 13, 400, 370, 341, 707, 18161, 3209, 510, 307, 264, 18161, 3209, 286, 478, 516, 281, 764, 965], "temperature": 0.0, "avg_logprob": -0.11458117167154948, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00010216272494290024}, {"id": 94, "seek": 59072, "start": 603.88, "end": 609.2, "text": " when going through some of the math. But actually, I'm going to make it even easier on myself.", "tokens": [562, 516, 807, 512, 295, 264, 5221, 13, 583, 767, 11, 286, 478, 516, 281, 652, 309, 754, 3571, 322, 2059, 13], "temperature": 0.0, "avg_logprob": -0.11458117167154948, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00010216272494290024}, {"id": 95, "seek": 59072, "start": 609.2, "end": 616.0400000000001, "text": " I'm going to throw away the logistic function at the top. And I'm really just going to", "tokens": [286, 478, 516, 281, 3507, 1314, 264, 3565, 3142, 2445, 412, 264, 1192, 13, 400, 286, 478, 534, 445, 516, 281], "temperature": 0.0, "avg_logprob": -0.11458117167154948, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00010216272494290024}, {"id": 96, "seek": 61604, "start": 616.04, "end": 623.16, "text": " work through the math of the bottom three quarters of this. If you look at Kevin Clark's handout", "tokens": [589, 807, 264, 5221, 295, 264, 2767, 1045, 20612, 295, 341, 13, 759, 291, 574, 412, 9954, 18572, 311, 48785], "temperature": 0.0, "avg_logprob": -0.16637617065792992, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.249351619975641e-05}, {"id": 97, "seek": 61604, "start": 623.16, "end": 628.8399999999999, "text": " that I just mentioned, he includes when he works through it also working through the logistic", "tokens": [300, 286, 445, 2835, 11, 415, 5974, 562, 415, 1985, 807, 309, 611, 1364, 807, 264, 3565, 3142], "temperature": 0.0, "avg_logprob": -0.16637617065792992, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.249351619975641e-05}, {"id": 98, "seek": 61604, "start": 628.8399999999999, "end": 635.24, "text": " function. And we also saw working through a softmax in the first lecture when I was", "tokens": [2445, 13, 400, 321, 611, 1866, 1364, 807, 257, 2787, 41167, 294, 264, 700, 7991, 562, 286, 390], "temperature": 0.0, "avg_logprob": -0.16637617065792992, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.249351619975641e-05}, {"id": 99, "seek": 61604, "start": 635.24, "end": 642.8399999999999, "text": " working through some of the word detect model. Okay. So the overall question we want to", "tokens": [1364, 807, 512, 295, 264, 1349, 5531, 2316, 13, 1033, 13, 407, 264, 4787, 1168, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.16637617065792992, "compression_ratio": 1.6529680365296804, "no_speech_prob": 7.249351619975641e-05}, {"id": 100, "seek": 64284, "start": 642.84, "end": 651.9200000000001, "text": " be able to answer is, so here's our stochastic gradient descent equation that we have existing", "tokens": [312, 1075, 281, 1867, 307, 11, 370, 510, 311, 527, 342, 8997, 2750, 16235, 23475, 5367, 300, 321, 362, 6741], "temperature": 0.0, "avg_logprob": -0.20594435579636516, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00010526622645556927}, {"id": 101, "seek": 64284, "start": 651.9200000000001, "end": 660.4, "text": " parameters of our model. And we want to update them based on our current loss, which is", "tokens": [9834, 295, 527, 2316, 13, 400, 321, 528, 281, 5623, 552, 2361, 322, 527, 2190, 4470, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.20594435579636516, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00010526622645556927}, {"id": 102, "seek": 64284, "start": 660.4, "end": 668.2, "text": " the j of theta. So for getting our loss here, that the true answer is to whether a word", "tokens": [264, 361, 295, 9725, 13, 407, 337, 1242, 527, 4470, 510, 11, 300, 264, 2074, 1867, 307, 281, 1968, 257, 1349], "temperature": 0.0, "avg_logprob": -0.20594435579636516, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00010526622645556927}, {"id": 103, "seek": 66820, "start": 668.2, "end": 675.0, "text": " is a location or not will be either one, if it is a location or zero, if it isn't, our", "tokens": [307, 257, 4914, 420, 406, 486, 312, 2139, 472, 11, 498, 309, 307, 257, 4914, 420, 4018, 11, 498, 309, 1943, 380, 11, 527], "temperature": 0.0, "avg_logprob": -0.1551087866438196, "compression_ratio": 1.6222222222222222, "no_speech_prob": 4.8980404244503006e-05}, {"id": 104, "seek": 66820, "start": 675.0, "end": 681.2, "text": " logistic class to file returns some number like 0.9. And we'll use the distance away from", "tokens": [3565, 3142, 1508, 281, 3991, 11247, 512, 1230, 411, 1958, 13, 24, 13, 400, 321, 603, 764, 264, 4560, 1314, 490], "temperature": 0.0, "avg_logprob": -0.1551087866438196, "compression_ratio": 1.6222222222222222, "no_speech_prob": 4.8980404244503006e-05}, {"id": 105, "seek": 66820, "start": 681.2, "end": 687.6400000000001, "text": " what it should have been squared as our loss. So we work out a loss. And then we're moving", "tokens": [437, 309, 820, 362, 668, 8889, 382, 527, 4470, 13, 407, 321, 589, 484, 257, 4470, 13, 400, 550, 321, 434, 2684], "temperature": 0.0, "avg_logprob": -0.1551087866438196, "compression_ratio": 1.6222222222222222, "no_speech_prob": 4.8980404244503006e-05}, {"id": 106, "seek": 66820, "start": 687.6400000000001, "end": 695.12, "text": " a little distance in the negative of the gradient, which will be changing our parameter estimates", "tokens": [257, 707, 4560, 294, 264, 3671, 295, 264, 16235, 11, 597, 486, 312, 4473, 527, 13075, 20561], "temperature": 0.0, "avg_logprob": -0.1551087866438196, "compression_ratio": 1.6222222222222222, "no_speech_prob": 4.8980404244503006e-05}, {"id": 107, "seek": 69512, "start": 695.12, "end": 701.64, "text": " in such a way that they reduce the loss. And so this is already being written in terms", "tokens": [294, 1270, 257, 636, 300, 436, 5407, 264, 4470, 13, 400, 370, 341, 307, 1217, 885, 3720, 294, 2115], "temperature": 0.0, "avg_logprob": -0.12684274602819373, "compression_ratio": 1.7073170731707317, "no_speech_prob": 9.593419963493943e-05}, {"id": 108, "seek": 69512, "start": 701.64, "end": 708.2, "text": " of a whole vector of parameters, which is being updated as to a new vector of parameters.", "tokens": [295, 257, 1379, 8062, 295, 9834, 11, 597, 307, 885, 10588, 382, 281, 257, 777, 8062, 295, 9834, 13], "temperature": 0.0, "avg_logprob": -0.12684274602819373, "compression_ratio": 1.7073170731707317, "no_speech_prob": 9.593419963493943e-05}, {"id": 109, "seek": 69512, "start": 708.2, "end": 714.16, "text": " But you can also think about it that for each individual parameter theta j that we're", "tokens": [583, 291, 393, 611, 519, 466, 309, 300, 337, 1184, 2609, 13075, 9725, 361, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.12684274602819373, "compression_ratio": 1.7073170731707317, "no_speech_prob": 9.593419963493943e-05}, {"id": 110, "seek": 69512, "start": 714.16, "end": 720.72, "text": " working out the partial derivative of the loss with respect to that parameter. And then", "tokens": [1364, 484, 264, 14641, 13760, 295, 264, 4470, 365, 3104, 281, 300, 13075, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.12684274602819373, "compression_ratio": 1.7073170731707317, "no_speech_prob": 9.593419963493943e-05}, {"id": 111, "seek": 72072, "start": 720.72, "end": 727.36, "text": " we're moving a little bit in the negative direction of that. That's going to give us", "tokens": [321, 434, 2684, 257, 707, 857, 294, 264, 3671, 3513, 295, 300, 13, 663, 311, 516, 281, 976, 505], "temperature": 0.0, "avg_logprob": -0.14165402518378364, "compression_ratio": 1.7169811320754718, "no_speech_prob": 2.3913842596812174e-05}, {"id": 112, "seek": 72072, "start": 727.36, "end": 735.2, "text": " a new value for parameter theta j. And we're going to update all of the parameters of our", "tokens": [257, 777, 2158, 337, 13075, 9725, 361, 13, 400, 321, 434, 516, 281, 5623, 439, 295, 264, 9834, 295, 527], "temperature": 0.0, "avg_logprob": -0.14165402518378364, "compression_ratio": 1.7169811320754718, "no_speech_prob": 2.3913842596812174e-05}, {"id": 113, "seek": 72072, "start": 735.2, "end": 743.12, "text": " model as we learn. I mean, in particular, in contrast to what commonly happens in statistics,", "tokens": [2316, 382, 321, 1466, 13, 286, 914, 11, 294, 1729, 11, 294, 8712, 281, 437, 12719, 2314, 294, 12523, 11], "temperature": 0.0, "avg_logprob": -0.14165402518378364, "compression_ratio": 1.7169811320754718, "no_speech_prob": 2.3913842596812174e-05}, {"id": 114, "seek": 72072, "start": 743.12, "end": 750.24, "text": " we also, we update not only the sort of parameters of our model that are sort of weights in the", "tokens": [321, 611, 11, 321, 5623, 406, 787, 264, 1333, 295, 9834, 295, 527, 2316, 300, 366, 1333, 295, 17443, 294, 264], "temperature": 0.0, "avg_logprob": -0.14165402518378364, "compression_ratio": 1.7169811320754718, "no_speech_prob": 2.3913842596812174e-05}, {"id": 115, "seek": 75024, "start": 750.24, "end": 756.16, "text": " classifier, but we also will update our data representation. So we'll also be changing", "tokens": [1508, 9902, 11, 457, 321, 611, 486, 5623, 527, 1412, 10290, 13, 407, 321, 603, 611, 312, 4473], "temperature": 0.0, "avg_logprob": -0.1288659406262775, "compression_ratio": 1.634703196347032, "no_speech_prob": 7.248821930261329e-05}, {"id": 116, "seek": 75024, "start": 756.16, "end": 763.0, "text": " our word vectors as we learn. Okay, so to build neural nets, I use to train neural nets based", "tokens": [527, 1349, 18875, 382, 321, 1466, 13, 1033, 11, 370, 281, 1322, 18161, 36170, 11, 286, 764, 281, 3847, 18161, 36170, 2361], "temperature": 0.0, "avg_logprob": -0.1288659406262775, "compression_ratio": 1.634703196347032, "no_speech_prob": 7.248821930261329e-05}, {"id": 117, "seek": 75024, "start": 763.0, "end": 770.44, "text": " on data, what we need is to be able to compute this gradient of the parameters so that we", "tokens": [322, 1412, 11, 437, 321, 643, 307, 281, 312, 1075, 281, 14722, 341, 16235, 295, 264, 9834, 370, 300, 321], "temperature": 0.0, "avg_logprob": -0.1288659406262775, "compression_ratio": 1.634703196347032, "no_speech_prob": 7.248821930261329e-05}, {"id": 118, "seek": 75024, "start": 770.44, "end": 776.48, "text": " can then iteratively update the weights of the model and efficiently train a model that", "tokens": [393, 550, 17138, 19020, 5623, 264, 17443, 295, 264, 2316, 293, 19621, 3847, 257, 2316, 300], "temperature": 0.0, "avg_logprob": -0.1288659406262775, "compression_ratio": 1.634703196347032, "no_speech_prob": 7.248821930261329e-05}, {"id": 119, "seek": 77648, "start": 776.48, "end": 785.28, "text": " has good weights, i.e. that has high accuracy. And so how can we do that? Well, what I'm", "tokens": [575, 665, 17443, 11, 741, 13, 68, 13, 300, 575, 1090, 14170, 13, 400, 370, 577, 393, 321, 360, 300, 30, 1042, 11, 437, 286, 478], "temperature": 0.0, "avg_logprob": -0.13075627644856772, "compression_ratio": 1.4886363636363635, "no_speech_prob": 6.299810047494248e-05}, {"id": 120, "seek": 77648, "start": 785.28, "end": 792.4, "text": " going to talk about today is first of all how you can do it by hand. And so for doing", "tokens": [516, 281, 751, 466, 965, 307, 700, 295, 439, 577, 291, 393, 360, 309, 538, 1011, 13, 400, 370, 337, 884], "temperature": 0.0, "avg_logprob": -0.13075627644856772, "compression_ratio": 1.4886363636363635, "no_speech_prob": 6.299810047494248e-05}, {"id": 121, "seek": 77648, "start": 792.4, "end": 800.44, "text": " it by hand, this is basically a review of matrix calculus. And that'll take quite a bit", "tokens": [309, 538, 1011, 11, 341, 307, 1936, 257, 3131, 295, 8141, 33400, 13, 400, 300, 603, 747, 1596, 257, 857], "temperature": 0.0, "avg_logprob": -0.13075627644856772, "compression_ratio": 1.4886363636363635, "no_speech_prob": 6.299810047494248e-05}, {"id": 122, "seek": 80044, "start": 800.44, "end": 808.2800000000001, "text": " of the lecture. And then after we've talked about that for a while, I'll then shift gears", "tokens": [295, 264, 7991, 13, 400, 550, 934, 321, 600, 2825, 466, 300, 337, 257, 1339, 11, 286, 603, 550, 5513, 20915], "temperature": 0.0, "avg_logprob": -0.10593726966954485, "compression_ratio": 1.6036036036036037, "no_speech_prob": 3.021999691554811e-05}, {"id": 123, "seek": 80044, "start": 808.2800000000001, "end": 814.2800000000001, "text": " and introduce the back propagation algorithm, which is the central technology for neural", "tokens": [293, 5366, 264, 646, 38377, 9284, 11, 597, 307, 264, 5777, 2899, 337, 18161], "temperature": 0.0, "avg_logprob": -0.10593726966954485, "compression_ratio": 1.6036036036036037, "no_speech_prob": 3.021999691554811e-05}, {"id": 124, "seek": 80044, "start": 814.2800000000001, "end": 821.48, "text": " networks. And that technology is essentially the efficient application of calculus on a", "tokens": [9590, 13, 400, 300, 2899, 307, 4476, 264, 7148, 3861, 295, 33400, 322, 257], "temperature": 0.0, "avg_logprob": -0.10593726966954485, "compression_ratio": 1.6036036036036037, "no_speech_prob": 3.021999691554811e-05}, {"id": 125, "seek": 80044, "start": 821.48, "end": 828.8800000000001, "text": " large scale as we'll come to talking about soon. So for computing gradients by hand, what", "tokens": [2416, 4373, 382, 321, 603, 808, 281, 1417, 466, 2321, 13, 407, 337, 15866, 2771, 2448, 538, 1011, 11, 437], "temperature": 0.0, "avg_logprob": -0.10593726966954485, "compression_ratio": 1.6036036036036037, "no_speech_prob": 3.021999691554811e-05}, {"id": 126, "seek": 82888, "start": 828.88, "end": 836.76, "text": " we're doing is matrix calculus. So we're working with vectors and matrices and working out", "tokens": [321, 434, 884, 307, 8141, 33400, 13, 407, 321, 434, 1364, 365, 18875, 293, 32284, 293, 1364, 484], "temperature": 0.0, "avg_logprob": -0.13019003186907088, "compression_ratio": 1.52, "no_speech_prob": 2.4291106456075795e-05}, {"id": 127, "seek": 82888, "start": 836.76, "end": 846.76, "text": " gradients. And this can seem like pretty scary stuff. And well, to extent that you're", "tokens": [2771, 2448, 13, 400, 341, 393, 1643, 411, 1238, 6958, 1507, 13, 400, 731, 11, 281, 8396, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.13019003186907088, "compression_ratio": 1.52, "no_speech_prob": 2.4291106456075795e-05}, {"id": 128, "seek": 82888, "start": 846.76, "end": 854.84, "text": " kind of scared and don't know what's going on, one choice is to work out a non-vectorized", "tokens": [733, 295, 5338, 293, 500, 380, 458, 437, 311, 516, 322, 11, 472, 3922, 307, 281, 589, 484, 257, 2107, 12, 303, 1672, 1602], "temperature": 0.0, "avg_logprob": -0.13019003186907088, "compression_ratio": 1.52, "no_speech_prob": 2.4291106456075795e-05}, {"id": 129, "seek": 85484, "start": 854.84, "end": 862.52, "text": " gradient by just working out what the partial derivative is for one parameter at a time.", "tokens": [16235, 538, 445, 1364, 484, 437, 264, 14641, 13760, 307, 337, 472, 13075, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.0938740535215898, "compression_ratio": 1.5575221238938053, "no_speech_prob": 4.004923175671138e-05}, {"id": 130, "seek": 85484, "start": 862.52, "end": 869.6800000000001, "text": " And I showed a little example of that in the first lecture. But it's much, much faster", "tokens": [400, 286, 4712, 257, 707, 1365, 295, 300, 294, 264, 700, 7991, 13, 583, 309, 311, 709, 11, 709, 4663], "temperature": 0.0, "avg_logprob": -0.0938740535215898, "compression_ratio": 1.5575221238938053, "no_speech_prob": 4.004923175671138e-05}, {"id": 131, "seek": 85484, "start": 869.6800000000001, "end": 879.24, "text": " and more useful to actually be able to work with vectorized gradients. And in some sense,", "tokens": [293, 544, 4420, 281, 767, 312, 1075, 281, 589, 365, 8062, 1602, 2771, 2448, 13, 400, 294, 512, 2020, 11], "temperature": 0.0, "avg_logprob": -0.0938740535215898, "compression_ratio": 1.5575221238938053, "no_speech_prob": 4.004923175671138e-05}, {"id": 132, "seek": 85484, "start": 879.24, "end": 884.52, "text": " if you're not very confident, this is kind of almost a leap of faith. But it really is", "tokens": [498, 291, 434, 406, 588, 6679, 11, 341, 307, 733, 295, 1920, 257, 19438, 295, 4522, 13, 583, 309, 534, 307], "temperature": 0.0, "avg_logprob": -0.0938740535215898, "compression_ratio": 1.5575221238938053, "no_speech_prob": 4.004923175671138e-05}, {"id": 133, "seek": 88452, "start": 884.52, "end": 890.76, "text": " the case that multivariable calculus is just like single variable calculus, except you're", "tokens": [264, 1389, 300, 2120, 592, 3504, 712, 33400, 307, 445, 411, 2167, 7006, 33400, 11, 3993, 291, 434], "temperature": 0.0, "avg_logprob": -0.22215466218836166, "compression_ratio": 1.6126126126126126, "no_speech_prob": 2.1441232092911378e-05}, {"id": 134, "seek": 88452, "start": 890.76, "end": 896.76, "text": " using vectors and matrices. So providing you to remember some basics of single variable", "tokens": [1228, 18875, 293, 32284, 13, 407, 6530, 291, 281, 1604, 512, 14688, 295, 2167, 7006], "temperature": 0.0, "avg_logprob": -0.22215466218836166, "compression_ratio": 1.6126126126126126, "no_speech_prob": 2.1441232092911378e-05}, {"id": 135, "seek": 88452, "start": 896.76, "end": 904.16, "text": " calculus, you really should be able to do this stuff and get it to work out. Lots of other", "tokens": [33400, 11, 291, 534, 820, 312, 1075, 281, 360, 341, 1507, 293, 483, 309, 281, 589, 484, 13, 15908, 295, 661], "temperature": 0.0, "avg_logprob": -0.22215466218836166, "compression_ratio": 1.6126126126126126, "no_speech_prob": 2.1441232092911378e-05}, {"id": 136, "seek": 88452, "start": 904.16, "end": 911.28, "text": " sources, I've mentioned the notes. You can also look at the textbook from A51, which also", "tokens": [7139, 11, 286, 600, 2835, 264, 5570, 13, 509, 393, 611, 574, 412, 264, 25591, 490, 316, 18682, 11, 597, 611], "temperature": 0.0, "avg_logprob": -0.22215466218836166, "compression_ratio": 1.6126126126126126, "no_speech_prob": 2.1441232092911378e-05}, {"id": 137, "seek": 91128, "start": 911.28, "end": 917.64, "text": " has quite a lot of material on this. I know some of you have bad memories of A51.", "tokens": [575, 1596, 257, 688, 295, 2527, 322, 341, 13, 286, 458, 512, 295, 291, 362, 1578, 8495, 295, 316, 18682, 13], "temperature": 0.0, "avg_logprob": -0.1675387849199011, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.480491432128474e-05}, {"id": 138, "seek": 91128, "start": 917.64, "end": 922.64, "text": " OK, so let's go through this and see how it works from ramping up from the beginning.", "tokens": [2264, 11, 370, 718, 311, 352, 807, 341, 293, 536, 577, 309, 1985, 490, 12428, 278, 493, 490, 264, 2863, 13], "temperature": 0.0, "avg_logprob": -0.1675387849199011, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.480491432128474e-05}, {"id": 139, "seek": 91128, "start": 922.64, "end": 928.24, "text": " So the beginning of calculus is, you know, we have a function with one input and one", "tokens": [407, 264, 2863, 295, 33400, 307, 11, 291, 458, 11, 321, 362, 257, 2445, 365, 472, 4846, 293, 472], "temperature": 0.0, "avg_logprob": -0.1675387849199011, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.480491432128474e-05}, {"id": 140, "seek": 91128, "start": 928.24, "end": 935.24, "text": " output, f of x equals x cubed. And so then its gradient is its slope, right? So that's", "tokens": [5598, 11, 283, 295, 2031, 6915, 2031, 36510, 13, 400, 370, 550, 1080, 16235, 307, 1080, 13525, 11, 558, 30, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.1675387849199011, "compression_ratio": 1.5201793721973094, "no_speech_prob": 3.480491432128474e-05}, {"id": 141, "seek": 93524, "start": 935.24, "end": 943.52, "text": " its derivative. So its derivative is 3x squared. And the way to think about this is how much", "tokens": [1080, 13760, 13, 407, 1080, 13760, 307, 805, 87, 8889, 13, 400, 264, 636, 281, 519, 466, 341, 307, 577, 709], "temperature": 0.0, "avg_logprob": -0.10875149430899785, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.00011229947267565876}, {"id": 142, "seek": 93524, "start": 943.52, "end": 949.24, "text": " will the output change if we change the input a little bit, right? So what we're wanting", "tokens": [486, 264, 5598, 1319, 498, 321, 1319, 264, 4846, 257, 707, 857, 11, 558, 30, 407, 437, 321, 434, 7935], "temperature": 0.0, "avg_logprob": -0.10875149430899785, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.00011229947267565876}, {"id": 143, "seek": 93524, "start": 949.24, "end": 956.12, "text": " to do in our neural net models is change what they output so that they do a better job", "tokens": [281, 360, 294, 527, 18161, 2533, 5245, 307, 1319, 437, 436, 5598, 370, 300, 436, 360, 257, 1101, 1691], "temperature": 0.0, "avg_logprob": -0.10875149430899785, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.00011229947267565876}, {"id": 144, "seek": 93524, "start": 956.12, "end": 961.36, "text": " of predicting the correct answers when we're doing supervised learning. And so what we want", "tokens": [295, 32884, 264, 3006, 6338, 562, 321, 434, 884, 46533, 2539, 13, 400, 370, 437, 321, 528], "temperature": 0.0, "avg_logprob": -0.10875149430899785, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.00011229947267565876}, {"id": 145, "seek": 96136, "start": 961.36, "end": 966.8000000000001, "text": " to know is if we fiddle different parameters of the model, how much will they have on the", "tokens": [281, 458, 307, 498, 321, 24553, 2285, 819, 9834, 295, 264, 2316, 11, 577, 709, 486, 436, 362, 322, 264], "temperature": 0.0, "avg_logprob": -0.15272781529377416, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.251044735312462e-05}, {"id": 146, "seek": 96136, "start": 966.8000000000001, "end": 972.24, "text": " output? Because then we can choose how to fiddle them in the right way to move things down,", "tokens": [5598, 30, 1436, 550, 321, 393, 2826, 577, 281, 24553, 2285, 552, 294, 264, 558, 636, 281, 1286, 721, 760, 11], "temperature": 0.0, "avg_logprob": -0.15272781529377416, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.251044735312462e-05}, {"id": 147, "seek": 96136, "start": 972.24, "end": 978.8000000000001, "text": " right? So, you know, when we're saying that the derivative here is 3x squared, well,", "tokens": [558, 30, 407, 11, 291, 458, 11, 562, 321, 434, 1566, 300, 264, 13760, 510, 307, 805, 87, 8889, 11, 731, 11], "temperature": 0.0, "avg_logprob": -0.15272781529377416, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.251044735312462e-05}, {"id": 148, "seek": 96136, "start": 978.8000000000001, "end": 986.6, "text": " what we're saying is that if you're at x equals 1, if you fiddle the input a little bit,", "tokens": [437, 321, 434, 1566, 307, 300, 498, 291, 434, 412, 2031, 6915, 502, 11, 498, 291, 24553, 2285, 264, 4846, 257, 707, 857, 11], "temperature": 0.0, "avg_logprob": -0.15272781529377416, "compression_ratio": 1.628440366972477, "no_speech_prob": 7.251044735312462e-05}, {"id": 149, "seek": 98660, "start": 986.6, "end": 992.6, "text": " the output will change 3 times as much, 3 times 1 squared. And it does. So if I say what's", "tokens": [264, 5598, 486, 1319, 805, 1413, 382, 709, 11, 805, 1413, 502, 8889, 13, 400, 309, 775, 13, 407, 498, 286, 584, 437, 311], "temperature": 0.0, "avg_logprob": -0.1503767080085222, "compression_ratio": 1.5542857142857143, "no_speech_prob": 9.751578909344971e-05}, {"id": 150, "seek": 98660, "start": 992.6, "end": 1000.16, "text": " the value at 1.01, it's about 1.03, it's changed 3 times as much and that's its slope. But", "tokens": [264, 2158, 412, 502, 13, 10607, 11, 309, 311, 466, 502, 13, 11592, 11, 309, 311, 3105, 805, 1413, 382, 709, 293, 300, 311, 1080, 13525, 13, 583], "temperature": 0.0, "avg_logprob": -0.1503767080085222, "compression_ratio": 1.5542857142857143, "no_speech_prob": 9.751578909344971e-05}, {"id": 151, "seek": 98660, "start": 1000.16, "end": 1009.96, "text": " it x equals 4, the derivative is 16 times 3, 48. So if we fiddle the input a little, it'll", "tokens": [309, 2031, 6915, 1017, 11, 264, 13760, 307, 3165, 1413, 805, 11, 11174, 13, 407, 498, 321, 24553, 2285, 264, 4846, 257, 707, 11, 309, 603], "temperature": 0.0, "avg_logprob": -0.1503767080085222, "compression_ratio": 1.5542857142857143, "no_speech_prob": 9.751578909344971e-05}, {"id": 152, "seek": 100996, "start": 1009.96, "end": 1017.1600000000001, "text": " change 48 times as much and that's roughly what happens, 4.01 cubed is 64.48. Now, of", "tokens": [1319, 11174, 1413, 382, 709, 293, 300, 311, 9810, 437, 2314, 11, 1017, 13, 10607, 36510, 307, 12145, 13, 13318, 13, 823, 11, 295], "temperature": 0.0, "avg_logprob": -0.14304008869209675, "compression_ratio": 1.5210084033613445, "no_speech_prob": 5.221370156505145e-05}, {"id": 153, "seek": 100996, "start": 1017.1600000000001, "end": 1022.8000000000001, "text": " course, you know, this is just sort of showing it for a small fiddle, but, you know, that's", "tokens": [1164, 11, 291, 458, 11, 341, 307, 445, 1333, 295, 4099, 309, 337, 257, 1359, 24553, 2285, 11, 457, 11, 291, 458, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.14304008869209675, "compression_ratio": 1.5210084033613445, "no_speech_prob": 5.221370156505145e-05}, {"id": 154, "seek": 100996, "start": 1022.8000000000001, "end": 1030.28, "text": " an approximation to the actual truth. Okay, so then we sort of ramp up to the more complex", "tokens": [364, 28023, 281, 264, 3539, 3494, 13, 1033, 11, 370, 550, 321, 1333, 295, 12428, 493, 281, 264, 544, 3997], "temperature": 0.0, "avg_logprob": -0.14304008869209675, "compression_ratio": 1.5210084033613445, "no_speech_prob": 5.221370156505145e-05}, {"id": 155, "seek": 100996, "start": 1030.28, "end": 1036.32, "text": " cases, which are more reflective of what we do with neural networks. So if we have a function", "tokens": [3331, 11, 597, 366, 544, 28931, 295, 437, 321, 360, 365, 18161, 9590, 13, 407, 498, 321, 362, 257, 2445], "temperature": 0.0, "avg_logprob": -0.14304008869209675, "compression_ratio": 1.5210084033613445, "no_speech_prob": 5.221370156505145e-05}, {"id": 156, "seek": 103632, "start": 1036.32, "end": 1043.76, "text": " with one output and n inputs, then we have a gradient. So a gradient is a vector of partial", "tokens": [365, 472, 5598, 293, 297, 15743, 11, 550, 321, 362, 257, 16235, 13, 407, 257, 16235, 307, 257, 8062, 295, 14641], "temperature": 0.0, "avg_logprob": -0.15072190630566942, "compression_ratio": 2.0112994350282487, "no_speech_prob": 5.919111572438851e-05}, {"id": 157, "seek": 103632, "start": 1043.76, "end": 1049.3999999999999, "text": " derivatives with respect to each input. So we've got n inputs x1 to xn and we're working", "tokens": [33733, 365, 3104, 281, 1184, 4846, 13, 407, 321, 600, 658, 297, 15743, 2031, 16, 281, 2031, 77, 293, 321, 434, 1364], "temperature": 0.0, "avg_logprob": -0.15072190630566942, "compression_ratio": 2.0112994350282487, "no_speech_prob": 5.919111572438851e-05}, {"id": 158, "seek": 103632, "start": 1049.3999999999999, "end": 1055.1599999999999, "text": " out the partial derivative f with respect to x1, the partial derivative f with respect to", "tokens": [484, 264, 14641, 13760, 283, 365, 3104, 281, 2031, 16, 11, 264, 14641, 13760, 283, 365, 3104, 281], "temperature": 0.0, "avg_logprob": -0.15072190630566942, "compression_ratio": 2.0112994350282487, "no_speech_prob": 5.919111572438851e-05}, {"id": 159, "seek": 103632, "start": 1055.1599999999999, "end": 1062.76, "text": " x2, et cetera. And we then get a vector of partial derivatives, where each element of", "tokens": [2031, 17, 11, 1030, 11458, 13, 400, 321, 550, 483, 257, 8062, 295, 14641, 33733, 11, 689, 1184, 4478, 295], "temperature": 0.0, "avg_logprob": -0.15072190630566942, "compression_ratio": 2.0112994350282487, "no_speech_prob": 5.919111572438851e-05}, {"id": 160, "seek": 106276, "start": 1062.76, "end": 1070.52, "text": " this vector is just like a simple derivative with respect to one variable. Okay, so from", "tokens": [341, 8062, 307, 445, 411, 257, 2199, 13760, 365, 3104, 281, 472, 7006, 13, 1033, 11, 370, 490], "temperature": 0.0, "avg_logprob": -0.11735054542278421, "compression_ratio": 1.6682242990654206, "no_speech_prob": 4.005429218523204e-05}, {"id": 161, "seek": 106276, "start": 1070.52, "end": 1077.24, "text": " that point, we just keep on ramping up for what we do with neural networks. So commonly,", "tokens": [300, 935, 11, 321, 445, 1066, 322, 12428, 278, 493, 337, 437, 321, 360, 365, 18161, 9590, 13, 407, 12719, 11], "temperature": 0.0, "avg_logprob": -0.11735054542278421, "compression_ratio": 1.6682242990654206, "no_speech_prob": 4.005429218523204e-05}, {"id": 162, "seek": 106276, "start": 1077.24, "end": 1083.84, "text": " when we have something like a layer in a neural network, we'll have a function with n inputs", "tokens": [562, 321, 362, 746, 411, 257, 4583, 294, 257, 18161, 3209, 11, 321, 603, 362, 257, 2445, 365, 297, 15743], "temperature": 0.0, "avg_logprob": -0.11735054542278421, "compression_ratio": 1.6682242990654206, "no_speech_prob": 4.005429218523204e-05}, {"id": 163, "seek": 106276, "start": 1083.84, "end": 1090.8, "text": " that will be like our word vectors, then we do something like multiply by a matrix and", "tokens": [300, 486, 312, 411, 527, 1349, 18875, 11, 550, 321, 360, 746, 411, 12972, 538, 257, 8141, 293], "temperature": 0.0, "avg_logprob": -0.11735054542278421, "compression_ratio": 1.6682242990654206, "no_speech_prob": 4.005429218523204e-05}, {"id": 164, "seek": 109080, "start": 1090.8, "end": 1097.32, "text": " then we'll have m outputs. So we have a function now, which is taking n inputs and is producing", "tokens": [550, 321, 603, 362, 275, 23930, 13, 407, 321, 362, 257, 2445, 586, 11, 597, 307, 1940, 297, 15743, 293, 307, 10501], "temperature": 0.0, "avg_logprob": -0.11595489554209253, "compression_ratio": 1.6748466257668713, "no_speech_prob": 1.3843540727975778e-05}, {"id": 165, "seek": 109080, "start": 1097.32, "end": 1105.2, "text": " m outputs. So at this point, what we're calculating for the gradient is what's called a", "tokens": [275, 23930, 13, 407, 412, 341, 935, 11, 437, 321, 434, 28258, 337, 264, 16235, 307, 437, 311, 1219, 257], "temperature": 0.0, "avg_logprob": -0.11595489554209253, "compression_ratio": 1.6748466257668713, "no_speech_prob": 1.3843540727975778e-05}, {"id": 166, "seek": 109080, "start": 1105.2, "end": 1114.36, "text": " Jacobian matrix. So for m inputs and n outputs, the Jacobian is an m by n matrix of every", "tokens": [14117, 952, 8141, 13, 407, 337, 275, 15743, 293, 297, 23930, 11, 264, 14117, 952, 307, 364, 275, 538, 297, 8141, 295, 633], "temperature": 0.0, "avg_logprob": -0.11595489554209253, "compression_ratio": 1.6748466257668713, "no_speech_prob": 1.3843540727975778e-05}, {"id": 167, "seek": 111436, "start": 1114.36, "end": 1123.52, "text": " combination of partial derivatives. So function f splits up into these different sub functions", "tokens": [6562, 295, 14641, 33733, 13, 407, 2445, 283, 37741, 493, 666, 613, 819, 1422, 6828], "temperature": 0.0, "avg_logprob": -0.1485914487517282, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.00010387996007921174}, {"id": 168, "seek": 111436, "start": 1123.52, "end": 1130.32, "text": " f1 through m, fm, which generate each of the m outputs. And so then we're taking the", "tokens": [283, 16, 807, 275, 11, 283, 76, 11, 597, 8460, 1184, 295, 264, 275, 23930, 13, 400, 370, 550, 321, 434, 1940, 264], "temperature": 0.0, "avg_logprob": -0.1485914487517282, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.00010387996007921174}, {"id": 169, "seek": 111436, "start": 1130.32, "end": 1136.52, "text": " partial derivative f1 with respect to x1 through the partial derivative f1 with respect to", "tokens": [14641, 13760, 283, 16, 365, 3104, 281, 2031, 16, 807, 264, 14641, 13760, 283, 16, 365, 3104, 281], "temperature": 0.0, "avg_logprob": -0.1485914487517282, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.00010387996007921174}, {"id": 170, "seek": 111436, "start": 1136.52, "end": 1142.36, "text": " xn, then heading down, you know, we make it up to the partial derivative of fm with respect", "tokens": [2031, 77, 11, 550, 9864, 760, 11, 291, 458, 11, 321, 652, 309, 493, 281, 264, 14641, 13760, 295, 283, 76, 365, 3104], "temperature": 0.0, "avg_logprob": -0.1485914487517282, "compression_ratio": 1.8564102564102565, "no_speech_prob": 0.00010387996007921174}, {"id": 171, "seek": 114236, "start": 1142.36, "end": 1149.0, "text": " to x1, et cetera. So we have every possible partial derivative of an output variable with", "tokens": [281, 2031, 16, 11, 1030, 11458, 13, 407, 321, 362, 633, 1944, 14641, 13760, 295, 364, 5598, 7006, 365], "temperature": 0.0, "avg_logprob": -0.12435356308432187, "compression_ratio": 1.5337078651685394, "no_speech_prob": 2.9764218197669834e-05}, {"id": 172, "seek": 114236, "start": 1149.0, "end": 1160.28, "text": " respect to one of the input variables. Okay. So in simple calculus, when you have a composition", "tokens": [3104, 281, 472, 295, 264, 4846, 9102, 13, 1033, 13, 407, 294, 2199, 33400, 11, 562, 291, 362, 257, 12686], "temperature": 0.0, "avg_logprob": -0.12435356308432187, "compression_ratio": 1.5337078651685394, "no_speech_prob": 2.9764218197669834e-05}, {"id": 173, "seek": 114236, "start": 1160.28, "end": 1170.0, "text": " of one variable functions, so that if you have y equals x squared and then z equals 3y,", "tokens": [295, 472, 7006, 6828, 11, 370, 300, 498, 291, 362, 288, 6915, 2031, 8889, 293, 550, 710, 6915, 805, 88, 11], "temperature": 0.0, "avg_logprob": -0.12435356308432187, "compression_ratio": 1.5337078651685394, "no_speech_prob": 2.9764218197669834e-05}, {"id": 174, "seek": 117000, "start": 1170.0, "end": 1178.16, "text": " then z is a composition of two functions of, or you're composing two functions, z is", "tokens": [550, 710, 307, 257, 12686, 295, 732, 6828, 295, 11, 420, 291, 434, 715, 6110, 732, 6828, 11, 710, 307], "temperature": 0.0, "avg_logprob": -0.16819361277988978, "compression_ratio": 1.7012987012987013, "no_speech_prob": 6.203359953360632e-05}, {"id": 175, "seek": 117000, "start": 1178.16, "end": 1184.0, "text": " a function of x. Then you can work out the derivative of z with respect to x. And the", "tokens": [257, 2445, 295, 2031, 13, 1396, 291, 393, 589, 484, 264, 13760, 295, 710, 365, 3104, 281, 2031, 13, 400, 264], "temperature": 0.0, "avg_logprob": -0.16819361277988978, "compression_ratio": 1.7012987012987013, "no_speech_prob": 6.203359953360632e-05}, {"id": 176, "seek": 117000, "start": 1184.0, "end": 1189.68, "text": " way you do that is with the chain rule. And so in the chain rule, you multiply derivatives.", "tokens": [636, 291, 360, 300, 307, 365, 264, 5021, 4978, 13, 400, 370, 294, 264, 5021, 4978, 11, 291, 12972, 33733, 13], "temperature": 0.0, "avg_logprob": -0.16819361277988978, "compression_ratio": 1.7012987012987013, "no_speech_prob": 6.203359953360632e-05}, {"id": 177, "seek": 118968, "start": 1189.68, "end": 1203.24, "text": " So dzdx equals dzdy times dydx. So dzdy is just three, and dydx is 2x. So we get three", "tokens": [407, 274, 89, 67, 87, 6915, 274, 89, 3173, 1413, 14584, 67, 87, 13, 407, 274, 89, 3173, 307, 445, 1045, 11, 293, 14584, 67, 87, 307, 568, 87, 13, 407, 321, 483, 1045], "temperature": 0.0, "avg_logprob": -0.2295452148195297, "compression_ratio": 1.3953488372093024, "no_speech_prob": 6.107244553277269e-05}, {"id": 178, "seek": 118968, "start": 1203.24, "end": 1213.6000000000001, "text": " times 2x. So that overall, the derivative here is 6x. And since if we multiply this together,", "tokens": [1413, 568, 87, 13, 407, 300, 4787, 11, 264, 13760, 510, 307, 1386, 87, 13, 400, 1670, 498, 321, 12972, 341, 1214, 11], "temperature": 0.0, "avg_logprob": -0.2295452148195297, "compression_ratio": 1.3953488372093024, "no_speech_prob": 6.107244553277269e-05}, {"id": 179, "seek": 121360, "start": 1213.6, "end": 1220.28, "text": " we're really saying that z equals 3x squared, you should trivially be able to see again,", "tokens": [321, 434, 534, 1566, 300, 710, 6915, 805, 87, 8889, 11, 291, 820, 1376, 85, 2270, 312, 1075, 281, 536, 797, 11], "temperature": 0.0, "avg_logprob": -0.177635254398469, "compression_ratio": 1.565217391304348, "no_speech_prob": 3.320947507745586e-05}, {"id": 180, "seek": 121360, "start": 1220.28, "end": 1227.9599999999998, "text": " aha, it's derivative is 6x. So that works. Okay. So once we move into vectors and matrices", "tokens": [47340, 11, 309, 311, 13760, 307, 1386, 87, 13, 407, 300, 1985, 13, 1033, 13, 407, 1564, 321, 1286, 666, 18875, 293, 32284], "temperature": 0.0, "avg_logprob": -0.177635254398469, "compression_ratio": 1.565217391304348, "no_speech_prob": 3.320947507745586e-05}, {"id": 181, "seek": 121360, "start": 1227.9599999999998, "end": 1235.1999999999998, "text": " and Jacobians, it's actually the same game. So when we're working with those, we can compose", "tokens": [293, 14117, 2567, 11, 309, 311, 767, 264, 912, 1216, 13, 407, 562, 321, 434, 1364, 365, 729, 11, 321, 393, 35925], "temperature": 0.0, "avg_logprob": -0.177635254398469, "compression_ratio": 1.565217391304348, "no_speech_prob": 3.320947507745586e-05}, {"id": 182, "seek": 121360, "start": 1235.1999999999998, "end": 1241.6, "text": " functions and work out their derivatives by simply multiplying Jacobians. So if we have", "tokens": [6828, 293, 589, 484, 641, 33733, 538, 2935, 30955, 14117, 2567, 13, 407, 498, 321, 362], "temperature": 0.0, "avg_logprob": -0.177635254398469, "compression_ratio": 1.565217391304348, "no_speech_prob": 3.320947507745586e-05}, {"id": 183, "seek": 124160, "start": 1241.6, "end": 1248.1599999999999, "text": " start with an input x and then put it through the simplest form of neural network layer", "tokens": [722, 365, 364, 4846, 2031, 293, 550, 829, 309, 807, 264, 22811, 1254, 295, 18161, 3209, 4583], "temperature": 0.0, "avg_logprob": -0.1773325066817434, "compression_ratio": 1.5874439461883407, "no_speech_prob": 7.24806304788217e-05}, {"id": 184, "seek": 124160, "start": 1248.1599999999999, "end": 1255.6, "text": " and say that z equals wx plus b. So we multiply the expected by matrix w and then add on a", "tokens": [293, 584, 300, 710, 6915, 261, 87, 1804, 272, 13, 407, 321, 12972, 264, 5176, 538, 8141, 261, 293, 550, 909, 322, 257], "temperature": 0.0, "avg_logprob": -0.1773325066817434, "compression_ratio": 1.5874439461883407, "no_speech_prob": 7.24806304788217e-05}, {"id": 185, "seek": 124160, "start": 1255.6, "end": 1261.7199999999998, "text": " bias vector b. And then typically we'd put things through a nonlinearity f. So f could", "tokens": [12577, 8062, 272, 13, 400, 550, 5850, 321, 1116, 829, 721, 807, 257, 2107, 1889, 17409, 283, 13, 407, 283, 727], "temperature": 0.0, "avg_logprob": -0.1773325066817434, "compression_ratio": 1.5874439461883407, "no_speech_prob": 7.24806304788217e-05}, {"id": 186, "seek": 124160, "start": 1261.7199999999998, "end": 1268.12, "text": " be a sigmoid function. We'll then say 8 equals f of z. So this is the composition of two", "tokens": [312, 257, 4556, 3280, 327, 2445, 13, 492, 603, 550, 584, 1649, 6915, 283, 295, 710, 13, 407, 341, 307, 264, 12686, 295, 732], "temperature": 0.0, "avg_logprob": -0.1773325066817434, "compression_ratio": 1.5874439461883407, "no_speech_prob": 7.24806304788217e-05}, {"id": 187, "seek": 126812, "start": 1268.12, "end": 1275.4399999999998, "text": " functions in terms of vectors and matrices. So we can use Jacobians and we can say the", "tokens": [6828, 294, 2115, 295, 18875, 293, 32284, 13, 407, 321, 393, 764, 14117, 2567, 293, 321, 393, 584, 264], "temperature": 0.0, "avg_logprob": -0.08763903209141323, "compression_ratio": 1.7549668874172186, "no_speech_prob": 3.704217306221835e-05}, {"id": 188, "seek": 126812, "start": 1275.4399999999998, "end": 1283.56, "text": " partial of h with respect to x is going to be the product of the partial of h with respect", "tokens": [14641, 295, 276, 365, 3104, 281, 2031, 307, 516, 281, 312, 264, 1674, 295, 264, 14641, 295, 276, 365, 3104], "temperature": 0.0, "avg_logprob": -0.08763903209141323, "compression_ratio": 1.7549668874172186, "no_speech_prob": 3.704217306221835e-05}, {"id": 189, "seek": 126812, "start": 1283.56, "end": 1291.4799999999998, "text": " to z and the partial of z with respect to x. And this all does work out. So let's start", "tokens": [281, 710, 293, 264, 14641, 295, 710, 365, 3104, 281, 2031, 13, 400, 341, 439, 775, 589, 484, 13, 407, 718, 311, 722], "temperature": 0.0, "avg_logprob": -0.08763903209141323, "compression_ratio": 1.7549668874172186, "no_speech_prob": 3.704217306221835e-05}, {"id": 190, "seek": 129148, "start": 1291.48, "end": 1299.08, "text": " going through some examples of how these things work slightly more concretely. First,", "tokens": [516, 807, 512, 5110, 295, 577, 613, 721, 589, 4748, 544, 39481, 736, 13, 2386, 11], "temperature": 0.0, "avg_logprob": -0.11968127373726137, "compression_ratio": 1.5114942528735633, "no_speech_prob": 1.8054724932881072e-05}, {"id": 191, "seek": 129148, "start": 1299.08, "end": 1307.32, "text": " just particular Jacobians and then composing them together. So one case we look at is the", "tokens": [445, 1729, 14117, 2567, 293, 550, 715, 6110, 552, 1214, 13, 407, 472, 1389, 321, 574, 412, 307, 264], "temperature": 0.0, "avg_logprob": -0.11968127373726137, "compression_ratio": 1.5114942528735633, "no_speech_prob": 1.8054724932881072e-05}, {"id": 192, "seek": 129148, "start": 1307.32, "end": 1313.4, "text": " nonlinearities that we put a vector through. So this is something like putting a vector", "tokens": [2107, 1889, 289, 1088, 300, 321, 829, 257, 8062, 807, 13, 407, 341, 307, 746, 411, 3372, 257, 8062], "temperature": 0.0, "avg_logprob": -0.11968127373726137, "compression_ratio": 1.5114942528735633, "no_speech_prob": 1.8054724932881072e-05}, {"id": 193, "seek": 131340, "start": 1313.4, "end": 1321.4, "text": " through the sigmoid function f. And so if we have an intermediate vector z and we're turning", "tokens": [807, 264, 4556, 3280, 327, 2445, 283, 13, 400, 370, 498, 321, 362, 364, 19376, 8062, 710, 293, 321, 434, 6246], "temperature": 0.0, "avg_logprob": -0.18019734996638886, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.9212093320675194e-05}, {"id": 194, "seek": 131340, "start": 1321.4, "end": 1329.5600000000002, "text": " into vector h by putting it through a logistic function, we can say what is dhdz.", "tokens": [666, 8062, 276, 538, 3372, 309, 807, 257, 3565, 3142, 2445, 11, 321, 393, 584, 437, 307, 274, 71, 28168, 13], "temperature": 0.0, "avg_logprob": -0.18019734996638886, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.9212093320675194e-05}, {"id": 195, "seek": 131340, "start": 1332.0400000000002, "end": 1342.2, "text": " Well, for this, formally, this is a function that has n inputs and n outputs. So at the end", "tokens": [1042, 11, 337, 341, 11, 25983, 11, 341, 307, 257, 2445, 300, 575, 297, 15743, 293, 297, 23930, 13, 407, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.18019734996638886, "compression_ratio": 1.528735632183908, "no_speech_prob": 1.9212093320675194e-05}, {"id": 196, "seek": 134220, "start": 1342.2, "end": 1350.92, "text": " of the day, we're computing an n by n Jacobian. And so what that's meaning is the elements", "tokens": [295, 264, 786, 11, 321, 434, 15866, 364, 297, 538, 297, 14117, 952, 13, 400, 370, 437, 300, 311, 3620, 307, 264, 4959], "temperature": 0.0, "avg_logprob": -0.07194022874574403, "compression_ratio": 1.5988372093023255, "no_speech_prob": 4.753992106998339e-05}, {"id": 197, "seek": 134220, "start": 1350.92, "end": 1359.32, "text": " of this n by n Jacobian are going to take the partial derivative of each output with respect", "tokens": [295, 341, 297, 538, 297, 14117, 952, 366, 516, 281, 747, 264, 14641, 13760, 295, 1184, 5598, 365, 3104], "temperature": 0.0, "avg_logprob": -0.07194022874574403, "compression_ratio": 1.5988372093023255, "no_speech_prob": 4.753992106998339e-05}, {"id": 198, "seek": 134220, "start": 1359.32, "end": 1368.28, "text": " to each input. And well, what is that going to be in this case? Well, in this case, because", "tokens": [281, 1184, 4846, 13, 400, 731, 11, 437, 307, 300, 516, 281, 312, 294, 341, 1389, 30, 1042, 11, 294, 341, 1389, 11, 570], "temperature": 0.0, "avg_logprob": -0.07194022874574403, "compression_ratio": 1.5988372093023255, "no_speech_prob": 4.753992106998339e-05}, {"id": 199, "seek": 136828, "start": 1368.28, "end": 1376.6, "text": " we're actually just computing element-wise a transformation such as a logistic transform", "tokens": [321, 434, 767, 445, 15866, 4478, 12, 3711, 257, 9887, 1270, 382, 257, 3565, 3142, 4088], "temperature": 0.0, "avg_logprob": -0.10932858310528655, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.533096969476901e-05}, {"id": 200, "seek": 136828, "start": 1376.6, "end": 1385.96, "text": " of each element zi, like the second equation here. If i equals j, we've got something to compute.", "tokens": [295, 1184, 4478, 710, 72, 11, 411, 264, 1150, 5367, 510, 13, 759, 741, 6915, 361, 11, 321, 600, 658, 746, 281, 14722, 13], "temperature": 0.0, "avg_logprob": -0.10932858310528655, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.533096969476901e-05}, {"id": 201, "seek": 136828, "start": 1385.96, "end": 1393.0, "text": " Whereas if i doesn't equal j, there's just the input has no influence on the output", "tokens": [13813, 498, 741, 1177, 380, 2681, 361, 11, 456, 311, 445, 264, 4846, 575, 572, 6503, 322, 264, 5598], "temperature": 0.0, "avg_logprob": -0.10932858310528655, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.533096969476901e-05}, {"id": 202, "seek": 139300, "start": 1393.0, "end": 1399.64, "text": " and so the derivative is zero. So if i doesn't equal j, we're going to get a zero. And if i does", "tokens": [293, 370, 264, 13760, 307, 4018, 13, 407, 498, 741, 1177, 380, 2681, 361, 11, 321, 434, 516, 281, 483, 257, 4018, 13, 400, 498, 741, 775], "temperature": 0.0, "avg_logprob": -0.12013486940033581, "compression_ratio": 1.7641509433962264, "no_speech_prob": 4.753850225824863e-05}, {"id": 203, "seek": 139300, "start": 1399.64, "end": 1407.16, "text": " equal j, then we're going to get the regular one variable derivative of the logistic function,", "tokens": [2681, 361, 11, 550, 321, 434, 516, 281, 483, 264, 3890, 472, 7006, 13760, 295, 264, 3565, 3142, 2445, 11], "temperature": 0.0, "avg_logprob": -0.12013486940033581, "compression_ratio": 1.7641509433962264, "no_speech_prob": 4.753850225824863e-05}, {"id": 204, "seek": 139300, "start": 1407.16, "end": 1414.68, "text": " which if I remember correctly, you were asked to compute, now I can't remember what's the", "tokens": [597, 498, 286, 1604, 8944, 11, 291, 645, 2351, 281, 14722, 11, 586, 286, 393, 380, 1604, 437, 311, 264], "temperature": 0.0, "avg_logprob": -0.12013486940033581, "compression_ratio": 1.7641509433962264, "no_speech_prob": 4.753850225824863e-05}, {"id": 205, "seek": 139300, "start": 1414.68, "end": 1420.6, "text": " assignment one or assignment two, but one of the two asks you to compute it. So our Jacobian", "tokens": [15187, 472, 420, 15187, 732, 11, 457, 472, 295, 264, 732, 8962, 291, 281, 14722, 309, 13, 407, 527, 14117, 952], "temperature": 0.0, "avg_logprob": -0.12013486940033581, "compression_ratio": 1.7641509433962264, "no_speech_prob": 4.753850225824863e-05}, {"id": 206, "seek": 142060, "start": 1420.6, "end": 1430.1999999999998, "text": " for this case looks like this. We have a diagonal matrix with the derivatives of each element", "tokens": [337, 341, 1389, 1542, 411, 341, 13, 492, 362, 257, 21539, 8141, 365, 264, 33733, 295, 1184, 4478], "temperature": 0.0, "avg_logprob": -0.1016801397005717, "compression_ratio": 1.5, "no_speech_prob": 3.0212269848561846e-05}, {"id": 207, "seek": 142060, "start": 1430.1999999999998, "end": 1437.6399999999999, "text": " along the diagonal and everything else is zero. Okay, so let's look at a couple of other Jacobians.", "tokens": [2051, 264, 21539, 293, 1203, 1646, 307, 4018, 13, 1033, 11, 370, 718, 311, 574, 412, 257, 1916, 295, 661, 14117, 2567, 13], "temperature": 0.0, "avg_logprob": -0.1016801397005717, "compression_ratio": 1.5, "no_speech_prob": 3.0212269848561846e-05}, {"id": 208, "seek": 142060, "start": 1438.84, "end": 1446.36, "text": " So if we're asking, if we've got this wx plus b basic neural network layer and we're asking", "tokens": [407, 498, 321, 434, 3365, 11, 498, 321, 600, 658, 341, 261, 87, 1804, 272, 3875, 18161, 3209, 4583, 293, 321, 434, 3365], "temperature": 0.0, "avg_logprob": -0.1016801397005717, "compression_ratio": 1.5, "no_speech_prob": 3.0212269848561846e-05}, {"id": 209, "seek": 144636, "start": 1446.36, "end": 1454.84, "text": " for the gradient with respect to x, then what we're going to have coming out is that's actually", "tokens": [337, 264, 16235, 365, 3104, 281, 2031, 11, 550, 437, 321, 434, 516, 281, 362, 1348, 484, 307, 300, 311, 767], "temperature": 0.0, "avg_logprob": -0.08003069603279846, "compression_ratio": 1.6089385474860336, "no_speech_prob": 8.344770321855322e-05}, {"id": 210, "seek": 144636, "start": 1454.84, "end": 1464.12, "text": " going to be the matrix w. So this is where what I hope you can do is look at the notes at home", "tokens": [516, 281, 312, 264, 8141, 261, 13, 407, 341, 307, 689, 437, 286, 1454, 291, 393, 360, 307, 574, 412, 264, 5570, 412, 1280], "temperature": 0.0, "avg_logprob": -0.08003069603279846, "compression_ratio": 1.6089385474860336, "no_speech_prob": 8.344770321855322e-05}, {"id": 211, "seek": 144636, "start": 1464.12, "end": 1473.08, "text": " and work through this exactly and see that this is actually the right answer. But this is the way", "tokens": [293, 589, 807, 341, 2293, 293, 536, 300, 341, 307, 767, 264, 558, 1867, 13, 583, 341, 307, 264, 636], "temperature": 0.0, "avg_logprob": -0.08003069603279846, "compression_ratio": 1.6089385474860336, "no_speech_prob": 8.344770321855322e-05}, {"id": 212, "seek": 147308, "start": 1473.08, "end": 1481.0, "text": " in which if you just have faith and think this is just like single variable calculus, except I've", "tokens": [294, 597, 498, 291, 445, 362, 4522, 293, 519, 341, 307, 445, 411, 2167, 7006, 33400, 11, 3993, 286, 600], "temperature": 0.0, "avg_logprob": -0.11315584854340889, "compression_ratio": 1.53125, "no_speech_prob": 7.480199565179646e-05}, {"id": 213, "seek": 147308, "start": 1481.0, "end": 1486.4399999999998, "text": " now got vectors and matrices, the answer you get is actually what you expected to get because this", "tokens": [586, 658, 18875, 293, 32284, 11, 264, 1867, 291, 483, 307, 767, 437, 291, 5176, 281, 483, 570, 341], "temperature": 0.0, "avg_logprob": -0.11315584854340889, "compression_ratio": 1.53125, "no_speech_prob": 7.480199565179646e-05}, {"id": 214, "seek": 147308, "start": 1486.4399999999998, "end": 1495.56, "text": " is just like the derivative of ax plus b with respect to x where it's a. So similarly, if we take", "tokens": [307, 445, 411, 264, 13760, 295, 6360, 1804, 272, 365, 3104, 281, 2031, 689, 309, 311, 257, 13, 407, 14138, 11, 498, 321, 747], "temperature": 0.0, "avg_logprob": -0.11315584854340889, "compression_ratio": 1.53125, "no_speech_prob": 7.480199565179646e-05}, {"id": 215, "seek": 149556, "start": 1495.56, "end": 1505.8, "text": " the partial derivative with respect to b of wx plus b, we get out the identity matrix. Okay, then one", "tokens": [264, 14641, 13760, 365, 3104, 281, 272, 295, 261, 87, 1804, 272, 11, 321, 483, 484, 264, 6575, 8141, 13, 1033, 11, 550, 472], "temperature": 0.0, "avg_logprob": -0.1369272994995117, "compression_ratio": 1.528497409326425, "no_speech_prob": 2.013411358348094e-05}, {"id": 216, "seek": 149556, "start": 1505.8, "end": 1512.76, "text": " other Jacobian that we mentioned while in the first lecture while working through word to veck", "tokens": [661, 14117, 952, 300, 321, 2835, 1339, 294, 264, 700, 7991, 1339, 1364, 807, 1349, 281, 1241, 547], "temperature": 0.0, "avg_logprob": -0.1369272994995117, "compression_ratio": 1.528497409326425, "no_speech_prob": 2.013411358348094e-05}, {"id": 217, "seek": 149556, "start": 1512.76, "end": 1524.28, "text": " is if you have the dot product of two vectors, i, that's a number, that what you get coming out of", "tokens": [307, 498, 291, 362, 264, 5893, 1674, 295, 732, 18875, 11, 741, 11, 300, 311, 257, 1230, 11, 300, 437, 291, 483, 1348, 484, 295], "temperature": 0.0, "avg_logprob": -0.1369272994995117, "compression_ratio": 1.528497409326425, "no_speech_prob": 2.013411358348094e-05}, {"id": 218, "seek": 152428, "start": 1524.28, "end": 1534.28, "text": " that, so the the partial derivative of uth with respect to u is h transpose. And at this point,", "tokens": [300, 11, 370, 264, 264, 14641, 13760, 295, 2839, 71, 365, 3104, 281, 344, 307, 276, 25167, 13, 400, 412, 341, 935, 11], "temperature": 0.0, "avg_logprob": -0.1353137254714966, "compression_ratio": 1.505050505050505, "no_speech_prob": 1.4724729226145428e-05}, {"id": 219, "seek": 152428, "start": 1534.28, "end": 1542.92, "text": " there's some fine print that I'm going to come back to in a minute. So this is the correct Jacobian,", "tokens": [456, 311, 512, 2489, 4482, 300, 286, 478, 516, 281, 808, 646, 281, 294, 257, 3456, 13, 407, 341, 307, 264, 3006, 14117, 952, 11], "temperature": 0.0, "avg_logprob": -0.1353137254714966, "compression_ratio": 1.505050505050505, "no_speech_prob": 1.4724729226145428e-05}, {"id": 220, "seek": 152428, "start": 1542.92, "end": 1553.56, "text": " right? Because in this case, we have the dimension of h inputs and we have one output. And so we want", "tokens": [558, 30, 1436, 294, 341, 1389, 11, 321, 362, 264, 10139, 295, 276, 15743, 293, 321, 362, 472, 5598, 13, 400, 370, 321, 528], "temperature": 0.0, "avg_logprob": -0.1353137254714966, "compression_ratio": 1.505050505050505, "no_speech_prob": 1.4724729226145428e-05}, {"id": 221, "seek": 155356, "start": 1553.56, "end": 1560.9199999999998, "text": " to have a row vector. But there's a little bit more to say on that that I'll come back to in", "tokens": [281, 362, 257, 5386, 8062, 13, 583, 456, 311, 257, 707, 857, 544, 281, 584, 322, 300, 300, 286, 603, 808, 646, 281, 294], "temperature": 0.0, "avg_logprob": -0.08635576775199488, "compression_ratio": 1.5025906735751295, "no_speech_prob": 3.0228495234041475e-05}, {"id": 222, "seek": 155356, "start": 1560.9199999999998, "end": 1569.32, "text": " about 20 slides. But this is the correct Jacobian. Okay, so if you aren't not familiar with these", "tokens": [466, 945, 9788, 13, 583, 341, 307, 264, 3006, 14117, 952, 13, 1033, 11, 370, 498, 291, 3212, 380, 406, 4963, 365, 613], "temperature": 0.0, "avg_logprob": -0.08635576775199488, "compression_ratio": 1.5025906735751295, "no_speech_prob": 3.0228495234041475e-05}, {"id": 223, "seek": 155356, "start": 1569.32, "end": 1577.72, "text": " kind of Jacobians, do please look at some of the notes that are available and try and compute these", "tokens": [733, 295, 14117, 2567, 11, 360, 1767, 574, 412, 512, 295, 264, 5570, 300, 366, 2435, 293, 853, 293, 14722, 613], "temperature": 0.0, "avg_logprob": -0.08635576775199488, "compression_ratio": 1.5025906735751295, "no_speech_prob": 3.0228495234041475e-05}, {"id": 224, "seek": 157772, "start": 1577.72, "end": 1583.56, "text": " in more detail element wise and convince yourself that they really are right. But I'm going to assume", "tokens": [294, 544, 2607, 4478, 10829, 293, 13447, 1803, 300, 436, 534, 366, 558, 13, 583, 286, 478, 516, 281, 6552], "temperature": 0.0, "avg_logprob": -0.09002466201782226, "compression_ratio": 1.525, "no_speech_prob": 5.647058424074203e-05}, {"id": 225, "seek": 157772, "start": 1583.56, "end": 1590.68, "text": " these now and show you what happens when we actually then work out gradients for at least a mini little", "tokens": [613, 586, 293, 855, 291, 437, 2314, 562, 321, 767, 550, 589, 484, 2771, 2448, 337, 412, 1935, 257, 8382, 707], "temperature": 0.0, "avg_logprob": -0.09002466201782226, "compression_ratio": 1.525, "no_speech_prob": 5.647058424074203e-05}, {"id": 226, "seek": 157772, "start": 1590.68, "end": 1607.0, "text": " neural net. Okay, so here is most of this neural net. I mean, as I commented that, you know, really", "tokens": [18161, 2533, 13, 1033, 11, 370, 510, 307, 881, 295, 341, 18161, 2533, 13, 286, 914, 11, 382, 286, 26940, 300, 11, 291, 458, 11, 534], "temperature": 0.0, "avg_logprob": -0.09002466201782226, "compression_ratio": 1.525, "no_speech_prob": 5.647058424074203e-05}, {"id": 227, "seek": 160700, "start": 1607.0, "end": 1613.24, "text": " we'd be working out the partial derivative of the loss j with respect to these variables.", "tokens": [321, 1116, 312, 1364, 484, 264, 14641, 13760, 295, 264, 4470, 361, 365, 3104, 281, 613, 9102, 13], "temperature": 0.0, "avg_logprob": -0.0683761437733968, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.00016007377416826785}, {"id": 228, "seek": 160700, "start": 1613.24, "end": 1618.6, "text": " But for the example I'm doing here, I just I've locked that off to keep it a little simpler and", "tokens": [583, 337, 264, 1365, 286, 478, 884, 510, 11, 286, 445, 286, 600, 9376, 300, 766, 281, 1066, 309, 257, 707, 18587, 293], "temperature": 0.0, "avg_logprob": -0.0683761437733968, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.00016007377416826785}, {"id": 229, "seek": 160700, "start": 1618.6, "end": 1624.28, "text": " more manageable for the lecture. And so we're going to just work out the partial derivative of the", "tokens": [544, 38798, 337, 264, 7991, 13, 400, 370, 321, 434, 516, 281, 445, 589, 484, 264, 14641, 13760, 295, 264], "temperature": 0.0, "avg_logprob": -0.0683761437733968, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.00016007377416826785}, {"id": 230, "seek": 160700, "start": 1624.28, "end": 1631.16, "text": " score s, which is a real number with respect to the different parameters of this model where the", "tokens": [6175, 262, 11, 597, 307, 257, 957, 1230, 365, 3104, 281, 264, 819, 9834, 295, 341, 2316, 689, 264], "temperature": 0.0, "avg_logprob": -0.0683761437733968, "compression_ratio": 1.7397260273972603, "no_speech_prob": 0.00016007377416826785}, {"id": 231, "seek": 163116, "start": 1631.16, "end": 1641.72, "text": " parameters of this model are going to be the w and the b and the u and also the input because we", "tokens": [9834, 295, 341, 2316, 366, 516, 281, 312, 264, 261, 293, 264, 272, 293, 264, 344, 293, 611, 264, 4846, 570, 321], "temperature": 0.0, "avg_logprob": -0.10069251782966382, "compression_ratio": 1.6327683615819208, "no_speech_prob": 2.109509296133183e-05}, {"id": 232, "seek": 163116, "start": 1641.72, "end": 1651.0, "text": " can update the weight vectors of the word vectors of different words based on tuning them to better", "tokens": [393, 5623, 264, 3364, 18875, 295, 264, 1349, 18875, 295, 819, 2283, 2361, 322, 15164, 552, 281, 1101], "temperature": 0.0, "avg_logprob": -0.10069251782966382, "compression_ratio": 1.6327683615819208, "no_speech_prob": 2.109509296133183e-05}, {"id": 233, "seek": 163116, "start": 1651.0, "end": 1657.72, "text": " predict the classification outputs that we desire. So let's start off with a fairly easy one", "tokens": [6069, 264, 21538, 23930, 300, 321, 7516, 13, 407, 718, 311, 722, 766, 365, 257, 6457, 1858, 472], "temperature": 0.0, "avg_logprob": -0.10069251782966382, "compression_ratio": 1.6327683615819208, "no_speech_prob": 2.109509296133183e-05}, {"id": 234, "seek": 165772, "start": 1657.72, "end": 1666.44, "text": " where we want to update the bias vector b to have our system classify better. So to be able to", "tokens": [689, 321, 528, 281, 5623, 264, 12577, 8062, 272, 281, 362, 527, 1185, 33872, 1101, 13, 407, 281, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.09065770088358129, "compression_ratio": 1.6515837104072397, "no_speech_prob": 3.119538814644329e-05}, {"id": 235, "seek": 165772, "start": 1666.44, "end": 1671.8, "text": " do that, what we want to work out is the partial derivatives of s with respect to b.", "tokens": [360, 300, 11, 437, 321, 528, 281, 589, 484, 307, 264, 14641, 33733, 295, 262, 365, 3104, 281, 272, 13], "temperature": 0.0, "avg_logprob": -0.09065770088358129, "compression_ratio": 1.6515837104072397, "no_speech_prob": 3.119538814644329e-05}, {"id": 236, "seek": 165772, "start": 1673.0, "end": 1679.56, "text": " So we know how to put that into our stochastic gradient update for the b parameters.", "tokens": [407, 321, 458, 577, 281, 829, 300, 666, 527, 342, 8997, 2750, 16235, 5623, 337, 264, 272, 9834, 13], "temperature": 0.0, "avg_logprob": -0.09065770088358129, "compression_ratio": 1.6515837104072397, "no_speech_prob": 3.119538814644329e-05}, {"id": 237, "seek": 165772, "start": 1680.28, "end": 1687.64, "text": " Okay, so how do we go about doing these things? So the first step is we want to sort of break things", "tokens": [1033, 11, 370, 577, 360, 321, 352, 466, 884, 613, 721, 30, 407, 264, 700, 1823, 307, 321, 528, 281, 1333, 295, 1821, 721], "temperature": 0.0, "avg_logprob": -0.09065770088358129, "compression_ratio": 1.6515837104072397, "no_speech_prob": 3.119538814644329e-05}, {"id": 238, "seek": 168764, "start": 1687.64, "end": 1695.64, "text": " up into different functions of minimal complexity that compose together. So in particular,", "tokens": [493, 666, 819, 6828, 295, 13206, 14024, 300, 35925, 1214, 13, 407, 294, 1729, 11], "temperature": 0.0, "avg_logprob": -0.12861213684082032, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.068871930940077e-05}, {"id": 239, "seek": 168764, "start": 1695.64, "end": 1702.5200000000002, "text": " this neural net layer, a equals f of wx plus b, it's still a little bit complex. So let's decompose", "tokens": [341, 18161, 2533, 4583, 11, 257, 6915, 283, 295, 261, 87, 1804, 272, 11, 309, 311, 920, 257, 707, 857, 3997, 13, 407, 718, 311, 22867, 541], "temperature": 0.0, "avg_logprob": -0.12861213684082032, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.068871930940077e-05}, {"id": 240, "seek": 168764, "start": 1702.5200000000002, "end": 1713.0, "text": " that one further step. So we have the input x, we then calculate the linear transformation z equals", "tokens": [300, 472, 3052, 1823, 13, 407, 321, 362, 264, 4846, 2031, 11, 321, 550, 8873, 264, 8213, 9887, 710, 6915], "temperature": 0.0, "avg_logprob": -0.12861213684082032, "compression_ratio": 1.576086956521739, "no_speech_prob": 4.068871930940077e-05}, {"id": 241, "seek": 171300, "start": 1713.0, "end": 1724.84, "text": " wx plus b. And then we put things through the sort of element wise non-linearity, a equals f of z,", "tokens": [261, 87, 1804, 272, 13, 400, 550, 321, 829, 721, 807, 264, 1333, 295, 4478, 10829, 2107, 12, 1889, 17409, 11, 257, 6915, 283, 295, 710, 11], "temperature": 0.0, "avg_logprob": -0.17210095637553446, "compression_ratio": 1.5351351351351352, "no_speech_prob": 3.372111314092763e-05}, {"id": 242, "seek": 171300, "start": 1724.84, "end": 1734.84, "text": " and then we do the dot product with u. And it's useful for working these things out to split into", "tokens": [293, 550, 321, 360, 264, 5893, 1674, 365, 344, 13, 400, 309, 311, 4420, 337, 1364, 613, 721, 484, 281, 7472, 666], "temperature": 0.0, "avg_logprob": -0.17210095637553446, "compression_ratio": 1.5351351351351352, "no_speech_prob": 3.372111314092763e-05}, {"id": 243, "seek": 171300, "start": 1734.84, "end": 1740.92, "text": " pieces like this, have straight what your different variables are, and to know what the", "tokens": [3755, 411, 341, 11, 362, 2997, 437, 428, 819, 9102, 366, 11, 293, 281, 458, 437, 264], "temperature": 0.0, "avg_logprob": -0.17210095637553446, "compression_ratio": 1.5351351351351352, "no_speech_prob": 3.372111314092763e-05}, {"id": 244, "seek": 174092, "start": 1740.92, "end": 1747.16, "text": " dimensionality of each of these variables is, it's well worth just writing out the dimensionality", "tokens": [10139, 1860, 295, 1184, 295, 613, 9102, 307, 11, 309, 311, 731, 3163, 445, 3579, 484, 264, 10139, 1860], "temperature": 0.0, "avg_logprob": -0.11357567203578664, "compression_ratio": 1.6820809248554913, "no_speech_prob": 2.7501269869389944e-05}, {"id": 245, "seek": 174092, "start": 1747.16, "end": 1752.92, "text": " of every variable and making sure that the answers that you're computing are of the right dimensionality.", "tokens": [295, 633, 7006, 293, 1455, 988, 300, 264, 6338, 300, 291, 434, 15866, 366, 295, 264, 558, 10139, 1860, 13], "temperature": 0.0, "avg_logprob": -0.11357567203578664, "compression_ratio": 1.6820809248554913, "no_speech_prob": 2.7501269869389944e-05}, {"id": 246, "seek": 174092, "start": 1753.88, "end": 1762.04, "text": " So at this point though, what we can see is that calculating s is the product of three,", "tokens": [407, 412, 341, 935, 1673, 11, 437, 321, 393, 536, 307, 300, 28258, 262, 307, 264, 1674, 295, 1045, 11], "temperature": 0.0, "avg_logprob": -0.11357567203578664, "compression_ratio": 1.6820809248554913, "no_speech_prob": 2.7501269869389944e-05}, {"id": 247, "seek": 176204, "start": 1762.04, "end": 1771.8799999999999, "text": " sorry, is the composition of three functions around x. So for working out the partials of s with", "tokens": [2597, 11, 307, 264, 12686, 295, 1045, 6828, 926, 2031, 13, 407, 337, 1364, 484, 264, 14641, 82, 295, 262, 365], "temperature": 0.0, "avg_logprob": -0.1443545784748776, "compression_ratio": 1.8048780487804879, "no_speech_prob": 5.302318822941743e-05}, {"id": 248, "seek": 176204, "start": 1771.8799999999999, "end": 1780.36, "text": " respect to b, it's the composition of the three functions shown on the left. And so therefore,", "tokens": [3104, 281, 272, 11, 309, 311, 264, 12686, 295, 264, 1045, 6828, 4898, 322, 264, 1411, 13, 400, 370, 4412, 11], "temperature": 0.0, "avg_logprob": -0.1443545784748776, "compression_ratio": 1.8048780487804879, "no_speech_prob": 5.302318822941743e-05}, {"id": 249, "seek": 178036, "start": 1780.36, "end": 1791.6399999999999, "text": " the gradient of s with respect to b, we're going to take the product of these three partial derivatives.", "tokens": [264, 16235, 295, 262, 365, 3104, 281, 272, 11, 321, 434, 516, 281, 747, 264, 1674, 295, 613, 1045, 14641, 33733, 13], "temperature": 0.0, "avg_logprob": -0.1818981170654297, "compression_ratio": 1.4233576642335766, "no_speech_prob": 4.459166302694939e-05}, {"id": 250, "seek": 178036, "start": 1793.8799999999999, "end": 1804.6799999999998, "text": " Okay, so how do we, so we've got the s equals uth, so that's sort of the top corresponding", "tokens": [1033, 11, 370, 577, 360, 321, 11, 370, 321, 600, 658, 264, 262, 6915, 2839, 71, 11, 370, 300, 311, 1333, 295, 264, 1192, 11760], "temperature": 0.0, "avg_logprob": -0.1818981170654297, "compression_ratio": 1.4233576642335766, "no_speech_prob": 4.459166302694939e-05}, {"id": 251, "seek": 180468, "start": 1804.68, "end": 1811.3200000000002, "text": " partial derivative, partial derivative of h with respect to z, partial derivative of z with respect", "tokens": [14641, 13760, 11, 14641, 13760, 295, 276, 365, 3104, 281, 710, 11, 14641, 13760, 295, 710, 365, 3104], "temperature": 0.0, "avg_logprob": -0.11404820456021074, "compression_ratio": 1.6951219512195121, "no_speech_prob": 4.0018876461545005e-05}, {"id": 252, "seek": 180468, "start": 1811.96, "end": 1818.68, "text": " to b, which is the first one that we're working out. Okay, so we want to work this out,", "tokens": [281, 272, 11, 597, 307, 264, 700, 472, 300, 321, 434, 1364, 484, 13, 1033, 11, 370, 321, 528, 281, 589, 341, 484, 11], "temperature": 0.0, "avg_logprob": -0.11404820456021074, "compression_ratio": 1.6951219512195121, "no_speech_prob": 4.0018876461545005e-05}, {"id": 253, "seek": 180468, "start": 1818.68, "end": 1825.8, "text": " and if we're lucky, we remember those Jacobians I showed previously about the Jacobian for", "tokens": [293, 498, 321, 434, 6356, 11, 321, 1604, 729, 14117, 2567, 286, 4712, 8046, 466, 264, 14117, 952, 337], "temperature": 0.0, "avg_logprob": -0.11404820456021074, "compression_ratio": 1.6951219512195121, "no_speech_prob": 4.0018876461545005e-05}, {"id": 254, "seek": 182580, "start": 1825.8, "end": 1834.9199999999998, "text": " a vector dot product, the Jacobian for the nonlinearity and the Jacobian for the simple linear", "tokens": [257, 8062, 5893, 1674, 11, 264, 14117, 952, 337, 264, 2107, 1889, 17409, 293, 264, 14117, 952, 337, 264, 2199, 8213], "temperature": 0.0, "avg_logprob": -0.09657325483348271, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.045764621376293e-06}, {"id": 255, "seek": 182580, "start": 1834.9199999999998, "end": 1843.72, "text": " transformation. And so we can use those. So for the partials of s with respect to h,", "tokens": [9887, 13, 400, 370, 321, 393, 764, 729, 13, 407, 337, 264, 14641, 82, 295, 262, 365, 3104, 281, 276, 11], "temperature": 0.0, "avg_logprob": -0.09657325483348271, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.045764621376293e-06}, {"id": 256, "seek": 182580, "start": 1845.0, "end": 1852.52, "text": " well, that's going to be ut using the first one. The partials of h with respect to z,", "tokens": [731, 11, 300, 311, 516, 281, 312, 2839, 1228, 264, 700, 472, 13, 440, 14641, 82, 295, 276, 365, 3104, 281, 710, 11], "temperature": 0.0, "avg_logprob": -0.09657325483348271, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.045764621376293e-06}, {"id": 257, "seek": 185252, "start": 1852.52, "end": 1859.72, "text": " okay, so that's the nonlinearity. And so that's going to be the matrix that's the diagonal matrix", "tokens": [1392, 11, 370, 300, 311, 264, 2107, 1889, 17409, 13, 400, 370, 300, 311, 516, 281, 312, 264, 8141, 300, 311, 264, 21539, 8141], "temperature": 0.0, "avg_logprob": -0.1258091070713141, "compression_ratio": 1.6123595505617978, "no_speech_prob": 2.2114765670266934e-05}, {"id": 258, "seek": 185252, "start": 1859.72, "end": 1869.48, "text": " with the element-wise derivative of f prime of z and zero elsewhere. And then for the wx plus b,", "tokens": [365, 264, 4478, 12, 3711, 13760, 295, 283, 5835, 295, 710, 293, 4018, 14517, 13, 400, 550, 337, 264, 261, 87, 1804, 272, 11], "temperature": 0.0, "avg_logprob": -0.1258091070713141, "compression_ratio": 1.6123595505617978, "no_speech_prob": 2.2114765670266934e-05}, {"id": 259, "seek": 185252, "start": 1869.48, "end": 1876.2, "text": " when we're taking the partials with respect to b, that's just the identity matrix. So we can", "tokens": [562, 321, 434, 1940, 264, 14641, 82, 365, 3104, 281, 272, 11, 300, 311, 445, 264, 6575, 8141, 13, 407, 321, 393], "temperature": 0.0, "avg_logprob": -0.1258091070713141, "compression_ratio": 1.6123595505617978, "no_speech_prob": 2.2114765670266934e-05}, {"id": 260, "seek": 187620, "start": 1876.2, "end": 1888.28, "text": " simplify that down a little, the identity matrix disappears. And since ut is a vector, and this is", "tokens": [20460, 300, 760, 257, 707, 11, 264, 6575, 8141, 25527, 13, 400, 1670, 2839, 307, 257, 8062, 11, 293, 341, 307], "temperature": 0.0, "avg_logprob": -0.13941785886690214, "compression_ratio": 1.5691489361702127, "no_speech_prob": 8.527150384907145e-06}, {"id": 261, "seek": 187620, "start": 1888.28, "end": 1896.68, "text": " a diagonal matrix, we can rewrite this as ut, Hadamard product of f prime of z. I think this is the", "tokens": [257, 21539, 8141, 11, 321, 393, 28132, 341, 382, 2839, 11, 12298, 335, 515, 1674, 295, 283, 5835, 295, 710, 13, 286, 519, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.13941785886690214, "compression_ratio": 1.5691489361702127, "no_speech_prob": 8.527150384907145e-06}, {"id": 262, "seek": 187620, "start": 1896.68, "end": 1903.88, "text": " first time I've used this little circle for Hadamard product, but it's something that you'll see", "tokens": [700, 565, 286, 600, 1143, 341, 707, 6329, 337, 12298, 335, 515, 1674, 11, 457, 309, 311, 746, 300, 291, 603, 536], "temperature": 0.0, "avg_logprob": -0.13941785886690214, "compression_ratio": 1.5691489361702127, "no_speech_prob": 8.527150384907145e-06}, {"id": 263, "seek": 190388, "start": 1903.88, "end": 1910.8400000000001, "text": " quite a bit in your network work since it's often used. So when we have two vectors,", "tokens": [1596, 257, 857, 294, 428, 3209, 589, 1670, 309, 311, 2049, 1143, 13, 407, 562, 321, 362, 732, 18875, 11], "temperature": 0.0, "avg_logprob": -0.09443505605061848, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.00011405811528675258}, {"id": 264, "seek": 190388, "start": 1913.24, "end": 1920.3600000000001, "text": " ut and this vector here, sometimes you want to do an element-wise product. So the output of this", "tokens": [2839, 293, 341, 8062, 510, 11, 2171, 291, 528, 281, 360, 364, 4478, 12, 3711, 1674, 13, 407, 264, 5598, 295, 341], "temperature": 0.0, "avg_logprob": -0.09443505605061848, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.00011405811528675258}, {"id": 265, "seek": 190388, "start": 1920.3600000000001, "end": 1925.0, "text": " will be a vector where you've taken the first element of each and multiply them, the second element", "tokens": [486, 312, 257, 8062, 689, 291, 600, 2726, 264, 700, 4478, 295, 1184, 293, 12972, 552, 11, 264, 1150, 4478], "temperature": 0.0, "avg_logprob": -0.09443505605061848, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.00011405811528675258}, {"id": 266, "seek": 190388, "start": 1925.0, "end": 1930.7600000000002, "text": " of each and multiply them, etc. downwards. And so that's called the Hadamard product, and it's", "tokens": [295, 1184, 293, 12972, 552, 11, 5183, 13, 39880, 13, 400, 370, 300, 311, 1219, 264, 12298, 335, 515, 1674, 11, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.09443505605061848, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.00011405811528675258}, {"id": 267, "seek": 193076, "start": 1930.76, "end": 1940.28, "text": " what we're calculating to calculate a vector, which is the gradient of s with respect to b.", "tokens": [437, 321, 434, 28258, 281, 8873, 257, 8062, 11, 597, 307, 264, 16235, 295, 262, 365, 3104, 281, 272, 13], "temperature": 0.0, "avg_logprob": -0.0890374755859375, "compression_ratio": 1.6428571428571428, "no_speech_prob": 6.601143832085654e-05}, {"id": 268, "seek": 193076, "start": 1942.84, "end": 1951.24, "text": " Okay, so that's good. So we now have a gradient of s with respect to b, and we could use that", "tokens": [1033, 11, 370, 300, 311, 665, 13, 407, 321, 586, 362, 257, 16235, 295, 262, 365, 3104, 281, 272, 11, 293, 321, 727, 764, 300], "temperature": 0.0, "avg_logprob": -0.0890374755859375, "compression_ratio": 1.6428571428571428, "no_speech_prob": 6.601143832085654e-05}, {"id": 269, "seek": 193076, "start": 1951.24, "end": 1958.68, "text": " in our stochastic gradient. But we don't stop there. We also want to work out the gradient", "tokens": [294, 527, 342, 8997, 2750, 16235, 13, 583, 321, 500, 380, 1590, 456, 13, 492, 611, 528, 281, 589, 484, 264, 16235], "temperature": 0.0, "avg_logprob": -0.0890374755859375, "compression_ratio": 1.6428571428571428, "no_speech_prob": 6.601143832085654e-05}, {"id": 270, "seek": 195868, "start": 1958.68, "end": 1967.3200000000002, "text": " with respect to others of our parameters. So we might want to next go on and work out the gradient", "tokens": [365, 3104, 281, 2357, 295, 527, 9834, 13, 407, 321, 1062, 528, 281, 958, 352, 322, 293, 589, 484, 264, 16235], "temperature": 0.0, "avg_logprob": -0.10414679409706429, "compression_ratio": 1.548913043478261, "no_speech_prob": 8.605581388110295e-05}, {"id": 271, "seek": 195868, "start": 1967.3200000000002, "end": 1977.72, "text": " of s with respect to w. Well, we can use the chain rule just like we did before. So we've got the", "tokens": [295, 262, 365, 3104, 281, 261, 13, 1042, 11, 321, 393, 764, 264, 5021, 4978, 445, 411, 321, 630, 949, 13, 407, 321, 600, 658, 264], "temperature": 0.0, "avg_logprob": -0.10414679409706429, "compression_ratio": 1.548913043478261, "no_speech_prob": 8.605581388110295e-05}, {"id": 272, "seek": 195868, "start": 1977.72, "end": 1983.88, "text": " same product of functions, and everything is going to be the same, apart from now taking", "tokens": [912, 1674, 295, 6828, 11, 293, 1203, 307, 516, 281, 312, 264, 912, 11, 4936, 490, 586, 1940], "temperature": 0.0, "avg_logprob": -0.10414679409706429, "compression_ratio": 1.548913043478261, "no_speech_prob": 8.605581388110295e-05}, {"id": 273, "seek": 198388, "start": 1983.88, "end": 1993.64, "text": " the derivatives with respect to w rather than b. So it's now going to be the partial of s with", "tokens": [264, 33733, 365, 3104, 281, 261, 2831, 813, 272, 13, 407, 309, 311, 586, 516, 281, 312, 264, 14641, 295, 262, 365], "temperature": 0.0, "avg_logprob": -0.08254088440986529, "compression_ratio": 1.696969696969697, "no_speech_prob": 6.010330616845749e-05}, {"id": 274, "seek": 198388, "start": 1993.64, "end": 2002.92, "text": " respect to h, h with respect to z, and z with respect to w. And the important thing to notice here,", "tokens": [3104, 281, 276, 11, 276, 365, 3104, 281, 710, 11, 293, 710, 365, 3104, 281, 261, 13, 400, 264, 1021, 551, 281, 3449, 510, 11], "temperature": 0.0, "avg_logprob": -0.08254088440986529, "compression_ratio": 1.696969696969697, "no_speech_prob": 6.010330616845749e-05}, {"id": 275, "seek": 198388, "start": 2002.92, "end": 2009.3200000000002, "text": " and this leads into what we do with the back propagation algorithm, is wait a minute,", "tokens": [293, 341, 6689, 666, 437, 321, 360, 365, 264, 646, 38377, 9284, 11, 307, 1699, 257, 3456, 11], "temperature": 0.0, "avg_logprob": -0.08254088440986529, "compression_ratio": 1.696969696969697, "no_speech_prob": 6.010330616845749e-05}, {"id": 276, "seek": 200932, "start": 2009.32, "end": 2016.4399999999998, "text": " that this is very similar to what we've already done. So when we're all working out the gradients", "tokens": [300, 341, 307, 588, 2531, 281, 437, 321, 600, 1217, 1096, 13, 407, 562, 321, 434, 439, 1364, 484, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.08817366377948081, "compression_ratio": 1.5343915343915344, "no_speech_prob": 7.840268517611548e-05}, {"id": 277, "seek": 200932, "start": 2016.4399999999998, "end": 2024.2, "text": " of s with respect to b, the first two terms were exactly the same. It's only the last one that", "tokens": [295, 262, 365, 3104, 281, 272, 11, 264, 700, 732, 2115, 645, 2293, 264, 912, 13, 467, 311, 787, 264, 1036, 472, 300], "temperature": 0.0, "avg_logprob": -0.08817366377948081, "compression_ratio": 1.5343915343915344, "no_speech_prob": 7.840268517611548e-05}, {"id": 278, "seek": 200932, "start": 2024.2, "end": 2034.28, "text": " differs. So to be able to build or to train neural networks efficiently, this is what happens all", "tokens": [37761, 13, 407, 281, 312, 1075, 281, 1322, 420, 281, 3847, 18161, 9590, 19621, 11, 341, 307, 437, 2314, 439], "temperature": 0.0, "avg_logprob": -0.08817366377948081, "compression_ratio": 1.5343915343915344, "no_speech_prob": 7.840268517611548e-05}, {"id": 279, "seek": 203428, "start": 2034.28, "end": 2041.96, "text": " the time, and it's absolutely essential that we use an algorithm that avoids repeated computation.", "tokens": [264, 565, 11, 293, 309, 311, 3122, 7115, 300, 321, 764, 364, 9284, 300, 3641, 3742, 10477, 24903, 13], "temperature": 0.0, "avg_logprob": -0.11107358540574165, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.398785313242115e-05}, {"id": 280, "seek": 203428, "start": 2042.68, "end": 2049.64, "text": " And so the idea we're going to develop is when we have this equation stack that there's sort of", "tokens": [400, 370, 264, 1558, 321, 434, 516, 281, 1499, 307, 562, 321, 362, 341, 5367, 8630, 300, 456, 311, 1333, 295], "temperature": 0.0, "avg_logprob": -0.11107358540574165, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.398785313242115e-05}, {"id": 281, "seek": 203428, "start": 2049.64, "end": 2057.32, "text": " stuff that's above where we compute z, and we're going to be sort of that'll be the same each time,", "tokens": [1507, 300, 311, 3673, 689, 321, 14722, 710, 11, 293, 321, 434, 516, 281, 312, 1333, 295, 300, 603, 312, 264, 912, 1184, 565, 11], "temperature": 0.0, "avg_logprob": -0.11107358540574165, "compression_ratio": 1.6153846153846154, "no_speech_prob": 4.398785313242115e-05}, {"id": 282, "seek": 205732, "start": 2057.32, "end": 2064.2000000000003, "text": " and we want to compute something from that that we can then sort of feed downwards when working", "tokens": [293, 321, 528, 281, 14722, 746, 490, 300, 300, 321, 393, 550, 1333, 295, 3154, 39880, 562, 1364], "temperature": 0.0, "avg_logprob": -0.09408843176705496, "compression_ratio": 1.5934065934065933, "no_speech_prob": 1.7769047190085985e-05}, {"id": 283, "seek": 205732, "start": 2064.2000000000003, "end": 2076.1200000000003, "text": " out the gradients with respect to w x or b. And so we do that by defining delta, which is delta is", "tokens": [484, 264, 2771, 2448, 365, 3104, 281, 261, 2031, 420, 272, 13, 400, 370, 321, 360, 300, 538, 17827, 8289, 11, 597, 307, 8289, 307], "temperature": 0.0, "avg_logprob": -0.09408843176705496, "compression_ratio": 1.5934065934065933, "no_speech_prob": 1.7769047190085985e-05}, {"id": 284, "seek": 205732, "start": 2076.1200000000003, "end": 2084.44, "text": " the partial's composed that are above the linear transform, and that's referred to as the local", "tokens": [264, 14641, 311, 18204, 300, 366, 3673, 264, 8213, 4088, 11, 293, 300, 311, 10839, 281, 382, 264, 2654], "temperature": 0.0, "avg_logprob": -0.09408843176705496, "compression_ratio": 1.5934065934065933, "no_speech_prob": 1.7769047190085985e-05}, {"id": 285, "seek": 208444, "start": 2084.44, "end": 2091.0, "text": " error signal. It's what's being passed in from above to the linear transform. And we've already", "tokens": [6713, 6358, 13, 467, 311, 437, 311, 885, 4678, 294, 490, 3673, 281, 264, 8213, 4088, 13, 400, 321, 600, 1217], "temperature": 0.0, "avg_logprob": -0.1122097306781345, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.9280274247867055e-05}, {"id": 286, "seek": 208444, "start": 2091.0, "end": 2100.04, "text": " computed the gradient of that in the preceding slides. And so the final form of the partial", "tokens": [40610, 264, 16235, 295, 300, 294, 264, 16969, 278, 9788, 13, 400, 370, 264, 2572, 1254, 295, 264, 14641], "temperature": 0.0, "avg_logprob": -0.1122097306781345, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.9280274247867055e-05}, {"id": 287, "seek": 208444, "start": 2100.04, "end": 2111.56, "text": " s with respect to b will be delta times the remaining part. And well, we'd seen that, you know,", "tokens": [262, 365, 3104, 281, 272, 486, 312, 8289, 1413, 264, 8877, 644, 13, 400, 731, 11, 321, 1116, 1612, 300, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1122097306781345, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.9280274247867055e-05}, {"id": 288, "seek": 211156, "start": 2111.56, "end": 2119.24, "text": " for partial s with respect to b, the partial z with respect to b is just the identity. So the end", "tokens": [337, 14641, 262, 365, 3104, 281, 272, 11, 264, 14641, 710, 365, 3104, 281, 272, 307, 445, 264, 6575, 13, 407, 264, 917], "temperature": 0.0, "avg_logprob": -0.10268314888602809, "compression_ratio": 1.7625, "no_speech_prob": 7.344470941461623e-05}, {"id": 289, "seek": 211156, "start": 2119.24, "end": 2125.56, "text": " result was delta. But in this time, we then go and have to work out the partial of z with respect to", "tokens": [1874, 390, 8289, 13, 583, 294, 341, 565, 11, 321, 550, 352, 293, 362, 281, 589, 484, 264, 14641, 295, 710, 365, 3104, 281], "temperature": 0.0, "avg_logprob": -0.10268314888602809, "compression_ratio": 1.7625, "no_speech_prob": 7.344470941461623e-05}, {"id": 290, "seek": 211156, "start": 2125.56, "end": 2133.16, "text": " w and multiply that by delta. So that's the part that we still haven't yet done. So", "tokens": [261, 293, 12972, 300, 538, 8289, 13, 407, 300, 311, 264, 644, 300, 321, 920, 2378, 380, 1939, 1096, 13, 407], "temperature": 0.0, "avg_logprob": -0.10268314888602809, "compression_ratio": 1.7625, "no_speech_prob": 7.344470941461623e-05}, {"id": 291, "seek": 213316, "start": 2133.16, "end": 2140.52, "text": " and this is where things get in some sense a little bit hairier.", "tokens": [293, 341, 307, 689, 721, 483, 294, 512, 2020, 257, 707, 857, 2578, 811, 13], "temperature": 0.0, "avg_logprob": -0.29373417344204217, "compression_ratio": 1.3008130081300813, "no_speech_prob": 5.200100349611603e-05}, {"id": 292, "seek": 213316, "start": 2142.68, "end": 2149.3999999999996, "text": " And so there's something that's important to explain. So, you know, what should we have for the", "tokens": [400, 370, 456, 311, 746, 300, 311, 1021, 281, 2903, 13, 407, 11, 291, 458, 11, 437, 820, 321, 362, 337, 264], "temperature": 0.0, "avg_logprob": -0.29373417344204217, "compression_ratio": 1.3008130081300813, "no_speech_prob": 5.200100349611603e-05}, {"id": 293, "seek": 214940, "start": 2149.4, "end": 2163.2400000000002, "text": " Jacobian of dsdw? Well, that's a function that has one output, the output is just a score of real", "tokens": [14117, 952, 295, 274, 82, 67, 86, 30, 1042, 11, 300, 311, 257, 2445, 300, 575, 472, 5598, 11, 264, 5598, 307, 445, 257, 6175, 295, 957], "temperature": 0.0, "avg_logprob": -0.20164806559934453, "compression_ratio": 1.3478260869565217, "no_speech_prob": 6.034993930370547e-06}, {"id": 294, "seek": 214940, "start": 2163.2400000000002, "end": 2176.6, "text": " number. And then it has n by m inputs. So the Jacobian is a 1 by m matrix. I a very long", "tokens": [1230, 13, 400, 550, 309, 575, 297, 538, 275, 15743, 13, 407, 264, 14117, 952, 307, 257, 502, 538, 275, 8141, 13, 286, 257, 588, 938], "temperature": 0.0, "avg_logprob": -0.20164806559934453, "compression_ratio": 1.3478260869565217, "no_speech_prob": 6.034993930370547e-06}, {"id": 295, "seek": 217660, "start": 2176.6, "end": 2185.24, "text": " row vector. But that's correct math. But it turns out that that's kind of bad for our neural", "tokens": [5386, 8062, 13, 583, 300, 311, 3006, 5221, 13, 583, 309, 4523, 484, 300, 300, 311, 733, 295, 1578, 337, 527, 18161], "temperature": 0.0, "avg_logprob": -0.10697338240487235, "compression_ratio": 1.6477272727272727, "no_speech_prob": 6.493205000879243e-05}, {"id": 296, "seek": 217660, "start": 2185.24, "end": 2191.3199999999997, "text": " networks. Because remember, what we want to do with our neural networks is do stochastic gradient", "tokens": [9590, 13, 1436, 1604, 11, 437, 321, 528, 281, 360, 365, 527, 18161, 9590, 307, 360, 342, 8997, 2750, 16235], "temperature": 0.0, "avg_logprob": -0.10697338240487235, "compression_ratio": 1.6477272727272727, "no_speech_prob": 6.493205000879243e-05}, {"id": 297, "seek": 217660, "start": 2191.3199999999997, "end": 2201.64, "text": " descent. And we want to say theta new equals theta old minus a small multiplier times the gradient.", "tokens": [23475, 13, 400, 321, 528, 281, 584, 9725, 777, 6915, 9725, 1331, 3175, 257, 1359, 44106, 1413, 264, 16235, 13], "temperature": 0.0, "avg_logprob": -0.10697338240487235, "compression_ratio": 1.6477272727272727, "no_speech_prob": 6.493205000879243e-05}, {"id": 298, "seek": 220164, "start": 2201.64, "end": 2216.04, "text": " And well, actually, the w matrix is an n by m matrix. And so we couldn't actually do the subtraction", "tokens": [400, 731, 11, 767, 11, 264, 261, 8141, 307, 364, 297, 538, 275, 8141, 13, 400, 370, 321, 2809, 380, 767, 360, 264, 16390, 313], "temperature": 0.0, "avg_logprob": -0.0970951005032188, "compression_ratio": 1.5434782608695652, "no_speech_prob": 2.24590166908456e-05}, {"id": 299, "seek": 220164, "start": 2216.04, "end": 2222.68, "text": " if this gradient we calculate is just a huge row vector. We'd like to have it as the same", "tokens": [498, 341, 16235, 321, 8873, 307, 445, 257, 2603, 5386, 8062, 13, 492, 1116, 411, 281, 362, 309, 382, 264, 912], "temperature": 0.0, "avg_logprob": -0.0970951005032188, "compression_ratio": 1.5434782608695652, "no_speech_prob": 2.24590166908456e-05}, {"id": 300, "seek": 220164, "start": 2222.68, "end": 2231.48, "text": " shape as the w matrix. In neural network land, when we do this, we depart from pure math that", "tokens": [3909, 382, 264, 261, 8141, 13, 682, 18161, 3209, 2117, 11, 562, 321, 360, 341, 11, 321, 9110, 490, 6075, 5221, 300], "temperature": 0.0, "avg_logprob": -0.0970951005032188, "compression_ratio": 1.5434782608695652, "no_speech_prob": 2.24590166908456e-05}, {"id": 301, "seek": 223148, "start": 2231.48, "end": 2237.88, "text": " this point. And we use what we call the shape convention. So what we're going to say is,", "tokens": [341, 935, 13, 400, 321, 764, 437, 321, 818, 264, 3909, 10286, 13, 407, 437, 321, 434, 516, 281, 584, 307, 11], "temperature": 0.0, "avg_logprob": -0.11864277720451355, "compression_ratio": 1.7574257425742574, "no_speech_prob": 5.296603558235802e-05}, {"id": 302, "seek": 223148, "start": 2239.0, "end": 2244.12, "text": " and you're meant to use this for answers in the assignment, that the shape of the gradient", "tokens": [293, 291, 434, 4140, 281, 764, 341, 337, 6338, 294, 264, 15187, 11, 300, 264, 3909, 295, 264, 16235], "temperature": 0.0, "avg_logprob": -0.11864277720451355, "compression_ratio": 1.7574257425742574, "no_speech_prob": 5.296603558235802e-05}, {"id": 303, "seek": 223148, "start": 2244.12, "end": 2249.72, "text": " we're always going to make to be the shape of the parameters. And so therefore,", "tokens": [321, 434, 1009, 516, 281, 652, 281, 312, 264, 3909, 295, 264, 9834, 13, 400, 370, 4412, 11], "temperature": 0.0, "avg_logprob": -0.11864277720451355, "compression_ratio": 1.7574257425742574, "no_speech_prob": 5.296603558235802e-05}, {"id": 304, "seek": 224972, "start": 2249.72, "end": 2260.7599999999998, "text": " the st w we're also going to represent as an n by m matrix just like w. And we're going to reshape", "tokens": [264, 342, 261, 321, 434, 611, 516, 281, 2906, 382, 364, 297, 538, 275, 8141, 445, 411, 261, 13, 400, 321, 434, 516, 281, 725, 42406], "temperature": 0.0, "avg_logprob": -0.19139461857931955, "compression_ratio": 1.5193798449612403, "no_speech_prob": 8.881172107066959e-05}, {"id": 305, "seek": 224972, "start": 2261.3999999999996, "end": 2270.52, "text": " the Jacobian, deplace it into this matrix shape. Okay, so if we want to place it into this matrix", "tokens": [264, 14117, 952, 11, 368, 6742, 309, 666, 341, 8141, 3909, 13, 1033, 11, 370, 498, 321, 528, 281, 1081, 309, 666, 341, 8141], "temperature": 0.0, "avg_logprob": -0.19139461857931955, "compression_ratio": 1.5193798449612403, "no_speech_prob": 8.881172107066959e-05}, {"id": 306, "seek": 227052, "start": 2270.52, "end": 2282.84, "text": " shape, what do we, what are we going to want to get for the st w? Well, we know that it's", "tokens": [3909, 11, 437, 360, 321, 11, 437, 366, 321, 516, 281, 528, 281, 483, 337, 264, 342, 261, 30, 1042, 11, 321, 458, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.16167528812701887, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.00011135917156934738}, {"id": 307, "seek": 227052, "start": 2283.88, "end": 2296.92, "text": " going to involve delta our local error signal. And then we have to work out something for d z", "tokens": [516, 281, 9494, 8289, 527, 2654, 6713, 6358, 13, 400, 550, 321, 362, 281, 589, 484, 746, 337, 274, 710], "temperature": 0.0, "avg_logprob": -0.16167528812701887, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.00011135917156934738}, {"id": 308, "seek": 229692, "start": 2296.92, "end": 2307.2400000000002, "text": " w. Well, since c equals w x plus b, you'd kind of expect that the answer should be x.", "tokens": [261, 13, 1042, 11, 1670, 269, 6915, 261, 2031, 1804, 272, 11, 291, 1116, 733, 295, 2066, 300, 264, 1867, 820, 312, 2031, 13], "temperature": 0.0, "avg_logprob": -0.10397566477457683, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.0001353612751699984}, {"id": 309, "seek": 229692, "start": 2308.6800000000003, "end": 2320.2000000000003, "text": " And that's right. So the answer to d s d w is going to be delta transpose times x transpose.", "tokens": [400, 300, 311, 558, 13, 407, 264, 1867, 281, 274, 262, 274, 261, 307, 516, 281, 312, 8289, 25167, 1413, 2031, 25167, 13], "temperature": 0.0, "avg_logprob": -0.10397566477457683, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.0001353612751699984}, {"id": 310, "seek": 229692, "start": 2320.2000000000003, "end": 2326.52, "text": " And so the form that we're getting for this derivative is going to be the product of the local", "tokens": [400, 370, 264, 1254, 300, 321, 434, 1242, 337, 341, 13760, 307, 516, 281, 312, 264, 1674, 295, 264, 2654], "temperature": 0.0, "avg_logprob": -0.10397566477457683, "compression_ratio": 1.5689655172413792, "no_speech_prob": 0.0001353612751699984}, {"id": 311, "seek": 232652, "start": 2326.52, "end": 2336.7599999999998, "text": " error signal that comes from above versus what we calculate from the local input x.", "tokens": [6713, 6358, 300, 1487, 490, 3673, 5717, 437, 321, 8873, 490, 264, 2654, 4846, 2031, 13], "temperature": 0.0, "avg_logprob": -0.13242456075307485, "compression_ratio": 1.5310734463276836, "no_speech_prob": 4.751778033096343e-05}, {"id": 312, "seek": 232652, "start": 2338.2, "end": 2344.52, "text": " So that shouldn't yet be obvious why that is true. So let me just go through in a bit more detail", "tokens": [407, 300, 4659, 380, 1939, 312, 6322, 983, 300, 307, 2074, 13, 407, 718, 385, 445, 352, 807, 294, 257, 857, 544, 2607], "temperature": 0.0, "avg_logprob": -0.13242456075307485, "compression_ratio": 1.5310734463276836, "no_speech_prob": 4.751778033096343e-05}, {"id": 313, "seek": 232652, "start": 2344.52, "end": 2354.28, "text": " why that's true. So when we want to work out d s d w, right, it's sort of delta times d z", "tokens": [983, 300, 311, 2074, 13, 407, 562, 321, 528, 281, 589, 484, 274, 262, 274, 261, 11, 558, 11, 309, 311, 1333, 295, 8289, 1413, 274, 710], "temperature": 0.0, "avg_logprob": -0.13242456075307485, "compression_ratio": 1.5310734463276836, "no_speech_prob": 4.751778033096343e-05}, {"id": 314, "seek": 235428, "start": 2354.28, "end": 2363.6400000000003, "text": " w, where what that's computing for z is w x plus b. So let's just consider for a moment what the", "tokens": [261, 11, 689, 437, 300, 311, 15866, 337, 710, 307, 261, 2031, 1804, 272, 13, 407, 718, 311, 445, 1949, 337, 257, 1623, 437, 264], "temperature": 0.0, "avg_logprob": -0.13100418043725284, "compression_ratio": 1.5638297872340425, "no_speech_prob": 0.00016318494454026222}, {"id": 315, "seek": 235428, "start": 2363.6400000000003, "end": 2374.1200000000003, "text": " derivative is with respect to a single weight w ij. So w ij might be w two three that's shown in", "tokens": [13760, 307, 365, 3104, 281, 257, 2167, 3364, 261, 741, 73, 13, 407, 261, 741, 73, 1062, 312, 261, 732, 1045, 300, 311, 4898, 294], "temperature": 0.0, "avg_logprob": -0.13100418043725284, "compression_ratio": 1.5638297872340425, "no_speech_prob": 0.00016318494454026222}, {"id": 316, "seek": 237412, "start": 2374.12, "end": 2384.52, "text": " my little neural network here. And so the first thing to notice is the w ij only contributes to z i.", "tokens": [452, 707, 18161, 3209, 510, 13, 400, 370, 264, 700, 551, 281, 3449, 307, 264, 261, 741, 73, 787, 32035, 281, 710, 741, 13], "temperature": 0.0, "avg_logprob": -0.09085837277499112, "compression_ratio": 1.3928571428571428, "no_speech_prob": 7.0682472141925246e-06}, {"id": 317, "seek": 237412, "start": 2384.52, "end": 2395.72, "text": " So it's going into z two, which then computes h two. And it has no effect whatsoever on h one.", "tokens": [407, 309, 311, 516, 666, 710, 732, 11, 597, 550, 715, 1819, 276, 732, 13, 400, 309, 575, 572, 1802, 17076, 322, 276, 472, 13], "temperature": 0.0, "avg_logprob": -0.09085837277499112, "compression_ratio": 1.3928571428571428, "no_speech_prob": 7.0682472141925246e-06}, {"id": 318, "seek": 239572, "start": 2395.72, "end": 2409.8799999999997, "text": " Okay, so when we're working out d z i d w ij, it's going to be d w i x that sort of row that", "tokens": [1033, 11, 370, 562, 321, 434, 1364, 484, 274, 710, 741, 274, 261, 741, 73, 11, 309, 311, 516, 281, 312, 274, 261, 741, 2031, 300, 1333, 295, 5386, 300], "temperature": 0.0, "avg_logprob": -0.19810381531715393, "compression_ratio": 1.4148148148148147, "no_speech_prob": 7.600244862260297e-05}, {"id": 319, "seek": 239572, "start": 2409.8799999999997, "end": 2420.6, "text": " that row of the matrix plus bi, which means that for we've got a kind of a sum of w i k times x k.", "tokens": [300, 5386, 295, 264, 8141, 1804, 3228, 11, 597, 1355, 300, 337, 321, 600, 658, 257, 733, 295, 257, 2408, 295, 261, 741, 350, 1413, 2031, 350, 13], "temperature": 0.0, "avg_logprob": -0.19810381531715393, "compression_ratio": 1.4148148148148147, "no_speech_prob": 7.600244862260297e-05}, {"id": 320, "seek": 242060, "start": 2420.6, "end": 2427.96, "text": " And then for this sum, this is like one variable calculus that when we're taking the derivative of", "tokens": [400, 550, 337, 341, 2408, 11, 341, 307, 411, 472, 7006, 33400, 300, 562, 321, 434, 1940, 264, 13760, 295], "temperature": 0.0, "avg_logprob": -0.10397122065226237, "compression_ratio": 1.748502994011976, "no_speech_prob": 0.00010713140363804996}, {"id": 321, "seek": 242060, "start": 2427.96, "end": 2436.2, "text": " this with respect to w ij, every term in this sum is going to be zero. The derivative is going to", "tokens": [341, 365, 3104, 281, 261, 741, 73, 11, 633, 1433, 294, 341, 2408, 307, 516, 281, 312, 4018, 13, 440, 13760, 307, 516, 281], "temperature": 0.0, "avg_logprob": -0.10397122065226237, "compression_ratio": 1.748502994011976, "no_speech_prob": 0.00010713140363804996}, {"id": 322, "seek": 242060, "start": 2436.2, "end": 2444.52, "text": " be zero except for the one that involves w ij. And then the derivative of that is just like a x", "tokens": [312, 4018, 3993, 337, 264, 472, 300, 11626, 261, 741, 73, 13, 400, 550, 264, 13760, 295, 300, 307, 445, 411, 257, 2031], "temperature": 0.0, "avg_logprob": -0.10397122065226237, "compression_ratio": 1.748502994011976, "no_speech_prob": 0.00010713140363804996}, {"id": 323, "seek": 244452, "start": 2444.52, "end": 2453.48, "text": " with respect to a, it's going to be x. So you get x j out as the answer. And so the end result of", "tokens": [365, 3104, 281, 257, 11, 309, 311, 516, 281, 312, 2031, 13, 407, 291, 483, 2031, 361, 484, 382, 264, 1867, 13, 400, 370, 264, 917, 1874, 295], "temperature": 0.0, "avg_logprob": -0.10421586878159467, "compression_ratio": 1.7771084337349397, "no_speech_prob": 3.760867184610106e-05}, {"id": 324, "seek": 244452, "start": 2453.48, "end": 2463.24, "text": " that is that when we're working out, what we want as the answer is that we're going to get that these", "tokens": [300, 307, 300, 562, 321, 434, 1364, 484, 11, 437, 321, 528, 382, 264, 1867, 307, 300, 321, 434, 516, 281, 483, 300, 613], "temperature": 0.0, "avg_logprob": -0.10421586878159467, "compression_ratio": 1.7771084337349397, "no_speech_prob": 3.760867184610106e-05}, {"id": 325, "seek": 244452, "start": 2464.52, "end": 2472.68, "text": " columns where x one is all that's left x two is all that's left through x m is all that's left.", "tokens": [13766, 689, 2031, 472, 307, 439, 300, 311, 1411, 2031, 732, 307, 439, 300, 311, 1411, 807, 2031, 275, 307, 439, 300, 311, 1411, 13], "temperature": 0.0, "avg_logprob": -0.10421586878159467, "compression_ratio": 1.7771084337349397, "no_speech_prob": 3.760867184610106e-05}, {"id": 326, "seek": 247268, "start": 2472.68, "end": 2480.44, "text": " And then that's multiplied by the vectors of the local error signal from above. And what we want", "tokens": [400, 550, 300, 311, 17207, 538, 264, 18875, 295, 264, 2654, 6713, 6358, 490, 3673, 13, 400, 437, 321, 528], "temperature": 0.0, "avg_logprob": -0.08708546818166539, "compression_ratio": 1.548913043478261, "no_speech_prob": 5.82424363528844e-05}, {"id": 327, "seek": 247268, "start": 2480.44, "end": 2487.72, "text": " to compute is this outer product matrix, we're getting the different combinations of the delta", "tokens": [281, 14722, 307, 341, 10847, 1674, 8141, 11, 321, 434, 1242, 264, 819, 21267, 295, 264, 8289], "temperature": 0.0, "avg_logprob": -0.08708546818166539, "compression_ratio": 1.548913043478261, "no_speech_prob": 5.82424363528844e-05}, {"id": 328, "seek": 247268, "start": 2488.2799999999997, "end": 2496.8399999999997, "text": " and the x. And so we can get the n by m matrix that we'd like to have by our shape convention", "tokens": [293, 264, 2031, 13, 400, 370, 321, 393, 483, 264, 297, 538, 275, 8141, 300, 321, 1116, 411, 281, 362, 538, 527, 3909, 10286], "temperature": 0.0, "avg_logprob": -0.08708546818166539, "compression_ratio": 1.548913043478261, "no_speech_prob": 5.82424363528844e-05}, {"id": 329, "seek": 249684, "start": 2496.84, "end": 2504.84, "text": " by taking delta transpose, which is n by one times x transpose, which is n one by m. And then we", "tokens": [538, 1940, 8289, 25167, 11, 597, 307, 297, 538, 472, 1413, 2031, 25167, 11, 597, 307, 297, 472, 538, 275, 13, 400, 550, 321], "temperature": 0.0, "avg_logprob": -0.12807895699325872, "compression_ratio": 1.6401673640167365, "no_speech_prob": 2.1774330889456905e-05}, {"id": 330, "seek": 249684, "start": 2504.84, "end": 2512.6800000000003, "text": " get this outer product matrix. So like that's a kind of a hacky argument that I've made. It's", "tokens": [483, 341, 10847, 1674, 8141, 13, 407, 411, 300, 311, 257, 733, 295, 257, 10339, 88, 6770, 300, 286, 600, 1027, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.12807895699325872, "compression_ratio": 1.6401673640167365, "no_speech_prob": 2.1774330889456905e-05}, {"id": 331, "seek": 249684, "start": 2512.6800000000003, "end": 2519.6400000000003, "text": " certainly a way of doing things that the dimensions work out and it sort of makes sense. There's a more", "tokens": [3297, 257, 636, 295, 884, 721, 300, 264, 12819, 589, 484, 293, 309, 1333, 295, 1669, 2020, 13, 821, 311, 257, 544], "temperature": 0.0, "avg_logprob": -0.12807895699325872, "compression_ratio": 1.6401673640167365, "no_speech_prob": 2.1774330889456905e-05}, {"id": 332, "seek": 251964, "start": 2519.64, "end": 2526.8399999999997, "text": " detailed run through this that appears in election notes. And I encourage you to sort of also look", "tokens": [9942, 1190, 807, 341, 300, 7038, 294, 6618, 5570, 13, 400, 286, 5373, 291, 281, 1333, 295, 611, 574], "temperature": 0.0, "avg_logprob": -0.15642173048378766, "compression_ratio": 1.4818652849740932, "no_speech_prob": 1.7746271623764187e-05}, {"id": 333, "seek": 251964, "start": 2526.8399999999997, "end": 2534.6, "text": " at the more Matthew version of that. Here's a little bit more information about the shape convention.", "tokens": [412, 264, 544, 12434, 3037, 295, 300, 13, 1692, 311, 257, 707, 857, 544, 1589, 466, 264, 3909, 10286, 13], "temperature": 0.0, "avg_logprob": -0.15642173048378766, "compression_ratio": 1.4818652849740932, "no_speech_prob": 1.7746271623764187e-05}, {"id": 334, "seek": 251964, "start": 2534.6, "end": 2546.52, "text": " So well, first of all, one more example of this. So when you're working out DSDB that", "tokens": [407, 731, 11, 700, 295, 439, 11, 472, 544, 1365, 295, 341, 13, 407, 562, 291, 434, 1364, 484, 15816, 27735, 300], "temperature": 0.0, "avg_logprob": -0.15642173048378766, "compression_ratio": 1.4818652849740932, "no_speech_prob": 1.7746271623764187e-05}, {"id": 335, "seek": 254652, "start": 2546.52, "end": 2557.88, "text": " that comes out as it's Jacobian is a row vector. But similarly, you know, according to shape", "tokens": [300, 1487, 484, 382, 309, 311, 14117, 952, 307, 257, 5386, 8062, 13, 583, 14138, 11, 291, 458, 11, 4650, 281, 3909], "temperature": 0.0, "avg_logprob": -0.15557709493135152, "compression_ratio": 1.5128205128205128, "no_speech_prob": 5.825913103763014e-05}, {"id": 336, "seek": 254652, "start": 2557.88, "end": 2566.84, "text": " convention, we want our gradient to be the same shape as B and B as a column vector. So that's sort", "tokens": [10286, 11, 321, 528, 527, 16235, 281, 312, 264, 912, 3909, 382, 363, 293, 363, 382, 257, 7738, 8062, 13, 407, 300, 311, 1333], "temperature": 0.0, "avg_logprob": -0.15557709493135152, "compression_ratio": 1.5128205128205128, "no_speech_prob": 5.825913103763014e-05}, {"id": 337, "seek": 254652, "start": 2566.84, "end": 2573.56, "text": " of again, they're different shapes and you have to transpose one to get the other. And so effectively,", "tokens": [295, 797, 11, 436, 434, 819, 10854, 293, 291, 362, 281, 25167, 472, 281, 483, 264, 661, 13, 400, 370, 8659, 11], "temperature": 0.0, "avg_logprob": -0.15557709493135152, "compression_ratio": 1.5128205128205128, "no_speech_prob": 5.825913103763014e-05}, {"id": 338, "seek": 257356, "start": 2573.56, "end": 2580.36, "text": " what we have is a disagreement between the Jacobian form. So the Jacobian form makes sense for", "tokens": [437, 321, 362, 307, 257, 38947, 1296, 264, 14117, 952, 1254, 13, 407, 264, 14117, 952, 1254, 1669, 2020, 337], "temperature": 0.0, "avg_logprob": -0.08616525476629083, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.8920203729066998e-05}, {"id": 339, "seek": 257356, "start": 2581.0, "end": 2588.2799999999997, "text": " you know, calculus and math. Because if you want to have it like I claimed that matrix calculus", "tokens": [291, 458, 11, 33400, 293, 5221, 13, 1436, 498, 291, 528, 281, 362, 309, 411, 286, 12941, 300, 8141, 33400], "temperature": 0.0, "avg_logprob": -0.08616525476629083, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.8920203729066998e-05}, {"id": 340, "seek": 257356, "start": 2588.2799999999997, "end": 2594.2, "text": " is just like single variable calculus apart from using vectors and matrices, you can just multiply", "tokens": [307, 445, 411, 2167, 7006, 33400, 4936, 490, 1228, 18875, 293, 32284, 11, 291, 393, 445, 12972], "temperature": 0.0, "avg_logprob": -0.08616525476629083, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.8920203729066998e-05}, {"id": 341, "seek": 257356, "start": 2594.2, "end": 2601.0, "text": " together the particles. That only works out if you're using Jacobians. But on the other hand,", "tokens": [1214, 264, 10007, 13, 663, 787, 1985, 484, 498, 291, 434, 1228, 14117, 2567, 13, 583, 322, 264, 661, 1011, 11], "temperature": 0.0, "avg_logprob": -0.08616525476629083, "compression_ratio": 1.6872246696035242, "no_speech_prob": 1.8920203729066998e-05}, {"id": 342, "seek": 260100, "start": 2601.0, "end": 2609.8, "text": " if you want to do stochastic gradient descent and be able to sort of subtract off a piece of the", "tokens": [498, 291, 528, 281, 360, 342, 8997, 2750, 16235, 23475, 293, 312, 1075, 281, 1333, 295, 16390, 766, 257, 2522, 295, 264], "temperature": 0.0, "avg_logprob": -0.09831627101114351, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.3842112821293995e-05}, {"id": 343, "seek": 260100, "start": 2609.8, "end": 2618.84, "text": " gradient, that only works if you have the same shape matrix for the gradient as you do for the", "tokens": [16235, 11, 300, 787, 1985, 498, 291, 362, 264, 912, 3909, 8141, 337, 264, 16235, 382, 291, 360, 337, 264], "temperature": 0.0, "avg_logprob": -0.09831627101114351, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.3842112821293995e-05}, {"id": 344, "seek": 260100, "start": 2618.84, "end": 2626.28, "text": " original matrix. And so this is a bit confusing, but that's just the reality. There are both of these", "tokens": [3380, 8141, 13, 400, 370, 341, 307, 257, 857, 13181, 11, 457, 300, 311, 445, 264, 4103, 13, 821, 366, 1293, 295, 613], "temperature": 0.0, "avg_logprob": -0.09831627101114351, "compression_ratio": 1.5837837837837838, "no_speech_prob": 1.3842112821293995e-05}, {"id": 345, "seek": 262628, "start": 2626.28, "end": 2637.1600000000003, "text": " two things. So the Jacobian form is useful in doing the calculus. But for the answers in the", "tokens": [732, 721, 13, 407, 264, 14117, 952, 1254, 307, 4420, 294, 884, 264, 33400, 13, 583, 337, 264, 6338, 294, 264], "temperature": 0.0, "avg_logprob": -0.0908189024244036, "compression_ratio": 1.6235955056179776, "no_speech_prob": 4.675128366216086e-05}, {"id": 346, "seek": 262628, "start": 2637.1600000000003, "end": 2646.6800000000003, "text": " assignment, we want the answers to be presented using the shape convention so that the gradient is", "tokens": [15187, 11, 321, 528, 264, 6338, 281, 312, 8212, 1228, 264, 3909, 10286, 370, 300, 264, 16235, 307], "temperature": 0.0, "avg_logprob": -0.0908189024244036, "compression_ratio": 1.6235955056179776, "no_speech_prob": 4.675128366216086e-05}, {"id": 347, "seek": 262628, "start": 2646.6800000000003, "end": 2654.1200000000003, "text": " shown in the same shape as the parameters. And therefore, you'll be able to, it's the right shape", "tokens": [4898, 294, 264, 912, 3909, 382, 264, 9834, 13, 400, 4412, 11, 291, 603, 312, 1075, 281, 11, 309, 311, 264, 558, 3909], "temperature": 0.0, "avg_logprob": -0.0908189024244036, "compression_ratio": 1.6235955056179776, "no_speech_prob": 4.675128366216086e-05}, {"id": 348, "seek": 265412, "start": 2654.12, "end": 2663.0, "text": " for doing a gradient update by just subtracting a small amount of the gradient. So for working", "tokens": [337, 884, 257, 16235, 5623, 538, 445, 16390, 278, 257, 1359, 2372, 295, 264, 16235, 13, 407, 337, 1364], "temperature": 0.0, "avg_logprob": -0.08628849616417518, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.061333023244515e-05}, {"id": 349, "seek": 265412, "start": 2663.0, "end": 2672.2799999999997, "text": " through things, there are then basically two choices. One choice is to work through all the math", "tokens": [807, 721, 11, 456, 366, 550, 1936, 732, 7994, 13, 1485, 3922, 307, 281, 589, 807, 439, 264, 5221], "temperature": 0.0, "avg_logprob": -0.08628849616417518, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.061333023244515e-05}, {"id": 350, "seek": 265412, "start": 2672.2799999999997, "end": 2679.96, "text": " using Jacobians and then write at the end to reshape following the shape convention to give the", "tokens": [1228, 14117, 2567, 293, 550, 2464, 412, 264, 917, 281, 725, 42406, 3480, 264, 3909, 10286, 281, 976, 264], "temperature": 0.0, "avg_logprob": -0.08628849616417518, "compression_ratio": 1.5856353591160222, "no_speech_prob": 1.061333023244515e-05}, {"id": 351, "seek": 267996, "start": 2679.96, "end": 2690.92, "text": " answer. So that's what I did when I worked out DSDB. We worked through it using Jacobians. We", "tokens": [1867, 13, 407, 300, 311, 437, 286, 630, 562, 286, 2732, 484, 15816, 27735, 13, 492, 2732, 807, 309, 1228, 14117, 2567, 13, 492], "temperature": 0.0, "avg_logprob": -0.07974826515495002, "compression_ratio": 1.4720812182741116, "no_speech_prob": 5.5506392527604476e-05}, {"id": 352, "seek": 267996, "start": 2690.92, "end": 2697.4, "text": " got an answer, but it turned out to be a row vector. And so, well, then we have to transpose it at", "tokens": [658, 364, 1867, 11, 457, 309, 3574, 484, 281, 312, 257, 5386, 8062, 13, 400, 370, 11, 731, 11, 550, 321, 362, 281, 25167, 309, 412], "temperature": 0.0, "avg_logprob": -0.07974826515495002, "compression_ratio": 1.4720812182741116, "no_speech_prob": 5.5506392527604476e-05}, {"id": 353, "seek": 267996, "start": 2697.4, "end": 2708.2, "text": " the end to get into the right shape for the shape convention. The alternative is to always follow", "tokens": [264, 917, 281, 483, 666, 264, 558, 3909, 337, 264, 3909, 10286, 13, 440, 8535, 307, 281, 1009, 1524], "temperature": 0.0, "avg_logprob": -0.07974826515495002, "compression_ratio": 1.4720812182741116, "no_speech_prob": 5.5506392527604476e-05}, {"id": 354, "seek": 270820, "start": 2708.2, "end": 2716.52, "text": " the shape convention. And that's kind of what I did when I was then working out DSDW. I didn't", "tokens": [264, 3909, 10286, 13, 400, 300, 311, 733, 295, 437, 286, 630, 562, 286, 390, 550, 1364, 484, 15816, 35, 54, 13, 286, 994, 380], "temperature": 0.0, "avg_logprob": -0.11857405575838956, "compression_ratio": 1.5208333333333333, "no_speech_prob": 3.813764487858862e-05}, {"id": 355, "seek": 270820, "start": 2716.52, "end": 2726.8399999999997, "text": " faultfully use Jacobians. I said, oh, well, when we work out, whatever was DZDW, let's work out what", "tokens": [7441, 2277, 764, 14117, 2567, 13, 286, 848, 11, 1954, 11, 731, 11, 562, 321, 589, 484, 11, 2035, 390, 413, 57, 35, 54, 11, 718, 311, 589, 484, 437], "temperature": 0.0, "avg_logprob": -0.11857405575838956, "compression_ratio": 1.5208333333333333, "no_speech_prob": 3.813764487858862e-05}, {"id": 356, "seek": 270820, "start": 2726.8399999999997, "end": 2734.52, "text": " shape we want it to be and what to fill in the cells with. And if you're sort of trying to do it", "tokens": [3909, 321, 528, 309, 281, 312, 293, 437, 281, 2836, 294, 264, 5438, 365, 13, 400, 498, 291, 434, 1333, 295, 1382, 281, 360, 309], "temperature": 0.0, "avg_logprob": -0.11857405575838956, "compression_ratio": 1.5208333333333333, "no_speech_prob": 3.813764487858862e-05}, {"id": 357, "seek": 273452, "start": 2734.52, "end": 2743.0, "text": " immediately with the shape convention, it's a little bit more hacky in a way since you know,", "tokens": [4258, 365, 264, 3909, 10286, 11, 309, 311, 257, 707, 857, 544, 10339, 88, 294, 257, 636, 1670, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1322566032409668, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00022497255122289062}, {"id": 358, "seek": 273452, "start": 2743.0, "end": 2748.52, "text": " you have to look at the dimensions for what you want and figure out when to transpose or to reshape", "tokens": [291, 362, 281, 574, 412, 264, 12819, 337, 437, 291, 528, 293, 2573, 484, 562, 281, 25167, 420, 281, 725, 42406], "temperature": 0.0, "avg_logprob": -0.1322566032409668, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00022497255122289062}, {"id": 359, "seek": 273452, "start": 2748.52, "end": 2756.6, "text": " the matrix to be it the right shape. But the kind of informal reasoning that I gave is what you do", "tokens": [264, 8141, 281, 312, 309, 264, 558, 3909, 13, 583, 264, 733, 295, 24342, 21577, 300, 286, 2729, 307, 437, 291, 360], "temperature": 0.0, "avg_logprob": -0.1322566032409668, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00022497255122289062}, {"id": 360, "seek": 273452, "start": 2756.6, "end": 2763.24, "text": " and what works. And you know, one way of, and there are sort of hints that you can use, right? That", "tokens": [293, 437, 1985, 13, 400, 291, 458, 11, 472, 636, 295, 11, 293, 456, 366, 1333, 295, 27271, 300, 291, 393, 764, 11, 558, 30, 663], "temperature": 0.0, "avg_logprob": -0.1322566032409668, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.00022497255122289062}, {"id": 361, "seek": 276324, "start": 2763.24, "end": 2769.7999999999997, "text": " you know that your gradient should always be the same shape as your parameters. And you know that", "tokens": [291, 458, 300, 428, 16235, 820, 1009, 312, 264, 912, 3909, 382, 428, 9834, 13, 400, 291, 458, 300], "temperature": 0.0, "avg_logprob": -0.10763319099650663, "compression_ratio": 1.6187845303867403, "no_speech_prob": 2.972978472826071e-05}, {"id": 362, "seek": 276324, "start": 2769.7999999999997, "end": 2776.7599999999998, "text": " the error message coming in will always have the same dimensionality as that hidden layer. And", "tokens": [264, 6713, 3636, 1348, 294, 486, 1009, 362, 264, 912, 10139, 1860, 382, 300, 7633, 4583, 13, 400], "temperature": 0.0, "avg_logprob": -0.10763319099650663, "compression_ratio": 1.6187845303867403, "no_speech_prob": 2.972978472826071e-05}, {"id": 363, "seek": 276324, "start": 2776.7599999999998, "end": 2790.52, "text": " you can sort of work it out always following the shape convention. Okay. So that is, hey, doing this", "tokens": [291, 393, 1333, 295, 589, 309, 484, 1009, 3480, 264, 3909, 10286, 13, 1033, 13, 407, 300, 307, 11, 4177, 11, 884, 341], "temperature": 0.0, "avg_logprob": -0.10763319099650663, "compression_ratio": 1.6187845303867403, "no_speech_prob": 2.972978472826071e-05}, {"id": 364, "seek": 279052, "start": 2790.52, "end": 2803.32, "text": " is all matrix calculus. So after pausing for breath for a second, the rest of the lecture is then,", "tokens": [307, 439, 8141, 33400, 13, 407, 934, 2502, 7981, 337, 6045, 337, 257, 1150, 11, 264, 1472, 295, 264, 7991, 307, 550, 11], "temperature": 0.0, "avg_logprob": -0.14423927968862105, "compression_ratio": 1.4130434782608696, "no_speech_prob": 3.0627474188804626e-05}, {"id": 365, "seek": 279052, "start": 2803.64, "end": 2813.0, "text": " okay, let's look at how our software trains neural networks using what's referred to as the back", "tokens": [1392, 11, 718, 311, 574, 412, 577, 527, 4722, 16329, 18161, 9590, 1228, 437, 311, 10839, 281, 382, 264, 646], "temperature": 0.0, "avg_logprob": -0.14423927968862105, "compression_ratio": 1.4130434782608696, "no_speech_prob": 3.0627474188804626e-05}, {"id": 366, "seek": 281300, "start": 2813.0, "end": 2831.08, "text": " propagation algorithm. So the short answer is, you know, basically we've already done it,", "tokens": [38377, 9284, 13, 407, 264, 2099, 1867, 307, 11, 291, 458, 11, 1936, 321, 600, 1217, 1096, 309, 11], "temperature": 0.0, "avg_logprob": -0.10768668547920558, "compression_ratio": 1.372093023255814, "no_speech_prob": 2.4242955987574533e-05}, {"id": 367, "seek": 281300, "start": 2831.08, "end": 2837.72, "text": " the rest of the lecture is easy. So, you know, essentially I've just shown you what the", "tokens": [264, 1472, 295, 264, 7991, 307, 1858, 13, 407, 11, 291, 458, 11, 4476, 286, 600, 445, 4898, 291, 437, 264], "temperature": 0.0, "avg_logprob": -0.10768668547920558, "compression_ratio": 1.372093023255814, "no_speech_prob": 2.4242955987574533e-05}, {"id": 368, "seek": 283772, "start": 2837.72, "end": 2848.3599999999997, "text": " back propagation algorithm does. So the back propagation algorithm is judiciously taking and", "tokens": [646, 38377, 9284, 775, 13, 407, 264, 646, 38377, 9284, 307, 3747, 3784, 356, 1940, 293], "temperature": 0.0, "avg_logprob": -0.08978354930877686, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.7196391127072275e-05}, {"id": 369, "seek": 283772, "start": 2850.8399999999997, "end": 2861.48, "text": " propagating derivatives using the matrix chain rule. The rest of the back propagation algorithm", "tokens": [12425, 990, 33733, 1228, 264, 8141, 5021, 4978, 13, 440, 1472, 295, 264, 646, 38377, 9284], "temperature": 0.0, "avg_logprob": -0.08978354930877686, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.7196391127072275e-05}, {"id": 370, "seek": 286148, "start": 2861.48, "end": 2870.76, "text": " is to say, okay, when we have these neural networks, we have a lot of shared structure and shared", "tokens": [307, 281, 584, 11, 1392, 11, 562, 321, 362, 613, 18161, 9590, 11, 321, 362, 257, 688, 295, 5507, 3877, 293, 5507], "temperature": 0.0, "avg_logprob": -0.06873592963585487, "compression_ratio": 1.5934065934065933, "no_speech_prob": 5.7303201174363494e-05}, {"id": 371, "seek": 286148, "start": 2870.76, "end": 2881.72, "text": " derivatives. So what we want to do is maximally, efficiently reuse derivatives of higher layers", "tokens": [33733, 13, 407, 437, 321, 528, 281, 360, 307, 5138, 379, 11, 19621, 26225, 33733, 295, 2946, 7914], "temperature": 0.0, "avg_logprob": -0.06873592963585487, "compression_ratio": 1.5934065934065933, "no_speech_prob": 5.7303201174363494e-05}, {"id": 372, "seek": 286148, "start": 2881.72, "end": 2888.36, "text": " when we're computing derivatives for lower layers so that we minimize computation. And I already", "tokens": [562, 321, 434, 15866, 33733, 337, 3126, 7914, 370, 300, 321, 17522, 24903, 13, 400, 286, 1217], "temperature": 0.0, "avg_logprob": -0.06873592963585487, "compression_ratio": 1.5934065934065933, "no_speech_prob": 5.7303201174363494e-05}, {"id": 373, "seek": 288836, "start": 2888.36, "end": 2896.1200000000003, "text": " pointed that out in the first half, but we want to systematically exploit that. And so the way we do", "tokens": [10932, 300, 484, 294, 264, 700, 1922, 11, 457, 321, 528, 281, 39531, 25924, 300, 13, 400, 370, 264, 636, 321, 360], "temperature": 0.0, "avg_logprob": -0.1046512590514289, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.00019094764138571918}, {"id": 374, "seek": 288836, "start": 2896.1200000000003, "end": 2906.36, "text": " that in our computational systems is they construct computation graphs. So this maybe looks a little", "tokens": [300, 294, 527, 28270, 3652, 307, 436, 7690, 24903, 24877, 13, 407, 341, 1310, 1542, 257, 707], "temperature": 0.0, "avg_logprob": -0.1046512590514289, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.00019094764138571918}, {"id": 375, "seek": 288836, "start": 2906.36, "end": 2914.1200000000003, "text": " bit like what you saw in a compiler's class if you did one, right, that you're creating, I call it", "tokens": [857, 411, 437, 291, 1866, 294, 257, 31958, 311, 1508, 498, 291, 630, 472, 11, 558, 11, 300, 291, 434, 4084, 11, 286, 818, 309], "temperature": 0.0, "avg_logprob": -0.1046512590514289, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.00019094764138571918}, {"id": 376, "seek": 291412, "start": 2914.12, "end": 2919.96, "text": " here computation graph, but it's really a tree, right. So you're creating here this tree of", "tokens": [510, 24903, 4295, 11, 457, 309, 311, 534, 257, 4230, 11, 558, 13, 407, 291, 434, 4084, 510, 341, 4230, 295], "temperature": 0.0, "avg_logprob": -0.19672901100582546, "compression_ratio": 1.7017543859649122, "no_speech_prob": 3.26845001836773e-05}, {"id": 377, "seek": 291412, "start": 2919.96, "end": 2927.16, "text": " computations in this case, but in more general case, it's some kind of directed graph of computations,", "tokens": [2807, 763, 294, 341, 1389, 11, 457, 294, 544, 2674, 1389, 11, 309, 311, 512, 733, 295, 12898, 4295, 295, 2807, 763, 11], "temperature": 0.0, "avg_logprob": -0.19672901100582546, "compression_ratio": 1.7017543859649122, "no_speech_prob": 3.26845001836773e-05}, {"id": 378, "seek": 291412, "start": 2928.52, "end": 2939.56, "text": " which has source nodes, which are imports either inputs like x or input parameters like w and b.", "tokens": [597, 575, 4009, 13891, 11, 597, 366, 41596, 2139, 15743, 411, 2031, 420, 4846, 9834, 411, 261, 293, 272, 13], "temperature": 0.0, "avg_logprob": -0.19672901100582546, "compression_ratio": 1.7017543859649122, "no_speech_prob": 3.26845001836773e-05}, {"id": 379, "seek": 293956, "start": 2939.56, "end": 2946.68, "text": " And it's interior nodes or operations. And so then once we've constructed a graph,", "tokens": [400, 309, 311, 10636, 13891, 420, 7705, 13, 400, 370, 550, 1564, 321, 600, 17083, 257, 4295, 11], "temperature": 0.0, "avg_logprob": -0.1324782265557183, "compression_ratio": 1.6891891891891893, "no_speech_prob": 6.596517050638795e-05}, {"id": 380, "seek": 293956, "start": 2946.68, "end": 2951.88, "text": " and so this graph corresponds to exactly the example I did before, right, that this was our little", "tokens": [293, 370, 341, 4295, 23249, 281, 2293, 264, 1365, 286, 630, 949, 11, 558, 11, 300, 341, 390, 527, 707], "temperature": 0.0, "avg_logprob": -0.1324782265557183, "compression_ratio": 1.6891891891891893, "no_speech_prob": 6.596517050638795e-05}, {"id": 381, "seek": 293956, "start": 2951.88, "end": 2957.56, "text": " neural net that's in the top right. And here's the corresponding computation graph of computing", "tokens": [18161, 2533, 300, 311, 294, 264, 1192, 558, 13, 400, 510, 311, 264, 11760, 24903, 4295, 295, 15866], "temperature": 0.0, "avg_logprob": -0.1324782265557183, "compression_ratio": 1.6891891891891893, "no_speech_prob": 6.596517050638795e-05}, {"id": 382, "seek": 293956, "start": 2957.56, "end": 2966.68, "text": " wx plus b, put it through the sigmoid nonlinearity f, multiply the resulting dot product that the", "tokens": [261, 87, 1804, 272, 11, 829, 309, 807, 264, 4556, 3280, 327, 2107, 1889, 17409, 283, 11, 12972, 264, 16505, 5893, 1674, 300, 264], "temperature": 0.0, "avg_logprob": -0.1324782265557183, "compression_ratio": 1.6891891891891893, "no_speech_prob": 6.596517050638795e-05}, {"id": 383, "seek": 296668, "start": 2966.68, "end": 2977.16, "text": " resulting vector with you gives us our output score s. Okay, so what we do to compute this is we", "tokens": [16505, 8062, 365, 291, 2709, 505, 527, 5598, 6175, 262, 13, 1033, 11, 370, 437, 321, 360, 281, 14722, 341, 307, 321], "temperature": 0.0, "avg_logprob": -0.11336885690689087, "compression_ratio": 1.6836158192090396, "no_speech_prob": 1.8338743757340126e-05}, {"id": 384, "seek": 296668, "start": 2977.16, "end": 2984.8399999999997, "text": " pass along the edges the results of operations. So this is wx, then z, then h, and then our output is s.", "tokens": [1320, 2051, 264, 8819, 264, 3542, 295, 7705, 13, 407, 341, 307, 261, 87, 11, 550, 710, 11, 550, 276, 11, 293, 550, 527, 5598, 307, 262, 13], "temperature": 0.0, "avg_logprob": -0.11336885690689087, "compression_ratio": 1.6836158192090396, "no_speech_prob": 1.8338743757340126e-05}, {"id": 385, "seek": 296668, "start": 2985.3999999999996, "end": 2991.3199999999997, "text": " And so the first thing we want to be able to do to compute with neural networks is to be able to", "tokens": [400, 370, 264, 700, 551, 321, 528, 281, 312, 1075, 281, 360, 281, 14722, 365, 18161, 9590, 307, 281, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.11336885690689087, "compression_ratio": 1.6836158192090396, "no_speech_prob": 1.8338743757340126e-05}, {"id": 386, "seek": 299132, "start": 2991.32, "end": 2999.6400000000003, "text": " compute for different inputs what the output is. And so that's referred to as forward propagation.", "tokens": [14722, 337, 819, 15743, 437, 264, 5598, 307, 13, 400, 370, 300, 311, 10839, 281, 382, 2128, 38377, 13], "temperature": 0.0, "avg_logprob": -0.08233057168813852, "compression_ratio": 1.6166666666666667, "no_speech_prob": 5.8255085605196655e-05}, {"id": 387, "seek": 299132, "start": 2999.6400000000003, "end": 3010.04, "text": " And so we simply run this expression much like you just standardly do in a compiler to compute", "tokens": [400, 370, 321, 2935, 1190, 341, 6114, 709, 411, 291, 445, 3832, 356, 360, 294, 257, 31958, 281, 14722], "temperature": 0.0, "avg_logprob": -0.08233057168813852, "compression_ratio": 1.6166666666666667, "no_speech_prob": 5.8255085605196655e-05}, {"id": 388, "seek": 299132, "start": 3010.04, "end": 3016.92, "text": " the value of s. And that's the forward propagation phase. But the essential additional element of", "tokens": [264, 2158, 295, 262, 13, 400, 300, 311, 264, 2128, 38377, 5574, 13, 583, 264, 7115, 4497, 4478, 295], "temperature": 0.0, "avg_logprob": -0.08233057168813852, "compression_ratio": 1.6166666666666667, "no_speech_prob": 5.8255085605196655e-05}, {"id": 389, "seek": 301692, "start": 3016.92, "end": 3025.08, "text": " neural networks is that we then also want to be able to send back gradients, which will tell us how", "tokens": [18161, 9590, 307, 300, 321, 550, 611, 528, 281, 312, 1075, 281, 2845, 646, 2771, 2448, 11, 597, 486, 980, 505, 577], "temperature": 0.0, "avg_logprob": -0.07544119107095819, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0001738729333737865}, {"id": 390, "seek": 301692, "start": 3025.08, "end": 3033.32, "text": " to update the parameters of the model. And so it's this ability to send back gradients, which gives us", "tokens": [281, 5623, 264, 9834, 295, 264, 2316, 13, 400, 370, 309, 311, 341, 3485, 281, 2845, 646, 2771, 2448, 11, 597, 2709, 505], "temperature": 0.0, "avg_logprob": -0.07544119107095819, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0001738729333737865}, {"id": 391, "seek": 301692, "start": 3033.32, "end": 3040.52, "text": " the ability for these models to learn once we have a loss function at the end, we can work out how to", "tokens": [264, 3485, 337, 613, 5245, 281, 1466, 1564, 321, 362, 257, 4470, 2445, 412, 264, 917, 11, 321, 393, 589, 484, 577, 281], "temperature": 0.0, "avg_logprob": -0.07544119107095819, "compression_ratio": 1.6795580110497237, "no_speech_prob": 0.0001738729333737865}, {"id": 392, "seek": 304052, "start": 3040.52, "end": 3048.44, "text": " change the parameters of the model so that they more accurately produce the desired output, i.e.", "tokens": [1319, 264, 9834, 295, 264, 2316, 370, 300, 436, 544, 20095, 5258, 264, 14721, 5598, 11, 741, 13, 68, 13], "temperature": 0.0, "avg_logprob": -0.12005280045902028, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.2468633687822148e-05}, {"id": 393, "seek": 304052, "start": 3048.44, "end": 3057.8, "text": " they minimize the loss. And so it's doing that part that then is called back propagation. So we then", "tokens": [436, 17522, 264, 4470, 13, 400, 370, 309, 311, 884, 300, 644, 300, 550, 307, 1219, 646, 38377, 13, 407, 321, 550], "temperature": 0.0, "avg_logprob": -0.12005280045902028, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.2468633687822148e-05}, {"id": 394, "seek": 304052, "start": 3057.8, "end": 3066.92, "text": " once we forward propagated a value with our current parameters, we then head backwards reversing", "tokens": [1564, 321, 2128, 12425, 770, 257, 2158, 365, 527, 2190, 9834, 11, 321, 550, 1378, 12204, 14582, 278], "temperature": 0.0, "avg_logprob": -0.12005280045902028, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.2468633687822148e-05}, {"id": 395, "seek": 306692, "start": 3066.92, "end": 3076.36, "text": " the direction of the arrows and pass along gradients down to the different parameters like B and W and U", "tokens": [264, 3513, 295, 264, 19669, 293, 1320, 2051, 2771, 2448, 760, 281, 264, 819, 9834, 411, 363, 293, 343, 293, 624], "temperature": 0.0, "avg_logprob": -0.1335126436673678, "compression_ratio": 1.5736842105263158, "no_speech_prob": 3.8812922866782174e-05}, {"id": 396, "seek": 306692, "start": 3076.36, "end": 3082.6800000000003, "text": " that we can use to change using stochastic gradient descent what the value of B is or what the", "tokens": [300, 321, 393, 764, 281, 1319, 1228, 342, 8997, 2750, 16235, 23475, 437, 264, 2158, 295, 363, 307, 420, 437, 264], "temperature": 0.0, "avg_logprob": -0.1335126436673678, "compression_ratio": 1.5736842105263158, "no_speech_prob": 3.8812922866782174e-05}, {"id": 397, "seek": 306692, "start": 3082.6800000000003, "end": 3092.28, "text": " value of W is. So we start off with ds ds, which is just one. And then we run our back propagation.", "tokens": [2158, 295, 343, 307, 13, 407, 321, 722, 766, 365, 274, 82, 274, 82, 11, 597, 307, 445, 472, 13, 400, 550, 321, 1190, 527, 646, 38377, 13], "temperature": 0.0, "avg_logprob": -0.1335126436673678, "compression_ratio": 1.5736842105263158, "no_speech_prob": 3.8812922866782174e-05}, {"id": 398, "seek": 309228, "start": 3092.28, "end": 3101.2400000000002, "text": " And we're using the sort of same kind of composition of Jacobian. So we have ds dh here and ds dz", "tokens": [400, 321, 434, 1228, 264, 1333, 295, 912, 733, 295, 12686, 295, 14117, 952, 13, 407, 321, 362, 274, 82, 274, 71, 510, 293, 274, 82, 274, 89], "temperature": 0.0, "avg_logprob": -0.09561584546015812, "compression_ratio": 1.53125, "no_speech_prob": 3.534197458066046e-05}, {"id": 399, "seek": 309228, "start": 3101.2400000000002, "end": 3109.8, "text": " and we progressively pass back those gradients. So we just need to work out how to efficiently and", "tokens": [293, 321, 46667, 1320, 646, 729, 2771, 2448, 13, 407, 321, 445, 643, 281, 589, 484, 577, 281, 19621, 293], "temperature": 0.0, "avg_logprob": -0.09561584546015812, "compression_ratio": 1.53125, "no_speech_prob": 3.534197458066046e-05}, {"id": 400, "seek": 309228, "start": 3109.8, "end": 3117.32, "text": " cleanly do this in a computational system. And so let's sort of work through again a few of these", "tokens": [2541, 356, 360, 341, 294, 257, 28270, 1185, 13, 400, 370, 718, 311, 1333, 295, 589, 807, 797, 257, 1326, 295, 613], "temperature": 0.0, "avg_logprob": -0.09561584546015812, "compression_ratio": 1.53125, "no_speech_prob": 3.534197458066046e-05}, {"id": 401, "seek": 311732, "start": 3117.32, "end": 3127.6400000000003, "text": " cases. So the general situation is we have a particular node. So a node is where some kind of", "tokens": [3331, 13, 407, 264, 2674, 2590, 307, 321, 362, 257, 1729, 9984, 13, 407, 257, 9984, 307, 689, 512, 733, 295], "temperature": 0.0, "avg_logprob": -0.1140421334799234, "compression_ratio": 1.6243093922651934, "no_speech_prob": 2.1105113773955964e-05}, {"id": 402, "seek": 311732, "start": 3128.36, "end": 3137.4, "text": " operation like multiplication or a nonlinearity happens. And so the simplest case is that we've got", "tokens": [6916, 411, 27290, 420, 257, 2107, 1889, 17409, 2314, 13, 400, 370, 264, 22811, 1389, 307, 300, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.1140421334799234, "compression_ratio": 1.6243093922651934, "no_speech_prob": 2.1105113773955964e-05}, {"id": 403, "seek": 311732, "start": 3137.4, "end": 3146.6000000000004, "text": " one output and one input. So we'll do that first. So that's like h equals f of z. So what we have is", "tokens": [472, 5598, 293, 472, 4846, 13, 407, 321, 603, 360, 300, 700, 13, 407, 300, 311, 411, 276, 6915, 283, 295, 710, 13, 407, 437, 321, 362, 307], "temperature": 0.0, "avg_logprob": -0.1140421334799234, "compression_ratio": 1.6243093922651934, "no_speech_prob": 2.1105113773955964e-05}, {"id": 404, "seek": 314660, "start": 3146.6, "end": 3158.36, "text": " an upstream gradient ds dh. And what we want to do is compute the downstream gradient of ds dz.", "tokens": [364, 33915, 16235, 274, 82, 274, 71, 13, 400, 437, 321, 528, 281, 360, 307, 14722, 264, 30621, 16235, 295, 274, 82, 274, 89, 13], "temperature": 0.0, "avg_logprob": -0.17050930261611938, "compression_ratio": 1.7098765432098766, "no_speech_prob": 2.8814942197641358e-05}, {"id": 405, "seek": 314660, "start": 3158.92, "end": 3166.92, "text": " And the way we're going to do that is say, well, for this function f, it's a function, it's got", "tokens": [400, 264, 636, 321, 434, 516, 281, 360, 300, 307, 584, 11, 731, 11, 337, 341, 2445, 283, 11, 309, 311, 257, 2445, 11, 309, 311, 658], "temperature": 0.0, "avg_logprob": -0.17050930261611938, "compression_ratio": 1.7098765432098766, "no_speech_prob": 2.8814942197641358e-05}, {"id": 406, "seek": 314660, "start": 3166.92, "end": 3173.96, "text": " a derivative for gradient. So what we want to do is work out that local gradient dhd.", "tokens": [257, 13760, 337, 16235, 13, 407, 437, 321, 528, 281, 360, 307, 589, 484, 300, 2654, 16235, 274, 20272, 13], "temperature": 0.0, "avg_logprob": -0.17050930261611938, "compression_ratio": 1.7098765432098766, "no_speech_prob": 2.8814942197641358e-05}, {"id": 407, "seek": 317396, "start": 3173.96, "end": 3183.4, "text": " And then that gives us everything that we need to work out ds dz. Because that's precisely we're", "tokens": [400, 550, 300, 2709, 505, 1203, 300, 321, 643, 281, 589, 484, 274, 82, 274, 89, 13, 1436, 300, 311, 13402, 321, 434], "temperature": 0.0, "avg_logprob": -0.12325940132141114, "compression_ratio": 1.591160220994475, "no_speech_prob": 5.736211460316554e-05}, {"id": 408, "seek": 317396, "start": 3183.4, "end": 3190.68, "text": " going to use the chain rule. We're going to say that ds dz equals the product of ds dh times dhd", "tokens": [516, 281, 764, 264, 5021, 4978, 13, 492, 434, 516, 281, 584, 300, 274, 82, 274, 89, 6915, 264, 1674, 295, 274, 82, 274, 71, 1413, 274, 20272], "temperature": 0.0, "avg_logprob": -0.12325940132141114, "compression_ratio": 1.591160220994475, "no_speech_prob": 5.736211460316554e-05}, {"id": 409, "seek": 317396, "start": 3190.68, "end": 3197.96, "text": " where this is again using Jacobians. Okay, so the general principle that we're going to use is", "tokens": [689, 341, 307, 797, 1228, 14117, 2567, 13, 1033, 11, 370, 264, 2674, 8665, 300, 321, 434, 516, 281, 764, 307], "temperature": 0.0, "avg_logprob": -0.12325940132141114, "compression_ratio": 1.591160220994475, "no_speech_prob": 5.736211460316554e-05}, {"id": 410, "seek": 319796, "start": 3197.96, "end": 3205.32, "text": " that downstream gradient equals the upstream gradient times the local gradient. Okay, sometimes", "tokens": [300, 30621, 16235, 6915, 264, 33915, 16235, 1413, 264, 2654, 16235, 13, 1033, 11, 2171], "temperature": 0.0, "avg_logprob": -0.07511466922182025, "compression_ratio": 1.7, "no_speech_prob": 4.068750058650039e-05}, {"id": 411, "seek": 319796, "start": 3205.32, "end": 3211.4, "text": " it gets a little bit more complicated. So we might have multiple inputs to a function. So this is", "tokens": [309, 2170, 257, 707, 857, 544, 6179, 13, 407, 321, 1062, 362, 3866, 15743, 281, 257, 2445, 13, 407, 341, 307], "temperature": 0.0, "avg_logprob": -0.07511466922182025, "compression_ratio": 1.7, "no_speech_prob": 4.068750058650039e-05}, {"id": 412, "seek": 319796, "start": 3212.28, "end": 3220.76, "text": " the matrix vector multiply. So z equals wx. Okay, when there are multiple inputs, we still have", "tokens": [264, 8141, 8062, 12972, 13, 407, 710, 6915, 261, 87, 13, 1033, 11, 562, 456, 366, 3866, 15743, 11, 321, 920, 362], "temperature": 0.0, "avg_logprob": -0.07511466922182025, "compression_ratio": 1.7, "no_speech_prob": 4.068750058650039e-05}, {"id": 413, "seek": 322076, "start": 3220.76, "end": 3230.92, "text": " an upstream gradient ds dz. But what we're going to do is work out a local gradient with respect to", "tokens": [364, 33915, 16235, 274, 82, 274, 89, 13, 583, 437, 321, 434, 516, 281, 360, 307, 589, 484, 257, 2654, 16235, 365, 3104, 281], "temperature": 0.0, "avg_logprob": -0.12334457485155127, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.1434454538393766e-05}, {"id": 414, "seek": 322076, "start": 3230.92, "end": 3240.2000000000003, "text": " each input. So we have dz dw and dz dx. And so then at that point, it's exactly the same for each", "tokens": [1184, 4846, 13, 407, 321, 362, 274, 89, 27379, 293, 274, 89, 30017, 13, 400, 370, 550, 412, 300, 935, 11, 309, 311, 2293, 264, 912, 337, 1184], "temperature": 0.0, "avg_logprob": -0.12334457485155127, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.1434454538393766e-05}, {"id": 415, "seek": 322076, "start": 3240.2000000000003, "end": 3248.2000000000003, "text": " piece of it. We're going to work out the downstream gradients ds dw and ds dx by using the chain rule", "tokens": [2522, 295, 309, 13, 492, 434, 516, 281, 589, 484, 264, 30621, 2771, 2448, 274, 82, 27379, 293, 274, 82, 274, 87, 538, 1228, 264, 5021, 4978], "temperature": 0.0, "avg_logprob": -0.12334457485155127, "compression_ratio": 1.5989304812834224, "no_speech_prob": 2.1434454538393766e-05}, {"id": 416, "seek": 324820, "start": 3248.2, "end": 3257.96, "text": " with respect to the particular local gradient. So let's go through an example of this. I mean,", "tokens": [365, 3104, 281, 264, 1729, 2654, 16235, 13, 407, 718, 311, 352, 807, 364, 1365, 295, 341, 13, 286, 914, 11], "temperature": 0.0, "avg_logprob": -0.05144878609539711, "compression_ratio": 1.5921787709497206, "no_speech_prob": 8.877764048520476e-05}, {"id": 417, "seek": 324820, "start": 3257.96, "end": 3264.52, "text": " this is kind of a silly example. It's not really an example that looks like a typical neural net.", "tokens": [341, 307, 733, 295, 257, 11774, 1365, 13, 467, 311, 406, 534, 364, 1365, 300, 1542, 411, 257, 7476, 18161, 2533, 13], "temperature": 0.0, "avg_logprob": -0.05144878609539711, "compression_ratio": 1.5921787709497206, "no_speech_prob": 8.877764048520476e-05}, {"id": 418, "seek": 324820, "start": 3264.52, "end": 3270.2, "text": " But it's sort of a simple example where we can show some of the components of what we do. So", "tokens": [583, 309, 311, 1333, 295, 257, 2199, 1365, 689, 321, 393, 855, 512, 295, 264, 6677, 295, 437, 321, 360, 13, 407], "temperature": 0.0, "avg_logprob": -0.05144878609539711, "compression_ratio": 1.5921787709497206, "no_speech_prob": 8.877764048520476e-05}, {"id": 419, "seek": 327020, "start": 3270.2, "end": 3279.3199999999997, "text": " what we're going to do is want to calculate f of xyz, which is being calculated as x plus y times", "tokens": [437, 321, 434, 516, 281, 360, 307, 528, 281, 8873, 283, 295, 2031, 37433, 11, 597, 307, 885, 15598, 382, 2031, 1804, 288, 1413], "temperature": 0.0, "avg_logprob": -0.0948590338230133, "compression_ratio": 1.582010582010582, "no_speech_prob": 5.0634946092031896e-05}, {"id": 420, "seek": 327020, "start": 3279.3199999999997, "end": 3287.56, "text": " the max of y and z. And we've got, you know, particular values that we're starting off with x equals", "tokens": [264, 11469, 295, 288, 293, 710, 13, 400, 321, 600, 658, 11, 291, 458, 11, 1729, 4190, 300, 321, 434, 2891, 766, 365, 2031, 6915], "temperature": 0.0, "avg_logprob": -0.0948590338230133, "compression_ratio": 1.582010582010582, "no_speech_prob": 5.0634946092031896e-05}, {"id": 421, "seek": 327020, "start": 3287.56, "end": 3295.3199999999997, "text": " one y equals two and z equals zero. So these are the current values of our parameters. And so we can", "tokens": [472, 288, 6915, 732, 293, 710, 6915, 4018, 13, 407, 613, 366, 264, 2190, 4190, 295, 527, 9834, 13, 400, 370, 321, 393], "temperature": 0.0, "avg_logprob": -0.0948590338230133, "compression_ratio": 1.582010582010582, "no_speech_prob": 5.0634946092031896e-05}, {"id": 422, "seek": 329532, "start": 3295.32, "end": 3303.2400000000002, "text": " say, okay, well, we want to build an expression tree for that. Here's our expression tree. We're", "tokens": [584, 11, 1392, 11, 731, 11, 321, 528, 281, 1322, 364, 6114, 4230, 337, 300, 13, 1692, 311, 527, 6114, 4230, 13, 492, 434], "temperature": 0.0, "avg_logprob": -0.08719105534739309, "compression_ratio": 1.5921787709497206, "no_speech_prob": 1.3844219211023301e-05}, {"id": 423, "seek": 329532, "start": 3303.2400000000002, "end": 3310.76, "text": " taking x plus y. We're taking the max of y and z. And then we're multiplying them. And so our", "tokens": [1940, 2031, 1804, 288, 13, 492, 434, 1940, 264, 11469, 295, 288, 293, 710, 13, 400, 550, 321, 434, 30955, 552, 13, 400, 370, 527], "temperature": 0.0, "avg_logprob": -0.08719105534739309, "compression_ratio": 1.5921787709497206, "no_speech_prob": 1.3844219211023301e-05}, {"id": 424, "seek": 329532, "start": 3310.76, "end": 3318.6800000000003, "text": " forward propagation phase is just to run this. So we take the values of our parameters. And we", "tokens": [2128, 38377, 5574, 307, 445, 281, 1190, 341, 13, 407, 321, 747, 264, 4190, 295, 527, 9834, 13, 400, 321], "temperature": 0.0, "avg_logprob": -0.08719105534739309, "compression_ratio": 1.5921787709497206, "no_speech_prob": 1.3844219211023301e-05}, {"id": 425, "seek": 331868, "start": 3318.68, "end": 3325.7999999999997, "text": " simply start to compute with them, right? So we have one, two, two, zero. And we add them as three,", "tokens": [2935, 722, 281, 14722, 365, 552, 11, 558, 30, 407, 321, 362, 472, 11, 732, 11, 732, 11, 4018, 13, 400, 321, 909, 552, 382, 1045, 11], "temperature": 0.0, "avg_logprob": -0.10024845294463329, "compression_ratio": 1.5628415300546448, "no_speech_prob": 3.0713086744071916e-05}, {"id": 426, "seek": 331868, "start": 3325.7999999999997, "end": 3335.96, "text": " the max is two. We multiply them. And that gives us six. Okay. So then at that point, we then", "tokens": [264, 11469, 307, 732, 13, 492, 12972, 552, 13, 400, 300, 2709, 505, 2309, 13, 1033, 13, 407, 550, 412, 300, 935, 11, 321, 550], "temperature": 0.0, "avg_logprob": -0.10024845294463329, "compression_ratio": 1.5628415300546448, "no_speech_prob": 3.0713086744071916e-05}, {"id": 427, "seek": 331868, "start": 3335.96, "end": 3345.56, "text": " want to go and work out how to do things for back propagation and how these back propagation", "tokens": [528, 281, 352, 293, 589, 484, 577, 281, 360, 721, 337, 646, 38377, 293, 577, 613, 646, 38377], "temperature": 0.0, "avg_logprob": -0.10024845294463329, "compression_ratio": 1.5628415300546448, "no_speech_prob": 3.0713086744071916e-05}, {"id": 428, "seek": 334556, "start": 3345.56, "end": 3352.12, "text": " steps work. And so the first part of that is sort of working out what our local gradients are", "tokens": [4439, 589, 13, 400, 370, 264, 700, 644, 295, 300, 307, 1333, 295, 1364, 484, 437, 527, 2654, 2771, 2448, 366], "temperature": 0.0, "avg_logprob": -0.1515069501153354, "compression_ratio": 1.3984962406015038, "no_speech_prob": 6.40071666566655e-05}, {"id": 429, "seek": 334556, "start": 3352.12, "end": 3363.64, "text": " going to be. So, so this is a here. And this is x and y. So dADX, since a equals x plus y is", "tokens": [516, 281, 312, 13, 407, 11, 370, 341, 307, 257, 510, 13, 400, 341, 307, 2031, 293, 288, 13, 407, 274, 6112, 55, 11, 1670, 257, 6915, 2031, 1804, 288, 307], "temperature": 0.0, "avg_logprob": -0.1515069501153354, "compression_ratio": 1.3984962406015038, "no_speech_prob": 6.40071666566655e-05}, {"id": 430, "seek": 336364, "start": 3363.64, "end": 3378.04, "text": " just going to be one. And dADY is also going to be one. Then for b equals the max of y z. So this", "tokens": [445, 516, 281, 312, 472, 13, 400, 274, 6112, 56, 307, 611, 516, 281, 312, 472, 13, 1396, 337, 272, 6915, 264, 11469, 295, 288, 710, 13, 407, 341], "temperature": 0.0, "avg_logprob": -0.15226410230000814, "compression_ratio": 1.4222222222222223, "no_speech_prob": 4.356339104560902e-06}, {"id": 431, "seek": 336364, "start": 3378.04, "end": 3385.72, "text": " is this max node. So the local gradients for that is it's going to depend on y, where the y is", "tokens": [307, 341, 11469, 9984, 13, 407, 264, 2654, 2771, 2448, 337, 300, 307, 309, 311, 516, 281, 5672, 322, 288, 11, 689, 264, 288, 307], "temperature": 0.0, "avg_logprob": -0.15226410230000814, "compression_ratio": 1.4222222222222223, "no_speech_prob": 4.356339104560902e-06}, {"id": 432, "seek": 338572, "start": 3385.72, "end": 3395.56, "text": " greater than z. So dBDY is going to be one, if and only if y is greater than z, which it is at", "tokens": [5044, 813, 710, 13, 407, 274, 33, 34891, 307, 516, 281, 312, 472, 11, 498, 293, 787, 498, 288, 307, 5044, 813, 710, 11, 597, 309, 307, 412], "temperature": 0.0, "avg_logprob": -0.10635146640595936, "compression_ratio": 1.8311688311688312, "no_speech_prob": 1.5933172107907012e-05}, {"id": 433, "seek": 338572, "start": 3395.56, "end": 3405.24, "text": " our particular point here. So that's one. And dBdz is going to be one only if z is greater than y.", "tokens": [527, 1729, 935, 510, 13, 407, 300, 311, 472, 13, 400, 274, 33, 28168, 307, 516, 281, 312, 472, 787, 498, 710, 307, 5044, 813, 288, 13], "temperature": 0.0, "avg_logprob": -0.10635146640595936, "compression_ratio": 1.8311688311688312, "no_speech_prob": 1.5933172107907012e-05}, {"id": 434, "seek": 338572, "start": 3405.24, "end": 3415.0, "text": " So for our particular values here, that one is going to be zero. And then finally, here,", "tokens": [407, 337, 527, 1729, 4190, 510, 11, 300, 472, 307, 516, 281, 312, 4018, 13, 400, 550, 2721, 11, 510, 11], "temperature": 0.0, "avg_logprob": -0.10635146640595936, "compression_ratio": 1.8311688311688312, "no_speech_prob": 1.5933172107907012e-05}, {"id": 435, "seek": 341500, "start": 3415.0, "end": 3428.52, "text": " we're calculating the product f equals a b. So for that, we're going to, sorry, that slides", "tokens": [321, 434, 28258, 264, 1674, 283, 6915, 257, 272, 13, 407, 337, 300, 11, 321, 434, 516, 281, 11, 2597, 11, 300, 9788], "temperature": 0.0, "avg_logprob": -0.14457586214139864, "compression_ratio": 1.680722891566265, "no_speech_prob": 4.4674379751086235e-05}, {"id": 436, "seek": 341500, "start": 3428.52, "end": 3435.08, "text": " all in perfect. Okay, so for the product, the derivative f with respect to a is equal to b,", "tokens": [439, 294, 2176, 13, 1033, 11, 370, 337, 264, 1674, 11, 264, 13760, 283, 365, 3104, 281, 257, 307, 2681, 281, 272, 11], "temperature": 0.0, "avg_logprob": -0.14457586214139864, "compression_ratio": 1.680722891566265, "no_speech_prob": 4.4674379751086235e-05}, {"id": 437, "seek": 341500, "start": 3435.08, "end": 3441.8, "text": " which is two. And the derivative f with respect to b is a equals three. So that gives us all of", "tokens": [597, 307, 732, 13, 400, 264, 13760, 283, 365, 3104, 281, 272, 307, 257, 6915, 1045, 13, 407, 300, 2709, 505, 439, 295], "temperature": 0.0, "avg_logprob": -0.14457586214139864, "compression_ratio": 1.680722891566265, "no_speech_prob": 4.4674379751086235e-05}, {"id": 438, "seek": 344180, "start": 3441.8, "end": 3450.2000000000003, "text": " the local gradients at each node. And so then to run back propagation, we start with dF dF,", "tokens": [264, 2654, 2771, 2448, 412, 1184, 9984, 13, 400, 370, 550, 281, 1190, 646, 38377, 11, 321, 722, 365, 274, 37, 274, 37, 11], "temperature": 0.0, "avg_logprob": -0.10467638113559821, "compression_ratio": 1.5508021390374331, "no_speech_prob": 2.7529729777597822e-05}, {"id": 439, "seek": 344180, "start": 3450.2000000000003, "end": 3459.7200000000003, "text": " which is just one. And then we're going to work out the downstream equals the upstream times the", "tokens": [597, 307, 445, 472, 13, 400, 550, 321, 434, 516, 281, 589, 484, 264, 30621, 6915, 264, 33915, 1413, 264], "temperature": 0.0, "avg_logprob": -0.10467638113559821, "compression_ratio": 1.5508021390374331, "no_speech_prob": 2.7529729777597822e-05}, {"id": 440, "seek": 344180, "start": 3459.7200000000003, "end": 3468.76, "text": " local. Okay, so the local, so when you have a product like this, note the sort of the gradients flip.", "tokens": [2654, 13, 1033, 11, 370, 264, 2654, 11, 370, 562, 291, 362, 257, 1674, 411, 341, 11, 3637, 264, 1333, 295, 264, 2771, 2448, 7929, 13], "temperature": 0.0, "avg_logprob": -0.10467638113559821, "compression_ratio": 1.5508021390374331, "no_speech_prob": 2.7529729777597822e-05}, {"id": 441, "seek": 346876, "start": 3468.76, "end": 3484.28, "text": " So we take upstream times the local, which is two. Oops. So the downstream is two on this side.", "tokens": [407, 321, 747, 33915, 1413, 264, 2654, 11, 597, 307, 732, 13, 21726, 13, 407, 264, 30621, 307, 732, 322, 341, 1252, 13], "temperature": 0.0, "avg_logprob": -0.23040270805358887, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.00016341113951057196}, {"id": 442, "seek": 346876, "start": 3486.6800000000003, "end": 3495.48, "text": " DFDB is three. So we're taking upstream times local. That gives us three. And so that gives us", "tokens": [48336, 27735, 307, 1045, 13, 407, 321, 434, 1940, 33915, 1413, 2654, 13, 663, 2709, 505, 1045, 13, 400, 370, 300, 2709, 505], "temperature": 0.0, "avg_logprob": -0.23040270805358887, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.00016341113951057196}, {"id": 443, "seek": 349548, "start": 3495.48, "end": 3504.04, "text": " back propagates values to the plus and max nodes. And so then we continue along. So for the max node,", "tokens": [646, 12425, 1024, 4190, 281, 264, 1804, 293, 11469, 13891, 13, 400, 370, 550, 321, 2354, 2051, 13, 407, 337, 264, 11469, 9984, 11], "temperature": 0.0, "avg_logprob": -0.17912168037600634, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.3548040189780295e-05}, {"id": 444, "seek": 349548, "start": 3505.8, "end": 3514.28, "text": " the local gradient dBDY equals one. So we're going to take upstream as three. So it's going to take", "tokens": [264, 2654, 16235, 274, 33, 34891, 6915, 472, 13, 407, 321, 434, 516, 281, 747, 33915, 382, 1045, 13, 407, 309, 311, 516, 281, 747], "temperature": 0.0, "avg_logprob": -0.17912168037600634, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.3548040189780295e-05}, {"id": 445, "seek": 349548, "start": 3514.28, "end": 3523.72, "text": " three times one. And that gives us three. DBDC is zero because of the fact that Z's value is not", "tokens": [1045, 1413, 472, 13, 400, 300, 2709, 505, 1045, 13, 413, 33, 25619, 307, 4018, 570, 295, 264, 1186, 300, 1176, 311, 2158, 307, 406], "temperature": 0.0, "avg_logprob": -0.17912168037600634, "compression_ratio": 1.568421052631579, "no_speech_prob": 2.3548040189780295e-05}, {"id": 446, "seek": 352372, "start": 3523.72, "end": 3530.68, "text": " the max. So we're taking three times zero and saying that the gradient there is zero. So finally,", "tokens": [264, 11469, 13, 407, 321, 434, 1940, 1045, 1413, 4018, 293, 1566, 300, 264, 16235, 456, 307, 4018, 13, 407, 2721, 11], "temperature": 0.0, "avg_logprob": -0.10743947676670404, "compression_ratio": 1.7283236994219653, "no_speech_prob": 2.2815705960965715e-05}, {"id": 447, "seek": 352372, "start": 3530.68, "end": 3538.7599999999998, "text": " doing the plus node, the local gradients for both x and y, there are one. So we're just getting two", "tokens": [884, 264, 1804, 9984, 11, 264, 2654, 2771, 2448, 337, 1293, 2031, 293, 288, 11, 456, 366, 472, 13, 407, 321, 434, 445, 1242, 732], "temperature": 0.0, "avg_logprob": -0.10743947676670404, "compression_ratio": 1.7283236994219653, "no_speech_prob": 2.2815705960965715e-05}, {"id": 448, "seek": 352372, "start": 3538.7599999999998, "end": 3546.8399999999997, "text": " times one in both cases. And we're saying the gradients there are two. Okay. And so again, at the end", "tokens": [1413, 472, 294, 1293, 3331, 13, 400, 321, 434, 1566, 264, 2771, 2448, 456, 366, 732, 13, 1033, 13, 400, 370, 797, 11, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.10743947676670404, "compression_ratio": 1.7283236994219653, "no_speech_prob": 2.2815705960965715e-05}, {"id": 449, "seek": 354684, "start": 3546.84, "end": 3555.48, "text": " of the day, the interpretation here is that this is giving us this information as to if we wiggle the", "tokens": [295, 264, 786, 11, 264, 14174, 510, 307, 300, 341, 307, 2902, 505, 341, 1589, 382, 281, 498, 321, 33377, 264], "temperature": 0.0, "avg_logprob": -0.09410367188630281, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.6221914595225826e-05}, {"id": 450, "seek": 354684, "start": 3555.48, "end": 3563.4, "text": " values of x, y and z, how much of a difference does it make to the output? What is the slope, the", "tokens": [4190, 295, 2031, 11, 288, 293, 710, 11, 577, 709, 295, 257, 2649, 775, 309, 652, 281, 264, 5598, 30, 708, 307, 264, 13525, 11, 264], "temperature": 0.0, "avg_logprob": -0.09410367188630281, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.6221914595225826e-05}, {"id": 451, "seek": 354684, "start": 3563.4, "end": 3575.08, "text": " gradient, with respect to the variable? So what we've seen is that since Z isn't the max of y and z,", "tokens": [16235, 11, 365, 3104, 281, 264, 7006, 30, 407, 437, 321, 600, 1612, 307, 300, 1670, 1176, 1943, 380, 264, 11469, 295, 288, 293, 710, 11], "temperature": 0.0, "avg_logprob": -0.09410367188630281, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.6221914595225826e-05}, {"id": 452, "seek": 357508, "start": 3575.08, "end": 3583.56, "text": " if I change the value of z a little, like I find, z.1 or minus.1, it makes no difference at all", "tokens": [498, 286, 1319, 264, 2158, 295, 710, 257, 707, 11, 411, 286, 915, 11, 710, 13, 16, 420, 3175, 13, 16, 11, 309, 1669, 572, 2649, 412, 439], "temperature": 0.0, "avg_logprob": -0.15002235365502628, "compression_ratio": 1.632183908045977, "no_speech_prob": 5.305151717038825e-05}, {"id": 453, "seek": 357508, "start": 3583.56, "end": 3592.04, "text": " to what I compute as the output. So therefore, the gradient there is zero. If I change the value", "tokens": [281, 437, 286, 14722, 382, 264, 5598, 13, 407, 4412, 11, 264, 16235, 456, 307, 4018, 13, 759, 286, 1319, 264, 2158], "temperature": 0.0, "avg_logprob": -0.15002235365502628, "compression_ratio": 1.632183908045977, "no_speech_prob": 5.305151717038825e-05}, {"id": 454, "seek": 357508, "start": 3592.04, "end": 3602.44, "text": " of x a little, then that is going to have an effect. And it's going to affect the output by", "tokens": [295, 2031, 257, 707, 11, 550, 300, 307, 516, 281, 362, 364, 1802, 13, 400, 309, 311, 516, 281, 3345, 264, 5598, 538], "temperature": 0.0, "avg_logprob": -0.15002235365502628, "compression_ratio": 1.632183908045977, "no_speech_prob": 5.305151717038825e-05}, {"id": 455, "seek": 360244, "start": 3602.44, "end": 3617.7200000000003, "text": " twice as much as the amount I change it. Right. So, and that's because the df dz equals two.", "tokens": [6091, 382, 709, 382, 264, 2372, 286, 1319, 309, 13, 1779, 13, 407, 11, 293, 300, 311, 570, 264, 274, 69, 9758, 6915, 732, 13], "temperature": 0.0, "avg_logprob": -0.22129321584896167, "compression_ratio": 1.317829457364341, "no_speech_prob": 0.00010712179209804162}, {"id": 456, "seek": 360244, "start": 3619.32, "end": 3627.2400000000002, "text": " So interestingly, so I mean, we can basically work that out. So if we imagine", "tokens": [407, 25873, 11, 370, 286, 914, 11, 321, 393, 1936, 589, 300, 484, 13, 407, 498, 321, 3811], "temperature": 0.0, "avg_logprob": -0.22129321584896167, "compression_ratio": 1.317829457364341, "no_speech_prob": 0.00010712179209804162}, {"id": 457, "seek": 362724, "start": 3627.24, "end": 3636.2, "text": " making sort of x 2.1, well, then what we'd calculate the max is two.", "tokens": [1455, 1333, 295, 2031, 568, 13, 16, 11, 731, 11, 550, 437, 321, 1116, 8873, 264, 11469, 307, 732, 13], "temperature": 0.0, "avg_logprob": -0.22175340485154538, "compression_ratio": 1.412280701754386, "no_speech_prob": 0.0001140890090027824}, {"id": 458, "seek": 362724, "start": 3637.8799999999997, "end": 3646.2, "text": " Oh, sorry, sorry, if we make x 1.1, we then get the max here is two, and we get 1.1 plus two", "tokens": [876, 11, 2597, 11, 2597, 11, 498, 321, 652, 2031, 502, 13, 16, 11, 321, 550, 483, 264, 11469, 510, 307, 732, 11, 293, 321, 483, 502, 13, 16, 1804, 732], "temperature": 0.0, "avg_logprob": -0.22175340485154538, "compression_ratio": 1.412280701754386, "no_speech_prob": 0.0001140890090027824}, {"id": 459, "seek": 364620, "start": 3646.2, "end": 3658.4399999999996, "text": " is 3.1. So we get 3.1 times two. So that'd be about 6.2. So changing x by 0.1 has added 0.2 to the", "tokens": [307, 805, 13, 16, 13, 407, 321, 483, 805, 13, 16, 1413, 732, 13, 407, 300, 1116, 312, 466, 1386, 13, 17, 13, 407, 4473, 2031, 538, 1958, 13, 16, 575, 3869, 1958, 13, 17, 281, 264], "temperature": 0.0, "avg_logprob": -0.10939149758250442, "compression_ratio": 1.5025641025641026, "no_speech_prob": 1.8627024473971687e-05}, {"id": 460, "seek": 364620, "start": 3658.4399999999996, "end": 3668.52, "text": " value of f. Conversely, for the value of y, we find that the df dy equals five. So what we do when", "tokens": [2158, 295, 283, 13, 33247, 736, 11, 337, 264, 2158, 295, 288, 11, 321, 915, 300, 264, 274, 69, 14584, 6915, 1732, 13, 407, 437, 321, 360, 562], "temperature": 0.0, "avg_logprob": -0.10939149758250442, "compression_ratio": 1.5025641025641026, "no_speech_prob": 1.8627024473971687e-05}, {"id": 461, "seek": 364620, "start": 3668.52, "end": 3674.04, "text": " we've got two things coming out here, as I'll go through again in a moment, is with summing the", "tokens": [321, 600, 658, 732, 721, 1348, 484, 510, 11, 382, 286, 603, 352, 807, 797, 294, 257, 1623, 11, 307, 365, 2408, 2810, 264], "temperature": 0.0, "avg_logprob": -0.10939149758250442, "compression_ratio": 1.5025641025641026, "no_speech_prob": 1.8627024473971687e-05}, {"id": 462, "seek": 367404, "start": 3674.04, "end": 3679.96, "text": " gradient. So again, three plus two equals five. And empirically, that's what happens. So if we", "tokens": [16235, 13, 407, 797, 11, 1045, 1804, 732, 6915, 1732, 13, 400, 25790, 984, 11, 300, 311, 437, 2314, 13, 407, 498, 321], "temperature": 0.0, "avg_logprob": -0.09520140989327136, "compression_ratio": 1.5235602094240839, "no_speech_prob": 1.1295889635221101e-05}, {"id": 463, "seek": 367404, "start": 3679.96, "end": 3688.6, "text": " consider fiddling the value of y a little, let's say we make it a value of 2.1, then the prediction", "tokens": [1949, 283, 14273, 1688, 264, 2158, 295, 288, 257, 707, 11, 718, 311, 584, 321, 652, 309, 257, 2158, 295, 568, 13, 16, 11, 550, 264, 17630], "temperature": 0.0, "avg_logprob": -0.09520140989327136, "compression_ratio": 1.5235602094240839, "no_speech_prob": 1.1295889635221101e-05}, {"id": 464, "seek": 367404, "start": 3688.6, "end": 3695.16, "text": " is they'll have five times as big an effect on the output value we compute. And well, what do we", "tokens": [307, 436, 603, 362, 1732, 1413, 382, 955, 364, 1802, 322, 264, 5598, 2158, 321, 14722, 13, 400, 731, 11, 437, 360, 321], "temperature": 0.0, "avg_logprob": -0.09520140989327136, "compression_ratio": 1.5235602094240839, "no_speech_prob": 1.1295889635221101e-05}, {"id": 465, "seek": 369516, "start": 3695.16, "end": 3707.72, "text": " compute? So we compute 1 plus 2.1. So that's 3.1. And we compute the max of 2.1 and 0 is 2.1. So", "tokens": [14722, 30, 407, 321, 14722, 502, 1804, 568, 13, 16, 13, 407, 300, 311, 805, 13, 16, 13, 400, 321, 14722, 264, 11469, 295, 568, 13, 16, 293, 1958, 307, 568, 13, 16, 13, 407], "temperature": 0.0, "avg_logprob": -0.09529071158551156, "compression_ratio": 1.630057803468208, "no_speech_prob": 4.9724967539077625e-05}, {"id": 466, "seek": 369516, "start": 3707.72, "end": 3714.2799999999997, "text": " we'll take the product of 2.1 and 3.1. And I calculate that in advance, as I can't really do", "tokens": [321, 603, 747, 264, 1674, 295, 568, 13, 16, 293, 805, 13, 16, 13, 400, 286, 8873, 300, 294, 7295, 11, 382, 286, 393, 380, 534, 360], "temperature": 0.0, "avg_logprob": -0.09529071158551156, "compression_ratio": 1.630057803468208, "no_speech_prob": 4.9724967539077625e-05}, {"id": 467, "seek": 369516, "start": 3714.2799999999997, "end": 3721.3999999999996, "text": " this arithmetic in my head. And the product of those two is 6.51. So it has gone up about by", "tokens": [341, 42973, 294, 452, 1378, 13, 400, 264, 1674, 295, 729, 732, 307, 1386, 13, 18682, 13, 407, 309, 575, 2780, 493, 466, 538], "temperature": 0.0, "avg_logprob": -0.09529071158551156, "compression_ratio": 1.630057803468208, "no_speech_prob": 4.9724967539077625e-05}, {"id": 468, "seek": 372140, "start": 3721.4, "end": 3729.7200000000003, "text": " 0.5. So we've multiplied my fiddly at by 0.1 by five times to work out the magnitude of the", "tokens": [1958, 13, 20, 13, 407, 321, 600, 17207, 452, 283, 14273, 356, 412, 538, 1958, 13, 16, 538, 1732, 1413, 281, 589, 484, 264, 15668, 295, 264], "temperature": 0.0, "avg_logprob": -0.14821363317555394, "compression_ratio": 1.3333333333333333, "no_speech_prob": 9.60302131716162e-05}, {"id": 469, "seek": 372140, "start": 3729.7200000000003, "end": 3739.88, "text": " effect of the output. Okay. So for this start, you know, before I did the case of, you know,", "tokens": [1802, 295, 264, 5598, 13, 1033, 13, 407, 337, 341, 722, 11, 291, 458, 11, 949, 286, 630, 264, 1389, 295, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.14821363317555394, "compression_ratio": 1.3333333333333333, "no_speech_prob": 9.60302131716162e-05}, {"id": 470, "seek": 373988, "start": 3739.88, "end": 3752.12, "text": " when we had one one in and one out here and multiple inns and one out here, the case that I", "tokens": [562, 321, 632, 472, 472, 294, 293, 472, 484, 510, 293, 3866, 294, 3695, 293, 472, 484, 510, 11, 264, 1389, 300, 286], "temperature": 0.0, "avg_logprob": -0.15127234391763178, "compression_ratio": 1.8012422360248448, "no_speech_prob": 7.174622169259237e-06}, {"id": 471, "seek": 373988, "start": 3752.12, "end": 3760.76, "text": " had actually dealt with is the case of when you have multiple outward branches, but that then turned", "tokens": [632, 767, 15991, 365, 307, 264, 1389, 295, 562, 291, 362, 3866, 26914, 14770, 11, 457, 300, 550, 3574], "temperature": 0.0, "avg_logprob": -0.15127234391763178, "compression_ratio": 1.8012422360248448, "no_speech_prob": 7.174622169259237e-06}, {"id": 472, "seek": 373988, "start": 3760.76, "end": 3768.6800000000003, "text": " up in the computation of y. So once you have multiple outward branches, what you're doing is your", "tokens": [493, 294, 264, 24903, 295, 288, 13, 407, 1564, 291, 362, 3866, 26914, 14770, 11, 437, 291, 434, 884, 307, 428], "temperature": 0.0, "avg_logprob": -0.15127234391763178, "compression_ratio": 1.8012422360248448, "no_speech_prob": 7.174622169259237e-06}, {"id": 473, "seek": 376868, "start": 3768.68, "end": 3783.08, "text": " summing. So that when you want to work out the dfdy, you've got a local gradient, you've got two", "tokens": [2408, 2810, 13, 407, 300, 562, 291, 528, 281, 589, 484, 264, 274, 69, 3173, 11, 291, 600, 658, 257, 2654, 16235, 11, 291, 600, 658, 732], "temperature": 0.0, "avg_logprob": -0.15340870943936435, "compression_ratio": 1.4402985074626866, "no_speech_prob": 1.2395076737448107e-05}, {"id": 474, "seek": 376868, "start": 3783.08, "end": 3791.8799999999997, "text": " upstream gradients. And you're working it out with respect to each of them as in the chain rule,", "tokens": [33915, 2771, 2448, 13, 400, 291, 434, 1364, 309, 484, 365, 3104, 281, 1184, 295, 552, 382, 294, 264, 5021, 4978, 11], "temperature": 0.0, "avg_logprob": -0.15340870943936435, "compression_ratio": 1.4402985074626866, "no_speech_prob": 1.2395076737448107e-05}, {"id": 475, "seek": 379188, "start": 3791.88, "end": 3802.36, "text": " and then you're summing them together to work out the impact at the end. Right. So we also saw", "tokens": [293, 550, 291, 434, 2408, 2810, 552, 1214, 281, 589, 484, 264, 2712, 412, 264, 917, 13, 1779, 13, 407, 321, 611, 1866], "temperature": 0.0, "avg_logprob": -0.09766474285641231, "compression_ratio": 1.5934065934065933, "no_speech_prob": 9.501990462013055e-06}, {"id": 476, "seek": 379188, "start": 3802.36, "end": 3810.84, "text": " some of the other node intuitions, which it's useful to have doing this. So when you have an addition,", "tokens": [512, 295, 264, 661, 9984, 16224, 626, 11, 597, 309, 311, 4420, 281, 362, 884, 341, 13, 407, 562, 291, 362, 364, 4500, 11], "temperature": 0.0, "avg_logprob": -0.09766474285641231, "compression_ratio": 1.5934065934065933, "no_speech_prob": 9.501990462013055e-06}, {"id": 477, "seek": 379188, "start": 3811.96, "end": 3820.44, "text": " that distributes the upstream gradient to each of the things the lowered. When you have max,", "tokens": [300, 4400, 1819, 264, 33915, 16235, 281, 1184, 295, 264, 721, 264, 28466, 13, 1133, 291, 362, 11469, 11], "temperature": 0.0, "avg_logprob": -0.09766474285641231, "compression_ratio": 1.5934065934065933, "no_speech_prob": 9.501990462013055e-06}, {"id": 478, "seek": 382044, "start": 3820.44, "end": 3827.0, "text": " it's like a routing node. So when you have max, you have the upstream gradient, and it goes to one", "tokens": [309, 311, 411, 257, 32722, 9984, 13, 407, 562, 291, 362, 11469, 11, 291, 362, 264, 33915, 16235, 11, 293, 309, 1709, 281, 472], "temperature": 0.0, "avg_logprob": -0.0749464614971264, "compression_ratio": 1.651685393258427, "no_speech_prob": 2.3519718524767086e-05}, {"id": 479, "seek": 382044, "start": 3827.0, "end": 3836.76, "text": " of the branches below it and the rest of them get no gradient. When you then have a multiplication,", "tokens": [295, 264, 14770, 2507, 309, 293, 264, 1472, 295, 552, 483, 572, 16235, 13, 1133, 291, 550, 362, 257, 27290, 11], "temperature": 0.0, "avg_logprob": -0.0749464614971264, "compression_ratio": 1.651685393258427, "no_speech_prob": 2.3519718524767086e-05}, {"id": 480, "seek": 382044, "start": 3836.76, "end": 3846.2000000000003, "text": " it has this effect of switching the gradient. So if you're taking three by two, the gradient on", "tokens": [309, 575, 341, 1802, 295, 16493, 264, 16235, 13, 407, 498, 291, 434, 1940, 1045, 538, 732, 11, 264, 16235, 322], "temperature": 0.0, "avg_logprob": -0.0749464614971264, "compression_ratio": 1.651685393258427, "no_speech_prob": 2.3519718524767086e-05}, {"id": 481, "seek": 384620, "start": 3846.2, "end": 3852.2799999999997, "text": " the two side is three, and on the three side is two. And if you think about in terms of how much", "tokens": [264, 732, 1252, 307, 1045, 11, 293, 322, 264, 1045, 1252, 307, 732, 13, 400, 498, 291, 519, 466, 294, 2115, 295, 577, 709], "temperature": 0.0, "avg_logprob": -0.0998250332075296, "compression_ratio": 1.6307053941908713, "no_speech_prob": 3.47872446582187e-05}, {"id": 482, "seek": 384620, "start": 3852.2799999999997, "end": 3858.04, "text": " effect you get from when you're doing this sort of wiggling, that totally makes sense, right? Because", "tokens": [1802, 291, 483, 490, 562, 291, 434, 884, 341, 1333, 295, 261, 24542, 11, 300, 3879, 1669, 2020, 11, 558, 30, 1436], "temperature": 0.0, "avg_logprob": -0.0998250332075296, "compression_ratio": 1.6307053941908713, "no_speech_prob": 3.47872446582187e-05}, {"id": 483, "seek": 384620, "start": 3858.04, "end": 3865.7999999999997, "text": " if you're multiplying another number by three, then any change here is going to be multiplied by three", "tokens": [498, 291, 434, 30955, 1071, 1230, 538, 1045, 11, 550, 604, 1319, 510, 307, 516, 281, 312, 17207, 538, 1045], "temperature": 0.0, "avg_logprob": -0.0998250332075296, "compression_ratio": 1.6307053941908713, "no_speech_prob": 3.47872446582187e-05}, {"id": 484, "seek": 386580, "start": 3865.8, "end": 3877.4, "text": " and vice versa. Okay. So this is the kind of computation graph that we want to use to work out", "tokens": [293, 11964, 25650, 13, 1033, 13, 407, 341, 307, 264, 733, 295, 24903, 4295, 300, 321, 528, 281, 764, 281, 589, 484], "temperature": 0.0, "avg_logprob": -0.09399356160845075, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.951755075424444e-05}, {"id": 485, "seek": 386580, "start": 3878.76, "end": 3885.96, "text": " derivatives in an automated computational fashion, which is the basis of the back propagation", "tokens": [33733, 294, 364, 18473, 28270, 6700, 11, 597, 307, 264, 5143, 295, 264, 646, 38377], "temperature": 0.0, "avg_logprob": -0.09399356160845075, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.951755075424444e-05}, {"id": 486, "seek": 386580, "start": 3885.96, "end": 3894.28, "text": " algorithm. But at that point, this is what we're doing, but there's still one mistake that we can make.", "tokens": [9284, 13, 583, 412, 300, 935, 11, 341, 307, 437, 321, 434, 884, 11, 457, 456, 311, 920, 472, 6146, 300, 321, 393, 652, 13], "temperature": 0.0, "avg_logprob": -0.09399356160845075, "compression_ratio": 1.5449735449735449, "no_speech_prob": 1.951755075424444e-05}, {"id": 487, "seek": 389428, "start": 3894.28, "end": 3900.52, "text": " It would be wrong for us to sort of say, okay, well, first of all, we want to work out DSDB.", "tokens": [467, 576, 312, 2085, 337, 505, 281, 1333, 295, 584, 11, 1392, 11, 731, 11, 700, 295, 439, 11, 321, 528, 281, 589, 484, 15816, 27735, 13], "temperature": 0.0, "avg_logprob": -0.11144018776809113, "compression_ratio": 1.622093023255814, "no_speech_prob": 6.196126196300611e-05}, {"id": 488, "seek": 389428, "start": 3900.52, "end": 3909.4, "text": " So look, we can start up here. We can propagate our upstream errors, work out local gradients,", "tokens": [407, 574, 11, 321, 393, 722, 493, 510, 13, 492, 393, 48256, 527, 33915, 13603, 11, 589, 484, 2654, 2771, 2448, 11], "temperature": 0.0, "avg_logprob": -0.11144018776809113, "compression_ratio": 1.622093023255814, "no_speech_prob": 6.196126196300611e-05}, {"id": 489, "seek": 389428, "start": 3909.4, "end": 3919.8, "text": " upstream error, local gradient, and keep all the way down and get the DSDB down here. Okay,", "tokens": [33915, 6713, 11, 2654, 16235, 11, 293, 1066, 439, 264, 636, 760, 293, 483, 264, 15816, 27735, 760, 510, 13, 1033, 11], "temperature": 0.0, "avg_logprob": -0.11144018776809113, "compression_ratio": 1.622093023255814, "no_speech_prob": 6.196126196300611e-05}, {"id": 490, "seek": 391980, "start": 3919.8, "end": 3927.96, "text": " next we want to do it for DSDW. Let's just run it all over again. Because if we do that, we'd be", "tokens": [958, 321, 528, 281, 360, 309, 337, 15816, 35, 54, 13, 961, 311, 445, 1190, 309, 439, 670, 797, 13, 1436, 498, 321, 360, 300, 11, 321, 1116, 312], "temperature": 0.0, "avg_logprob": -0.10807102109179084, "compression_ratio": 1.7696969696969698, "no_speech_prob": 2.543815026001539e-05}, {"id": 491, "seek": 391980, "start": 3927.96, "end": 3936.2000000000003, "text": " doing repeated computation, as I showed in the first half, that this term is the same both times,", "tokens": [884, 10477, 24903, 11, 382, 286, 4712, 294, 264, 700, 1922, 11, 300, 341, 1433, 307, 264, 912, 1293, 1413, 11], "temperature": 0.0, "avg_logprob": -0.10807102109179084, "compression_ratio": 1.7696969696969698, "no_speech_prob": 2.543815026001539e-05}, {"id": 492, "seek": 391980, "start": 3936.2000000000003, "end": 3942.44, "text": " this term is the same both times, this term is the same both times, that only the bits at the end", "tokens": [341, 1433, 307, 264, 912, 1293, 1413, 11, 341, 1433, 307, 264, 912, 1293, 1413, 11, 300, 787, 264, 9239, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.10807102109179084, "compression_ratio": 1.7696969696969698, "no_speech_prob": 2.543815026001539e-05}, {"id": 493, "seek": 394244, "start": 3942.44, "end": 3950.84, "text": " differ. So what we want to do is avoid duplicated computation and compute all the gradients", "tokens": [743, 13, 407, 437, 321, 528, 281, 360, 307, 5042, 1581, 564, 3587, 24903, 293, 14722, 439, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.10983357826868693, "compression_ratio": 1.5591397849462365, "no_speech_prob": 6.693742034258321e-05}, {"id": 494, "seek": 394244, "start": 3953.0, "end": 3960.12, "text": " that we're going to need, successively, so that we only do them once. And so that was analogous", "tokens": [300, 321, 434, 516, 281, 643, 11, 2245, 3413, 11, 370, 300, 321, 787, 360, 552, 1564, 13, 400, 370, 300, 390, 16660, 563], "temperature": 0.0, "avg_logprob": -0.10983357826868693, "compression_ratio": 1.5591397849462365, "no_speech_prob": 6.693742034258321e-05}, {"id": 495, "seek": 394244, "start": 3960.12, "end": 3968.12, "text": " to when I introduced this delta variable when we computed gradients by hand. So starting off here from", "tokens": [281, 562, 286, 7268, 341, 8289, 7006, 562, 321, 40610, 2771, 2448, 538, 1011, 13, 407, 2891, 766, 510, 490], "temperature": 0.0, "avg_logprob": -0.10983357826868693, "compression_ratio": 1.5591397849462365, "no_speech_prob": 6.693742034258321e-05}, {"id": 496, "seek": 396812, "start": 3968.12, "end": 3981.4, "text": " DSD, we're starting off here with DSDS is one. We then want to one time compute gradients in the", "tokens": [15816, 35, 11, 321, 434, 2891, 766, 510, 365, 15816, 11844, 307, 472, 13, 492, 550, 528, 281, 472, 565, 14722, 2771, 2448, 294, 264], "temperature": 0.0, "avg_logprob": -0.1725773261143611, "compression_ratio": 1.5, "no_speech_prob": 0.00011392927262932062}, {"id": 497, "seek": 396812, "start": 3981.4, "end": 3988.2799999999997, "text": " green here, one time compute the gradient and green here, that's all common work. Then we're", "tokens": [3092, 510, 11, 472, 565, 14722, 264, 16235, 293, 3092, 510, 11, 300, 311, 439, 2689, 589, 13, 1396, 321, 434], "temperature": 0.0, "avg_logprob": -0.1725773261143611, "compression_ratio": 1.5, "no_speech_prob": 0.00011392927262932062}, {"id": 498, "seek": 398828, "start": 3988.28, "end": 3998.6000000000004, "text": " going to take the local gradient for DZDB and multiply that by the upstream gradient to have", "tokens": [516, 281, 747, 264, 2654, 16235, 337, 413, 57, 27735, 293, 12972, 300, 538, 264, 33915, 16235, 281, 362], "temperature": 0.0, "avg_logprob": -0.11911281295444655, "compression_ratio": 1.5203252032520325, "no_speech_prob": 1.2994755707040895e-05}, {"id": 499, "seek": 398828, "start": 3998.6000000000004, "end": 4007.48, "text": " worked out DSDB. And then we're going to take the same upstream gradient and then work out the", "tokens": [2732, 484, 15816, 27735, 13, 400, 550, 321, 434, 516, 281, 747, 264, 912, 33915, 16235, 293, 550, 589, 484, 264], "temperature": 0.0, "avg_logprob": -0.11911281295444655, "compression_ratio": 1.5203252032520325, "no_speech_prob": 1.2994755707040895e-05}, {"id": 500, "seek": 400748, "start": 4007.48, "end": 4018.68, "text": " local gradient here and then propagate that down to give us DSDW. So the end result is we want to", "tokens": [2654, 16235, 510, 293, 550, 48256, 300, 760, 281, 976, 505, 15816, 35, 54, 13, 407, 264, 917, 1874, 307, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.120542601933555, "compression_ratio": 1.6746987951807228, "no_speech_prob": 8.385835826629773e-06}, {"id": 501, "seek": 400748, "start": 4018.68, "end": 4026.6, "text": " systematically work to forward computation forward in the graph and backward computation,", "tokens": [39531, 589, 281, 2128, 24903, 2128, 294, 264, 4295, 293, 23897, 24903, 11], "temperature": 0.0, "avg_logprob": -0.120542601933555, "compression_ratio": 1.6746987951807228, "no_speech_prob": 8.385835826629773e-06}, {"id": 502, "seek": 400748, "start": 4026.6, "end": 4034.6, "text": " back propagation, backward in the graph in a way that we do things efficiently. So this is", "tokens": [646, 38377, 11, 23897, 294, 264, 4295, 294, 257, 636, 300, 321, 360, 721, 19621, 13, 407, 341, 307], "temperature": 0.0, "avg_logprob": -0.120542601933555, "compression_ratio": 1.6746987951807228, "no_speech_prob": 8.385835826629773e-06}, {"id": 503, "seek": 403460, "start": 4034.6, "end": 4044.7599999999998, "text": " the general form of the algorithm which works for an arbitrary computation graph. So at the end", "tokens": [264, 2674, 1254, 295, 264, 9284, 597, 1985, 337, 364, 23211, 24903, 4295, 13, 407, 412, 264, 917], "temperature": 0.0, "avg_logprob": -0.15145427915785048, "compression_ratio": 1.4233576642335766, "no_speech_prob": 7.128646393539384e-05}, {"id": 504, "seek": 403460, "start": 4044.7599999999998, "end": 4056.68, "text": " of the day, we've got a single scalar output Z and then we have inputs and parameters which compute", "tokens": [295, 264, 786, 11, 321, 600, 658, 257, 2167, 39684, 5598, 1176, 293, 550, 321, 362, 15743, 293, 9834, 597, 14722], "temperature": 0.0, "avg_logprob": -0.15145427915785048, "compression_ratio": 1.4233576642335766, "no_speech_prob": 7.128646393539384e-05}, {"id": 505, "seek": 405668, "start": 4056.68, "end": 4066.04, "text": " Z. And so once we have this computation graph and I added in this funky extra arrow here to make", "tokens": [1176, 13, 400, 370, 1564, 321, 362, 341, 24903, 4295, 293, 286, 3869, 294, 341, 33499, 2857, 11610, 510, 281, 652], "temperature": 0.0, "avg_logprob": -0.11101982934134347, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.00012921048619318753}, {"id": 506, "seek": 405668, "start": 4066.04, "end": 4073.96, "text": " it a more general computation graph, well we can always say that we can work out a starting point,", "tokens": [309, 257, 544, 2674, 24903, 4295, 11, 731, 321, 393, 1009, 584, 300, 321, 393, 589, 484, 257, 2891, 935, 11], "temperature": 0.0, "avg_logprob": -0.11101982934134347, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.00012921048619318753}, {"id": 507, "seek": 405668, "start": 4073.96, "end": 4080.04, "text": " something that doesn't depend on anything. So in this case both of these bottom two nodes don't", "tokens": [746, 300, 1177, 380, 5672, 322, 1340, 13, 407, 294, 341, 1389, 1293, 295, 613, 2767, 732, 13891, 500, 380], "temperature": 0.0, "avg_logprob": -0.11101982934134347, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.00012921048619318753}, {"id": 508, "seek": 408004, "start": 4080.04, "end": 4087.32, "text": " depend on anything else. So we can start with them and we can start to compute forward. We can compute", "tokens": [5672, 322, 1340, 1646, 13, 407, 321, 393, 722, 365, 552, 293, 321, 393, 722, 281, 14722, 2128, 13, 492, 393, 14722], "temperature": 0.0, "avg_logprob": -0.09669820921761649, "compression_ratio": 1.686046511627907, "no_speech_prob": 5.9186106227571145e-05}, {"id": 509, "seek": 408004, "start": 4087.32, "end": 4093.16, "text": " values for all of these sort of second row from the bottom nodes and then we're able to compute", "tokens": [4190, 337, 439, 295, 613, 1333, 295, 1150, 5386, 490, 264, 2767, 13891, 293, 550, 321, 434, 1075, 281, 14722], "temperature": 0.0, "avg_logprob": -0.09669820921761649, "compression_ratio": 1.686046511627907, "no_speech_prob": 5.9186106227571145e-05}, {"id": 510, "seek": 408004, "start": 4095.0, "end": 4102.76, "text": " the third lens up. So we can have a top logical sort of the nodes based on the dependencies", "tokens": [264, 2636, 6765, 493, 13, 407, 321, 393, 362, 257, 1192, 14978, 1333, 295, 264, 13891, 2361, 322, 264, 36606], "temperature": 0.0, "avg_logprob": -0.09669820921761649, "compression_ratio": 1.686046511627907, "no_speech_prob": 5.9186106227571145e-05}, {"id": 511, "seek": 410276, "start": 4102.76, "end": 4111.64, "text": " in this directed graph and we can compute the value of each node given some subset of its pre-decesses", "tokens": [294, 341, 12898, 4295, 293, 321, 393, 14722, 264, 2158, 295, 1184, 9984, 2212, 512, 25993, 295, 1080, 659, 12, 1479, 780, 279], "temperature": 0.0, "avg_logprob": -0.1580707493113048, "compression_ratio": 1.5869565217391304, "no_speech_prob": 2.2819780497229658e-05}, {"id": 512, "seek": 410276, "start": 4111.64, "end": 4118.2, "text": " which it depends on. And so doing that as referred to as the forward propagation phase and gives us", "tokens": [597, 309, 5946, 322, 13, 400, 370, 884, 300, 382, 10839, 281, 382, 264, 2128, 38377, 5574, 293, 2709, 505], "temperature": 0.0, "avg_logprob": -0.1580707493113048, "compression_ratio": 1.5869565217391304, "no_speech_prob": 2.2819780497229658e-05}, {"id": 513, "seek": 410276, "start": 4118.2, "end": 4125.16, "text": " a computation of the scalar output Z using our current parameters and our current inputs.", "tokens": [257, 24903, 295, 264, 39684, 5598, 1176, 1228, 527, 2190, 9834, 293, 527, 2190, 15743, 13], "temperature": 0.0, "avg_logprob": -0.1580707493113048, "compression_ratio": 1.5869565217391304, "no_speech_prob": 2.2819780497229658e-05}, {"id": 514, "seek": 412516, "start": 4125.16, "end": 4133.72, "text": " And so then after that we run back propagation. So for back propagation we initialize the output", "tokens": [400, 370, 550, 934, 300, 321, 1190, 646, 38377, 13, 407, 337, 646, 38377, 321, 5883, 1125, 264, 5598], "temperature": 0.0, "avg_logprob": -0.14109047253926596, "compression_ratio": 1.574585635359116, "no_speech_prob": 4.395957512315363e-05}, {"id": 515, "seek": 412516, "start": 4133.72, "end": 4144.84, "text": " gradient, DZ, DZ as one and then we visit nodes in the reverse order of the top logical sort", "tokens": [16235, 11, 413, 57, 11, 413, 57, 382, 472, 293, 550, 321, 3441, 13891, 294, 264, 9943, 1668, 295, 264, 1192, 14978, 1333], "temperature": 0.0, "avg_logprob": -0.14109047253926596, "compression_ratio": 1.574585635359116, "no_speech_prob": 4.395957512315363e-05}, {"id": 516, "seek": 412516, "start": 4144.84, "end": 4152.76, "text": " and we compute the gradients downward. And so our recipe is that for each node as we head down,", "tokens": [293, 321, 14722, 264, 2771, 2448, 24805, 13, 400, 370, 527, 6782, 307, 300, 337, 1184, 9984, 382, 321, 1378, 760, 11], "temperature": 0.0, "avg_logprob": -0.14109047253926596, "compression_ratio": 1.574585635359116, "no_speech_prob": 4.395957512315363e-05}, {"id": 517, "seek": 415276, "start": 4152.76, "end": 4161.4800000000005, "text": " we're going to compute the gradient of the node with respect to its successes, the things that it", "tokens": [321, 434, 516, 281, 14722, 264, 16235, 295, 264, 9984, 365, 3104, 281, 1080, 26101, 11, 264, 721, 300, 309], "temperature": 0.0, "avg_logprob": -0.08871765975113753, "compression_ratio": 1.7798165137614679, "no_speech_prob": 3.263408143538982e-05}, {"id": 518, "seek": 415276, "start": 4161.4800000000005, "end": 4169.56, "text": " feeds into. And how we compute that gradient is using this chain rule that we've looked at. So", "tokens": [23712, 666, 13, 400, 577, 321, 14722, 300, 16235, 307, 1228, 341, 5021, 4978, 300, 321, 600, 2956, 412, 13, 407], "temperature": 0.0, "avg_logprob": -0.08871765975113753, "compression_ratio": 1.7798165137614679, "no_speech_prob": 3.263408143538982e-05}, {"id": 519, "seek": 415276, "start": 4169.56, "end": 4175.96, "text": " this is sort of the generalized form of the chain rule where we have multiple outputs. And so we're", "tokens": [341, 307, 1333, 295, 264, 44498, 1254, 295, 264, 5021, 4978, 689, 321, 362, 3866, 23930, 13, 400, 370, 321, 434], "temperature": 0.0, "avg_logprob": -0.08871765975113753, "compression_ratio": 1.7798165137614679, "no_speech_prob": 3.263408143538982e-05}, {"id": 520, "seek": 415276, "start": 4175.96, "end": 4181.64, "text": " summing over the different outputs. And then for each output we're computing the product of the", "tokens": [2408, 2810, 670, 264, 819, 23930, 13, 400, 550, 337, 1184, 5598, 321, 434, 15866, 264, 1674, 295, 264], "temperature": 0.0, "avg_logprob": -0.08871765975113753, "compression_ratio": 1.7798165137614679, "no_speech_prob": 3.263408143538982e-05}, {"id": 521, "seek": 418164, "start": 4181.64, "end": 4189.0, "text": " upstream gradient and the local gradient with respect to that node. And so we head downwards.", "tokens": [33915, 16235, 293, 264, 2654, 16235, 365, 3104, 281, 300, 9984, 13, 400, 370, 321, 1378, 39880, 13], "temperature": 0.0, "avg_logprob": -0.08193693463764494, "compression_ratio": 1.7017543859649122, "no_speech_prob": 3.477474820101634e-05}, {"id": 522, "seek": 418164, "start": 4189.0, "end": 4197.400000000001, "text": " And we continue down the reverse top logical sort order and we work out the gradient with respect", "tokens": [400, 321, 2354, 760, 264, 9943, 1192, 14978, 1333, 1668, 293, 321, 589, 484, 264, 16235, 365, 3104], "temperature": 0.0, "avg_logprob": -0.08193693463764494, "compression_ratio": 1.7017543859649122, "no_speech_prob": 3.477474820101634e-05}, {"id": 523, "seek": 418164, "start": 4197.400000000001, "end": 4208.4400000000005, "text": " to each variable in this graph. And so it hopefully looks kind of intuitive looking at this picture", "tokens": [281, 1184, 7006, 294, 341, 4295, 13, 400, 370, 309, 4696, 1542, 733, 295, 21769, 1237, 412, 341, 3036], "temperature": 0.0, "avg_logprob": -0.08193693463764494, "compression_ratio": 1.7017543859649122, "no_speech_prob": 3.477474820101634e-05}, {"id": 524, "seek": 420844, "start": 4208.44, "end": 4218.12, "text": " that if you think of it like this, the big oak complexity of forward propagation and backward", "tokens": [300, 498, 291, 519, 295, 309, 411, 341, 11, 264, 955, 31322, 14024, 295, 2128, 38377, 293, 23897], "temperature": 0.0, "avg_logprob": -0.13280184819148136, "compression_ratio": 1.5737704918032787, "no_speech_prob": 6.601879431400448e-05}, {"id": 525, "seek": 420844, "start": 4218.12, "end": 4226.04, "text": " propagation is the same. Right. In both cases you're doing a linear pass through all of these nodes", "tokens": [38377, 307, 264, 912, 13, 1779, 13, 682, 1293, 3331, 291, 434, 884, 257, 8213, 1320, 807, 439, 295, 613, 13891], "temperature": 0.0, "avg_logprob": -0.13280184819148136, "compression_ratio": 1.5737704918032787, "no_speech_prob": 6.601879431400448e-05}, {"id": 526, "seek": 420844, "start": 4226.04, "end": 4233.24, "text": " and calculating values given predecessors and then values given successors. I mean you have to", "tokens": [293, 28258, 4190, 2212, 24874, 45700, 293, 550, 4190, 2212, 2245, 830, 13, 286, 914, 291, 362, 281], "temperature": 0.0, "avg_logprob": -0.13280184819148136, "compression_ratio": 1.5737704918032787, "no_speech_prob": 6.601879431400448e-05}, {"id": 527, "seek": 423324, "start": 4233.24, "end": 4240.2, "text": " do a little bit more work is for working out the gradients sort of as shown by this chain rule", "tokens": [360, 257, 707, 857, 544, 589, 307, 337, 1364, 484, 264, 2771, 2448, 1333, 295, 382, 4898, 538, 341, 5021, 4978], "temperature": 0.0, "avg_logprob": -0.09313205192829001, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.825370342587121e-05}, {"id": 528, "seek": 423324, "start": 4240.2, "end": 4245.639999999999, "text": " that it's the same big oak complexity. So if somehow you're implementing stuff for yourself rather", "tokens": [300, 309, 311, 264, 912, 955, 31322, 14024, 13, 407, 498, 6063, 291, 434, 18114, 1507, 337, 1803, 2831], "temperature": 0.0, "avg_logprob": -0.09313205192829001, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.825370342587121e-05}, {"id": 529, "seek": 423324, "start": 4245.639999999999, "end": 4252.12, "text": " than relying on the software and you're calculating the gradients of a different order of complexity", "tokens": [813, 24140, 322, 264, 4722, 293, 291, 434, 28258, 264, 2771, 2448, 295, 257, 819, 1668, 295, 14024], "temperature": 0.0, "avg_logprob": -0.09313205192829001, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.825370342587121e-05}, {"id": 530, "seek": 423324, "start": 4252.12, "end": 4257.719999999999, "text": " of forward propagation, it means that you're doing something wrong. You're doing repeated work that", "tokens": [295, 2128, 38377, 11, 309, 1355, 300, 291, 434, 884, 746, 2085, 13, 509, 434, 884, 10477, 589, 300], "temperature": 0.0, "avg_logprob": -0.09313205192829001, "compression_ratio": 1.720524017467249, "no_speech_prob": 4.825370342587121e-05}, {"id": 531, "seek": 425772, "start": 4257.72, "end": 4264.92, "text": " you shouldn't have to do. Okay. So this algorithm works for a completely arbitrary", "tokens": [291, 4659, 380, 362, 281, 360, 13, 1033, 13, 407, 341, 9284, 1985, 337, 257, 2584, 23211], "temperature": 0.0, "avg_logprob": -0.15063563788809428, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.3670105040073395e-05}, {"id": 532, "seek": 425772, "start": 4264.92, "end": 4271.320000000001, "text": " computation graph, any directed a cyclic graph. You can apply this algorithm.", "tokens": [24903, 4295, 11, 604, 12898, 257, 38154, 1050, 4295, 13, 509, 393, 3079, 341, 9284, 13], "temperature": 0.0, "avg_logprob": -0.15063563788809428, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.3670105040073395e-05}, {"id": 533, "seek": 425772, "start": 4272.360000000001, "end": 4278.92, "text": " In general, what we find is that we build neural networks that have a regular layer structure.", "tokens": [682, 2674, 11, 437, 321, 915, 307, 300, 321, 1322, 18161, 9590, 300, 362, 257, 3890, 4583, 3877, 13], "temperature": 0.0, "avg_logprob": -0.15063563788809428, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.3670105040073395e-05}, {"id": 534, "seek": 425772, "start": 4278.92, "end": 4285.16, "text": " So we have things like a vector of inputs and then that's multiplied by matrix. It's transformed", "tokens": [407, 321, 362, 721, 411, 257, 8062, 295, 15743, 293, 550, 300, 311, 17207, 538, 8141, 13, 467, 311, 16894], "temperature": 0.0, "avg_logprob": -0.15063563788809428, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.3670105040073395e-05}, {"id": 535, "seek": 428516, "start": 4285.16, "end": 4291.08, "text": " into another vector which might be multiplied by another matrix or some with another matrix", "tokens": [666, 1071, 8062, 597, 1062, 312, 17207, 538, 1071, 8141, 420, 512, 365, 1071, 8141], "temperature": 0.0, "avg_logprob": -0.08606032282114029, "compression_ratio": 1.5869565217391304, "no_speech_prob": 7.028032996458933e-05}, {"id": 536, "seek": 428516, "start": 4291.08, "end": 4297.5599999999995, "text": " or something. Right. So once we're using that kind of regular layer structure, we can then parallelize", "tokens": [420, 746, 13, 1779, 13, 407, 1564, 321, 434, 1228, 300, 733, 295, 3890, 4583, 3877, 11, 321, 393, 550, 8952, 1125], "temperature": 0.0, "avg_logprob": -0.08606032282114029, "compression_ratio": 1.5869565217391304, "no_speech_prob": 7.028032996458933e-05}, {"id": 537, "seek": 428516, "start": 4297.5599999999995, "end": 4308.04, "text": " the computation by working out the gradients in terms of Jacobians of vectors and matrices and do", "tokens": [264, 24903, 538, 1364, 484, 264, 2771, 2448, 294, 2115, 295, 14117, 2567, 295, 18875, 293, 32284, 293, 360], "temperature": 0.0, "avg_logprob": -0.08606032282114029, "compression_ratio": 1.5869565217391304, "no_speech_prob": 7.028032996458933e-05}, {"id": 538, "seek": 430804, "start": 4308.04, "end": 4316.28, "text": " things in parallel much more efficiently. Okay. So doing this is then referred to as automatic", "tokens": [721, 294, 8952, 709, 544, 19621, 13, 1033, 13, 407, 884, 341, 307, 550, 10839, 281, 382, 12509], "temperature": 0.0, "avg_logprob": -0.1115203062693278, "compression_ratio": 1.5052631578947369, "no_speech_prob": 5.648971637128852e-05}, {"id": 539, "seek": 430804, "start": 4316.28, "end": 4325.4, "text": " differentiation. And so essentially if you know the computation graph, you should be able to have", "tokens": [38902, 13, 400, 370, 4476, 498, 291, 458, 264, 24903, 4295, 11, 291, 820, 312, 1075, 281, 362], "temperature": 0.0, "avg_logprob": -0.1115203062693278, "compression_ratio": 1.5052631578947369, "no_speech_prob": 5.648971637128852e-05}, {"id": 540, "seek": 430804, "start": 4325.4, "end": 4335.56, "text": " your computer, clever computer system work out what the derivatives of everything is and then", "tokens": [428, 3820, 11, 13494, 3820, 1185, 589, 484, 437, 264, 33733, 295, 1203, 307, 293, 550], "temperature": 0.0, "avg_logprob": -0.1115203062693278, "compression_ratio": 1.5052631578947369, "no_speech_prob": 5.648971637128852e-05}, {"id": 541, "seek": 433556, "start": 4335.56, "end": 4343.96, "text": " apply back propagation to work out how to update the parameters and learn. And there's actually", "tokens": [3079, 646, 38377, 281, 589, 484, 577, 281, 5623, 264, 9834, 293, 1466, 13, 400, 456, 311, 767], "temperature": 0.0, "avg_logprob": -0.097411148250103, "compression_ratio": 1.4921465968586387, "no_speech_prob": 3.761582775041461e-05}, {"id": 542, "seek": 433556, "start": 4343.96, "end": 4353.160000000001, "text": " a sort of an interesting sort of thing of how history has gone backwards here, which I'll just", "tokens": [257, 1333, 295, 364, 1880, 1333, 295, 551, 295, 577, 2503, 575, 2780, 12204, 510, 11, 597, 286, 603, 445], "temperature": 0.0, "avg_logprob": -0.097411148250103, "compression_ratio": 1.4921465968586387, "no_speech_prob": 3.761582775041461e-05}, {"id": 543, "seek": 433556, "start": 4353.160000000001, "end": 4363.56, "text": " note. So some of you might be familiar with symbolic computation packages. So those are things", "tokens": [3637, 13, 407, 512, 295, 291, 1062, 312, 4963, 365, 25755, 24903, 17401, 13, 407, 729, 366, 721], "temperature": 0.0, "avg_logprob": -0.097411148250103, "compression_ratio": 1.4921465968586387, "no_speech_prob": 3.761582775041461e-05}, {"id": 544, "seek": 436356, "start": 4363.56, "end": 4371.96, "text": " like mathematical. So mathematical, you can give it a symbolic form of a computation and then it", "tokens": [411, 18894, 13, 407, 18894, 11, 291, 393, 976, 309, 257, 25755, 1254, 295, 257, 24903, 293, 550, 309], "temperature": 0.0, "avg_logprob": -0.09343060482753797, "compression_ratio": 1.9408866995073892, "no_speech_prob": 6.498342554550618e-05}, {"id": 545, "seek": 436356, "start": 4371.96, "end": 4378.360000000001, "text": " can work out derivatives for you. So it should be the case that if you give a complete symbolic form", "tokens": [393, 589, 484, 33733, 337, 291, 13, 407, 309, 820, 312, 264, 1389, 300, 498, 291, 976, 257, 3566, 25755, 1254], "temperature": 0.0, "avg_logprob": -0.09343060482753797, "compression_ratio": 1.9408866995073892, "no_speech_prob": 6.498342554550618e-05}, {"id": 546, "seek": 436356, "start": 4378.360000000001, "end": 4386.360000000001, "text": " of a computation graph, then it should be able to work out all the derivatives for you and you never", "tokens": [295, 257, 24903, 4295, 11, 550, 309, 820, 312, 1075, 281, 589, 484, 439, 264, 33733, 337, 291, 293, 291, 1128], "temperature": 0.0, "avg_logprob": -0.09343060482753797, "compression_ratio": 1.9408866995073892, "no_speech_prob": 6.498342554550618e-05}, {"id": 547, "seek": 436356, "start": 4386.360000000001, "end": 4392.68, "text": " have to work out a derivative by hand whatsoever. And that was actually attempted in the famous", "tokens": [362, 281, 589, 484, 257, 13760, 538, 1011, 17076, 13, 400, 300, 390, 767, 18997, 294, 264, 4618], "temperature": 0.0, "avg_logprob": -0.09343060482753797, "compression_ratio": 1.9408866995073892, "no_speech_prob": 6.498342554550618e-05}, {"id": 548, "seek": 439268, "start": 4392.68, "end": 4399.4800000000005, "text": " deep learning library called Fiano, which came out of Joshua Bendios group at the University of", "tokens": [2452, 2539, 6405, 1219, 479, 6254, 11, 597, 1361, 484, 295, 24005, 32451, 2717, 1594, 412, 264, 3535, 295], "temperature": 0.0, "avg_logprob": -0.1916743645301232, "compression_ratio": 1.44, "no_speech_prob": 0.0001291846128879115}, {"id": 549, "seek": 439268, "start": 4399.4800000000005, "end": 4408.280000000001, "text": " Montreal that had a compiler that did that kind of symbolic manipulation. But you know somehow", "tokens": [34180, 300, 632, 257, 31958, 300, 630, 300, 733, 295, 25755, 26475, 13, 583, 291, 458, 6063], "temperature": 0.0, "avg_logprob": -0.1916743645301232, "compression_ratio": 1.44, "no_speech_prob": 0.0001291846128879115}, {"id": 550, "seek": 439268, "start": 4408.280000000001, "end": 4417.08, "text": " that sort of proved a little bit too hard a road to follow. I imagine it actually might come back", "tokens": [300, 1333, 295, 14617, 257, 707, 857, 886, 1152, 257, 3060, 281, 1524, 13, 286, 3811, 309, 767, 1062, 808, 646], "temperature": 0.0, "avg_logprob": -0.1916743645301232, "compression_ratio": 1.44, "no_speech_prob": 0.0001291846128879115}, {"id": 551, "seek": 441708, "start": 4417.08, "end": 4424.92, "text": " again in the future. And so for modern deep learning frameworks, which includes both TensorFlow", "tokens": [797, 294, 264, 2027, 13, 400, 370, 337, 4363, 2452, 2539, 29834, 11, 597, 5974, 1293, 37624], "temperature": 0.0, "avg_logprob": -0.08604606986045837, "compression_ratio": 1.425, "no_speech_prob": 8.458687807433307e-05}, {"id": 552, "seek": 441708, "start": 4424.92, "end": 4436.12, "text": " or PyTorch, they do 90% of this computation of automatic differentiation for you, but they don't", "tokens": [420, 9953, 51, 284, 339, 11, 436, 360, 4289, 4, 295, 341, 24903, 295, 12509, 38902, 337, 291, 11, 457, 436, 500, 380], "temperature": 0.0, "avg_logprob": -0.08604606986045837, "compression_ratio": 1.425, "no_speech_prob": 8.458687807433307e-05}, {"id": 553, "seek": 441708, "start": 4436.12, "end": 4444.5199999999995, "text": " actually symbolically compute derivatives. So for each particular node or layer of your deep", "tokens": [767, 5986, 984, 14722, 33733, 13, 407, 337, 1184, 1729, 9984, 420, 4583, 295, 428, 2452], "temperature": 0.0, "avg_logprob": -0.08604606986045837, "compression_ratio": 1.425, "no_speech_prob": 8.458687807433307e-05}, {"id": 554, "seek": 444452, "start": 4444.52, "end": 4455.320000000001, "text": " learning system, somebody, either you or the person who wrote that layer, has handwritten the", "tokens": [2539, 1185, 11, 2618, 11, 2139, 291, 420, 264, 954, 567, 4114, 300, 4583, 11, 575, 1011, 26859, 264], "temperature": 0.0, "avg_logprob": -0.11337091773748398, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.0001790774695109576}, {"id": 555, "seek": 444452, "start": 4455.320000000001, "end": 4462.120000000001, "text": " local derivatives. But then everything from that point on, the sort of the taking, doing the", "tokens": [2654, 33733, 13, 583, 550, 1203, 490, 300, 935, 322, 11, 264, 1333, 295, 264, 1940, 11, 884, 264], "temperature": 0.0, "avg_logprob": -0.11337091773748398, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.0001790774695109576}, {"id": 556, "seek": 444452, "start": 4462.120000000001, "end": 4469.0, "text": " chain rule of combining upstream gradients with local gradients to work out downstream gradients,", "tokens": [5021, 4978, 295, 21928, 33915, 2771, 2448, 365, 2654, 2771, 2448, 281, 589, 484, 30621, 2771, 2448, 11], "temperature": 0.0, "avg_logprob": -0.11337091773748398, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.0001790774695109576}, {"id": 557, "seek": 446900, "start": 4469.0, "end": 4474.6, "text": " that's then all being done automatically for back propagation on the computation graph.", "tokens": [300, 311, 550, 439, 885, 1096, 6772, 337, 646, 38377, 322, 264, 24903, 4295, 13], "temperature": 0.0, "avg_logprob": -0.09546608983734507, "compression_ratio": 1.8265306122448979, "no_speech_prob": 5.5572014389326796e-05}, {"id": 558, "seek": 446900, "start": 4475.8, "end": 4482.68, "text": " And so that what that means is for a whole neural network, you have a computation graph,", "tokens": [400, 370, 300, 437, 300, 1355, 307, 337, 257, 1379, 18161, 3209, 11, 291, 362, 257, 24903, 4295, 11], "temperature": 0.0, "avg_logprob": -0.09546608983734507, "compression_ratio": 1.8265306122448979, "no_speech_prob": 5.5572014389326796e-05}, {"id": 559, "seek": 446900, "start": 4482.68, "end": 4490.04, "text": " and it's going to have a forward pass and a backward pass. And so for the forward pass,", "tokens": [293, 309, 311, 516, 281, 362, 257, 2128, 1320, 293, 257, 23897, 1320, 13, 400, 370, 337, 264, 2128, 1320, 11], "temperature": 0.0, "avg_logprob": -0.09546608983734507, "compression_ratio": 1.8265306122448979, "no_speech_prob": 5.5572014389326796e-05}, {"id": 560, "seek": 446900, "start": 4490.04, "end": 4496.68, "text": " you're topologically sorting the nodes based on their dependencies and the computation graph.", "tokens": [291, 434, 1192, 17157, 32411, 264, 13891, 2361, 322, 641, 36606, 293, 264, 24903, 4295, 13], "temperature": 0.0, "avg_logprob": -0.09546608983734507, "compression_ratio": 1.8265306122448979, "no_speech_prob": 5.5572014389326796e-05}, {"id": 561, "seek": 449668, "start": 4496.68, "end": 4504.76, "text": " And then for each node, you're running forward, the forward computation on that node. And then", "tokens": [400, 550, 337, 1184, 9984, 11, 291, 434, 2614, 2128, 11, 264, 2128, 24903, 322, 300, 9984, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.12217539129122881, "compression_ratio": 1.8974358974358974, "no_speech_prob": 5.647454599966295e-05}, {"id": 562, "seek": 449668, "start": 4504.76, "end": 4511.320000000001, "text": " for backward propagation, you're reversing the topological sort of the graph. And then for each node", "tokens": [337, 23897, 38377, 11, 291, 434, 14582, 278, 264, 1192, 4383, 1333, 295, 264, 4295, 13, 400, 550, 337, 1184, 9984], "temperature": 0.0, "avg_logprob": -0.12217539129122881, "compression_ratio": 1.8974358974358974, "no_speech_prob": 5.647454599966295e-05}, {"id": 563, "seek": 449668, "start": 4511.320000000001, "end": 4517.240000000001, "text": " in the graph, you're running the backward propagation, which is a little bit of back crop, the chain", "tokens": [294, 264, 4295, 11, 291, 434, 2614, 264, 23897, 38377, 11, 597, 307, 257, 707, 857, 295, 646, 9086, 11, 264, 5021], "temperature": 0.0, "avg_logprob": -0.12217539129122881, "compression_ratio": 1.8974358974358974, "no_speech_prob": 5.647454599966295e-05}, {"id": 564, "seek": 451724, "start": 4517.24, "end": 4526.44, "text": " rule at that node. And then the result of doing that is you have gradients for your inputs and parameters.", "tokens": [4978, 412, 300, 9984, 13, 400, 550, 264, 1874, 295, 884, 300, 307, 291, 362, 2771, 2448, 337, 428, 15743, 293, 9834, 13], "temperature": 0.0, "avg_logprob": -0.0822211538042341, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.8909790014731698e-05}, {"id": 565, "seek": 451724, "start": 4528.04, "end": 4538.44, "text": " And so this is the overall software runs this for you. And so what you want to do is then actually", "tokens": [400, 370, 341, 307, 264, 4787, 4722, 6676, 341, 337, 291, 13, 400, 370, 437, 291, 528, 281, 360, 307, 550, 767], "temperature": 0.0, "avg_logprob": -0.0822211538042341, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.8909790014731698e-05}, {"id": 566, "seek": 451724, "start": 4538.44, "end": 4545.08, "text": " have stuff for particular nodes or layers in the graph. So if I have a multiply", "tokens": [362, 1507, 337, 1729, 13891, 420, 7914, 294, 264, 4295, 13, 407, 498, 286, 362, 257, 12972], "temperature": 0.0, "avg_logprob": -0.0822211538042341, "compression_ratio": 1.6101694915254237, "no_speech_prob": 1.8909790014731698e-05}, {"id": 567, "seek": 454508, "start": 4545.08, "end": 4554.12, "text": " gate, it's going to have a forward algorithm, which just computes that the output is x times y in terms", "tokens": [8539, 11, 309, 311, 516, 281, 362, 257, 2128, 9284, 11, 597, 445, 715, 1819, 300, 264, 5598, 307, 2031, 1413, 288, 294, 2115], "temperature": 0.0, "avg_logprob": -0.15075113715195074, "compression_ratio": 1.6519337016574585, "no_speech_prob": 0.00010873866267502308}, {"id": 568, "seek": 454508, "start": 4554.12, "end": 4560.28, "text": " of the two inputs. And then I'm going to want to compute, to tell it also how to calculate the", "tokens": [295, 264, 732, 15743, 13, 400, 550, 286, 478, 516, 281, 528, 281, 14722, 11, 281, 980, 309, 611, 577, 281, 8873, 264], "temperature": 0.0, "avg_logprob": -0.15075113715195074, "compression_ratio": 1.6519337016574585, "no_speech_prob": 0.00010873866267502308}, {"id": 569, "seek": 454508, "start": 4560.28, "end": 4570.84, "text": " local derivative. So I want to say, what is the local derivative? So dL dx and dL dy in terms of the", "tokens": [2654, 13760, 13, 407, 286, 528, 281, 584, 11, 437, 307, 264, 2654, 13760, 30, 407, 274, 43, 30017, 293, 274, 43, 14584, 294, 2115, 295, 264], "temperature": 0.0, "avg_logprob": -0.15075113715195074, "compression_ratio": 1.6519337016574585, "no_speech_prob": 0.00010873866267502308}, {"id": 570, "seek": 457084, "start": 4570.84, "end": 4579.8, "text": " upstream gradient, dL dz. And so I will then manually work out how to calculate that. And normally,", "tokens": [33915, 16235, 11, 274, 43, 9758, 13, 400, 370, 286, 486, 550, 16945, 589, 484, 577, 281, 8873, 300, 13, 400, 5646, 11], "temperature": 0.0, "avg_logprob": -0.14606892903645832, "compression_ratio": 1.49746192893401, "no_speech_prob": 3.319008828839287e-05}, {"id": 571, "seek": 457084, "start": 4579.8, "end": 4589.56, "text": " what I have to do is I assume the forward pass is being run first. And I'm going to shove into some", "tokens": [437, 286, 362, 281, 360, 307, 286, 6552, 264, 2128, 1320, 307, 885, 1190, 700, 13, 400, 286, 478, 516, 281, 35648, 666, 512], "temperature": 0.0, "avg_logprob": -0.14606892903645832, "compression_ratio": 1.49746192893401, "no_speech_prob": 3.319008828839287e-05}, {"id": 572, "seek": 457084, "start": 4589.56, "end": 4595.88, "text": " local variables for my class, the values that we used in the forward computation. So as well as", "tokens": [2654, 9102, 337, 452, 1508, 11, 264, 4190, 300, 321, 1143, 294, 264, 2128, 24903, 13, 407, 382, 731, 382], "temperature": 0.0, "avg_logprob": -0.14606892903645832, "compression_ratio": 1.49746192893401, "no_speech_prob": 3.319008828839287e-05}, {"id": 573, "seek": 459588, "start": 4595.88, "end": 4604.4400000000005, "text": " computing z equals x times y, I'm going to sort of remember what x and y were. So then when I'm", "tokens": [15866, 710, 6915, 2031, 1413, 288, 11, 286, 478, 516, 281, 1333, 295, 1604, 437, 2031, 293, 288, 645, 13, 407, 550, 562, 286, 478], "temperature": 0.0, "avg_logprob": -0.11162241697311401, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.3625130122818518e-05}, {"id": 574, "seek": 459588, "start": 4604.4400000000005, "end": 4612.92, "text": " asked to compute the backward pass, I'm then going to have implemented here what we saw earlier of", "tokens": [2351, 281, 14722, 264, 23897, 1320, 11, 286, 478, 550, 516, 281, 362, 12270, 510, 437, 321, 1866, 3071, 295], "temperature": 0.0, "avg_logprob": -0.11162241697311401, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.3625130122818518e-05}, {"id": 575, "seek": 459588, "start": 4614.28, "end": 4622.36, "text": " that when it's xy, you're going to sort of swap the y and the x to work out the local gradients.", "tokens": [300, 562, 309, 311, 2031, 88, 11, 291, 434, 516, 281, 1333, 295, 18135, 264, 288, 293, 264, 2031, 281, 589, 484, 264, 2654, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.11162241697311401, "compression_ratio": 1.5901639344262295, "no_speech_prob": 1.3625130122818518e-05}, {"id": 576, "seek": 462236, "start": 4622.36, "end": 4627.16, "text": " And so then I'm going to multiply those by the upstream gradient. And I'm going to return,", "tokens": [400, 370, 550, 286, 478, 516, 281, 12972, 729, 538, 264, 33915, 16235, 13, 400, 286, 478, 516, 281, 2736, 11], "temperature": 0.0, "avg_logprob": -0.11319137524954881, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.00011186017218278721}, {"id": 577, "seek": 462236, "start": 4627.96, "end": 4634.04, "text": " I've just written it here as a sort of a little list, but really it's going to be a numpy vector", "tokens": [286, 600, 445, 3720, 309, 510, 382, 257, 1333, 295, 257, 707, 1329, 11, 457, 534, 309, 311, 516, 281, 312, 257, 1031, 8200, 8062], "temperature": 0.0, "avg_logprob": -0.11319137524954881, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.00011186017218278721}, {"id": 578, "seek": 462236, "start": 4634.04, "end": 4645.639999999999, "text": " of the gradients. Okay, so that's 98% of what I wanted to cover today, just a couple of quick", "tokens": [295, 264, 2771, 2448, 13, 1033, 11, 370, 300, 311, 20860, 4, 295, 437, 286, 1415, 281, 2060, 965, 11, 445, 257, 1916, 295, 1702], "temperature": 0.0, "avg_logprob": -0.11319137524954881, "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.00011186017218278721}, {"id": 579, "seek": 464564, "start": 4645.64, "end": 4654.6, "text": " comments left. So that can and should all be automated. Sometimes you want to just check if you're", "tokens": [3053, 1411, 13, 407, 300, 393, 293, 820, 439, 312, 18473, 13, 4803, 291, 528, 281, 445, 1520, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.09201352492622707, "compression_ratio": 1.7383720930232558, "no_speech_prob": 2.5860015739453956e-05}, {"id": 580, "seek": 464564, "start": 4655.4800000000005, "end": 4661.56, "text": " computing the right gradients. And so the standard way of checking that you're computing the right", "tokens": [15866, 264, 558, 2771, 2448, 13, 400, 370, 264, 3832, 636, 295, 8568, 300, 291, 434, 15866, 264, 558], "temperature": 0.0, "avg_logprob": -0.09201352492622707, "compression_ratio": 1.7383720930232558, "no_speech_prob": 2.5860015739453956e-05}, {"id": 581, "seek": 464564, "start": 4661.56, "end": 4669.320000000001, "text": " gradients is to manually work out the gradient by doing a numeric calculation of the gradient. And so", "tokens": [2771, 2448, 307, 281, 16945, 589, 484, 264, 16235, 538, 884, 257, 7866, 299, 17108, 295, 264, 16235, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.09201352492622707, "compression_ratio": 1.7383720930232558, "no_speech_prob": 2.5860015739453956e-05}, {"id": 582, "seek": 466932, "start": 4669.32, "end": 4678.5199999999995, "text": " you can do that. So you can work out what the derivative of x of f with respect to x should be", "tokens": [291, 393, 360, 300, 13, 407, 291, 393, 589, 484, 437, 264, 13760, 295, 2031, 295, 283, 365, 3104, 281, 2031, 820, 312], "temperature": 0.0, "avg_logprob": -0.14518513256990456, "compression_ratio": 1.5257731958762886, "no_speech_prob": 3.217970879632048e-05}, {"id": 583, "seek": 466932, "start": 4679.08, "end": 4686.679999999999, "text": " by choosing some sort of small number like 10 to the minus 4, adding it to x, subtracting it from x.", "tokens": [538, 10875, 512, 1333, 295, 1359, 1230, 411, 1266, 281, 264, 3175, 1017, 11, 5127, 309, 281, 2031, 11, 16390, 278, 309, 490, 2031, 13], "temperature": 0.0, "avg_logprob": -0.14518513256990456, "compression_ratio": 1.5257731958762886, "no_speech_prob": 3.217970879632048e-05}, {"id": 584, "seek": 466932, "start": 4686.679999999999, "end": 4692.84, "text": " And then so the difference between these numbers is 2h, dividing it through by 2h. And you're simply", "tokens": [400, 550, 370, 264, 2649, 1296, 613, 3547, 307, 568, 71, 11, 26764, 309, 807, 538, 568, 71, 13, 400, 291, 434, 2935], "temperature": 0.0, "avg_logprob": -0.14518513256990456, "compression_ratio": 1.5257731958762886, "no_speech_prob": 3.217970879632048e-05}, {"id": 585, "seek": 469284, "start": 4692.84, "end": 4699.32, "text": " working out the rise over the run, which is the slope at that point with respect to x. And that's", "tokens": [1364, 484, 264, 6272, 670, 264, 1190, 11, 597, 307, 264, 13525, 412, 300, 935, 365, 3104, 281, 2031, 13, 400, 300, 311], "temperature": 0.0, "avg_logprob": -0.07779949269396194, "compression_ratio": 1.7168949771689497, "no_speech_prob": 4.605204230756499e-05}, {"id": 586, "seek": 469284, "start": 4699.32, "end": 4708.6, "text": " an approximation of the gradient of f with respect to x at that value of x. So this is so simple,", "tokens": [364, 28023, 295, 264, 16235, 295, 283, 365, 3104, 281, 2031, 412, 300, 2158, 295, 2031, 13, 407, 341, 307, 370, 2199, 11], "temperature": 0.0, "avg_logprob": -0.07779949269396194, "compression_ratio": 1.7168949771689497, "no_speech_prob": 4.605204230756499e-05}, {"id": 587, "seek": 469284, "start": 4708.6, "end": 4713.24, "text": " you can't make a mistake implementing it. And so therefore you can use this to check", "tokens": [291, 393, 380, 652, 257, 6146, 18114, 309, 13, 400, 370, 4412, 291, 393, 764, 341, 281, 1520], "temperature": 0.0, "avg_logprob": -0.07779949269396194, "compression_ratio": 1.7168949771689497, "no_speech_prob": 4.605204230756499e-05}, {"id": 588, "seek": 469284, "start": 4714.12, "end": 4721.0, "text": " where your gradient values are correct or not. This isn't something that you'd want to use much", "tokens": [689, 428, 16235, 4190, 366, 3006, 420, 406, 13, 639, 1943, 380, 746, 300, 291, 1116, 528, 281, 764, 709], "temperature": 0.0, "avg_logprob": -0.07779949269396194, "compression_ratio": 1.7168949771689497, "no_speech_prob": 4.605204230756499e-05}, {"id": 589, "seek": 472100, "start": 4721.0, "end": 4727.72, "text": " because not only is it approximate that it's extremely slow. Because to work this out you have to run", "tokens": [570, 406, 787, 307, 309, 30874, 300, 309, 311, 4664, 2964, 13, 1436, 281, 589, 341, 484, 291, 362, 281, 1190], "temperature": 0.0, "avg_logprob": -0.10612333197342722, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00013731852232012898}, {"id": 590, "seek": 472100, "start": 4727.72, "end": 4733.4, "text": " the forward computation for every parameter of the model. So if you have a model with a million", "tokens": [264, 2128, 24903, 337, 633, 13075, 295, 264, 2316, 13, 407, 498, 291, 362, 257, 2316, 365, 257, 2459], "temperature": 0.0, "avg_logprob": -0.10612333197342722, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00013731852232012898}, {"id": 591, "seek": 472100, "start": 4733.4, "end": 4740.2, "text": " parameters, you're now doing a million times as much work to run backprop as you would do if you're", "tokens": [9834, 11, 291, 434, 586, 884, 257, 2459, 1413, 382, 709, 589, 281, 1190, 646, 79, 1513, 382, 291, 576, 360, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.10612333197342722, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00013731852232012898}, {"id": 592, "seek": 472100, "start": 4740.2, "end": 4746.2, "text": " actually using calculus. So calculus is a good thing to know. But it can be really useful to check", "tokens": [767, 1228, 33400, 13, 407, 33400, 307, 257, 665, 551, 281, 458, 13, 583, 309, 393, 312, 534, 4420, 281, 1520], "temperature": 0.0, "avg_logprob": -0.10612333197342722, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.00013731852232012898}, {"id": 593, "seek": 474620, "start": 4746.2, "end": 4752.36, "text": " that the right values are being calculated. And the old days when we hand wrote everything,", "tokens": [300, 264, 558, 4190, 366, 885, 15598, 13, 400, 264, 1331, 1708, 562, 321, 1011, 4114, 1203, 11], "temperature": 0.0, "avg_logprob": -0.10735577344894409, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.0001352186081930995}, {"id": 594, "seek": 474620, "start": 4753.08, "end": 4758.92, "text": " this was kind of the key unit test that people used everywhere. These days most of the time you're", "tokens": [341, 390, 733, 295, 264, 2141, 4985, 1500, 300, 561, 1143, 5315, 13, 1981, 1708, 881, 295, 264, 565, 291, 434], "temperature": 0.0, "avg_logprob": -0.10735577344894409, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.0001352186081930995}, {"id": 595, "seek": 474620, "start": 4758.92, "end": 4764.679999999999, "text": " reusing layers that are built into PyTorch or some other deep learning framework. So it's much less", "tokens": [319, 7981, 7914, 300, 366, 3094, 666, 9953, 51, 284, 339, 420, 512, 661, 2452, 2539, 8388, 13, 407, 309, 311, 709, 1570], "temperature": 0.0, "avg_logprob": -0.10735577344894409, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.0001352186081930995}, {"id": 596, "seek": 474620, "start": 4765.32, "end": 4769.5599999999995, "text": " needed. But sometimes you're implementing your own layer and you really do want to check", "tokens": [2978, 13, 583, 2171, 291, 434, 18114, 428, 1065, 4583, 293, 291, 534, 360, 528, 281, 1520], "temperature": 0.0, "avg_logprob": -0.10735577344894409, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.0001352186081930995}, {"id": 597, "seek": 474620, "start": 4769.5599999999995, "end": 4775.72, "text": " the things are implemented correctly. There's a fine point in the way this has written. If you saw", "tokens": [264, 721, 366, 12270, 8944, 13, 821, 311, 257, 2489, 935, 294, 264, 636, 341, 575, 3720, 13, 759, 291, 1866], "temperature": 0.0, "avg_logprob": -0.10735577344894409, "compression_ratio": 1.6890459363957597, "no_speech_prob": 0.0001352186081930995}, {"id": 598, "seek": 477572, "start": 4775.72, "end": 4784.92, "text": " this in sort of high school calculus class, you will have seen rise over run of f of x plus h minus", "tokens": [341, 294, 1333, 295, 1090, 1395, 33400, 1508, 11, 291, 486, 362, 1612, 6272, 670, 1190, 295, 283, 295, 2031, 1804, 276, 3175], "temperature": 0.0, "avg_logprob": -0.10171749140765216, "compression_ratio": 1.565217391304348, "no_speech_prob": 5.063923526904546e-05}, {"id": 599, "seek": 477572, "start": 4784.92, "end": 4795.8, "text": " f of x divided by h. It turns out that doing this two-sided estimate like this is much, much more", "tokens": [283, 295, 2031, 6666, 538, 276, 13, 467, 4523, 484, 300, 884, 341, 732, 12, 30941, 12539, 411, 341, 307, 709, 11, 709, 544], "temperature": 0.0, "avg_logprob": -0.10171749140765216, "compression_ratio": 1.565217391304348, "no_speech_prob": 5.063923526904546e-05}, {"id": 600, "seek": 477572, "start": 4795.8, "end": 4801.16, "text": " accurate than doing a one-sided estimate. And so you're really much encouraged to use this", "tokens": [8559, 813, 884, 257, 472, 12, 30941, 12539, 13, 400, 370, 291, 434, 534, 709, 14658, 281, 764, 341], "temperature": 0.0, "avg_logprob": -0.10171749140765216, "compression_ratio": 1.565217391304348, "no_speech_prob": 5.063923526904546e-05}, {"id": 601, "seek": 480116, "start": 4801.16, "end": 4808.92, "text": " approximation. Okay, so at that point, we've mastered the core technology of neural nets. Back", "tokens": [28023, 13, 1033, 11, 370, 412, 300, 935, 11, 321, 600, 38686, 264, 4965, 2899, 295, 18161, 36170, 13, 5833], "temperature": 0.0, "avg_logprob": -0.13533066895048498, "compression_ratio": 1.5392670157068062, "no_speech_prob": 4.197948146611452e-05}, {"id": 602, "seek": 480116, "start": 4808.92, "end": 4816.5199999999995, "text": " propagation is recursively and hence efficiently applying the chain rule along the computation graph", "tokens": [38377, 307, 20560, 3413, 293, 16678, 19621, 9275, 264, 5021, 4978, 2051, 264, 24903, 4295], "temperature": 0.0, "avg_logprob": -0.13533066895048498, "compression_ratio": 1.5392670157068062, "no_speech_prob": 4.197948146611452e-05}, {"id": 603, "seek": 480116, "start": 4816.5199999999995, "end": 4825.48, "text": " with this sort of key step that downstream gradient equals upstream gradient times local gradient.", "tokens": [365, 341, 1333, 295, 2141, 1823, 300, 30621, 16235, 6915, 33915, 16235, 1413, 2654, 16235, 13], "temperature": 0.0, "avg_logprob": -0.13533066895048498, "compression_ratio": 1.5392670157068062, "no_speech_prob": 4.197948146611452e-05}, {"id": 604, "seek": 482548, "start": 4825.48, "end": 4831.799999999999, "text": " And so for calculating with neural nets, we do the forward pass to work out values with current", "tokens": [400, 370, 337, 28258, 365, 18161, 36170, 11, 321, 360, 264, 2128, 1320, 281, 589, 484, 4190, 365, 2190], "temperature": 0.0, "avg_logprob": -0.13737388090653854, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.2603704817593098e-05}, {"id": 605, "seek": 482548, "start": 4831.799999999999, "end": 4840.44, "text": " parameters, then run back propagation to work out the gradient of the loss currently computer", "tokens": [9834, 11, 550, 1190, 646, 38377, 281, 589, 484, 264, 16235, 295, 264, 4470, 4362, 3820], "temperature": 0.0, "avg_logprob": -0.13737388090653854, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.2603704817593098e-05}, {"id": 606, "seek": 482548, "start": 4840.44, "end": 4849.16, "text": " loss with respect to those parameters. Now to some extent, you know, with modern deep learning", "tokens": [4470, 365, 3104, 281, 729, 9834, 13, 823, 281, 512, 8396, 11, 291, 458, 11, 365, 4363, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.13737388090653854, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.2603704817593098e-05}, {"id": 607, "seek": 482548, "start": 4849.16, "end": 4854.12, "text": " frameworks, you don't actually have to know how to do any of this, right? It's the same as you", "tokens": [29834, 11, 291, 500, 380, 767, 362, 281, 458, 577, 281, 360, 604, 295, 341, 11, 558, 30, 467, 311, 264, 912, 382, 291], "temperature": 0.0, "avg_logprob": -0.13737388090653854, "compression_ratio": 1.6550218340611353, "no_speech_prob": 1.2603704817593098e-05}, {"id": 608, "seek": 485412, "start": 4854.12, "end": 4862.68, "text": " don't have to know how to implement a C compiler. You can just write C code and say GCC and it'll", "tokens": [500, 380, 362, 281, 458, 577, 281, 4445, 257, 383, 31958, 13, 509, 393, 445, 2464, 383, 3089, 293, 584, 460, 11717, 293, 309, 603], "temperature": 0.0, "avg_logprob": -0.09014395371224117, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.47789355146233e-05}, {"id": 609, "seek": 485412, "start": 4862.68, "end": 4869.96, "text": " compile it and it'll run the right stuff for you. And that's the kind of functionality you get", "tokens": [31413, 309, 293, 309, 603, 1190, 264, 558, 1507, 337, 291, 13, 400, 300, 311, 264, 733, 295, 14980, 291, 483], "temperature": 0.0, "avg_logprob": -0.09014395371224117, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.47789355146233e-05}, {"id": 610, "seek": 485412, "start": 4869.96, "end": 4876.44, "text": " from the PyTorch framework. So do come along to the PyTorch tutorial this Friday and get a sense", "tokens": [490, 264, 9953, 51, 284, 339, 8388, 13, 407, 360, 808, 2051, 281, 264, 9953, 51, 284, 339, 7073, 341, 6984, 293, 483, 257, 2020], "temperature": 0.0, "avg_logprob": -0.09014395371224117, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.47789355146233e-05}, {"id": 611, "seek": 485412, "start": 4876.44, "end": 4883.64, "text": " about how easy it is to write neural networks using a framework like PyTorch or TensorFlow. And you", "tokens": [466, 577, 1858, 309, 307, 281, 2464, 18161, 9590, 1228, 257, 8388, 411, 9953, 51, 284, 339, 420, 37624, 13, 400, 291], "temperature": 0.0, "avg_logprob": -0.09014395371224117, "compression_ratio": 1.634453781512605, "no_speech_prob": 3.47789355146233e-05}, {"id": 612, "seek": 488364, "start": 4883.64, "end": 4889.72, "text": " know, it's so easy. That's why high school students across the nation are now doing their science", "tokens": [458, 11, 309, 311, 370, 1858, 13, 663, 311, 983, 1090, 1395, 1731, 2108, 264, 4790, 366, 586, 884, 641, 3497], "temperature": 0.0, "avg_logprob": -0.1134215650104341, "compression_ratio": 1.5875, "no_speech_prob": 7.355761044891551e-05}, {"id": 613, "seek": 488364, "start": 4889.72, "end": 4896.4400000000005, "text": " projects, training deep learning systems because you don't actually have to understand very much", "tokens": [4455, 11, 3097, 2452, 2539, 3652, 570, 291, 500, 380, 767, 362, 281, 1223, 588, 709], "temperature": 0.0, "avg_logprob": -0.1134215650104341, "compression_ratio": 1.5875, "no_speech_prob": 7.355761044891551e-05}, {"id": 614, "seek": 488364, "start": 4896.4400000000005, "end": 4903.0, "text": " to bung a few neural network layers together and set it computing on some data. But you know,", "tokens": [281, 50045, 257, 1326, 18161, 3209, 7914, 1214, 293, 992, 309, 15866, 322, 512, 1412, 13, 583, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1134215650104341, "compression_ratio": 1.5875, "no_speech_prob": 7.355761044891551e-05}, {"id": 615, "seek": 488364, "start": 4903.0, "end": 4908.4400000000005, "text": " we hope in this class that you actually are also learning how these things that implemented.", "tokens": [321, 1454, 294, 341, 1508, 300, 291, 767, 366, 611, 2539, 577, 613, 721, 300, 12270, 13], "temperature": 0.0, "avg_logprob": -0.1134215650104341, "compression_ratio": 1.5875, "no_speech_prob": 7.355761044891551e-05}, {"id": 616, "seek": 490844, "start": 4908.44, "end": 4915.16, "text": " So you have a deeper understanding of than that. And you know, it turns out that sometimes you", "tokens": [407, 291, 362, 257, 7731, 3701, 295, 813, 300, 13, 400, 291, 458, 11, 309, 4523, 484, 300, 2171, 291], "temperature": 0.0, "avg_logprob": -0.10766058497958714, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00015566199726890773}, {"id": 617, "seek": 490844, "start": 4915.16, "end": 4921.639999999999, "text": " need to have a deeper understanding. So back propagation doesn't always work carefully, perfectly.", "tokens": [643, 281, 362, 257, 7731, 3701, 13, 407, 646, 38377, 1177, 380, 1009, 589, 7500, 11, 6239, 13], "temperature": 0.0, "avg_logprob": -0.10766058497958714, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00015566199726890773}, {"id": 618, "seek": 490844, "start": 4921.639999999999, "end": 4927.08, "text": " And so understanding what it's really doing can be crucial to debugging things. And so we'll", "tokens": [400, 370, 3701, 437, 309, 311, 534, 884, 393, 312, 11462, 281, 45592, 721, 13, 400, 370, 321, 603], "temperature": 0.0, "avg_logprob": -0.10766058497958714, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00015566199726890773}, {"id": 619, "seek": 490844, "start": 4927.08, "end": 4932.599999999999, "text": " actually see an example of that fairly soon when we start looking at recurrent models and some of", "tokens": [767, 536, 364, 1365, 295, 300, 6457, 2321, 562, 321, 722, 1237, 412, 18680, 1753, 5245, 293, 512, 295], "temperature": 0.0, "avg_logprob": -0.10766058497958714, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00015566199726890773}, {"id": 620, "seek": 490844, "start": 4932.599999999999, "end": 4938.04, "text": " the problems that they have, which will require us to think a bit more deeply about what's happening", "tokens": [264, 2740, 300, 436, 362, 11, 597, 486, 3651, 505, 281, 519, 257, 857, 544, 8760, 466, 437, 311, 2737], "temperature": 0.0, "avg_logprob": -0.10766058497958714, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00015566199726890773}, {"id": 621, "seek": 493804, "start": 4938.04, "end": 4946.12, "text": " in our gradient computations. Okay, that's it for the day.", "tokens": [50364, 294, 527, 16235, 2807, 763, 13, 1033, 11, 300, 311, 309, 337, 264, 786, 13, 50768], "temperature": 0.0, "avg_logprob": -0.3108728991614448, "compression_ratio": 0.90625, "no_speech_prob": 0.00011674894631141797}], "language": "en"}