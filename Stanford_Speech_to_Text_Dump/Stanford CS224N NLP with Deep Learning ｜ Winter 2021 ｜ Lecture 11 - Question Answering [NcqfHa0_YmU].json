{"text": " Okay, hi everyone. Welcome back to Winner and pass the halfway point week six of CS 224 N. And so let me just give a couple of quick announcements first. So today is the day that you have to have done the mid quarter survey by hundreds of people have. But if you haven't, this is your last chance to get the half point for that. Today is also the day that final project proposals are due. We really encourage you to try and hand them in on time or nearly on time. That's really just to help you so we can more quickly give you feedback on final project proposals. And in the background then there's also assignment five. You'll have seen the message that we're giving you one extra day for that. But we do certainly encourage you to be hard at work on assignment five at this point. Hopefully it's a great exciting opportunity to be learning all the latest stuff about transformers. And then today delighted to have our first invited speaker. Let me just mention that going along with the half point of participation credit is you guys writing a reaction paragraph for talking about something that the speaker talks about their instructions up for that on Ed. But without further ado, let me introduce Dan Tichun. So Dan Tich is one of the foremost researchers in question answering. And she's particularly well known in recent work for being one of the co-authors of the Roberta paper, the Spanbert paper, and on using dense passage retrieval methods for open domain question answering. And as a professor at the Princeton University, but as one other comment, Dan Tichun, once upon a time was the head TA of CS224N. So she's quite familiar with the context of this class. So really delighted to have Dan Tichun here to give this lecture on question answering. Thanks. Thank you Chris for the introduction. For me, it's a great opportunity for me to come back to CS224N today and give this lecture although being virtually. So questions are in the areas that have been working quite a bit in the last few years. So today I'm very happy to introduce you some of the fundamentals in this field as well as some cutting edge and saves our topics. So here is my plan for this lecture. So first I will give a brief introduction of what is question answering and what kind of problems that people are starting today. Then I'm going to use the most of the this lecture focusing one type of question answering problems called reading comprehension. So this is basically a problem that of how we build systems to answer questions over a single passive text. So I know that many of you are going to do a default project on the Stanford question answering data set. So understanding this part will be very crucial for your final project. So at the end of this lecture, I'm hoping to spend hopefully like 10 days a minute to talk about a more practical and a more exciting problem called open-to-man question answering. So basically try to answer questions over a very large collection of the documents. So my plan is try to quickly go over some of the state art methods in this area. Okay, so let's just get started. So first, what is the question answering? So the goal of question answering is to build systems that can automatically answer questions posed by humans in a natural language. Question answering or let's say QA in short is one of the earliest tasks and the early systems can even date back to 1960s. So here is one example of the early QA systems in back to 1964. So I think I see that this system is trying to answer questions like what do you want it and finally return on the answer that's the graph. So to do this, so this system is better to try to find some kind of text matching between the question and some kind of text segments and by using some kind of dependency analysis, I assume that you have already learned the defense of course in this class. And there are many different types of the question answering problems. And we can also look at category or this question answer problems based on the either the information source or the type of questions or the type of answers. So for the information source, we can build a system that can put like condition or text or very large collection of documents or even like structured database or structured knowledge based basis or even tables or images. So for the question part, we can also be a system that can also affect questions or non-factual questions, open open questions or coastal questions or simple questions versus more complex or compositional questions. And for the answer type, it can also be like a short segment of text or a paragraph or document or list or even the yes or no questions. So just have in mind there's many different types of question answering problems and all these problems may require very different techniques or different data or even different evaluation metrics to evaluate all these different problems. And the question answer has enabled a lot of the use for real-world applications. For example, today if you're just putting a question in a search engine like Google, for example, you can put in a question like where's the deepest lake in the world. So you can see that the current system basically can find a short snippet of text with including like like by call, by call in Siberia holds a distinction of being both the deepest lake in the world and the largest fresh water lake blah blah. And then it can actually ping pong the crack answer which is actually a concise answer which should be Siberia. And those current systems are also able to handle more complex questions like how two questions. I guess this is probably a question that everyone currently cares about. So the question is how can I protect myself from COVID-19? So there is another really simple and short answer to this question. So you can see that the system actually returns a very long paragraph including the best way to prevent the year is to avoid being exposed to this virus. And to help prevent the spread of COVID-19 you can do the following. So actually this paragraph is actually a summary from this CDC article if you just click this link and read through the article. So this is also one kind of question. And now this is a survey of the use cases for the current digital system such as Alexa or Google Home. So according to this survey result in January 2020 which is one year ago. So you can see that also people actually really like to ask questions on this digital assistance. So you can see that also question is actually the second most used case only around after listening to music and before the check weather in a set of time timer. So question is really useful in these digital systems. Another very famous example of the question answering system is this IBM was from the question answering system. So in 2011 so this IBM was not used to a system has been shown to be too national. Jeffrey Champions in answering chapter questions. So this is kind of this all like a historical event and it's in the LK history. So if you look at the you are working of this system more closely. So you can see that it is actually a very complicated and highly modularized system. So it's a system that builds on both the unstructured text and also the structured data. So by looking at the system if you go from the left to right you can see that this system consists of the four stages including the question processing. The candidate answer generation and the candidate answer scoring and the confidence margin and ranking. And then if you look at each stage you can see that there are many different LP techniques that have actually included in this complex QE system including question classification, parsing, relation extraction, correctness. So it's actually there really a lot of the LP systems modules that have been included. And this system has been over 10 years actually exactly 10 years now and this is actually represented in a safe art like 10 years ago at that time. So we know that this class is about deep learning. So today deep learning has completely really transformed the landscape of the question answering systems. So there's no doubt that we can say that almost all the states are question answering systems today are built on top of the end-training of the deep learning networks and the pretty chain language models such as BERT. So today in this lecture we also going to learn a lot of deep learning models in for question answering. And this statement is probably also true for almost all the LP problems that we can see today. But we can also argue that question answering is probably one of those fields that we have seen the most remarkable progress in the last couple of years driven by deep learning. So in this lecture I would be mostly focused on like focusing on the text based or textual question answering problems. So basically we are trying to answer questions based on the unstructured text. So before I start I jump to that part. I also would quickly point out that there are many other really bigger question answering problems and the issue of them can be really a like a big subfield in NLP and they actually have very different challenges and also model designs. So one bigger class of this question answer problem is this knowledge based question answering. So basically we want to build question answering systems to answer questions that can answer answer questions over a very large database. So to solve this problem some approaches need to take this question and convert this question into some kind of logic forms and this kind of logic forms can be executed against this database to give you the final answer. And another class bigger class of the question answering problem is called visual question answering. So it's basically need to answer questions based on the images. So this problem basically requires both understanding of the questions and also images and there is actually a very active field between the computer vision and NLP. So if we are interested in all these type of problems I encourage you to check out these problems but I'm not going to give you these problems today. Okay so next I'm going to start with a part to review comprehension. I just want to quickly check if there are any quick questions I can answer before I get started. I'll start with a part to you. Now I think we could do now. Okay so yeah so let's talk about the review comprehension then. So a read comprehension is a basic problem that we want to compare and passive text and answer questions about the content. So it's an input of discovering visual passive text a question and going to return the answer that actually can't answer this question. So here is one example. So let's talk here is a passive text and we want to answer a question the question is what language the test will start if while you sport. Okay so I'm going to pause like five or 10 seconds and see if you will find the answer to this question based on this passage. And you guys. Okay. Well people stress the German. Yeah, German is a crap. So the answer should be German. So basically answer this question so you need to find this sentence like in 1861 test out 10 years school where he started German arithmetic and religion and only the German is a language so the answer to this question should be German. Okay here is another example. Okay another passive text and the question is which linguistic minority larger painting or Malal Yalan I think yeah. Five seconds. Okay so the answer to this question should be Hindi. So this probably is not very hard question for humans it actually a pretty hard question for machines because to get this question correctly so the machine is basically to understand that for the Hindi like 3.3% of the population speaks Hindi and only like 1.27% speaks Malal Yalan this language and then also compare these two numbers and the final case 3% 3.3% is a bigger number so the answer should be Hindi to this question. Okay so next I'm going to talk a little bit so why do we care about this problem so why do we care about the reading comprehension problem so besides that it actually tries many useful real work practical applications so as I already saw some examples at the beginning I think there are also two other few reasons so the first reason also besides the application the first reason is so reading comprehension has been also viewed as a very important test path for evaluating how well computer systems understand human language so this is really just similar to how we humans actually test a reading comprehension test to evaluate how well we actually understand what language so this is also the way that we actually post questions to test the machines language understanding and language understanding ability so this actually has been formally stated in back in 1937 by Wendy Lengard in her dissertation so she's fitting the saying is that she said that these questions can be devised to query any aspect of test comprehension so be it to answer questions is the strongest possible demonstration understanding so that's why reading comprehension can be a very important test path because we can't be devised design very complex questions to test that and also I think there's another interesting and important reason that reading comprehension is important so in the recent few years so many some researchers actually found that okay so for many other NLP tests that we also reduced them to a reading comprehension problem so I'm going to give you two examples so one example is already a information extraction so basically if we want to so give answers to a person like subject brought Obama give a relation educated at so we want to fill in what is a fill in this question mark and figure out okay where Barack Obama was educated at so one way to solve this problem is basically trying to cover this relation into a question so where did the Barack Obama graduate from and take a relevant piece of text and then by writing a reading comprehension problem then basically we can find out the extract the correct answer should be Columbia University that is also the output of this information extraction system another example is actually called a cement for the labeling I'm not sure if you have noticed in the class yet probably not but it basically is a task of the technical labeling is trying to taking one sentence and trying to identify the roles for different verbs at least for verbs in this case in one sentence so basically trying to give one sentence or give one verb finish trying to figure out like who did what to whom and then and well so by trying to so to go you try to figure out all this like roles with respect to the words so one way to solve this problem is by also by converting all these different roles into questions such as who finished something what is someone finished and what is someone finished something else so by converting all these kind of like cement relations we can also just apply the reading comprehension problem and give you a correct answer so this is actually a very interesting perspective that reading comprehension can be actually very universally useful to many other questions so next I'm going to introduce this like a Stanford question three data set calls God so if you're going to do the before the final project you will need to use this data set so Stanford question three data set is actually a super advised reading comprehension data set so which consists of 100 K annotated passage and answer question answer triples so here is one example from this data set and I just want to say that also what important thing to have in mind is that so this data set has consists of 100 K annotated examples and this kind of large scale supervised data set is also very key in grade and for the training the effective neural models for reading comprehension so after this God data set many other like later data set having also collected basically runs this size or around like 100 K so 100 K is actually very important to trend these neural models so for this data set so the question the passages is like a single passage a single paragraph selected from the English Wikipedia which usually consists of like 100 to 150 words and the questions are crowdsourced basically like from the kind of turkey and there is a very important property of this data set is that each answer is a short segment text or we can spend in the passage so as you can see from this example so here are three different questions and each of this answer can be actually find also like a short segment text in the passage so this is actually a pretty interesting property you know it's also important property of this data set but also just to your coverage that this also limitation because not all the questions can be answered in this way so only the questions that that you can find answered as a stand in the passage can actually be included in this data set basically but today so this data set yeah I forgot to say so this data set was collected in 2016 by several researchers at Stanford so it's called Stanford Question 3 data set and today like after four or five years now so Scott still remains the most popular reading comprehension data set so he's actually he's a very clean on the high quality data set but he's also not as very difficult data set so today basically the score data set has been almost sold and what safe are already since estimated human performance and also until quickly mentioned the evaluation for this Stanford question data set so there are basically two evaluation metrics to evaluate how well a system can do on this data set the two metrics are like the exact match and the affine score so when you find match is basically just a binary indicator zero one based measures whether the answer can actually be exactly matched to the gold answer and the affine score basically measures kind of some partial credit and not to do the evaluation so basically for the development and testing set there will be like three gold answers collected because for some questions there might be not just one one unique answer so they're quite multiple possible answers and the evaluation makes it basically takes a pretty good answer and compares or compares the predicted answer to each gold answer with some kind of like some articles and also the computations excluded and the picture you can compute a exact match score and also a score by comparing the predicted answer to the gold answer and finally you take the match scores and because there are many different examples in the demo test set and finally we just take the average of all the examples for the post-example match and the reference score so by using this evaluation metric so estimating the human performance is by the researchers as a time estimated by the researchers as a time is the exact match score is 82.3% and the affine score is 91.2 so here's just a quick example so here's a question what do tests are doing in December 1878 and the Diatry possible answers so you can see that the first two answers are the same left grass and the third answer is left grass and as a serve as is a title here or relations with his family and then you feel that if you find a prediction is a span which is left grass and serve so you can see that the exact there is an exact match score between the predicted answer and any of the gold answer so the exact match will be zero and the affine score will be taking the max I'm not going to talk about how this is computed so I suggest you check out the original paper so by computing this scores and taking the max and the final is the affine score will be 0.67 which is the affine score for this predicted answer on this data set. So Danchi one question you might answer is so if you can do other tasks like named entity recognition or relation extraction by sticking something on top of bird as and fine tuning forward or do it as question answering there's one or the other method work better and by how much. That's an interesting question so I haven't really seen the okay so there has been some claims that okay also tasks can be converted into questions and tasks but I'm not sure there is a really a very fair comparison let's say an entity recognition but by really converting that into questions and tasks so I don't have to answer to that so the kind of states are in your system and still trying to just change sequence tagger on top of the bird so yeah I don't really have a pre-set answer to that. Should I continue? Okay so next I'm going to talk about how to build newer models for really comprehension in particular how we can build a model to solve this Stanford question answering data sets for data sets. Also I want to just quickly mention that because there are many different papers it actually uses like different notions to refer to the sensing so I'm starting from so I'm going to use the passage paragraph and context and also question query basically interchangeably so they are basically referred to sensing because different papers use also different notions so I just want to quickly mention that okay so can we build a model to solve this problem so let's first form this problem so the input of this problem is let's take let's take our context or paragraph so see which consists of the intel can see one to see and and also we take our question q and look the question consists of m tokens q1 to qm so n could be something like around 100 to on between 100 and 200 for Scott and m would be much shorter be something like 10 or 15 and because the answer has these constraints as the answer must be a second text in the passage so the output can be just reading this way so we are going to predict a start and so start an end will be wrench be basically in the wrench between the one and so it is basically just two check points oh sorry two end points of the answer and then so Scott has been collected by the late 2016 so after 2016 they are having like visit two families of the models newer models to solve to solve in this like STEM score data set so the first family basically like there are a lot of models that count out during the lecture between 2016 and 2018 so this was family models because they are always team based models result tension so these are like just like a list of the representing models that come out during the period and including some works that I did when I was a PhD student at Stanford and also second the second class models I put here is really there that divided here before the birth and after birth so after birth 10 miles so all of the system reading comprehension models were built on like how to find two the first models not just bird models are put a bird like models so pretty and long with models and for this kind of reading comprehension problems so here I like to the some you know to the illustrations of these two families of the models so on the left is like I always team based models result tension on the right is on the version model and then we need to find two this model for the passion for the reading comprehension task so I know that so my plan today is first I talk to talk about this I always team based models so I'm going to spend a little bit more time on this part because I know that for the default final project you need to increment this model from the scratch so I'm going to work through how to build this model like step by step and hopefully that you can have a good understanding of how this model works and then I'm just going to briefly talk about how to build this use the bird models for the reading comprehension okay so before I start talking about this always team models I know that you have already learned sequence to sequence models result tension for machine translation so I was I want to draw some connections between the machine translation part and the reading comprehension problem because they really share little similarities so first so in the machine translation model all these like sequence use sequence model there is a like source and package the sentence so basically two sequences so that in our case in this reading comprehension case that we also have two sequences one is a passage and another is a question but the lens could be a slightly in balance because the passage really much longer than the question but it's essentially also two sequences and so in the reading comprehension we need to model like which words in a passage are most relevant to the question and then if they're relevant to the question so it's also relevant to which that of the question works so this is basically a very key important thing that the important thing that we actually need to model and this is actually very similar to the machine translation model that we need to model which words in the source sentence that actually are most relevant to the current packet word so if I imagine that the attention will be also really the key break in here that just like some sequence to six model we need to model the attention between the source sentence and the packet sentence we also need to model the attention between the passage and the question so this is actually very similar so something that's actually not very similar is for the sequence to six model we need to build like a decoder auto-regressivity decoder to generate the packet sentence word by word but in this reading comprehension problem we we don't need to really generate anything so we just take the pass into question so at least for the scope on data set we just need to try to cross-spare to predict the start and positions of answer so that's very much you simply find so we need to be to try the decoder to generate the target sentence okay so next I'm going to talk about one this model called by death so it's sent for by directional attention flow for machine comprehension so either was proposed by mention seal and other folks in 2017 so it remains before the first time out it remains one of the most popular reading comprehension models and a very good performance at that time at least on the spot dataset so you can see that this model seems to be pretty complicated but if you look look at this model from the bottom to the top it actually can be decomposed into many different layers so the next I'm going to just dissect this model layer by layer and talk about okay what this layer is actually doing and how we can really build this model from the bottom layer to the top layer and the final retrancess like model in an end to end away okay so the first part it actually the bottom three layers called character embedding layer wording embedding layer and the first thing that later so I just put them together called this as a encoding function so the idea here is that okay let's take the context query or the passage in question we need to encode them separately so to do this so this model basically proposed to use a concatenation of the wording embedding as well as the character embedding for each word in the context and query so for the wording embedding straightforward so you have a wording embedding so you can just look up the word for the this word like Seattle just use the global embedding as a reputation for this word and for the character embedding part so you basically need to represent each character in this word like Seattle and the hypothesis to a convolutional neural network with some kind of max point operations and finally you can just get one reputation I will talk and then you just concatenate the wording embedding and the character embedding so this character embedding has been shown exactly to improve the reputation for the unseen or the real world so mathematically a mathematical you can see that so for each word in the context query you can just we can just represent the rotation of the block with the embedding and the character embedding and then we just concatenate them and pass each other all highway networks so I don't write the function here so so you can just look up orange on paper and the second part so other we call the issue a very visual work so and the next we are going to pass this wording embedding into two separate by directional LSTNs to separate me to produce these contextualized embeddings for both the context and query so let's look at these equations so we take the reputation of this word and then we just base this in like a one LSTN model from one direction and this LSTN model from another direction so we just need to concatenate the two key rotations two directions and then finally we can get a contextualized reputation for each single word in the context and then we can do the same similar thing for the question reputation also want to query mention because I mentioned the sequence your sequence model so sequence to signal although we can already do this bad directional LSTNs for the two sequences again like because the decoder is all all the embarrassing model so that's why the decoder is usually just implemented as a unit direction on this team but because of here we don't really care about the generation so we can just use two bad directional LSTNs to represent the rotation this is actually very important this bad directional lentil is actually very important to capture the context from both the left and right set okay so the next component is the next layer it's called the attention flow layer so I just call it the attention here so the attention idea the idea of attention is trying to capture the interactions between the context and query and in this paper the baddest paper they propose two types of tension so the first type of tension we call the context your query attention so the idea is for each context word can we find the most relevant words in the question from the question for the query for the query words so here's the one example so here the context context the problem of Rama is a present of the USA so for each context word we need to find an alignment because it's fun like the wish words in the question can be actually aligned with this context word so we can see that both both of them can be aligned to pool and the president will align to the list and the USA is aligned to the United States so basically for each content we'll try to find the most relevant query words and then not the second type of tension is called query to context or tension so it's a very or not direction so here the idea is to choose some context words that are most relevant to one of the query words because the context can be very long so a lot of the context could be just not relevant to the discussion so we just run over several examples you can see that the first thing we need to do is try to locate okay wish cards of the sentences in this context can be actually relevant to this question so this type of query to context or tension is trying to capture so which which context words actually can be most relevant to the query to one of the query words so for this example the question is which seeding the glooming in winter so because the question asked about glooming so you can find a triacy of okay glooming species is actually very relevant to this question and now we also find this in winter because in winter it also mentioned the question so this part of context words to be also relevant to this question so this context words could be probably need to capture and not in this tension that okay this actually relevant to this question okay so this actually basically just a intuition of this two types of tension and this also wise model is called a bi-directional tension flow because there is a context query or tension and there is also a parity context tension so let me just talk about how to actually do this like query to context tension the context query or tension in this model so the way they do this is first to compute a similarity sport for every period of the contextualized vector C i and for every pair of the question with QJ so this is actually the output from the encoding layer so this already is the output from the LSTM layers and the way they say basically just compute a similarity sport by taking the C i QJ and also the element wise amygdication of the C i QJ so it's a basically just concatenate these three vectors so the output will be a six-inch dimensional vector and they just match this to compute the dot product of another like a learnable vector and the family just this can't this can't give you one scalar one number the is ij which matters how on the similarity between this context word C i and also this question word QJ so if you have so if I learned some attention before so this is actually just one choice of this model so there could be many different ways to define this similarity and a similarity sport so this is basically just one design choice of this model okay so after defined this similarity sport is ij so the context to query attention again like which question words are most relevant to C i so the way they do this is so basically just taking this matrix the similarity sport is ij for each row each row basically corresponds to like one context word for each row they are going to compute a soft max for each row and this can give us like normalization sports r for ij which is our probability distribution over all the question words r for ij so this is just really similar to all the attention on the kind of things that you probably have seen in this class so basically for each context word taking the soft max over all the question words and get us probability distribution and finally just take the linear combination of the weighted combination of these attention score r for ij and also the question vector the QJ and the finally you can't get a vector a i which is actually two h-dimensional vector so this context to query attention basically just try to capture which questions was a most relevant to each context word so the next part part is a query to sorry the title here sorry this actually the query to context or attention so which means that which context was relevant to some question words so we don't so a lot of context words would be no relevant to this question so the idea to do this is for each row of this is ij this basically just takes a mass scores over all the question words and after taking this mass score they compute the soft max over all the context words here so here i actually numerous over all the context words and this can give us like attention another attention score beta i which captures how important this context word is relevant to this question so after computing this beta i so we can again like compute this like a weighted combination by computing by some by summing up the beta i and also the context context vector ci and the finally you can't get a vector bi which is also another two h-dimensional vector and the final output of this attention function that's a very complicated here is also the design choice of this model so the takes a context vector ci and as it takes a i from this part the context to query attention and the takes the element of multiplication between the ci and ai and also the ci and bi and the final is to take the contactination and the can give you a produce of h-dimensional vector okay maybe i want to pause a little bit and check if there are any questions because this part is a little bit complicated yeah one one question is why is query to context and context to query attention not symmetrical um um that's a good question yes so here because essentially the goal is trying to because the goal is final goal you're trying to final span in the passage so the the whole the point of the this attention function is trying to produce a rotation for each single context word in this context so that's um so so we are not trying to generate questions rotations here it's going to try to generate the um contact rotations so one so the difference between these two like first try to see which questions are relevant to this context work another part is trying to figure out which contact work can be relevant and which contact work can be not relevant i hope it sounds just answers your question yeah here's an easier question sort of on the same topic which might help is there a reason why you use both query to context and context to query attention is it sometimes advantageous or okay to use just one that's a good question um um the reason is yeah so the i'm going to show some relations already from this figure so they basically just find both both directions can really help um by drawing the context for and query to context so there'll be some relations studies so by using one strategy useful but then just not a bit as using the both directions yeah um right let's see uh in the bottom right we sum over i so far does the i remain in bi is that correct or so typo there uh this is not a typo so again sorry so the output yeah you know it's a bit confusing so the output of this model this come for module it to get a rotation for each context work at the end so both the output for AI and bi i is actually um in numerates from like um actually in numerates uh you know if first over all the context works so bi would be still um just to try to aggregate over all the questions uh on the over all the context works but the beta i measures the importance of this context works compared to all the context works so both AI and bi are actually disrespecting the context works yes so you can see that here is basically doing some kind of the animal wise multiplication so the output of the g i would be actually only arranged from the one to and uh in which is another the context works there are lots of questions about this um what is the rationale for the expression for g i how does one come up with such an expression okay i don't know i guess not also try out a lot of things uh so okay so keep on here trying to understand okay so the roles of the context require attention or context or tension so i bet there's actually been many different formations to do this i also think there also have child many different variants but uh just what they kind of can up as and i think that if after week um it's gonna be smart the way string copper is a both attention but it doesn't have to be written this way yeah i mean one other question would be in the query the context attention why do you do a max inside the soft max yeah oh yeah sorry i should have expense more clearly so here again query to contest attention to try to measure whether this the importance of this context works with respect to some some answer or question words so if the so the so by taking the max for each row in this ace matrix so it's basically trying to see okay which question word um is actually most relevant to this context word if this number is still very low that means there isn't any question words that could be online with this context word so that we would just by taking after taking the max if this number is still very low that means this query context word is not very relevant so so basically just as well we take the soft max uh try to soft max i won't talk to the next uh i know do you want even more do you want to go on uh i probably should know all i do have a lot of slides but i'm happy on the questions after the answer yeah maybe you should go on yeah okay so the last part of this model is actually um the idea is the most simple so it's two of the last last three there are two layers smaller layer and all four layers so for the smaller layer so again for absolute attention layer they take the key derivation um of so basically g i which captures the attention between all contacts and the query and then basically just passes g i to another two layers of bi-directional errors gems and the many reasons they do this is the attention layer is basically modeling the interactions between the query and context and the bi-passing this to another two layers of bi-directional errors gems the modeling layers is basically modeling they can also first model the interactions using the context words so this is a formulation here um so these are two layers bi-directional and here by taking the g i as input and the output will be on the m i which is another two-h direction dimensional vector for each context work in the in the passage okay so the final is all four layers so all four layers they did just two cross-fers just trying to predict the starting end positions so by doing this so the first contact in the g i and m i so this would be actually a 10-h dimensional vector and by computing the dot project over another vector called double start and this resulting vector and they can get basic data score for each position in the context and then you can just up up higher softmax uh and then this will give you a probability that okay what is the probability this position i will be actually uh be on the start position on the final answer string and they also have another class file to predict the end position of the answer but there also be something a little bit more complicated so they actually passed the m i to another bi-directional error is tm here so they call the m fine i and they come cat in g i and m prime i oh sorry this is the title so this will be w and so the computer dot project between w and and this vector and this can be reproduced on all the probability probability over all the positions which predicts the um how likely this position will be the end position of the answer so by doing it by crossing the m i to another bi-directional is tm the reason is that they're trying to capture some kind of dependence between the choice of the start and end so you can imagine that start and shouldn't be too separate so shouldn't be a cool business independent predict it but if they claim that if you add some kind of dependence between the m i and um the p-start and p-end this can actually perform better okay and don't visit this part on describing the bi-directional model any quick questions i can ask this i think you can actually go on okay okay sorry i forgot to mention this is the okay the final training loss will be just by taking these two probability distributions and this is basically just next next lot like the code of the gold as a gold answer does the protocol start position of the gold answer and end position of the answer and by just um basically taking the product of the these two probabilities but you're planning a pilot lock so it's a sum of the two next log terms will be the final training loss and the whole the whole model can be just changing the end to end away from the encoding layer to a tension layer to modern layer and to outlayer so this will be just to accomplish the whole the whole model of the bi-directional model okay so this model is actually achieved like on the data set it achieved a 77 point history f1 school so as i mentioned earlier so just on some operations started they found the both of tension in two directions are actually important if we remove the one direction the performance will actually drop a bit if we remove the contrast to error tension the performance will drop to 67 point seven f1 school and if we remove this part it will drop to four point f1 four and then also the character embedding styles help so if we remove the character embedding you'll get like a 1.9 point drop and all the right of this figure you can see slide you can see a very big table so it's basically all the models that account of as that time between 2016 and 2018 so you can see that um by that we're here so you're achieved a 77 point three f1 school and the basic all the models are actually you know very similar ballpark so numbers range from like the highest number here is 79.8 until like after the Elmo was introduced the numbers have been actually improved quite a bit so before the Elmo basically all the numbers are actually kind of similar so each model actually improved our primers primers model by like a 1.2 points and now here is our tension visualization to show that on how these like the similarities for the tension actually can capture the similarity between the question words and the contrast words so here's an example of the question the word in super form 50 takes place so each show is actually a question word here and each column is matrix based in the case the attention score the similarity score that has been learned by this model so you can see that on the right is basically trying to print out or display so the the the contrast words that have the highest scores so you can see that the where it has been online very well with the at the stadium liva and also the super bowl 50 is basically line very well with the super bowl 50 so this basically really tells that this kind of attention scores can actually capture the similarity scores pretty well yeah okay so next i'm going to talk about bird now how to use the bird model to solve this problem so i know that you have learned the bird in the last lecture so i'm not going to repeat this so very quick so bird is basically a deep vibrational transformer encoder pre channel on the large amount of text and is a channel the two channel objectives you will invest in which modeling and the next sentence prediction and this model has a lot of parameters so the bird base has a 110 million parameters and the bird logic model has 330 million parameters so okay so how we can actually use bird for the for reading comprehension so it's actually very easy as a very straightforward the idea is to take the person as a segment okay so you know so the bird criteria are like two segments for the next sentence prediction task so then you apply the bird on the reading comprehension task you basically just take the question as a segment A and take the passage as a segment B and finally you to go with the trying to create two end points in segment B so here's one more concrete example so question how many parameters does bird logic have so you can see that so the basically just takes the question here and then takes a passage here and by putting the cio is token and the acp token and by just contacting the question of the passage tokens and also for the question setting you just need to pass the 8 to a segment embedding and the passage you just need to put in the bird the segment B embedding and finally the training loss is also the same so you basically just try to maximize the probability the some of the next lot like could also both the start and end positions but here's the way that the compute the start and end probability is slightly different so it actually very straightforward so it just passes on impolitation into bird and the bird can give you the hit on that h i that actually presents the hit of that corresponding to the context word context word c i so we can just introduce another two vectors W start and W and by computing the top product and then apply the softmax then you can just give you a very similar to what we had before but here is the h i just output from the bird's encoder and then we are training on these two W to start and W and for these two probability distribution P start and P and okay so for this model so all the bird parameters that is actually very much number if you use the bird base you will be 110 million parameters as well as a newly introduced parameters h start and h end which is if you take the bird base so hidden side will be 7608 so it's only like 1,500 new parameters there will be just to optimize together jointly for this training objective error and then it actually works really really well this model so if you just take this mod bird model and by just optimizing all the parameters together you can give you very high performance I will show you very in a minute and even the strong even if you use the stronger approach and long models more than like the standard um the stronger models than the bird models they can evenly to better performance on scot and the scot that has also become a standard data set for testing this kind of virtual models let me show you some numbers so again human performance in 91 and by that is 77.3 and then if we just do this fun shooting model so bird base can give you like 88.5 bird large can give you 99.9 so you can see that this is a huge jump from the by that model to the bird models and the final if you see that even the latest um pretty long with the models include the X-O-Ned or the belt or Albert so these models are either like a bigger or these model are channel bigger covers or the model size are bigger so basically these models can give you a not a like 34 point iF1 score compared to the bird large model so this is already way higher than estimate iF1 score so this just works really well any quick questions this might be okay okay so okay so yeah i guess i've been a little bit fast for this bird models but next what's so i would also do a bit of the comparisons between the by-dash models and the bird models so bird model has many many more parameters so it's like it's like one 10 more million or 300 to 13 million parameters but the by-dash has only like 2.5 million parameters and the by-dash is built on top of several by-directional-errish teams and while bird is built on top of the transformers so transformers means that there isn't any recurrence structure architecture so the trans-weapons are much easier to paralyze and a very different difference between the bird models and the by-back models is bird model is a pretender but by-back models only built on top of the glove that's which is the pretender and the other remaining of two parameters new peer learner found this is called a data set all the other supervision data set so here it is very clear that pretending is a game changer here that pretender basically can just change everything and also give you very very large boost in terms of the performance but also want to create another passion so if we don't think of this like on pretending this like by-demo on bird models are really fundamentally different I don't think so because of the below is actually my audience so let's try to see how these two models actually connected especially in terms of the model you've done so by that model essentially they're trying to model the interactions between the question and passage right so both of the questions to passage and passage to question and the bird model essentially they're trying to use a self-attention on top of the concatenation of the question passage so this is a transformable model so you should take the question the passage so these are questions in the passage and then you apply many many different layers of the self-attention essentially that this self-attention is able to capture the tension between the contests and the tension between the passage and the question words and the attention from the questions to passage side and also the attention from between from the question was to another question was so compared to by that by that is trying to model this part but the bird model essentially can capture the tension between all these four parts and actually after by that kind of so this also before the bird can before the bird can out so people have been also showing that if we just add a self-attention layer for the passage side so basically you're trying to explicitly model this attention between the passage words and passage words to the bad act this also you put the performance so you can see that these two models essentially just trying to model the tension between the passing question also the attention between the passage words and the passage words and this actually what exactly the bird model is doing okay so if there's no further questions so at this point I'll talk about bird models can do really well on this kind of reading comprehension data sets and we just talk about pretending can really change the performance can be again changing your reading comprehension I can put it on don't you I can ask add one question first people wonder whether you can do well with a transformer that isn't pre-trained right if you tried to build a question answering system using a transformer rather than RSTM's then no pre-training does that work that's a good question yeah it works but you probably cannot review the model as big as like one one 10 million premise or 230 million parameters models so actually there's a model between the sorry between the this is like a family of RSTM models and bird models they're called a QA in that from Google so QA in that is actually built on top of the transformers we saw the real pre-training so that model actually can perform better than the bad act models and other models but actually on the performance of that a bird model quite a bit so just check it out for QA in that okay I will just continue so okay so given pre-training has been so important so next I want quickly talk about okay question here is that can we actually even define better pre-training objective for reading comprehension or question answering and the answer is actually yes so this actually work I did with Mender-Drosion other folks like one year ago called Spambert so think about this so for the squad and other a lot of these types of reading comprehension data set the goal is trying to predict the answer span from the passage as a question so the as an answer to the discussion so there are two key ideas being proposed in Spambert so first idea is that instead of using only the masking of individual words we propose that we want to master particular spence of words in the passage because the final answer would be just a segment of text in the passage so we are trying to so mask out all these possible answers spence from the passage as a pre-training objective and the second idea of compulsory Spambert is that because at the end of we want to predict an answer spence so we actually essentially trying to predict two end points as a answer so the idea is that can we try to compress the two end points of answer span so can we try to compress all the information in this span into the two end points so here's the idea is that here let's think about this if we mask out the four words here and can we try to use the two end points here in this figure like an x4 and x9 to predict all the words in the middle so essentially we are trying to predict takes the two end points and also the position some kind of position coding and finally we are going to try to predict all the words in this span so this is why this code is spambert so I encourage you to check out our paper and this actually really helps a lot at least for the questions and data sets so as you can see from this figure so this is called 1.1 and it's called 2.0 and this are many other questions and data sets so you can see here so the blue bars here we call the google bird is actually the original check points that released by google researchers and our bird is actually just exactly our re-evaluation of the bird model but we are having trying to using the same data but we have been trying to transit model for slightly longer so it's actually achieved a better performance than our original google bird so as you can see the yellow bars here is actually the spambert so spambert actually brings with also performed google bird and all the bird basically across all the data sets that really tells us that okay even if we are not going to increase the model size we are not going to increase the data by designing better criteria objectives can also be very go a long way and do a much better job in at least in the question answering and reading comprehension data sets okay so I have several few slides left in this part so so far I have to demonstrate that on by using by death model and by using bird models we can get a very good performance on the scope data set and this number has already exists even the human performance on scope that this means that reading comprehension is already solved the answer is of course not so let me just so in the recent last couple of years that's been a lot of evidence showing that the current systems still perform poorly on adversarial examples or the examples from the out of domain distributions so here is a very classical example so proposed by Robin John personally on 2017 so the idea is that they take a pass and take a question and they're trying to just insert a random sentence to the end of the paragraph so you can see that distance passes like even like a nonsense entity in this context, drafting here but this sentence actually has a like a great some love score overlap between the question is actually very similar to this question but actually the word numbers have been changed the entinence has been changed and they found that these kind of adversarial examples can actually very easy to fool the prime systems and the final and the makes the system to predict answer to be the drafting so the by shoots the table shows that by adding a lot of these adversarial examples they found that the performance actually drops a lot this by that model so drops from 75.5 to even like 30% so for even like this kind of attack the performance will just drop to very low like 4.8% so here's another paper that actually just came out in 2020 so it has made a lot of the evidence showing the similar things that so today we can be a very good reading kind of attention data set on individual data on the individual data sets but this system is channel one data sets basically cannot really generalize to other data sets so the diagonal is basically of this table is basically channel one model on one data set and the evaluate on the send data set and for all the other numbers in this table basically shows that if you turn one from system on one data set and then evaluate on another data set the performance will drop a lot so it's basically really cannot generalize from one data set to another data set so finally this is actually a very interesting result so this model this paper is actually the best paper from ACL 2020 is called checklist paper so the idea is that this this also basically try to propose some kind of the test cases to check whether this model can actually really under also some simple questions whether we sound specific or particular film they find that by just kind of a really simple question for example here Jeremy is more optimistic than Taylor and who is more pessimistic and the they found that a birth lot model channel stop and this basically can fill this type of test cases 100% time and up here is another table so you can see that here is another correct example like Victoria and Alex are friends her mom is an agent who's mom is an agent and so to get this kind of question correctly it has to understand the Victoria actually refers to a female person and Alex refers to a male person so this this model this kind of questions also makes a kind of model for large models can also totally fill on this kind of test cases okay so I have 10 minutes left Chris is any question I should answer this point here you can go on okay so in the last 10 minutes I'm going to be very very very with introduction of what is open to my question and what we are having trying to do in the last couple years so open the main question is the problem that so it different from reading comprehension that we don't assume a given passage so here we have assumption that we only have access to a large collection of weapons so one example is just taking the whole way you should be keeping it on which has like 5 million articles so we don't really know where the answer is located and the goal is to return the answer for any open of questions so this problem so there is an annual single passage so we have to answer questions against a very large collection document or even the whole web documents so this is actually much more challenging and also more practical problem so if you look at the example of Google example I showed at the beginning so this is with techniques where will be very useful in the practical applications so the time here open domain is just in contrast to closed domains that deal with questions under specific domain here okay so how can we solve this type problem because for the reading comprehension problem we just need to answer questions based on single passage so this is a paper that I wrote in 2017 four years now so the paper is called reading Wikipedia to answer open domain questions and system called up to Q8 so the paper basically proposes the idea that we can actually solve this problem by using a retrieval and also read a framework so idea is that let's take a question okay so here all we go is trying to answer questions using like a very large collection document such as the Wikipedia so the idea is that there's a retrieval and also read a component so the retrieval takes in the question and I try to find out like a smaller number of the our documents that to be relevant to this question and this reading model basically trying to read through all the documents that this retrieval return and the fact try to find out the correct answers so formally defined here is that you put a large collection documents D and the question Q and the output could be our answer stream A so we had just decomposed this problem into as I just mentioned in our retrieval and the reader component so the retrieval is basically trying to take a large collection document D and Q and try to return a set document or set of passages so here the set this number K could be very small could be very small such as like one of just like a 100 so it's basically trying to pull out the found out like 100 passages of documents from like let's say five million documents and the finally the reader is basically takes a question and takes this set of the passages and finally finally returns the answer so the second problem exactly the reading component model that we just learned so in the just 70 paper result so it's actually doing a very simple thing so the retrieval is just a standard a new formation retrieval model the sparse pfid information retrieval sparse model and the real model essentially just a new already comprehend the model I just talked about so it's very trying to solve and some other questions during the process so this is the drill it's the idea is very simple but trying to bridge two things how to how to have bridge this retrieval and also the reader to do this kind of open domain question history so so I'm just going to quickly go over some really exciting ideas that that has been having in the last two years basically so the first idea is that this retrieval part can be also trained so we can actually even do this kind of drawing the training of the retrieval and the reader so here is actually so this this idea has been first proposed in Cantonese paper in 2019 called later in the retrieval for weekly supervised open domain questions so this part is basically the first model for reading comprehension and this not how it's based in the retrieval model so to get this in retrieval model working they also try to use the birth to you call the passage and also you call the question and they try to use a birth product between the question station the passage repetition to model how the relevance the similarity between the question the passage but this is actually a very difficult problem because the scalar scalability of this problem because there are like 20 million passages in a Wikipedia so it's actually very hard to model this part but so I encourage you to check out this paper and also on second paper I want to quickly mention is also work ideas on last year it's called the best pass of the retrieval so the idea is actually very similar to the the the previous paper because the idea is that is actually much more simply by model and very easy very simple straightforward approach the idea is that we can also really just trend the retrieval part by using two birth models using only the question answer pairs and this model can work really well and you can largely all form the traditional IR retrieval models if you see this figure so the blue curve here is a traditional IR approach like a BM25 approach and the so the other curve the orange curve based training this kind of retrieval is only 1000 question answer pairs so by looking at all these different curves basically using different number of training examples so it's actually largely crazy from the traditional IR models okay so again really I don't have time to talk about the details of all these approaches so I just encourage you to check out this paper this paper is nice and this result is really exciting so here's actually a really nice demo so the demo is actually hosted at this website you can check out so again so the database here is a whole Wikipedia you can see that if you ask a question who tells higher portals that he is a wizard and the higher you know higher portals series and the system I really found out the correct article should be higher portals film series and finally give you the correct answer which is exactly what you have seen from the Google example here so the answer could be the rubers hybrid which is actually the person who tells higher portals that he is a wizard so this is actually the perfect answer to this question okay I'm going to skip this slide and the final is very quick so so this is something that can out very recently that some researchers have demonstrated that maybe you don't even need to do this a retrieval study so you can if you just use a very large language model you can also just do the open domain question answering so the way they did this is that I hope that you have learned the TFI model in this class already so they just take a prediction language model TFI and they're trying to find to this model by taking the question and taking the question as an answer as output without any explicit retrieval and they just find to this on the data set and they find this model can be pretty well at the testing time by just taking the question and the directly generous answer without resorting to any like documents or like a retrieval system so this is actually very amazing so this kind of model is also called close book Q-assistence okay very long the last life so so this is one direction and personally I'm very excited about so this is actually a new direction that it basically shows that maybe for the open domain question answering maybe this rhythm model is also not necessary anymore so so this idea was first proposed by a museum in 2019 and we recently wrote a paper called dense phrases that try to demonstrate that maybe it doesn't even need this like a rhythm model so instead we can just you you code all the phrases in Wikipedia using some kind of dense letters so what you just need to do is just to do this kind of nearest neighbor search in the answer space you just encode all encode all the phrases in Wikipedia encodes and using vectors and by taking a question you can just encode this question using a vector and then we can just do the vector the nearest neighbor search and then you can directly give you the answer so this is a bit of a new paradigm of this kind of the question answer model so you don't need the you just need a retrieval you don't need a rhythm so good a great advantage for doing this is that so for the perfect rhythm model essentially you have to run a very model at the entrance time this is actually very expensive you can just do the similarity search you can just do the nearest neighbor search without running a burden model so this could be very fast and it can even run on the CPUs without needing to run like a very expensive different neural network and it can still run very well perfect very well okay finally I hope this works so I actually prepared a memo for this dance versus so I want to show you how this actually works so you can see that I've been trying this question like who on the not no bell prize EPs into 2014 so everything just tied for little piece of the input question and all this system can basically just to find out the answer the relevant test test it is and the family's answer is actually it shows up it's actually very fast because it's a bit of real time we don't we don't do wrong in the version model so it's just a ritual model here okay I'm actually done this is lecture so they are you're 515 now yeah thank you very much Dan chief that awesome survey of question answering I guess given that demo at the end people will want to know whether you're launching your own search engine soon but at any rate Dan chief can stay for a bit to answer questions but not forever but today because of you know she doesn't have a standard login we're going to do questions inside zoom so if you'd like to ask a question if you use the raise hand button we can promote you so that you appear in the regular zoom window and can just ask questions and see each other and if you hang around and don't leave the zoom for more than a few minutes maybe we'll just promote everybody who's still there into people in the regular zoom for some bits of discussion but we'd welcome anyone who'd like to ask a question by asking it themselves at this point okay I've got a one volunteer I've got more volunteers sure I mean so questions oh sure look at a chev or I mean so there are now four people who've been promoted there four people was the first so maybe he could start by asking a question and then the other people that we've promoted okay so thank you so much for the lecture today my question is mainly like if you use like a model mate for example Burke how a small kind of training dataset be really to get like reasonable results so the question is how we can try to recover a single model using only a small number of training samples yeah I think it's a really good question especially like you probably have heard the GPT stream model side you show that if you only use like a few very few examples you can also do the open-to-one question answering pretty well so but this kind of model is huge like what numbers like how many parameters I've got in the GPT stream model yeah so it's a very large very few model but okay so this amount is that if we can leverage a very large and very powerful precision-longed model there is a way that we are there is a possibility that we can actually do the question stream well we only have small number examples and also there are some other promising directions including like on supervised passion three so by using some kind of approach like the from the machine on supervised machine translation this kind of idea that can be borrowed and by yeah the kind of borrow ideas can also work pretty well reasonable reasonably well in on supervised passion three yeah also I have seen some of our other works like very pretty showing that since that you clearly assess how it's also quite helpful not imposing the performance if you don't have enough supervised data sets so nice examples yeah so my question is it's I guess it's kind of interesting that there's not really that strong of a transfer effect between data sets that are kind of ostensibly similar so my question is like has there been any research done on how close I guess like the formatting and the semantic content of these question answering data sets actually adheres to the data that like BERT is pre-trained on and if so like has there been sort of any effect found between those similarities or differences I use a question asking like there has been like a stonk cap okay maybe I can just try to clarify it but why the current models can already generalize well from one data set from the data set yeah so I actually really believe that most existing question stream data set already comprehension data set have been collected from the kind of perk so it's very hard it's very difficult to avoid some kind of artifact or like a simple clue or super visual clue that is not super visual but some simple clue that for the machines to pick up so for let's take those photos example set so it has to be that actually if you look at the data set more closely there has been a lot of examples that the question had been like a lot overlap in terms of the words between the question and the passage so the model is actually very good at picking up this kind of clues to get very high performance on this data set and another data set is called job so it's basically about comparison the two numbers something like that so that's the reason that one specialized model that has been very well in one one data set is very easy to pick up this kind of clues then there is a very hard to generalize this kind of thing to another dataset what about the natural questions data set doesn't better avoid that objection yeah natural questions would be much better but there are some other issues I'm not sure you have seen that there are the recent paper called like a question or a trend passed overlap paper so that means it demonstrate natural questions was a data set that Google put out about a year and a half ago maybe where they were actually taking real questions from Google search logs and then finding answers trying to find answers for them in web documents sorry go on Dachi oh I just want to see yeah I think the definite natural questions is on much better data set because the questions are natural like you're collected are real like real questions that are asking by like users so it kind of avoid this kind of are super fish of the artifact between the question the passage but there are some other issues that people like to ask some common questions so if you just do the Reynolds bait of questions you do trend that in test and there's a recent paper that's showing that there is actually a big model is inevitable that there is a high overlap between the trends that so if you find the question if one question that you're trying to test in the test set that has already appeared in the trends that that's a really generalization right yeah but this is more also like all open domain settings not in the reading comprehension set yeah yeah um do you want to ask a question yes so you mentioned that in the last part of the presentations that the read of models may not be necessary and you presented the answers please kind of also work well to use so do we know how how it performs on the question and answering data sets and compared to other other models including bread as well as other and some GPU of course yeah I just encourage you to check out this paper so this model is basically performs on par with the like the dense path retrieval retrieval model so it is performs on par with all the retrieval with the models but it is actually Reynolds so I skipped one slide so so right now the saved art is actually dominating by just kind of dense path retrieval as a generating model so this kind of so using a T5 model class that's a retrieval this is actually performed really well so I would just say so this is can work with a similar in this block but compared to this kind of generous model we still like have two points behind yeah okay and what is the kind of the uh intuition behind the test phrases upperformed like the answers are probably on the close proximity and what if the data sets has answers and has answers to a specific question like very far from the actual information let's see the answers to the question may may not be may not reside in close proximity to the to the words in the question so let me just clarify this okay it's a goal of this project is trying to um index all the phrases in the Wikipedia so and by and the these kind of expressions are built using the training set of the questions in data sets so the assumption still the distribution of the examples in the different test set will be similar to the Chinese set for sure that is that is also a question like so basically we still trying to consider all the phrases in the Wikipedia and that test now we just take the question of vision and I would compute the thought okay so if we use say a different data sets that does not present the information using a structure presented in Wikipedia then this model may not work as well as for what do you what do you mean by structure you present so uh say if we um lean more towards uh lean more towards uh structures like the passages we see in standardized tests where the answers to the question may not be like um may not be close proximity to where the information was first introduced oh no so the the the answers doesn't have to be seen or Chinese stuff so basically it's a goal is to take in the Chinese set channel in code for the phrases and by using and then we apply this in code to all the free all the phrases all the a lot of like six billion phrases in this video so it so the model is definitely able to generalize from the Chinese set to all the phrases in this video so it doesn't have to be seen in the things that that is this what you're asking this actually very so it's actually similar to like the retrieval of the best passive retrieval so you still like um yeah try to channel pass irritation here is the first representation but the the revisions only try to use the the Chinese set of the cross answered data sets but um by taking the encoder and then we are going to encode uh all the rotations all the passages of phrases in Wikipedia and then we can um use practice this rotation can actually generalize well for the unseen questions yeah so uh so the question is um what if the nearest neighbor search doesn't return to answer so why do you think the nearest neighbor I mean you always can find something right you just a question is that whether it's a question is not not yes so the question is what if in the data says that the answer is not close enough then um yeah this good question I don't know uh if you really come up with something that is really very far away from all the questions that we have been seeing the Chinese set that could be possible I don't know basically depend on um uh found the text or um formatted then uh the nearest neighbor search may not uh work as well as the models so again the question is also the question is also returned by a question encoder so so the question is the whether this question encoder can give you something reasonable that space or not but uh yeah so we have been testing along like a random even the input sentences or even like the question then I have to go real question could be a sentence it doesn't seem to be a problem so far yeah maybe maybe maybe we should give a couple of other people ago and you're allowed to turn your camera on and ask a question if you want um so um next person is all right hi uh thank you for taking the time to meet us um my my question is kind of quick so you mentioned work that brought up a set of relatively simple questions that show how brittle or poor the current models can be right I'm curious if that's right yeah yeah exactly exactly did that turn out to change the community to improve how to evaluate the models because they're actually doing pretty poorly on some of those right yeah so first these questions are simple in uh in terms of the the wording is very simple the for the template is very simple but they still trying to test like a negational temporal relational forever so the questions are not the I mean in terms of the reasoning of the code of plastic is not that simple it's just the wording very simple um I do think um okay so this paper that we receive a lot of attention you the best paper last year and I see all the biggest conference um so I think a lot of people are trying to solve the problem I cannot tell you that okay whether we really have a solution to this yet or not yeah cool yeah thank you for bringing this one up it's really interesting okay next is yeah thanks for saying it at time so my question is kind of not relevant but like to view the robust system of question answering in what extent can in context learning how models to be more robust with respect to different domains oh so like uh basically you provide um template generated by bird and then instead of directly predicting the classes of text classifications it is um use some word to represent that class of enter to the word so okay um so I assume that you are actually referred to the in context learning in the GPS stream or that's the way that okay um actually I have been doing something related to the learning recently um questions but I'm not sure how we actually use that in context learning in at least in for school type of problems um yeah so I don't know if that could solve the robustness or not or even the whole how to use that technique for the questions or yet nice thanks and I also mentioned that we can train a retriever without a reader so is there a paper of the current like attempt to do that yeah so the library also just uh yeah thank you all okay next is hey how's it going uh thanks so much for the uh for the lecture um i put a little broader question um so we got the about the future of NLP um do you think that in order to solve NLP in a sense that you can perform on par with humans on all NLP tasks it's efficient to only interact with with text you know whatever do you think will eventually need some sort of with the sort of experience and confidence that you get um only from seeing and then sort of feeling the world and having the sake of interactions that we assume it's how yeah I mean conversation is definitely very difficult even in the context of our sensory conversation is a very yeah no very very important topic that um still remains I think it still remains on three so uh that's for that part the one six definitely yes and also want to mention that okay so for a lot of the reading conversation data sets or questions and data sets you have seen that we are people will start start to achieve the human performance but this but we also see that how great all these systems are because yeah I mean they cannot regenerize also the easy problems all these things need to be with a solved um it depends so if you think about your descent time maybe a little bit too easy oh yeah one point oh no sorry guys yeah one point out for what is there is that uh dabbing a lot of trans-versa in trying to have a few maintenance um a framework to evaluate this kind of system just try to break the current system come out with some harder questions so um yeah so that means maybe it's a kind of static data system of good enough to measure the progress so we actually really need some kind of dynamic evaleration and also introduce all more these kind of adverse examples or the um yeah harder questions or something like yeah hey still game for a couple more questions uh sure I don't only mention my 10 p.m uh yeah um only used cost yeah okay um so next is next is hey yeah that's a much more much to the arm so in uh just 2020 there was this efficient open domain question answering challenge um and from you know from performance to team what there was like quite substantial uh decreased versus human accuracy um probably like yeah well for primarily dear quantization and um processing drift that occurred uh when they were quantizing um so I I recently encountered this paper called uh random quantizers uh which is essentially learned uh learns like basis representations for the quantizers like jointly with the weight of the network um and while this would be like extremely effective if you were to just like say change of scratch I was just really curious do you think it like such a uh there's a way to do this with say a pre-trained model or something like that um I had a few ideas with like the insurance insurance bloggers uh I don't think uh I don't see a very clear way of doing them um yeah I don't think I'm really experts to answer those questions um yeah I'm not sure if I really have the answer but I also want to give you mentions that yeah quantization has been very useful technique to make the model smaller right so we have been also exploring the quantization in the dense forest's project recently because of the storage has been still very uh has been still very large so we are having trying to reduce that storage um yeah I'm not sure uh about the question about the connection between conversation and also creating uh yeah I'm not sure I'm I'm I have also to ask you sorry thank you that don't you is too um modest to mention that she was one of the co-organizers of the efficient QA um share task um okay next question is I think she thinks so much for being here today um so my question is a bit different um so one example you gave the competition was this Alex Victoria example the checklist um and I was like in technically Alex was an evolved answer right it's gender neutral and there wasn't enough context context in the question to determine who it's referring to so my question is how concerned should we be about potentially including uh sort of biases into these or go labels or how we evaluate them or is that just more of a concern for more open and new questions um yeah this is definitely a very important again a lot of people are trying to study okay how much bias have been coding this model time how we can yeah um yeah I'm not sure if I'm good on so to do that um again I like it just I want to say like talk to do the debiasing of the pre-tronement models all these things are very important and um yeah this is just one so you're talking about this example right so this is just one test case um yeah um yeah yeah right yeah so I guess I'm just wondering who comes up with a test case is we will have more discussion of toxicity and bias coming up very soon including actually first-day lecture as well as a later lecture um not specifically about QA though um okay next person is uh right thank you for the lecture um yeah and my question is also related to the open domain question answering so um I was just wondering how much of like the learning side of um domain like sort of authorization or like domain alignments um techniques can be combined with like the language level like question answering like to what extends where they work and like what kind of like the language specific design should be leveraged to combine with those two two different um it's if if we want like higher performance and stuff like that the question about how to generalize between different domains or like all about how to extend open domain to assist them for different languages I'm watching together yeah great so I was wondering so um so there's like um some some like many specific designs like um um uh domain alignments and like uh future level disentanglement techniques uh that have been that has shown some like like interesting performance and um other tasks um it may solve that like recently some people also like leverage some other things um like for for for question answering so I was just wondering um like to what extent these kind of techniques come work on um like uh what in group tasks modules limited question answering but like um mainly uh for question answering sorry which which work I was talking about um not you know sure what do you mean by this in hand go to your school questions right so uh so basically um believe this is like a little bit more specific for so um so there is this um paper called um um doing um um I forgot the exact name uh it's uh like sorry align true domains um by decent okay so I have okay just want to make sure that we are on the same page so I have things of work that trying to learn some kind of in decent angle rotation second better generalize to the different domains or adverse examples is this what do you say yeah yeah and the question is whether this technique kind of general general general you apply to question answering oh yeah you're just wondering how um to what extent would they work because um I think language has like a lot of like specific things like dependency and to other stuff that like these techniques does not like actually um take care of several languages um yeah um yeah um yeah I'm not sure uh I think we have to try that for the subtle interesting point yeah I don't know at least for the work that I have since though fact all all of the title operated at a very simple uh sentence classification task maybe I'm almost maybe that's not correct so my own thing is that's basically take a label encoder apply to a simple test of classification task and take a documentation do some time to solve or transformation and make sure that um yeah it can learn some kind of invariant features about the header rotation something like that very yeah cool very input yeah I'm not sure I feel like QA is a more structured task and also kind of longer uh balance sequences um yeah so I don't know if it works unless people have tried that yeah thank you thank you okay and then we've got and maybe we should call this the last question um I'm just wondering what is like the alternative difference between solving question answering with um dance models like T5 versus encoder swipeboard okay um that's good point uh okay so I so I skipped this slide so why does model work so well uh the reason is actually it's not really about extracting model versus generating model the reason is that they actually um for the extracting model so if the retrieval returns let's say 100 passages so they have to extract also from each of the passages and finally figure out which one that has a might hide score but for the generation model essentially they're trying to aggregate all the 100 passages and the direct stations together and do their generation um jointly do you understand so essentially taking the 100 stations together to the joint generation instead of only do the extraction from each of the passages so I think that's actually the T difference so that's why this generating model can do really well compared to the extracting models so also I'll dimension that okay so if you look at this i.g model it's actually um like uh compress this DPR and i.g model the i.g model is always doing the generating model but they're not doing this kind of aggregation they're just trying to take a single passage and doing the generation so the i.g model actually um doesn't perform as well as this model but we also animation i.g model is actually not doing better than DPR because this base model is this large model so this this numbers are a little bit confusing so they're actually basically a really own part they're basically performed similarly but um so the the key difference between the generating model and extracting model is that for general models you can actually leverage more on input passage together and do the generation look does this is that clear or not um yes thanks yeah i know that's usually just check out this paper yeah so this paper asks the query well then why this model has worked better than the previous generating model actually uh follow up this page is janity models to do like wait um simple fashion and you know when you have the documents and vice-fabriccial page janity models to generate the span wait the yeah you you can definitely do that here oh so you are talking about this tip i'm not this yeah so i'm just wondering about like if you use encoders is like you're finding similarities between very encodings uh and then generating models are you like remembering a whole question and you try to retrieve them in a way okay so for this model there is an any retrieval so you can't really find the answer from the question right so this model really has to rely on all the parameters you remember all the information so by just taking the scene for the task to just rely on the parameters to infer this answer so it's actually very hard to so it's yeah it's definitely a balance between all the yeah it's a memory and a generalization problem yeah i see so um i'm just gonna say what i like is it but when you've got this question because you're adding it in some space and then using that you're adding the generator matches that to 18, 18, 18, is that what is going on either yeah exactly yes the model has to like it's very large like 11 billion parameters so all this so parameters visit trying to uh yeah memorize a lot of information that has been because the model has been retrieved from the text and also has been funky so the model has been trying to memorize a lot of information about the text here um do you want to call it an id or do you want one more question uh either way yeah i'm sorry you just show around how do you take one more question okay let me just do okay one more question okay the first question is about how how you really are these techniques generalized to other languages like say languages that are quite different or quite different grammatical rules which Chinese um Japanese or Arabic or some other languages sort of wonderful question maybe not right exactly your domain expertise is there's a lot of interest in modeling user behavior say online searching behavior browsing behavior as a sequence using state transformers self-attention um and then you can use that to predict how user or can be like an user or a selector and then predict user's actions hope promising do you think that would be i know this may not be your domain expertise but there is a lot of interest in extending these uh questions from techniques or just encoding techniques embedding techniques to recommend their systems um just want to get your thoughts on your mind okay um the first question is whether the these techniques can be generalized to other languages i think it's also yes and that has been a lot of active research in this version but there has no concerns that um as a lot of models of systems i described here actually require a lot of them require very strong like a pre-term language model and also require the lots of training examples for the QDSS so that would be actually um i would say a bottleneck for many low resource languages right so so it's very hard to collect so many examples called other languages as if we have actually i see the techniques can can be generalized generally applied to other languages as and i there has been also a lot of work trying to do to cross symbol questions so that's it there", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.84, "text": " Okay, hi everyone. Welcome back to Winner and pass the halfway point week six of CS 224", "tokens": [1033, 11, 4879, 1518, 13, 4027, 646, 281, 10427, 1193, 293, 1320, 264, 15461, 935, 1243, 2309, 295, 9460, 568, 7911], "temperature": 0.0, "avg_logprob": -0.3829101113712086, "compression_ratio": 1.3862433862433863, "no_speech_prob": 0.060511913150548935}, {"id": 1, "seek": 0, "start": 13.84, "end": 23.0, "text": " N. And so let me just give a couple of quick announcements first. So today is the day", "tokens": [426, 13, 400, 370, 718, 385, 445, 976, 257, 1916, 295, 1702, 23785, 700, 13, 407, 965, 307, 264, 786], "temperature": 0.0, "avg_logprob": -0.3829101113712086, "compression_ratio": 1.3862433862433863, "no_speech_prob": 0.060511913150548935}, {"id": 2, "seek": 0, "start": 23.0, "end": 29.240000000000002, "text": " that you have to have done the mid quarter survey by hundreds of people have. But if you", "tokens": [300, 291, 362, 281, 362, 1096, 264, 2062, 6555, 8984, 538, 6779, 295, 561, 362, 13, 583, 498, 291], "temperature": 0.0, "avg_logprob": -0.3829101113712086, "compression_ratio": 1.3862433862433863, "no_speech_prob": 0.060511913150548935}, {"id": 3, "seek": 2924, "start": 29.24, "end": 36.6, "text": " haven't, this is your last chance to get the half point for that. Today is also the day", "tokens": [2378, 380, 11, 341, 307, 428, 1036, 2931, 281, 483, 264, 1922, 935, 337, 300, 13, 2692, 307, 611, 264, 786], "temperature": 0.0, "avg_logprob": -0.17088344609625986, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0009071360109373927}, {"id": 4, "seek": 2924, "start": 36.6, "end": 41.68, "text": " that final project proposals are due. We really encourage you to try and hand them in", "tokens": [300, 2572, 1716, 20198, 366, 3462, 13, 492, 534, 5373, 291, 281, 853, 293, 1011, 552, 294], "temperature": 0.0, "avg_logprob": -0.17088344609625986, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0009071360109373927}, {"id": 5, "seek": 2924, "start": 41.68, "end": 47.32, "text": " on time or nearly on time. That's really just to help you so we can more quickly give you", "tokens": [322, 565, 420, 6217, 322, 565, 13, 663, 311, 534, 445, 281, 854, 291, 370, 321, 393, 544, 2661, 976, 291], "temperature": 0.0, "avg_logprob": -0.17088344609625986, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0009071360109373927}, {"id": 6, "seek": 2924, "start": 47.32, "end": 53.2, "text": " feedback on final project proposals. And in the background then there's also assignment", "tokens": [5824, 322, 2572, 1716, 20198, 13, 400, 294, 264, 3678, 550, 456, 311, 611, 15187], "temperature": 0.0, "avg_logprob": -0.17088344609625986, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0009071360109373927}, {"id": 7, "seek": 2924, "start": 53.2, "end": 58.36, "text": " five. You'll have seen the message that we're giving you one extra day for that. But we", "tokens": [1732, 13, 509, 603, 362, 1612, 264, 3636, 300, 321, 434, 2902, 291, 472, 2857, 786, 337, 300, 13, 583, 321], "temperature": 0.0, "avg_logprob": -0.17088344609625986, "compression_ratio": 1.694980694980695, "no_speech_prob": 0.0009071360109373927}, {"id": 8, "seek": 5836, "start": 58.36, "end": 63.72, "text": " do certainly encourage you to be hard at work on assignment five at this point. Hopefully", "tokens": [360, 3297, 5373, 291, 281, 312, 1152, 412, 589, 322, 15187, 1732, 412, 341, 935, 13, 10429], "temperature": 0.0, "avg_logprob": -0.194815058457224, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0004769090737681836}, {"id": 9, "seek": 5836, "start": 63.72, "end": 70.56, "text": " it's a great exciting opportunity to be learning all the latest stuff about transformers.", "tokens": [309, 311, 257, 869, 4670, 2650, 281, 312, 2539, 439, 264, 6792, 1507, 466, 4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.194815058457224, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0004769090737681836}, {"id": 10, "seek": 5836, "start": 70.56, "end": 77.24, "text": " And then today delighted to have our first invited speaker. Let me just mention that going", "tokens": [400, 550, 965, 18783, 281, 362, 527, 700, 9185, 8145, 13, 961, 385, 445, 2152, 300, 516], "temperature": 0.0, "avg_logprob": -0.194815058457224, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0004769090737681836}, {"id": 11, "seek": 5836, "start": 77.24, "end": 87.24, "text": " along with the half point of participation credit is you guys writing a reaction paragraph", "tokens": [2051, 365, 264, 1922, 935, 295, 13487, 5397, 307, 291, 1074, 3579, 257, 5480, 18865], "temperature": 0.0, "avg_logprob": -0.194815058457224, "compression_ratio": 1.5627705627705628, "no_speech_prob": 0.0004769090737681836}, {"id": 12, "seek": 8724, "start": 87.24, "end": 93.52, "text": " for talking about something that the speaker talks about their instructions up for that", "tokens": [337, 1417, 466, 746, 300, 264, 8145, 6686, 466, 641, 9415, 493, 337, 300], "temperature": 0.0, "avg_logprob": -0.2726762728257613, "compression_ratio": 1.6, "no_speech_prob": 0.0006007932825013995}, {"id": 13, "seek": 8724, "start": 93.52, "end": 102.08, "text": " on Ed. But without further ado, let me introduce Dan Tichun. So Dan Tich is one of the foremost", "tokens": [322, 3977, 13, 583, 1553, 3052, 22450, 11, 718, 385, 5366, 3394, 314, 480, 409, 13, 407, 3394, 314, 480, 307, 472, 295, 264, 18864], "temperature": 0.0, "avg_logprob": -0.2726762728257613, "compression_ratio": 1.6, "no_speech_prob": 0.0006007932825013995}, {"id": 14, "seek": 8724, "start": 102.08, "end": 108.64, "text": " researchers in question answering. And she's particularly well known in recent work for", "tokens": [10309, 294, 1168, 13430, 13, 400, 750, 311, 4098, 731, 2570, 294, 5162, 589, 337], "temperature": 0.0, "avg_logprob": -0.2726762728257613, "compression_ratio": 1.6, "no_speech_prob": 0.0006007932825013995}, {"id": 15, "seek": 8724, "start": 108.64, "end": 115.44, "text": " being one of the co-authors of the Roberta paper, the Spanbert paper, and on using dense", "tokens": [885, 472, 295, 264, 598, 12, 40198, 830, 295, 264, 15800, 1328, 3035, 11, 264, 1738, 282, 4290, 3035, 11, 293, 322, 1228, 18011], "temperature": 0.0, "avg_logprob": -0.2726762728257613, "compression_ratio": 1.6, "no_speech_prob": 0.0006007932825013995}, {"id": 16, "seek": 11544, "start": 115.44, "end": 122.44, "text": " passage retrieval methods for open domain question answering. And as a professor at the Princeton", "tokens": [11497, 19817, 3337, 7150, 337, 1269, 9274, 1168, 13430, 13, 400, 382, 257, 8304, 412, 264, 36592], "temperature": 0.0, "avg_logprob": -0.2580885952466155, "compression_ratio": 1.4158415841584158, "no_speech_prob": 0.00020823811064474285}, {"id": 17, "seek": 11544, "start": 122.44, "end": 132.04, "text": " University, but as one other comment, Dan Tichun, once upon a time was the head TA of CS224N.", "tokens": [3535, 11, 457, 382, 472, 661, 2871, 11, 3394, 314, 480, 409, 11, 1564, 3564, 257, 565, 390, 264, 1378, 20094, 295, 9460, 7490, 19, 45, 13], "temperature": 0.0, "avg_logprob": -0.2580885952466155, "compression_ratio": 1.4158415841584158, "no_speech_prob": 0.00020823811064474285}, {"id": 18, "seek": 11544, "start": 132.04, "end": 139.68, "text": " So she's quite familiar with the context of this class. So really delighted to have Dan Tichun", "tokens": [407, 750, 311, 1596, 4963, 365, 264, 4319, 295, 341, 1508, 13, 407, 534, 18783, 281, 362, 3394, 314, 480, 409], "temperature": 0.0, "avg_logprob": -0.2580885952466155, "compression_ratio": 1.4158415841584158, "no_speech_prob": 0.00020823811064474285}, {"id": 19, "seek": 13968, "start": 139.68, "end": 146.92000000000002, "text": " here to give this lecture on question answering. Thanks. Thank you Chris for the introduction.", "tokens": [510, 281, 976, 341, 7991, 322, 1168, 13430, 13, 2561, 13, 1044, 291, 6688, 337, 264, 9339, 13], "temperature": 0.0, "avg_logprob": -0.22868084085398707, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.0002477510424796492}, {"id": 20, "seek": 13968, "start": 146.92000000000002, "end": 153.48000000000002, "text": " For me, it's a great opportunity for me to come back to CS224N today and give this lecture", "tokens": [1171, 385, 11, 309, 311, 257, 869, 2650, 337, 385, 281, 808, 646, 281, 9460, 7490, 19, 45, 965, 293, 976, 341, 7991], "temperature": 0.0, "avg_logprob": -0.22868084085398707, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.0002477510424796492}, {"id": 21, "seek": 13968, "start": 153.48000000000002, "end": 157.88, "text": " although being virtually. So questions are in the areas that have been working quite a", "tokens": [4878, 885, 14103, 13, 407, 1651, 366, 294, 264, 3179, 300, 362, 668, 1364, 1596, 257], "temperature": 0.0, "avg_logprob": -0.22868084085398707, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.0002477510424796492}, {"id": 22, "seek": 13968, "start": 157.88, "end": 162.84, "text": " bit in the last few years. So today I'm very happy to introduce you some of the fundamentals", "tokens": [857, 294, 264, 1036, 1326, 924, 13, 407, 965, 286, 478, 588, 2055, 281, 5366, 291, 512, 295, 264, 29505], "temperature": 0.0, "avg_logprob": -0.22868084085398707, "compression_ratio": 1.58008658008658, "no_speech_prob": 0.0002477510424796492}, {"id": 23, "seek": 16284, "start": 162.84, "end": 169.68, "text": " in this field as well as some cutting edge and saves our topics. So here is my plan for", "tokens": [294, 341, 2519, 382, 731, 382, 512, 6492, 4691, 293, 19155, 527, 8378, 13, 407, 510, 307, 452, 1393, 337], "temperature": 0.0, "avg_logprob": -0.26980968397490834, "compression_ratio": 1.7421875, "no_speech_prob": 2.6631312721292488e-05}, {"id": 24, "seek": 16284, "start": 169.68, "end": 175.6, "text": " this lecture. So first I will give a brief introduction of what is question answering", "tokens": [341, 7991, 13, 407, 700, 286, 486, 976, 257, 5353, 9339, 295, 437, 307, 1168, 13430], "temperature": 0.0, "avg_logprob": -0.26980968397490834, "compression_ratio": 1.7421875, "no_speech_prob": 2.6631312721292488e-05}, {"id": 25, "seek": 16284, "start": 175.6, "end": 180.64000000000001, "text": " and what kind of problems that people are starting today. Then I'm going to use the most", "tokens": [293, 437, 733, 295, 2740, 300, 561, 366, 2891, 965, 13, 1396, 286, 478, 516, 281, 764, 264, 881], "temperature": 0.0, "avg_logprob": -0.26980968397490834, "compression_ratio": 1.7421875, "no_speech_prob": 2.6631312721292488e-05}, {"id": 26, "seek": 16284, "start": 180.64000000000001, "end": 186.08, "text": " of the this lecture focusing one type of question answering problems called reading comprehension.", "tokens": [295, 264, 341, 7991, 8416, 472, 2010, 295, 1168, 13430, 2740, 1219, 3760, 44991, 13], "temperature": 0.0, "avg_logprob": -0.26980968397490834, "compression_ratio": 1.7421875, "no_speech_prob": 2.6631312721292488e-05}, {"id": 27, "seek": 16284, "start": 186.08, "end": 190.8, "text": " So this is basically a problem that of how we build systems to answer questions over", "tokens": [407, 341, 307, 1936, 257, 1154, 300, 295, 577, 321, 1322, 3652, 281, 1867, 1651, 670], "temperature": 0.0, "avg_logprob": -0.26980968397490834, "compression_ratio": 1.7421875, "no_speech_prob": 2.6631312721292488e-05}, {"id": 28, "seek": 19080, "start": 190.8, "end": 196.28, "text": " a single passive text. So I know that many of you are going to do a default project on", "tokens": [257, 2167, 14975, 2487, 13, 407, 286, 458, 300, 867, 295, 291, 366, 516, 281, 360, 257, 7576, 1716, 322], "temperature": 0.0, "avg_logprob": -0.29190458035936545, "compression_ratio": 1.6867924528301887, "no_speech_prob": 6.70132867526263e-05}, {"id": 29, "seek": 19080, "start": 196.28, "end": 200.96, "text": " the Stanford question answering data set. So understanding this part will be very crucial", "tokens": [264, 20374, 1168, 13430, 1412, 992, 13, 407, 3701, 341, 644, 486, 312, 588, 11462], "temperature": 0.0, "avg_logprob": -0.29190458035936545, "compression_ratio": 1.6867924528301887, "no_speech_prob": 6.70132867526263e-05}, {"id": 30, "seek": 19080, "start": 200.96, "end": 206.48000000000002, "text": " for your final project. So at the end of this lecture, I'm hoping to spend hopefully", "tokens": [337, 428, 2572, 1716, 13, 407, 412, 264, 917, 295, 341, 7991, 11, 286, 478, 7159, 281, 3496, 4696], "temperature": 0.0, "avg_logprob": -0.29190458035936545, "compression_ratio": 1.6867924528301887, "no_speech_prob": 6.70132867526263e-05}, {"id": 31, "seek": 19080, "start": 206.48000000000002, "end": 212.88000000000002, "text": " like 10 days a minute to talk about a more practical and a more exciting problem called", "tokens": [411, 1266, 1708, 257, 3456, 281, 751, 466, 257, 544, 8496, 293, 257, 544, 4670, 1154, 1219], "temperature": 0.0, "avg_logprob": -0.29190458035936545, "compression_ratio": 1.6867924528301887, "no_speech_prob": 6.70132867526263e-05}, {"id": 32, "seek": 19080, "start": 212.88000000000002, "end": 217.96, "text": " open-to-man question answering. So basically try to answer questions over a very large collection", "tokens": [1269, 12, 1353, 12, 1601, 1168, 13430, 13, 407, 1936, 853, 281, 1867, 1651, 670, 257, 588, 2416, 5765], "temperature": 0.0, "avg_logprob": -0.29190458035936545, "compression_ratio": 1.6867924528301887, "no_speech_prob": 6.70132867526263e-05}, {"id": 33, "seek": 21796, "start": 217.96, "end": 223.84, "text": " of the documents. So my plan is try to quickly go over some of the state art methods in", "tokens": [295, 264, 8512, 13, 407, 452, 1393, 307, 853, 281, 2661, 352, 670, 512, 295, 264, 1785, 1523, 7150, 294], "temperature": 0.0, "avg_logprob": -0.23066973966710708, "compression_ratio": 1.6572769953051643, "no_speech_prob": 3.3146552596008405e-05}, {"id": 34, "seek": 21796, "start": 223.84, "end": 233.28, "text": " this area. Okay, so let's just get started. So first, what is the question answering?", "tokens": [341, 1859, 13, 1033, 11, 370, 718, 311, 445, 483, 1409, 13, 407, 700, 11, 437, 307, 264, 1168, 13430, 30], "temperature": 0.0, "avg_logprob": -0.23066973966710708, "compression_ratio": 1.6572769953051643, "no_speech_prob": 3.3146552596008405e-05}, {"id": 35, "seek": 21796, "start": 233.28, "end": 238.44, "text": " So the goal of question answering is to build systems that can automatically answer questions", "tokens": [407, 264, 3387, 295, 1168, 13430, 307, 281, 1322, 3652, 300, 393, 6772, 1867, 1651], "temperature": 0.0, "avg_logprob": -0.23066973966710708, "compression_ratio": 1.6572769953051643, "no_speech_prob": 3.3146552596008405e-05}, {"id": 36, "seek": 21796, "start": 238.44, "end": 245.20000000000002, "text": " posed by humans in a natural language. Question answering or let's say QA in short is", "tokens": [31399, 538, 6255, 294, 257, 3303, 2856, 13, 14464, 13430, 420, 718, 311, 584, 1249, 32, 294, 2099, 307], "temperature": 0.0, "avg_logprob": -0.23066973966710708, "compression_ratio": 1.6572769953051643, "no_speech_prob": 3.3146552596008405e-05}, {"id": 37, "seek": 24520, "start": 245.2, "end": 252.67999999999998, "text": " one of the earliest tasks and the early systems can even date back to 1960s. So here is one", "tokens": [472, 295, 264, 20573, 9608, 293, 264, 2440, 3652, 393, 754, 4002, 646, 281, 16157, 82, 13, 407, 510, 307, 472], "temperature": 0.0, "avg_logprob": -0.3719860574473505, "compression_ratio": 1.6837209302325582, "no_speech_prob": 5.135371247888543e-05}, {"id": 38, "seek": 24520, "start": 252.67999999999998, "end": 262.59999999999997, "text": " example of the early QA systems in back to 1964. So I think I see that this system is trying", "tokens": [1365, 295, 264, 2440, 1249, 32, 3652, 294, 646, 281, 34314, 13, 407, 286, 519, 286, 536, 300, 341, 1185, 307, 1382], "temperature": 0.0, "avg_logprob": -0.3719860574473505, "compression_ratio": 1.6837209302325582, "no_speech_prob": 5.135371247888543e-05}, {"id": 39, "seek": 24520, "start": 262.59999999999997, "end": 268.48, "text": " to answer questions like what do you want it and finally return on the answer that's", "tokens": [281, 1867, 1651, 411, 437, 360, 291, 528, 309, 293, 2721, 2736, 322, 264, 1867, 300, 311], "temperature": 0.0, "avg_logprob": -0.3719860574473505, "compression_ratio": 1.6837209302325582, "no_speech_prob": 5.135371247888543e-05}, {"id": 40, "seek": 24520, "start": 268.48, "end": 273.88, "text": " the graph. So to do this, so this system is better to try to find some kind of text matching", "tokens": [264, 4295, 13, 407, 281, 360, 341, 11, 370, 341, 1185, 307, 1101, 281, 853, 281, 915, 512, 733, 295, 2487, 14324], "temperature": 0.0, "avg_logprob": -0.3719860574473505, "compression_ratio": 1.6837209302325582, "no_speech_prob": 5.135371247888543e-05}, {"id": 41, "seek": 27388, "start": 273.88, "end": 278.84, "text": " between the question and some kind of text segments and by using some kind of dependency", "tokens": [1296, 264, 1168, 293, 512, 733, 295, 2487, 19904, 293, 538, 1228, 512, 733, 295, 33621], "temperature": 0.0, "avg_logprob": -0.31431402762730914, "compression_ratio": 1.902127659574468, "no_speech_prob": 1.7210904843523167e-05}, {"id": 42, "seek": 27388, "start": 278.84, "end": 285.6, "text": " analysis, I assume that you have already learned the defense of course in this class.", "tokens": [5215, 11, 286, 6552, 300, 291, 362, 1217, 3264, 264, 7654, 295, 1164, 294, 341, 1508, 13], "temperature": 0.0, "avg_logprob": -0.31431402762730914, "compression_ratio": 1.902127659574468, "no_speech_prob": 1.7210904843523167e-05}, {"id": 43, "seek": 27388, "start": 285.6, "end": 289.88, "text": " And there are many different types of the question answering problems. And we can also", "tokens": [400, 456, 366, 867, 819, 3467, 295, 264, 1168, 13430, 2740, 13, 400, 321, 393, 611], "temperature": 0.0, "avg_logprob": -0.31431402762730914, "compression_ratio": 1.902127659574468, "no_speech_prob": 1.7210904843523167e-05}, {"id": 44, "seek": 27388, "start": 289.88, "end": 295.28, "text": " look at category or this question answer problems based on the either the information source", "tokens": [574, 412, 7719, 420, 341, 1168, 1867, 2740, 2361, 322, 264, 2139, 264, 1589, 4009], "temperature": 0.0, "avg_logprob": -0.31431402762730914, "compression_ratio": 1.902127659574468, "no_speech_prob": 1.7210904843523167e-05}, {"id": 45, "seek": 27388, "start": 295.28, "end": 301.48, "text": " or the type of questions or the type of answers. So for the information source, we can build", "tokens": [420, 264, 2010, 295, 1651, 420, 264, 2010, 295, 6338, 13, 407, 337, 264, 1589, 4009, 11, 321, 393, 1322], "temperature": 0.0, "avg_logprob": -0.31431402762730914, "compression_ratio": 1.902127659574468, "no_speech_prob": 1.7210904843523167e-05}, {"id": 46, "seek": 30148, "start": 301.48, "end": 309.76, "text": " a system that can put like condition or text or very large collection of documents or", "tokens": [257, 1185, 300, 393, 829, 411, 4188, 420, 2487, 420, 588, 2416, 5765, 295, 8512, 420], "temperature": 0.0, "avg_logprob": -0.47083366691292106, "compression_ratio": 1.8717948717948718, "no_speech_prob": 1.6422905900981277e-05}, {"id": 47, "seek": 30148, "start": 309.76, "end": 316.76, "text": " even like structured database or structured knowledge based basis or even tables or images.", "tokens": [754, 411, 18519, 8149, 420, 18519, 3601, 2361, 5143, 420, 754, 8020, 420, 5267, 13], "temperature": 0.0, "avg_logprob": -0.47083366691292106, "compression_ratio": 1.8717948717948718, "no_speech_prob": 1.6422905900981277e-05}, {"id": 48, "seek": 30148, "start": 316.76, "end": 322.16, "text": " So for the question part, we can also be a system that can also affect questions or non-factual", "tokens": [407, 337, 264, 1168, 644, 11, 321, 393, 611, 312, 257, 1185, 300, 393, 611, 3345, 1651, 420, 2107, 12, 44919, 901], "temperature": 0.0, "avg_logprob": -0.47083366691292106, "compression_ratio": 1.8717948717948718, "no_speech_prob": 1.6422905900981277e-05}, {"id": 49, "seek": 30148, "start": 322.16, "end": 328.52000000000004, "text": " questions, open open questions or coastal questions or simple questions versus more complex", "tokens": [1651, 11, 1269, 1269, 1651, 420, 25050, 1651, 420, 2199, 1651, 5717, 544, 3997], "temperature": 0.0, "avg_logprob": -0.47083366691292106, "compression_ratio": 1.8717948717948718, "no_speech_prob": 1.6422905900981277e-05}, {"id": 50, "seek": 32852, "start": 328.52, "end": 334.4, "text": " or compositional questions. And for the answer type, it can also be like a short segment", "tokens": [420, 10199, 2628, 1651, 13, 400, 337, 264, 1867, 2010, 11, 309, 393, 611, 312, 411, 257, 2099, 9469], "temperature": 0.0, "avg_logprob": -0.25747185565055686, "compression_ratio": 1.9017094017094016, "no_speech_prob": 1.8613736756378785e-05}, {"id": 51, "seek": 32852, "start": 334.4, "end": 340.84, "text": " of text or a paragraph or document or list or even the yes or no questions. So just", "tokens": [295, 2487, 420, 257, 18865, 420, 4166, 420, 1329, 420, 754, 264, 2086, 420, 572, 1651, 13, 407, 445], "temperature": 0.0, "avg_logprob": -0.25747185565055686, "compression_ratio": 1.9017094017094016, "no_speech_prob": 1.8613736756378785e-05}, {"id": 52, "seek": 32852, "start": 340.84, "end": 344.84, "text": " have in mind there's many different types of question answering problems and all these", "tokens": [362, 294, 1575, 456, 311, 867, 819, 3467, 295, 1168, 13430, 2740, 293, 439, 613], "temperature": 0.0, "avg_logprob": -0.25747185565055686, "compression_ratio": 1.9017094017094016, "no_speech_prob": 1.8613736756378785e-05}, {"id": 53, "seek": 32852, "start": 344.84, "end": 350.28, "text": " problems may require very different techniques or different data or even different evaluation", "tokens": [2740, 815, 3651, 588, 819, 7512, 420, 819, 1412, 420, 754, 819, 13344], "temperature": 0.0, "avg_logprob": -0.25747185565055686, "compression_ratio": 1.9017094017094016, "no_speech_prob": 1.8613736756378785e-05}, {"id": 54, "seek": 32852, "start": 350.28, "end": 358.44, "text": " metrics to evaluate all these different problems. And the question answer has enabled a lot", "tokens": [16367, 281, 13059, 439, 613, 819, 2740, 13, 400, 264, 1168, 1867, 575, 15172, 257, 688], "temperature": 0.0, "avg_logprob": -0.25747185565055686, "compression_ratio": 1.9017094017094016, "no_speech_prob": 1.8613736756378785e-05}, {"id": 55, "seek": 35844, "start": 358.44, "end": 364.2, "text": " of the use for real-world applications. For example, today if you're just putting a question", "tokens": [295, 264, 764, 337, 957, 12, 13217, 5821, 13, 1171, 1365, 11, 965, 498, 291, 434, 445, 3372, 257, 1168], "temperature": 0.0, "avg_logprob": -0.35065770149230957, "compression_ratio": 1.6233183856502242, "no_speech_prob": 8.87293936102651e-05}, {"id": 56, "seek": 35844, "start": 364.2, "end": 370.2, "text": " in a search engine like Google, for example, you can put in a question like where's the", "tokens": [294, 257, 3164, 2848, 411, 3329, 11, 337, 1365, 11, 291, 393, 829, 294, 257, 1168, 411, 689, 311, 264], "temperature": 0.0, "avg_logprob": -0.35065770149230957, "compression_ratio": 1.6233183856502242, "no_speech_prob": 8.87293936102651e-05}, {"id": 57, "seek": 35844, "start": 370.2, "end": 375.8, "text": " deepest lake in the world. So you can see that the current system basically can find a short", "tokens": [28288, 11001, 294, 264, 1002, 13, 407, 291, 393, 536, 300, 264, 2190, 1185, 1936, 393, 915, 257, 2099], "temperature": 0.0, "avg_logprob": -0.35065770149230957, "compression_ratio": 1.6233183856502242, "no_speech_prob": 8.87293936102651e-05}, {"id": 58, "seek": 35844, "start": 375.8, "end": 385.32, "text": " snippet of text with including like like by call, by call in Siberia holds a distinction", "tokens": [35623, 302, 295, 2487, 365, 3009, 411, 411, 538, 818, 11, 538, 818, 294, 42608, 654, 9190, 257, 16844], "temperature": 0.0, "avg_logprob": -0.35065770149230957, "compression_ratio": 1.6233183856502242, "no_speech_prob": 8.87293936102651e-05}, {"id": 59, "seek": 38532, "start": 385.32, "end": 390.92, "text": " of being both the deepest lake in the world and the largest fresh water lake blah blah.", "tokens": [295, 885, 1293, 264, 28288, 11001, 294, 264, 1002, 293, 264, 6443, 4451, 1281, 11001, 12288, 12288, 13], "temperature": 0.0, "avg_logprob": -0.24755508422851563, "compression_ratio": 1.671641791044776, "no_speech_prob": 1.4501684745482635e-05}, {"id": 60, "seek": 38532, "start": 390.92, "end": 396.04, "text": " And then it can actually ping pong the crack answer which is actually a concise answer", "tokens": [400, 550, 309, 393, 767, 26151, 36164, 264, 6226, 1867, 597, 307, 767, 257, 44882, 1867], "temperature": 0.0, "avg_logprob": -0.24755508422851563, "compression_ratio": 1.671641791044776, "no_speech_prob": 1.4501684745482635e-05}, {"id": 61, "seek": 38532, "start": 396.04, "end": 403.56, "text": " which should be Siberia. And those current systems are also able to handle more complex", "tokens": [597, 820, 312, 42608, 654, 13, 400, 729, 2190, 3652, 366, 611, 1075, 281, 4813, 544, 3997], "temperature": 0.0, "avg_logprob": -0.24755508422851563, "compression_ratio": 1.671641791044776, "no_speech_prob": 1.4501684745482635e-05}, {"id": 62, "seek": 38532, "start": 403.56, "end": 408.36, "text": " questions like how two questions. I guess this is probably a question that everyone currently", "tokens": [1651, 411, 577, 732, 1651, 13, 286, 2041, 341, 307, 1391, 257, 1168, 300, 1518, 4362], "temperature": 0.0, "avg_logprob": -0.24755508422851563, "compression_ratio": 1.671641791044776, "no_speech_prob": 1.4501684745482635e-05}, {"id": 63, "seek": 38532, "start": 408.36, "end": 415.24, "text": " cares about. So the question is how can I protect myself from COVID-19? So there is another", "tokens": [12310, 466, 13, 407, 264, 1168, 307, 577, 393, 286, 2371, 2059, 490, 4566, 12, 3405, 30, 407, 456, 307, 1071], "temperature": 0.0, "avg_logprob": -0.24755508422851563, "compression_ratio": 1.671641791044776, "no_speech_prob": 1.4501684745482635e-05}, {"id": 64, "seek": 41524, "start": 415.24, "end": 420.28000000000003, "text": " really simple and short answer to this question. So you can see that the system actually returns", "tokens": [534, 2199, 293, 2099, 1867, 281, 341, 1168, 13, 407, 291, 393, 536, 300, 264, 1185, 767, 11247], "temperature": 0.0, "avg_logprob": -0.20381779897780644, "compression_ratio": 1.626086956521739, "no_speech_prob": 4.9011101509677246e-05}, {"id": 65, "seek": 41524, "start": 420.28000000000003, "end": 425.8, "text": " a very long paragraph including the best way to prevent the year is to avoid being exposed to", "tokens": [257, 588, 938, 18865, 3009, 264, 1151, 636, 281, 4871, 264, 1064, 307, 281, 5042, 885, 9495, 281], "temperature": 0.0, "avg_logprob": -0.20381779897780644, "compression_ratio": 1.626086956521739, "no_speech_prob": 4.9011101509677246e-05}, {"id": 66, "seek": 41524, "start": 425.8, "end": 432.04, "text": " this virus. And to help prevent the spread of COVID-19 you can do the following. So actually", "tokens": [341, 5752, 13, 400, 281, 854, 4871, 264, 3974, 295, 4566, 12, 3405, 291, 393, 360, 264, 3480, 13, 407, 767], "temperature": 0.0, "avg_logprob": -0.20381779897780644, "compression_ratio": 1.626086956521739, "no_speech_prob": 4.9011101509677246e-05}, {"id": 67, "seek": 41524, "start": 432.76, "end": 439.8, "text": " this paragraph is actually a summary from this CDC article if you just click this link and", "tokens": [341, 18865, 307, 767, 257, 12691, 490, 341, 17133, 7222, 498, 291, 445, 2052, 341, 2113, 293], "temperature": 0.0, "avg_logprob": -0.20381779897780644, "compression_ratio": 1.626086956521739, "no_speech_prob": 4.9011101509677246e-05}, {"id": 68, "seek": 43980, "start": 439.8, "end": 447.56, "text": " read through the article. So this is also one kind of question. And now this is a survey of the", "tokens": [1401, 807, 264, 7222, 13, 407, 341, 307, 611, 472, 733, 295, 1168, 13, 400, 586, 341, 307, 257, 8984, 295, 264], "temperature": 0.0, "avg_logprob": -0.1991331336203586, "compression_ratio": 1.7321428571428572, "no_speech_prob": 1.3841713553119916e-05}, {"id": 69, "seek": 43980, "start": 447.56, "end": 454.36, "text": " use cases for the current digital system such as Alexa or Google Home. So according to this survey", "tokens": [764, 3331, 337, 264, 2190, 4562, 1185, 1270, 382, 22595, 420, 3329, 8719, 13, 407, 4650, 281, 341, 8984], "temperature": 0.0, "avg_logprob": -0.1991331336203586, "compression_ratio": 1.7321428571428572, "no_speech_prob": 1.3841713553119916e-05}, {"id": 70, "seek": 43980, "start": 454.36, "end": 461.24, "text": " result in January 2020 which is one year ago. So you can see that also people actually really like", "tokens": [1874, 294, 7061, 4808, 597, 307, 472, 1064, 2057, 13, 407, 291, 393, 536, 300, 611, 561, 767, 534, 411], "temperature": 0.0, "avg_logprob": -0.1991331336203586, "compression_ratio": 1.7321428571428572, "no_speech_prob": 1.3841713553119916e-05}, {"id": 71, "seek": 43980, "start": 461.24, "end": 467.56, "text": " to ask questions on this digital assistance. So you can see that also question is actually the", "tokens": [281, 1029, 1651, 322, 341, 4562, 9683, 13, 407, 291, 393, 536, 300, 611, 1168, 307, 767, 264], "temperature": 0.0, "avg_logprob": -0.1991331336203586, "compression_ratio": 1.7321428571428572, "no_speech_prob": 1.3841713553119916e-05}, {"id": 72, "seek": 46756, "start": 467.56, "end": 473.64, "text": " second most used case only around after listening to music and before the check weather in a set of", "tokens": [1150, 881, 1143, 1389, 787, 926, 934, 4764, 281, 1318, 293, 949, 264, 1520, 5503, 294, 257, 992, 295], "temperature": 0.0, "avg_logprob": -0.3200448251539661, "compression_ratio": 1.5846994535519126, "no_speech_prob": 2.3159382180892862e-05}, {"id": 73, "seek": 46756, "start": 473.64, "end": 482.52, "text": " time timer. So question is really useful in these digital systems. Another very famous example", "tokens": [565, 19247, 13, 407, 1168, 307, 534, 4420, 294, 613, 4562, 3652, 13, 3996, 588, 4618, 1365], "temperature": 0.0, "avg_logprob": -0.3200448251539661, "compression_ratio": 1.5846994535519126, "no_speech_prob": 2.3159382180892862e-05}, {"id": 74, "seek": 46756, "start": 482.52, "end": 490.28, "text": " of the question answering system is this IBM was from the question answering system. So in 2011", "tokens": [295, 264, 1168, 13430, 1185, 307, 341, 23487, 390, 490, 264, 1168, 13430, 1185, 13, 407, 294, 10154], "temperature": 0.0, "avg_logprob": -0.3200448251539661, "compression_ratio": 1.5846994535519126, "no_speech_prob": 2.3159382180892862e-05}, {"id": 75, "seek": 49028, "start": 490.28, "end": 497.96, "text": " so this IBM was not used to a system has been shown to be too national. Jeffrey Champions in answering", "tokens": [370, 341, 23487, 390, 406, 1143, 281, 257, 1185, 575, 668, 4898, 281, 312, 886, 4048, 13, 28721, 14391, 294, 13430], "temperature": 0.0, "avg_logprob": -0.42261448910361843, "compression_ratio": 1.6115702479338843, "no_speech_prob": 1.860438169387635e-05}, {"id": 76, "seek": 49028, "start": 497.96, "end": 505.4, "text": " chapter questions. So this is kind of this all like a historical event and it's in the LK history.", "tokens": [7187, 1651, 13, 407, 341, 307, 733, 295, 341, 439, 411, 257, 8584, 2280, 293, 309, 311, 294, 264, 441, 42, 2503, 13], "temperature": 0.0, "avg_logprob": -0.42261448910361843, "compression_ratio": 1.6115702479338843, "no_speech_prob": 1.860438169387635e-05}, {"id": 77, "seek": 49028, "start": 507.08, "end": 512.68, "text": " So if you look at the you are working of this system more closely. So you can see that it is", "tokens": [407, 498, 291, 574, 412, 264, 291, 366, 1364, 295, 341, 1185, 544, 8185, 13, 407, 291, 393, 536, 300, 309, 307], "temperature": 0.0, "avg_logprob": -0.42261448910361843, "compression_ratio": 1.6115702479338843, "no_speech_prob": 1.860438169387635e-05}, {"id": 78, "seek": 49028, "start": 512.68, "end": 518.76, "text": " actually a very complicated and highly modularized system. So it's a system that builds on both", "tokens": [767, 257, 588, 6179, 293, 5405, 31111, 1602, 1185, 13, 407, 309, 311, 257, 1185, 300, 15182, 322, 1293], "temperature": 0.0, "avg_logprob": -0.42261448910361843, "compression_ratio": 1.6115702479338843, "no_speech_prob": 1.860438169387635e-05}, {"id": 79, "seek": 51876, "start": 518.76, "end": 526.2, "text": " the unstructured text and also the structured data. So by looking at the system if you go from the", "tokens": [264, 18799, 46847, 2487, 293, 611, 264, 18519, 1412, 13, 407, 538, 1237, 412, 264, 1185, 498, 291, 352, 490, 264], "temperature": 0.0, "avg_logprob": -0.22411340770154897, "compression_ratio": 1.8625954198473282, "no_speech_prob": 4.001956040156074e-05}, {"id": 80, "seek": 51876, "start": 526.2, "end": 531.96, "text": " left to right you can see that this system consists of the four stages including the question processing.", "tokens": [1411, 281, 558, 291, 393, 536, 300, 341, 1185, 14689, 295, 264, 1451, 10232, 3009, 264, 1168, 9007, 13], "temperature": 0.0, "avg_logprob": -0.22411340770154897, "compression_ratio": 1.8625954198473282, "no_speech_prob": 4.001956040156074e-05}, {"id": 81, "seek": 51876, "start": 532.52, "end": 537.48, "text": " The candidate answer generation and the candidate answer scoring and the confidence margin", "tokens": [440, 11532, 1867, 5125, 293, 264, 11532, 1867, 22358, 293, 264, 6687, 10270], "temperature": 0.0, "avg_logprob": -0.22411340770154897, "compression_ratio": 1.8625954198473282, "no_speech_prob": 4.001956040156074e-05}, {"id": 82, "seek": 51876, "start": 537.48, "end": 542.04, "text": " and ranking. And then if you look at each stage you can see that there are many different LP", "tokens": [293, 17833, 13, 400, 550, 498, 291, 574, 412, 1184, 3233, 291, 393, 536, 300, 456, 366, 867, 819, 441, 47], "temperature": 0.0, "avg_logprob": -0.22411340770154897, "compression_ratio": 1.8625954198473282, "no_speech_prob": 4.001956040156074e-05}, {"id": 83, "seek": 51876, "start": 542.04, "end": 548.4399999999999, "text": " techniques that have actually included in this complex QE system including question classification,", "tokens": [7512, 300, 362, 767, 5556, 294, 341, 3997, 1249, 36, 1185, 3009, 1168, 21538, 11], "temperature": 0.0, "avg_logprob": -0.22411340770154897, "compression_ratio": 1.8625954198473282, "no_speech_prob": 4.001956040156074e-05}, {"id": 84, "seek": 54844, "start": 548.44, "end": 553.96, "text": " parsing, relation extraction, correctness. So it's actually there really a lot of the LP systems", "tokens": [21156, 278, 11, 9721, 30197, 11, 3006, 1287, 13, 407, 309, 311, 767, 456, 534, 257, 688, 295, 264, 441, 47, 3652], "temperature": 0.0, "avg_logprob": -0.3267606847426471, "compression_ratio": 1.6944444444444444, "no_speech_prob": 3.0208258976927027e-05}, {"id": 85, "seek": 54844, "start": 553.96, "end": 559.96, "text": " modules that have been included. And this system has been over 10 years actually exactly 10 years now", "tokens": [16679, 300, 362, 668, 5556, 13, 400, 341, 1185, 575, 668, 670, 1266, 924, 767, 2293, 1266, 924, 586], "temperature": 0.0, "avg_logprob": -0.3267606847426471, "compression_ratio": 1.6944444444444444, "no_speech_prob": 3.0208258976927027e-05}, {"id": 86, "seek": 54844, "start": 560.6, "end": 565.1600000000001, "text": " and this is actually represented in a safe art like 10 years ago at that time.", "tokens": [293, 341, 307, 767, 10379, 294, 257, 3273, 1523, 411, 1266, 924, 2057, 412, 300, 565, 13], "temperature": 0.0, "avg_logprob": -0.3267606847426471, "compression_ratio": 1.6944444444444444, "no_speech_prob": 3.0208258976927027e-05}, {"id": 87, "seek": 54844, "start": 568.0400000000001, "end": 573.48, "text": " So we know that this class is about deep learning. So today deep learning has completely", "tokens": [407, 321, 458, 300, 341, 1508, 307, 466, 2452, 2539, 13, 407, 965, 2452, 2539, 575, 2584], "temperature": 0.0, "avg_logprob": -0.3267606847426471, "compression_ratio": 1.6944444444444444, "no_speech_prob": 3.0208258976927027e-05}, {"id": 88, "seek": 57348, "start": 573.48, "end": 579.24, "text": " really transformed the landscape of the question answering systems. So there's no doubt that we", "tokens": [534, 16894, 264, 9661, 295, 264, 1168, 13430, 3652, 13, 407, 456, 311, 572, 6385, 300, 321], "temperature": 0.0, "avg_logprob": -0.2526823799565153, "compression_ratio": 1.8893280632411067, "no_speech_prob": 3.069357626372948e-05}, {"id": 89, "seek": 57348, "start": 579.24, "end": 584.76, "text": " can say that almost all the states are question answering systems today are built on top of the", "tokens": [393, 584, 300, 1920, 439, 264, 4368, 366, 1168, 13430, 3652, 965, 366, 3094, 322, 1192, 295, 264], "temperature": 0.0, "avg_logprob": -0.2526823799565153, "compression_ratio": 1.8893280632411067, "no_speech_prob": 3.069357626372948e-05}, {"id": 90, "seek": 57348, "start": 584.76, "end": 590.28, "text": " end-training of the deep learning networks and the pretty chain language models such as BERT.", "tokens": [917, 12, 17227, 1760, 295, 264, 2452, 2539, 9590, 293, 264, 1238, 5021, 2856, 5245, 1270, 382, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.2526823799565153, "compression_ratio": 1.8893280632411067, "no_speech_prob": 3.069357626372948e-05}, {"id": 91, "seek": 57348, "start": 590.28, "end": 594.9200000000001, "text": " So today in this lecture we also going to learn a lot of deep learning models in for question", "tokens": [407, 965, 294, 341, 7991, 321, 611, 516, 281, 1466, 257, 688, 295, 2452, 2539, 5245, 294, 337, 1168], "temperature": 0.0, "avg_logprob": -0.2526823799565153, "compression_ratio": 1.8893280632411067, "no_speech_prob": 3.069357626372948e-05}, {"id": 92, "seek": 57348, "start": 594.9200000000001, "end": 601.08, "text": " answering. And this statement is probably also true for almost all the LP problems that we can see", "tokens": [13430, 13, 400, 341, 5629, 307, 1391, 611, 2074, 337, 1920, 439, 264, 441, 47, 2740, 300, 321, 393, 536], "temperature": 0.0, "avg_logprob": -0.2526823799565153, "compression_ratio": 1.8893280632411067, "no_speech_prob": 3.069357626372948e-05}, {"id": 93, "seek": 60108, "start": 601.08, "end": 606.44, "text": " today. But we can also argue that question answering is probably one of those fields that we have", "tokens": [965, 13, 583, 321, 393, 611, 9695, 300, 1168, 13430, 307, 1391, 472, 295, 729, 7909, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.2245803956062563, "compression_ratio": 1.5536723163841808, "no_speech_prob": 1.1291438568150625e-05}, {"id": 94, "seek": 60108, "start": 606.44, "end": 612.12, "text": " seen the most remarkable progress in the last couple of years driven by deep learning.", "tokens": [1612, 264, 881, 12802, 4205, 294, 264, 1036, 1916, 295, 924, 9555, 538, 2452, 2539, 13], "temperature": 0.0, "avg_logprob": -0.2245803956062563, "compression_ratio": 1.5536723163841808, "no_speech_prob": 1.1291438568150625e-05}, {"id": 95, "seek": 60108, "start": 616.9200000000001, "end": 623.72, "text": " So in this lecture I would be mostly focused on like focusing on the text based or textual", "tokens": [407, 294, 341, 7991, 286, 576, 312, 5240, 5178, 322, 411, 8416, 322, 264, 2487, 2361, 420, 2487, 901], "temperature": 0.0, "avg_logprob": -0.2245803956062563, "compression_ratio": 1.5536723163841808, "no_speech_prob": 1.1291438568150625e-05}, {"id": 96, "seek": 62372, "start": 623.72, "end": 629.88, "text": " question answering problems. So basically we are trying to answer questions based on the unstructured text.", "tokens": [1168, 13430, 2740, 13, 407, 1936, 321, 366, 1382, 281, 1867, 1651, 2361, 322, 264, 18799, 46847, 2487, 13], "temperature": 0.0, "avg_logprob": -0.23570997574750116, "compression_ratio": 1.87109375, "no_speech_prob": 2.6237879865220748e-05}, {"id": 97, "seek": 62372, "start": 629.88, "end": 635.4, "text": " So before I start I jump to that part. I also would quickly point out that there are many", "tokens": [407, 949, 286, 722, 286, 3012, 281, 300, 644, 13, 286, 611, 576, 2661, 935, 484, 300, 456, 366, 867], "temperature": 0.0, "avg_logprob": -0.23570997574750116, "compression_ratio": 1.87109375, "no_speech_prob": 2.6237879865220748e-05}, {"id": 98, "seek": 62372, "start": 635.4, "end": 640.28, "text": " other really bigger question answering problems and the issue of them can be really a", "tokens": [661, 534, 3801, 1168, 13430, 2740, 293, 264, 2734, 295, 552, 393, 312, 534, 257], "temperature": 0.0, "avg_logprob": -0.23570997574750116, "compression_ratio": 1.87109375, "no_speech_prob": 2.6237879865220748e-05}, {"id": 99, "seek": 62372, "start": 640.28, "end": 646.52, "text": " like a big subfield in NLP and they actually have very different challenges and also model designs.", "tokens": [411, 257, 955, 1422, 7610, 294, 426, 45196, 293, 436, 767, 362, 588, 819, 4759, 293, 611, 2316, 11347, 13], "temperature": 0.0, "avg_logprob": -0.23570997574750116, "compression_ratio": 1.87109375, "no_speech_prob": 2.6237879865220748e-05}, {"id": 100, "seek": 62372, "start": 647.1600000000001, "end": 651.8000000000001, "text": " So one bigger class of this question answer problem is this knowledge based question answering.", "tokens": [407, 472, 3801, 1508, 295, 341, 1168, 1867, 1154, 307, 341, 3601, 2361, 1168, 13430, 13], "temperature": 0.0, "avg_logprob": -0.23570997574750116, "compression_ratio": 1.87109375, "no_speech_prob": 2.6237879865220748e-05}, {"id": 101, "seek": 65180, "start": 651.8, "end": 656.76, "text": " So basically we want to build question answering systems to answer questions that can answer", "tokens": [407, 1936, 321, 528, 281, 1322, 1168, 13430, 3652, 281, 1867, 1651, 300, 393, 1867], "temperature": 0.0, "avg_logprob": -0.1588361456587508, "compression_ratio": 1.854922279792746, "no_speech_prob": 6.045238023943966e-06}, {"id": 102, "seek": 65180, "start": 657.56, "end": 663.7199999999999, "text": " answer questions over a very large database. So to solve this problem some approaches need to", "tokens": [1867, 1651, 670, 257, 588, 2416, 8149, 13, 407, 281, 5039, 341, 1154, 512, 11587, 643, 281], "temperature": 0.0, "avg_logprob": -0.1588361456587508, "compression_ratio": 1.854922279792746, "no_speech_prob": 6.045238023943966e-06}, {"id": 103, "seek": 65180, "start": 663.7199999999999, "end": 669.4799999999999, "text": " take this question and convert this question into some kind of logic forms and this kind of logic", "tokens": [747, 341, 1168, 293, 7620, 341, 1168, 666, 512, 733, 295, 9952, 6422, 293, 341, 733, 295, 9952], "temperature": 0.0, "avg_logprob": -0.1588361456587508, "compression_ratio": 1.854922279792746, "no_speech_prob": 6.045238023943966e-06}, {"id": 104, "seek": 65180, "start": 669.4799999999999, "end": 674.1999999999999, "text": " forms can be executed against this database to give you the final answer.", "tokens": [6422, 393, 312, 17577, 1970, 341, 8149, 281, 976, 291, 264, 2572, 1867, 13], "temperature": 0.0, "avg_logprob": -0.1588361456587508, "compression_ratio": 1.854922279792746, "no_speech_prob": 6.045238023943966e-06}, {"id": 105, "seek": 67420, "start": 674.2, "end": 681.8000000000001, "text": " And another class bigger class of the question answering problem is called visual question answering.", "tokens": [400, 1071, 1508, 3801, 1508, 295, 264, 1168, 13430, 1154, 307, 1219, 5056, 1168, 13430, 13], "temperature": 0.0, "avg_logprob": -0.2103446388244629, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.286029707145644e-06}, {"id": 106, "seek": 67420, "start": 681.8000000000001, "end": 686.5200000000001, "text": " So it's basically need to answer questions based on the images. So this problem basically", "tokens": [407, 309, 311, 1936, 643, 281, 1867, 1651, 2361, 322, 264, 5267, 13, 407, 341, 1154, 1936], "temperature": 0.0, "avg_logprob": -0.2103446388244629, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.286029707145644e-06}, {"id": 107, "seek": 67420, "start": 686.5200000000001, "end": 692.2, "text": " requires both understanding of the questions and also images and there is actually a very active", "tokens": [7029, 1293, 3701, 295, 264, 1651, 293, 611, 5267, 293, 456, 307, 767, 257, 588, 4967], "temperature": 0.0, "avg_logprob": -0.2103446388244629, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.286029707145644e-06}, {"id": 108, "seek": 67420, "start": 692.2, "end": 697.24, "text": " field between the computer vision and NLP. So if we are interested in all these type of problems", "tokens": [2519, 1296, 264, 3820, 5201, 293, 426, 45196, 13, 407, 498, 321, 366, 3102, 294, 439, 613, 2010, 295, 2740], "temperature": 0.0, "avg_logprob": -0.2103446388244629, "compression_ratio": 1.8571428571428572, "no_speech_prob": 4.286029707145644e-06}, {"id": 109, "seek": 69724, "start": 697.24, "end": 704.52, "text": " I encourage you to check out these problems but I'm not going to give you these problems today.", "tokens": [286, 5373, 291, 281, 1520, 484, 613, 2740, 457, 286, 478, 406, 516, 281, 976, 291, 613, 2740, 965, 13], "temperature": 0.0, "avg_logprob": -0.298154730545847, "compression_ratio": 1.7824074074074074, "no_speech_prob": 9.970231076295022e-06}, {"id": 110, "seek": 69724, "start": 705.48, "end": 711.64, "text": " Okay so next I'm going to start with a part to review comprehension. I just want to quickly check", "tokens": [1033, 370, 958, 286, 478, 516, 281, 722, 365, 257, 644, 281, 3131, 44991, 13, 286, 445, 528, 281, 2661, 1520], "temperature": 0.0, "avg_logprob": -0.298154730545847, "compression_ratio": 1.7824074074074074, "no_speech_prob": 9.970231076295022e-06}, {"id": 111, "seek": 69724, "start": 711.64, "end": 717.24, "text": " if there are any quick questions I can answer before I get started. I'll start with a part to you.", "tokens": [498, 456, 366, 604, 1702, 1651, 286, 393, 1867, 949, 286, 483, 1409, 13, 286, 603, 722, 365, 257, 644, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.298154730545847, "compression_ratio": 1.7824074074074074, "no_speech_prob": 9.970231076295022e-06}, {"id": 112, "seek": 69724, "start": 718.04, "end": 724.04, "text": " Now I think we could do now. Okay so yeah so let's talk about the review comprehension then.", "tokens": [823, 286, 519, 321, 727, 360, 586, 13, 1033, 370, 1338, 370, 718, 311, 751, 466, 264, 3131, 44991, 550, 13], "temperature": 0.0, "avg_logprob": -0.298154730545847, "compression_ratio": 1.7824074074074074, "no_speech_prob": 9.970231076295022e-06}, {"id": 113, "seek": 72404, "start": 724.04, "end": 729.0, "text": " So a read comprehension is a basic problem that we want to compare", "tokens": [407, 257, 1401, 44991, 307, 257, 3875, 1154, 300, 321, 528, 281, 6794], "temperature": 0.0, "avg_logprob": -0.3670003754752023, "compression_ratio": 1.8385416666666667, "no_speech_prob": 2.8835131161031313e-05}, {"id": 114, "seek": 72404, "start": 729.0, "end": 734.76, "text": " and passive text and answer questions about the content. So it's an input of discovering", "tokens": [293, 1320, 488, 2487, 293, 1867, 1651, 466, 264, 2701, 13, 407, 309, 311, 364, 4846, 295, 24773], "temperature": 0.0, "avg_logprob": -0.3670003754752023, "compression_ratio": 1.8385416666666667, "no_speech_prob": 2.8835131161031313e-05}, {"id": 115, "seek": 72404, "start": 734.76, "end": 740.5999999999999, "text": " visual passive text a question and going to return the answer that actually can't answer this question.", "tokens": [5056, 1320, 488, 2487, 257, 1168, 293, 516, 281, 2736, 264, 1867, 300, 767, 393, 380, 1867, 341, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3670003754752023, "compression_ratio": 1.8385416666666667, "no_speech_prob": 2.8835131161031313e-05}, {"id": 116, "seek": 72404, "start": 740.5999999999999, "end": 748.4399999999999, "text": " So here is one example. So let's talk here is a passive text and we want to answer a question", "tokens": [407, 510, 307, 472, 1365, 13, 407, 718, 311, 751, 510, 307, 257, 1320, 488, 2487, 293, 321, 528, 281, 1867, 257, 1168], "temperature": 0.0, "avg_logprob": -0.3670003754752023, "compression_ratio": 1.8385416666666667, "no_speech_prob": 2.8835131161031313e-05}, {"id": 117, "seek": 74844, "start": 748.44, "end": 754.2800000000001, "text": " the question is what language the test will start if while you sport. Okay so I'm going to pause like", "tokens": [264, 1168, 307, 437, 2856, 264, 1500, 486, 722, 498, 1339, 291, 7282, 13, 1033, 370, 286, 478, 516, 281, 10465, 411], "temperature": 0.0, "avg_logprob": -0.38134018580118817, "compression_ratio": 1.3900709219858156, "no_speech_prob": 1.1653455658233725e-05}, {"id": 118, "seek": 74844, "start": 754.2800000000001, "end": 764.36, "text": " five or 10 seconds and see if you will find the answer to this question based on this passage.", "tokens": [1732, 420, 1266, 3949, 293, 536, 498, 291, 486, 915, 264, 1867, 281, 341, 1168, 2361, 322, 341, 11497, 13], "temperature": 0.0, "avg_logprob": -0.38134018580118817, "compression_ratio": 1.3900709219858156, "no_speech_prob": 1.1653455658233725e-05}, {"id": 119, "seek": 76436, "start": 764.36, "end": 784.92, "text": " And you guys. Okay. Well people stress the German. Yeah, German is a crap. So the answer should be", "tokens": [400, 291, 1074, 13, 1033, 13, 1042, 561, 4244, 264, 6521, 13, 865, 11, 6521, 307, 257, 12426, 13, 407, 264, 1867, 820, 312], "temperature": 0.0, "avg_logprob": -0.448919880146883, "compression_ratio": 1.3426573426573427, "no_speech_prob": 0.00014393914898391813}, {"id": 120, "seek": 76436, "start": 784.92, "end": 790.52, "text": " German. So basically answer this question so you need to find this sentence like in 1861 test", "tokens": [6521, 13, 407, 1936, 1867, 341, 1168, 370, 291, 643, 281, 915, 341, 8174, 411, 294, 2443, 31537, 1500], "temperature": 0.0, "avg_logprob": -0.448919880146883, "compression_ratio": 1.3426573426573427, "no_speech_prob": 0.00014393914898391813}, {"id": 121, "seek": 79052, "start": 790.52, "end": 796.68, "text": " out 10 years school where he started German arithmetic and religion and only the German is a language", "tokens": [484, 1266, 924, 1395, 689, 415, 1409, 6521, 42973, 293, 7561, 293, 787, 264, 6521, 307, 257, 2856], "temperature": 0.0, "avg_logprob": -0.36862569955679086, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.9825936760753393e-05}, {"id": 122, "seek": 79052, "start": 796.68, "end": 803.0, "text": " so the answer to this question should be German. Okay here is another example. Okay another passive", "tokens": [370, 264, 1867, 281, 341, 1168, 820, 312, 6521, 13, 1033, 510, 307, 1071, 1365, 13, 1033, 1071, 1320, 488], "temperature": 0.0, "avg_logprob": -0.36862569955679086, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.9825936760753393e-05}, {"id": 123, "seek": 79052, "start": 803.0, "end": 812.36, "text": " text and the question is which linguistic minority larger painting or Malal Yalan I think yeah.", "tokens": [2487, 293, 264, 1168, 307, 597, 43002, 16166, 4833, 5370, 420, 5746, 304, 398, 14163, 286, 519, 1338, 13], "temperature": 0.0, "avg_logprob": -0.36862569955679086, "compression_ratio": 1.5549738219895288, "no_speech_prob": 1.9825936760753393e-05}, {"id": 124, "seek": 81236, "start": 812.36, "end": 818.44, "text": " Five seconds.", "tokens": [9436, 3949, 13], "temperature": 0.0, "avg_logprob": -0.28859519958496094, "compression_ratio": 1.5220588235294117, "no_speech_prob": 5.644249904435128e-05}, {"id": 125, "seek": 81236, "start": 829.64, "end": 835.4, "text": " Okay so the answer to this question should be Hindi. So this probably is not very hard question", "tokens": [1033, 370, 264, 1867, 281, 341, 1168, 820, 312, 36225, 13, 407, 341, 1391, 307, 406, 588, 1152, 1168], "temperature": 0.0, "avg_logprob": -0.28859519958496094, "compression_ratio": 1.5220588235294117, "no_speech_prob": 5.644249904435128e-05}, {"id": 126, "seek": 81236, "start": 835.4, "end": 840.44, "text": " for humans it actually a pretty hard question for machines because to get this question correctly", "tokens": [337, 6255, 309, 767, 257, 1238, 1152, 1168, 337, 8379, 570, 281, 483, 341, 1168, 8944], "temperature": 0.0, "avg_logprob": -0.28859519958496094, "compression_ratio": 1.5220588235294117, "no_speech_prob": 5.644249904435128e-05}, {"id": 127, "seek": 84044, "start": 840.44, "end": 845.96, "text": " so the machine is basically to understand that for the Hindi like 3.3% of the population", "tokens": [370, 264, 3479, 307, 1936, 281, 1223, 300, 337, 264, 36225, 411, 805, 13, 18, 4, 295, 264, 4415], "temperature": 0.0, "avg_logprob": -0.18140058806448273, "compression_ratio": 1.625531914893617, "no_speech_prob": 1.6695244994480163e-05}, {"id": 128, "seek": 84044, "start": 845.96, "end": 854.44, "text": " speaks Hindi and only like 1.27% speaks Malal Yalan this language and then also compare these two", "tokens": [10789, 36225, 293, 787, 411, 502, 13, 10076, 4, 10789, 5746, 304, 398, 14163, 341, 2856, 293, 550, 611, 6794, 613, 732], "temperature": 0.0, "avg_logprob": -0.18140058806448273, "compression_ratio": 1.625531914893617, "no_speech_prob": 1.6695244994480163e-05}, {"id": 129, "seek": 84044, "start": 854.44, "end": 861.48, "text": " numbers and the final case 3% 3.3% is a bigger number so the answer should be Hindi to this question.", "tokens": [3547, 293, 264, 2572, 1389, 805, 4, 805, 13, 18, 4, 307, 257, 3801, 1230, 370, 264, 1867, 820, 312, 36225, 281, 341, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18140058806448273, "compression_ratio": 1.625531914893617, "no_speech_prob": 1.6695244994480163e-05}, {"id": 130, "seek": 84044, "start": 863.5600000000001, "end": 868.44, "text": " Okay so next I'm going to talk a little bit so why do we care about this problem so why do we", "tokens": [1033, 370, 958, 286, 478, 516, 281, 751, 257, 707, 857, 370, 983, 360, 321, 1127, 466, 341, 1154, 370, 983, 360, 321], "temperature": 0.0, "avg_logprob": -0.18140058806448273, "compression_ratio": 1.625531914893617, "no_speech_prob": 1.6695244994480163e-05}, {"id": 131, "seek": 86844, "start": 868.44, "end": 874.12, "text": " care about the reading comprehension problem so besides that it actually tries many useful real", "tokens": [1127, 466, 264, 3760, 44991, 1154, 370, 11868, 300, 309, 767, 9898, 867, 4420, 957], "temperature": 0.0, "avg_logprob": -0.3051458157991108, "compression_ratio": 1.8215962441314555, "no_speech_prob": 2.9522721888497472e-05}, {"id": 132, "seek": 86844, "start": 874.12, "end": 880.7600000000001, "text": " work practical applications so as I already saw some examples at the beginning I think there are", "tokens": [589, 8496, 5821, 370, 382, 286, 1217, 1866, 512, 5110, 412, 264, 2863, 286, 519, 456, 366], "temperature": 0.0, "avg_logprob": -0.3051458157991108, "compression_ratio": 1.8215962441314555, "no_speech_prob": 2.9522721888497472e-05}, {"id": 133, "seek": 86844, "start": 880.7600000000001, "end": 888.12, "text": " also two other few reasons so the first reason also besides the application the first reason is", "tokens": [611, 732, 661, 1326, 4112, 370, 264, 700, 1778, 611, 11868, 264, 3861, 264, 700, 1778, 307], "temperature": 0.0, "avg_logprob": -0.3051458157991108, "compression_ratio": 1.8215962441314555, "no_speech_prob": 2.9522721888497472e-05}, {"id": 134, "seek": 86844, "start": 888.9200000000001, "end": 894.6800000000001, "text": " so reading comprehension has been also viewed as a very important test path for evaluating how well", "tokens": [370, 3760, 44991, 575, 668, 611, 19174, 382, 257, 588, 1021, 1500, 3100, 337, 27479, 577, 731], "temperature": 0.0, "avg_logprob": -0.3051458157991108, "compression_ratio": 1.8215962441314555, "no_speech_prob": 2.9522721888497472e-05}, {"id": 135, "seek": 89468, "start": 894.68, "end": 901.88, "text": " computer systems understand human language so this is really just similar to how we humans actually", "tokens": [3820, 3652, 1223, 1952, 2856, 370, 341, 307, 534, 445, 2531, 281, 577, 321, 6255, 767], "temperature": 0.0, "avg_logprob": -0.2461976231755437, "compression_ratio": 1.935, "no_speech_prob": 2.9292277758941054e-05}, {"id": 136, "seek": 89468, "start": 901.88, "end": 907.56, "text": " test a reading comprehension test to evaluate how well we actually understand what language", "tokens": [1500, 257, 3760, 44991, 1500, 281, 13059, 577, 731, 321, 767, 1223, 437, 2856], "temperature": 0.0, "avg_logprob": -0.2461976231755437, "compression_ratio": 1.935, "no_speech_prob": 2.9292277758941054e-05}, {"id": 137, "seek": 89468, "start": 907.56, "end": 912.68, "text": " so this is also the way that we actually post questions to test the machines language understanding", "tokens": [370, 341, 307, 611, 264, 636, 300, 321, 767, 2183, 1651, 281, 1500, 264, 8379, 2856, 3701], "temperature": 0.0, "avg_logprob": -0.2461976231755437, "compression_ratio": 1.935, "no_speech_prob": 2.9292277758941054e-05}, {"id": 138, "seek": 89468, "start": 912.68, "end": 920.68, "text": " and language understanding ability so this actually has been formally stated in back in 1937 by", "tokens": [293, 2856, 3701, 3485, 370, 341, 767, 575, 668, 25983, 11323, 294, 646, 294, 1294, 12851, 538], "temperature": 0.0, "avg_logprob": -0.2461976231755437, "compression_ratio": 1.935, "no_speech_prob": 2.9292277758941054e-05}, {"id": 139, "seek": 92068, "start": 920.68, "end": 925.9599999999999, "text": " Wendy Lengard in her dissertation so she's fitting the saying is that she said that", "tokens": [21850, 441, 1501, 515, 294, 720, 39555, 370, 750, 311, 15669, 264, 1566, 307, 300, 750, 848, 300], "temperature": 0.0, "avg_logprob": -0.2966150442759196, "compression_ratio": 1.8844621513944224, "no_speech_prob": 1.0127017958438955e-05}, {"id": 140, "seek": 92068, "start": 926.68, "end": 932.1999999999999, "text": " these questions can be devised to query any aspect of test comprehension so be it to answer", "tokens": [613, 1651, 393, 312, 1905, 2640, 281, 14581, 604, 4171, 295, 1500, 44991, 370, 312, 309, 281, 1867], "temperature": 0.0, "avg_logprob": -0.2966150442759196, "compression_ratio": 1.8844621513944224, "no_speech_prob": 1.0127017958438955e-05}, {"id": 141, "seek": 92068, "start": 932.1999999999999, "end": 937.3199999999999, "text": " questions is the strongest possible demonstration understanding so that's why reading comprehension", "tokens": [1651, 307, 264, 16595, 1944, 16520, 3701, 370, 300, 311, 983, 3760, 44991], "temperature": 0.0, "avg_logprob": -0.2966150442759196, "compression_ratio": 1.8844621513944224, "no_speech_prob": 1.0127017958438955e-05}, {"id": 142, "seek": 92068, "start": 937.3199999999999, "end": 943.16, "text": " can be a very important test path because we can't be devised design very complex questions to test", "tokens": [393, 312, 257, 588, 1021, 1500, 3100, 570, 321, 393, 380, 312, 1905, 2640, 1715, 588, 3997, 1651, 281, 1500], "temperature": 0.0, "avg_logprob": -0.2966150442759196, "compression_ratio": 1.8844621513944224, "no_speech_prob": 1.0127017958438955e-05}, {"id": 143, "seek": 92068, "start": 943.16, "end": 950.4399999999999, "text": " that and also I think there's another interesting and important reason that reading comprehension", "tokens": [300, 293, 611, 286, 519, 456, 311, 1071, 1880, 293, 1021, 1778, 300, 3760, 44991], "temperature": 0.0, "avg_logprob": -0.2966150442759196, "compression_ratio": 1.8844621513944224, "no_speech_prob": 1.0127017958438955e-05}, {"id": 144, "seek": 95044, "start": 950.44, "end": 956.6800000000001, "text": " is important so in the recent few years so many some researchers actually found that okay so for", "tokens": [307, 1021, 370, 294, 264, 5162, 1326, 924, 370, 867, 512, 10309, 767, 1352, 300, 1392, 370, 337], "temperature": 0.0, "avg_logprob": -0.324733689252068, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.8617392925079912e-05}, {"id": 145, "seek": 95044, "start": 956.6800000000001, "end": 962.6, "text": " many other NLP tests that we also reduced them to a reading comprehension problem so I'm going", "tokens": [867, 661, 426, 45196, 6921, 300, 321, 611, 9212, 552, 281, 257, 3760, 44991, 1154, 370, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.324733689252068, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.8617392925079912e-05}, {"id": 146, "seek": 95044, "start": 962.6, "end": 969.48, "text": " to give you two examples so one example is already a information extraction so basically if we want to", "tokens": [281, 976, 291, 732, 5110, 370, 472, 1365, 307, 1217, 257, 1589, 30197, 370, 1936, 498, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.324733689252068, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.8617392925079912e-05}, {"id": 147, "seek": 95044, "start": 970.5200000000001, "end": 977.24, "text": " so give answers to a person like subject brought Obama give a relation educated at so we want to", "tokens": [370, 976, 6338, 281, 257, 954, 411, 3983, 3038, 9560, 976, 257, 9721, 15872, 412, 370, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.324733689252068, "compression_ratio": 1.6567796610169492, "no_speech_prob": 1.8617392925079912e-05}, {"id": 148, "seek": 97724, "start": 977.24, "end": 983.5600000000001, "text": " fill in what is a fill in this question mark and figure out okay where Barack Obama was educated at", "tokens": [2836, 294, 437, 307, 257, 2836, 294, 341, 1168, 1491, 293, 2573, 484, 1392, 689, 31705, 9560, 390, 15872, 412], "temperature": 0.0, "avg_logprob": -0.28053170442581177, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.4733253010490444e-05}, {"id": 149, "seek": 97724, "start": 983.5600000000001, "end": 989.64, "text": " so one way to solve this problem is basically trying to cover this relation into a question", "tokens": [370, 472, 636, 281, 5039, 341, 1154, 307, 1936, 1382, 281, 2060, 341, 9721, 666, 257, 1168], "temperature": 0.0, "avg_logprob": -0.28053170442581177, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.4733253010490444e-05}, {"id": 150, "seek": 97724, "start": 989.64, "end": 995.64, "text": " so where did the Barack Obama graduate from and take a relevant piece of text and then", "tokens": [370, 689, 630, 264, 31705, 9560, 8080, 490, 293, 747, 257, 7340, 2522, 295, 2487, 293, 550], "temperature": 0.0, "avg_logprob": -0.28053170442581177, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.4733253010490444e-05}, {"id": 151, "seek": 97724, "start": 995.64, "end": 1001.24, "text": " by writing a reading comprehension problem then basically we can find out the extract the correct", "tokens": [538, 3579, 257, 3760, 44991, 1154, 550, 1936, 321, 393, 915, 484, 264, 8947, 264, 3006], "temperature": 0.0, "avg_logprob": -0.28053170442581177, "compression_ratio": 1.7723880597014925, "no_speech_prob": 1.4733253010490444e-05}, {"id": 152, "seek": 100124, "start": 1001.24, "end": 1008.28, "text": " answer should be Columbia University that is also the output of this information extraction system", "tokens": [1867, 820, 312, 17339, 3535, 300, 307, 611, 264, 5598, 295, 341, 1589, 30197, 1185], "temperature": 0.0, "avg_logprob": -0.3135116673723052, "compression_ratio": 1.6724890829694323, "no_speech_prob": 5.420665729616303e-06}, {"id": 153, "seek": 100124, "start": 1008.28, "end": 1012.76, "text": " another example is actually called a cement for the labeling I'm not sure if you have noticed", "tokens": [1071, 1365, 307, 767, 1219, 257, 19729, 337, 264, 40244, 286, 478, 406, 988, 498, 291, 362, 5694], "temperature": 0.0, "avg_logprob": -0.3135116673723052, "compression_ratio": 1.6724890829694323, "no_speech_prob": 5.420665729616303e-06}, {"id": 154, "seek": 100124, "start": 1012.76, "end": 1018.6800000000001, "text": " in the class yet probably not but it basically is a task of the technical labeling is trying to", "tokens": [294, 264, 1508, 1939, 1391, 406, 457, 309, 1936, 307, 257, 5633, 295, 264, 6191, 40244, 307, 1382, 281], "temperature": 0.0, "avg_logprob": -0.3135116673723052, "compression_ratio": 1.6724890829694323, "no_speech_prob": 5.420665729616303e-06}, {"id": 155, "seek": 100124, "start": 1018.6800000000001, "end": 1024.84, "text": " taking one sentence and trying to identify the roles for different verbs at least for verbs in", "tokens": [1940, 472, 8174, 293, 1382, 281, 5876, 264, 9604, 337, 819, 30051, 412, 1935, 337, 30051, 294], "temperature": 0.0, "avg_logprob": -0.3135116673723052, "compression_ratio": 1.6724890829694323, "no_speech_prob": 5.420665729616303e-06}, {"id": 156, "seek": 102484, "start": 1024.84, "end": 1031.8, "text": " this case in one sentence so basically trying to give one sentence or give one verb finish trying", "tokens": [341, 1389, 294, 472, 8174, 370, 1936, 1382, 281, 976, 472, 8174, 420, 976, 472, 9595, 2413, 1382], "temperature": 0.0, "avg_logprob": -0.21941149512002633, "compression_ratio": 1.8502415458937198, "no_speech_prob": 2.2458443709183484e-05}, {"id": 157, "seek": 102484, "start": 1031.8, "end": 1039.32, "text": " to figure out like who did what to whom and then and well so by trying to so to go you try to", "tokens": [281, 2573, 484, 411, 567, 630, 437, 281, 7101, 293, 550, 293, 731, 370, 538, 1382, 281, 370, 281, 352, 291, 853, 281], "temperature": 0.0, "avg_logprob": -0.21941149512002633, "compression_ratio": 1.8502415458937198, "no_speech_prob": 2.2458443709183484e-05}, {"id": 158, "seek": 102484, "start": 1039.32, "end": 1046.36, "text": " figure out all this like roles with respect to the words so one way to solve this problem is by", "tokens": [2573, 484, 439, 341, 411, 9604, 365, 3104, 281, 264, 2283, 370, 472, 636, 281, 5039, 341, 1154, 307, 538], "temperature": 0.0, "avg_logprob": -0.21941149512002633, "compression_ratio": 1.8502415458937198, "no_speech_prob": 2.2458443709183484e-05}, {"id": 159, "seek": 102484, "start": 1046.36, "end": 1052.76, "text": " also by converting all these different roles into questions such as who finished something what", "tokens": [611, 538, 29942, 439, 613, 819, 9604, 666, 1651, 1270, 382, 567, 4335, 746, 437], "temperature": 0.0, "avg_logprob": -0.21941149512002633, "compression_ratio": 1.8502415458937198, "no_speech_prob": 2.2458443709183484e-05}, {"id": 160, "seek": 105276, "start": 1052.76, "end": 1058.2, "text": " is someone finished and what is someone finished something else so by converting all these kind of", "tokens": [307, 1580, 4335, 293, 437, 307, 1580, 4335, 746, 1646, 370, 538, 29942, 439, 613, 733, 295], "temperature": 0.0, "avg_logprob": -0.27180789268180117, "compression_ratio": 1.7674418604651163, "no_speech_prob": 1.9212879124097526e-05}, {"id": 161, "seek": 105276, "start": 1058.2, "end": 1065.72, "text": " like cement relations we can also just apply the reading comprehension problem and give you a", "tokens": [411, 19729, 2299, 321, 393, 611, 445, 3079, 264, 3760, 44991, 1154, 293, 976, 291, 257], "temperature": 0.0, "avg_logprob": -0.27180789268180117, "compression_ratio": 1.7674418604651163, "no_speech_prob": 1.9212879124097526e-05}, {"id": 162, "seek": 105276, "start": 1065.72, "end": 1070.92, "text": " correct answer so this is actually a very interesting perspective that reading comprehension", "tokens": [3006, 1867, 370, 341, 307, 767, 257, 588, 1880, 4585, 300, 3760, 44991], "temperature": 0.0, "avg_logprob": -0.27180789268180117, "compression_ratio": 1.7674418604651163, "no_speech_prob": 1.9212879124097526e-05}, {"id": 163, "seek": 105276, "start": 1070.92, "end": 1079.08, "text": " can be actually very universally useful to many other questions so next I'm going to introduce", "tokens": [393, 312, 767, 588, 43995, 4420, 281, 867, 661, 1651, 370, 958, 286, 478, 516, 281, 5366], "temperature": 0.0, "avg_logprob": -0.27180789268180117, "compression_ratio": 1.7674418604651163, "no_speech_prob": 1.9212879124097526e-05}, {"id": 164, "seek": 107908, "start": 1079.08, "end": 1084.4399999999998, "text": " this like a Stanford question three data set calls God so if you're going to do the before the final", "tokens": [341, 411, 257, 20374, 1168, 1045, 1412, 992, 5498, 1265, 370, 498, 291, 434, 516, 281, 360, 264, 949, 264, 2572], "temperature": 0.0, "avg_logprob": -0.3040503557990579, "compression_ratio": 1.7330316742081449, "no_speech_prob": 4.130922752665356e-05}, {"id": 165, "seek": 107908, "start": 1084.4399999999998, "end": 1090.4399999999998, "text": " project you will need to use this data set so Stanford question three data set is actually a", "tokens": [1716, 291, 486, 643, 281, 764, 341, 1412, 992, 370, 20374, 1168, 1045, 1412, 992, 307, 767, 257], "temperature": 0.0, "avg_logprob": -0.3040503557990579, "compression_ratio": 1.7330316742081449, "no_speech_prob": 4.130922752665356e-05}, {"id": 166, "seek": 107908, "start": 1090.4399999999998, "end": 1097.8, "text": " super advised reading comprehension data set so which consists of 100 K annotated passage and", "tokens": [1687, 26269, 3760, 44991, 1412, 992, 370, 597, 14689, 295, 2319, 591, 25339, 770, 11497, 293], "temperature": 0.0, "avg_logprob": -0.3040503557990579, "compression_ratio": 1.7330316742081449, "no_speech_prob": 4.130922752665356e-05}, {"id": 167, "seek": 107908, "start": 1097.8, "end": 1106.9199999999998, "text": " answer question answer triples so here is one example from this data set and I just want to say", "tokens": [1867, 1168, 1867, 1376, 2622, 370, 510, 307, 472, 1365, 490, 341, 1412, 992, 293, 286, 445, 528, 281, 584], "temperature": 0.0, "avg_logprob": -0.3040503557990579, "compression_ratio": 1.7330316742081449, "no_speech_prob": 4.130922752665356e-05}, {"id": 168, "seek": 110692, "start": 1106.92, "end": 1113.0800000000002, "text": " that also what important thing to have in mind is that so this data set has consists of 100", "tokens": [300, 611, 437, 1021, 551, 281, 362, 294, 1575, 307, 300, 370, 341, 1412, 992, 575, 14689, 295, 2319], "temperature": 0.0, "avg_logprob": -0.2911503620636769, "compression_ratio": 1.660633484162896, "no_speech_prob": 9.816129022510722e-06}, {"id": 169, "seek": 110692, "start": 1113.0800000000002, "end": 1119.0, "text": " K annotated examples and this kind of large scale supervised data set is also very key", "tokens": [591, 25339, 770, 5110, 293, 341, 733, 295, 2416, 4373, 46533, 1412, 992, 307, 611, 588, 2141], "temperature": 0.0, "avg_logprob": -0.2911503620636769, "compression_ratio": 1.660633484162896, "no_speech_prob": 9.816129022510722e-06}, {"id": 170, "seek": 110692, "start": 1119.0, "end": 1123.88, "text": " in grade and for the training the effective neural models for reading comprehension so after", "tokens": [294, 7204, 293, 337, 264, 3097, 264, 4942, 18161, 5245, 337, 3760, 44991, 370, 934], "temperature": 0.0, "avg_logprob": -0.2911503620636769, "compression_ratio": 1.660633484162896, "no_speech_prob": 9.816129022510722e-06}, {"id": 171, "seek": 110692, "start": 1123.88, "end": 1129.96, "text": " this God data set many other like later data set having also collected basically runs this size", "tokens": [341, 1265, 1412, 992, 867, 661, 411, 1780, 1412, 992, 1419, 611, 11087, 1936, 6676, 341, 2744], "temperature": 0.0, "avg_logprob": -0.2911503620636769, "compression_ratio": 1.660633484162896, "no_speech_prob": 9.816129022510722e-06}, {"id": 172, "seek": 112996, "start": 1129.96, "end": 1137.0, "text": " or around like 100 K so 100 K is actually very important to trend these neural models so for this", "tokens": [420, 926, 411, 2319, 591, 370, 2319, 591, 307, 767, 588, 1021, 281, 6028, 613, 18161, 5245, 370, 337, 341], "temperature": 0.0, "avg_logprob": -0.3429348792558835, "compression_ratio": 1.7227272727272727, "no_speech_prob": 4.156434897595318e-06}, {"id": 173, "seek": 112996, "start": 1137.0, "end": 1144.2, "text": " data set so the question the passages is like a single passage a single paragraph selected from", "tokens": [1412, 992, 370, 264, 1168, 264, 31589, 307, 411, 257, 2167, 11497, 257, 2167, 18865, 8209, 490], "temperature": 0.0, "avg_logprob": -0.3429348792558835, "compression_ratio": 1.7227272727272727, "no_speech_prob": 4.156434897595318e-06}, {"id": 174, "seek": 112996, "start": 1144.2, "end": 1149.72, "text": " the English Wikipedia which usually consists of like 100 to 150 words and the questions are", "tokens": [264, 3669, 28999, 597, 2673, 14689, 295, 411, 2319, 281, 8451, 2283, 293, 264, 1651, 366], "temperature": 0.0, "avg_logprob": -0.3429348792558835, "compression_ratio": 1.7227272727272727, "no_speech_prob": 4.156434897595318e-06}, {"id": 175, "seek": 112996, "start": 1149.72, "end": 1155.32, "text": " crowdsourced basically like from the kind of turkey and there is a very important property of", "tokens": [26070, 396, 1232, 1936, 411, 490, 264, 733, 295, 21551, 293, 456, 307, 257, 588, 1021, 4707, 295], "temperature": 0.0, "avg_logprob": -0.3429348792558835, "compression_ratio": 1.7227272727272727, "no_speech_prob": 4.156434897595318e-06}, {"id": 176, "seek": 115532, "start": 1155.32, "end": 1163.3999999999999, "text": " this data set is that each answer is a short segment text or we can spend in the passage so as", "tokens": [341, 1412, 992, 307, 300, 1184, 1867, 307, 257, 2099, 9469, 2487, 420, 321, 393, 3496, 294, 264, 11497, 370, 382], "temperature": 0.0, "avg_logprob": -0.20744805219696788, "compression_ratio": 1.8535353535353536, "no_speech_prob": 5.092441824672278e-06}, {"id": 177, "seek": 115532, "start": 1163.3999999999999, "end": 1169.8799999999999, "text": " you can see from this example so here are three different questions and each of this answer can", "tokens": [291, 393, 536, 490, 341, 1365, 370, 510, 366, 1045, 819, 1651, 293, 1184, 295, 341, 1867, 393], "temperature": 0.0, "avg_logprob": -0.20744805219696788, "compression_ratio": 1.8535353535353536, "no_speech_prob": 5.092441824672278e-06}, {"id": 178, "seek": 115532, "start": 1169.8799999999999, "end": 1174.6, "text": " be actually find also like a short segment text in the passage so this is actually a", "tokens": [312, 767, 915, 611, 411, 257, 2099, 9469, 2487, 294, 264, 11497, 370, 341, 307, 767, 257], "temperature": 0.0, "avg_logprob": -0.20744805219696788, "compression_ratio": 1.8535353535353536, "no_speech_prob": 5.092441824672278e-06}, {"id": 179, "seek": 115532, "start": 1174.6, "end": 1181.08, "text": " pretty interesting property you know it's also important property of this data set but also", "tokens": [1238, 1880, 4707, 291, 458, 309, 311, 611, 1021, 4707, 295, 341, 1412, 992, 457, 611], "temperature": 0.0, "avg_logprob": -0.20744805219696788, "compression_ratio": 1.8535353535353536, "no_speech_prob": 5.092441824672278e-06}, {"id": 180, "seek": 118108, "start": 1181.08, "end": 1188.1999999999998, "text": " just to your coverage that this also limitation because not all the questions can be answered in", "tokens": [445, 281, 428, 9645, 300, 341, 611, 27432, 570, 406, 439, 264, 1651, 393, 312, 10103, 294], "temperature": 0.0, "avg_logprob": -0.2816941134900932, "compression_ratio": 1.8151658767772512, "no_speech_prob": 3.0684266675962135e-05}, {"id": 181, "seek": 118108, "start": 1188.1999999999998, "end": 1194.1999999999998, "text": " this way so only the questions that that you can find answered as a stand in the passage can actually", "tokens": [341, 636, 370, 787, 264, 1651, 300, 300, 291, 393, 915, 10103, 382, 257, 1463, 294, 264, 11497, 393, 767], "temperature": 0.0, "avg_logprob": -0.2816941134900932, "compression_ratio": 1.8151658767772512, "no_speech_prob": 3.0684266675962135e-05}, {"id": 182, "seek": 118108, "start": 1194.1999999999998, "end": 1202.28, "text": " be included in this data set basically but today so this data set yeah I forgot to say so this", "tokens": [312, 5556, 294, 341, 1412, 992, 1936, 457, 965, 370, 341, 1412, 992, 1338, 286, 5298, 281, 584, 370, 341], "temperature": 0.0, "avg_logprob": -0.2816941134900932, "compression_ratio": 1.8151658767772512, "no_speech_prob": 3.0684266675962135e-05}, {"id": 183, "seek": 118108, "start": 1202.28, "end": 1208.76, "text": " data set was collected in 2016 by several researchers at Stanford so it's called Stanford", "tokens": [1412, 992, 390, 11087, 294, 6549, 538, 2940, 10309, 412, 20374, 370, 309, 311, 1219, 20374], "temperature": 0.0, "avg_logprob": -0.2816941134900932, "compression_ratio": 1.8151658767772512, "no_speech_prob": 3.0684266675962135e-05}, {"id": 184, "seek": 120876, "start": 1208.76, "end": 1214.92, "text": " Question 3 data set and today like after four or five years now so Scott still remains the most", "tokens": [14464, 805, 1412, 992, 293, 965, 411, 934, 1451, 420, 1732, 924, 586, 370, 6659, 920, 7023, 264, 881], "temperature": 0.0, "avg_logprob": -0.3086832022365135, "compression_ratio": 1.6682027649769586, "no_speech_prob": 2.6239242288284004e-05}, {"id": 185, "seek": 120876, "start": 1214.92, "end": 1219.96, "text": " popular reading comprehension data set so he's actually he's a very clean on the high quality data", "tokens": [3743, 3760, 44991, 1412, 992, 370, 415, 311, 767, 415, 311, 257, 588, 2541, 322, 264, 1090, 3125, 1412], "temperature": 0.0, "avg_logprob": -0.3086832022365135, "compression_ratio": 1.6682027649769586, "no_speech_prob": 2.6239242288284004e-05}, {"id": 186, "seek": 120876, "start": 1219.96, "end": 1225.72, "text": " set but he's also not as very difficult data set so today basically the score data set has", "tokens": [992, 457, 415, 311, 611, 406, 382, 588, 2252, 1412, 992, 370, 965, 1936, 264, 6175, 1412, 992, 575], "temperature": 0.0, "avg_logprob": -0.3086832022365135, "compression_ratio": 1.6682027649769586, "no_speech_prob": 2.6239242288284004e-05}, {"id": 187, "seek": 120876, "start": 1225.72, "end": 1230.68, "text": " been almost sold and what safe are already since estimated human performance", "tokens": [668, 1920, 3718, 293, 437, 3273, 366, 1217, 1670, 14109, 1952, 3389], "temperature": 0.0, "avg_logprob": -0.3086832022365135, "compression_ratio": 1.6682027649769586, "no_speech_prob": 2.6239242288284004e-05}, {"id": 188, "seek": 123068, "start": 1230.68, "end": 1241.0800000000002, "text": " and also until quickly mentioned the evaluation for this Stanford question data set so there are", "tokens": [293, 611, 1826, 2661, 2835, 264, 13344, 337, 341, 20374, 1168, 1412, 992, 370, 456, 366], "temperature": 0.0, "avg_logprob": -0.35134685039520264, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.0948587259917986e-05}, {"id": 189, "seek": 123068, "start": 1241.0800000000002, "end": 1246.44, "text": " basically two evaluation metrics to evaluate how well a system can do on this data set the two", "tokens": [1936, 732, 13344, 16367, 281, 13059, 577, 731, 257, 1185, 393, 360, 322, 341, 1412, 992, 264, 732], "temperature": 0.0, "avg_logprob": -0.35134685039520264, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.0948587259917986e-05}, {"id": 190, "seek": 123068, "start": 1246.44, "end": 1252.2, "text": " metrics are like the exact match and the affine score so when you find match is basically just a binary", "tokens": [16367, 366, 411, 264, 1900, 2995, 293, 264, 2096, 533, 6175, 370, 562, 291, 915, 2995, 307, 1936, 445, 257, 17434], "temperature": 0.0, "avg_logprob": -0.35134685039520264, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.0948587259917986e-05}, {"id": 191, "seek": 123068, "start": 1252.2, "end": 1257.24, "text": " indicator zero one based measures whether the answer can actually be exactly matched to the", "tokens": [16961, 4018, 472, 2361, 8000, 1968, 264, 1867, 393, 767, 312, 2293, 21447, 281, 264], "temperature": 0.0, "avg_logprob": -0.35134685039520264, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.0948587259917986e-05}, {"id": 192, "seek": 125724, "start": 1257.24, "end": 1263.72, "text": " gold answer and the affine score basically measures kind of some partial credit and not to do the", "tokens": [3821, 1867, 293, 264, 2096, 533, 6175, 1936, 8000, 733, 295, 512, 14641, 5397, 293, 406, 281, 360, 264], "temperature": 0.0, "avg_logprob": -0.21186590686286846, "compression_ratio": 1.9328063241106719, "no_speech_prob": 6.436188868974568e-06}, {"id": 193, "seek": 125724, "start": 1263.72, "end": 1268.92, "text": " evaluation so basically for the development and testing set there will be like three gold answers", "tokens": [13344, 370, 1936, 337, 264, 3250, 293, 4997, 992, 456, 486, 312, 411, 1045, 3821, 6338], "temperature": 0.0, "avg_logprob": -0.21186590686286846, "compression_ratio": 1.9328063241106719, "no_speech_prob": 6.436188868974568e-06}, {"id": 194, "seek": 125724, "start": 1268.92, "end": 1274.68, "text": " collected because for some questions there might be not just one one unique answer so they're", "tokens": [11087, 570, 337, 512, 1651, 456, 1062, 312, 406, 445, 472, 472, 3845, 1867, 370, 436, 434], "temperature": 0.0, "avg_logprob": -0.21186590686286846, "compression_ratio": 1.9328063241106719, "no_speech_prob": 6.436188868974568e-06}, {"id": 195, "seek": 125724, "start": 1274.68, "end": 1280.28, "text": " quite multiple possible answers and the evaluation makes it basically takes a pretty good answer", "tokens": [1596, 3866, 1944, 6338, 293, 264, 13344, 1669, 309, 1936, 2516, 257, 1238, 665, 1867], "temperature": 0.0, "avg_logprob": -0.21186590686286846, "compression_ratio": 1.9328063241106719, "no_speech_prob": 6.436188868974568e-06}, {"id": 196, "seek": 125724, "start": 1280.28, "end": 1286.84, "text": " and compares or compares the predicted answer to each gold answer with some kind of like some articles", "tokens": [293, 38334, 420, 38334, 264, 19147, 1867, 281, 1184, 3821, 1867, 365, 512, 733, 295, 411, 512, 11290], "temperature": 0.0, "avg_logprob": -0.21186590686286846, "compression_ratio": 1.9328063241106719, "no_speech_prob": 6.436188868974568e-06}, {"id": 197, "seek": 128684, "start": 1286.84, "end": 1292.76, "text": " and also the computations excluded and the picture you can compute a exact match score and also", "tokens": [293, 611, 264, 2807, 763, 29486, 293, 264, 3036, 291, 393, 14722, 257, 1900, 2995, 6175, 293, 611], "temperature": 0.0, "avg_logprob": -0.29940371793859144, "compression_ratio": 1.9396984924623115, "no_speech_prob": 5.680077720171539e-06}, {"id": 198, "seek": 128684, "start": 1292.76, "end": 1300.04, "text": " a score by comparing the predicted answer to the gold answer and finally you take the match scores", "tokens": [257, 6175, 538, 15763, 264, 19147, 1867, 281, 264, 3821, 1867, 293, 2721, 291, 747, 264, 2995, 13444], "temperature": 0.0, "avg_logprob": -0.29940371793859144, "compression_ratio": 1.9396984924623115, "no_speech_prob": 5.680077720171539e-06}, {"id": 199, "seek": 128684, "start": 1300.04, "end": 1305.3999999999999, "text": " and because there are many different examples in the demo test set and finally we just take the", "tokens": [293, 570, 456, 366, 867, 819, 5110, 294, 264, 10723, 1500, 992, 293, 2721, 321, 445, 747, 264], "temperature": 0.0, "avg_logprob": -0.29940371793859144, "compression_ratio": 1.9396984924623115, "no_speech_prob": 5.680077720171539e-06}, {"id": 200, "seek": 128684, "start": 1305.3999999999999, "end": 1311.3999999999999, "text": " average of all the examples for the post-example match and the reference score so by using this", "tokens": [4274, 295, 439, 264, 5110, 337, 264, 2183, 12, 3121, 335, 781, 2995, 293, 264, 6408, 6175, 370, 538, 1228, 341], "temperature": 0.0, "avg_logprob": -0.29940371793859144, "compression_ratio": 1.9396984924623115, "no_speech_prob": 5.680077720171539e-06}, {"id": 201, "seek": 131140, "start": 1311.4, "end": 1318.1200000000001, "text": " evaluation metric so estimating the human performance is by the researchers as a time", "tokens": [13344, 20678, 370, 8017, 990, 264, 1952, 3389, 307, 538, 264, 10309, 382, 257, 565], "temperature": 0.0, "avg_logprob": -0.2177212987627302, "compression_ratio": 1.627906976744186, "no_speech_prob": 9.219604180543683e-06}, {"id": 202, "seek": 131140, "start": 1318.92, "end": 1325.88, "text": " estimated by the researchers as a time is the exact match score is 82.3% and the affine score is", "tokens": [14109, 538, 264, 10309, 382, 257, 565, 307, 264, 1900, 2995, 6175, 307, 29097, 13, 18, 4, 293, 264, 2096, 533, 6175, 307], "temperature": 0.0, "avg_logprob": -0.2177212987627302, "compression_ratio": 1.627906976744186, "no_speech_prob": 9.219604180543683e-06}, {"id": 203, "seek": 131140, "start": 1325.88, "end": 1336.68, "text": " 91.2 so here's just a quick example so here's a question what do tests are doing in December 1878", "tokens": [31064, 13, 17, 370, 510, 311, 445, 257, 1702, 1365, 370, 510, 311, 257, 1168, 437, 360, 6921, 366, 884, 294, 7687, 2443, 30693], "temperature": 0.0, "avg_logprob": -0.2177212987627302, "compression_ratio": 1.627906976744186, "no_speech_prob": 9.219604180543683e-06}, {"id": 204, "seek": 133668, "start": 1336.68, "end": 1342.68, "text": " and the Diatry possible answers so you can see that the first two answers are the same left", "tokens": [293, 264, 413, 7676, 627, 1944, 6338, 370, 291, 393, 536, 300, 264, 700, 732, 6338, 366, 264, 912, 1411], "temperature": 0.0, "avg_logprob": -0.3947271711370918, "compression_ratio": 2.0, "no_speech_prob": 1.4496576113742776e-05}, {"id": 205, "seek": 133668, "start": 1342.68, "end": 1352.68, "text": " grass and the third answer is left grass and as a serve as is a title here or relations with his family", "tokens": [8054, 293, 264, 2636, 1867, 307, 1411, 8054, 293, 382, 257, 4596, 382, 307, 257, 4876, 510, 420, 2299, 365, 702, 1605], "temperature": 0.0, "avg_logprob": -0.3947271711370918, "compression_ratio": 2.0, "no_speech_prob": 1.4496576113742776e-05}, {"id": 206, "seek": 133668, "start": 1353.0, "end": 1357.96, "text": " and then you feel that if you find a prediction is a span which is left grass and serve", "tokens": [293, 550, 291, 841, 300, 498, 291, 915, 257, 17630, 307, 257, 16174, 597, 307, 1411, 8054, 293, 4596], "temperature": 0.0, "avg_logprob": -0.3947271711370918, "compression_ratio": 2.0, "no_speech_prob": 1.4496576113742776e-05}, {"id": 207, "seek": 133668, "start": 1358.68, "end": 1364.1200000000001, "text": " so you can see that the exact there is an exact match score between the predicted answer and", "tokens": [370, 291, 393, 536, 300, 264, 1900, 456, 307, 364, 1900, 2995, 6175, 1296, 264, 19147, 1867, 293], "temperature": 0.0, "avg_logprob": -0.3947271711370918, "compression_ratio": 2.0, "no_speech_prob": 1.4496576113742776e-05}, {"id": 208, "seek": 136412, "start": 1364.12, "end": 1370.1999999999998, "text": " any of the gold answer so the exact match will be zero and the affine score will be taking the max", "tokens": [604, 295, 264, 3821, 1867, 370, 264, 1900, 2995, 486, 312, 4018, 293, 264, 2096, 533, 6175, 486, 312, 1940, 264, 11469], "temperature": 0.0, "avg_logprob": -0.21042780929736876, "compression_ratio": 1.803030303030303, "no_speech_prob": 1.384315146424342e-05}, {"id": 209, "seek": 136412, "start": 1370.1999999999998, "end": 1375.8799999999999, "text": " I'm not going to talk about how this is computed so I suggest you check out the original paper", "tokens": [286, 478, 406, 516, 281, 751, 466, 577, 341, 307, 40610, 370, 286, 3402, 291, 1520, 484, 264, 3380, 3035], "temperature": 0.0, "avg_logprob": -0.21042780929736876, "compression_ratio": 1.803030303030303, "no_speech_prob": 1.384315146424342e-05}, {"id": 210, "seek": 136412, "start": 1375.8799999999999, "end": 1382.76, "text": " so by computing this scores and taking the max and the final is the affine score will be 0.67", "tokens": [370, 538, 15866, 341, 13444, 293, 1940, 264, 11469, 293, 264, 2572, 307, 264, 2096, 533, 6175, 486, 312, 1958, 13, 22452], "temperature": 0.0, "avg_logprob": -0.21042780929736876, "compression_ratio": 1.803030303030303, "no_speech_prob": 1.384315146424342e-05}, {"id": 211, "seek": 136412, "start": 1382.76, "end": 1387.0, "text": " which is the affine score for this predicted answer on this data set.", "tokens": [597, 307, 264, 2096, 533, 6175, 337, 341, 19147, 1867, 322, 341, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.21042780929736876, "compression_ratio": 1.803030303030303, "no_speech_prob": 1.384315146424342e-05}, {"id": 212, "seek": 138700, "start": 1387.0, "end": 1395.32, "text": " So Danchi one question you might answer is so if you can do other tasks like named entity", "tokens": [407, 3394, 8036, 472, 1168, 291, 1062, 1867, 307, 370, 498, 291, 393, 360, 661, 9608, 411, 4926, 13977], "temperature": 0.0, "avg_logprob": -0.225799318343874, "compression_ratio": 1.5642458100558658, "no_speech_prob": 0.00010513226152397692}, {"id": 213, "seek": 138700, "start": 1395.32, "end": 1402.2, "text": " recognition or relation extraction by sticking something on top of bird as and fine tuning", "tokens": [11150, 420, 9721, 30197, 538, 13465, 746, 322, 1192, 295, 5255, 382, 293, 2489, 15164], "temperature": 0.0, "avg_logprob": -0.225799318343874, "compression_ratio": 1.5642458100558658, "no_speech_prob": 0.00010513226152397692}, {"id": 214, "seek": 138700, "start": 1402.2, "end": 1408.28, "text": " forward or do it as question answering there's one or the other method work better and by how much.", "tokens": [2128, 420, 360, 309, 382, 1168, 13430, 456, 311, 472, 420, 264, 661, 3170, 589, 1101, 293, 538, 577, 709, 13], "temperature": 0.0, "avg_logprob": -0.225799318343874, "compression_ratio": 1.5642458100558658, "no_speech_prob": 0.00010513226152397692}, {"id": 215, "seek": 140828, "start": 1408.28, "end": 1418.12, "text": " That's an interesting question so I haven't really seen the okay so there has been some", "tokens": [663, 311, 364, 1880, 1168, 370, 286, 2378, 380, 534, 1612, 264, 1392, 370, 456, 575, 668, 512], "temperature": 0.0, "avg_logprob": -0.29050525399141536, "compression_ratio": 1.8038277511961722, "no_speech_prob": 6.802500138292089e-05}, {"id": 216, "seek": 140828, "start": 1418.12, "end": 1423.3999999999999, "text": " claims that okay also tasks can be converted into questions and tasks but I'm not sure there", "tokens": [9441, 300, 1392, 611, 9608, 393, 312, 16424, 666, 1651, 293, 9608, 457, 286, 478, 406, 988, 456], "temperature": 0.0, "avg_logprob": -0.29050525399141536, "compression_ratio": 1.8038277511961722, "no_speech_prob": 6.802500138292089e-05}, {"id": 217, "seek": 140828, "start": 1423.3999999999999, "end": 1429.48, "text": " is a really a very fair comparison let's say an entity recognition but by really converting that into", "tokens": [307, 257, 534, 257, 588, 3143, 9660, 718, 311, 584, 364, 13977, 11150, 457, 538, 534, 29942, 300, 666], "temperature": 0.0, "avg_logprob": -0.29050525399141536, "compression_ratio": 1.8038277511961722, "no_speech_prob": 6.802500138292089e-05}, {"id": 218, "seek": 140828, "start": 1429.48, "end": 1434.92, "text": " questions and tasks so I don't have to answer to that so the kind of states are in your system", "tokens": [1651, 293, 9608, 370, 286, 500, 380, 362, 281, 1867, 281, 300, 370, 264, 733, 295, 4368, 366, 294, 428, 1185], "temperature": 0.0, "avg_logprob": -0.29050525399141536, "compression_ratio": 1.8038277511961722, "no_speech_prob": 6.802500138292089e-05}, {"id": 219, "seek": 143492, "start": 1434.92, "end": 1441.8000000000002, "text": " and still trying to just change sequence tagger on top of the bird so yeah I don't really have", "tokens": [293, 920, 1382, 281, 445, 1319, 8310, 6162, 1321, 322, 1192, 295, 264, 5255, 370, 1338, 286, 500, 380, 534, 362], "temperature": 0.0, "avg_logprob": -0.4290420470699187, "compression_ratio": 1.4011627906976745, "no_speech_prob": 2.0451945601962507e-05}, {"id": 220, "seek": 143492, "start": 1441.8000000000002, "end": 1443.0800000000002, "text": " a pre-set answer to that.", "tokens": [257, 659, 12, 3854, 1867, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.4290420470699187, "compression_ratio": 1.4011627906976745, "no_speech_prob": 2.0451945601962507e-05}, {"id": 221, "seek": 143492, "start": 1447.3200000000002, "end": 1448.1200000000001, "text": " Should I continue?", "tokens": [6454, 286, 2354, 30], "temperature": 0.0, "avg_logprob": -0.4290420470699187, "compression_ratio": 1.4011627906976745, "no_speech_prob": 2.0451945601962507e-05}, {"id": 222, "seek": 143492, "start": 1450.2, "end": 1459.88, "text": " Okay so next I'm going to talk about how to build newer models for really comprehension in particular", "tokens": [1033, 370, 958, 286, 478, 516, 281, 751, 466, 577, 281, 1322, 17628, 5245, 337, 534, 44991, 294, 1729], "temperature": 0.0, "avg_logprob": -0.4290420470699187, "compression_ratio": 1.4011627906976745, "no_speech_prob": 2.0451945601962507e-05}, {"id": 223, "seek": 145988, "start": 1459.88, "end": 1464.7600000000002, "text": " how we can build a model to solve this Stanford question answering data sets for data sets.", "tokens": [577, 321, 393, 1322, 257, 2316, 281, 5039, 341, 20374, 1168, 13430, 1412, 6352, 337, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.27102349728954084, "compression_ratio": 1.8937007874015748, "no_speech_prob": 1.3843357919540722e-05}, {"id": 224, "seek": 145988, "start": 1464.7600000000002, "end": 1469.64, "text": " Also I want to just quickly mention that because there are many different papers it actually uses", "tokens": [2743, 286, 528, 281, 445, 2661, 2152, 300, 570, 456, 366, 867, 819, 10577, 309, 767, 4960], "temperature": 0.0, "avg_logprob": -0.27102349728954084, "compression_ratio": 1.8937007874015748, "no_speech_prob": 1.3843357919540722e-05}, {"id": 225, "seek": 145988, "start": 1469.64, "end": 1476.2800000000002, "text": " like different notions to refer to the sensing so I'm starting from so I'm going to use the passage", "tokens": [411, 819, 35799, 281, 2864, 281, 264, 30654, 370, 286, 478, 2891, 490, 370, 286, 478, 516, 281, 764, 264, 11497], "temperature": 0.0, "avg_logprob": -0.27102349728954084, "compression_ratio": 1.8937007874015748, "no_speech_prob": 1.3843357919540722e-05}, {"id": 226, "seek": 145988, "start": 1476.2800000000002, "end": 1481.88, "text": " paragraph and context and also question query basically interchangeably so they are basically", "tokens": [18865, 293, 4319, 293, 611, 1168, 14581, 1936, 30358, 1188, 370, 436, 366, 1936], "temperature": 0.0, "avg_logprob": -0.27102349728954084, "compression_ratio": 1.8937007874015748, "no_speech_prob": 1.3843357919540722e-05}, {"id": 227, "seek": 145988, "start": 1481.88, "end": 1486.92, "text": " referred to sensing because different papers use also different notions so I just want to quickly", "tokens": [10839, 281, 30654, 570, 819, 10577, 764, 611, 819, 35799, 370, 286, 445, 528, 281, 2661], "temperature": 0.0, "avg_logprob": -0.27102349728954084, "compression_ratio": 1.8937007874015748, "no_speech_prob": 1.3843357919540722e-05}, {"id": 228, "seek": 148692, "start": 1486.92, "end": 1493.88, "text": " mention that okay so can we build a model to solve this problem so let's first form this problem", "tokens": [2152, 300, 1392, 370, 393, 321, 1322, 257, 2316, 281, 5039, 341, 1154, 370, 718, 311, 700, 1254, 341, 1154], "temperature": 0.0, "avg_logprob": -0.33109857702767975, "compression_ratio": 1.813397129186603, "no_speech_prob": 1.4965306945668999e-05}, {"id": 229, "seek": 148692, "start": 1494.44, "end": 1500.8400000000001, "text": " so the input of this problem is let's take let's take our context or paragraph so see which", "tokens": [370, 264, 4846, 295, 341, 1154, 307, 718, 311, 747, 718, 311, 747, 527, 4319, 420, 18865, 370, 536, 597], "temperature": 0.0, "avg_logprob": -0.33109857702767975, "compression_ratio": 1.813397129186603, "no_speech_prob": 1.4965306945668999e-05}, {"id": 230, "seek": 148692, "start": 1500.8400000000001, "end": 1507.88, "text": " consists of the intel can see one to see and and also we take our question q and look the question", "tokens": [14689, 295, 264, 24777, 393, 536, 472, 281, 536, 293, 293, 611, 321, 747, 527, 1168, 9505, 293, 574, 264, 1168], "temperature": 0.0, "avg_logprob": -0.33109857702767975, "compression_ratio": 1.813397129186603, "no_speech_prob": 1.4965306945668999e-05}, {"id": 231, "seek": 148692, "start": 1507.88, "end": 1514.92, "text": " consists of m tokens q1 to qm so n could be something like around 100 to on between 100 and", "tokens": [14689, 295, 275, 22667, 9505, 16, 281, 9505, 76, 370, 297, 727, 312, 746, 411, 926, 2319, 281, 322, 1296, 2319, 293], "temperature": 0.0, "avg_logprob": -0.33109857702767975, "compression_ratio": 1.813397129186603, "no_speech_prob": 1.4965306945668999e-05}, {"id": 232, "seek": 151492, "start": 1514.92, "end": 1522.44, "text": " 200 for Scott and m would be much shorter be something like 10 or 15 and because the answer has", "tokens": [2331, 337, 6659, 293, 275, 576, 312, 709, 11639, 312, 746, 411, 1266, 420, 2119, 293, 570, 264, 1867, 575], "temperature": 0.0, "avg_logprob": -0.29746779989688954, "compression_ratio": 1.7991071428571428, "no_speech_prob": 4.4222542783245444e-06}, {"id": 233, "seek": 151492, "start": 1522.44, "end": 1527.88, "text": " these constraints as the answer must be a second text in the passage so the output can be just", "tokens": [613, 18491, 382, 264, 1867, 1633, 312, 257, 1150, 2487, 294, 264, 11497, 370, 264, 5598, 393, 312, 445], "temperature": 0.0, "avg_logprob": -0.29746779989688954, "compression_ratio": 1.7991071428571428, "no_speech_prob": 4.4222542783245444e-06}, {"id": 234, "seek": 151492, "start": 1528.68, "end": 1534.44, "text": " reading this way so we are going to predict a start and so start an end will be", "tokens": [3760, 341, 636, 370, 321, 366, 516, 281, 6069, 257, 722, 293, 370, 722, 364, 917, 486, 312], "temperature": 0.0, "avg_logprob": -0.29746779989688954, "compression_ratio": 1.7991071428571428, "no_speech_prob": 4.4222542783245444e-06}, {"id": 235, "seek": 151492, "start": 1534.44, "end": 1540.1200000000001, "text": " wrench be basically in the wrench between the one and so it is basically just two check points", "tokens": [25406, 312, 1936, 294, 264, 25406, 1296, 264, 472, 293, 370, 309, 307, 1936, 445, 732, 1520, 2793], "temperature": 0.0, "avg_logprob": -0.29746779989688954, "compression_ratio": 1.7991071428571428, "no_speech_prob": 4.4222542783245444e-06}, {"id": 236, "seek": 154012, "start": 1540.12, "end": 1551.1599999999999, "text": " oh sorry two end points of the answer and then so Scott has been collected by the late 2016", "tokens": [1954, 2597, 732, 917, 2793, 295, 264, 1867, 293, 550, 370, 6659, 575, 668, 11087, 538, 264, 3469, 6549], "temperature": 0.0, "avg_logprob": -0.3403278589248657, "compression_ratio": 1.5738636363636365, "no_speech_prob": 2.4283293896587566e-05}, {"id": 237, "seek": 154012, "start": 1551.1599999999999, "end": 1559.32, "text": " so after 2016 they are having like visit two families of the models newer models to solve", "tokens": [370, 934, 6549, 436, 366, 1419, 411, 3441, 732, 4466, 295, 264, 5245, 17628, 5245, 281, 5039], "temperature": 0.0, "avg_logprob": -0.3403278589248657, "compression_ratio": 1.5738636363636365, "no_speech_prob": 2.4283293896587566e-05}, {"id": 238, "seek": 154012, "start": 1559.32, "end": 1565.9599999999998, "text": " to solve in this like STEM score data set so the first family basically like there are a lot of", "tokens": [281, 5039, 294, 341, 411, 25043, 6175, 1412, 992, 370, 264, 700, 1605, 1936, 411, 456, 366, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.3403278589248657, "compression_ratio": 1.5738636363636365, "no_speech_prob": 2.4283293896587566e-05}, {"id": 239, "seek": 156596, "start": 1565.96, "end": 1572.44, "text": " models that count out during the lecture between 2016 and 2018 so this was family models because", "tokens": [5245, 300, 1207, 484, 1830, 264, 7991, 1296, 6549, 293, 6096, 370, 341, 390, 1605, 5245, 570], "temperature": 0.0, "avg_logprob": -0.37719297409057617, "compression_ratio": 1.6877828054298643, "no_speech_prob": 1.951172816916369e-05}, {"id": 240, "seek": 156596, "start": 1572.44, "end": 1578.2, "text": " they are always team based models result tension so these are like just like a list of the", "tokens": [436, 366, 1009, 1469, 2361, 5245, 1874, 8980, 370, 613, 366, 411, 445, 411, 257, 1329, 295, 264], "temperature": 0.0, "avg_logprob": -0.37719297409057617, "compression_ratio": 1.6877828054298643, "no_speech_prob": 1.951172816916369e-05}, {"id": 241, "seek": 156596, "start": 1578.2, "end": 1583.56, "text": " representing models that come out during the period and including some works that I did when", "tokens": [13460, 5245, 300, 808, 484, 1830, 264, 2896, 293, 3009, 512, 1985, 300, 286, 630, 562], "temperature": 0.0, "avg_logprob": -0.37719297409057617, "compression_ratio": 1.6877828054298643, "no_speech_prob": 1.951172816916369e-05}, {"id": 242, "seek": 156596, "start": 1583.56, "end": 1590.68, "text": " I was a PhD student at Stanford and also second the second class models I put here is really", "tokens": [286, 390, 257, 14476, 3107, 412, 20374, 293, 611, 1150, 264, 1150, 1508, 5245, 286, 829, 510, 307, 534], "temperature": 0.0, "avg_logprob": -0.37719297409057617, "compression_ratio": 1.6877828054298643, "no_speech_prob": 1.951172816916369e-05}, {"id": 243, "seek": 159068, "start": 1590.68, "end": 1597.4, "text": " there that divided here before the birth and after birth so after birth 10 miles so all of the", "tokens": [456, 300, 6666, 510, 949, 264, 3965, 293, 934, 3965, 370, 934, 3965, 1266, 6193, 370, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.4449833329901638, "compression_ratio": 1.8682926829268294, "no_speech_prob": 1.1655622074613348e-05}, {"id": 244, "seek": 159068, "start": 1597.4, "end": 1602.28, "text": " system reading comprehension models were built on like how to find two the first models not just", "tokens": [1185, 3760, 44991, 5245, 645, 3094, 322, 411, 577, 281, 915, 732, 264, 700, 5245, 406, 445], "temperature": 0.0, "avg_logprob": -0.4449833329901638, "compression_ratio": 1.8682926829268294, "no_speech_prob": 1.1655622074613348e-05}, {"id": 245, "seek": 159068, "start": 1602.28, "end": 1607.0800000000002, "text": " bird models are put a bird like models so pretty and long with models and for this kind of reading", "tokens": [5255, 5245, 366, 829, 257, 5255, 411, 5245, 370, 1238, 293, 938, 365, 5245, 293, 337, 341, 733, 295, 3760], "temperature": 0.0, "avg_logprob": -0.4449833329901638, "compression_ratio": 1.8682926829268294, "no_speech_prob": 1.1655622074613348e-05}, {"id": 246, "seek": 159068, "start": 1607.0800000000002, "end": 1615.96, "text": " comprehension problems so here I like to the some you know to the illustrations of these two", "tokens": [44991, 2740, 370, 510, 286, 411, 281, 264, 512, 291, 458, 281, 264, 34540, 295, 613, 732], "temperature": 0.0, "avg_logprob": -0.4449833329901638, "compression_ratio": 1.8682926829268294, "no_speech_prob": 1.1655622074613348e-05}, {"id": 247, "seek": 161596, "start": 1615.96, "end": 1622.28, "text": " families of the models so on the left is like I always team based models result tension on the", "tokens": [4466, 295, 264, 5245, 370, 322, 264, 1411, 307, 411, 286, 1009, 1469, 2361, 5245, 1874, 8980, 322, 264], "temperature": 0.0, "avg_logprob": -0.2768918143378364, "compression_ratio": 1.8454106280193237, "no_speech_prob": 1.5199315384961665e-05}, {"id": 248, "seek": 161596, "start": 1622.28, "end": 1629.32, "text": " right is on the version model and then we need to find two this model for the passion for the", "tokens": [558, 307, 322, 264, 3037, 2316, 293, 550, 321, 643, 281, 915, 732, 341, 2316, 337, 264, 5418, 337, 264], "temperature": 0.0, "avg_logprob": -0.2768918143378364, "compression_ratio": 1.8454106280193237, "no_speech_prob": 1.5199315384961665e-05}, {"id": 249, "seek": 161596, "start": 1629.32, "end": 1636.44, "text": " reading comprehension task so I know that so my plan today is first I talk to talk about this", "tokens": [3760, 44991, 5633, 370, 286, 458, 300, 370, 452, 1393, 965, 307, 700, 286, 751, 281, 751, 466, 341], "temperature": 0.0, "avg_logprob": -0.2768918143378364, "compression_ratio": 1.8454106280193237, "no_speech_prob": 1.5199315384961665e-05}, {"id": 250, "seek": 161596, "start": 1636.44, "end": 1641.8, "text": " I always team based models so I'm going to spend a little bit more time on this part because I know", "tokens": [286, 1009, 1469, 2361, 5245, 370, 286, 478, 516, 281, 3496, 257, 707, 857, 544, 565, 322, 341, 644, 570, 286, 458], "temperature": 0.0, "avg_logprob": -0.2768918143378364, "compression_ratio": 1.8454106280193237, "no_speech_prob": 1.5199315384961665e-05}, {"id": 251, "seek": 164180, "start": 1641.8, "end": 1647.3999999999999, "text": " that for the default final project you need to increment this model from the scratch so I'm", "tokens": [300, 337, 264, 7576, 2572, 1716, 291, 643, 281, 26200, 341, 2316, 490, 264, 8459, 370, 286, 478], "temperature": 0.0, "avg_logprob": -0.15717403487403794, "compression_ratio": 1.858267716535433, "no_speech_prob": 1.2407168469508179e-05}, {"id": 252, "seek": 164180, "start": 1647.3999999999999, "end": 1651.8, "text": " going to work through how to build this model like step by step and hopefully that you can have", "tokens": [516, 281, 589, 807, 577, 281, 1322, 341, 2316, 411, 1823, 538, 1823, 293, 4696, 300, 291, 393, 362], "temperature": 0.0, "avg_logprob": -0.15717403487403794, "compression_ratio": 1.858267716535433, "no_speech_prob": 1.2407168469508179e-05}, {"id": 253, "seek": 164180, "start": 1651.8, "end": 1656.28, "text": " a good understanding of how this model works and then I'm just going to briefly talk about how", "tokens": [257, 665, 3701, 295, 577, 341, 2316, 1985, 293, 550, 286, 478, 445, 516, 281, 10515, 751, 466, 577], "temperature": 0.0, "avg_logprob": -0.15717403487403794, "compression_ratio": 1.858267716535433, "no_speech_prob": 1.2407168469508179e-05}, {"id": 254, "seek": 164180, "start": 1656.9199999999998, "end": 1663.3999999999999, "text": " to build this use the bird models for the reading comprehension okay so before I start talking", "tokens": [281, 1322, 341, 764, 264, 5255, 5245, 337, 264, 3760, 44991, 1392, 370, 949, 286, 722, 1417], "temperature": 0.0, "avg_logprob": -0.15717403487403794, "compression_ratio": 1.858267716535433, "no_speech_prob": 1.2407168469508179e-05}, {"id": 255, "seek": 164180, "start": 1663.3999999999999, "end": 1668.12, "text": " about this always team models I know that you have already learned sequence to sequence models", "tokens": [466, 341, 1009, 1469, 5245, 286, 458, 300, 291, 362, 1217, 3264, 8310, 281, 8310, 5245], "temperature": 0.0, "avg_logprob": -0.15717403487403794, "compression_ratio": 1.858267716535433, "no_speech_prob": 1.2407168469508179e-05}, {"id": 256, "seek": 166812, "start": 1668.12, "end": 1674.9199999999998, "text": " result tension for machine translation so I was I want to draw some connections between the", "tokens": [1874, 8980, 337, 3479, 12853, 370, 286, 390, 286, 528, 281, 2642, 512, 9271, 1296, 264], "temperature": 0.0, "avg_logprob": -0.29411433463872866, "compression_ratio": 2.0179372197309418, "no_speech_prob": 6.142075108073186e-06}, {"id": 257, "seek": 166812, "start": 1674.9199999999998, "end": 1679.1599999999999, "text": " machine translation part and the reading comprehension problem because they really share", "tokens": [3479, 12853, 644, 293, 264, 3760, 44991, 1154, 570, 436, 534, 2073], "temperature": 0.0, "avg_logprob": -0.29411433463872866, "compression_ratio": 2.0179372197309418, "no_speech_prob": 6.142075108073186e-06}, {"id": 258, "seek": 166812, "start": 1679.1599999999999, "end": 1686.04, "text": " little similarities so first so in the machine translation model all these like sequence", "tokens": [707, 24197, 370, 700, 370, 294, 264, 3479, 12853, 2316, 439, 613, 411, 8310], "temperature": 0.0, "avg_logprob": -0.29411433463872866, "compression_ratio": 2.0179372197309418, "no_speech_prob": 6.142075108073186e-06}, {"id": 259, "seek": 166812, "start": 1686.04, "end": 1692.12, "text": " use sequence model there is a like source and package the sentence so basically two sequences", "tokens": [764, 8310, 2316, 456, 307, 257, 411, 4009, 293, 7372, 264, 8174, 370, 1936, 732, 22978], "temperature": 0.0, "avg_logprob": -0.29411433463872866, "compression_ratio": 2.0179372197309418, "no_speech_prob": 6.142075108073186e-06}, {"id": 260, "seek": 166812, "start": 1692.6799999999998, "end": 1697.6399999999999, "text": " so that in our case in this reading comprehension case that we also have two sequences", "tokens": [370, 300, 294, 527, 1389, 294, 341, 3760, 44991, 1389, 300, 321, 611, 362, 732, 22978], "temperature": 0.0, "avg_logprob": -0.29411433463872866, "compression_ratio": 2.0179372197309418, "no_speech_prob": 6.142075108073186e-06}, {"id": 261, "seek": 169764, "start": 1697.64, "end": 1702.3600000000001, "text": " one is a passage and another is a question but the lens could be a slightly in balance because", "tokens": [472, 307, 257, 11497, 293, 1071, 307, 257, 1168, 457, 264, 6765, 727, 312, 257, 4748, 294, 4772, 570], "temperature": 0.0, "avg_logprob": -0.21091905454309975, "compression_ratio": 1.898989898989899, "no_speech_prob": 9.810609299165662e-06}, {"id": 262, "seek": 169764, "start": 1702.3600000000001, "end": 1706.8400000000001, "text": " the passage really much longer than the question but it's essentially also two sequences", "tokens": [264, 11497, 534, 709, 2854, 813, 264, 1168, 457, 309, 311, 4476, 611, 732, 22978], "temperature": 0.0, "avg_logprob": -0.21091905454309975, "compression_ratio": 1.898989898989899, "no_speech_prob": 9.810609299165662e-06}, {"id": 263, "seek": 169764, "start": 1709.4, "end": 1715.5600000000002, "text": " and so in the reading comprehension we need to model like which words in a passage are most", "tokens": [293, 370, 294, 264, 3760, 44991, 321, 643, 281, 2316, 411, 597, 2283, 294, 257, 11497, 366, 881], "temperature": 0.0, "avg_logprob": -0.21091905454309975, "compression_ratio": 1.898989898989899, "no_speech_prob": 9.810609299165662e-06}, {"id": 264, "seek": 169764, "start": 1715.5600000000002, "end": 1721.48, "text": " relevant to the question and then if they're relevant to the question so it's also relevant to which", "tokens": [7340, 281, 264, 1168, 293, 550, 498, 436, 434, 7340, 281, 264, 1168, 370, 309, 311, 611, 7340, 281, 597], "temperature": 0.0, "avg_logprob": -0.21091905454309975, "compression_ratio": 1.898989898989899, "no_speech_prob": 9.810609299165662e-06}, {"id": 265, "seek": 172148, "start": 1721.48, "end": 1728.3600000000001, "text": " that of the question works so this is basically a very key important thing that the important", "tokens": [300, 295, 264, 1168, 1985, 370, 341, 307, 1936, 257, 588, 2141, 1021, 551, 300, 264, 1021], "temperature": 0.0, "avg_logprob": -0.19050372730601917, "compression_ratio": 1.9873417721518987, "no_speech_prob": 1.667960168560967e-05}, {"id": 266, "seek": 172148, "start": 1728.3600000000001, "end": 1733.64, "text": " thing that we actually need to model and this is actually very similar to the machine translation", "tokens": [551, 300, 321, 767, 643, 281, 2316, 293, 341, 307, 767, 588, 2531, 281, 264, 3479, 12853], "temperature": 0.0, "avg_logprob": -0.19050372730601917, "compression_ratio": 1.9873417721518987, "no_speech_prob": 1.667960168560967e-05}, {"id": 267, "seek": 172148, "start": 1733.64, "end": 1739.56, "text": " model that we need to model which words in the source sentence that actually are most relevant", "tokens": [2316, 300, 321, 643, 281, 2316, 597, 2283, 294, 264, 4009, 8174, 300, 767, 366, 881, 7340], "temperature": 0.0, "avg_logprob": -0.19050372730601917, "compression_ratio": 1.9873417721518987, "no_speech_prob": 1.667960168560967e-05}, {"id": 268, "seek": 172148, "start": 1739.56, "end": 1744.3600000000001, "text": " to the current packet word so if I imagine that the attention will be also really the key", "tokens": [281, 264, 2190, 20300, 1349, 370, 498, 286, 3811, 300, 264, 3202, 486, 312, 611, 534, 264, 2141], "temperature": 0.0, "avg_logprob": -0.19050372730601917, "compression_ratio": 1.9873417721518987, "no_speech_prob": 1.667960168560967e-05}, {"id": 269, "seek": 172148, "start": 1744.3600000000001, "end": 1749.8, "text": " break in here that just like some sequence to six model we need to model the attention between", "tokens": [1821, 294, 510, 300, 445, 411, 512, 8310, 281, 2309, 2316, 321, 643, 281, 2316, 264, 3202, 1296], "temperature": 0.0, "avg_logprob": -0.19050372730601917, "compression_ratio": 1.9873417721518987, "no_speech_prob": 1.667960168560967e-05}, {"id": 270, "seek": 174980, "start": 1749.8, "end": 1755.24, "text": " the source sentence and the packet sentence we also need to model the attention between the", "tokens": [264, 4009, 8174, 293, 264, 20300, 8174, 321, 611, 643, 281, 2316, 264, 3202, 1296, 264], "temperature": 0.0, "avg_logprob": -0.19014928873302867, "compression_ratio": 1.9545454545454546, "no_speech_prob": 1.028467795549659e-05}, {"id": 271, "seek": 174980, "start": 1755.24, "end": 1760.28, "text": " passage and the question so this is actually very similar so something that's actually not very", "tokens": [11497, 293, 264, 1168, 370, 341, 307, 767, 588, 2531, 370, 746, 300, 311, 767, 406, 588], "temperature": 0.0, "avg_logprob": -0.19014928873302867, "compression_ratio": 1.9545454545454546, "no_speech_prob": 1.028467795549659e-05}, {"id": 272, "seek": 174980, "start": 1760.28, "end": 1766.36, "text": " similar is for the sequence to six model we need to build like a decoder auto-regressivity", "tokens": [2531, 307, 337, 264, 8310, 281, 2309, 2316, 321, 643, 281, 1322, 411, 257, 979, 19866, 8399, 12, 3375, 735, 4253], "temperature": 0.0, "avg_logprob": -0.19014928873302867, "compression_ratio": 1.9545454545454546, "no_speech_prob": 1.028467795549659e-05}, {"id": 273, "seek": 174980, "start": 1766.36, "end": 1772.2, "text": " decoder to generate the packet sentence word by word but in this reading comprehension problem", "tokens": [979, 19866, 281, 8460, 264, 20300, 8174, 1349, 538, 1349, 457, 294, 341, 3760, 44991, 1154], "temperature": 0.0, "avg_logprob": -0.19014928873302867, "compression_ratio": 1.9545454545454546, "no_speech_prob": 1.028467795549659e-05}, {"id": 274, "seek": 174980, "start": 1772.2, "end": 1777.72, "text": " we we don't need to really generate anything so we just take the pass into question so at least for", "tokens": [321, 321, 500, 380, 643, 281, 534, 8460, 1340, 370, 321, 445, 747, 264, 1320, 666, 1168, 370, 412, 1935, 337], "temperature": 0.0, "avg_logprob": -0.19014928873302867, "compression_ratio": 1.9545454545454546, "no_speech_prob": 1.028467795549659e-05}, {"id": 275, "seek": 177772, "start": 1777.72, "end": 1784.84, "text": " the scope on data set we just need to try to cross-spare to predict the start and positions of", "tokens": [264, 11923, 322, 1412, 992, 321, 445, 643, 281, 853, 281, 3278, 12, 4952, 543, 281, 6069, 264, 722, 293, 8432, 295], "temperature": 0.0, "avg_logprob": -0.4034182808615945, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.1653834917524364e-05}, {"id": 276, "seek": 177772, "start": 1784.84, "end": 1790.76, "text": " answer so that's very much you simply find so we need to be to try the decoder to generate the", "tokens": [1867, 370, 300, 311, 588, 709, 291, 2935, 915, 370, 321, 643, 281, 312, 281, 853, 264, 979, 19866, 281, 8460, 264], "temperature": 0.0, "avg_logprob": -0.4034182808615945, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.1653834917524364e-05}, {"id": 277, "seek": 177772, "start": 1790.76, "end": 1799.96, "text": " target sentence okay so next I'm going to talk about one this model called by death so it's", "tokens": [3779, 8174, 1392, 370, 958, 286, 478, 516, 281, 751, 466, 472, 341, 2316, 1219, 538, 2966, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.4034182808615945, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.1653834917524364e-05}, {"id": 278, "seek": 177772, "start": 1799.96, "end": 1805.32, "text": " sent for by directional attention flow for machine comprehension so either was proposed by", "tokens": [2279, 337, 538, 42242, 3202, 3095, 337, 3479, 44991, 370, 2139, 390, 10348, 538], "temperature": 0.0, "avg_logprob": -0.4034182808615945, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.1653834917524364e-05}, {"id": 279, "seek": 180532, "start": 1805.32, "end": 1812.76, "text": " mention seal and other folks in 2017 so it remains before the first time out it remains one of the", "tokens": [2152, 12185, 293, 661, 4024, 294, 6591, 370, 309, 7023, 949, 264, 700, 565, 484, 309, 7023, 472, 295, 264], "temperature": 0.0, "avg_logprob": -0.27688425849465764, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.9286775315995328e-05}, {"id": 280, "seek": 180532, "start": 1812.76, "end": 1818.28, "text": " most popular reading comprehension models and a very good performance at that time at least on", "tokens": [881, 3743, 3760, 44991, 5245, 293, 257, 588, 665, 3389, 412, 300, 565, 412, 1935, 322], "temperature": 0.0, "avg_logprob": -0.27688425849465764, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.9286775315995328e-05}, {"id": 281, "seek": 180532, "start": 1818.28, "end": 1826.28, "text": " the spot dataset so you can see that this model seems to be pretty complicated but if you look", "tokens": [264, 4008, 28872, 370, 291, 393, 536, 300, 341, 2316, 2544, 281, 312, 1238, 6179, 457, 498, 291, 574], "temperature": 0.0, "avg_logprob": -0.27688425849465764, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.9286775315995328e-05}, {"id": 282, "seek": 180532, "start": 1826.28, "end": 1833.1599999999999, "text": " look at this model from the bottom to the top it actually can be decomposed into many different layers", "tokens": [574, 412, 341, 2316, 490, 264, 2767, 281, 264, 1192, 309, 767, 393, 312, 22867, 1744, 666, 867, 819, 7914], "temperature": 0.0, "avg_logprob": -0.27688425849465764, "compression_ratio": 1.722466960352423, "no_speech_prob": 2.9286775315995328e-05}, {"id": 283, "seek": 183316, "start": 1833.16, "end": 1839.72, "text": " so the next I'm going to just dissect this model layer by layer and talk about okay what this", "tokens": [370, 264, 958, 286, 478, 516, 281, 445, 48332, 341, 2316, 4583, 538, 4583, 293, 751, 466, 1392, 437, 341], "temperature": 0.0, "avg_logprob": -0.29216700586779365, "compression_ratio": 1.915, "no_speech_prob": 4.4664659071713686e-05}, {"id": 284, "seek": 183316, "start": 1839.72, "end": 1844.8400000000001, "text": " layer is actually doing and how we can really build this model from the bottom layer to the top layer", "tokens": [4583, 307, 767, 884, 293, 577, 321, 393, 534, 1322, 341, 2316, 490, 264, 2767, 4583, 281, 264, 1192, 4583], "temperature": 0.0, "avg_logprob": -0.29216700586779365, "compression_ratio": 1.915, "no_speech_prob": 4.4664659071713686e-05}, {"id": 285, "seek": 183316, "start": 1844.8400000000001, "end": 1853.88, "text": " and the final retrancess like model in an end to end away okay so the first part it actually", "tokens": [293, 264, 2572, 1533, 4257, 780, 411, 2316, 294, 364, 917, 281, 917, 1314, 1392, 370, 264, 700, 644, 309, 767], "temperature": 0.0, "avg_logprob": -0.29216700586779365, "compression_ratio": 1.915, "no_speech_prob": 4.4664659071713686e-05}, {"id": 286, "seek": 183316, "start": 1853.88, "end": 1859.72, "text": " the bottom three layers called character embedding layer wording embedding layer and the first", "tokens": [264, 2767, 1045, 7914, 1219, 2517, 12240, 3584, 4583, 47602, 12240, 3584, 4583, 293, 264, 700], "temperature": 0.0, "avg_logprob": -0.29216700586779365, "compression_ratio": 1.915, "no_speech_prob": 4.4664659071713686e-05}, {"id": 287, "seek": 185972, "start": 1859.72, "end": 1865.88, "text": " thing that later so I just put them together called this as a encoding function so the idea here", "tokens": [551, 300, 1780, 370, 286, 445, 829, 552, 1214, 1219, 341, 382, 257, 43430, 2445, 370, 264, 1558, 510], "temperature": 0.0, "avg_logprob": -0.19257532612661296, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.0135159502387978e-05}, {"id": 288, "seek": 185972, "start": 1865.88, "end": 1871.56, "text": " is that okay let's take the context query or the passage in question we need to encode them separately", "tokens": [307, 300, 1392, 718, 311, 747, 264, 4319, 14581, 420, 264, 11497, 294, 1168, 321, 643, 281, 2058, 1429, 552, 14759], "temperature": 0.0, "avg_logprob": -0.19257532612661296, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.0135159502387978e-05}, {"id": 289, "seek": 185972, "start": 1873.16, "end": 1879.96, "text": " so to do this so this model basically proposed to use a concatenation of the wording embedding", "tokens": [370, 281, 360, 341, 370, 341, 2316, 1936, 10348, 281, 764, 257, 1588, 7186, 399, 295, 264, 47602, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.19257532612661296, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.0135159502387978e-05}, {"id": 290, "seek": 185972, "start": 1879.96, "end": 1886.6000000000001, "text": " as well as the character embedding for each word in the context and query so for the wording", "tokens": [382, 731, 382, 264, 2517, 12240, 3584, 337, 1184, 1349, 294, 264, 4319, 293, 14581, 370, 337, 264, 47602], "temperature": 0.0, "avg_logprob": -0.19257532612661296, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.0135159502387978e-05}, {"id": 291, "seek": 188660, "start": 1886.6, "end": 1891.56, "text": " embedding straightforward so you have a wording embedding so you can just look up the word", "tokens": [12240, 3584, 15325, 370, 291, 362, 257, 47602, 12240, 3584, 370, 291, 393, 445, 574, 493, 264, 1349], "temperature": 0.0, "avg_logprob": -0.2539550511523931, "compression_ratio": 1.9913419913419914, "no_speech_prob": 1.8910895960289054e-05}, {"id": 292, "seek": 188660, "start": 1891.56, "end": 1897.24, "text": " for the this word like Seattle just use the global embedding as a reputation for this word", "tokens": [337, 264, 341, 1349, 411, 15721, 445, 764, 264, 4338, 12240, 3584, 382, 257, 13061, 337, 341, 1349], "temperature": 0.0, "avg_logprob": -0.2539550511523931, "compression_ratio": 1.9913419913419914, "no_speech_prob": 1.8910895960289054e-05}, {"id": 293, "seek": 188660, "start": 1897.8799999999999, "end": 1904.12, "text": " and for the character embedding part so you basically need to represent each character in this", "tokens": [293, 337, 264, 2517, 12240, 3584, 644, 370, 291, 1936, 643, 281, 2906, 1184, 2517, 294, 341], "temperature": 0.0, "avg_logprob": -0.2539550511523931, "compression_ratio": 1.9913419913419914, "no_speech_prob": 1.8910895960289054e-05}, {"id": 294, "seek": 188660, "start": 1904.12, "end": 1910.04, "text": " word like Seattle and the hypothesis to a convolutional neural network with some kind of max", "tokens": [1349, 411, 15721, 293, 264, 17291, 281, 257, 45216, 304, 18161, 3209, 365, 512, 733, 295, 11469], "temperature": 0.0, "avg_logprob": -0.2539550511523931, "compression_ratio": 1.9913419913419914, "no_speech_prob": 1.8910895960289054e-05}, {"id": 295, "seek": 188660, "start": 1910.04, "end": 1914.6799999999998, "text": " point operations and finally you can just get one reputation I will talk and then you just", "tokens": [935, 7705, 293, 2721, 291, 393, 445, 483, 472, 13061, 286, 486, 751, 293, 550, 291, 445], "temperature": 0.0, "avg_logprob": -0.2539550511523931, "compression_ratio": 1.9913419913419914, "no_speech_prob": 1.8910895960289054e-05}, {"id": 296, "seek": 191468, "start": 1914.68, "end": 1920.68, "text": " concatenate the wording embedding and the character embedding so this character embedding has been", "tokens": [1588, 7186, 473, 264, 47602, 12240, 3584, 293, 264, 2517, 12240, 3584, 370, 341, 2517, 12240, 3584, 575, 668], "temperature": 0.0, "avg_logprob": -0.2181099664597284, "compression_ratio": 2.060085836909871, "no_speech_prob": 9.512282304058317e-06}, {"id": 297, "seek": 191468, "start": 1920.68, "end": 1927.3200000000002, "text": " shown exactly to improve the reputation for the unseen or the real world so mathematically a", "tokens": [4898, 2293, 281, 3470, 264, 13061, 337, 264, 40608, 420, 264, 957, 1002, 370, 44003, 257], "temperature": 0.0, "avg_logprob": -0.2181099664597284, "compression_ratio": 2.060085836909871, "no_speech_prob": 9.512282304058317e-06}, {"id": 298, "seek": 191468, "start": 1927.3200000000002, "end": 1932.76, "text": " mathematical you can see that so for each word in the context query you can just we can just", "tokens": [18894, 291, 393, 536, 300, 370, 337, 1184, 1349, 294, 264, 4319, 14581, 291, 393, 445, 321, 393, 445], "temperature": 0.0, "avg_logprob": -0.2181099664597284, "compression_ratio": 2.060085836909871, "no_speech_prob": 9.512282304058317e-06}, {"id": 299, "seek": 191468, "start": 1932.76, "end": 1938.2, "text": " represent the rotation of the block with the embedding and the character embedding and then we", "tokens": [2906, 264, 12447, 295, 264, 3461, 365, 264, 12240, 3584, 293, 264, 2517, 12240, 3584, 293, 550, 321], "temperature": 0.0, "avg_logprob": -0.2181099664597284, "compression_ratio": 2.060085836909871, "no_speech_prob": 9.512282304058317e-06}, {"id": 300, "seek": 191468, "start": 1938.2, "end": 1943.5600000000002, "text": " just concatenate them and pass each other all highway networks so I don't write the function here so", "tokens": [445, 1588, 7186, 473, 552, 293, 1320, 1184, 661, 439, 17205, 9590, 370, 286, 500, 380, 2464, 264, 2445, 510, 370], "temperature": 0.0, "avg_logprob": -0.2181099664597284, "compression_ratio": 2.060085836909871, "no_speech_prob": 9.512282304058317e-06}, {"id": 301, "seek": 194356, "start": 1943.56, "end": 1951.6399999999999, "text": " so you can just look up orange on paper and the second part so other we call the issue a very visual", "tokens": [370, 291, 393, 445, 574, 493, 7671, 322, 3035, 293, 264, 1150, 644, 370, 661, 321, 818, 264, 2734, 257, 588, 5056], "temperature": 0.0, "avg_logprob": -0.30632408989800347, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.568331936141476e-05}, {"id": 302, "seek": 194356, "start": 1951.6399999999999, "end": 1958.84, "text": " work so and the next we are going to pass this wording embedding into two separate by directional", "tokens": [589, 370, 293, 264, 958, 321, 366, 516, 281, 1320, 341, 47602, 12240, 3584, 666, 732, 4994, 538, 42242], "temperature": 0.0, "avg_logprob": -0.30632408989800347, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.568331936141476e-05}, {"id": 303, "seek": 194356, "start": 1958.84, "end": 1965.96, "text": " LSTNs to separate me to produce these contextualized embeddings for both the context and query so", "tokens": [441, 6840, 45, 82, 281, 4994, 385, 281, 5258, 613, 35526, 1602, 12240, 29432, 337, 1293, 264, 4319, 293, 14581, 370], "temperature": 0.0, "avg_logprob": -0.30632408989800347, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.568331936141476e-05}, {"id": 304, "seek": 194356, "start": 1967.24, "end": 1972.36, "text": " let's look at these equations so we take the reputation of this word and then we just", "tokens": [718, 311, 574, 412, 613, 11787, 370, 321, 747, 264, 13061, 295, 341, 1349, 293, 550, 321, 445], "temperature": 0.0, "avg_logprob": -0.30632408989800347, "compression_ratio": 1.7285067873303168, "no_speech_prob": 1.568331936141476e-05}, {"id": 305, "seek": 197236, "start": 1972.36, "end": 1979.9599999999998, "text": " base this in like a one LSTN model from one direction and this LSTN model from another direction", "tokens": [3096, 341, 294, 411, 257, 472, 441, 6840, 45, 2316, 490, 472, 3513, 293, 341, 441, 6840, 45, 2316, 490, 1071, 3513], "temperature": 0.0, "avg_logprob": -0.2428236673044604, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.3834352103003766e-05}, {"id": 306, "seek": 197236, "start": 1979.9599999999998, "end": 1986.6799999999998, "text": " so we just need to concatenate the two key rotations two directions and then finally we can get", "tokens": [370, 321, 445, 643, 281, 1588, 7186, 473, 264, 732, 2141, 44796, 732, 11095, 293, 550, 2721, 321, 393, 483], "temperature": 0.0, "avg_logprob": -0.2428236673044604, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.3834352103003766e-05}, {"id": 307, "seek": 197236, "start": 1986.6799999999998, "end": 1993.0, "text": " a contextualized reputation for each single word in the context and then we can do the same", "tokens": [257, 35526, 1602, 13061, 337, 1184, 2167, 1349, 294, 264, 4319, 293, 550, 321, 393, 360, 264, 912], "temperature": 0.0, "avg_logprob": -0.2428236673044604, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.3834352103003766e-05}, {"id": 308, "seek": 197236, "start": 1993.0, "end": 1999.08, "text": " similar thing for the question reputation also want to query mention because I mentioned the sequence", "tokens": [2531, 551, 337, 264, 1168, 13061, 611, 528, 281, 14581, 2152, 570, 286, 2835, 264, 8310], "temperature": 0.0, "avg_logprob": -0.2428236673044604, "compression_ratio": 1.812206572769953, "no_speech_prob": 1.3834352103003766e-05}, {"id": 309, "seek": 199908, "start": 1999.08, "end": 2004.36, "text": " your sequence model so sequence to signal although we can already do this bad directional LSTNs", "tokens": [428, 8310, 2316, 370, 8310, 281, 6358, 4878, 321, 393, 1217, 360, 341, 1578, 42242, 441, 6840, 45, 82], "temperature": 0.0, "avg_logprob": -0.25625904628208707, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.80536626430694e-05}, {"id": 310, "seek": 199908, "start": 2004.36, "end": 2009.96, "text": " for the two sequences again like because the decoder is all all the embarrassing model so that's", "tokens": [337, 264, 732, 22978, 797, 411, 570, 264, 979, 19866, 307, 439, 439, 264, 17299, 2316, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.25625904628208707, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.80536626430694e-05}, {"id": 311, "seek": 199908, "start": 2009.96, "end": 2015.6399999999999, "text": " why the decoder is usually just implemented as a unit direction on this team but because of", "tokens": [983, 264, 979, 19866, 307, 2673, 445, 12270, 382, 257, 4985, 3513, 322, 341, 1469, 457, 570, 295], "temperature": 0.0, "avg_logprob": -0.25625904628208707, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.80536626430694e-05}, {"id": 312, "seek": 199908, "start": 2015.6399999999999, "end": 2021.32, "text": " here we don't really care about the generation so we can just use two bad directional LSTNs to", "tokens": [510, 321, 500, 380, 534, 1127, 466, 264, 5125, 370, 321, 393, 445, 764, 732, 1578, 42242, 441, 6840, 45, 82, 281], "temperature": 0.0, "avg_logprob": -0.25625904628208707, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.80536626430694e-05}, {"id": 313, "seek": 199908, "start": 2021.32, "end": 2027.3999999999999, "text": " represent the rotation this is actually very important this bad directional lentil is actually", "tokens": [2906, 264, 12447, 341, 307, 767, 588, 1021, 341, 1578, 42242, 23556, 388, 307, 767], "temperature": 0.0, "avg_logprob": -0.25625904628208707, "compression_ratio": 1.9268292682926829, "no_speech_prob": 1.80536626430694e-05}, {"id": 314, "seek": 202740, "start": 2027.4, "end": 2035.96, "text": " very important to capture the context from both the left and right set okay so the next component", "tokens": [588, 1021, 281, 7983, 264, 4319, 490, 1293, 264, 1411, 293, 558, 992, 1392, 370, 264, 958, 6542], "temperature": 0.0, "avg_logprob": -0.27902023792266845, "compression_ratio": 1.8928571428571428, "no_speech_prob": 1.0775570444820914e-05}, {"id": 315, "seek": 202740, "start": 2035.96, "end": 2040.44, "text": " is the next layer it's called the attention flow layer so I just call it the attention here", "tokens": [307, 264, 958, 4583, 309, 311, 1219, 264, 3202, 3095, 4583, 370, 286, 445, 818, 309, 264, 3202, 510], "temperature": 0.0, "avg_logprob": -0.27902023792266845, "compression_ratio": 1.8928571428571428, "no_speech_prob": 1.0775570444820914e-05}, {"id": 316, "seek": 202740, "start": 2041.0800000000002, "end": 2046.6000000000001, "text": " so the attention idea the idea of attention is trying to capture the interactions between the", "tokens": [370, 264, 3202, 1558, 264, 1558, 295, 3202, 307, 1382, 281, 7983, 264, 13280, 1296, 264], "temperature": 0.0, "avg_logprob": -0.27902023792266845, "compression_ratio": 1.8928571428571428, "no_speech_prob": 1.0775570444820914e-05}, {"id": 317, "seek": 202740, "start": 2046.6000000000001, "end": 2052.6800000000003, "text": " context and query and in this paper the baddest paper they propose two types of tension", "tokens": [4319, 293, 14581, 293, 294, 341, 3035, 264, 1578, 23748, 3035, 436, 17421, 732, 3467, 295, 8980], "temperature": 0.0, "avg_logprob": -0.27902023792266845, "compression_ratio": 1.8928571428571428, "no_speech_prob": 1.0775570444820914e-05}, {"id": 318, "seek": 205268, "start": 2052.68, "end": 2060.7599999999998, "text": " so the first type of tension we call the context your query attention so the idea is for each context", "tokens": [370, 264, 700, 2010, 295, 8980, 321, 818, 264, 4319, 428, 14581, 3202, 370, 264, 1558, 307, 337, 1184, 4319], "temperature": 0.0, "avg_logprob": -0.33646342333625345, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.04448381534894e-05}, {"id": 319, "seek": 205268, "start": 2060.7599999999998, "end": 2069.24, "text": " word can we find the most relevant words in the question from the question for the query for the", "tokens": [1349, 393, 321, 915, 264, 881, 7340, 2283, 294, 264, 1168, 490, 264, 1168, 337, 264, 14581, 337, 264], "temperature": 0.0, "avg_logprob": -0.33646342333625345, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.04448381534894e-05}, {"id": 320, "seek": 205268, "start": 2069.24, "end": 2075.7999999999997, "text": " query words so here's the one example so here the context context the problem of Rama is a present", "tokens": [14581, 2283, 370, 510, 311, 264, 472, 1365, 370, 510, 264, 4319, 4319, 264, 1154, 295, 497, 2404, 307, 257, 1974], "temperature": 0.0, "avg_logprob": -0.33646342333625345, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.04448381534894e-05}, {"id": 321, "seek": 207580, "start": 2075.8, "end": 2082.36, "text": " of the USA so for each context word we need to find an alignment because it's fun like the wish words", "tokens": [295, 264, 10827, 370, 337, 1184, 4319, 1349, 321, 643, 281, 915, 364, 18515, 570, 309, 311, 1019, 411, 264, 3172, 2283], "temperature": 0.0, "avg_logprob": -0.31584426459916143, "compression_ratio": 1.892, "no_speech_prob": 7.069169441820122e-06}, {"id": 322, "seek": 207580, "start": 2082.36, "end": 2086.92, "text": " in the question can be actually aligned with this context word so we can see that both", "tokens": [294, 264, 1168, 393, 312, 767, 17962, 365, 341, 4319, 1349, 370, 321, 393, 536, 300, 1293], "temperature": 0.0, "avg_logprob": -0.31584426459916143, "compression_ratio": 1.892, "no_speech_prob": 7.069169441820122e-06}, {"id": 323, "seek": 207580, "start": 2086.92, "end": 2092.44, "text": " both of them can be aligned to pool and the president will align to the list and the USA is", "tokens": [1293, 295, 552, 393, 312, 17962, 281, 7005, 293, 264, 3868, 486, 7975, 281, 264, 1329, 293, 264, 10827, 307], "temperature": 0.0, "avg_logprob": -0.31584426459916143, "compression_ratio": 1.892, "no_speech_prob": 7.069169441820122e-06}, {"id": 324, "seek": 207580, "start": 2092.44, "end": 2096.6800000000003, "text": " aligned to the United States so basically for each content we'll try to find the most relevant", "tokens": [17962, 281, 264, 2824, 3040, 370, 1936, 337, 1184, 2701, 321, 603, 853, 281, 915, 264, 881, 7340], "temperature": 0.0, "avg_logprob": -0.31584426459916143, "compression_ratio": 1.892, "no_speech_prob": 7.069169441820122e-06}, {"id": 325, "seek": 207580, "start": 2096.6800000000003, "end": 2104.76, "text": " query words and then not the second type of tension is called query to context or tension so it's", "tokens": [14581, 2283, 293, 550, 406, 264, 1150, 2010, 295, 8980, 307, 1219, 14581, 281, 4319, 420, 8980, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.31584426459916143, "compression_ratio": 1.892, "no_speech_prob": 7.069169441820122e-06}, {"id": 326, "seek": 210476, "start": 2104.76, "end": 2112.1200000000003, "text": " a very or not direction so here the idea is to choose some context words that are most relevant to", "tokens": [257, 588, 420, 406, 3513, 370, 510, 264, 1558, 307, 281, 2826, 512, 4319, 2283, 300, 366, 881, 7340, 281], "temperature": 0.0, "avg_logprob": -0.15183561901713527, "compression_ratio": 1.8, "no_speech_prob": 1.641736162127927e-05}, {"id": 327, "seek": 210476, "start": 2112.1200000000003, "end": 2118.5200000000004, "text": " one of the query words because the context can be very long so a lot of the context could be", "tokens": [472, 295, 264, 14581, 2283, 570, 264, 4319, 393, 312, 588, 938, 370, 257, 688, 295, 264, 4319, 727, 312], "temperature": 0.0, "avg_logprob": -0.15183561901713527, "compression_ratio": 1.8, "no_speech_prob": 1.641736162127927e-05}, {"id": 328, "seek": 210476, "start": 2118.5200000000004, "end": 2125.96, "text": " just not relevant to the discussion so we just run over several examples you can see that", "tokens": [445, 406, 7340, 281, 264, 5017, 370, 321, 445, 1190, 670, 2940, 5110, 291, 393, 536, 300], "temperature": 0.0, "avg_logprob": -0.15183561901713527, "compression_ratio": 1.8, "no_speech_prob": 1.641736162127927e-05}, {"id": 329, "seek": 210476, "start": 2125.96, "end": 2130.84, "text": " the first thing we need to do is try to locate okay wish cards of the sentences in this", "tokens": [264, 700, 551, 321, 643, 281, 360, 307, 853, 281, 22370, 1392, 3172, 5632, 295, 264, 16579, 294, 341], "temperature": 0.0, "avg_logprob": -0.15183561901713527, "compression_ratio": 1.8, "no_speech_prob": 1.641736162127927e-05}, {"id": 330, "seek": 213084, "start": 2130.84, "end": 2136.36, "text": " context can be actually relevant to this question so this type of query to context or tension", "tokens": [4319, 393, 312, 767, 7340, 281, 341, 1168, 370, 341, 2010, 295, 14581, 281, 4319, 420, 8980], "temperature": 0.0, "avg_logprob": -0.2654099918547131, "compression_ratio": 2.0108108108108107, "no_speech_prob": 2.8827220376115292e-05}, {"id": 331, "seek": 213084, "start": 2136.36, "end": 2145.48, "text": " is trying to capture so which which context words actually can be most relevant to the query to", "tokens": [307, 1382, 281, 7983, 370, 597, 597, 4319, 2283, 767, 393, 312, 881, 7340, 281, 264, 14581, 281], "temperature": 0.0, "avg_logprob": -0.2654099918547131, "compression_ratio": 2.0108108108108107, "no_speech_prob": 2.8827220376115292e-05}, {"id": 332, "seek": 213084, "start": 2145.48, "end": 2151.7200000000003, "text": " one of the query words so for this example the question is which seeding the glooming in winter", "tokens": [472, 295, 264, 14581, 2283, 370, 337, 341, 1365, 264, 1168, 307, 597, 8871, 278, 264, 3114, 10539, 294, 6355], "temperature": 0.0, "avg_logprob": -0.2654099918547131, "compression_ratio": 2.0108108108108107, "no_speech_prob": 2.8827220376115292e-05}, {"id": 333, "seek": 213084, "start": 2151.7200000000003, "end": 2157.0, "text": " so because the question asked about glooming so you can find a triacy of okay glooming", "tokens": [370, 570, 264, 1168, 2351, 466, 3114, 10539, 370, 291, 393, 915, 257, 1376, 2551, 295, 1392, 3114, 10539], "temperature": 0.0, "avg_logprob": -0.2654099918547131, "compression_ratio": 2.0108108108108107, "no_speech_prob": 2.8827220376115292e-05}, {"id": 334, "seek": 215700, "start": 2157.0, "end": 2165.16, "text": " species is actually very relevant to this question and now we also find this in winter because", "tokens": [6172, 307, 767, 588, 7340, 281, 341, 1168, 293, 586, 321, 611, 915, 341, 294, 6355, 570], "temperature": 0.0, "avg_logprob": -0.20623804393567538, "compression_ratio": 2.096590909090909, "no_speech_prob": 1.260251610801788e-05}, {"id": 335, "seek": 215700, "start": 2165.16, "end": 2171.08, "text": " in winter it also mentioned the question so this part of context words to be also relevant to", "tokens": [294, 6355, 309, 611, 2835, 264, 1168, 370, 341, 644, 295, 4319, 2283, 281, 312, 611, 7340, 281], "temperature": 0.0, "avg_logprob": -0.20623804393567538, "compression_ratio": 2.096590909090909, "no_speech_prob": 1.260251610801788e-05}, {"id": 336, "seek": 215700, "start": 2171.08, "end": 2177.96, "text": " this question so this context words could be probably need to capture and not in this", "tokens": [341, 1168, 370, 341, 4319, 2283, 727, 312, 1391, 643, 281, 7983, 293, 406, 294, 341], "temperature": 0.0, "avg_logprob": -0.20623804393567538, "compression_ratio": 2.096590909090909, "no_speech_prob": 1.260251610801788e-05}, {"id": 337, "seek": 215700, "start": 2177.96, "end": 2183.8, "text": " tension that okay this actually relevant to this question okay so this actually basically just", "tokens": [8980, 300, 1392, 341, 767, 7340, 281, 341, 1168, 1392, 370, 341, 767, 1936, 445], "temperature": 0.0, "avg_logprob": -0.20623804393567538, "compression_ratio": 2.096590909090909, "no_speech_prob": 1.260251610801788e-05}, {"id": 338, "seek": 218380, "start": 2183.8, "end": 2190.6000000000004, "text": " a intuition of this two types of tension and this also wise model is called a bi-directional", "tokens": [257, 24002, 295, 341, 732, 3467, 295, 8980, 293, 341, 611, 10829, 2316, 307, 1219, 257, 3228, 12, 18267, 41048], "temperature": 0.0, "avg_logprob": -0.27352235430762883, "compression_ratio": 1.9787234042553192, "no_speech_prob": 8.010018973436672e-06}, {"id": 339, "seek": 218380, "start": 2190.6000000000004, "end": 2196.1200000000003, "text": " tension flow because there is a context query or tension and there is also a parity context", "tokens": [8980, 3095, 570, 456, 307, 257, 4319, 14581, 420, 8980, 293, 456, 307, 611, 257, 44747, 4319], "temperature": 0.0, "avg_logprob": -0.27352235430762883, "compression_ratio": 1.9787234042553192, "no_speech_prob": 8.010018973436672e-06}, {"id": 340, "seek": 218380, "start": 2196.1200000000003, "end": 2205.5600000000004, "text": " tension so let me just talk about how to actually do this like query to context tension the", "tokens": [8980, 370, 718, 385, 445, 751, 466, 577, 281, 767, 360, 341, 411, 14581, 281, 4319, 8980, 264], "temperature": 0.0, "avg_logprob": -0.27352235430762883, "compression_ratio": 1.9787234042553192, "no_speech_prob": 8.010018973436672e-06}, {"id": 341, "seek": 218380, "start": 2205.5600000000004, "end": 2212.36, "text": " context query or tension in this model so the way they do this is first to compute a similarity", "tokens": [4319, 14581, 420, 8980, 294, 341, 2316, 370, 264, 636, 436, 360, 341, 307, 700, 281, 14722, 257, 32194], "temperature": 0.0, "avg_logprob": -0.27352235430762883, "compression_ratio": 1.9787234042553192, "no_speech_prob": 8.010018973436672e-06}, {"id": 342, "seek": 221236, "start": 2212.36, "end": 2219.96, "text": " sport for every period of the contextualized vector C i and for every pair of the question with", "tokens": [7282, 337, 633, 2896, 295, 264, 35526, 1602, 8062, 383, 741, 293, 337, 633, 6119, 295, 264, 1168, 365], "temperature": 0.0, "avg_logprob": -0.2755261872944079, "compression_ratio": 1.766355140186916, "no_speech_prob": 1.3839676284987945e-05}, {"id": 343, "seek": 221236, "start": 2219.96, "end": 2225.48, "text": " QJ so this is actually the output from the encoding layer so this already is the output from the LSTM", "tokens": [1249, 41, 370, 341, 307, 767, 264, 5598, 490, 264, 43430, 4583, 370, 341, 1217, 307, 264, 5598, 490, 264, 441, 6840, 44], "temperature": 0.0, "avg_logprob": -0.2755261872944079, "compression_ratio": 1.766355140186916, "no_speech_prob": 1.3839676284987945e-05}, {"id": 344, "seek": 221236, "start": 2225.48, "end": 2233.48, "text": " layers and the way they say basically just compute a similarity sport by taking the C i QJ", "tokens": [7914, 293, 264, 636, 436, 584, 1936, 445, 14722, 257, 32194, 7282, 538, 1940, 264, 383, 741, 1249, 41], "temperature": 0.0, "avg_logprob": -0.2755261872944079, "compression_ratio": 1.766355140186916, "no_speech_prob": 1.3839676284987945e-05}, {"id": 345, "seek": 221236, "start": 2234.1200000000003, "end": 2240.84, "text": " and also the element wise amygdication of the C i QJ so it's a basically just concatenate", "tokens": [293, 611, 264, 4478, 10829, 669, 18103, 67, 8758, 295, 264, 383, 741, 1249, 41, 370, 309, 311, 257, 1936, 445, 1588, 7186, 473], "temperature": 0.0, "avg_logprob": -0.2755261872944079, "compression_ratio": 1.766355140186916, "no_speech_prob": 1.3839676284987945e-05}, {"id": 346, "seek": 224084, "start": 2240.84, "end": 2247.1600000000003, "text": " these three vectors so the output will be a six-inch dimensional vector and they just", "tokens": [613, 1045, 18875, 370, 264, 5598, 486, 312, 257, 2309, 12, 12415, 18795, 8062, 293, 436, 445], "temperature": 0.0, "avg_logprob": -0.325661069891426, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6375121200981084e-06}, {"id": 347, "seek": 224084, "start": 2247.1600000000003, "end": 2253.88, "text": " match this to compute the dot product of another like a learnable vector and the family just", "tokens": [2995, 341, 281, 14722, 264, 5893, 1674, 295, 1071, 411, 257, 1466, 712, 8062, 293, 264, 1605, 445], "temperature": 0.0, "avg_logprob": -0.325661069891426, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6375121200981084e-06}, {"id": 348, "seek": 224084, "start": 2253.88, "end": 2261.2400000000002, "text": " this can't this can't give you one scalar one number the is ij which matters how on the similarity", "tokens": [341, 393, 380, 341, 393, 380, 976, 291, 472, 39684, 472, 1230, 264, 307, 741, 73, 597, 7001, 577, 322, 264, 32194], "temperature": 0.0, "avg_logprob": -0.325661069891426, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6375121200981084e-06}, {"id": 349, "seek": 224084, "start": 2261.2400000000002, "end": 2268.28, "text": " between this context word C i and also this question word QJ so if you have so if I learned some", "tokens": [1296, 341, 4319, 1349, 383, 741, 293, 611, 341, 1168, 1349, 1249, 41, 370, 498, 291, 362, 370, 498, 286, 3264, 512], "temperature": 0.0, "avg_logprob": -0.325661069891426, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6375121200981084e-06}, {"id": 350, "seek": 226828, "start": 2268.28, "end": 2274.36, "text": " attention before so this is actually just one choice of this model so there could be many different", "tokens": [3202, 949, 370, 341, 307, 767, 445, 472, 3922, 295, 341, 2316, 370, 456, 727, 312, 867, 819], "temperature": 0.0, "avg_logprob": -0.17569338557231856, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.747462975909002e-06}, {"id": 351, "seek": 226828, "start": 2274.36, "end": 2280.44, "text": " ways to define this similarity and a similarity sport so this is basically just one design choice", "tokens": [2098, 281, 6964, 341, 32194, 293, 257, 32194, 7282, 370, 341, 307, 1936, 445, 472, 1715, 3922], "temperature": 0.0, "avg_logprob": -0.17569338557231856, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.747462975909002e-06}, {"id": 352, "seek": 226828, "start": 2280.44, "end": 2289.5600000000004, "text": " of this model okay so after defined this similarity sport is ij so the context to query", "tokens": [295, 341, 2316, 1392, 370, 934, 7642, 341, 32194, 7282, 307, 741, 73, 370, 264, 4319, 281, 14581], "temperature": 0.0, "avg_logprob": -0.17569338557231856, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.747462975909002e-06}, {"id": 353, "seek": 226828, "start": 2289.5600000000004, "end": 2295.88, "text": " attention again like which question words are most relevant to C i so the way they do this is so", "tokens": [3202, 797, 411, 597, 1168, 2283, 366, 881, 7340, 281, 383, 741, 370, 264, 636, 436, 360, 341, 307, 370], "temperature": 0.0, "avg_logprob": -0.17569338557231856, "compression_ratio": 1.9292929292929293, "no_speech_prob": 6.747462975909002e-06}, {"id": 354, "seek": 229588, "start": 2295.88, "end": 2301.6400000000003, "text": " basically just taking this matrix the similarity sport is ij for each row each row basically", "tokens": [1936, 445, 1940, 341, 8141, 264, 32194, 7282, 307, 741, 73, 337, 1184, 5386, 1184, 5386, 1936], "temperature": 0.0, "avg_logprob": -0.23018310183570498, "compression_ratio": 1.829268292682927, "no_speech_prob": 3.0890735160937766e-06}, {"id": 355, "seek": 229588, "start": 2301.6400000000003, "end": 2310.12, "text": " corresponds to like one context word for each row they are going to compute a soft max for each", "tokens": [23249, 281, 411, 472, 4319, 1349, 337, 1184, 5386, 436, 366, 516, 281, 14722, 257, 2787, 11469, 337, 1184], "temperature": 0.0, "avg_logprob": -0.23018310183570498, "compression_ratio": 1.829268292682927, "no_speech_prob": 3.0890735160937766e-06}, {"id": 356, "seek": 229588, "start": 2310.12, "end": 2316.84, "text": " row and this can give us like normalization sports r for ij which is our probability distribution", "tokens": [5386, 293, 341, 393, 976, 505, 411, 2710, 2144, 6573, 367, 337, 741, 73, 597, 307, 527, 8482, 7316], "temperature": 0.0, "avg_logprob": -0.23018310183570498, "compression_ratio": 1.829268292682927, "no_speech_prob": 3.0890735160937766e-06}, {"id": 357, "seek": 229588, "start": 2316.84, "end": 2322.84, "text": " over all the question words r for ij so this is just really similar to all the attention", "tokens": [670, 439, 264, 1168, 2283, 367, 337, 741, 73, 370, 341, 307, 445, 534, 2531, 281, 439, 264, 3202], "temperature": 0.0, "avg_logprob": -0.23018310183570498, "compression_ratio": 1.829268292682927, "no_speech_prob": 3.0890735160937766e-06}, {"id": 358, "seek": 232284, "start": 2322.84, "end": 2327.88, "text": " on the kind of things that you probably have seen in this class so basically for each", "tokens": [322, 264, 733, 295, 721, 300, 291, 1391, 362, 1612, 294, 341, 1508, 370, 1936, 337, 1184], "temperature": 0.0, "avg_logprob": -0.2132884399800361, "compression_ratio": 1.7584541062801933, "no_speech_prob": 4.028161583846668e-06}, {"id": 359, "seek": 232284, "start": 2327.88, "end": 2337.48, "text": " context word taking the soft max over all the question words and get us probability distribution", "tokens": [4319, 1349, 1940, 264, 2787, 11469, 670, 439, 264, 1168, 2283, 293, 483, 505, 8482, 7316], "temperature": 0.0, "avg_logprob": -0.2132884399800361, "compression_ratio": 1.7584541062801933, "no_speech_prob": 4.028161583846668e-06}, {"id": 360, "seek": 232284, "start": 2337.48, "end": 2343.1600000000003, "text": " and finally just take the linear combination of the weighted combination of these attention", "tokens": [293, 2721, 445, 747, 264, 8213, 6562, 295, 264, 32807, 6562, 295, 613, 3202], "temperature": 0.0, "avg_logprob": -0.2132884399800361, "compression_ratio": 1.7584541062801933, "no_speech_prob": 4.028161583846668e-06}, {"id": 361, "seek": 232284, "start": 2343.1600000000003, "end": 2348.6800000000003, "text": " score r for ij and also the question vector the QJ and the finally you can't get a vector", "tokens": [6175, 367, 337, 741, 73, 293, 611, 264, 1168, 8062, 264, 1249, 41, 293, 264, 2721, 291, 393, 380, 483, 257, 8062], "temperature": 0.0, "avg_logprob": -0.2132884399800361, "compression_ratio": 1.7584541062801933, "no_speech_prob": 4.028161583846668e-06}, {"id": 362, "seek": 234868, "start": 2348.68, "end": 2355.0, "text": " a i which is actually two h-dimensional vector so this context to query attention basically", "tokens": [257, 741, 597, 307, 767, 732, 276, 12, 18759, 8062, 370, 341, 4319, 281, 14581, 3202, 1936], "temperature": 0.0, "avg_logprob": -0.2468219435358622, "compression_ratio": 1.912820512820513, "no_speech_prob": 8.935998266679235e-06}, {"id": 363, "seek": 234868, "start": 2355.0, "end": 2361.96, "text": " just try to capture which questions was a most relevant to each context word so the next part", "tokens": [445, 853, 281, 7983, 597, 1651, 390, 257, 881, 7340, 281, 1184, 4319, 1349, 370, 264, 958, 644], "temperature": 0.0, "avg_logprob": -0.2468219435358622, "compression_ratio": 1.912820512820513, "no_speech_prob": 8.935998266679235e-06}, {"id": 364, "seek": 234868, "start": 2361.96, "end": 2369.0, "text": " part is a query to sorry the title here sorry this actually the query to context or attention so", "tokens": [644, 307, 257, 14581, 281, 2597, 264, 4876, 510, 2597, 341, 767, 264, 14581, 281, 4319, 420, 3202, 370], "temperature": 0.0, "avg_logprob": -0.2468219435358622, "compression_ratio": 1.912820512820513, "no_speech_prob": 8.935998266679235e-06}, {"id": 365, "seek": 234868, "start": 2369.0, "end": 2375.08, "text": " which means that which context was relevant to some question words so we don't so a lot of", "tokens": [597, 1355, 300, 597, 4319, 390, 7340, 281, 512, 1168, 2283, 370, 321, 500, 380, 370, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.2468219435358622, "compression_ratio": 1.912820512820513, "no_speech_prob": 8.935998266679235e-06}, {"id": 366, "seek": 237508, "start": 2375.08, "end": 2381.56, "text": " context words would be no relevant to this question so the idea to do this is for each row of", "tokens": [4319, 2283, 576, 312, 572, 7340, 281, 341, 1168, 370, 264, 1558, 281, 360, 341, 307, 337, 1184, 5386, 295], "temperature": 0.0, "avg_logprob": -0.19839422386812877, "compression_ratio": 1.96875, "no_speech_prob": 1.16503651952371e-05}, {"id": 367, "seek": 237508, "start": 2381.56, "end": 2388.92, "text": " this is ij this basically just takes a mass scores over all the question words and after taking", "tokens": [341, 307, 741, 73, 341, 1936, 445, 2516, 257, 2758, 13444, 670, 439, 264, 1168, 2283, 293, 934, 1940], "temperature": 0.0, "avg_logprob": -0.19839422386812877, "compression_ratio": 1.96875, "no_speech_prob": 1.16503651952371e-05}, {"id": 368, "seek": 237508, "start": 2388.92, "end": 2395.64, "text": " this mass score they compute the soft max over all the context words here so here i actually", "tokens": [341, 2758, 6175, 436, 14722, 264, 2787, 11469, 670, 439, 264, 4319, 2283, 510, 370, 510, 741, 767], "temperature": 0.0, "avg_logprob": -0.19839422386812877, "compression_ratio": 1.96875, "no_speech_prob": 1.16503651952371e-05}, {"id": 369, "seek": 237508, "start": 2395.64, "end": 2401.48, "text": " numerous over all the context words and this can give us like attention another attention score", "tokens": [12546, 670, 439, 264, 4319, 2283, 293, 341, 393, 976, 505, 411, 3202, 1071, 3202, 6175], "temperature": 0.0, "avg_logprob": -0.19839422386812877, "compression_ratio": 1.96875, "no_speech_prob": 1.16503651952371e-05}, {"id": 370, "seek": 240148, "start": 2401.48, "end": 2409.2400000000002, "text": " beta i which captures how important this context word is relevant to this question so after computing", "tokens": [9861, 741, 597, 27986, 577, 1021, 341, 4319, 1349, 307, 7340, 281, 341, 1168, 370, 934, 15866], "temperature": 0.0, "avg_logprob": -0.192276808794807, "compression_ratio": 1.8476190476190477, "no_speech_prob": 8.13676706457045e-06}, {"id": 371, "seek": 240148, "start": 2409.2400000000002, "end": 2417.88, "text": " this beta i so we can again like compute this like a weighted combination by computing by some", "tokens": [341, 9861, 741, 370, 321, 393, 797, 411, 14722, 341, 411, 257, 32807, 6562, 538, 15866, 538, 512], "temperature": 0.0, "avg_logprob": -0.192276808794807, "compression_ratio": 1.8476190476190477, "no_speech_prob": 8.13676706457045e-06}, {"id": 372, "seek": 240148, "start": 2418.44, "end": 2424.44, "text": " by summing up the beta i and also the context context vector ci and the finally you can't get", "tokens": [538, 2408, 2810, 493, 264, 9861, 741, 293, 611, 264, 4319, 4319, 8062, 6983, 293, 264, 2721, 291, 393, 380, 483], "temperature": 0.0, "avg_logprob": -0.192276808794807, "compression_ratio": 1.8476190476190477, "no_speech_prob": 8.13676706457045e-06}, {"id": 373, "seek": 240148, "start": 2424.44, "end": 2431.0, "text": " a vector bi which is also another two h-dimensional vector and the final output of this attention", "tokens": [257, 8062, 3228, 597, 307, 611, 1071, 732, 276, 12, 18759, 8062, 293, 264, 2572, 5598, 295, 341, 3202], "temperature": 0.0, "avg_logprob": -0.192276808794807, "compression_ratio": 1.8476190476190477, "no_speech_prob": 8.13676706457045e-06}, {"id": 374, "seek": 243100, "start": 2431.0, "end": 2437.4, "text": " function that's a very complicated here is also the design choice of this model so the takes a", "tokens": [2445, 300, 311, 257, 588, 6179, 510, 307, 611, 264, 1715, 3922, 295, 341, 2316, 370, 264, 2516, 257], "temperature": 0.0, "avg_logprob": -0.3035146376665901, "compression_ratio": 1.8578680203045685, "no_speech_prob": 1.4275405192165636e-05}, {"id": 375, "seek": 243100, "start": 2437.4, "end": 2444.92, "text": " context vector ci and as it takes a i from this part the context to query attention and the", "tokens": [4319, 8062, 6983, 293, 382, 309, 2516, 257, 741, 490, 341, 644, 264, 4319, 281, 14581, 3202, 293, 264], "temperature": 0.0, "avg_logprob": -0.3035146376665901, "compression_ratio": 1.8578680203045685, "no_speech_prob": 1.4275405192165636e-05}, {"id": 376, "seek": 243100, "start": 2444.92, "end": 2450.76, "text": " takes the element of multiplication between the ci and ai and also the ci and bi and the final", "tokens": [2516, 264, 4478, 295, 27290, 1296, 264, 6983, 293, 9783, 293, 611, 264, 6983, 293, 3228, 293, 264, 2572], "temperature": 0.0, "avg_logprob": -0.3035146376665901, "compression_ratio": 1.8578680203045685, "no_speech_prob": 1.4275405192165636e-05}, {"id": 377, "seek": 243100, "start": 2450.76, "end": 2456.04, "text": " is to take the contactination and the can give you a produce of h-dimensional vector", "tokens": [307, 281, 747, 264, 3385, 2486, 293, 264, 393, 976, 291, 257, 5258, 295, 276, 12, 18759, 8062], "temperature": 0.0, "avg_logprob": -0.3035146376665901, "compression_ratio": 1.8578680203045685, "no_speech_prob": 1.4275405192165636e-05}, {"id": 378, "seek": 245604, "start": 2456.04, "end": 2461.72, "text": " okay maybe i want to pause a little bit and check if there are any questions because this part", "tokens": [1392, 1310, 741, 528, 281, 10465, 257, 707, 857, 293, 1520, 498, 456, 366, 604, 1651, 570, 341, 644], "temperature": 0.0, "avg_logprob": -0.27529677425522403, "compression_ratio": 1.875, "no_speech_prob": 4.3968459067400545e-05}, {"id": 379, "seek": 245604, "start": 2461.72, "end": 2470.12, "text": " is a little bit complicated yeah one one question is why is query to context and context to query", "tokens": [307, 257, 707, 857, 6179, 1338, 472, 472, 1168, 307, 983, 307, 14581, 281, 4319, 293, 4319, 281, 14581], "temperature": 0.0, "avg_logprob": -0.27529677425522403, "compression_ratio": 1.875, "no_speech_prob": 4.3968459067400545e-05}, {"id": 380, "seek": 245604, "start": 2470.12, "end": 2480.12, "text": " attention not symmetrical um um that's a good question yes so here because essentially the", "tokens": [3202, 406, 40360, 1105, 1105, 300, 311, 257, 665, 1168, 2086, 370, 510, 570, 4476, 264], "temperature": 0.0, "avg_logprob": -0.27529677425522403, "compression_ratio": 1.875, "no_speech_prob": 4.3968459067400545e-05}, {"id": 381, "seek": 245604, "start": 2480.12, "end": 2485.48, "text": " goal is trying to because the goal is final goal you're trying to final span in the passage", "tokens": [3387, 307, 1382, 281, 570, 264, 3387, 307, 2572, 3387, 291, 434, 1382, 281, 2572, 16174, 294, 264, 11497], "temperature": 0.0, "avg_logprob": -0.27529677425522403, "compression_ratio": 1.875, "no_speech_prob": 4.3968459067400545e-05}, {"id": 382, "seek": 248548, "start": 2485.48, "end": 2491.64, "text": " so the the whole the point of the this attention function is trying to produce a rotation", "tokens": [370, 264, 264, 1379, 264, 935, 295, 264, 341, 3202, 2445, 307, 1382, 281, 5258, 257, 12447], "temperature": 0.0, "avg_logprob": -0.2626581889827077, "compression_ratio": 1.9090909090909092, "no_speech_prob": 1.6689480617060326e-05}, {"id": 383, "seek": 248548, "start": 2491.64, "end": 2499.16, "text": " for each single context word in this context so that's um so so we are not trying to generate", "tokens": [337, 1184, 2167, 4319, 1349, 294, 341, 4319, 370, 300, 311, 1105, 370, 370, 321, 366, 406, 1382, 281, 8460], "temperature": 0.0, "avg_logprob": -0.2626581889827077, "compression_ratio": 1.9090909090909092, "no_speech_prob": 1.6689480617060326e-05}, {"id": 384, "seek": 248548, "start": 2499.16, "end": 2505.2400000000002, "text": " questions rotations here it's going to try to generate the um contact rotations so one so the", "tokens": [1651, 44796, 510, 309, 311, 516, 281, 853, 281, 8460, 264, 1105, 3385, 44796, 370, 472, 370, 264], "temperature": 0.0, "avg_logprob": -0.2626581889827077, "compression_ratio": 1.9090909090909092, "no_speech_prob": 1.6689480617060326e-05}, {"id": 385, "seek": 248548, "start": 2505.2400000000002, "end": 2511.08, "text": " difference between these two like first try to see which questions are relevant to this context work", "tokens": [2649, 1296, 613, 732, 411, 700, 853, 281, 536, 597, 1651, 366, 7340, 281, 341, 4319, 589], "temperature": 0.0, "avg_logprob": -0.2626581889827077, "compression_ratio": 1.9090909090909092, "no_speech_prob": 1.6689480617060326e-05}, {"id": 386, "seek": 251108, "start": 2511.08, "end": 2515.72, "text": " another part is trying to figure out which contact work can be relevant and which contact work can", "tokens": [1071, 644, 307, 1382, 281, 2573, 484, 597, 3385, 589, 393, 312, 7340, 293, 597, 3385, 589, 393], "temperature": 0.0, "avg_logprob": -0.18815468097555227, "compression_ratio": 1.8325581395348838, "no_speech_prob": 8.263929885288235e-06}, {"id": 387, "seek": 251108, "start": 2515.72, "end": 2522.52, "text": " be not relevant i hope it sounds just answers your question yeah here's an easier question sort of", "tokens": [312, 406, 7340, 741, 1454, 309, 3263, 445, 6338, 428, 1168, 1338, 510, 311, 364, 3571, 1168, 1333, 295], "temperature": 0.0, "avg_logprob": -0.18815468097555227, "compression_ratio": 1.8325581395348838, "no_speech_prob": 8.263929885288235e-06}, {"id": 388, "seek": 251108, "start": 2522.52, "end": 2529.16, "text": " on the same topic which might help is there a reason why you use both query to context and context", "tokens": [322, 264, 912, 4829, 597, 1062, 854, 307, 456, 257, 1778, 983, 291, 764, 1293, 14581, 281, 4319, 293, 4319], "temperature": 0.0, "avg_logprob": -0.18815468097555227, "compression_ratio": 1.8325581395348838, "no_speech_prob": 8.263929885288235e-06}, {"id": 389, "seek": 251108, "start": 2529.16, "end": 2537.64, "text": " to query attention is it sometimes advantageous or okay to use just one that's a good question um", "tokens": [281, 14581, 3202, 307, 309, 2171, 5002, 563, 420, 1392, 281, 764, 445, 472, 300, 311, 257, 665, 1168, 1105], "temperature": 0.0, "avg_logprob": -0.18815468097555227, "compression_ratio": 1.8325581395348838, "no_speech_prob": 8.263929885288235e-06}, {"id": 390, "seek": 253764, "start": 2537.64, "end": 2543.24, "text": " um the reason is yeah so the i'm going to show some relations already from this figure so", "tokens": [1105, 264, 1778, 307, 1338, 370, 264, 741, 478, 516, 281, 855, 512, 2299, 1217, 490, 341, 2573, 370], "temperature": 0.0, "avg_logprob": -0.314644109635126, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.9828574295388535e-05}, {"id": 391, "seek": 253764, "start": 2543.24, "end": 2549.16, "text": " they basically just find both both directions can really help um by drawing the context for", "tokens": [436, 1936, 445, 915, 1293, 1293, 11095, 393, 534, 854, 1105, 538, 6316, 264, 4319, 337], "temperature": 0.0, "avg_logprob": -0.314644109635126, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.9828574295388535e-05}, {"id": 392, "seek": 253764, "start": 2549.16, "end": 2554.6, "text": " and query to context so there'll be some relations studies so by using one strategy useful", "tokens": [293, 14581, 281, 4319, 370, 456, 603, 312, 512, 2299, 5313, 370, 538, 1228, 472, 5206, 4420], "temperature": 0.0, "avg_logprob": -0.314644109635126, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.9828574295388535e-05}, {"id": 393, "seek": 253764, "start": 2554.6, "end": 2566.04, "text": " but then just not a bit as using the both directions yeah um right let's see uh in the bottom right", "tokens": [457, 550, 445, 406, 257, 857, 382, 1228, 264, 1293, 11095, 1338, 1105, 558, 718, 311, 536, 2232, 294, 264, 2767, 558], "temperature": 0.0, "avg_logprob": -0.314644109635126, "compression_ratio": 1.8235294117647058, "no_speech_prob": 1.9828574295388535e-05}, {"id": 394, "seek": 256604, "start": 2566.04, "end": 2575.48, "text": " we sum over i so far does the i remain in bi is that correct or so typo there uh this is not a typo", "tokens": [321, 2408, 670, 741, 370, 1400, 775, 264, 741, 6222, 294, 3228, 307, 300, 3006, 420, 370, 2125, 78, 456, 2232, 341, 307, 406, 257, 2125, 78], "temperature": 0.0, "avg_logprob": -0.33457469940185547, "compression_ratio": 1.7, "no_speech_prob": 3.9414553612004966e-05}, {"id": 395, "seek": 256604, "start": 2575.48, "end": 2582.36, "text": " so again sorry so the output yeah you know it's a bit confusing so the output of this model this", "tokens": [370, 797, 2597, 370, 264, 5598, 1338, 291, 458, 309, 311, 257, 857, 13181, 370, 264, 5598, 295, 341, 2316, 341], "temperature": 0.0, "avg_logprob": -0.33457469940185547, "compression_ratio": 1.7, "no_speech_prob": 3.9414553612004966e-05}, {"id": 396, "seek": 256604, "start": 2582.36, "end": 2590.04, "text": " come for module it to get a rotation for each context work at the end so both the output for", "tokens": [808, 337, 10088, 309, 281, 483, 257, 12447, 337, 1184, 4319, 589, 412, 264, 917, 370, 1293, 264, 5598, 337], "temperature": 0.0, "avg_logprob": -0.33457469940185547, "compression_ratio": 1.7, "no_speech_prob": 3.9414553612004966e-05}, {"id": 397, "seek": 259004, "start": 2590.04, "end": 2597.48, "text": " AI and bi i is actually um in numerates from like um actually in numerates uh you know if first over", "tokens": [7318, 293, 3228, 741, 307, 767, 1105, 294, 7866, 1024, 490, 411, 1105, 767, 294, 7866, 1024, 2232, 291, 458, 498, 700, 670], "temperature": 0.0, "avg_logprob": -0.3095606900333019, "compression_ratio": 2.0051546391752577, "no_speech_prob": 7.76139677327592e-06}, {"id": 398, "seek": 259004, "start": 2597.48, "end": 2604.68, "text": " all the context works so bi would be still um just to try to aggregate over all the questions", "tokens": [439, 264, 4319, 1985, 370, 3228, 576, 312, 920, 1105, 445, 281, 853, 281, 26118, 670, 439, 264, 1651], "temperature": 0.0, "avg_logprob": -0.3095606900333019, "compression_ratio": 2.0051546391752577, "no_speech_prob": 7.76139677327592e-06}, {"id": 399, "seek": 259004, "start": 2604.68, "end": 2611.48, "text": " uh on the over all the context works but the beta i measures the importance of this context works", "tokens": [2232, 322, 264, 670, 439, 264, 4319, 1985, 457, 264, 9861, 741, 8000, 264, 7379, 295, 341, 4319, 1985], "temperature": 0.0, "avg_logprob": -0.3095606900333019, "compression_ratio": 2.0051546391752577, "no_speech_prob": 7.76139677327592e-06}, {"id": 400, "seek": 259004, "start": 2612.04, "end": 2618.52, "text": " compared to all the context works so both AI and bi are actually disrespecting the context works", "tokens": [5347, 281, 439, 264, 4319, 1985, 370, 1293, 7318, 293, 3228, 366, 767, 27058, 278, 264, 4319, 1985], "temperature": 0.0, "avg_logprob": -0.3095606900333019, "compression_ratio": 2.0051546391752577, "no_speech_prob": 7.76139677327592e-06}, {"id": 401, "seek": 261852, "start": 2618.52, "end": 2624.6, "text": " yes so you can see that here is basically doing some kind of the animal wise multiplication so the", "tokens": [2086, 370, 291, 393, 536, 300, 510, 307, 1936, 884, 512, 733, 295, 264, 5496, 10829, 27290, 370, 264], "temperature": 0.0, "avg_logprob": -0.2610973645282048, "compression_ratio": 1.6822033898305084, "no_speech_prob": 1.1840204933832865e-05}, {"id": 402, "seek": 261852, "start": 2624.6, "end": 2630.7599999999998, "text": " output of the g i would be actually only arranged from the one to and uh in which is another the context", "tokens": [5598, 295, 264, 290, 741, 576, 312, 767, 787, 18721, 490, 264, 472, 281, 293, 2232, 294, 597, 307, 1071, 264, 4319], "temperature": 0.0, "avg_logprob": -0.2610973645282048, "compression_ratio": 1.6822033898305084, "no_speech_prob": 1.1840204933832865e-05}, {"id": 403, "seek": 261852, "start": 2630.7599999999998, "end": 2637.48, "text": " works there are lots of questions about this um what is the rationale for the expression for g i", "tokens": [1985, 456, 366, 3195, 295, 1651, 466, 341, 1105, 437, 307, 264, 41989, 337, 264, 6114, 337, 290, 741], "temperature": 0.0, "avg_logprob": -0.2610973645282048, "compression_ratio": 1.6822033898305084, "no_speech_prob": 1.1840204933832865e-05}, {"id": 404, "seek": 261852, "start": 2638.2, "end": 2645.88, "text": " how does one come up with such an expression okay i don't know i guess not also try out a lot of", "tokens": [577, 775, 472, 808, 493, 365, 1270, 364, 6114, 1392, 741, 500, 380, 458, 741, 2041, 406, 611, 853, 484, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.2610973645282048, "compression_ratio": 1.6822033898305084, "no_speech_prob": 1.1840204933832865e-05}, {"id": 405, "seek": 264588, "start": 2645.88, "end": 2652.12, "text": " things uh so okay so keep on here trying to understand okay so the roles of the context require", "tokens": [721, 2232, 370, 1392, 370, 1066, 322, 510, 1382, 281, 1223, 1392, 370, 264, 9604, 295, 264, 4319, 3651], "temperature": 0.0, "avg_logprob": -0.455392676663686, "compression_ratio": 1.7285714285714286, "no_speech_prob": 4.35549145549885e-06}, {"id": 406, "seek": 264588, "start": 2652.12, "end": 2656.6, "text": " attention or context or tension so i bet there's actually been many different", "tokens": [3202, 420, 4319, 420, 8980, 370, 741, 778, 456, 311, 767, 668, 867, 819], "temperature": 0.0, "avg_logprob": -0.455392676663686, "compression_ratio": 1.7285714285714286, "no_speech_prob": 4.35549145549885e-06}, {"id": 407, "seek": 264588, "start": 2656.6, "end": 2661.96, "text": " formations to do this i also think there also have child many different variants but uh just", "tokens": [39652, 281, 360, 341, 741, 611, 519, 456, 611, 362, 1440, 867, 819, 21669, 457, 2232, 445], "temperature": 0.0, "avg_logprob": -0.455392676663686, "compression_ratio": 1.7285714285714286, "no_speech_prob": 4.35549145549885e-06}, {"id": 408, "seek": 264588, "start": 2661.96, "end": 2669.48, "text": " what they kind of can up as and i think that if after week um it's gonna be smart the way string", "tokens": [437, 436, 733, 295, 393, 493, 382, 293, 741, 519, 300, 498, 934, 1243, 1105, 309, 311, 799, 312, 4069, 264, 636, 6798], "temperature": 0.0, "avg_logprob": -0.455392676663686, "compression_ratio": 1.7285714285714286, "no_speech_prob": 4.35549145549885e-06}, {"id": 409, "seek": 266948, "start": 2669.48, "end": 2679.0, "text": " copper is a both attention but it doesn't have to be written this way yeah i mean one other question", "tokens": [15007, 307, 257, 1293, 3202, 457, 309, 1177, 380, 362, 281, 312, 3720, 341, 636, 1338, 741, 914, 472, 661, 1168], "temperature": 0.0, "avg_logprob": -0.23116733133792877, "compression_ratio": 1.701219512195122, "no_speech_prob": 3.904853201674996e-06}, {"id": 410, "seek": 266948, "start": 2679.0, "end": 2686.92, "text": " would be in the query the context attention why do you do a max inside the soft max", "tokens": [576, 312, 294, 264, 14581, 264, 4319, 3202, 983, 360, 291, 360, 257, 11469, 1854, 264, 2787, 11469], "temperature": 0.0, "avg_logprob": -0.23116733133792877, "compression_ratio": 1.701219512195122, "no_speech_prob": 3.904853201674996e-06}, {"id": 411, "seek": 266948, "start": 2688.2, "end": 2694.52, "text": " yeah oh yeah sorry i should have expense more clearly so here again query to contest attention", "tokens": [1338, 1954, 1338, 2597, 741, 820, 362, 18406, 544, 4448, 370, 510, 797, 14581, 281, 10287, 3202], "temperature": 0.0, "avg_logprob": -0.23116733133792877, "compression_ratio": 1.701219512195122, "no_speech_prob": 3.904853201674996e-06}, {"id": 412, "seek": 269452, "start": 2694.52, "end": 2701.16, "text": " to try to measure whether this the importance of this context works with respect to some", "tokens": [281, 853, 281, 3481, 1968, 341, 264, 7379, 295, 341, 4319, 1985, 365, 3104, 281, 512], "temperature": 0.0, "avg_logprob": -0.28473758697509766, "compression_ratio": 1.75, "no_speech_prob": 8.010981218831148e-06}, {"id": 413, "seek": 269452, "start": 2701.16, "end": 2709.24, "text": " some answer or question words so if the so the so by taking the max for each row in this ace", "tokens": [512, 1867, 420, 1168, 2283, 370, 498, 264, 370, 264, 370, 538, 1940, 264, 11469, 337, 1184, 5386, 294, 341, 17117], "temperature": 0.0, "avg_logprob": -0.28473758697509766, "compression_ratio": 1.75, "no_speech_prob": 8.010981218831148e-06}, {"id": 414, "seek": 269452, "start": 2709.24, "end": 2715.4, "text": " matrix so it's basically trying to see okay which question word um is actually most relevant", "tokens": [8141, 370, 309, 311, 1936, 1382, 281, 536, 1392, 597, 1168, 1349, 1105, 307, 767, 881, 7340], "temperature": 0.0, "avg_logprob": -0.28473758697509766, "compression_ratio": 1.75, "no_speech_prob": 8.010981218831148e-06}, {"id": 415, "seek": 269452, "start": 2715.4, "end": 2721.4, "text": " to this context word if this number is still very low that means there isn't any question", "tokens": [281, 341, 4319, 1349, 498, 341, 1230, 307, 920, 588, 2295, 300, 1355, 456, 1943, 380, 604, 1168], "temperature": 0.0, "avg_logprob": -0.28473758697509766, "compression_ratio": 1.75, "no_speech_prob": 8.010981218831148e-06}, {"id": 416, "seek": 272140, "start": 2721.4, "end": 2727.2400000000002, "text": " words that could be online with this context word so that we would just by taking after taking", "tokens": [2283, 300, 727, 312, 2950, 365, 341, 4319, 1349, 370, 300, 321, 576, 445, 538, 1940, 934, 1940], "temperature": 0.0, "avg_logprob": -0.32374539218106113, "compression_ratio": 1.78743961352657, "no_speech_prob": 4.784528300660895e-06}, {"id": 417, "seek": 272140, "start": 2727.2400000000002, "end": 2732.12, "text": " the max if this number is still very low that means this query context word is not very relevant", "tokens": [264, 11469, 498, 341, 1230, 307, 920, 588, 2295, 300, 1355, 341, 14581, 4319, 1349, 307, 406, 588, 7340], "temperature": 0.0, "avg_logprob": -0.32374539218106113, "compression_ratio": 1.78743961352657, "no_speech_prob": 4.784528300660895e-06}, {"id": 418, "seek": 272140, "start": 2732.12, "end": 2737.56, "text": " so so basically just as well we take the soft max uh try to soft max i won't talk to the next", "tokens": [370, 370, 1936, 445, 382, 731, 321, 747, 264, 2787, 11469, 2232, 853, 281, 2787, 11469, 741, 1582, 380, 751, 281, 264, 958], "temperature": 0.0, "avg_logprob": -0.32374539218106113, "compression_ratio": 1.78743961352657, "no_speech_prob": 4.784528300660895e-06}, {"id": 419, "seek": 272140, "start": 2741.8, "end": 2748.04, "text": " uh i know do you want even more do you want to go on uh i probably should know all i", "tokens": [2232, 741, 458, 360, 291, 528, 754, 544, 360, 291, 528, 281, 352, 322, 2232, 741, 1391, 820, 458, 439, 741], "temperature": 0.0, "avg_logprob": -0.32374539218106113, "compression_ratio": 1.78743961352657, "no_speech_prob": 4.784528300660895e-06}, {"id": 420, "seek": 274804, "start": 2748.04, "end": 2753.4, "text": " do have a lot of slides but i'm happy on the questions after the answer yeah maybe you should go on", "tokens": [360, 362, 257, 688, 295, 9788, 457, 741, 478, 2055, 322, 264, 1651, 934, 264, 1867, 1338, 1310, 291, 820, 352, 322], "temperature": 0.0, "avg_logprob": -0.46167149013943143, "compression_ratio": 1.7777777777777777, "no_speech_prob": 8.529044862370938e-06}, {"id": 421, "seek": 274804, "start": 2754.44, "end": 2762.84, "text": " yeah okay so the last part of this model is actually um the idea is the most simple so it's", "tokens": [1338, 1392, 370, 264, 1036, 644, 295, 341, 2316, 307, 767, 1105, 264, 1558, 307, 264, 881, 2199, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.46167149013943143, "compression_ratio": 1.7777777777777777, "no_speech_prob": 8.529044862370938e-06}, {"id": 422, "seek": 274804, "start": 2762.84, "end": 2768.04, "text": " two of the last last three there are two layers smaller layer and all four layers so for the", "tokens": [732, 295, 264, 1036, 1036, 1045, 456, 366, 732, 7914, 4356, 4583, 293, 439, 1451, 7914, 370, 337, 264], "temperature": 0.0, "avg_logprob": -0.46167149013943143, "compression_ratio": 1.7777777777777777, "no_speech_prob": 8.529044862370938e-06}, {"id": 423, "seek": 274804, "start": 2768.04, "end": 2775.48, "text": " smaller layer so again for absolute attention layer they take the key derivation um of so basically", "tokens": [4356, 4583, 370, 797, 337, 8236, 3202, 4583, 436, 747, 264, 2141, 10151, 399, 1105, 295, 370, 1936], "temperature": 0.0, "avg_logprob": -0.46167149013943143, "compression_ratio": 1.7777777777777777, "no_speech_prob": 8.529044862370938e-06}, {"id": 424, "seek": 277548, "start": 2775.48, "end": 2782.76, "text": " g i which captures the attention between all contacts and the query and then basically just", "tokens": [290, 741, 597, 27986, 264, 3202, 1296, 439, 15836, 293, 264, 14581, 293, 550, 1936, 445], "temperature": 0.0, "avg_logprob": -0.31015232631138395, "compression_ratio": 2.2572815533980584, "no_speech_prob": 3.1872516501607606e-06}, {"id": 425, "seek": 277548, "start": 2782.76, "end": 2788.68, "text": " passes g i to another two layers of bi-directional errors gems and the many reasons they do this is", "tokens": [11335, 290, 741, 281, 1071, 732, 7914, 295, 3228, 12, 18267, 41048, 13603, 29296, 293, 264, 867, 4112, 436, 360, 341, 307], "temperature": 0.0, "avg_logprob": -0.31015232631138395, "compression_ratio": 2.2572815533980584, "no_speech_prob": 3.1872516501607606e-06}, {"id": 426, "seek": 277548, "start": 2789.2400000000002, "end": 2793.08, "text": " the attention layer is basically modeling the interactions between the query and context", "tokens": [264, 3202, 4583, 307, 1936, 15983, 264, 13280, 1296, 264, 14581, 293, 4319], "temperature": 0.0, "avg_logprob": -0.31015232631138395, "compression_ratio": 2.2572815533980584, "no_speech_prob": 3.1872516501607606e-06}, {"id": 427, "seek": 277548, "start": 2793.96, "end": 2798.6, "text": " and the bi-passing this to another two layers of bi-directional errors gems the modeling layers", "tokens": [293, 264, 3228, 12, 9216, 278, 341, 281, 1071, 732, 7914, 295, 3228, 12, 18267, 41048, 13603, 29296, 264, 15983, 7914], "temperature": 0.0, "avg_logprob": -0.31015232631138395, "compression_ratio": 2.2572815533980584, "no_speech_prob": 3.1872516501607606e-06}, {"id": 428, "seek": 277548, "start": 2798.6, "end": 2803.88, "text": " is basically modeling they can also first model the interactions using the context words", "tokens": [307, 1936, 15983, 436, 393, 611, 700, 2316, 264, 13280, 1228, 264, 4319, 2283], "temperature": 0.0, "avg_logprob": -0.31015232631138395, "compression_ratio": 2.2572815533980584, "no_speech_prob": 3.1872516501607606e-06}, {"id": 429, "seek": 280388, "start": 2803.88, "end": 2810.12, "text": " so this is a formulation here um so these are two layers bi-directional and", "tokens": [370, 341, 307, 257, 37642, 510, 1105, 370, 613, 366, 732, 7914, 3228, 12, 18267, 41048, 293], "temperature": 0.0, "avg_logprob": -0.49108407555556877, "compression_ratio": 1.7219251336898396, "no_speech_prob": 4.934989192406647e-06}, {"id": 430, "seek": 280388, "start": 2810.12, "end": 2815.6400000000003, "text": " here by taking the g i as input and the output will be on the m i which is another two-h", "tokens": [510, 538, 1940, 264, 290, 741, 382, 4846, 293, 264, 5598, 486, 312, 322, 264, 275, 741, 597, 307, 1071, 732, 12, 71], "temperature": 0.0, "avg_logprob": -0.49108407555556877, "compression_ratio": 1.7219251336898396, "no_speech_prob": 4.934989192406647e-06}, {"id": 431, "seek": 280388, "start": 2815.6400000000003, "end": 2820.92, "text": " direction dimensional vector for each context work in the in the passage", "tokens": [3513, 18795, 8062, 337, 1184, 4319, 589, 294, 264, 294, 264, 11497], "temperature": 0.0, "avg_logprob": -0.49108407555556877, "compression_ratio": 1.7219251336898396, "no_speech_prob": 4.934989192406647e-06}, {"id": 432, "seek": 280388, "start": 2823.1600000000003, "end": 2828.28, "text": " okay so the final is all four layers so all four layers they did just two cross-fers", "tokens": [1392, 370, 264, 2572, 307, 439, 1451, 7914, 370, 439, 1451, 7914, 436, 630, 445, 732, 3278, 12, 69, 433], "temperature": 0.0, "avg_logprob": -0.49108407555556877, "compression_ratio": 1.7219251336898396, "no_speech_prob": 4.934989192406647e-06}, {"id": 433, "seek": 282828, "start": 2828.28, "end": 2835.5600000000004, "text": " just trying to predict the starting end positions so by doing this so the first contact in the g i", "tokens": [445, 1382, 281, 6069, 264, 2891, 917, 8432, 370, 538, 884, 341, 370, 264, 700, 3385, 294, 264, 290, 741], "temperature": 0.0, "avg_logprob": -0.22647809434211116, "compression_ratio": 1.761467889908257, "no_speech_prob": 7.294078841368901e-06}, {"id": 434, "seek": 282828, "start": 2835.5600000000004, "end": 2842.52, "text": " and m i so this would be actually a 10-h dimensional vector and by computing the dot project over", "tokens": [293, 275, 741, 370, 341, 576, 312, 767, 257, 1266, 12, 71, 18795, 8062, 293, 538, 15866, 264, 5893, 1716, 670], "temperature": 0.0, "avg_logprob": -0.22647809434211116, "compression_ratio": 1.761467889908257, "no_speech_prob": 7.294078841368901e-06}, {"id": 435, "seek": 282828, "start": 2842.52, "end": 2848.0400000000004, "text": " another vector called double start and this resulting vector and they can get basic data score", "tokens": [1071, 8062, 1219, 3834, 722, 293, 341, 16505, 8062, 293, 436, 393, 483, 3875, 1412, 6175], "temperature": 0.0, "avg_logprob": -0.22647809434211116, "compression_ratio": 1.761467889908257, "no_speech_prob": 7.294078841368901e-06}, {"id": 436, "seek": 282828, "start": 2848.52, "end": 2855.4, "text": " for each position in the context and then you can just up up higher softmax uh and then this", "tokens": [337, 1184, 2535, 294, 264, 4319, 293, 550, 291, 393, 445, 493, 493, 2946, 2787, 41167, 2232, 293, 550, 341], "temperature": 0.0, "avg_logprob": -0.22647809434211116, "compression_ratio": 1.761467889908257, "no_speech_prob": 7.294078841368901e-06}, {"id": 437, "seek": 285540, "start": 2855.4, "end": 2861.88, "text": " will give you a probability that okay what is the probability this position i will be actually", "tokens": [486, 976, 291, 257, 8482, 300, 1392, 437, 307, 264, 8482, 341, 2535, 741, 486, 312, 767], "temperature": 0.0, "avg_logprob": -0.293584325096824, "compression_ratio": 1.8564593301435406, "no_speech_prob": 5.95313758822158e-06}, {"id": 438, "seek": 285540, "start": 2863.0, "end": 2871.96, "text": " uh be on the start position on the final answer string and they also have another class file", "tokens": [2232, 312, 322, 264, 722, 2535, 322, 264, 2572, 1867, 6798, 293, 436, 611, 362, 1071, 1508, 3991], "temperature": 0.0, "avg_logprob": -0.293584325096824, "compression_ratio": 1.8564593301435406, "no_speech_prob": 5.95313758822158e-06}, {"id": 439, "seek": 285540, "start": 2871.96, "end": 2877.08, "text": " to predict the end position of the answer but there also be something a little bit more complicated", "tokens": [281, 6069, 264, 917, 2535, 295, 264, 1867, 457, 456, 611, 312, 746, 257, 707, 857, 544, 6179], "temperature": 0.0, "avg_logprob": -0.293584325096824, "compression_ratio": 1.8564593301435406, "no_speech_prob": 5.95313758822158e-06}, {"id": 440, "seek": 285540, "start": 2877.08, "end": 2882.76, "text": " so they actually passed the m i to another bi-directional error is tm here so they call the m fine i", "tokens": [370, 436, 767, 4678, 264, 275, 741, 281, 1071, 3228, 12, 18267, 41048, 6713, 307, 256, 76, 510, 370, 436, 818, 264, 275, 2489, 741], "temperature": 0.0, "avg_logprob": -0.293584325096824, "compression_ratio": 1.8564593301435406, "no_speech_prob": 5.95313758822158e-06}, {"id": 441, "seek": 288276, "start": 2882.76, "end": 2890.36, "text": " and they come cat in g i and m prime i oh sorry this is the title so this will be w and so the", "tokens": [293, 436, 808, 3857, 294, 290, 741, 293, 275, 5835, 741, 1954, 2597, 341, 307, 264, 4876, 370, 341, 486, 312, 261, 293, 370, 264], "temperature": 0.0, "avg_logprob": -0.30460825468364516, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.2792811503459234e-05}, {"id": 442, "seek": 288276, "start": 2890.36, "end": 2897.6400000000003, "text": " computer dot project between w and and this vector and this can be reproduced on all the probability", "tokens": [3820, 5893, 1716, 1296, 261, 293, 293, 341, 8062, 293, 341, 393, 312, 11408, 1232, 322, 439, 264, 8482], "temperature": 0.0, "avg_logprob": -0.30460825468364516, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.2792811503459234e-05}, {"id": 443, "seek": 288276, "start": 2898.36, "end": 2904.84, "text": " probability over all the positions which predicts the um how likely this position will be the", "tokens": [8482, 670, 439, 264, 8432, 597, 6069, 82, 264, 1105, 577, 3700, 341, 2535, 486, 312, 264], "temperature": 0.0, "avg_logprob": -0.30460825468364516, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.2792811503459234e-05}, {"id": 444, "seek": 288276, "start": 2904.84, "end": 2911.0, "text": " end position of the answer so by doing it by crossing the m i to another bi-directional is tm the", "tokens": [917, 2535, 295, 264, 1867, 370, 538, 884, 309, 538, 14712, 264, 275, 741, 281, 1071, 3228, 12, 18267, 41048, 307, 256, 76, 264], "temperature": 0.0, "avg_logprob": -0.30460825468364516, "compression_ratio": 1.8254716981132075, "no_speech_prob": 1.2792811503459234e-05}, {"id": 445, "seek": 291100, "start": 2911.0, "end": 2916.44, "text": " reason is that they're trying to capture some kind of dependence between the choice of the start and", "tokens": [1778, 307, 300, 436, 434, 1382, 281, 7983, 512, 733, 295, 31704, 1296, 264, 3922, 295, 264, 722, 293], "temperature": 0.0, "avg_logprob": -0.2937836474682911, "compression_ratio": 1.839378238341969, "no_speech_prob": 1.2204654012748506e-05}, {"id": 446, "seek": 291100, "start": 2916.44, "end": 2922.92, "text": " end so you can imagine that start and shouldn't be too separate so shouldn't be a cool", "tokens": [917, 370, 291, 393, 3811, 300, 722, 293, 4659, 380, 312, 886, 4994, 370, 4659, 380, 312, 257, 1627], "temperature": 0.0, "avg_logprob": -0.2937836474682911, "compression_ratio": 1.839378238341969, "no_speech_prob": 1.2204654012748506e-05}, {"id": 447, "seek": 291100, "start": 2922.92, "end": 2929.0, "text": " business independent predict it but if they claim that if you add some kind of dependence", "tokens": [1606, 6695, 6069, 309, 457, 498, 436, 3932, 300, 498, 291, 909, 512, 733, 295, 31704], "temperature": 0.0, "avg_logprob": -0.2937836474682911, "compression_ratio": 1.839378238341969, "no_speech_prob": 1.2204654012748506e-05}, {"id": 448, "seek": 291100, "start": 2929.0, "end": 2934.12, "text": " between the m i and um the p-start and p-end this can actually perform better", "tokens": [1296, 264, 275, 741, 293, 1105, 264, 280, 12, 24419, 293, 280, 12, 521, 341, 393, 767, 2042, 1101], "temperature": 0.0, "avg_logprob": -0.2937836474682911, "compression_ratio": 1.839378238341969, "no_speech_prob": 1.2204654012748506e-05}, {"id": 449, "seek": 293412, "start": 2934.12, "end": 2943.0, "text": " okay and don't visit this part on describing the bi-directional model any quick questions i can ask", "tokens": [1392, 293, 500, 380, 3441, 341, 644, 322, 16141, 264, 3228, 12, 18267, 41048, 2316, 604, 1702, 1651, 741, 393, 1029], "temperature": 0.0, "avg_logprob": -0.43286396906926083, "compression_ratio": 1.6277777777777778, "no_speech_prob": 1.3208826203481294e-05}, {"id": 450, "seek": 293412, "start": 2943.0, "end": 2952.52, "text": " this i think you can actually go on okay okay sorry i forgot to mention this is the okay the final", "tokens": [341, 741, 519, 291, 393, 767, 352, 322, 1392, 1392, 2597, 741, 5298, 281, 2152, 341, 307, 264, 1392, 264, 2572], "temperature": 0.0, "avg_logprob": -0.43286396906926083, "compression_ratio": 1.6277777777777778, "no_speech_prob": 1.3208826203481294e-05}, {"id": 451, "seek": 293412, "start": 2952.52, "end": 2958.6, "text": " training loss will be just by taking these two probability distributions and this is basically", "tokens": [3097, 4470, 486, 312, 445, 538, 1940, 613, 732, 8482, 37870, 293, 341, 307, 1936], "temperature": 0.0, "avg_logprob": -0.43286396906926083, "compression_ratio": 1.6277777777777778, "no_speech_prob": 1.3208826203481294e-05}, {"id": 452, "seek": 295860, "start": 2958.6, "end": 2965.72, "text": " just next next lot like the code of the gold as a gold answer does the protocol start position", "tokens": [445, 958, 958, 688, 411, 264, 3089, 295, 264, 3821, 382, 257, 3821, 1867, 775, 264, 10336, 722, 2535], "temperature": 0.0, "avg_logprob": -0.3459032376607259, "compression_ratio": 1.8802083333333333, "no_speech_prob": 1.5187222743406892e-05}, {"id": 453, "seek": 295860, "start": 2965.72, "end": 2973.0, "text": " of the gold answer and end position of the answer and by just um basically taking the", "tokens": [295, 264, 3821, 1867, 293, 917, 2535, 295, 264, 1867, 293, 538, 445, 1105, 1936, 1940, 264], "temperature": 0.0, "avg_logprob": -0.3459032376607259, "compression_ratio": 1.8802083333333333, "no_speech_prob": 1.5187222743406892e-05}, {"id": 454, "seek": 295860, "start": 2973.64, "end": 2979.4, "text": " product of the these two probabilities but you're planning a pilot lock so it's a", "tokens": [1674, 295, 264, 613, 732, 33783, 457, 291, 434, 5038, 257, 9691, 4017, 370, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.3459032376607259, "compression_ratio": 1.8802083333333333, "no_speech_prob": 1.5187222743406892e-05}, {"id": 455, "seek": 295860, "start": 2979.4, "end": 2985.7999999999997, "text": " sum of the two next log terms will be the final training loss and the whole the whole model can be", "tokens": [2408, 295, 264, 732, 958, 3565, 2115, 486, 312, 264, 2572, 3097, 4470, 293, 264, 1379, 264, 1379, 2316, 393, 312], "temperature": 0.0, "avg_logprob": -0.3459032376607259, "compression_ratio": 1.8802083333333333, "no_speech_prob": 1.5187222743406892e-05}, {"id": 456, "seek": 298580, "start": 2985.8, "end": 2991.1600000000003, "text": " just changing the end to end away from the encoding layer to a tension layer to modern layer and to", "tokens": [445, 4473, 264, 917, 281, 917, 1314, 490, 264, 43430, 4583, 281, 257, 8980, 4583, 281, 4363, 4583, 293, 281], "temperature": 0.0, "avg_logprob": -0.3619407165882199, "compression_ratio": 1.7836538461538463, "no_speech_prob": 2.1422694771899842e-05}, {"id": 457, "seek": 298580, "start": 2991.1600000000003, "end": 2997.32, "text": " outlayer so this will be just to accomplish the whole the whole model of the bi-directional model", "tokens": [484, 8376, 260, 370, 341, 486, 312, 445, 281, 9021, 264, 1379, 264, 1379, 2316, 295, 264, 3228, 12, 18267, 41048, 2316], "temperature": 0.0, "avg_logprob": -0.3619407165882199, "compression_ratio": 1.7836538461538463, "no_speech_prob": 2.1422694771899842e-05}, {"id": 458, "seek": 298580, "start": 2998.6800000000003, "end": 3006.2000000000003, "text": " okay so this model is actually achieved like on the data set it achieved a 77 point", "tokens": [1392, 370, 341, 2316, 307, 767, 11042, 411, 322, 264, 1412, 992, 309, 11042, 257, 25546, 935], "temperature": 0.0, "avg_logprob": -0.3619407165882199, "compression_ratio": 1.7836538461538463, "no_speech_prob": 2.1422694771899842e-05}, {"id": 459, "seek": 298580, "start": 3006.2000000000003, "end": 3011.5600000000004, "text": " history f1 school so as i mentioned earlier so just on some operations started they found", "tokens": [2503, 283, 16, 1395, 370, 382, 741, 2835, 3071, 370, 445, 322, 512, 7705, 1409, 436, 1352], "temperature": 0.0, "avg_logprob": -0.3619407165882199, "compression_ratio": 1.7836538461538463, "no_speech_prob": 2.1422694771899842e-05}, {"id": 460, "seek": 301156, "start": 3011.56, "end": 3017.96, "text": " the both of tension in two directions are actually important if we remove the one direction the", "tokens": [264, 1293, 295, 8980, 294, 732, 11095, 366, 767, 1021, 498, 321, 4159, 264, 472, 3513, 264], "temperature": 0.0, "avg_logprob": -0.3148417245774042, "compression_ratio": 2.0483870967741935, "no_speech_prob": 3.529959940351546e-05}, {"id": 461, "seek": 301156, "start": 3017.96, "end": 3023.32, "text": " performance will actually drop a bit if we remove the contrast to error tension the performance", "tokens": [3389, 486, 767, 3270, 257, 857, 498, 321, 4159, 264, 8712, 281, 6713, 8980, 264, 3389], "temperature": 0.0, "avg_logprob": -0.3148417245774042, "compression_ratio": 2.0483870967741935, "no_speech_prob": 3.529959940351546e-05}, {"id": 462, "seek": 301156, "start": 3023.32, "end": 3030.12, "text": " will drop to 67 point seven f1 school and if we remove this part it will drop to four point f1", "tokens": [486, 3270, 281, 23879, 935, 3407, 283, 16, 1395, 293, 498, 321, 4159, 341, 644, 309, 486, 3270, 281, 1451, 935, 283, 16], "temperature": 0.0, "avg_logprob": -0.3148417245774042, "compression_ratio": 2.0483870967741935, "no_speech_prob": 3.529959940351546e-05}, {"id": 463, "seek": 301156, "start": 3030.12, "end": 3035.4, "text": " four and then also the character embedding styles help so if we remove the character embedding", "tokens": [1451, 293, 550, 611, 264, 2517, 12240, 3584, 13273, 854, 370, 498, 321, 4159, 264, 2517, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.3148417245774042, "compression_ratio": 2.0483870967741935, "no_speech_prob": 3.529959940351546e-05}, {"id": 464, "seek": 303540, "start": 3035.4, "end": 3042.76, "text": " you'll get like a 1.9 point drop and all the right of this figure you can", "tokens": [291, 603, 483, 411, 257, 502, 13, 24, 935, 3270, 293, 439, 264, 558, 295, 341, 2573, 291, 393], "temperature": 0.0, "avg_logprob": -0.3662621634347098, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.2212092769914307e-05}, {"id": 465, "seek": 303540, "start": 3043.32, "end": 3048.92, "text": " see slide you can see a very big table so it's basically all the models that account of as", "tokens": [536, 4137, 291, 393, 536, 257, 588, 955, 3199, 370, 309, 311, 1936, 439, 264, 5245, 300, 2696, 295, 382], "temperature": 0.0, "avg_logprob": -0.3662621634347098, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.2212092769914307e-05}, {"id": 466, "seek": 303540, "start": 3048.92, "end": 3055.8, "text": " that time between 2016 and 2018 so you can see that um by that we're here so you're achieved a", "tokens": [300, 565, 1296, 6549, 293, 6096, 370, 291, 393, 536, 300, 1105, 538, 300, 321, 434, 510, 370, 291, 434, 11042, 257], "temperature": 0.0, "avg_logprob": -0.3662621634347098, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.2212092769914307e-05}, {"id": 467, "seek": 303540, "start": 3055.8, "end": 3062.04, "text": " 77 point three f1 school and the basic all the models are actually you know very similar ballpark", "tokens": [25546, 935, 1045, 283, 16, 1395, 293, 264, 3875, 439, 264, 5245, 366, 767, 291, 458, 588, 2531, 2594, 31239], "temperature": 0.0, "avg_logprob": -0.3662621634347098, "compression_ratio": 1.7246376811594204, "no_speech_prob": 1.2212092769914307e-05}, {"id": 468, "seek": 306204, "start": 3062.04, "end": 3070.12, "text": " so numbers range from like the highest number here is 79.8 until like after the Elmo was introduced", "tokens": [370, 3547, 3613, 490, 411, 264, 6343, 1230, 510, 307, 32803, 13, 23, 1826, 411, 934, 264, 38722, 390, 7268], "temperature": 0.0, "avg_logprob": -0.24061316113139308, "compression_ratio": 1.790909090909091, "no_speech_prob": 1.1468486263765953e-05}, {"id": 469, "seek": 306204, "start": 3070.12, "end": 3074.84, "text": " the numbers have been actually improved quite a bit so before the Elmo basically all the numbers", "tokens": [264, 3547, 362, 668, 767, 9689, 1596, 257, 857, 370, 949, 264, 38722, 1936, 439, 264, 3547], "temperature": 0.0, "avg_logprob": -0.24061316113139308, "compression_ratio": 1.790909090909091, "no_speech_prob": 1.1468486263765953e-05}, {"id": 470, "seek": 306204, "start": 3074.84, "end": 3080.6, "text": " are actually kind of similar so each model actually improved our primers primers model by like a", "tokens": [366, 767, 733, 295, 2531, 370, 1184, 2316, 767, 9689, 527, 2886, 433, 2886, 433, 2316, 538, 411, 257], "temperature": 0.0, "avg_logprob": -0.24061316113139308, "compression_ratio": 1.790909090909091, "no_speech_prob": 1.1468486263765953e-05}, {"id": 471, "seek": 306204, "start": 3080.6, "end": 3089.48, "text": " 1.2 points and now here is our tension visualization to show that on how these like the similarities", "tokens": [502, 13, 17, 2793, 293, 586, 510, 307, 527, 8980, 25801, 281, 855, 300, 322, 577, 613, 411, 264, 24197], "temperature": 0.0, "avg_logprob": -0.24061316113139308, "compression_ratio": 1.790909090909091, "no_speech_prob": 1.1468486263765953e-05}, {"id": 472, "seek": 308948, "start": 3089.48, "end": 3094.2, "text": " for the tension actually can capture the similarity between the question words and the contrast words", "tokens": [337, 264, 8980, 767, 393, 7983, 264, 32194, 1296, 264, 1168, 2283, 293, 264, 8712, 2283], "temperature": 0.0, "avg_logprob": -0.3446717779320407, "compression_ratio": 1.8436018957345972, "no_speech_prob": 1.7224445400643162e-05}, {"id": 473, "seek": 308948, "start": 3094.76, "end": 3100.84, "text": " so here's an example of the question the word in super form 50 takes place so each show is actually", "tokens": [370, 510, 311, 364, 1365, 295, 264, 1168, 264, 1349, 294, 1687, 1254, 2625, 2516, 1081, 370, 1184, 855, 307, 767], "temperature": 0.0, "avg_logprob": -0.3446717779320407, "compression_ratio": 1.8436018957345972, "no_speech_prob": 1.7224445400643162e-05}, {"id": 474, "seek": 308948, "start": 3101.64, "end": 3107.56, "text": " a question word here and each column is matrix based in the case the attention score the", "tokens": [257, 1168, 1349, 510, 293, 1184, 7738, 307, 8141, 2361, 294, 264, 1389, 264, 3202, 6175, 264], "temperature": 0.0, "avg_logprob": -0.3446717779320407, "compression_ratio": 1.8436018957345972, "no_speech_prob": 1.7224445400643162e-05}, {"id": 475, "seek": 308948, "start": 3107.56, "end": 3114.2, "text": " similarity score that has been learned by this model so you can see that on the right is basically", "tokens": [32194, 6175, 300, 575, 668, 3264, 538, 341, 2316, 370, 291, 393, 536, 300, 322, 264, 558, 307, 1936], "temperature": 0.0, "avg_logprob": -0.3446717779320407, "compression_ratio": 1.8436018957345972, "no_speech_prob": 1.7224445400643162e-05}, {"id": 476, "seek": 311420, "start": 3114.2, "end": 3122.6, "text": " trying to print out or display so the the the contrast words that have the highest scores", "tokens": [1382, 281, 4482, 484, 420, 4674, 370, 264, 264, 264, 8712, 2283, 300, 362, 264, 6343, 13444], "temperature": 0.0, "avg_logprob": -0.3224702695520913, "compression_ratio": 1.88, "no_speech_prob": 8.01044916443061e-06}, {"id": 477, "seek": 311420, "start": 3123.64, "end": 3130.12, "text": " so you can see that the where it has been online very well with the at the stadium liva", "tokens": [370, 291, 393, 536, 300, 264, 689, 309, 575, 668, 2950, 588, 731, 365, 264, 412, 264, 18585, 375, 2757], "temperature": 0.0, "avg_logprob": -0.3224702695520913, "compression_ratio": 1.88, "no_speech_prob": 8.01044916443061e-06}, {"id": 478, "seek": 311420, "start": 3130.12, "end": 3135.48, "text": " and also the super bowl 50 is basically line very well with the super bowl 50 so this basically", "tokens": [293, 611, 264, 1687, 6571, 2625, 307, 1936, 1622, 588, 731, 365, 264, 1687, 6571, 2625, 370, 341, 1936], "temperature": 0.0, "avg_logprob": -0.3224702695520913, "compression_ratio": 1.88, "no_speech_prob": 8.01044916443061e-06}, {"id": 479, "seek": 311420, "start": 3135.48, "end": 3140.8399999999997, "text": " really tells that this kind of attention scores can actually capture the similarity scores pretty well", "tokens": [534, 5112, 300, 341, 733, 295, 3202, 13444, 393, 767, 7983, 264, 32194, 13444, 1238, 731], "temperature": 0.0, "avg_logprob": -0.3224702695520913, "compression_ratio": 1.88, "no_speech_prob": 8.01044916443061e-06}, {"id": 480, "seek": 314084, "start": 3140.84, "end": 3150.44, "text": " yeah okay so next i'm going to talk about bird now how to use the bird model to solve this problem", "tokens": [1338, 1392, 370, 958, 741, 478, 516, 281, 751, 466, 5255, 586, 577, 281, 764, 264, 5255, 2316, 281, 5039, 341, 1154], "temperature": 0.0, "avg_logprob": -0.281460708446717, "compression_ratio": 1.7252252252252251, "no_speech_prob": 6.6424499891581945e-06}, {"id": 481, "seek": 314084, "start": 3150.44, "end": 3155.4, "text": " so i know that you have learned the bird in the last lecture so i'm not going to repeat this", "tokens": [370, 741, 458, 300, 291, 362, 3264, 264, 5255, 294, 264, 1036, 7991, 370, 741, 478, 406, 516, 281, 7149, 341], "temperature": 0.0, "avg_logprob": -0.281460708446717, "compression_ratio": 1.7252252252252251, "no_speech_prob": 6.6424499891581945e-06}, {"id": 482, "seek": 314084, "start": 3155.4, "end": 3161.4, "text": " so very quick so bird is basically a deep vibrational transformer encoder pre channel on the large", "tokens": [370, 588, 1702, 370, 5255, 307, 1936, 257, 2452, 11599, 1478, 31782, 2058, 19866, 659, 2269, 322, 264, 2416], "temperature": 0.0, "avg_logprob": -0.281460708446717, "compression_ratio": 1.7252252252252251, "no_speech_prob": 6.6424499891581945e-06}, {"id": 483, "seek": 314084, "start": 3161.4, "end": 3166.76, "text": " amount of text and is a channel the two channel objectives you will invest in which modeling", "tokens": [2372, 295, 2487, 293, 307, 257, 2269, 264, 732, 2269, 15961, 291, 486, 1963, 294, 597, 15983], "temperature": 0.0, "avg_logprob": -0.281460708446717, "compression_ratio": 1.7252252252252251, "no_speech_prob": 6.6424499891581945e-06}, {"id": 484, "seek": 316676, "start": 3166.76, "end": 3173.2400000000002, "text": " and the next sentence prediction and this model has a lot of parameters so the bird base has a", "tokens": [293, 264, 958, 8174, 17630, 293, 341, 2316, 575, 257, 688, 295, 9834, 370, 264, 5255, 3096, 575, 257], "temperature": 0.0, "avg_logprob": -0.3008095844682441, "compression_ratio": 1.8177339901477831, "no_speech_prob": 9.663309356255922e-06}, {"id": 485, "seek": 316676, "start": 3173.2400000000002, "end": 3181.32, "text": " 110 million parameters and the bird logic model has 330 million parameters so okay so how we can", "tokens": [20154, 2459, 9834, 293, 264, 5255, 9952, 2316, 575, 805, 3446, 2459, 9834, 370, 1392, 370, 577, 321, 393], "temperature": 0.0, "avg_logprob": -0.3008095844682441, "compression_ratio": 1.8177339901477831, "no_speech_prob": 9.663309356255922e-06}, {"id": 486, "seek": 316676, "start": 3181.32, "end": 3187.1600000000003, "text": " actually use bird for the for reading comprehension so it's actually very easy as a very straightforward", "tokens": [767, 764, 5255, 337, 264, 337, 3760, 44991, 370, 309, 311, 767, 588, 1858, 382, 257, 588, 15325], "temperature": 0.0, "avg_logprob": -0.3008095844682441, "compression_ratio": 1.8177339901477831, "no_speech_prob": 9.663309356255922e-06}, {"id": 487, "seek": 316676, "start": 3187.1600000000003, "end": 3192.1200000000003, "text": " the idea is to take the person as a segment okay so you know so the bird", "tokens": [264, 1558, 307, 281, 747, 264, 954, 382, 257, 9469, 1392, 370, 291, 458, 370, 264, 5255], "temperature": 0.0, "avg_logprob": -0.3008095844682441, "compression_ratio": 1.8177339901477831, "no_speech_prob": 9.663309356255922e-06}, {"id": 488, "seek": 319212, "start": 3192.12, "end": 3198.44, "text": " criteria are like two segments for the next sentence prediction task so then you apply the bird", "tokens": [11101, 366, 411, 732, 19904, 337, 264, 958, 8174, 17630, 5633, 370, 550, 291, 3079, 264, 5255], "temperature": 0.0, "avg_logprob": -0.3213358158018531, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.129372958530439e-05}, {"id": 489, "seek": 319212, "start": 3198.44, "end": 3203.64, "text": " on the reading comprehension task you basically just take the question as a segment A and take", "tokens": [322, 264, 3760, 44991, 5633, 291, 1936, 445, 747, 264, 1168, 382, 257, 9469, 316, 293, 747], "temperature": 0.0, "avg_logprob": -0.3213358158018531, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.129372958530439e-05}, {"id": 490, "seek": 319212, "start": 3203.64, "end": 3209.64, "text": " the passage as a segment B and finally you to go with the trying to create two end points in segment B", "tokens": [264, 11497, 382, 257, 9469, 363, 293, 2721, 291, 281, 352, 365, 264, 1382, 281, 1884, 732, 917, 2793, 294, 9469, 363], "temperature": 0.0, "avg_logprob": -0.3213358158018531, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.129372958530439e-05}, {"id": 491, "seek": 319212, "start": 3211.4, "end": 3217.08, "text": " so here's one more concrete example so question how many parameters does bird logic have", "tokens": [370, 510, 311, 472, 544, 9859, 1365, 370, 1168, 577, 867, 9834, 775, 5255, 9952, 362], "temperature": 0.0, "avg_logprob": -0.3213358158018531, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.129372958530439e-05}, {"id": 492, "seek": 321708, "start": 3217.08, "end": 3224.04, "text": " so you can see that so the basically just takes the question here and then takes a passage here", "tokens": [370, 291, 393, 536, 300, 370, 264, 1936, 445, 2516, 264, 1168, 510, 293, 550, 2516, 257, 11497, 510], "temperature": 0.0, "avg_logprob": -0.3844145428050648, "compression_ratio": 2.095505617977528, "no_speech_prob": 8.391489245695993e-06}, {"id": 493, "seek": 321708, "start": 3224.04, "end": 3229.96, "text": " and by putting the cio is token and the acp token and by just contacting the question of the", "tokens": [293, 538, 3372, 264, 269, 1004, 307, 14862, 293, 264, 696, 79, 14862, 293, 538, 445, 41482, 264, 1168, 295, 264], "temperature": 0.0, "avg_logprob": -0.3844145428050648, "compression_ratio": 2.095505617977528, "no_speech_prob": 8.391489245695993e-06}, {"id": 494, "seek": 321708, "start": 3229.96, "end": 3234.36, "text": " passage tokens and also for the question setting you just need to pass the 8 to a segment", "tokens": [11497, 22667, 293, 611, 337, 264, 1168, 3287, 291, 445, 643, 281, 1320, 264, 1649, 281, 257, 9469], "temperature": 0.0, "avg_logprob": -0.3844145428050648, "compression_ratio": 2.095505617977528, "no_speech_prob": 8.391489245695993e-06}, {"id": 495, "seek": 321708, "start": 3234.36, "end": 3241.16, "text": " embedding and the passage you just need to put in the bird the segment B embedding and finally", "tokens": [12240, 3584, 293, 264, 11497, 291, 445, 643, 281, 829, 294, 264, 5255, 264, 9469, 363, 12240, 3584, 293, 2721], "temperature": 0.0, "avg_logprob": -0.3844145428050648, "compression_ratio": 2.095505617977528, "no_speech_prob": 8.391489245695993e-06}, {"id": 496, "seek": 324116, "start": 3241.16, "end": 3247.3999999999996, "text": " the training loss is also the same so you basically just try to maximize the probability the some of", "tokens": [264, 3097, 4470, 307, 611, 264, 912, 370, 291, 1936, 445, 853, 281, 19874, 264, 8482, 264, 512, 295], "temperature": 0.0, "avg_logprob": -0.39375482603561046, "compression_ratio": 1.7808219178082192, "no_speech_prob": 8.798154340183828e-06}, {"id": 497, "seek": 324116, "start": 3247.3999999999996, "end": 3253.48, "text": " the next lot like could also both the start and end positions but here's the way that the compute", "tokens": [264, 958, 688, 411, 727, 611, 1293, 264, 722, 293, 917, 8432, 457, 510, 311, 264, 636, 300, 264, 14722], "temperature": 0.0, "avg_logprob": -0.39375482603561046, "compression_ratio": 1.7808219178082192, "no_speech_prob": 8.798154340183828e-06}, {"id": 498, "seek": 324116, "start": 3253.48, "end": 3260.52, "text": " the start and end probability is slightly different so it actually very straightforward so it just", "tokens": [264, 722, 293, 917, 8482, 307, 4748, 819, 370, 309, 767, 588, 15325, 370, 309, 445], "temperature": 0.0, "avg_logprob": -0.39375482603561046, "compression_ratio": 1.7808219178082192, "no_speech_prob": 8.798154340183828e-06}, {"id": 499, "seek": 324116, "start": 3260.52, "end": 3268.8399999999997, "text": " passes on impolitation into bird and the bird can give you the hit on that h i that actually", "tokens": [11335, 322, 704, 401, 4614, 666, 5255, 293, 264, 5255, 393, 976, 291, 264, 2045, 322, 300, 276, 741, 300, 767], "temperature": 0.0, "avg_logprob": -0.39375482603561046, "compression_ratio": 1.7808219178082192, "no_speech_prob": 8.798154340183828e-06}, {"id": 500, "seek": 326884, "start": 3268.84, "end": 3275.96, "text": " presents the hit of that corresponding to the context word context word c i so we can just", "tokens": [13533, 264, 2045, 295, 300, 11760, 281, 264, 4319, 1349, 4319, 1349, 269, 741, 370, 321, 393, 445], "temperature": 0.0, "avg_logprob": -0.391043459431509, "compression_ratio": 1.7571428571428571, "no_speech_prob": 1.7220727386302315e-05}, {"id": 501, "seek": 326884, "start": 3277.1600000000003, "end": 3283.0, "text": " introduce another two vectors W start and W and by computing the top product and then apply the", "tokens": [5366, 1071, 732, 18875, 343, 722, 293, 343, 293, 538, 15866, 264, 1192, 1674, 293, 550, 3079, 264], "temperature": 0.0, "avg_logprob": -0.391043459431509, "compression_ratio": 1.7571428571428571, "no_speech_prob": 1.7220727386302315e-05}, {"id": 502, "seek": 326884, "start": 3283.0, "end": 3288.6800000000003, "text": " softmax then you can just give you a very similar to what we had before but here is the h i", "tokens": [2787, 41167, 550, 291, 393, 445, 976, 291, 257, 588, 2531, 281, 437, 321, 632, 949, 457, 510, 307, 264, 276, 741], "temperature": 0.0, "avg_logprob": -0.391043459431509, "compression_ratio": 1.7571428571428571, "no_speech_prob": 1.7220727386302315e-05}, {"id": 503, "seek": 326884, "start": 3288.6800000000003, "end": 3294.2000000000003, "text": " just output from the bird's encoder and then we are training on these two W to start and W", "tokens": [445, 5598, 490, 264, 5255, 311, 2058, 19866, 293, 550, 321, 366, 3097, 322, 613, 732, 343, 281, 722, 293, 343], "temperature": 0.0, "avg_logprob": -0.391043459431509, "compression_ratio": 1.7571428571428571, "no_speech_prob": 1.7220727386302315e-05}, {"id": 504, "seek": 329420, "start": 3294.2, "end": 3304.2, "text": " and for these two probability distribution P start and P and okay so for this model", "tokens": [293, 337, 613, 732, 8482, 7316, 430, 722, 293, 430, 293, 1392, 370, 337, 341, 2316], "temperature": 0.0, "avg_logprob": -0.418096923828125, "compression_ratio": 1.674641148325359, "no_speech_prob": 7.295003797480604e-06}, {"id": 505, "seek": 329420, "start": 3305.16, "end": 3310.2, "text": " so all the bird parameters that is actually very much number if you use the bird base", "tokens": [370, 439, 264, 5255, 9834, 300, 307, 767, 588, 709, 1230, 498, 291, 764, 264, 5255, 3096], "temperature": 0.0, "avg_logprob": -0.418096923828125, "compression_ratio": 1.674641148325359, "no_speech_prob": 7.295003797480604e-06}, {"id": 506, "seek": 329420, "start": 3310.2, "end": 3316.2799999999997, "text": " you will be 110 million parameters as well as a newly introduced parameters h start and h end", "tokens": [291, 486, 312, 20154, 2459, 9834, 382, 731, 382, 257, 15109, 7268, 9834, 276, 722, 293, 276, 917], "temperature": 0.0, "avg_logprob": -0.418096923828125, "compression_ratio": 1.674641148325359, "no_speech_prob": 7.295003797480604e-06}, {"id": 507, "seek": 329420, "start": 3317.08, "end": 3324.04, "text": " which is if you take the bird base so hidden side will be 7608 so it's only like 1,500", "tokens": [597, 307, 498, 291, 747, 264, 5255, 3096, 370, 7633, 1252, 486, 312, 1614, 4550, 23, 370, 309, 311, 787, 411, 502, 11, 7526], "temperature": 0.0, "avg_logprob": -0.418096923828125, "compression_ratio": 1.674641148325359, "no_speech_prob": 7.295003797480604e-06}, {"id": 508, "seek": 332404, "start": 3324.04, "end": 3329.16, "text": " new parameters there will be just to optimize together jointly for this training objective error", "tokens": [777, 9834, 456, 486, 312, 445, 281, 19719, 1214, 46557, 337, 341, 3097, 10024, 6713], "temperature": 0.0, "avg_logprob": -0.30697909990946454, "compression_ratio": 1.9743589743589745, "no_speech_prob": 9.221517757396214e-06}, {"id": 509, "seek": 332404, "start": 3330.52, "end": 3336.6, "text": " and then it actually works really really well this model so if you just take this mod bird model", "tokens": [293, 550, 309, 767, 1985, 534, 534, 731, 341, 2316, 370, 498, 291, 445, 747, 341, 1072, 5255, 2316], "temperature": 0.0, "avg_logprob": -0.30697909990946454, "compression_ratio": 1.9743589743589745, "no_speech_prob": 9.221517757396214e-06}, {"id": 510, "seek": 332404, "start": 3336.6, "end": 3341.16, "text": " and by just optimizing all the parameters together you can give you very high performance", "tokens": [293, 538, 445, 40425, 439, 264, 9834, 1214, 291, 393, 976, 291, 588, 1090, 3389], "temperature": 0.0, "avg_logprob": -0.30697909990946454, "compression_ratio": 1.9743589743589745, "no_speech_prob": 9.221517757396214e-06}, {"id": 511, "seek": 332404, "start": 3341.16, "end": 3345.8, "text": " I will show you very in a minute and even the strong even if you use the stronger", "tokens": [286, 486, 855, 291, 588, 294, 257, 3456, 293, 754, 264, 2068, 754, 498, 291, 764, 264, 7249], "temperature": 0.0, "avg_logprob": -0.30697909990946454, "compression_ratio": 1.9743589743589745, "no_speech_prob": 9.221517757396214e-06}, {"id": 512, "seek": 332404, "start": 3345.8, "end": 3351.88, "text": " approach and long models more than like the standard um the stronger models than the bird models", "tokens": [3109, 293, 938, 5245, 544, 813, 411, 264, 3832, 1105, 264, 7249, 5245, 813, 264, 5255, 5245], "temperature": 0.0, "avg_logprob": -0.30697909990946454, "compression_ratio": 1.9743589743589745, "no_speech_prob": 9.221517757396214e-06}, {"id": 513, "seek": 335188, "start": 3351.88, "end": 3356.92, "text": " they can evenly to better performance on scot and the scot that has also become a standard", "tokens": [436, 393, 17658, 281, 1101, 3389, 322, 795, 310, 293, 264, 795, 310, 300, 575, 611, 1813, 257, 3832], "temperature": 0.0, "avg_logprob": -0.3116138003288059, "compression_ratio": 1.7654320987654322, "no_speech_prob": 9.514088560536038e-06}, {"id": 514, "seek": 335188, "start": 3356.92, "end": 3361.08, "text": " data set for testing this kind of virtual models let me show you some numbers", "tokens": [1412, 992, 337, 4997, 341, 733, 295, 6374, 5245, 718, 385, 855, 291, 512, 3547], "temperature": 0.0, "avg_logprob": -0.3116138003288059, "compression_ratio": 1.7654320987654322, "no_speech_prob": 9.514088560536038e-06}, {"id": 515, "seek": 335188, "start": 3362.12, "end": 3368.92, "text": " so again human performance in 91 and by that is 77.3 and then if we just do this", "tokens": [370, 797, 1952, 3389, 294, 31064, 293, 538, 300, 307, 25546, 13, 18, 293, 550, 498, 321, 445, 360, 341], "temperature": 0.0, "avg_logprob": -0.3116138003288059, "compression_ratio": 1.7654320987654322, "no_speech_prob": 9.514088560536038e-06}, {"id": 516, "seek": 335188, "start": 3368.92, "end": 3376.12, "text": " fun shooting model so bird base can give you like 88.5 bird large can give you 99.9 so you can", "tokens": [1019, 5942, 2316, 370, 5255, 3096, 393, 976, 291, 411, 24587, 13, 20, 5255, 2416, 393, 976, 291, 11803, 13, 24, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.3116138003288059, "compression_ratio": 1.7654320987654322, "no_speech_prob": 9.514088560536038e-06}, {"id": 517, "seek": 335188, "start": 3376.12, "end": 3381.8, "text": " see that this is a huge jump from the by that model to the bird models and the final", "tokens": [536, 300, 341, 307, 257, 2603, 3012, 490, 264, 538, 300, 2316, 281, 264, 5255, 5245, 293, 264, 2572], "temperature": 0.0, "avg_logprob": -0.3116138003288059, "compression_ratio": 1.7654320987654322, "no_speech_prob": 9.514088560536038e-06}, {"id": 518, "seek": 338180, "start": 3381.8, "end": 3388.04, "text": " if you see that even the latest um pretty long with the models include the X-O-Ned", "tokens": [498, 291, 536, 300, 754, 264, 6792, 1105, 1238, 938, 365, 264, 5245, 4090, 264, 1783, 12, 46, 12, 45, 292], "temperature": 0.0, "avg_logprob": -0.4394613348919412, "compression_ratio": 1.7031963470319635, "no_speech_prob": 7.763054782117251e-06}, {"id": 519, "seek": 338180, "start": 3388.04, "end": 3393.2400000000002, "text": " or the belt or Albert so these models are either like a bigger or these model are channel bigger", "tokens": [420, 264, 10750, 420, 20812, 370, 613, 5245, 366, 2139, 411, 257, 3801, 420, 613, 2316, 366, 2269, 3801], "temperature": 0.0, "avg_logprob": -0.4394613348919412, "compression_ratio": 1.7031963470319635, "no_speech_prob": 7.763054782117251e-06}, {"id": 520, "seek": 338180, "start": 3393.2400000000002, "end": 3401.1600000000003, "text": " covers or the model size are bigger so basically these models can give you a not a like 34 point", "tokens": [10538, 420, 264, 2316, 2744, 366, 3801, 370, 1936, 613, 5245, 393, 976, 291, 257, 406, 257, 411, 12790, 935], "temperature": 0.0, "avg_logprob": -0.4394613348919412, "compression_ratio": 1.7031963470319635, "no_speech_prob": 7.763054782117251e-06}, {"id": 521, "seek": 338180, "start": 3401.1600000000003, "end": 3408.76, "text": " iF1 score compared to the bird large model so this is already way higher than estimate iF1 score", "tokens": [741, 37, 16, 6175, 5347, 281, 264, 5255, 2416, 2316, 370, 341, 307, 1217, 636, 2946, 813, 12539, 741, 37, 16, 6175], "temperature": 0.0, "avg_logprob": -0.4394613348919412, "compression_ratio": 1.7031963470319635, "no_speech_prob": 7.763054782117251e-06}, {"id": 522, "seek": 340876, "start": 3408.76, "end": 3414.1200000000003, "text": " so this just works really well any quick questions", "tokens": [370, 341, 445, 1985, 534, 731, 604, 1702, 1651], "temperature": 0.0, "avg_logprob": -0.2899998080345892, "compression_ratio": 1.5316455696202531, "no_speech_prob": 5.173139470571186e-06}, {"id": 523, "seek": 340876, "start": 3419.8, "end": 3429.1600000000003, "text": " this might be okay okay so okay so yeah i guess i've been a little bit fast for this bird models", "tokens": [341, 1062, 312, 1392, 1392, 370, 1392, 370, 1338, 741, 2041, 741, 600, 668, 257, 707, 857, 2370, 337, 341, 5255, 5245], "temperature": 0.0, "avg_logprob": -0.2899998080345892, "compression_ratio": 1.5316455696202531, "no_speech_prob": 5.173139470571186e-06}, {"id": 524, "seek": 340876, "start": 3429.1600000000003, "end": 3434.36, "text": " but next what's so i would also do a bit of the comparisons between the by-dash models and the", "tokens": [457, 958, 437, 311, 370, 741, 576, 611, 360, 257, 857, 295, 264, 33157, 1296, 264, 538, 12, 67, 1299, 5245, 293, 264], "temperature": 0.0, "avg_logprob": -0.2899998080345892, "compression_ratio": 1.5316455696202531, "no_speech_prob": 5.173139470571186e-06}, {"id": 525, "seek": 343436, "start": 3434.36, "end": 3441.2400000000002, "text": " bird models so bird model has many many more parameters so it's like it's like one 10 more", "tokens": [5255, 5245, 370, 5255, 2316, 575, 867, 867, 544, 9834, 370, 309, 311, 411, 309, 311, 411, 472, 1266, 544], "temperature": 0.0, "avg_logprob": -0.3164443373680115, "compression_ratio": 1.91, "no_speech_prob": 5.506382422026945e-06}, {"id": 526, "seek": 343436, "start": 3441.2400000000002, "end": 3447.2400000000002, "text": " million or 300 to 13 million parameters but the by-dash has only like 2.5 million parameters", "tokens": [2459, 420, 6641, 281, 3705, 2459, 9834, 457, 264, 538, 12, 67, 1299, 575, 787, 411, 568, 13, 20, 2459, 9834], "temperature": 0.0, "avg_logprob": -0.3164443373680115, "compression_ratio": 1.91, "no_speech_prob": 5.506382422026945e-06}, {"id": 527, "seek": 343436, "start": 3448.04, "end": 3453.2400000000002, "text": " and the by-dash is built on top of several by-directional-errish teams and while bird is built on top", "tokens": [293, 264, 538, 12, 67, 1299, 307, 3094, 322, 1192, 295, 2940, 538, 12, 18267, 41048, 12, 260, 81, 742, 5491, 293, 1339, 5255, 307, 3094, 322, 1192], "temperature": 0.0, "avg_logprob": -0.3164443373680115, "compression_ratio": 1.91, "no_speech_prob": 5.506382422026945e-06}, {"id": 528, "seek": 343436, "start": 3453.2400000000002, "end": 3459.48, "text": " of the transformers so transformers means that there isn't any recurrence structure architecture", "tokens": [295, 264, 4088, 433, 370, 4088, 433, 1355, 300, 456, 1943, 380, 604, 18680, 10760, 3877, 9482], "temperature": 0.0, "avg_logprob": -0.3164443373680115, "compression_ratio": 1.91, "no_speech_prob": 5.506382422026945e-06}, {"id": 529, "seek": 345948, "start": 3459.48, "end": 3465.08, "text": " so the trans-weapons are much easier to paralyze and a very different difference between the", "tokens": [370, 264, 1145, 12, 826, 48071, 366, 709, 3571, 281, 32645, 1381, 293, 257, 588, 819, 2649, 1296, 264], "temperature": 0.0, "avg_logprob": -0.44680723253187243, "compression_ratio": 1.8056872037914693, "no_speech_prob": 8.527381396561395e-06}, {"id": 530, "seek": 345948, "start": 3465.08, "end": 3470.92, "text": " bird models and the by-back models is bird model is a pretender but by-back models only built on top", "tokens": [5255, 5245, 293, 264, 538, 12, 3207, 5245, 307, 5255, 2316, 307, 257, 1162, 3216, 457, 538, 12, 3207, 5245, 787, 3094, 322, 1192], "temperature": 0.0, "avg_logprob": -0.44680723253187243, "compression_ratio": 1.8056872037914693, "no_speech_prob": 8.527381396561395e-06}, {"id": 531, "seek": 345948, "start": 3470.92, "end": 3477.0, "text": " of the glove that's which is the pretender and the other remaining of two parameters new peer", "tokens": [295, 264, 26928, 300, 311, 597, 307, 264, 1162, 3216, 293, 264, 661, 8877, 295, 732, 9834, 777, 15108], "temperature": 0.0, "avg_logprob": -0.44680723253187243, "compression_ratio": 1.8056872037914693, "no_speech_prob": 8.527381396561395e-06}, {"id": 532, "seek": 345948, "start": 3477.0, "end": 3482.68, "text": " learner found this is called a data set all the other supervision data set so here it is very", "tokens": [33347, 1352, 341, 307, 1219, 257, 1412, 992, 439, 264, 661, 32675, 1412, 992, 370, 510, 309, 307, 588], "temperature": 0.0, "avg_logprob": -0.44680723253187243, "compression_ratio": 1.8056872037914693, "no_speech_prob": 8.527381396561395e-06}, {"id": 533, "seek": 348268, "start": 3482.68, "end": 3490.52, "text": " clear that pretending is a game changer here that pretender basically can just change everything", "tokens": [1850, 300, 22106, 307, 257, 1216, 22822, 510, 300, 1162, 3216, 1936, 393, 445, 1319, 1203], "temperature": 0.0, "avg_logprob": -0.3954010009765625, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.6250898372381926e-05}, {"id": 534, "seek": 348268, "start": 3490.52, "end": 3493.7999999999997, "text": " and also give you very very large boost in terms of the performance", "tokens": [293, 611, 976, 291, 588, 588, 2416, 9194, 294, 2115, 295, 264, 3389], "temperature": 0.0, "avg_logprob": -0.3954010009765625, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.6250898372381926e-05}, {"id": 535, "seek": 348268, "start": 3495.96, "end": 3501.3999999999996, "text": " but also want to create another passion so if we don't think of this like on pretending", "tokens": [457, 611, 528, 281, 1884, 1071, 5418, 370, 498, 321, 500, 380, 519, 295, 341, 411, 322, 1162, 2029], "temperature": 0.0, "avg_logprob": -0.3954010009765625, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.6250898372381926e-05}, {"id": 536, "seek": 348268, "start": 3502.2, "end": 3506.52, "text": " this like by-demo on bird models are really fundamentally different", "tokens": [341, 411, 538, 12, 10730, 78, 322, 5255, 5245, 366, 534, 17879, 819], "temperature": 0.0, "avg_logprob": -0.3954010009765625, "compression_ratio": 1.6494845360824741, "no_speech_prob": 2.6250898372381926e-05}, {"id": 537, "seek": 350652, "start": 3506.52, "end": 3512.28, "text": " I don't think so because of the below is actually my audience so let's try to see how these two", "tokens": [286, 500, 380, 519, 370, 570, 295, 264, 2507, 307, 767, 452, 4034, 370, 718, 311, 853, 281, 536, 577, 613, 732], "temperature": 0.0, "avg_logprob": -0.31664597161925667, "compression_ratio": 2.0672645739910314, "no_speech_prob": 3.999829277745448e-05}, {"id": 538, "seek": 350652, "start": 3512.28, "end": 3518.84, "text": " models actually connected especially in terms of the model you've done so by that model essentially", "tokens": [5245, 767, 4582, 2318, 294, 2115, 295, 264, 2316, 291, 600, 1096, 370, 538, 300, 2316, 4476], "temperature": 0.0, "avg_logprob": -0.31664597161925667, "compression_ratio": 2.0672645739910314, "no_speech_prob": 3.999829277745448e-05}, {"id": 539, "seek": 350652, "start": 3518.84, "end": 3524.04, "text": " they're trying to model the interactions between the question and passage right so both of the", "tokens": [436, 434, 1382, 281, 2316, 264, 13280, 1296, 264, 1168, 293, 11497, 558, 370, 1293, 295, 264], "temperature": 0.0, "avg_logprob": -0.31664597161925667, "compression_ratio": 2.0672645739910314, "no_speech_prob": 3.999829277745448e-05}, {"id": 540, "seek": 350652, "start": 3524.04, "end": 3529.0, "text": " questions to passage and passage to question and the bird model essentially they're trying to", "tokens": [1651, 281, 11497, 293, 11497, 281, 1168, 293, 264, 5255, 2316, 4476, 436, 434, 1382, 281], "temperature": 0.0, "avg_logprob": -0.31664597161925667, "compression_ratio": 2.0672645739910314, "no_speech_prob": 3.999829277745448e-05}, {"id": 541, "seek": 352900, "start": 3529.0, "end": 3537.72, "text": " use a self-attention on top of the concatenation of the question passage so this is a transformable", "tokens": [764, 257, 2698, 12, 1591, 1251, 322, 1192, 295, 264, 1588, 7186, 399, 295, 264, 1168, 11497, 370, 341, 307, 257, 4088, 712], "temperature": 0.0, "avg_logprob": -0.25037193298339844, "compression_ratio": 2.1933701657458564, "no_speech_prob": 1.3604459127236623e-05}, {"id": 542, "seek": 352900, "start": 3537.72, "end": 3542.28, "text": " model so you should take the question the passage so these are questions in the passage and then", "tokens": [2316, 370, 291, 820, 747, 264, 1168, 264, 11497, 370, 613, 366, 1651, 294, 264, 11497, 293, 550], "temperature": 0.0, "avg_logprob": -0.25037193298339844, "compression_ratio": 2.1933701657458564, "no_speech_prob": 1.3604459127236623e-05}, {"id": 543, "seek": 352900, "start": 3542.28, "end": 3548.2, "text": " you apply many many different layers of the self-attention essentially that this self-attention is able", "tokens": [291, 3079, 867, 867, 819, 7914, 295, 264, 2698, 12, 1591, 1251, 4476, 300, 341, 2698, 12, 1591, 1251, 307, 1075], "temperature": 0.0, "avg_logprob": -0.25037193298339844, "compression_ratio": 2.1933701657458564, "no_speech_prob": 1.3604459127236623e-05}, {"id": 544, "seek": 352900, "start": 3548.2, "end": 3555.08, "text": " to capture the tension between the contests and the tension between the passage and the question", "tokens": [281, 7983, 264, 8980, 1296, 264, 660, 4409, 293, 264, 8980, 1296, 264, 11497, 293, 264, 1168], "temperature": 0.0, "avg_logprob": -0.25037193298339844, "compression_ratio": 2.1933701657458564, "no_speech_prob": 1.3604459127236623e-05}, {"id": 545, "seek": 355508, "start": 3555.08, "end": 3560.7599999999998, "text": " words and the attention from the questions to passage side and also the attention from between", "tokens": [2283, 293, 264, 3202, 490, 264, 1651, 281, 11497, 1252, 293, 611, 264, 3202, 490, 1296], "temperature": 0.0, "avg_logprob": -0.2986328752734993, "compression_ratio": 2.0552486187845305, "no_speech_prob": 2.3154136215453036e-05}, {"id": 546, "seek": 355508, "start": 3561.0, "end": 3567.16, "text": " from the question was to another question was so compared to by that by that is trying to model", "tokens": [490, 264, 1168, 390, 281, 1071, 1168, 390, 370, 5347, 281, 538, 300, 538, 300, 307, 1382, 281, 2316], "temperature": 0.0, "avg_logprob": -0.2986328752734993, "compression_ratio": 2.0552486187845305, "no_speech_prob": 2.3154136215453036e-05}, {"id": 547, "seek": 355508, "start": 3567.16, "end": 3572.92, "text": " this part but the bird model essentially can capture the tension between all these four parts", "tokens": [341, 644, 457, 264, 5255, 2316, 4476, 393, 7983, 264, 8980, 1296, 439, 613, 1451, 3166], "temperature": 0.0, "avg_logprob": -0.2986328752734993, "compression_ratio": 2.0552486187845305, "no_speech_prob": 2.3154136215453036e-05}, {"id": 548, "seek": 355508, "start": 3573.7999999999997, "end": 3579.72, "text": " and actually after by that kind of so this also before the bird can before the bird can", "tokens": [293, 767, 934, 538, 300, 733, 295, 370, 341, 611, 949, 264, 5255, 393, 949, 264, 5255, 393], "temperature": 0.0, "avg_logprob": -0.2986328752734993, "compression_ratio": 2.0552486187845305, "no_speech_prob": 2.3154136215453036e-05}, {"id": 549, "seek": 357972, "start": 3579.72, "end": 3586.52, "text": " out so people have been also showing that if we just add a self-attention layer for the passage side", "tokens": [484, 370, 561, 362, 668, 611, 4099, 300, 498, 321, 445, 909, 257, 2698, 12, 1591, 1251, 4583, 337, 264, 11497, 1252], "temperature": 0.0, "avg_logprob": -0.2672531928545163, "compression_ratio": 1.8780487804878048, "no_speech_prob": 5.009791038901312e-06}, {"id": 550, "seek": 357972, "start": 3586.52, "end": 3591.3199999999997, "text": " so basically you're trying to explicitly model this attention between the passage words and", "tokens": [370, 1936, 291, 434, 1382, 281, 20803, 2316, 341, 3202, 1296, 264, 11497, 2283, 293], "temperature": 0.0, "avg_logprob": -0.2672531928545163, "compression_ratio": 1.8780487804878048, "no_speech_prob": 5.009791038901312e-06}, {"id": 551, "seek": 357972, "start": 3591.3199999999997, "end": 3597.3199999999997, "text": " passage words to the bad act this also you put the performance so you can see that these two models", "tokens": [11497, 2283, 281, 264, 1578, 605, 341, 611, 291, 829, 264, 3389, 370, 291, 393, 536, 300, 613, 732, 5245], "temperature": 0.0, "avg_logprob": -0.2672531928545163, "compression_ratio": 1.8780487804878048, "no_speech_prob": 5.009791038901312e-06}, {"id": 552, "seek": 357972, "start": 3597.3199999999997, "end": 3603.24, "text": " essentially just trying to model the tension between the passing question also the attention", "tokens": [4476, 445, 1382, 281, 2316, 264, 8980, 1296, 264, 8437, 1168, 611, 264, 3202], "temperature": 0.0, "avg_logprob": -0.2672531928545163, "compression_ratio": 1.8780487804878048, "no_speech_prob": 5.009791038901312e-06}, {"id": 553, "seek": 360324, "start": 3603.24, "end": 3610.4399999999996, "text": " between the passage words and the passage words and this actually what exactly the bird model is doing", "tokens": [1296, 264, 11497, 2283, 293, 264, 11497, 2283, 293, 341, 767, 437, 2293, 264, 5255, 2316, 307, 884], "temperature": 0.0, "avg_logprob": -0.24479395548502605, "compression_ratio": 1.7939698492462313, "no_speech_prob": 1.341545157629298e-05}, {"id": 554, "seek": 360324, "start": 3610.4399999999996, "end": 3617.3999999999996, "text": " okay so if there's no further questions so at this point I'll talk about", "tokens": [1392, 370, 498, 456, 311, 572, 3052, 1651, 370, 412, 341, 935, 286, 603, 751, 466], "temperature": 0.0, "avg_logprob": -0.24479395548502605, "compression_ratio": 1.7939698492462313, "no_speech_prob": 1.341545157629298e-05}, {"id": 555, "seek": 360324, "start": 3618.04, "end": 3622.52, "text": " bird models can do really well on this kind of reading comprehension data sets and we just", "tokens": [5255, 5245, 393, 360, 534, 731, 322, 341, 733, 295, 3760, 44991, 1412, 6352, 293, 321, 445], "temperature": 0.0, "avg_logprob": -0.24479395548502605, "compression_ratio": 1.7939698492462313, "no_speech_prob": 1.341545157629298e-05}, {"id": 556, "seek": 360324, "start": 3622.52, "end": 3627.56, "text": " talk about pretending can really change the performance can be again changing your reading", "tokens": [751, 466, 22106, 393, 534, 1319, 264, 3389, 393, 312, 797, 4473, 428, 3760], "temperature": 0.0, "avg_logprob": -0.24479395548502605, "compression_ratio": 1.7939698492462313, "no_speech_prob": 1.341545157629298e-05}, {"id": 557, "seek": 362756, "start": 3627.56, "end": 3634.92, "text": " comprehension I can put it on don't you I can ask add one question first people wonder whether you", "tokens": [44991, 286, 393, 829, 309, 322, 500, 380, 291, 286, 393, 1029, 909, 472, 1168, 700, 561, 2441, 1968, 291], "temperature": 0.0, "avg_logprob": -0.22670485947158311, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.108286389557179e-05}, {"id": 558, "seek": 362756, "start": 3634.92, "end": 3640.92, "text": " can do well with a transformer that isn't pre-trained right if you tried to build a question", "tokens": [393, 360, 731, 365, 257, 31782, 300, 1943, 380, 659, 12, 17227, 2001, 558, 498, 291, 3031, 281, 1322, 257, 1168], "temperature": 0.0, "avg_logprob": -0.22670485947158311, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.108286389557179e-05}, {"id": 559, "seek": 362756, "start": 3640.92, "end": 3646.12, "text": " answering system using a transformer rather than RSTM's then no pre-training does that work", "tokens": [13430, 1185, 1228, 257, 31782, 2831, 813, 497, 6840, 44, 311, 550, 572, 659, 12, 17227, 1760, 775, 300, 589], "temperature": 0.0, "avg_logprob": -0.22670485947158311, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.108286389557179e-05}, {"id": 560, "seek": 362756, "start": 3647.4, "end": 3653.16, "text": " that's a good question yeah it works but you probably cannot review the model as big as like one", "tokens": [300, 311, 257, 665, 1168, 1338, 309, 1985, 457, 291, 1391, 2644, 3131, 264, 2316, 382, 955, 382, 411, 472], "temperature": 0.0, "avg_logprob": -0.22670485947158311, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.108286389557179e-05}, {"id": 561, "seek": 365316, "start": 3653.16, "end": 3661.56, "text": " one 10 million premise or 230 million parameters models so actually there's a model between the", "tokens": [472, 1266, 2459, 22045, 420, 35311, 2459, 9834, 5245, 370, 767, 456, 311, 257, 2316, 1296, 264], "temperature": 0.0, "avg_logprob": -0.33842743997988495, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3510370738222264e-05}, {"id": 562, "seek": 365316, "start": 3663.48, "end": 3669.8799999999997, "text": " sorry between the this is like a family of RSTM models and bird models they're called a QA in that", "tokens": [2597, 1296, 264, 341, 307, 411, 257, 1605, 295, 497, 6840, 44, 5245, 293, 5255, 5245, 436, 434, 1219, 257, 1249, 32, 294, 300], "temperature": 0.0, "avg_logprob": -0.33842743997988495, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3510370738222264e-05}, {"id": 563, "seek": 365316, "start": 3669.8799999999997, "end": 3674.92, "text": " from Google so QA in that is actually built on top of the transformers we saw the real pre-training", "tokens": [490, 3329, 370, 1249, 32, 294, 300, 307, 767, 3094, 322, 1192, 295, 264, 4088, 433, 321, 1866, 264, 957, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.33842743997988495, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3510370738222264e-05}, {"id": 564, "seek": 365316, "start": 3674.92, "end": 3679.72, "text": " so that model actually can perform better than the bad act models and other models but actually", "tokens": [370, 300, 2316, 767, 393, 2042, 1101, 813, 264, 1578, 605, 5245, 293, 661, 5245, 457, 767], "temperature": 0.0, "avg_logprob": -0.33842743997988495, "compression_ratio": 1.7727272727272727, "no_speech_prob": 2.3510370738222264e-05}, {"id": 565, "seek": 367972, "start": 3679.72, "end": 3685.72, "text": " on the performance of that a bird model quite a bit so just check it out for QA in that", "tokens": [322, 264, 3389, 295, 300, 257, 5255, 2316, 1596, 257, 857, 370, 445, 1520, 309, 484, 337, 1249, 32, 294, 300], "temperature": 0.0, "avg_logprob": -0.2456366370705997, "compression_ratio": 1.6787330316742082, "no_speech_prob": 1.6957055777311325e-05}, {"id": 566, "seek": 367972, "start": 3689.3999999999996, "end": 3696.9199999999996, "text": " okay I will just continue so okay so given pre-training has been so important so next I want", "tokens": [1392, 286, 486, 445, 2354, 370, 1392, 370, 2212, 659, 12, 17227, 1760, 575, 668, 370, 1021, 370, 958, 286, 528], "temperature": 0.0, "avg_logprob": -0.2456366370705997, "compression_ratio": 1.6787330316742082, "no_speech_prob": 1.6957055777311325e-05}, {"id": 567, "seek": 367972, "start": 3696.9199999999996, "end": 3702.68, "text": " quickly talk about okay question here is that can we actually even define better pre-training", "tokens": [2661, 751, 466, 1392, 1168, 510, 307, 300, 393, 321, 767, 754, 6964, 1101, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.2456366370705997, "compression_ratio": 1.6787330316742082, "no_speech_prob": 1.6957055777311325e-05}, {"id": 568, "seek": 367972, "start": 3702.68, "end": 3707.7999999999997, "text": " objective for reading comprehension or question answering and the answer is actually yes so this", "tokens": [10024, 337, 3760, 44991, 420, 1168, 13430, 293, 264, 1867, 307, 767, 2086, 370, 341], "temperature": 0.0, "avg_logprob": -0.2456366370705997, "compression_ratio": 1.6787330316742082, "no_speech_prob": 1.6957055777311325e-05}, {"id": 569, "seek": 370780, "start": 3707.8, "end": 3714.1200000000003, "text": " actually work I did with Mender-Drosion other folks like one year ago called Spambert so think about", "tokens": [767, 589, 286, 630, 365, 376, 3216, 12, 35, 2635, 313, 661, 4024, 411, 472, 1064, 2057, 1219, 1738, 335, 4290, 370, 519, 466], "temperature": 0.0, "avg_logprob": -0.348332025671518, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.069095318001928e-06}, {"id": 570, "seek": 370780, "start": 3714.1200000000003, "end": 3720.76, "text": " this so for the squad and other a lot of these types of reading comprehension data set the goal", "tokens": [341, 370, 337, 264, 15310, 293, 661, 257, 688, 295, 613, 3467, 295, 3760, 44991, 1412, 992, 264, 3387], "temperature": 0.0, "avg_logprob": -0.348332025671518, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.069095318001928e-06}, {"id": 571, "seek": 370780, "start": 3720.76, "end": 3727.6400000000003, "text": " is trying to predict the answer span from the passage as a question so the as an answer to the", "tokens": [307, 1382, 281, 6069, 264, 1867, 16174, 490, 264, 11497, 382, 257, 1168, 370, 264, 382, 364, 1867, 281, 264], "temperature": 0.0, "avg_logprob": -0.348332025671518, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.069095318001928e-06}, {"id": 572, "seek": 370780, "start": 3727.6400000000003, "end": 3735.0800000000004, "text": " discussion so there are two key ideas being proposed in Spambert so first idea is that instead of", "tokens": [5017, 370, 456, 366, 732, 2141, 3487, 885, 10348, 294, 1738, 335, 4290, 370, 700, 1558, 307, 300, 2602, 295], "temperature": 0.0, "avg_logprob": -0.348332025671518, "compression_ratio": 1.6767241379310345, "no_speech_prob": 7.069095318001928e-06}, {"id": 573, "seek": 373508, "start": 3735.08, "end": 3740.92, "text": " using only the masking of individual words we propose that we want to master particular", "tokens": [1228, 787, 264, 31226, 295, 2609, 2283, 321, 17421, 300, 321, 528, 281, 4505, 1729], "temperature": 0.0, "avg_logprob": -0.28624317563813306, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.709229870059062e-06}, {"id": 574, "seek": 373508, "start": 3740.92, "end": 3747.16, "text": " spence of words in the passage because the final answer would be just a segment of text in the", "tokens": [637, 655, 295, 2283, 294, 264, 11497, 570, 264, 2572, 1867, 576, 312, 445, 257, 9469, 295, 2487, 294, 264], "temperature": 0.0, "avg_logprob": -0.28624317563813306, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.709229870059062e-06}, {"id": 575, "seek": 373508, "start": 3747.16, "end": 3753.72, "text": " passage so we are trying to so mask out all these possible answers spence from the passage as a", "tokens": [11497, 370, 321, 366, 1382, 281, 370, 6094, 484, 439, 613, 1944, 6338, 637, 655, 490, 264, 11497, 382, 257], "temperature": 0.0, "avg_logprob": -0.28624317563813306, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.709229870059062e-06}, {"id": 576, "seek": 373508, "start": 3753.72, "end": 3760.44, "text": " pre-training objective and the second idea of compulsory Spambert is that because at the end of", "tokens": [659, 12, 17227, 1760, 10024, 293, 264, 1150, 1558, 295, 42773, 827, 1738, 335, 4290, 307, 300, 570, 412, 264, 917, 295], "temperature": 0.0, "avg_logprob": -0.28624317563813306, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.709229870059062e-06}, {"id": 577, "seek": 376044, "start": 3760.44, "end": 3766.68, "text": " we want to predict an answer spence so we actually essentially trying to predict two end points", "tokens": [321, 528, 281, 6069, 364, 1867, 637, 655, 370, 321, 767, 4476, 1382, 281, 6069, 732, 917, 2793], "temperature": 0.0, "avg_logprob": -0.23294165894225405, "compression_ratio": 2.1186440677966103, "no_speech_prob": 1.3618951015814673e-05}, {"id": 578, "seek": 376044, "start": 3766.68, "end": 3773.7200000000003, "text": " as a answer so the idea is that can we try to compress the two end points of answer span", "tokens": [382, 257, 1867, 370, 264, 1558, 307, 300, 393, 321, 853, 281, 14778, 264, 732, 917, 2793, 295, 1867, 16174], "temperature": 0.0, "avg_logprob": -0.23294165894225405, "compression_ratio": 2.1186440677966103, "no_speech_prob": 1.3618951015814673e-05}, {"id": 579, "seek": 376044, "start": 3776.92, "end": 3782.68, "text": " so can we try to compress all the information in this span into the two end points so here's", "tokens": [370, 393, 321, 853, 281, 14778, 439, 264, 1589, 294, 341, 16174, 666, 264, 732, 917, 2793, 370, 510, 311], "temperature": 0.0, "avg_logprob": -0.23294165894225405, "compression_ratio": 2.1186440677966103, "no_speech_prob": 1.3618951015814673e-05}, {"id": 580, "seek": 376044, "start": 3782.68, "end": 3788.84, "text": " the idea is that here let's think about this if we mask out the four words here and can we try to", "tokens": [264, 1558, 307, 300, 510, 718, 311, 519, 466, 341, 498, 321, 6094, 484, 264, 1451, 2283, 510, 293, 393, 321, 853, 281], "temperature": 0.0, "avg_logprob": -0.23294165894225405, "compression_ratio": 2.1186440677966103, "no_speech_prob": 1.3618951015814673e-05}, {"id": 581, "seek": 378884, "start": 3788.84, "end": 3796.52, "text": " use the two end points here in this figure like an x4 and x9 to predict all the words in the middle", "tokens": [764, 264, 732, 917, 2793, 510, 294, 341, 2573, 411, 364, 2031, 19, 293, 2031, 24, 281, 6069, 439, 264, 2283, 294, 264, 2808], "temperature": 0.0, "avg_logprob": -0.18974634396132603, "compression_ratio": 1.897560975609756, "no_speech_prob": 3.1165815016720444e-05}, {"id": 582, "seek": 378884, "start": 3796.52, "end": 3801.8, "text": " so essentially we are trying to predict takes the two end points and also the position some kind", "tokens": [370, 4476, 321, 366, 1382, 281, 6069, 2516, 264, 732, 917, 2793, 293, 611, 264, 2535, 512, 733], "temperature": 0.0, "avg_logprob": -0.18974634396132603, "compression_ratio": 1.897560975609756, "no_speech_prob": 3.1165815016720444e-05}, {"id": 583, "seek": 378884, "start": 3801.8, "end": 3807.32, "text": " of position coding and finally we are going to try to predict all the words in this span so this", "tokens": [295, 2535, 17720, 293, 2721, 321, 366, 516, 281, 853, 281, 6069, 439, 264, 2283, 294, 341, 16174, 370, 341], "temperature": 0.0, "avg_logprob": -0.18974634396132603, "compression_ratio": 1.897560975609756, "no_speech_prob": 3.1165815016720444e-05}, {"id": 584, "seek": 378884, "start": 3807.32, "end": 3813.8, "text": " is why this code is spambert so I encourage you to check out our paper and this actually really", "tokens": [307, 983, 341, 3089, 307, 637, 335, 4290, 370, 286, 5373, 291, 281, 1520, 484, 527, 3035, 293, 341, 767, 534], "temperature": 0.0, "avg_logprob": -0.18974634396132603, "compression_ratio": 1.897560975609756, "no_speech_prob": 3.1165815016720444e-05}, {"id": 585, "seek": 381380, "start": 3813.8, "end": 3819.7200000000003, "text": " helps a lot at least for the questions and data sets so as you can see from this figure so this", "tokens": [3665, 257, 688, 412, 1935, 337, 264, 1651, 293, 1412, 6352, 370, 382, 291, 393, 536, 490, 341, 2573, 370, 341], "temperature": 0.0, "avg_logprob": -0.3310777415399966, "compression_ratio": 1.8095238095238095, "no_speech_prob": 4.39807990915142e-05}, {"id": 586, "seek": 381380, "start": 3819.7200000000003, "end": 3826.92, "text": " is called 1.1 and it's called 2.0 and this are many other questions and data sets so you can see", "tokens": [307, 1219, 502, 13, 16, 293, 309, 311, 1219, 568, 13, 15, 293, 341, 366, 867, 661, 1651, 293, 1412, 6352, 370, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.3310777415399966, "compression_ratio": 1.8095238095238095, "no_speech_prob": 4.39807990915142e-05}, {"id": 587, "seek": 381380, "start": 3826.92, "end": 3834.52, "text": " here so the blue bars here we call the google bird is actually the original check points that", "tokens": [510, 370, 264, 3344, 10228, 510, 321, 818, 264, 20742, 5255, 307, 767, 264, 3380, 1520, 2793, 300], "temperature": 0.0, "avg_logprob": -0.3310777415399966, "compression_ratio": 1.8095238095238095, "no_speech_prob": 4.39807990915142e-05}, {"id": 588, "seek": 381380, "start": 3834.52, "end": 3840.6000000000004, "text": " released by google researchers and our bird is actually just exactly our re-evaluation of the", "tokens": [4736, 538, 20742, 10309, 293, 527, 5255, 307, 767, 445, 2293, 527, 319, 12, 68, 46504, 295, 264], "temperature": 0.0, "avg_logprob": -0.3310777415399966, "compression_ratio": 1.8095238095238095, "no_speech_prob": 4.39807990915142e-05}, {"id": 589, "seek": 384060, "start": 3840.6, "end": 3845.96, "text": " bird model but we are having trying to using the same data but we have been trying to transit", "tokens": [5255, 2316, 457, 321, 366, 1419, 1382, 281, 1228, 264, 912, 1412, 457, 321, 362, 668, 1382, 281, 17976], "temperature": 0.0, "avg_logprob": -0.28668590692373425, "compression_ratio": 1.933609958506224, "no_speech_prob": 1.279309162782738e-05}, {"id": 590, "seek": 384060, "start": 3845.96, "end": 3851.72, "text": " model for slightly longer so it's actually achieved a better performance than our original google bird", "tokens": [2316, 337, 4748, 2854, 370, 309, 311, 767, 11042, 257, 1101, 3389, 813, 527, 3380, 20742, 5255], "temperature": 0.0, "avg_logprob": -0.28668590692373425, "compression_ratio": 1.933609958506224, "no_speech_prob": 1.279309162782738e-05}, {"id": 591, "seek": 384060, "start": 3851.72, "end": 3856.7599999999998, "text": " so as you can see the yellow bars here is actually the spambert so spambert actually", "tokens": [370, 382, 291, 393, 536, 264, 5566, 10228, 510, 307, 767, 264, 637, 335, 4290, 370, 637, 335, 4290, 767], "temperature": 0.0, "avg_logprob": -0.28668590692373425, "compression_ratio": 1.933609958506224, "no_speech_prob": 1.279309162782738e-05}, {"id": 592, "seek": 384060, "start": 3857.3199999999997, "end": 3862.7599999999998, "text": " brings with also performed google bird and all the bird basically across all the data sets", "tokens": [5607, 365, 611, 10332, 20742, 5255, 293, 439, 264, 5255, 1936, 2108, 439, 264, 1412, 6352], "temperature": 0.0, "avg_logprob": -0.28668590692373425, "compression_ratio": 1.933609958506224, "no_speech_prob": 1.279309162782738e-05}, {"id": 593, "seek": 384060, "start": 3862.7599999999998, "end": 3868.04, "text": " that really tells us that okay even if we are not going to increase the model size we are not", "tokens": [300, 534, 5112, 505, 300, 1392, 754, 498, 321, 366, 406, 516, 281, 3488, 264, 2316, 2744, 321, 366, 406], "temperature": 0.0, "avg_logprob": -0.28668590692373425, "compression_ratio": 1.933609958506224, "no_speech_prob": 1.279309162782738e-05}, {"id": 594, "seek": 386804, "start": 3868.04, "end": 3873.56, "text": " going to increase the data by designing better criteria objectives can also be very go a long way", "tokens": [516, 281, 3488, 264, 1412, 538, 14685, 1101, 11101, 15961, 393, 611, 312, 588, 352, 257, 938, 636], "temperature": 0.0, "avg_logprob": -0.3235486802600679, "compression_ratio": 1.6891891891891893, "no_speech_prob": 7.5265998020768166e-06}, {"id": 595, "seek": 386804, "start": 3873.56, "end": 3877.88, "text": " and do a much better job in at least in the question answering and reading comprehension", "tokens": [293, 360, 257, 709, 1101, 1691, 294, 412, 1935, 294, 264, 1168, 13430, 293, 3760, 44991], "temperature": 0.0, "avg_logprob": -0.3235486802600679, "compression_ratio": 1.6891891891891893, "no_speech_prob": 7.5265998020768166e-06}, {"id": 596, "seek": 386804, "start": 3877.88, "end": 3888.7599999999998, "text": " data sets okay so I have several few slides left in this part so so far I have to demonstrate", "tokens": [1412, 6352, 1392, 370, 286, 362, 2940, 1326, 9788, 1411, 294, 341, 644, 370, 370, 1400, 286, 362, 281, 11698], "temperature": 0.0, "avg_logprob": -0.3235486802600679, "compression_ratio": 1.6891891891891893, "no_speech_prob": 7.5265998020768166e-06}, {"id": 597, "seek": 386804, "start": 3888.7599999999998, "end": 3895.08, "text": " that on by using by death model and by using bird models we can get a very good performance on", "tokens": [300, 322, 538, 1228, 538, 2966, 2316, 293, 538, 1228, 5255, 5245, 321, 393, 483, 257, 588, 665, 3389, 322], "temperature": 0.0, "avg_logprob": -0.3235486802600679, "compression_ratio": 1.6891891891891893, "no_speech_prob": 7.5265998020768166e-06}, {"id": 598, "seek": 389508, "start": 3895.08, "end": 3900.6, "text": " the scope data set and this number has already exists even the human performance on scope", "tokens": [264, 11923, 1412, 992, 293, 341, 1230, 575, 1217, 8198, 754, 264, 1952, 3389, 322, 11923], "temperature": 0.0, "avg_logprob": -0.25278661092122395, "compression_ratio": 1.7233009708737863, "no_speech_prob": 2.4628889150335453e-05}, {"id": 599, "seek": 389508, "start": 3900.6, "end": 3905.96, "text": " that this means that reading comprehension is already solved the answer is of course not", "tokens": [300, 341, 1355, 300, 3760, 44991, 307, 1217, 13041, 264, 1867, 307, 295, 1164, 406], "temperature": 0.0, "avg_logprob": -0.25278661092122395, "compression_ratio": 1.7233009708737863, "no_speech_prob": 2.4628889150335453e-05}, {"id": 600, "seek": 389508, "start": 3906.68, "end": 3912.36, "text": " so let me just so in the recent last couple of years that's been a lot of evidence", "tokens": [370, 718, 385, 445, 370, 294, 264, 5162, 1036, 1916, 295, 924, 300, 311, 668, 257, 688, 295, 4467], "temperature": 0.0, "avg_logprob": -0.25278661092122395, "compression_ratio": 1.7233009708737863, "no_speech_prob": 2.4628889150335453e-05}, {"id": 601, "seek": 389508, "start": 3912.36, "end": 3918.6, "text": " showing that the current systems still perform poorly on adversarial examples or the examples", "tokens": [4099, 300, 264, 2190, 3652, 920, 2042, 22271, 322, 17641, 44745, 5110, 420, 264, 5110], "temperature": 0.0, "avg_logprob": -0.25278661092122395, "compression_ratio": 1.7233009708737863, "no_speech_prob": 2.4628889150335453e-05}, {"id": 602, "seek": 391860, "start": 3918.6, "end": 3926.04, "text": " from the out of domain distributions so here is a very classical example so proposed by", "tokens": [490, 264, 484, 295, 9274, 37870, 370, 510, 307, 257, 588, 13735, 1365, 370, 10348, 538], "temperature": 0.0, "avg_logprob": -0.3361358878053265, "compression_ratio": 1.5782608695652174, "no_speech_prob": 1.801863436412532e-05}, {"id": 603, "seek": 391860, "start": 3926.04, "end": 3934.36, "text": " Robin John personally on 2017 so the idea is that they take a pass and take a question", "tokens": [16533, 2619, 5665, 322, 6591, 370, 264, 1558, 307, 300, 436, 747, 257, 1320, 293, 747, 257, 1168], "temperature": 0.0, "avg_logprob": -0.3361358878053265, "compression_ratio": 1.5782608695652174, "no_speech_prob": 1.801863436412532e-05}, {"id": 604, "seek": 391860, "start": 3934.36, "end": 3940.52, "text": " and they're trying to just insert a random sentence to the end of the paragraph so you can see", "tokens": [293, 436, 434, 1382, 281, 445, 8969, 257, 4974, 8174, 281, 264, 917, 295, 264, 18865, 370, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.3361358878053265, "compression_ratio": 1.5782608695652174, "no_speech_prob": 1.801863436412532e-05}, {"id": 605, "seek": 391860, "start": 3940.52, "end": 3946.7599999999998, "text": " that distance passes like even like a nonsense entity in this context, drafting here but this", "tokens": [300, 4560, 11335, 411, 754, 411, 257, 14925, 13977, 294, 341, 4319, 11, 46378, 510, 457, 341], "temperature": 0.0, "avg_logprob": -0.3361358878053265, "compression_ratio": 1.5782608695652174, "no_speech_prob": 1.801863436412532e-05}, {"id": 606, "seek": 394676, "start": 3946.76, "end": 3951.7200000000003, "text": " sentence actually has a like a great some love score overlap between the question", "tokens": [8174, 767, 575, 257, 411, 257, 869, 512, 959, 6175, 19959, 1296, 264, 1168], "temperature": 0.0, "avg_logprob": -0.27179298798243207, "compression_ratio": 1.8697478991596639, "no_speech_prob": 1.4504691534966696e-05}, {"id": 607, "seek": 394676, "start": 3951.7200000000003, "end": 3955.96, "text": " is actually very similar to this question but actually the word numbers have been changed", "tokens": [307, 767, 588, 2531, 281, 341, 1168, 457, 767, 264, 1349, 3547, 362, 668, 3105], "temperature": 0.0, "avg_logprob": -0.27179298798243207, "compression_ratio": 1.8697478991596639, "no_speech_prob": 1.4504691534966696e-05}, {"id": 608, "seek": 394676, "start": 3955.96, "end": 3961.4, "text": " the entinence has been changed and they found that these kind of adversarial examples can actually", "tokens": [264, 948, 259, 655, 575, 668, 3105, 293, 436, 1352, 300, 613, 733, 295, 17641, 44745, 5110, 393, 767], "temperature": 0.0, "avg_logprob": -0.27179298798243207, "compression_ratio": 1.8697478991596639, "no_speech_prob": 1.4504691534966696e-05}, {"id": 609, "seek": 394676, "start": 3961.4, "end": 3966.92, "text": " very easy to fool the prime systems and the final and the makes the system to predict", "tokens": [588, 1858, 281, 7979, 264, 5835, 3652, 293, 264, 2572, 293, 264, 1669, 264, 1185, 281, 6069], "temperature": 0.0, "avg_logprob": -0.27179298798243207, "compression_ratio": 1.8697478991596639, "no_speech_prob": 1.4504691534966696e-05}, {"id": 610, "seek": 394676, "start": 3966.92, "end": 3973.88, "text": " answer to be the drafting so the by shoots the table shows that by adding a lot of these", "tokens": [1867, 281, 312, 264, 46378, 370, 264, 538, 20704, 264, 3199, 3110, 300, 538, 5127, 257, 688, 295, 613], "temperature": 0.0, "avg_logprob": -0.27179298798243207, "compression_ratio": 1.8697478991596639, "no_speech_prob": 1.4504691534966696e-05}, {"id": 611, "seek": 397388, "start": 3973.88, "end": 3979.48, "text": " adversarial examples they found that the performance actually drops a lot this by that model so", "tokens": [17641, 44745, 5110, 436, 1352, 300, 264, 3389, 767, 11438, 257, 688, 341, 538, 300, 2316, 370], "temperature": 0.0, "avg_logprob": -0.18505002093571488, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.4958049177948851e-05}, {"id": 612, "seek": 397388, "start": 3979.48, "end": 3986.92, "text": " drops from 75.5 to even like 30% so for even like this kind of attack the performance will just drop", "tokens": [11438, 490, 9562, 13, 20, 281, 754, 411, 2217, 4, 370, 337, 754, 411, 341, 733, 295, 2690, 264, 3389, 486, 445, 3270], "temperature": 0.0, "avg_logprob": -0.18505002093571488, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.4958049177948851e-05}, {"id": 613, "seek": 397388, "start": 3986.92, "end": 3995.32, "text": " to very low like 4.8% so here's another paper that actually just came out in 2020 so it has", "tokens": [281, 588, 2295, 411, 1017, 13, 23, 4, 370, 510, 311, 1071, 3035, 300, 767, 445, 1361, 484, 294, 4808, 370, 309, 575], "temperature": 0.0, "avg_logprob": -0.18505002093571488, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.4958049177948851e-05}, {"id": 614, "seek": 397388, "start": 3995.32, "end": 4000.12, "text": " made a lot of the evidence showing the similar things that so today we can be a very good reading", "tokens": [1027, 257, 688, 295, 264, 4467, 4099, 264, 2531, 721, 300, 370, 965, 321, 393, 312, 257, 588, 665, 3760], "temperature": 0.0, "avg_logprob": -0.18505002093571488, "compression_ratio": 1.6782608695652175, "no_speech_prob": 1.4958049177948851e-05}, {"id": 615, "seek": 400012, "start": 4000.12, "end": 4005.24, "text": " kind of attention data set on individual data on the individual data sets but this system is", "tokens": [733, 295, 3202, 1412, 992, 322, 2609, 1412, 322, 264, 2609, 1412, 6352, 457, 341, 1185, 307], "temperature": 0.0, "avg_logprob": -0.288619141351609, "compression_ratio": 2.2155963302752295, "no_speech_prob": 5.0517312047304586e-05}, {"id": 616, "seek": 400012, "start": 4005.24, "end": 4010.6, "text": " channel one data sets basically cannot really generalize to other data sets so the diagonal is basically", "tokens": [2269, 472, 1412, 6352, 1936, 2644, 534, 2674, 1125, 281, 661, 1412, 6352, 370, 264, 21539, 307, 1936], "temperature": 0.0, "avg_logprob": -0.288619141351609, "compression_ratio": 2.2155963302752295, "no_speech_prob": 5.0517312047304586e-05}, {"id": 617, "seek": 400012, "start": 4011.3199999999997, "end": 4017.3199999999997, "text": " of this table is basically channel one model on one data set and the evaluate on the send data set", "tokens": [295, 341, 3199, 307, 1936, 2269, 472, 2316, 322, 472, 1412, 992, 293, 264, 13059, 322, 264, 2845, 1412, 992], "temperature": 0.0, "avg_logprob": -0.288619141351609, "compression_ratio": 2.2155963302752295, "no_speech_prob": 5.0517312047304586e-05}, {"id": 618, "seek": 400012, "start": 4017.3199999999997, "end": 4023.08, "text": " and for all the other numbers in this table basically shows that if you turn one from system", "tokens": [293, 337, 439, 264, 661, 3547, 294, 341, 3199, 1936, 3110, 300, 498, 291, 1261, 472, 490, 1185], "temperature": 0.0, "avg_logprob": -0.288619141351609, "compression_ratio": 2.2155963302752295, "no_speech_prob": 5.0517312047304586e-05}, {"id": 619, "seek": 400012, "start": 4023.08, "end": 4028.68, "text": " on one data set and then evaluate on another data set the performance will drop a lot so it's", "tokens": [322, 472, 1412, 992, 293, 550, 13059, 322, 1071, 1412, 992, 264, 3389, 486, 3270, 257, 688, 370, 309, 311], "temperature": 0.0, "avg_logprob": -0.288619141351609, "compression_ratio": 2.2155963302752295, "no_speech_prob": 5.0517312047304586e-05}, {"id": 620, "seek": 402868, "start": 4028.68, "end": 4034.12, "text": " basically really cannot generalize from one data set to another data set so finally this is", "tokens": [1936, 534, 2644, 2674, 1125, 490, 472, 1412, 992, 281, 1071, 1412, 992, 370, 2721, 341, 307], "temperature": 0.0, "avg_logprob": -0.2195640865125154, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.928223693743348e-05}, {"id": 621, "seek": 402868, "start": 4034.12, "end": 4040.52, "text": " actually a very interesting result so this model this paper is actually the best paper from", "tokens": [767, 257, 588, 1880, 1874, 370, 341, 2316, 341, 3035, 307, 767, 264, 1151, 3035, 490], "temperature": 0.0, "avg_logprob": -0.2195640865125154, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.928223693743348e-05}, {"id": 622, "seek": 402868, "start": 4040.52, "end": 4048.04, "text": " ACL 2020 is called checklist paper so the idea is that this this also basically try to propose", "tokens": [43873, 4808, 307, 1219, 30357, 3035, 370, 264, 1558, 307, 300, 341, 341, 611, 1936, 853, 281, 17421], "temperature": 0.0, "avg_logprob": -0.2195640865125154, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.928223693743348e-05}, {"id": 623, "seek": 402868, "start": 4048.04, "end": 4054.7599999999998, "text": " some kind of the test cases to check whether this model can actually really under", "tokens": [512, 733, 295, 264, 1500, 3331, 281, 1520, 1968, 341, 2316, 393, 767, 534, 833], "temperature": 0.0, "avg_logprob": -0.2195640865125154, "compression_ratio": 1.791044776119403, "no_speech_prob": 2.928223693743348e-05}, {"id": 624, "seek": 405476, "start": 4054.76, "end": 4060.92, "text": " also some simple questions whether we sound specific or particular film they find that by just", "tokens": [611, 512, 2199, 1651, 1968, 321, 1626, 2685, 420, 1729, 2007, 436, 915, 300, 538, 445], "temperature": 0.0, "avg_logprob": -0.36447897075135033, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.668580443947576e-05}, {"id": 625, "seek": 405476, "start": 4061.88, "end": 4068.5200000000004, "text": " kind of a really simple question for example here Jeremy is more optimistic than Taylor", "tokens": [733, 295, 257, 534, 2199, 1168, 337, 1365, 510, 17809, 307, 544, 19397, 813, 12060], "temperature": 0.0, "avg_logprob": -0.36447897075135033, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.668580443947576e-05}, {"id": 626, "seek": 405476, "start": 4068.5200000000004, "end": 4074.84, "text": " and who is more pessimistic and the they found that a birth lot model channel stop and this", "tokens": [293, 567, 307, 544, 37399, 3142, 293, 264, 436, 1352, 300, 257, 3965, 688, 2316, 2269, 1590, 293, 341], "temperature": 0.0, "avg_logprob": -0.36447897075135033, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.668580443947576e-05}, {"id": 627, "seek": 405476, "start": 4074.84, "end": 4084.2000000000003, "text": " basically can fill this type of test cases 100% time and up here is another table so you can see", "tokens": [1936, 393, 2836, 341, 2010, 295, 1500, 3331, 2319, 4, 565, 293, 493, 510, 307, 1071, 3199, 370, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.36447897075135033, "compression_ratio": 1.701834862385321, "no_speech_prob": 1.668580443947576e-05}, {"id": 628, "seek": 408420, "start": 4084.2, "end": 4091.16, "text": " that here is another correct example like Victoria and Alex are friends her mom is an agent", "tokens": [300, 510, 307, 1071, 3006, 1365, 411, 16656, 293, 5202, 366, 1855, 720, 1225, 307, 364, 9461], "temperature": 0.0, "avg_logprob": -0.2987426996231079, "compression_ratio": 1.9037433155080214, "no_speech_prob": 3.1125334771786584e-06}, {"id": 629, "seek": 408420, "start": 4091.16, "end": 4097.48, "text": " who's mom is an agent and so to get this kind of question correctly it has to understand", "tokens": [567, 311, 1225, 307, 364, 9461, 293, 370, 281, 483, 341, 733, 295, 1168, 8944, 309, 575, 281, 1223], "temperature": 0.0, "avg_logprob": -0.2987426996231079, "compression_ratio": 1.9037433155080214, "no_speech_prob": 3.1125334771786584e-06}, {"id": 630, "seek": 408420, "start": 4097.48, "end": 4103.639999999999, "text": " the Victoria actually refers to a female person and Alex refers to a male person so this", "tokens": [264, 16656, 767, 14942, 281, 257, 6556, 954, 293, 5202, 14942, 281, 257, 7133, 954, 370, 341], "temperature": 0.0, "avg_logprob": -0.2987426996231079, "compression_ratio": 1.9037433155080214, "no_speech_prob": 3.1125334771786584e-06}, {"id": 631, "seek": 408420, "start": 4103.639999999999, "end": 4108.679999999999, "text": " this model this kind of questions also makes a kind of model for large models can also", "tokens": [341, 2316, 341, 733, 295, 1651, 611, 1669, 257, 733, 295, 2316, 337, 2416, 5245, 393, 611], "temperature": 0.0, "avg_logprob": -0.2987426996231079, "compression_ratio": 1.9037433155080214, "no_speech_prob": 3.1125334771786584e-06}, {"id": 632, "seek": 410868, "start": 4108.68, "end": 4120.04, "text": " totally fill on this kind of test cases okay so I have 10 minutes left Chris is any question", "tokens": [3879, 2836, 322, 341, 733, 295, 1500, 3331, 1392, 370, 286, 362, 1266, 2077, 1411, 6688, 307, 604, 1168], "temperature": 0.0, "avg_logprob": -0.37473545652447326, "compression_ratio": 1.5722543352601157, "no_speech_prob": 2.2815409465692937e-05}, {"id": 633, "seek": 410868, "start": 4120.04, "end": 4127.320000000001, "text": " I should answer this point here you can go on okay so in the last 10 minutes I'm going to be", "tokens": [286, 820, 1867, 341, 935, 510, 291, 393, 352, 322, 1392, 370, 294, 264, 1036, 1266, 2077, 286, 478, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.37473545652447326, "compression_ratio": 1.5722543352601157, "no_speech_prob": 2.2815409465692937e-05}, {"id": 634, "seek": 410868, "start": 4127.320000000001, "end": 4132.76, "text": " very very very with introduction of what is open to my question and what we are having", "tokens": [588, 588, 588, 365, 9339, 295, 437, 307, 1269, 281, 452, 1168, 293, 437, 321, 366, 1419], "temperature": 0.0, "avg_logprob": -0.37473545652447326, "compression_ratio": 1.5722543352601157, "no_speech_prob": 2.2815409465692937e-05}, {"id": 635, "seek": 413276, "start": 4132.76, "end": 4139.64, "text": " trying to do in the last couple years so open the main question is the problem that so it", "tokens": [1382, 281, 360, 294, 264, 1036, 1916, 924, 370, 1269, 264, 2135, 1168, 307, 264, 1154, 300, 370, 309], "temperature": 0.0, "avg_logprob": -0.24944350298713236, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.8912087398348376e-05}, {"id": 636, "seek": 413276, "start": 4139.64, "end": 4144.68, "text": " different from reading comprehension that we don't assume a given passage so here we have", "tokens": [819, 490, 3760, 44991, 300, 321, 500, 380, 6552, 257, 2212, 11497, 370, 510, 321, 362], "temperature": 0.0, "avg_logprob": -0.24944350298713236, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.8912087398348376e-05}, {"id": 637, "seek": 413276, "start": 4144.68, "end": 4150.6, "text": " assumption that we only have access to a large collection of weapons so one example is just", "tokens": [15302, 300, 321, 787, 362, 2105, 281, 257, 2416, 5765, 295, 7278, 370, 472, 1365, 307, 445], "temperature": 0.0, "avg_logprob": -0.24944350298713236, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.8912087398348376e-05}, {"id": 638, "seek": 413276, "start": 4150.6, "end": 4155.16, "text": " taking the whole way you should be keeping it on which has like 5 million articles so we don't", "tokens": [1940, 264, 1379, 636, 291, 820, 312, 5145, 309, 322, 597, 575, 411, 1025, 2459, 11290, 370, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.24944350298713236, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.8912087398348376e-05}, {"id": 639, "seek": 413276, "start": 4155.16, "end": 4160.2, "text": " really know where the answer is located and the goal is to return the answer for any open", "tokens": [534, 458, 689, 264, 1867, 307, 6870, 293, 264, 3387, 307, 281, 2736, 264, 1867, 337, 604, 1269], "temperature": 0.0, "avg_logprob": -0.24944350298713236, "compression_ratio": 1.7471264367816093, "no_speech_prob": 1.8912087398348376e-05}, {"id": 640, "seek": 416020, "start": 4160.2, "end": 4165.08, "text": " of questions so this problem so there is an annual single passage so we have to answer questions", "tokens": [295, 1651, 370, 341, 1154, 370, 456, 307, 364, 9784, 2167, 11497, 370, 321, 362, 281, 1867, 1651], "temperature": 0.0, "avg_logprob": -0.21053992645649972, "compression_ratio": 1.7616822429906542, "no_speech_prob": 2.0453589968383312e-05}, {"id": 641, "seek": 416020, "start": 4165.08, "end": 4170.599999999999, "text": " against a very large collection document or even the whole web documents so this is actually", "tokens": [1970, 257, 588, 2416, 5765, 4166, 420, 754, 264, 1379, 3670, 8512, 370, 341, 307, 767], "temperature": 0.0, "avg_logprob": -0.21053992645649972, "compression_ratio": 1.7616822429906542, "no_speech_prob": 2.0453589968383312e-05}, {"id": 642, "seek": 416020, "start": 4170.599999999999, "end": 4178.599999999999, "text": " much more challenging and also more practical problem so if you look at the example of Google", "tokens": [709, 544, 7595, 293, 611, 544, 8496, 1154, 370, 498, 291, 574, 412, 264, 1365, 295, 3329], "temperature": 0.0, "avg_logprob": -0.21053992645649972, "compression_ratio": 1.7616822429906542, "no_speech_prob": 2.0453589968383312e-05}, {"id": 643, "seek": 416020, "start": 4178.599999999999, "end": 4183.639999999999, "text": " example I showed at the beginning so this is with techniques where will be very useful in the", "tokens": [1365, 286, 4712, 412, 264, 2863, 370, 341, 307, 365, 7512, 689, 486, 312, 588, 4420, 294, 264], "temperature": 0.0, "avg_logprob": -0.21053992645649972, "compression_ratio": 1.7616822429906542, "no_speech_prob": 2.0453589968383312e-05}, {"id": 644, "seek": 418364, "start": 4183.64, "end": 4191.4800000000005, "text": " practical applications so the time here open domain is just in contrast to closed domains that", "tokens": [8496, 5821, 370, 264, 565, 510, 1269, 9274, 307, 445, 294, 8712, 281, 5395, 25514, 300], "temperature": 0.0, "avg_logprob": -0.24093389511108398, "compression_ratio": 1.6515837104072397, "no_speech_prob": 7.236945748445578e-06}, {"id": 645, "seek": 418364, "start": 4191.4800000000005, "end": 4201.4800000000005, "text": " deal with questions under specific domain here okay so how can we solve this type problem", "tokens": [2028, 365, 1651, 833, 2685, 9274, 510, 1392, 370, 577, 393, 321, 5039, 341, 2010, 1154], "temperature": 0.0, "avg_logprob": -0.24093389511108398, "compression_ratio": 1.6515837104072397, "no_speech_prob": 7.236945748445578e-06}, {"id": 646, "seek": 418364, "start": 4203.160000000001, "end": 4206.4400000000005, "text": " because for the reading comprehension problem we just need to answer questions based on", "tokens": [570, 337, 264, 3760, 44991, 1154, 321, 445, 643, 281, 1867, 1651, 2361, 322], "temperature": 0.0, "avg_logprob": -0.24093389511108398, "compression_ratio": 1.6515837104072397, "no_speech_prob": 7.236945748445578e-06}, {"id": 647, "seek": 418364, "start": 4206.4400000000005, "end": 4212.76, "text": " single passage so this is a paper that I wrote in 2017 four years now so the paper is called", "tokens": [2167, 11497, 370, 341, 307, 257, 3035, 300, 286, 4114, 294, 6591, 1451, 924, 586, 370, 264, 3035, 307, 1219], "temperature": 0.0, "avg_logprob": -0.24093389511108398, "compression_ratio": 1.6515837104072397, "no_speech_prob": 7.236945748445578e-06}, {"id": 648, "seek": 421276, "start": 4212.76, "end": 4219.0, "text": " reading Wikipedia to answer open domain questions and system called up to Q8 so the paper", "tokens": [3760, 28999, 281, 1867, 1269, 9274, 1651, 293, 1185, 1219, 493, 281, 1249, 23, 370, 264, 3035], "temperature": 0.0, "avg_logprob": -0.3392428229836857, "compression_ratio": 1.7477064220183487, "no_speech_prob": 1.8334214473725297e-05}, {"id": 649, "seek": 421276, "start": 4219.0, "end": 4225.0, "text": " basically proposes the idea that we can actually solve this problem by using a retrieval and also", "tokens": [1936, 2365, 4201, 264, 1558, 300, 321, 393, 767, 5039, 341, 1154, 538, 1228, 257, 19817, 3337, 293, 611], "temperature": 0.0, "avg_logprob": -0.3392428229836857, "compression_ratio": 1.7477064220183487, "no_speech_prob": 1.8334214473725297e-05}, {"id": 650, "seek": 421276, "start": 4225.0, "end": 4232.12, "text": " read a framework so idea is that let's take a question okay so here all we go is trying to answer", "tokens": [1401, 257, 8388, 370, 1558, 307, 300, 718, 311, 747, 257, 1168, 1392, 370, 510, 439, 321, 352, 307, 1382, 281, 1867], "temperature": 0.0, "avg_logprob": -0.3392428229836857, "compression_ratio": 1.7477064220183487, "no_speech_prob": 1.8334214473725297e-05}, {"id": 651, "seek": 421276, "start": 4232.12, "end": 4237.96, "text": " questions using like a very large collection document such as the Wikipedia so the idea is that", "tokens": [1651, 1228, 411, 257, 588, 2416, 5765, 4166, 1270, 382, 264, 28999, 370, 264, 1558, 307, 300], "temperature": 0.0, "avg_logprob": -0.3392428229836857, "compression_ratio": 1.7477064220183487, "no_speech_prob": 1.8334214473725297e-05}, {"id": 652, "seek": 423796, "start": 4237.96, "end": 4244.04, "text": " there's a retrieval and also read a component so the retrieval takes in the question and I try to", "tokens": [456, 311, 257, 19817, 3337, 293, 611, 1401, 257, 6542, 370, 264, 19817, 3337, 2516, 294, 264, 1168, 293, 286, 853, 281], "temperature": 0.0, "avg_logprob": -0.23729885583636404, "compression_ratio": 1.8591549295774648, "no_speech_prob": 1.4059435670787934e-05}, {"id": 653, "seek": 423796, "start": 4244.04, "end": 4249.0, "text": " find out like a smaller number of the our documents that to be relevant to this question and this", "tokens": [915, 484, 411, 257, 4356, 1230, 295, 264, 527, 8512, 300, 281, 312, 7340, 281, 341, 1168, 293, 341], "temperature": 0.0, "avg_logprob": -0.23729885583636404, "compression_ratio": 1.8591549295774648, "no_speech_prob": 1.4059435670787934e-05}, {"id": 654, "seek": 423796, "start": 4249.0, "end": 4255.32, "text": " reading model basically trying to read through all the documents that this retrieval return and the", "tokens": [3760, 2316, 1936, 1382, 281, 1401, 807, 439, 264, 8512, 300, 341, 19817, 3337, 2736, 293, 264], "temperature": 0.0, "avg_logprob": -0.23729885583636404, "compression_ratio": 1.8591549295774648, "no_speech_prob": 1.4059435670787934e-05}, {"id": 655, "seek": 423796, "start": 4255.32, "end": 4263.16, "text": " fact try to find out the correct answers so formally defined here is that you put a large collection", "tokens": [1186, 853, 281, 915, 484, 264, 3006, 6338, 370, 25983, 7642, 510, 307, 300, 291, 829, 257, 2416, 5765], "temperature": 0.0, "avg_logprob": -0.23729885583636404, "compression_ratio": 1.8591549295774648, "no_speech_prob": 1.4059435670787934e-05}, {"id": 656, "seek": 426316, "start": 4263.16, "end": 4270.599999999999, "text": " documents D and the question Q and the output could be our answer stream A so we had just", "tokens": [8512, 413, 293, 264, 1168, 1249, 293, 264, 5598, 727, 312, 527, 1867, 4309, 316, 370, 321, 632, 445], "temperature": 0.0, "avg_logprob": -0.26442977360316683, "compression_ratio": 1.7203791469194314, "no_speech_prob": 1.7226153431693092e-05}, {"id": 657, "seek": 426316, "start": 4270.599999999999, "end": 4275.48, "text": " decomposed this problem into as I just mentioned in our retrieval and the reader component so the", "tokens": [22867, 1744, 341, 1154, 666, 382, 286, 445, 2835, 294, 527, 19817, 3337, 293, 264, 15149, 6542, 370, 264], "temperature": 0.0, "avg_logprob": -0.26442977360316683, "compression_ratio": 1.7203791469194314, "no_speech_prob": 1.7226153431693092e-05}, {"id": 658, "seek": 426316, "start": 4275.48, "end": 4281.5599999999995, "text": " retrieval is basically trying to take a large collection document D and Q and try to return", "tokens": [19817, 3337, 307, 1936, 1382, 281, 747, 257, 2416, 5765, 4166, 413, 293, 1249, 293, 853, 281, 2736], "temperature": 0.0, "avg_logprob": -0.26442977360316683, "compression_ratio": 1.7203791469194314, "no_speech_prob": 1.7226153431693092e-05}, {"id": 659, "seek": 426316, "start": 4281.5599999999995, "end": 4286.92, "text": " a set document or set of passages so here the set this number K could be very small", "tokens": [257, 992, 4166, 420, 992, 295, 31589, 370, 510, 264, 992, 341, 1230, 591, 727, 312, 588, 1359], "temperature": 0.0, "avg_logprob": -0.26442977360316683, "compression_ratio": 1.7203791469194314, "no_speech_prob": 1.7226153431693092e-05}, {"id": 660, "seek": 428692, "start": 4286.92, "end": 4295.56, "text": " could be very small such as like one of just like a 100 so it's basically trying to pull out", "tokens": [727, 312, 588, 1359, 1270, 382, 411, 472, 295, 445, 411, 257, 2319, 370, 309, 311, 1936, 1382, 281, 2235, 484], "temperature": 0.0, "avg_logprob": -0.2608373103997646, "compression_ratio": 1.7621359223300972, "no_speech_prob": 9.365413461637218e-06}, {"id": 661, "seek": 428692, "start": 4295.56, "end": 4301.72, "text": " the found out like 100 passages of documents from like let's say five million documents", "tokens": [264, 1352, 484, 411, 2319, 31589, 295, 8512, 490, 411, 718, 311, 584, 1732, 2459, 8512], "temperature": 0.0, "avg_logprob": -0.2608373103997646, "compression_ratio": 1.7621359223300972, "no_speech_prob": 9.365413461637218e-06}, {"id": 662, "seek": 428692, "start": 4301.72, "end": 4307.0, "text": " and the finally the reader is basically takes a question and takes this set of the passages and", "tokens": [293, 264, 2721, 264, 15149, 307, 1936, 2516, 257, 1168, 293, 2516, 341, 992, 295, 264, 31589, 293], "temperature": 0.0, "avg_logprob": -0.2608373103997646, "compression_ratio": 1.7621359223300972, "no_speech_prob": 9.365413461637218e-06}, {"id": 663, "seek": 428692, "start": 4307.0, "end": 4312.12, "text": " finally finally returns the answer so the second problem exactly the reading component", "tokens": [2721, 2721, 11247, 264, 1867, 370, 264, 1150, 1154, 2293, 264, 3760, 6542], "temperature": 0.0, "avg_logprob": -0.2608373103997646, "compression_ratio": 1.7621359223300972, "no_speech_prob": 9.365413461637218e-06}, {"id": 664, "seek": 431212, "start": 4312.12, "end": 4320.5199999999995, "text": " model that we just learned so in the just 70 paper result so it's actually doing a very simple", "tokens": [2316, 300, 321, 445, 3264, 370, 294, 264, 445, 5285, 3035, 1874, 370, 309, 311, 767, 884, 257, 588, 2199], "temperature": 0.0, "avg_logprob": -0.5497019631522042, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.9766921215923503e-05}, {"id": 665, "seek": 431212, "start": 4320.5199999999995, "end": 4325.5599999999995, "text": " thing so the retrieval is just a standard a new formation retrieval model the sparse", "tokens": [551, 370, 264, 19817, 3337, 307, 445, 257, 3832, 257, 777, 11723, 19817, 3337, 2316, 264, 637, 11668], "temperature": 0.0, "avg_logprob": -0.5497019631522042, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.9766921215923503e-05}, {"id": 666, "seek": 431212, "start": 4326.12, "end": 4331.16, "text": " pfid information retrieval sparse model and the real model essentially just a new already", "tokens": [280, 69, 327, 1589, 19817, 3337, 637, 11668, 2316, 293, 264, 957, 2316, 4476, 445, 257, 777, 1217], "temperature": 0.0, "avg_logprob": -0.5497019631522042, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.9766921215923503e-05}, {"id": 667, "seek": 431212, "start": 4331.16, "end": 4337.08, "text": " comprehend the model I just talked about so it's very trying to solve and some other questions", "tokens": [38183, 264, 2316, 286, 445, 2825, 466, 370, 309, 311, 588, 1382, 281, 5039, 293, 512, 661, 1651], "temperature": 0.0, "avg_logprob": -0.5497019631522042, "compression_ratio": 1.829145728643216, "no_speech_prob": 2.9766921215923503e-05}, {"id": 668, "seek": 433708, "start": 4337.08, "end": 4343.32, "text": " during the process so this is the drill it's the idea is very simple but trying to bridge two things", "tokens": [1830, 264, 1399, 370, 341, 307, 264, 11392, 309, 311, 264, 1558, 307, 588, 2199, 457, 1382, 281, 7283, 732, 721], "temperature": 0.0, "avg_logprob": -0.28000619676378036, "compression_ratio": 1.7834101382488479, "no_speech_prob": 1.6695821614121087e-05}, {"id": 669, "seek": 433708, "start": 4343.32, "end": 4348.68, "text": " how to how to have bridge this retrieval and also the reader to do this kind of open domain question", "tokens": [577, 281, 577, 281, 362, 7283, 341, 19817, 3337, 293, 611, 264, 15149, 281, 360, 341, 733, 295, 1269, 9274, 1168], "temperature": 0.0, "avg_logprob": -0.28000619676378036, "compression_ratio": 1.7834101382488479, "no_speech_prob": 1.6695821614121087e-05}, {"id": 670, "seek": 433708, "start": 4348.68, "end": 4358.36, "text": " history so so I'm just going to quickly go over some really exciting ideas that that has been", "tokens": [2503, 370, 370, 286, 478, 445, 516, 281, 2661, 352, 670, 512, 534, 4670, 3487, 300, 300, 575, 668], "temperature": 0.0, "avg_logprob": -0.28000619676378036, "compression_ratio": 1.7834101382488479, "no_speech_prob": 1.6695821614121087e-05}, {"id": 671, "seek": 433708, "start": 4358.36, "end": 4365.88, "text": " having in the last two years basically so the first idea is that this retrieval part can be", "tokens": [1419, 294, 264, 1036, 732, 924, 1936, 370, 264, 700, 1558, 307, 300, 341, 19817, 3337, 644, 393, 312], "temperature": 0.0, "avg_logprob": -0.28000619676378036, "compression_ratio": 1.7834101382488479, "no_speech_prob": 1.6695821614121087e-05}, {"id": 672, "seek": 436588, "start": 4365.88, "end": 4370.36, "text": " also trained so we can actually even do this kind of drawing the training of the retrieval and the", "tokens": [611, 8895, 370, 321, 393, 767, 754, 360, 341, 733, 295, 6316, 264, 3097, 295, 264, 19817, 3337, 293, 264], "temperature": 0.0, "avg_logprob": -0.2500044958932059, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.3207375559431966e-05}, {"id": 673, "seek": 436588, "start": 4370.36, "end": 4377.88, "text": " reader so here is actually so this this idea has been first proposed in Cantonese paper in 2019", "tokens": [15149, 370, 510, 307, 767, 370, 341, 341, 1558, 575, 668, 700, 10348, 294, 44170, 1130, 3035, 294, 6071], "temperature": 0.0, "avg_logprob": -0.2500044958932059, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.3207375559431966e-05}, {"id": 674, "seek": 436588, "start": 4377.88, "end": 4384.6, "text": " called later in the retrieval for weekly supervised open domain questions so this part is basically", "tokens": [1219, 1780, 294, 264, 19817, 3337, 337, 12460, 46533, 1269, 9274, 1651, 370, 341, 644, 307, 1936], "temperature": 0.0, "avg_logprob": -0.2500044958932059, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.3207375559431966e-05}, {"id": 675, "seek": 436588, "start": 4384.6, "end": 4391.0, "text": " the first model for reading comprehension and this not how it's based in the retrieval model", "tokens": [264, 700, 2316, 337, 3760, 44991, 293, 341, 406, 577, 309, 311, 2361, 294, 264, 19817, 3337, 2316], "temperature": 0.0, "avg_logprob": -0.2500044958932059, "compression_ratio": 1.7916666666666667, "no_speech_prob": 1.3207375559431966e-05}, {"id": 676, "seek": 439100, "start": 4391.0, "end": 4396.36, "text": " so to get this in retrieval model working they also try to use the birth to you call the passage", "tokens": [370, 281, 483, 341, 294, 19817, 3337, 2316, 1364, 436, 611, 853, 281, 764, 264, 3965, 281, 291, 818, 264, 11497], "temperature": 0.0, "avg_logprob": -0.24934743881225585, "compression_ratio": 2.0384615384615383, "no_speech_prob": 2.4668968762853183e-05}, {"id": 677, "seek": 439100, "start": 4396.36, "end": 4399.96, "text": " and also you call the question and they try to use a birth product between the question", "tokens": [293, 611, 291, 818, 264, 1168, 293, 436, 853, 281, 764, 257, 3965, 1674, 1296, 264, 1168], "temperature": 0.0, "avg_logprob": -0.24934743881225585, "compression_ratio": 2.0384615384615383, "no_speech_prob": 2.4668968762853183e-05}, {"id": 678, "seek": 439100, "start": 4399.96, "end": 4407.24, "text": " station the passage repetition to model how the relevance the similarity between the question", "tokens": [5214, 264, 11497, 30432, 281, 2316, 577, 264, 32684, 264, 32194, 1296, 264, 1168], "temperature": 0.0, "avg_logprob": -0.24934743881225585, "compression_ratio": 2.0384615384615383, "no_speech_prob": 2.4668968762853183e-05}, {"id": 679, "seek": 439100, "start": 4407.24, "end": 4413.16, "text": " the passage but this is actually a very difficult problem because the scalar scalability of this", "tokens": [264, 11497, 457, 341, 307, 767, 257, 588, 2252, 1154, 570, 264, 39684, 15664, 2310, 295, 341], "temperature": 0.0, "avg_logprob": -0.24934743881225585, "compression_ratio": 2.0384615384615383, "no_speech_prob": 2.4668968762853183e-05}, {"id": 680, "seek": 439100, "start": 4413.16, "end": 4418.84, "text": " problem because there are like 20 million passages in a Wikipedia so it's actually very hard to model", "tokens": [1154, 570, 456, 366, 411, 945, 2459, 31589, 294, 257, 28999, 370, 309, 311, 767, 588, 1152, 281, 2316], "temperature": 0.0, "avg_logprob": -0.24934743881225585, "compression_ratio": 2.0384615384615383, "no_speech_prob": 2.4668968762853183e-05}, {"id": 681, "seek": 441884, "start": 4418.84, "end": 4427.8, "text": " this part but so I encourage you to check out this paper and also on second paper I want to quickly", "tokens": [341, 644, 457, 370, 286, 5373, 291, 281, 1520, 484, 341, 3035, 293, 611, 322, 1150, 3035, 286, 528, 281, 2661], "temperature": 0.0, "avg_logprob": -0.3032821091738614, "compression_ratio": 1.798165137614679, "no_speech_prob": 1.1840492334158625e-05}, {"id": 682, "seek": 441884, "start": 4427.8, "end": 4435.32, "text": " mention is also work ideas on last year it's called the best pass of the retrieval so the idea is", "tokens": [2152, 307, 611, 589, 3487, 322, 1036, 1064, 309, 311, 1219, 264, 1151, 1320, 295, 264, 19817, 3337, 370, 264, 1558, 307], "temperature": 0.0, "avg_logprob": -0.3032821091738614, "compression_ratio": 1.798165137614679, "no_speech_prob": 1.1840492334158625e-05}, {"id": 683, "seek": 441884, "start": 4435.32, "end": 4441.4800000000005, "text": " actually very similar to the the the previous paper because the idea is that is actually much more", "tokens": [767, 588, 2531, 281, 264, 264, 264, 3894, 3035, 570, 264, 1558, 307, 300, 307, 767, 709, 544], "temperature": 0.0, "avg_logprob": -0.3032821091738614, "compression_ratio": 1.798165137614679, "no_speech_prob": 1.1840492334158625e-05}, {"id": 684, "seek": 441884, "start": 4441.4800000000005, "end": 4446.84, "text": " simply by model and very easy very simple straightforward approach the idea is that we can also", "tokens": [2935, 538, 2316, 293, 588, 1858, 588, 2199, 15325, 3109, 264, 1558, 307, 300, 321, 393, 611], "temperature": 0.0, "avg_logprob": -0.3032821091738614, "compression_ratio": 1.798165137614679, "no_speech_prob": 1.1840492334158625e-05}, {"id": 685, "seek": 444684, "start": 4446.84, "end": 4452.04, "text": " really just trend the retrieval part by using two birth models using only the question answer pairs", "tokens": [534, 445, 6028, 264, 19817, 3337, 644, 538, 1228, 732, 3965, 5245, 1228, 787, 264, 1168, 1867, 15494], "temperature": 0.0, "avg_logprob": -0.2552900426528033, "compression_ratio": 1.7706422018348624, "no_speech_prob": 2.2445230570156127e-05}, {"id": 686, "seek": 444684, "start": 4452.6, "end": 4459.64, "text": " and this model can work really well and you can largely all form the traditional IR retrieval models", "tokens": [293, 341, 2316, 393, 589, 534, 731, 293, 291, 393, 11611, 439, 1254, 264, 5164, 16486, 19817, 3337, 5245], "temperature": 0.0, "avg_logprob": -0.2552900426528033, "compression_ratio": 1.7706422018348624, "no_speech_prob": 2.2445230570156127e-05}, {"id": 687, "seek": 444684, "start": 4459.64, "end": 4467.0, "text": " if you see this figure so the blue curve here is a traditional IR approach like a BM25 approach", "tokens": [498, 291, 536, 341, 2573, 370, 264, 3344, 7605, 510, 307, 257, 5164, 16486, 3109, 411, 257, 15901, 6074, 3109], "temperature": 0.0, "avg_logprob": -0.2552900426528033, "compression_ratio": 1.7706422018348624, "no_speech_prob": 2.2445230570156127e-05}, {"id": 688, "seek": 444684, "start": 4467.0, "end": 4472.04, "text": " and the so the other curve the orange curve based training this kind of retrieval is only", "tokens": [293, 264, 370, 264, 661, 7605, 264, 7671, 7605, 2361, 3097, 341, 733, 295, 19817, 3337, 307, 787], "temperature": 0.0, "avg_logprob": -0.2552900426528033, "compression_ratio": 1.7706422018348624, "no_speech_prob": 2.2445230570156127e-05}, {"id": 689, "seek": 447204, "start": 4472.04, "end": 4477.8, "text": " 1000 question answer pairs so by looking at all these different curves basically using different", "tokens": [9714, 1168, 1867, 15494, 370, 538, 1237, 412, 439, 613, 819, 19490, 1936, 1228, 819], "temperature": 0.0, "avg_logprob": -0.23083245909059202, "compression_ratio": 1.632034632034632, "no_speech_prob": 7.405088581435848e-06}, {"id": 690, "seek": 447204, "start": 4477.8, "end": 4484.28, "text": " number of training examples so it's actually largely crazy from the traditional IR models", "tokens": [1230, 295, 3097, 5110, 370, 309, 311, 767, 11611, 3219, 490, 264, 5164, 16486, 5245], "temperature": 0.0, "avg_logprob": -0.23083245909059202, "compression_ratio": 1.632034632034632, "no_speech_prob": 7.405088581435848e-06}, {"id": 691, "seek": 447204, "start": 4489.48, "end": 4493.88, "text": " okay so again really I don't have time to talk about the details of all these approaches", "tokens": [1392, 370, 797, 534, 286, 500, 380, 362, 565, 281, 751, 466, 264, 4365, 295, 439, 613, 11587], "temperature": 0.0, "avg_logprob": -0.23083245909059202, "compression_ratio": 1.632034632034632, "no_speech_prob": 7.405088581435848e-06}, {"id": 692, "seek": 449388, "start": 4493.88, "end": 4501.08, "text": " so I just encourage you to check out this paper this paper is nice and this result is really exciting", "tokens": [370, 286, 445, 5373, 291, 281, 1520, 484, 341, 3035, 341, 3035, 307, 1481, 293, 341, 1874, 307, 534, 4670], "temperature": 0.0, "avg_logprob": -0.313609250386556, "compression_ratio": 1.897560975609756, "no_speech_prob": 2.3169781343312934e-05}, {"id": 693, "seek": 449388, "start": 4502.04, "end": 4508.6, "text": " so here's actually a really nice demo so the demo is actually hosted at this website you", "tokens": [370, 510, 311, 767, 257, 534, 1481, 10723, 370, 264, 10723, 307, 767, 19204, 412, 341, 3144, 291], "temperature": 0.0, "avg_logprob": -0.313609250386556, "compression_ratio": 1.897560975609756, "no_speech_prob": 2.3169781343312934e-05}, {"id": 694, "seek": 449388, "start": 4508.6, "end": 4515.56, "text": " can check out so again so the database here is a whole Wikipedia you can see that if you ask a question", "tokens": [393, 1520, 484, 370, 797, 370, 264, 8149, 510, 307, 257, 1379, 28999, 291, 393, 536, 300, 498, 291, 1029, 257, 1168], "temperature": 0.0, "avg_logprob": -0.313609250386556, "compression_ratio": 1.897560975609756, "no_speech_prob": 2.3169781343312934e-05}, {"id": 695, "seek": 449388, "start": 4515.56, "end": 4521.24, "text": " who tells higher portals that he is a wizard and the higher you know higher portals series and", "tokens": [567, 5112, 2946, 2436, 1124, 300, 415, 307, 257, 25807, 293, 264, 2946, 291, 458, 2946, 2436, 1124, 2638, 293], "temperature": 0.0, "avg_logprob": -0.313609250386556, "compression_ratio": 1.897560975609756, "no_speech_prob": 2.3169781343312934e-05}, {"id": 696, "seek": 452124, "start": 4521.24, "end": 4526.92, "text": " the system I really found out the correct article should be higher portals film series and finally give", "tokens": [264, 1185, 286, 534, 1352, 484, 264, 3006, 7222, 820, 312, 2946, 2436, 1124, 2007, 2638, 293, 2721, 976], "temperature": 0.0, "avg_logprob": -0.2428380812721691, "compression_ratio": 1.8082191780821917, "no_speech_prob": 1.2796696864825208e-05}, {"id": 697, "seek": 452124, "start": 4526.92, "end": 4533.719999999999, "text": " you the correct answer which is exactly what you have seen from the Google example here so the answer", "tokens": [291, 264, 3006, 1867, 597, 307, 2293, 437, 291, 362, 1612, 490, 264, 3329, 1365, 510, 370, 264, 1867], "temperature": 0.0, "avg_logprob": -0.2428380812721691, "compression_ratio": 1.8082191780821917, "no_speech_prob": 1.2796696864825208e-05}, {"id": 698, "seek": 452124, "start": 4533.719999999999, "end": 4539.88, "text": " could be the rubers hybrid which is actually the person who tells higher portals that he is a wizard", "tokens": [727, 312, 264, 5915, 433, 13051, 597, 307, 767, 264, 954, 567, 5112, 2946, 2436, 1124, 300, 415, 307, 257, 25807], "temperature": 0.0, "avg_logprob": -0.2428380812721691, "compression_ratio": 1.8082191780821917, "no_speech_prob": 1.2796696864825208e-05}, {"id": 699, "seek": 452124, "start": 4539.88, "end": 4547.08, "text": " so this is actually the perfect answer to this question okay I'm going to skip this slide", "tokens": [370, 341, 307, 767, 264, 2176, 1867, 281, 341, 1168, 1392, 286, 478, 516, 281, 10023, 341, 4137], "temperature": 0.0, "avg_logprob": -0.2428380812721691, "compression_ratio": 1.8082191780821917, "no_speech_prob": 1.2796696864825208e-05}, {"id": 700, "seek": 454708, "start": 4547.08, "end": 4556.36, "text": " and the final is very quick so so this is something that can out very recently that some researchers", "tokens": [293, 264, 2572, 307, 588, 1702, 370, 370, 341, 307, 746, 300, 393, 484, 588, 3938, 300, 512, 10309], "temperature": 0.0, "avg_logprob": -0.21670032886976606, "compression_ratio": 1.7276785714285714, "no_speech_prob": 3.0216195227694698e-05}, {"id": 701, "seek": 454708, "start": 4556.36, "end": 4562.04, "text": " have demonstrated that maybe you don't even need to do this a retrieval study so you can if you", "tokens": [362, 18772, 300, 1310, 291, 500, 380, 754, 643, 281, 360, 341, 257, 19817, 3337, 2979, 370, 291, 393, 498, 291], "temperature": 0.0, "avg_logprob": -0.21670032886976606, "compression_ratio": 1.7276785714285714, "no_speech_prob": 3.0216195227694698e-05}, {"id": 702, "seek": 454708, "start": 4562.04, "end": 4566.68, "text": " just use a very large language model you can also just do the open domain question answering", "tokens": [445, 764, 257, 588, 2416, 2856, 2316, 291, 393, 611, 445, 360, 264, 1269, 9274, 1168, 13430], "temperature": 0.0, "avg_logprob": -0.21670032886976606, "compression_ratio": 1.7276785714285714, "no_speech_prob": 3.0216195227694698e-05}, {"id": 703, "seek": 454708, "start": 4567.4, "end": 4572.36, "text": " so the way they did this is that I hope that you have learned the TFI model in this class already", "tokens": [370, 264, 636, 436, 630, 341, 307, 300, 286, 1454, 300, 291, 362, 3264, 264, 314, 38568, 2316, 294, 341, 1508, 1217], "temperature": 0.0, "avg_logprob": -0.21670032886976606, "compression_ratio": 1.7276785714285714, "no_speech_prob": 3.0216195227694698e-05}, {"id": 704, "seek": 457236, "start": 4572.36, "end": 4578.12, "text": " so they just take a prediction language model TFI and they're trying to find to this model by taking", "tokens": [370, 436, 445, 747, 257, 17630, 2856, 2316, 314, 38568, 293, 436, 434, 1382, 281, 915, 281, 341, 2316, 538, 1940], "temperature": 0.0, "avg_logprob": -0.22032906792380594, "compression_ratio": 1.90521327014218, "no_speech_prob": 2.281880733789876e-05}, {"id": 705, "seek": 457236, "start": 4578.12, "end": 4585.719999999999, "text": " the question and taking the question as an answer as output without any explicit retrieval and they", "tokens": [264, 1168, 293, 1940, 264, 1168, 382, 364, 1867, 382, 5598, 1553, 604, 13691, 19817, 3337, 293, 436], "temperature": 0.0, "avg_logprob": -0.22032906792380594, "compression_ratio": 1.90521327014218, "no_speech_prob": 2.281880733789876e-05}, {"id": 706, "seek": 457236, "start": 4585.719999999999, "end": 4591.16, "text": " just find to this on the data set and they find this model can be pretty well at the testing time by", "tokens": [445, 915, 281, 341, 322, 264, 1412, 992, 293, 436, 915, 341, 2316, 393, 312, 1238, 731, 412, 264, 4997, 565, 538], "temperature": 0.0, "avg_logprob": -0.22032906792380594, "compression_ratio": 1.90521327014218, "no_speech_prob": 2.281880733789876e-05}, {"id": 707, "seek": 457236, "start": 4591.16, "end": 4597.719999999999, "text": " just taking the question and the directly generous answer without resorting to any like documents or", "tokens": [445, 1940, 264, 1168, 293, 264, 3838, 14537, 1867, 1553, 19606, 278, 281, 604, 411, 8512, 420], "temperature": 0.0, "avg_logprob": -0.22032906792380594, "compression_ratio": 1.90521327014218, "no_speech_prob": 2.281880733789876e-05}, {"id": 708, "seek": 459772, "start": 4597.72, "end": 4602.360000000001, "text": " like a retrieval system so this is actually very amazing so this kind of model is also called", "tokens": [411, 257, 19817, 3337, 1185, 370, 341, 307, 767, 588, 2243, 370, 341, 733, 295, 2316, 307, 611, 1219], "temperature": 0.0, "avg_logprob": -0.3163654143551746, "compression_ratio": 1.7570093457943925, "no_speech_prob": 7.067821115924744e-06}, {"id": 709, "seek": 459772, "start": 4603.240000000001, "end": 4613.72, "text": " close book Q-assistence okay very long the last life so so this is one direction and personally", "tokens": [1998, 1446, 1249, 12, 640, 468, 655, 1392, 588, 938, 264, 1036, 993, 370, 370, 341, 307, 472, 3513, 293, 5665], "temperature": 0.0, "avg_logprob": -0.3163654143551746, "compression_ratio": 1.7570093457943925, "no_speech_prob": 7.067821115924744e-06}, {"id": 710, "seek": 459772, "start": 4613.72, "end": 4619.4800000000005, "text": " I'm very excited about so this is actually a new direction that it basically shows that maybe", "tokens": [286, 478, 588, 2919, 466, 370, 341, 307, 767, 257, 777, 3513, 300, 309, 1936, 3110, 300, 1310], "temperature": 0.0, "avg_logprob": -0.3163654143551746, "compression_ratio": 1.7570093457943925, "no_speech_prob": 7.067821115924744e-06}, {"id": 711, "seek": 459772, "start": 4619.4800000000005, "end": 4624.04, "text": " for the open domain question answering maybe this rhythm model is also not necessary anymore", "tokens": [337, 264, 1269, 9274, 1168, 13430, 1310, 341, 11801, 2316, 307, 611, 406, 4818, 3602], "temperature": 0.0, "avg_logprob": -0.3163654143551746, "compression_ratio": 1.7570093457943925, "no_speech_prob": 7.067821115924744e-06}, {"id": 712, "seek": 462404, "start": 4624.04, "end": 4631.96, "text": " so so this idea was first proposed by a museum in 2019 and we recently wrote a paper called dense", "tokens": [370, 370, 341, 1558, 390, 700, 10348, 538, 257, 8441, 294, 6071, 293, 321, 3938, 4114, 257, 3035, 1219, 18011], "temperature": 0.0, "avg_logprob": -0.29217577528679506, "compression_ratio": 1.6508620689655173, "no_speech_prob": 2.5448427550145425e-05}, {"id": 713, "seek": 462404, "start": 4631.96, "end": 4638.12, "text": " phrases that try to demonstrate that maybe it doesn't even need this like a rhythm model", "tokens": [20312, 300, 853, 281, 11698, 300, 1310, 309, 1177, 380, 754, 643, 341, 411, 257, 11801, 2316], "temperature": 0.0, "avg_logprob": -0.29217577528679506, "compression_ratio": 1.6508620689655173, "no_speech_prob": 2.5448427550145425e-05}, {"id": 714, "seek": 462404, "start": 4638.12, "end": 4645.08, "text": " so instead we can just you you code all the phrases in Wikipedia using some kind of dense letters", "tokens": [370, 2602, 321, 393, 445, 291, 291, 3089, 439, 264, 20312, 294, 28999, 1228, 512, 733, 295, 18011, 7825], "temperature": 0.0, "avg_logprob": -0.29217577528679506, "compression_ratio": 1.6508620689655173, "no_speech_prob": 2.5448427550145425e-05}, {"id": 715, "seek": 462404, "start": 4645.08, "end": 4650.28, "text": " so what you just need to do is just to do this kind of nearest neighbor search in the answer space", "tokens": [370, 437, 291, 445, 643, 281, 360, 307, 445, 281, 360, 341, 733, 295, 23831, 5987, 3164, 294, 264, 1867, 1901], "temperature": 0.0, "avg_logprob": -0.29217577528679506, "compression_ratio": 1.6508620689655173, "no_speech_prob": 2.5448427550145425e-05}, {"id": 716, "seek": 465028, "start": 4650.28, "end": 4656.599999999999, "text": " you just encode all encode all the phrases in Wikipedia encodes and using vectors and by taking a", "tokens": [291, 445, 2058, 1429, 439, 2058, 1429, 439, 264, 20312, 294, 28999, 2058, 4789, 293, 1228, 18875, 293, 538, 1940, 257], "temperature": 0.0, "avg_logprob": -0.2023900349934896, "compression_ratio": 2.004166666666667, "no_speech_prob": 1.9823823095066473e-05}, {"id": 717, "seek": 465028, "start": 4656.599999999999, "end": 4661.4, "text": " question you can just encode this question using a vector and then we can just do the vector the", "tokens": [1168, 291, 393, 445, 2058, 1429, 341, 1168, 1228, 257, 8062, 293, 550, 321, 393, 445, 360, 264, 8062, 264], "temperature": 0.0, "avg_logprob": -0.2023900349934896, "compression_ratio": 2.004166666666667, "no_speech_prob": 1.9823823095066473e-05}, {"id": 718, "seek": 465028, "start": 4661.4, "end": 4667.16, "text": " nearest neighbor search and then you can directly give you the answer so this is a bit of a new", "tokens": [23831, 5987, 3164, 293, 550, 291, 393, 3838, 976, 291, 264, 1867, 370, 341, 307, 257, 857, 295, 257, 777], "temperature": 0.0, "avg_logprob": -0.2023900349934896, "compression_ratio": 2.004166666666667, "no_speech_prob": 1.9823823095066473e-05}, {"id": 719, "seek": 465028, "start": 4667.16, "end": 4672.12, "text": " paradigm of this kind of the question answer model so you don't need the you just need a retrieval", "tokens": [24709, 295, 341, 733, 295, 264, 1168, 1867, 2316, 370, 291, 500, 380, 643, 264, 291, 445, 643, 257, 19817, 3337], "temperature": 0.0, "avg_logprob": -0.2023900349934896, "compression_ratio": 2.004166666666667, "no_speech_prob": 1.9823823095066473e-05}, {"id": 720, "seek": 465028, "start": 4672.12, "end": 4678.04, "text": " you don't need a rhythm so good a great advantage for doing this is that so for the perfect", "tokens": [291, 500, 380, 643, 257, 11801, 370, 665, 257, 869, 5002, 337, 884, 341, 307, 300, 370, 337, 264, 2176], "temperature": 0.0, "avg_logprob": -0.2023900349934896, "compression_ratio": 2.004166666666667, "no_speech_prob": 1.9823823095066473e-05}, {"id": 721, "seek": 467804, "start": 4678.04, "end": 4682.04, "text": " rhythm model essentially you have to run a very model at the entrance time this is actually very", "tokens": [11801, 2316, 4476, 291, 362, 281, 1190, 257, 588, 2316, 412, 264, 12014, 565, 341, 307, 767, 588], "temperature": 0.0, "avg_logprob": -0.23442206382751465, "compression_ratio": 1.7655502392344498, "no_speech_prob": 2.3541771952295676e-05}, {"id": 722, "seek": 467804, "start": 4682.04, "end": 4689.32, "text": " expensive you can just do the similarity search you can just do the nearest neighbor search", "tokens": [5124, 291, 393, 445, 360, 264, 32194, 3164, 291, 393, 445, 360, 264, 23831, 5987, 3164], "temperature": 0.0, "avg_logprob": -0.23442206382751465, "compression_ratio": 1.7655502392344498, "no_speech_prob": 2.3541771952295676e-05}, {"id": 723, "seek": 467804, "start": 4689.32, "end": 4694.6, "text": " without running a burden model so this could be very fast and it can even run on the CPUs", "tokens": [1553, 2614, 257, 12578, 2316, 370, 341, 727, 312, 588, 2370, 293, 309, 393, 754, 1190, 322, 264, 13199, 82], "temperature": 0.0, "avg_logprob": -0.23442206382751465, "compression_ratio": 1.7655502392344498, "no_speech_prob": 2.3541771952295676e-05}, {"id": 724, "seek": 467804, "start": 4694.6, "end": 4701.08, "text": " without needing to run like a very expensive different neural network and it can still run", "tokens": [1553, 18006, 281, 1190, 411, 257, 588, 5124, 819, 18161, 3209, 293, 309, 393, 920, 1190], "temperature": 0.0, "avg_logprob": -0.23442206382751465, "compression_ratio": 1.7655502392344498, "no_speech_prob": 2.3541771952295676e-05}, {"id": 725, "seek": 470108, "start": 4701.08, "end": 4709.0, "text": " very well perfect very well okay finally I hope this works so I actually prepared a", "tokens": [588, 731, 2176, 588, 731, 1392, 2721, 286, 1454, 341, 1985, 370, 286, 767, 4927, 257], "temperature": 0.0, "avg_logprob": -0.46116345723470054, "compression_ratio": 1.5705128205128205, "no_speech_prob": 2.4273798771901056e-05}, {"id": 726, "seek": 470108, "start": 4709.0, "end": 4713.32, "text": " memo for this dance versus so I want to show you how this actually works", "tokens": [35900, 337, 341, 4489, 5717, 370, 286, 528, 281, 855, 291, 577, 341, 767, 1985], "temperature": 0.0, "avg_logprob": -0.46116345723470054, "compression_ratio": 1.5705128205128205, "no_speech_prob": 2.4273798771901056e-05}, {"id": 727, "seek": 470108, "start": 4723.08, "end": 4728.44, "text": " so you can see that I've been trying this question like who on the not no bell prize EPs", "tokens": [370, 291, 393, 536, 300, 286, 600, 668, 1382, 341, 1168, 411, 567, 322, 264, 406, 572, 4549, 12818, 462, 23043], "temperature": 0.0, "avg_logprob": -0.46116345723470054, "compression_ratio": 1.5705128205128205, "no_speech_prob": 2.4273798771901056e-05}, {"id": 728, "seek": 472844, "start": 4728.44, "end": 4734.759999999999, "text": " into 2014 so everything just tied for little piece of the input question and all this system", "tokens": [666, 8227, 370, 1203, 445, 9601, 337, 707, 2522, 295, 264, 4846, 1168, 293, 439, 341, 1185], "temperature": 0.0, "avg_logprob": -0.36588144826365043, "compression_ratio": 1.7072072072072073, "no_speech_prob": 2.8842830943176523e-05}, {"id": 729, "seek": 472844, "start": 4734.759999999999, "end": 4740.679999999999, "text": " can basically just to find out the answer the relevant test test it is and the family's answer", "tokens": [393, 1936, 445, 281, 915, 484, 264, 1867, 264, 7340, 1500, 1500, 309, 307, 293, 264, 1605, 311, 1867], "temperature": 0.0, "avg_logprob": -0.36588144826365043, "compression_ratio": 1.7072072072072073, "no_speech_prob": 2.8842830943176523e-05}, {"id": 730, "seek": 472844, "start": 4740.679999999999, "end": 4745.16, "text": " is actually it shows up it's actually very fast because it's a bit of real time we don't we don't", "tokens": [307, 767, 309, 3110, 493, 309, 311, 767, 588, 2370, 570, 309, 311, 257, 857, 295, 957, 565, 321, 500, 380, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.36588144826365043, "compression_ratio": 1.7072072072072073, "no_speech_prob": 2.8842830943176523e-05}, {"id": 731, "seek": 472844, "start": 4745.16, "end": 4753.0, "text": " do wrong in the version model so it's just a ritual model here okay I'm actually done this is", "tokens": [360, 2085, 294, 264, 3037, 2316, 370, 309, 311, 445, 257, 13792, 2316, 510, 1392, 286, 478, 767, 1096, 341, 307], "temperature": 0.0, "avg_logprob": -0.36588144826365043, "compression_ratio": 1.7072072072072073, "no_speech_prob": 2.8842830943176523e-05}, {"id": 732, "seek": 475300, "start": 4753.0, "end": 4761.4, "text": " lecture so they are you're 515 now yeah thank you very much Dan chief that awesome survey of", "tokens": [7991, 370, 436, 366, 291, 434, 1025, 5211, 586, 1338, 1309, 291, 588, 709, 3394, 9588, 300, 3476, 8984, 295], "temperature": 0.0, "avg_logprob": -0.26420105071294875, "compression_ratio": 1.6255506607929515, "no_speech_prob": 9.697285713627934e-05}, {"id": 733, "seek": 475300, "start": 4762.68, "end": 4766.68, "text": " question answering I guess given that demo at the end people will want to know whether you're", "tokens": [1168, 13430, 286, 2041, 2212, 300, 10723, 412, 264, 917, 561, 486, 528, 281, 458, 1968, 291, 434], "temperature": 0.0, "avg_logprob": -0.26420105071294875, "compression_ratio": 1.6255506607929515, "no_speech_prob": 9.697285713627934e-05}, {"id": 734, "seek": 475300, "start": 4766.68, "end": 4775.72, "text": " launching your own search engine soon but at any rate Dan chief can stay for a bit to answer", "tokens": [18354, 428, 1065, 3164, 2848, 2321, 457, 412, 604, 3314, 3394, 9588, 393, 1754, 337, 257, 857, 281, 1867], "temperature": 0.0, "avg_logprob": -0.26420105071294875, "compression_ratio": 1.6255506607929515, "no_speech_prob": 9.697285713627934e-05}, {"id": 735, "seek": 475300, "start": 4775.72, "end": 4782.44, "text": " questions but not forever but today because of you know she doesn't have a standard login", "tokens": [1651, 457, 406, 5680, 457, 965, 570, 295, 291, 458, 750, 1177, 380, 362, 257, 3832, 24276], "temperature": 0.0, "avg_logprob": -0.26420105071294875, "compression_ratio": 1.6255506607929515, "no_speech_prob": 9.697285713627934e-05}, {"id": 736, "seek": 478244, "start": 4782.44, "end": 4790.04, "text": " we're going to do questions inside zoom so if you'd like to ask a question if you use the raise hand", "tokens": [321, 434, 516, 281, 360, 1651, 1854, 8863, 370, 498, 291, 1116, 411, 281, 1029, 257, 1168, 498, 291, 764, 264, 5300, 1011], "temperature": 0.0, "avg_logprob": -0.06697044769922893, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00014983263099566102}, {"id": 737, "seek": 478244, "start": 4790.04, "end": 4797.4, "text": " button we can promote you so that you appear in the regular zoom window and can just ask questions", "tokens": [2960, 321, 393, 9773, 291, 370, 300, 291, 4204, 294, 264, 3890, 8863, 4910, 293, 393, 445, 1029, 1651], "temperature": 0.0, "avg_logprob": -0.06697044769922893, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00014983263099566102}, {"id": 738, "seek": 478244, "start": 4797.4, "end": 4805.32, "text": " and see each other and if you hang around and don't leave the zoom for more than a few minutes maybe", "tokens": [293, 536, 1184, 661, 293, 498, 291, 3967, 926, 293, 500, 380, 1856, 264, 8863, 337, 544, 813, 257, 1326, 2077, 1310], "temperature": 0.0, "avg_logprob": -0.06697044769922893, "compression_ratio": 1.6853932584269662, "no_speech_prob": 0.00014983263099566102}, {"id": 739, "seek": 480532, "start": 4805.32, "end": 4813.0, "text": " we'll just promote everybody who's still there into people in the regular zoom for some bits of", "tokens": [321, 603, 445, 9773, 2201, 567, 311, 920, 456, 666, 561, 294, 264, 3890, 8863, 337, 512, 9239, 295], "temperature": 0.0, "avg_logprob": -0.14803777892014075, "compression_ratio": 1.5365853658536586, "no_speech_prob": 5.4676056606695056e-05}, {"id": 740, "seek": 480532, "start": 4813.0, "end": 4820.44, "text": " discussion but we'd welcome anyone who'd like to ask a question by asking it themselves at this point", "tokens": [5017, 457, 321, 1116, 2928, 2878, 567, 1116, 411, 281, 1029, 257, 1168, 538, 3365, 309, 2969, 412, 341, 935], "temperature": 0.0, "avg_logprob": -0.14803777892014075, "compression_ratio": 1.5365853658536586, "no_speech_prob": 5.4676056606695056e-05}, {"id": 741, "seek": 482044, "start": 4820.44, "end": 4832.599999999999, "text": " okay I've got a one volunteer I've got more volunteers", "tokens": [1392, 286, 600, 658, 257, 472, 13835, 286, 600, 658, 544, 14352], "temperature": 0.0, "avg_logprob": -0.402385851232017, "compression_ratio": 1.4368932038834952, "no_speech_prob": 0.00026791420532390475}, {"id": 742, "seek": 482044, "start": 4835.24, "end": 4841.879999999999, "text": " sure I mean so questions oh sure look at a chev or I mean so there are now four people who've", "tokens": [988, 286, 914, 370, 1651, 1954, 988, 574, 412, 257, 947, 85, 420, 286, 914, 370, 456, 366, 586, 1451, 561, 567, 600], "temperature": 0.0, "avg_logprob": -0.402385851232017, "compression_ratio": 1.4368932038834952, "no_speech_prob": 0.00026791420532390475}, {"id": 743, "seek": 484188, "start": 4841.88, "end": 4851.4800000000005, "text": " been promoted there four people was the first so maybe he could start by asking a question and then", "tokens": [668, 21162, 456, 1451, 561, 390, 264, 700, 370, 1310, 415, 727, 722, 538, 3365, 257, 1168, 293, 550], "temperature": 0.0, "avg_logprob": -0.21128468146690957, "compression_ratio": 1.6123595505617978, "no_speech_prob": 9.149751713266596e-05}, {"id": 744, "seek": 484188, "start": 4851.4800000000005, "end": 4859.400000000001, "text": " the other people that we've promoted okay so thank you so much for the lecture today my question", "tokens": [264, 661, 561, 300, 321, 600, 21162, 1392, 370, 1309, 291, 370, 709, 337, 264, 7991, 965, 452, 1168], "temperature": 0.0, "avg_logprob": -0.21128468146690957, "compression_ratio": 1.6123595505617978, "no_speech_prob": 9.149751713266596e-05}, {"id": 745, "seek": 484188, "start": 4859.400000000001, "end": 4866.36, "text": " is mainly like if you use like a model mate for example Burke how a small kind of training", "tokens": [307, 8704, 411, 498, 291, 764, 411, 257, 2316, 11709, 337, 1365, 37396, 577, 257, 1359, 733, 295, 3097], "temperature": 0.0, "avg_logprob": -0.21128468146690957, "compression_ratio": 1.6123595505617978, "no_speech_prob": 9.149751713266596e-05}, {"id": 746, "seek": 486636, "start": 4866.36, "end": 4875.639999999999, "text": " dataset be really to get like reasonable results so the question is how we can try to", "tokens": [28872, 312, 534, 281, 483, 411, 10585, 3542, 370, 264, 1168, 307, 577, 321, 393, 853, 281], "temperature": 0.0, "avg_logprob": -0.479947707232307, "compression_ratio": 1.65625, "no_speech_prob": 0.00010710610513342544}, {"id": 747, "seek": 486636, "start": 4875.639999999999, "end": 4883.0, "text": " recover a single model using only a small number of training samples yeah I think it's a really", "tokens": [8114, 257, 2167, 2316, 1228, 787, 257, 1359, 1230, 295, 3097, 10938, 1338, 286, 519, 309, 311, 257, 534], "temperature": 0.0, "avg_logprob": -0.479947707232307, "compression_ratio": 1.65625, "no_speech_prob": 0.00010710610513342544}, {"id": 748, "seek": 486636, "start": 4883.0, "end": 4891.08, "text": " good question especially like you probably have heard the GPT stream model side you show that", "tokens": [665, 1168, 2318, 411, 291, 1391, 362, 2198, 264, 26039, 51, 4309, 2316, 1252, 291, 855, 300], "temperature": 0.0, "avg_logprob": -0.479947707232307, "compression_ratio": 1.65625, "no_speech_prob": 0.00010710610513342544}, {"id": 749, "seek": 486636, "start": 4891.08, "end": 4895.96, "text": " if you only use like a few very few examples you can also do the open-to-one question answering", "tokens": [498, 291, 787, 764, 411, 257, 1326, 588, 1326, 5110, 291, 393, 611, 360, 264, 1269, 12, 1353, 12, 546, 1168, 13430], "temperature": 0.0, "avg_logprob": -0.479947707232307, "compression_ratio": 1.65625, "no_speech_prob": 0.00010710610513342544}, {"id": 750, "seek": 489596, "start": 4895.96, "end": 4905.0, "text": " pretty well so but this kind of model is huge like what numbers like how many parameters I've", "tokens": [1238, 731, 370, 457, 341, 733, 295, 2316, 307, 2603, 411, 437, 3547, 411, 577, 867, 9834, 286, 600], "temperature": 0.0, "avg_logprob": -0.335197834486372, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.211196806456428e-05}, {"id": 751, "seek": 489596, "start": 4905.0, "end": 4910.76, "text": " got in the GPT stream model yeah so it's a very large very few model but okay so this", "tokens": [658, 294, 264, 26039, 51, 4309, 2316, 1338, 370, 309, 311, 257, 588, 2416, 588, 1326, 2316, 457, 1392, 370, 341], "temperature": 0.0, "avg_logprob": -0.335197834486372, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.211196806456428e-05}, {"id": 752, "seek": 489596, "start": 4910.76, "end": 4917.16, "text": " amount is that if we can leverage a very large and very powerful precision-longed model there is a", "tokens": [2372, 307, 300, 498, 321, 393, 13982, 257, 588, 2416, 293, 588, 4005, 18356, 12, 13025, 292, 2316, 456, 307, 257], "temperature": 0.0, "avg_logprob": -0.335197834486372, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.211196806456428e-05}, {"id": 753, "seek": 489596, "start": 4917.16, "end": 4923.08, "text": " way that we are there is a possibility that we can actually do the question stream well we", "tokens": [636, 300, 321, 366, 456, 307, 257, 7959, 300, 321, 393, 767, 360, 264, 1168, 4309, 731, 321], "temperature": 0.0, "avg_logprob": -0.335197834486372, "compression_ratio": 1.6772727272727272, "no_speech_prob": 2.211196806456428e-05}, {"id": 754, "seek": 492308, "start": 4923.08, "end": 4928.92, "text": " only have small number examples and also there are some other promising directions including like", "tokens": [787, 362, 1359, 1230, 5110, 293, 611, 456, 366, 512, 661, 20257, 11095, 3009, 411], "temperature": 0.0, "avg_logprob": -0.3099038407609269, "compression_ratio": 1.895, "no_speech_prob": 1.9814169718301855e-05}, {"id": 755, "seek": 492308, "start": 4928.92, "end": 4936.6, "text": " on supervised passion three so by using some kind of approach like the from the machine on supervised", "tokens": [322, 46533, 5418, 1045, 370, 538, 1228, 512, 733, 295, 3109, 411, 264, 490, 264, 3479, 322, 46533], "temperature": 0.0, "avg_logprob": -0.3099038407609269, "compression_ratio": 1.895, "no_speech_prob": 1.9814169718301855e-05}, {"id": 756, "seek": 492308, "start": 4936.6, "end": 4942.92, "text": " machine translation this kind of idea that can be borrowed and by yeah the kind of borrow ideas", "tokens": [3479, 12853, 341, 733, 295, 1558, 300, 393, 312, 26805, 293, 538, 1338, 264, 733, 295, 11172, 3487], "temperature": 0.0, "avg_logprob": -0.3099038407609269, "compression_ratio": 1.895, "no_speech_prob": 1.9814169718301855e-05}, {"id": 757, "seek": 492308, "start": 4944.6, "end": 4949.5599999999995, "text": " can also work pretty well reasonable reasonably well in on supervised passion three", "tokens": [393, 611, 589, 1238, 731, 10585, 23551, 731, 294, 322, 46533, 5418, 1045], "temperature": 0.0, "avg_logprob": -0.3099038407609269, "compression_ratio": 1.895, "no_speech_prob": 1.9814169718301855e-05}, {"id": 758, "seek": 494956, "start": 4949.56, "end": 4958.120000000001, "text": " yeah also I have seen some of our other works like very pretty showing that since that you clearly", "tokens": [1338, 611, 286, 362, 1612, 512, 295, 527, 661, 1985, 411, 588, 1238, 4099, 300, 1670, 300, 291, 4448], "temperature": 0.0, "avg_logprob": -0.3561786242893764, "compression_ratio": 1.68, "no_speech_prob": 7.480754720745608e-05}, {"id": 759, "seek": 494956, "start": 4958.120000000001, "end": 4963.320000000001, "text": " assess how it's also quite helpful not imposing the performance if you don't have enough", "tokens": [5877, 577, 309, 311, 611, 1596, 4961, 406, 40288, 264, 3389, 498, 291, 500, 380, 362, 1547], "temperature": 0.0, "avg_logprob": -0.3561786242893764, "compression_ratio": 1.68, "no_speech_prob": 7.480754720745608e-05}, {"id": 760, "seek": 494956, "start": 4963.96, "end": 4971.64, "text": " supervised data sets so nice examples yeah so my question is it's I guess it's kind of interesting", "tokens": [46533, 1412, 6352, 370, 1481, 5110, 1338, 370, 452, 1168, 307, 309, 311, 286, 2041, 309, 311, 733, 295, 1880], "temperature": 0.0, "avg_logprob": -0.3561786242893764, "compression_ratio": 1.68, "no_speech_prob": 7.480754720745608e-05}, {"id": 761, "seek": 494956, "start": 4971.64, "end": 4978.280000000001, "text": " that there's not really that strong of a transfer effect between data sets that are kind of", "tokens": [300, 456, 311, 406, 534, 300, 2068, 295, 257, 5003, 1802, 1296, 1412, 6352, 300, 366, 733, 295], "temperature": 0.0, "avg_logprob": -0.3561786242893764, "compression_ratio": 1.68, "no_speech_prob": 7.480754720745608e-05}, {"id": 762, "seek": 497828, "start": 4978.28, "end": 4986.28, "text": " ostensibly similar so my question is like has there been any research done on how close I", "tokens": [32946, 694, 3545, 2531, 370, 452, 1168, 307, 411, 575, 456, 668, 604, 2132, 1096, 322, 577, 1998, 286], "temperature": 0.0, "avg_logprob": -0.1233518123626709, "compression_ratio": 1.6055555555555556, "no_speech_prob": 4.682760845753364e-05}, {"id": 763, "seek": 497828, "start": 4986.28, "end": 4993.32, "text": " guess like the formatting and the semantic content of these question answering data sets actually", "tokens": [2041, 411, 264, 39366, 293, 264, 47982, 2701, 295, 613, 1168, 13430, 1412, 6352, 767], "temperature": 0.0, "avg_logprob": -0.1233518123626709, "compression_ratio": 1.6055555555555556, "no_speech_prob": 4.682760845753364e-05}, {"id": 764, "seek": 497828, "start": 4993.32, "end": 5001.96, "text": " adheres to the data that like BERT is pre-trained on and if so like has there been sort of any effect", "tokens": [614, 19464, 281, 264, 1412, 300, 411, 363, 31479, 307, 659, 12, 17227, 2001, 322, 293, 498, 370, 411, 575, 456, 668, 1333, 295, 604, 1802], "temperature": 0.0, "avg_logprob": -0.1233518123626709, "compression_ratio": 1.6055555555555556, "no_speech_prob": 4.682760845753364e-05}, {"id": 765, "seek": 500196, "start": 5001.96, "end": 5010.6, "text": " found between those similarities or differences I use a question asking like there has been like", "tokens": [1352, 1296, 729, 24197, 420, 7300, 286, 764, 257, 1168, 3365, 411, 456, 575, 668, 411], "temperature": 0.0, "avg_logprob": -0.46013450622558594, "compression_ratio": 1.688073394495413, "no_speech_prob": 2.4284356186399236e-05}, {"id": 766, "seek": 500196, "start": 5010.6, "end": 5017.16, "text": " a stonk cap okay maybe I can just try to clarify it but why the current models can already", "tokens": [257, 342, 266, 74, 1410, 1392, 1310, 286, 393, 445, 853, 281, 17594, 309, 457, 983, 264, 2190, 5245, 393, 1217], "temperature": 0.0, "avg_logprob": -0.46013450622558594, "compression_ratio": 1.688073394495413, "no_speech_prob": 2.4284356186399236e-05}, {"id": 767, "seek": 500196, "start": 5017.16, "end": 5024.92, "text": " generalize well from one data set from the data set yeah so I actually really believe that", "tokens": [2674, 1125, 731, 490, 472, 1412, 992, 490, 264, 1412, 992, 1338, 370, 286, 767, 534, 1697, 300], "temperature": 0.0, "avg_logprob": -0.46013450622558594, "compression_ratio": 1.688073394495413, "no_speech_prob": 2.4284356186399236e-05}, {"id": 768, "seek": 500196, "start": 5024.92, "end": 5029.64, "text": " most existing question stream data set already comprehension data set have been collected", "tokens": [881, 6741, 1168, 4309, 1412, 992, 1217, 44991, 1412, 992, 362, 668, 11087], "temperature": 0.0, "avg_logprob": -0.46013450622558594, "compression_ratio": 1.688073394495413, "no_speech_prob": 2.4284356186399236e-05}, {"id": 769, "seek": 502964, "start": 5029.64, "end": 5036.6, "text": " from the kind of perk so it's very hard it's very difficult to avoid some kind of artifact or", "tokens": [490, 264, 733, 295, 38839, 370, 309, 311, 588, 1152, 309, 311, 588, 2252, 281, 5042, 512, 733, 295, 34806, 420], "temperature": 0.0, "avg_logprob": -0.3307869389372052, "compression_ratio": 1.887966804979253, "no_speech_prob": 1.3831473552272655e-05}, {"id": 770, "seek": 502964, "start": 5036.6, "end": 5043.400000000001, "text": " like a simple clue or super visual clue that is not super visual but some simple clue that", "tokens": [411, 257, 2199, 13602, 420, 1687, 5056, 13602, 300, 307, 406, 1687, 5056, 457, 512, 2199, 13602, 300], "temperature": 0.0, "avg_logprob": -0.3307869389372052, "compression_ratio": 1.887966804979253, "no_speech_prob": 1.3831473552272655e-05}, {"id": 771, "seek": 502964, "start": 5043.400000000001, "end": 5049.72, "text": " for the machines to pick up so for let's take those photos example set so it has to be that", "tokens": [337, 264, 8379, 281, 1888, 493, 370, 337, 718, 311, 747, 729, 5787, 1365, 992, 370, 309, 575, 281, 312, 300], "temperature": 0.0, "avg_logprob": -0.3307869389372052, "compression_ratio": 1.887966804979253, "no_speech_prob": 1.3831473552272655e-05}, {"id": 772, "seek": 502964, "start": 5049.72, "end": 5054.12, "text": " actually if you look at the data set more closely there has been a lot of examples that the", "tokens": [767, 498, 291, 574, 412, 264, 1412, 992, 544, 8185, 456, 575, 668, 257, 688, 295, 5110, 300, 264], "temperature": 0.0, "avg_logprob": -0.3307869389372052, "compression_ratio": 1.887966804979253, "no_speech_prob": 1.3831473552272655e-05}, {"id": 773, "seek": 505412, "start": 5054.12, "end": 5059.88, "text": " question had been like a lot overlap in terms of the words between the question and the passage so", "tokens": [1168, 632, 668, 411, 257, 688, 19959, 294, 2115, 295, 264, 2283, 1296, 264, 1168, 293, 264, 11497, 370], "temperature": 0.0, "avg_logprob": -0.24673616745892693, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.365105142933317e-06}, {"id": 774, "seek": 505412, "start": 5059.88, "end": 5067.08, "text": " the model is actually very good at picking up this kind of clues to get very high performance on", "tokens": [264, 2316, 307, 767, 588, 665, 412, 8867, 493, 341, 733, 295, 20936, 281, 483, 588, 1090, 3389, 322], "temperature": 0.0, "avg_logprob": -0.24673616745892693, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.365105142933317e-06}, {"id": 775, "seek": 505412, "start": 5067.08, "end": 5074.599999999999, "text": " this data set and another data set is called job so it's basically about comparison the two numbers", "tokens": [341, 1412, 992, 293, 1071, 1412, 992, 307, 1219, 1691, 370, 309, 311, 1936, 466, 9660, 264, 732, 3547], "temperature": 0.0, "avg_logprob": -0.24673616745892693, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.365105142933317e-06}, {"id": 776, "seek": 505412, "start": 5074.599999999999, "end": 5080.599999999999, "text": " something like that so that's the reason that one specialized model that has been very well in", "tokens": [746, 411, 300, 370, 300, 311, 264, 1778, 300, 472, 19813, 2316, 300, 575, 668, 588, 731, 294], "temperature": 0.0, "avg_logprob": -0.24673616745892693, "compression_ratio": 1.7333333333333334, "no_speech_prob": 9.365105142933317e-06}, {"id": 777, "seek": 508060, "start": 5080.6, "end": 5086.360000000001, "text": " one one data set is very easy to pick up this kind of clues then there is a very hard to generalize", "tokens": [472, 472, 1412, 992, 307, 588, 1858, 281, 1888, 493, 341, 733, 295, 20936, 550, 456, 307, 257, 588, 1152, 281, 2674, 1125], "temperature": 0.0, "avg_logprob": -0.2244816745620176, "compression_ratio": 1.784688995215311, "no_speech_prob": 1.3825726455252152e-05}, {"id": 778, "seek": 508060, "start": 5086.360000000001, "end": 5091.64, "text": " this kind of thing to another dataset what about the natural questions data set doesn't", "tokens": [341, 733, 295, 551, 281, 1071, 28872, 437, 466, 264, 3303, 1651, 1412, 992, 1177, 380], "temperature": 0.0, "avg_logprob": -0.2244816745620176, "compression_ratio": 1.784688995215311, "no_speech_prob": 1.3825726455252152e-05}, {"id": 779, "seek": 508060, "start": 5091.64, "end": 5099.4800000000005, "text": " better avoid that objection yeah natural questions would be much better but there are some other", "tokens": [1101, 5042, 300, 35756, 1338, 3303, 1651, 576, 312, 709, 1101, 457, 456, 366, 512, 661], "temperature": 0.0, "avg_logprob": -0.2244816745620176, "compression_ratio": 1.784688995215311, "no_speech_prob": 1.3825726455252152e-05}, {"id": 780, "seek": 508060, "start": 5099.4800000000005, "end": 5104.84, "text": " issues I'm not sure you have seen that there are the recent paper called like a question", "tokens": [2663, 286, 478, 406, 988, 291, 362, 1612, 300, 456, 366, 264, 5162, 3035, 1219, 411, 257, 1168], "temperature": 0.0, "avg_logprob": -0.2244816745620176, "compression_ratio": 1.784688995215311, "no_speech_prob": 1.3825726455252152e-05}, {"id": 781, "seek": 510484, "start": 5104.84, "end": 5111.56, "text": " or a trend passed overlap paper so that means it demonstrate natural questions was a", "tokens": [420, 257, 6028, 4678, 19959, 3035, 370, 300, 1355, 309, 11698, 3303, 1651, 390, 257], "temperature": 0.0, "avg_logprob": -0.2953449249267578, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.801053440431133e-05}, {"id": 782, "seek": 510484, "start": 5111.56, "end": 5117.8, "text": " data set that Google put out about a year and a half ago maybe where they were actually taking", "tokens": [1412, 992, 300, 3329, 829, 484, 466, 257, 1064, 293, 257, 1922, 2057, 1310, 689, 436, 645, 767, 1940], "temperature": 0.0, "avg_logprob": -0.2953449249267578, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.801053440431133e-05}, {"id": 783, "seek": 510484, "start": 5117.8, "end": 5125.64, "text": " real questions from Google search logs and then finding answers trying to find answers for them", "tokens": [957, 1651, 490, 3329, 3164, 20820, 293, 550, 5006, 6338, 1382, 281, 915, 6338, 337, 552], "temperature": 0.0, "avg_logprob": -0.2953449249267578, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.801053440431133e-05}, {"id": 784, "seek": 510484, "start": 5125.64, "end": 5133.08, "text": " in web documents sorry go on Dachi oh I just want to see yeah I think the definite natural", "tokens": [294, 3670, 8512, 2597, 352, 322, 413, 21791, 1954, 286, 445, 528, 281, 536, 1338, 286, 519, 264, 25131, 3303], "temperature": 0.0, "avg_logprob": -0.2953449249267578, "compression_ratio": 1.6712328767123288, "no_speech_prob": 1.801053440431133e-05}, {"id": 785, "seek": 513308, "start": 5133.08, "end": 5137.08, "text": " questions is on much better data set because the questions are natural like you're collected", "tokens": [1651, 307, 322, 709, 1101, 1412, 992, 570, 264, 1651, 366, 3303, 411, 291, 434, 11087], "temperature": 0.0, "avg_logprob": -0.329346264109892, "compression_ratio": 1.8795180722891567, "no_speech_prob": 3.587630999390967e-05}, {"id": 786, "seek": 513308, "start": 5137.88, "end": 5143.72, "text": " are real like real questions that are asking by like users so it kind of avoid this kind of", "tokens": [366, 957, 411, 957, 1651, 300, 366, 3365, 538, 411, 5022, 370, 309, 733, 295, 5042, 341, 733, 295], "temperature": 0.0, "avg_logprob": -0.329346264109892, "compression_ratio": 1.8795180722891567, "no_speech_prob": 3.587630999390967e-05}, {"id": 787, "seek": 513308, "start": 5143.72, "end": 5148.84, "text": " are super fish of the artifact between the question the passage but there are some other issues", "tokens": [366, 1687, 3506, 295, 264, 34806, 1296, 264, 1168, 264, 11497, 457, 456, 366, 512, 661, 2663], "temperature": 0.0, "avg_logprob": -0.329346264109892, "compression_ratio": 1.8795180722891567, "no_speech_prob": 3.587630999390967e-05}, {"id": 788, "seek": 513308, "start": 5148.84, "end": 5156.5199999999995, "text": " that people like to ask some common questions so if you just do the Reynolds bait of questions", "tokens": [300, 561, 411, 281, 1029, 512, 2689, 1651, 370, 498, 291, 445, 360, 264, 29516, 16865, 295, 1651], "temperature": 0.0, "avg_logprob": -0.329346264109892, "compression_ratio": 1.8795180722891567, "no_speech_prob": 3.587630999390967e-05}, {"id": 789, "seek": 513308, "start": 5156.5199999999995, "end": 5161.8, "text": " you do trend that in test and there's a recent paper that's showing that there is actually a", "tokens": [291, 360, 6028, 300, 294, 1500, 293, 456, 311, 257, 5162, 3035, 300, 311, 4099, 300, 456, 307, 767, 257], "temperature": 0.0, "avg_logprob": -0.329346264109892, "compression_ratio": 1.8795180722891567, "no_speech_prob": 3.587630999390967e-05}, {"id": 790, "seek": 516180, "start": 5161.8, "end": 5169.64, "text": " big model is inevitable that there is a high overlap between the trends that so if you find", "tokens": [955, 2316, 307, 21451, 300, 456, 307, 257, 1090, 19959, 1296, 264, 13892, 300, 370, 498, 291, 915], "temperature": 0.0, "avg_logprob": -0.3986914690803079, "compression_ratio": 1.7850467289719627, "no_speech_prob": 1.804256135073956e-05}, {"id": 791, "seek": 516180, "start": 5169.64, "end": 5177.0, "text": " the question if one question that you're trying to test in the test set that has already appeared", "tokens": [264, 1168, 498, 472, 1168, 300, 291, 434, 1382, 281, 1500, 294, 264, 1500, 992, 300, 575, 1217, 8516], "temperature": 0.0, "avg_logprob": -0.3986914690803079, "compression_ratio": 1.7850467289719627, "no_speech_prob": 1.804256135073956e-05}, {"id": 792, "seek": 516180, "start": 5177.0, "end": 5183.08, "text": " in the trends that that's a really generalization right yeah but this is more also like all", "tokens": [294, 264, 13892, 300, 300, 311, 257, 534, 2674, 2144, 558, 1338, 457, 341, 307, 544, 611, 411, 439], "temperature": 0.0, "avg_logprob": -0.3986914690803079, "compression_ratio": 1.7850467289719627, "no_speech_prob": 1.804256135073956e-05}, {"id": 793, "seek": 516180, "start": 5183.08, "end": 5189.8, "text": " open domain settings not in the reading comprehension set yeah yeah um do you want to ask a question", "tokens": [1269, 9274, 6257, 406, 294, 264, 3760, 44991, 992, 1338, 1338, 1105, 360, 291, 528, 281, 1029, 257, 1168], "temperature": 0.0, "avg_logprob": -0.3986914690803079, "compression_ratio": 1.7850467289719627, "no_speech_prob": 1.804256135073956e-05}, {"id": 794, "seek": 518980, "start": 5189.8, "end": 5197.56, "text": " yes so you mentioned that in the last part of the presentations that the read of models may not", "tokens": [2086, 370, 291, 2835, 300, 294, 264, 1036, 644, 295, 264, 18964, 300, 264, 1401, 295, 5245, 815, 406], "temperature": 0.0, "avg_logprob": -0.4749793541140673, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00018797862867359072}, {"id": 795, "seek": 518980, "start": 5197.56, "end": 5203.64, "text": " be necessary and you presented the answers please kind of also work well to use so", "tokens": [312, 4818, 293, 291, 8212, 264, 6338, 1767, 733, 295, 611, 589, 731, 281, 764, 370], "temperature": 0.0, "avg_logprob": -0.4749793541140673, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00018797862867359072}, {"id": 796, "seek": 518980, "start": 5205.64, "end": 5212.68, "text": " do we know how how it performs on the question and answering data sets and compared to other", "tokens": [360, 321, 458, 577, 577, 309, 26213, 322, 264, 1168, 293, 13430, 1412, 6352, 293, 5347, 281, 661], "temperature": 0.0, "avg_logprob": -0.4749793541140673, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00018797862867359072}, {"id": 797, "seek": 518980, "start": 5212.68, "end": 5219.08, "text": " other models including bread as well as other and some GPU of course yeah I just encourage you to", "tokens": [661, 5245, 3009, 5961, 382, 731, 382, 661, 293, 512, 18407, 295, 1164, 1338, 286, 445, 5373, 291, 281], "temperature": 0.0, "avg_logprob": -0.4749793541140673, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00018797862867359072}, {"id": 798, "seek": 521908, "start": 5219.08, "end": 5224.92, "text": " check out this paper so this model is basically performs on par with the like the dense path", "tokens": [1520, 484, 341, 3035, 370, 341, 2316, 307, 1936, 26213, 322, 971, 365, 264, 411, 264, 18011, 3100], "temperature": 0.0, "avg_logprob": -0.3121205447764879, "compression_ratio": 1.9340101522842639, "no_speech_prob": 3.938966983696446e-05}, {"id": 799, "seek": 521908, "start": 5224.92, "end": 5232.84, "text": " retrieval retrieval model so it is performs on par with all the retrieval with the models but it", "tokens": [19817, 3337, 19817, 3337, 2316, 370, 309, 307, 26213, 322, 971, 365, 439, 264, 19817, 3337, 365, 264, 5245, 457, 309], "temperature": 0.0, "avg_logprob": -0.3121205447764879, "compression_ratio": 1.9340101522842639, "no_speech_prob": 3.938966983696446e-05}, {"id": 800, "seek": 521908, "start": 5232.84, "end": 5239.4, "text": " is actually Reynolds so I skipped one slide so so right now the saved art is actually dominating by", "tokens": [307, 767, 29516, 370, 286, 30193, 472, 4137, 370, 370, 558, 586, 264, 6624, 1523, 307, 767, 43306, 538], "temperature": 0.0, "avg_logprob": -0.3121205447764879, "compression_ratio": 1.9340101522842639, "no_speech_prob": 3.938966983696446e-05}, {"id": 801, "seek": 521908, "start": 5239.4, "end": 5246.2, "text": " just kind of dense path retrieval as a generating model so this kind of so using a T5 model", "tokens": [445, 733, 295, 18011, 3100, 19817, 3337, 382, 257, 17746, 2316, 370, 341, 733, 295, 370, 1228, 257, 314, 20, 2316], "temperature": 0.0, "avg_logprob": -0.3121205447764879, "compression_ratio": 1.9340101522842639, "no_speech_prob": 3.938966983696446e-05}, {"id": 802, "seek": 524620, "start": 5246.2, "end": 5251.72, "text": " class that's a retrieval this is actually performed really well so I would just say so this is", "tokens": [1508, 300, 311, 257, 19817, 3337, 341, 307, 767, 10332, 534, 731, 370, 286, 576, 445, 584, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.3816978099734284, "compression_ratio": 1.7072072072072073, "no_speech_prob": 1.520183195680147e-05}, {"id": 803, "seek": 524620, "start": 5251.72, "end": 5257.48, "text": " can work with a similar in this block but compared to this kind of generous model we still like", "tokens": [393, 589, 365, 257, 2531, 294, 341, 3461, 457, 5347, 281, 341, 733, 295, 14537, 2316, 321, 920, 411], "temperature": 0.0, "avg_logprob": -0.3816978099734284, "compression_ratio": 1.7072072072072073, "no_speech_prob": 1.520183195680147e-05}, {"id": 804, "seek": 524620, "start": 5257.48, "end": 5266.44, "text": " have two points behind yeah okay and what is the kind of the uh intuition behind the test phrases", "tokens": [362, 732, 2793, 2261, 1338, 1392, 293, 437, 307, 264, 733, 295, 264, 2232, 24002, 2261, 264, 1500, 20312], "temperature": 0.0, "avg_logprob": -0.3816978099734284, "compression_ratio": 1.7072072072072073, "no_speech_prob": 1.520183195680147e-05}, {"id": 805, "seek": 524620, "start": 5267.0, "end": 5272.679999999999, "text": " upperformed like the answers are probably on the close proximity and what if the data sets", "tokens": [493, 610, 22892, 411, 264, 6338, 366, 1391, 322, 264, 1998, 27632, 293, 437, 498, 264, 1412, 6352], "temperature": 0.0, "avg_logprob": -0.3816978099734284, "compression_ratio": 1.7072072072072073, "no_speech_prob": 1.520183195680147e-05}, {"id": 806, "seek": 527268, "start": 5272.68, "end": 5281.4800000000005, "text": " has answers and has answers to a specific question like very far from the actual information", "tokens": [575, 6338, 293, 575, 6338, 281, 257, 2685, 1168, 411, 588, 1400, 490, 264, 3539, 1589], "temperature": 0.0, "avg_logprob": -0.2516759525645863, "compression_ratio": 1.7763975155279503, "no_speech_prob": 1.2804076504835393e-05}, {"id": 807, "seek": 527268, "start": 5284.360000000001, "end": 5289.96, "text": " let's see the answers to the question may may not be may not reside in close proximity to the", "tokens": [718, 311, 536, 264, 6338, 281, 264, 1168, 815, 815, 406, 312, 815, 406, 40134, 294, 1998, 27632, 281, 264], "temperature": 0.0, "avg_logprob": -0.2516759525645863, "compression_ratio": 1.7763975155279503, "no_speech_prob": 1.2804076504835393e-05}, {"id": 808, "seek": 527268, "start": 5290.92, "end": 5299.72, "text": " to the words in the question so let me just clarify this okay it's a goal of this project is trying", "tokens": [281, 264, 2283, 294, 264, 1168, 370, 718, 385, 445, 17594, 341, 1392, 309, 311, 257, 3387, 295, 341, 1716, 307, 1382], "temperature": 0.0, "avg_logprob": -0.2516759525645863, "compression_ratio": 1.7763975155279503, "no_speech_prob": 1.2804076504835393e-05}, {"id": 809, "seek": 529972, "start": 5299.72, "end": 5307.8, "text": " to um index all the phrases in the Wikipedia so and by and the these kind of expressions are", "tokens": [281, 1105, 8186, 439, 264, 20312, 294, 264, 28999, 370, 293, 538, 293, 264, 613, 733, 295, 15277, 366], "temperature": 0.0, "avg_logprob": -0.3910232636986709, "compression_ratio": 1.8203883495145632, "no_speech_prob": 2.317297003173735e-05}, {"id": 810, "seek": 529972, "start": 5307.8, "end": 5314.92, "text": " built using the training set of the questions in data sets so the assumption still the distribution", "tokens": [3094, 1228, 264, 3097, 992, 295, 264, 1651, 294, 1412, 6352, 370, 264, 15302, 920, 264, 7316], "temperature": 0.0, "avg_logprob": -0.3910232636986709, "compression_ratio": 1.8203883495145632, "no_speech_prob": 2.317297003173735e-05}, {"id": 811, "seek": 529972, "start": 5314.92, "end": 5318.52, "text": " of the examples in the different test set will be similar to the Chinese set for sure", "tokens": [295, 264, 5110, 294, 264, 819, 1500, 992, 486, 312, 2531, 281, 264, 4649, 992, 337, 988], "temperature": 0.0, "avg_logprob": -0.3910232636986709, "compression_ratio": 1.8203883495145632, "no_speech_prob": 2.317297003173735e-05}, {"id": 812, "seek": 529972, "start": 5320.4400000000005, "end": 5324.68, "text": " that is that is also a question like so basically we still trying to consider all the phrases in", "tokens": [300, 307, 300, 307, 611, 257, 1168, 411, 370, 1936, 321, 920, 1382, 281, 1949, 439, 264, 20312, 294], "temperature": 0.0, "avg_logprob": -0.3910232636986709, "compression_ratio": 1.8203883495145632, "no_speech_prob": 2.317297003173735e-05}, {"id": 813, "seek": 532468, "start": 5324.68, "end": 5329.88, "text": " the Wikipedia and that test now we just take the question of vision and I would compute the thought", "tokens": [264, 28999, 293, 300, 1500, 586, 321, 445, 747, 264, 1168, 295, 5201, 293, 286, 576, 14722, 264, 1194], "temperature": 0.0, "avg_logprob": -0.4095973234910231, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.2799095202353783e-05}, {"id": 814, "seek": 532468, "start": 5329.88, "end": 5337.0, "text": " okay so if we use say a different data sets that does not present the information using a", "tokens": [1392, 370, 498, 321, 764, 584, 257, 819, 1412, 6352, 300, 775, 406, 1974, 264, 1589, 1228, 257], "temperature": 0.0, "avg_logprob": -0.4095973234910231, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.2799095202353783e-05}, {"id": 815, "seek": 532468, "start": 5337.0, "end": 5342.360000000001, "text": " structure presented in Wikipedia then this model may not work as well as", "tokens": [3877, 8212, 294, 28999, 550, 341, 2316, 815, 406, 589, 382, 731, 382], "temperature": 0.0, "avg_logprob": -0.4095973234910231, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.2799095202353783e-05}, {"id": 816, "seek": 532468, "start": 5343.8, "end": 5349.72, "text": " for what do you what do you mean by structure you present so uh say if we um", "tokens": [337, 437, 360, 291, 437, 360, 291, 914, 538, 3877, 291, 1974, 370, 2232, 584, 498, 321, 1105], "temperature": 0.0, "avg_logprob": -0.4095973234910231, "compression_ratio": 1.7384615384615385, "no_speech_prob": 1.2799095202353783e-05}, {"id": 817, "seek": 534972, "start": 5349.72, "end": 5357.72, "text": " lean more towards uh lean more towards uh structures like the passages we see in standardized tests", "tokens": [11659, 544, 3030, 2232, 11659, 544, 3030, 2232, 9227, 411, 264, 31589, 321, 536, 294, 31677, 6921], "temperature": 0.0, "avg_logprob": -0.4120751357660061, "compression_ratio": 1.8018867924528301, "no_speech_prob": 2.4258577468572184e-05}, {"id": 818, "seek": 534972, "start": 5357.72, "end": 5366.52, "text": " where the answers to the question may not be like um may not be close proximity to where the", "tokens": [689, 264, 6338, 281, 264, 1168, 815, 406, 312, 411, 1105, 815, 406, 312, 1998, 27632, 281, 689, 264], "temperature": 0.0, "avg_logprob": -0.4120751357660061, "compression_ratio": 1.8018867924528301, "no_speech_prob": 2.4258577468572184e-05}, {"id": 819, "seek": 534972, "start": 5366.52, "end": 5373.56, "text": " information was first introduced oh no so the the the answers doesn't have to be seen or Chinese", "tokens": [1589, 390, 700, 7268, 1954, 572, 370, 264, 264, 264, 6338, 1177, 380, 362, 281, 312, 1612, 420, 4649], "temperature": 0.0, "avg_logprob": -0.4120751357660061, "compression_ratio": 1.8018867924528301, "no_speech_prob": 2.4258577468572184e-05}, {"id": 820, "seek": 537356, "start": 5373.56, "end": 5379.88, "text": " stuff so basically it's a goal is to take in the Chinese set channel in code for the phrases and", "tokens": [1507, 370, 1936, 309, 311, 257, 3387, 307, 281, 747, 294, 264, 4649, 992, 2269, 294, 3089, 337, 264, 20312, 293], "temperature": 0.0, "avg_logprob": -0.2646211869645827, "compression_ratio": 1.9027777777777777, "no_speech_prob": 2.7677795060299104e-06}, {"id": 821, "seek": 537356, "start": 5379.88, "end": 5385.8, "text": " by using and then we apply this in code to all the free all the phrases all the a lot of like", "tokens": [538, 1228, 293, 550, 321, 3079, 341, 294, 3089, 281, 439, 264, 1737, 439, 264, 20312, 439, 264, 257, 688, 295, 411], "temperature": 0.0, "avg_logprob": -0.2646211869645827, "compression_ratio": 1.9027777777777777, "no_speech_prob": 2.7677795060299104e-06}, {"id": 822, "seek": 537356, "start": 5385.8, "end": 5391.72, "text": " six billion phrases in this video so it so the model is definitely able to generalize from the", "tokens": [2309, 5218, 20312, 294, 341, 960, 370, 309, 370, 264, 2316, 307, 2138, 1075, 281, 2674, 1125, 490, 264], "temperature": 0.0, "avg_logprob": -0.2646211869645827, "compression_ratio": 1.9027777777777777, "no_speech_prob": 2.7677795060299104e-06}, {"id": 823, "seek": 537356, "start": 5392.6, "end": 5397.0, "text": " Chinese set to all the phrases in this video so it doesn't have to be seen in the things that", "tokens": [4649, 992, 281, 439, 264, 20312, 294, 341, 960, 370, 309, 1177, 380, 362, 281, 312, 1612, 294, 264, 721, 300], "temperature": 0.0, "avg_logprob": -0.2646211869645827, "compression_ratio": 1.9027777777777777, "no_speech_prob": 2.7677795060299104e-06}, {"id": 824, "seek": 539700, "start": 5397.0, "end": 5405.96, "text": " that is this what you're asking", "tokens": [300, 307, 341, 437, 291, 434, 3365], "temperature": 0.0, "avg_logprob": -0.5062115091673085, "compression_ratio": 1.734463276836158, "no_speech_prob": 2.7889902412425727e-05}, {"id": 825, "seek": 539700, "start": 5408.76, "end": 5413.64, "text": " this actually very so it's actually similar to like the retrieval of the best passive retrieval", "tokens": [341, 767, 588, 370, 309, 311, 767, 2531, 281, 411, 264, 19817, 3337, 295, 264, 1151, 14975, 19817, 3337], "temperature": 0.0, "avg_logprob": -0.5062115091673085, "compression_ratio": 1.734463276836158, "no_speech_prob": 2.7889902412425727e-05}, {"id": 826, "seek": 539700, "start": 5413.64, "end": 5419.16, "text": " so you still like um yeah try to channel pass irritation here is the first representation", "tokens": [370, 291, 920, 411, 1105, 1338, 853, 281, 2269, 1320, 50031, 510, 307, 264, 700, 10290], "temperature": 0.0, "avg_logprob": -0.5062115091673085, "compression_ratio": 1.734463276836158, "no_speech_prob": 2.7889902412425727e-05}, {"id": 827, "seek": 539700, "start": 5420.04, "end": 5425.96, "text": " but the the revisions only try to use the the Chinese set of the cross answered data sets", "tokens": [457, 264, 264, 3698, 4252, 787, 853, 281, 764, 264, 264, 4649, 992, 295, 264, 3278, 10103, 1412, 6352], "temperature": 0.0, "avg_logprob": -0.5062115091673085, "compression_ratio": 1.734463276836158, "no_speech_prob": 2.7889902412425727e-05}, {"id": 828, "seek": 542596, "start": 5425.96, "end": 5432.84, "text": " but um by taking the encoder and then we are going to encode uh all the rotations all the passages", "tokens": [457, 1105, 538, 1940, 264, 2058, 19866, 293, 550, 321, 366, 516, 281, 2058, 1429, 2232, 439, 264, 44796, 439, 264, 31589], "temperature": 0.0, "avg_logprob": -0.24130760641659008, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.2142979762284085e-05}, {"id": 829, "seek": 542596, "start": 5432.84, "end": 5438.76, "text": " of phrases in Wikipedia and then we can um use practice this rotation can actually generalize", "tokens": [295, 20312, 294, 28999, 293, 550, 321, 393, 1105, 764, 3124, 341, 12447, 393, 767, 2674, 1125], "temperature": 0.0, "avg_logprob": -0.24130760641659008, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.2142979762284085e-05}, {"id": 830, "seek": 542596, "start": 5438.76, "end": 5447.64, "text": " well for the unseen questions yeah so uh so the question is um what if the nearest neighbor", "tokens": [731, 337, 264, 40608, 1651, 1338, 370, 2232, 370, 264, 1168, 307, 1105, 437, 498, 264, 23831, 5987], "temperature": 0.0, "avg_logprob": -0.24130760641659008, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.2142979762284085e-05}, {"id": 831, "seek": 542596, "start": 5447.64, "end": 5455.08, "text": " search doesn't return to answer so why do you think the nearest neighbor I mean you always", "tokens": [3164, 1177, 380, 2736, 281, 1867, 370, 983, 360, 291, 519, 264, 23831, 5987, 286, 914, 291, 1009], "temperature": 0.0, "avg_logprob": -0.24130760641659008, "compression_ratio": 1.744186046511628, "no_speech_prob": 3.2142979762284085e-05}, {"id": 832, "seek": 545508, "start": 5455.08, "end": 5459.4, "text": " can find something right you just a question is that whether it's a question is not not", "tokens": [393, 915, 746, 558, 291, 445, 257, 1168, 307, 300, 1968, 309, 311, 257, 1168, 307, 406, 406], "temperature": 0.0, "avg_logprob": -0.29926208047305836, "compression_ratio": 1.8059701492537314, "no_speech_prob": 3.2158717658603564e-05}, {"id": 833, "seek": 545508, "start": 5460.28, "end": 5465.24, "text": " yes so the question is what if in the data says that the answer is not close enough then", "tokens": [2086, 370, 264, 1168, 307, 437, 498, 294, 264, 1412, 1619, 300, 264, 1867, 307, 406, 1998, 1547, 550], "temperature": 0.0, "avg_logprob": -0.29926208047305836, "compression_ratio": 1.8059701492537314, "no_speech_prob": 3.2158717658603564e-05}, {"id": 834, "seek": 545508, "start": 5466.84, "end": 5473.08, "text": " um yeah this good question I don't know uh if you really come up with something that is really", "tokens": [1105, 1338, 341, 665, 1168, 286, 500, 380, 458, 2232, 498, 291, 534, 808, 493, 365, 746, 300, 307, 534], "temperature": 0.0, "avg_logprob": -0.29926208047305836, "compression_ratio": 1.8059701492537314, "no_speech_prob": 3.2158717658603564e-05}, {"id": 835, "seek": 545508, "start": 5473.08, "end": 5478.5199999999995, "text": " very far away from all the questions that we have been seeing the Chinese set that could be", "tokens": [588, 1400, 1314, 490, 439, 264, 1651, 300, 321, 362, 668, 2577, 264, 4649, 992, 300, 727, 312], "temperature": 0.0, "avg_logprob": -0.29926208047305836, "compression_ratio": 1.8059701492537314, "no_speech_prob": 3.2158717658603564e-05}, {"id": 836, "seek": 547852, "start": 5478.52, "end": 5486.6, "text": " possible I don't know basically depend on um uh found the text or um formatted", "tokens": [1944, 286, 500, 380, 458, 1936, 5672, 322, 1105, 2232, 1352, 264, 2487, 420, 1105, 1254, 32509], "temperature": 0.0, "avg_logprob": -0.33903838492728566, "compression_ratio": 1.8100558659217878, "no_speech_prob": 1.093568789656274e-05}, {"id": 837, "seek": 547852, "start": 5487.320000000001, "end": 5492.360000000001, "text": " then uh the nearest neighbor search may not uh work as well as the models", "tokens": [550, 2232, 264, 23831, 5987, 3164, 815, 406, 2232, 589, 382, 731, 382, 264, 5245], "temperature": 0.0, "avg_logprob": -0.33903838492728566, "compression_ratio": 1.8100558659217878, "no_speech_prob": 1.093568789656274e-05}, {"id": 838, "seek": 547852, "start": 5493.56, "end": 5498.280000000001, "text": " so again the question is also the question is also returned by a question encoder", "tokens": [370, 797, 264, 1168, 307, 611, 264, 1168, 307, 611, 8752, 538, 257, 1168, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.33903838492728566, "compression_ratio": 1.8100558659217878, "no_speech_prob": 1.093568789656274e-05}, {"id": 839, "seek": 547852, "start": 5499.160000000001, "end": 5504.52, "text": " so so the question is the whether this question encoder can give you something reasonable", "tokens": [370, 370, 264, 1168, 307, 264, 1968, 341, 1168, 2058, 19866, 393, 976, 291, 746, 10585], "temperature": 0.0, "avg_logprob": -0.33903838492728566, "compression_ratio": 1.8100558659217878, "no_speech_prob": 1.093568789656274e-05}, {"id": 840, "seek": 550452, "start": 5504.52, "end": 5511.400000000001, "text": " that space or not but uh yeah so we have been testing along like a random even the input", "tokens": [300, 1901, 420, 406, 457, 2232, 1338, 370, 321, 362, 668, 4997, 2051, 411, 257, 4974, 754, 264, 4846], "temperature": 0.0, "avg_logprob": -0.3035633380596454, "compression_ratio": 1.738532110091743, "no_speech_prob": 5.302208592183888e-05}, {"id": 841, "seek": 550452, "start": 5511.400000000001, "end": 5516.52, "text": " sentences or even like the question then I have to go real question could be a sentence", "tokens": [16579, 420, 754, 411, 264, 1168, 550, 286, 362, 281, 352, 957, 1168, 727, 312, 257, 8174], "temperature": 0.0, "avg_logprob": -0.3035633380596454, "compression_ratio": 1.738532110091743, "no_speech_prob": 5.302208592183888e-05}, {"id": 842, "seek": 550452, "start": 5516.52, "end": 5522.360000000001, "text": " it doesn't seem to be a problem so far yeah maybe maybe maybe we should give a couple of other people", "tokens": [309, 1177, 380, 1643, 281, 312, 257, 1154, 370, 1400, 1338, 1310, 1310, 1310, 321, 820, 976, 257, 1916, 295, 661, 561], "temperature": 0.0, "avg_logprob": -0.3035633380596454, "compression_ratio": 1.738532110091743, "no_speech_prob": 5.302208592183888e-05}, {"id": 843, "seek": 550452, "start": 5522.360000000001, "end": 5528.84, "text": " ago and you're allowed to turn your camera on and ask a question if you want um so um next person is", "tokens": [2057, 293, 291, 434, 4350, 281, 1261, 428, 2799, 322, 293, 1029, 257, 1168, 498, 291, 528, 1105, 370, 1105, 958, 954, 307], "temperature": 0.0, "avg_logprob": -0.3035633380596454, "compression_ratio": 1.738532110091743, "no_speech_prob": 5.302208592183888e-05}, {"id": 844, "seek": 552884, "start": 5528.84, "end": 5538.2, "text": " all right hi uh thank you for taking the time to meet us um my my question is kind of quick", "tokens": [439, 558, 4879, 2232, 1309, 291, 337, 1940, 264, 565, 281, 1677, 505, 1105, 452, 452, 1168, 307, 733, 295, 1702], "temperature": 0.0, "avg_logprob": -0.205835299058394, "compression_ratio": 1.615819209039548, "no_speech_prob": 0.0001419293985236436}, {"id": 845, "seek": 552884, "start": 5538.2, "end": 5544.04, "text": " so you mentioned work that brought up a set of relatively simple questions that show how brittle", "tokens": [370, 291, 2835, 589, 300, 3038, 493, 257, 992, 295, 7226, 2199, 1651, 300, 855, 577, 49325], "temperature": 0.0, "avg_logprob": -0.205835299058394, "compression_ratio": 1.615819209039548, "no_speech_prob": 0.0001419293985236436}, {"id": 846, "seek": 552884, "start": 5544.04, "end": 5553.72, "text": " or poor the current models can be right I'm curious if that's right yeah yeah exactly exactly did", "tokens": [420, 4716, 264, 2190, 5245, 393, 312, 558, 286, 478, 6369, 498, 300, 311, 558, 1338, 1338, 2293, 2293, 630], "temperature": 0.0, "avg_logprob": -0.205835299058394, "compression_ratio": 1.615819209039548, "no_speech_prob": 0.0001419293985236436}, {"id": 847, "seek": 555372, "start": 5553.72, "end": 5559.56, "text": " that turn out to change the community to improve how to evaluate the models because", "tokens": [300, 1261, 484, 281, 1319, 264, 1768, 281, 3470, 577, 281, 13059, 264, 5245, 570], "temperature": 0.0, "avg_logprob": -0.3585420627038456, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.473284555686405e-05}, {"id": 848, "seek": 555372, "start": 5560.280000000001, "end": 5566.360000000001, "text": " they're actually doing pretty poorly on some of those right yeah so first these questions are", "tokens": [436, 434, 767, 884, 1238, 22271, 322, 512, 295, 729, 558, 1338, 370, 700, 613, 1651, 366], "temperature": 0.0, "avg_logprob": -0.3585420627038456, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.473284555686405e-05}, {"id": 849, "seek": 555372, "start": 5566.360000000001, "end": 5573.0, "text": " simple in uh in terms of the the wording is very simple the for the template is very simple", "tokens": [2199, 294, 2232, 294, 2115, 295, 264, 264, 47602, 307, 588, 2199, 264, 337, 264, 12379, 307, 588, 2199], "temperature": 0.0, "avg_logprob": -0.3585420627038456, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.473284555686405e-05}, {"id": 850, "seek": 555372, "start": 5573.0, "end": 5578.12, "text": " but they still trying to test like a negational temporal relational forever so the questions are not", "tokens": [457, 436, 920, 1382, 281, 1500, 411, 257, 2485, 1478, 30881, 38444, 5680, 370, 264, 1651, 366, 406], "temperature": 0.0, "avg_logprob": -0.3585420627038456, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.473284555686405e-05}, {"id": 851, "seek": 555372, "start": 5578.68, "end": 5582.68, "text": " the I mean in terms of the reasoning of the code of plastic is not that simple it's just the wording", "tokens": [264, 286, 914, 294, 2115, 295, 264, 21577, 295, 264, 3089, 295, 5900, 307, 406, 300, 2199, 309, 311, 445, 264, 47602], "temperature": 0.0, "avg_logprob": -0.3585420627038456, "compression_ratio": 1.891566265060241, "no_speech_prob": 1.473284555686405e-05}, {"id": 852, "seek": 558268, "start": 5582.68, "end": 5589.0, "text": " very simple um I do think um okay so this paper that we receive a lot of attention you the best", "tokens": [588, 2199, 1105, 286, 360, 519, 1105, 1392, 370, 341, 3035, 300, 321, 4774, 257, 688, 295, 3202, 291, 264, 1151], "temperature": 0.0, "avg_logprob": -0.2522215253851387, "compression_ratio": 1.758139534883721, "no_speech_prob": 2.9295770218595862e-05}, {"id": 853, "seek": 558268, "start": 5589.0, "end": 5595.320000000001, "text": " paper last year and I see all the biggest conference um so I think a lot of people are trying to", "tokens": [3035, 1036, 1064, 293, 286, 536, 439, 264, 3880, 7586, 1105, 370, 286, 519, 257, 688, 295, 561, 366, 1382, 281], "temperature": 0.0, "avg_logprob": -0.2522215253851387, "compression_ratio": 1.758139534883721, "no_speech_prob": 2.9295770218595862e-05}, {"id": 854, "seek": 558268, "start": 5595.320000000001, "end": 5601.4800000000005, "text": " solve the problem I cannot tell you that okay whether we really have a solution to this yet or not", "tokens": [5039, 264, 1154, 286, 2644, 980, 291, 300, 1392, 1968, 321, 534, 362, 257, 3827, 281, 341, 1939, 420, 406], "temperature": 0.0, "avg_logprob": -0.2522215253851387, "compression_ratio": 1.758139534883721, "no_speech_prob": 2.9295770218595862e-05}, {"id": 855, "seek": 558268, "start": 5601.4800000000005, "end": 5609.16, "text": " yeah cool yeah thank you for bringing this one up it's really interesting okay next is", "tokens": [1338, 1627, 1338, 1309, 291, 337, 5062, 341, 472, 493, 309, 311, 534, 1880, 1392, 958, 307], "temperature": 0.0, "avg_logprob": -0.2522215253851387, "compression_ratio": 1.758139534883721, "no_speech_prob": 2.9295770218595862e-05}, {"id": 856, "seek": 560916, "start": 5609.16, "end": 5617.88, "text": " yeah thanks for saying it at time so my question is kind of not relevant but like to view the", "tokens": [1338, 3231, 337, 1566, 309, 412, 565, 370, 452, 1168, 307, 733, 295, 406, 7340, 457, 411, 281, 1910, 264], "temperature": 0.0, "avg_logprob": -0.2275882403055827, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.00013973339810036123}, {"id": 857, "seek": 560916, "start": 5617.88, "end": 5624.92, "text": " robust system of question answering in what extent can in context learning how models to be more", "tokens": [13956, 1185, 295, 1168, 13430, 294, 437, 8396, 393, 294, 4319, 2539, 577, 5245, 281, 312, 544], "temperature": 0.0, "avg_logprob": -0.2275882403055827, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.00013973339810036123}, {"id": 858, "seek": 560916, "start": 5624.92, "end": 5638.68, "text": " robust with respect to different domains oh so like uh basically you provide um template", "tokens": [13956, 365, 3104, 281, 819, 25514, 1954, 370, 411, 2232, 1936, 291, 2893, 1105, 12379], "temperature": 0.0, "avg_logprob": -0.2275882403055827, "compression_ratio": 1.5942857142857143, "no_speech_prob": 0.00013973339810036123}, {"id": 859, "seek": 563868, "start": 5638.68, "end": 5646.04, "text": " generated by bird and then instead of directly predicting the classes of text classifications", "tokens": [10833, 538, 5255, 293, 550, 2602, 295, 3838, 32884, 264, 5359, 295, 2487, 1508, 7833], "temperature": 0.0, "avg_logprob": -0.4206776200679311, "compression_ratio": 1.5521472392638036, "no_speech_prob": 2.2106076357886195e-05}, {"id": 860, "seek": 563868, "start": 5646.04, "end": 5652.4400000000005, "text": " it is um use some word to represent that class of enter to the word so", "tokens": [309, 307, 1105, 764, 512, 1349, 281, 2906, 300, 1508, 295, 3242, 281, 264, 1349, 370], "temperature": 0.0, "avg_logprob": -0.4206776200679311, "compression_ratio": 1.5521472392638036, "no_speech_prob": 2.2106076357886195e-05}, {"id": 861, "seek": 563868, "start": 5655.64, "end": 5662.52, "text": " okay um so I assume that you are actually referred to the in context learning in the GPS", "tokens": [1392, 1105, 370, 286, 6552, 300, 291, 366, 767, 10839, 281, 264, 294, 4319, 2539, 294, 264, 19462], "temperature": 0.0, "avg_logprob": -0.4206776200679311, "compression_ratio": 1.5521472392638036, "no_speech_prob": 2.2106076357886195e-05}, {"id": 862, "seek": 566252, "start": 5662.52, "end": 5669.400000000001, "text": " stream or that's the way that okay um actually I have been doing something related to the", "tokens": [4309, 420, 300, 311, 264, 636, 300, 1392, 1105, 767, 286, 362, 668, 884, 746, 4077, 281, 264], "temperature": 0.0, "avg_logprob": -0.5192201195693598, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.186259345966391e-05}, {"id": 863, "seek": 566252, "start": 5669.400000000001, "end": 5674.84, "text": " learning recently um questions but I'm not sure how we actually use that in context learning", "tokens": [2539, 3938, 1105, 1651, 457, 286, 478, 406, 988, 577, 321, 767, 764, 300, 294, 4319, 2539], "temperature": 0.0, "avg_logprob": -0.5192201195693598, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.186259345966391e-05}, {"id": 864, "seek": 566252, "start": 5674.84, "end": 5682.4400000000005, "text": " in at least in for school type of problems um yeah so I don't know if that could", "tokens": [294, 412, 1935, 294, 337, 1395, 2010, 295, 2740, 1105, 1338, 370, 286, 500, 380, 458, 498, 300, 727], "temperature": 0.0, "avg_logprob": -0.5192201195693598, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.186259345966391e-05}, {"id": 865, "seek": 566252, "start": 5682.4400000000005, "end": 5687.72, "text": " solve the robustness or not or even the whole how to use that technique for the questions", "tokens": [5039, 264, 13956, 1287, 420, 406, 420, 754, 264, 1379, 577, 281, 764, 300, 6532, 337, 264, 1651], "temperature": 0.0, "avg_logprob": -0.5192201195693598, "compression_ratio": 1.721951219512195, "no_speech_prob": 3.186259345966391e-05}, {"id": 866, "seek": 568772, "start": 5687.72, "end": 5695.400000000001, "text": " or yet nice thanks and I also mentioned that we can train a retriever without a reader so", "tokens": [420, 1939, 1481, 3231, 293, 286, 611, 2835, 300, 321, 393, 3847, 257, 19817, 331, 1553, 257, 15149, 370], "temperature": 0.0, "avg_logprob": -0.49848610500119767, "compression_ratio": 1.5441176470588236, "no_speech_prob": 3.023034514626488e-05}, {"id": 867, "seek": 568772, "start": 5695.400000000001, "end": 5704.52, "text": " is there a paper of the current like attempt to do that yeah so the library also just uh yeah", "tokens": [307, 456, 257, 3035, 295, 264, 2190, 411, 5217, 281, 360, 300, 1338, 370, 264, 6405, 611, 445, 2232, 1338], "temperature": 0.0, "avg_logprob": -0.49848610500119767, "compression_ratio": 1.5441176470588236, "no_speech_prob": 3.023034514626488e-05}, {"id": 868, "seek": 568772, "start": 5707.96, "end": 5710.92, "text": " thank you all okay next is", "tokens": [1309, 291, 439, 1392, 958, 307], "temperature": 0.0, "avg_logprob": -0.49848610500119767, "compression_ratio": 1.5441176470588236, "no_speech_prob": 3.023034514626488e-05}, {"id": 869, "seek": 571092, "start": 5710.92, "end": 5719.56, "text": " hey how's it going uh thanks so much for the uh for the lecture um i put a little broader question", "tokens": [4177, 577, 311, 309, 516, 2232, 3231, 370, 709, 337, 264, 2232, 337, 264, 7991, 1105, 741, 829, 257, 707, 13227, 1168], "temperature": 0.0, "avg_logprob": -0.276360600789388, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.00011589255882427096}, {"id": 870, "seek": 571092, "start": 5719.56, "end": 5728.36, "text": " um so we got the about the future of NLP um do you think that in order to solve NLP in a sense", "tokens": [1105, 370, 321, 658, 264, 466, 264, 2027, 295, 426, 45196, 1105, 360, 291, 519, 300, 294, 1668, 281, 5039, 426, 45196, 294, 257, 2020], "temperature": 0.0, "avg_logprob": -0.276360600789388, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.00011589255882427096}, {"id": 871, "seek": 571092, "start": 5728.36, "end": 5736.4400000000005, "text": " that you can perform on par with humans on all NLP tasks it's efficient to only interact with", "tokens": [300, 291, 393, 2042, 322, 971, 365, 6255, 322, 439, 426, 45196, 9608, 309, 311, 7148, 281, 787, 4648, 365], "temperature": 0.0, "avg_logprob": -0.276360600789388, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.00011589255882427096}, {"id": 872, "seek": 573644, "start": 5736.44, "end": 5741.799999999999, "text": " with text you know whatever do you think will eventually need some sort of with the sort of experience", "tokens": [365, 2487, 291, 458, 2035, 360, 291, 519, 486, 4728, 643, 512, 1333, 295, 365, 264, 1333, 295, 1752], "temperature": 0.0, "avg_logprob": -0.3964606959645341, "compression_ratio": 1.7612612612612613, "no_speech_prob": 3.762157939490862e-05}, {"id": 873, "seek": 573644, "start": 5741.799999999999, "end": 5747.4, "text": " and confidence that you get um only from seeing and then sort of feeling the world and having the", "tokens": [293, 6687, 300, 291, 483, 1105, 787, 490, 2577, 293, 550, 1333, 295, 2633, 264, 1002, 293, 1419, 264], "temperature": 0.0, "avg_logprob": -0.3964606959645341, "compression_ratio": 1.7612612612612613, "no_speech_prob": 3.762157939490862e-05}, {"id": 874, "seek": 573644, "start": 5747.4, "end": 5754.2, "text": " sake of interactions that we assume it's how yeah I mean conversation is definitely very", "tokens": [9717, 295, 13280, 300, 321, 6552, 309, 311, 577, 1338, 286, 914, 3761, 307, 2138, 588], "temperature": 0.0, "avg_logprob": -0.3964606959645341, "compression_ratio": 1.7612612612612613, "no_speech_prob": 3.762157939490862e-05}, {"id": 875, "seek": 573644, "start": 5754.2, "end": 5760.5199999999995, "text": " difficult even in the context of our sensory conversation is a very yeah no very very important topic", "tokens": [2252, 754, 294, 264, 4319, 295, 527, 27233, 3761, 307, 257, 588, 1338, 572, 588, 588, 1021, 4829], "temperature": 0.0, "avg_logprob": -0.3964606959645341, "compression_ratio": 1.7612612612612613, "no_speech_prob": 3.762157939490862e-05}, {"id": 876, "seek": 576052, "start": 5760.52, "end": 5767.0, "text": " that um still remains I think it still remains on three so uh that's for that part the one", "tokens": [300, 1105, 920, 7023, 286, 519, 309, 920, 7023, 322, 1045, 370, 2232, 300, 311, 337, 300, 644, 264, 472], "temperature": 0.0, "avg_logprob": -0.3244982764834449, "compression_ratio": 1.779467680608365, "no_speech_prob": 1.8043383533949964e-05}, {"id": 877, "seek": 576052, "start": 5767.0, "end": 5773.320000000001, "text": " six definitely yes and also want to mention that okay so for a lot of the reading conversation", "tokens": [2309, 2138, 2086, 293, 611, 528, 281, 2152, 300, 1392, 370, 337, 257, 688, 295, 264, 3760, 3761], "temperature": 0.0, "avg_logprob": -0.3244982764834449, "compression_ratio": 1.779467680608365, "no_speech_prob": 1.8043383533949964e-05}, {"id": 878, "seek": 576052, "start": 5773.320000000001, "end": 5778.360000000001, "text": " data sets or questions and data sets you have seen that we are people will start start to achieve", "tokens": [1412, 6352, 420, 1651, 293, 1412, 6352, 291, 362, 1612, 300, 321, 366, 561, 486, 722, 722, 281, 4584], "temperature": 0.0, "avg_logprob": -0.3244982764834449, "compression_ratio": 1.779467680608365, "no_speech_prob": 1.8043383533949964e-05}, {"id": 879, "seek": 576052, "start": 5778.360000000001, "end": 5783.56, "text": " the human performance but this but we also see that how great all these systems are because", "tokens": [264, 1952, 3389, 457, 341, 457, 321, 611, 536, 300, 577, 869, 439, 613, 3652, 366, 570], "temperature": 0.0, "avg_logprob": -0.3244982764834449, "compression_ratio": 1.779467680608365, "no_speech_prob": 1.8043383533949964e-05}, {"id": 880, "seek": 576052, "start": 5784.040000000001, "end": 5789.400000000001, "text": " yeah I mean they cannot regenerize also the easy problems all these things need to be with a", "tokens": [1338, 286, 914, 436, 2644, 26358, 1125, 611, 264, 1858, 2740, 439, 613, 721, 643, 281, 312, 365, 257], "temperature": 0.0, "avg_logprob": -0.3244982764834449, "compression_ratio": 1.779467680608365, "no_speech_prob": 1.8043383533949964e-05}, {"id": 881, "seek": 578940, "start": 5789.4, "end": 5797.16, "text": " solved um it depends so if you think about your descent time maybe a little bit too easy", "tokens": [13041, 1105, 309, 5946, 370, 498, 291, 519, 466, 428, 23475, 565, 1310, 257, 707, 857, 886, 1858], "temperature": 0.0, "avg_logprob": -0.6779586791992187, "compression_ratio": 1.649746192893401, "no_speech_prob": 3.586881575756706e-05}, {"id": 882, "seek": 578940, "start": 5798.759999999999, "end": 5805.48, "text": " oh yeah one point oh no sorry guys yeah one point out for what is there is that", "tokens": [1954, 1338, 472, 935, 1954, 572, 2597, 1074, 1338, 472, 935, 484, 337, 437, 307, 456, 307, 300], "temperature": 0.0, "avg_logprob": -0.6779586791992187, "compression_ratio": 1.649746192893401, "no_speech_prob": 3.586881575756706e-05}, {"id": 883, "seek": 578940, "start": 5805.48, "end": 5809.96, "text": " uh dabbing a lot of trans-versa in trying to have a few maintenance", "tokens": [2232, 28964, 4324, 257, 688, 295, 1145, 12, 840, 64, 294, 1382, 281, 362, 257, 1326, 11258], "temperature": 0.0, "avg_logprob": -0.6779586791992187, "compression_ratio": 1.649746192893401, "no_speech_prob": 3.586881575756706e-05}, {"id": 884, "seek": 578940, "start": 5809.96, "end": 5816.5199999999995, "text": " um a framework to evaluate this kind of system just try to break the current system come", "tokens": [1105, 257, 8388, 281, 13059, 341, 733, 295, 1185, 445, 853, 281, 1821, 264, 2190, 1185, 808], "temperature": 0.0, "avg_logprob": -0.6779586791992187, "compression_ratio": 1.649746192893401, "no_speech_prob": 3.586881575756706e-05}, {"id": 885, "seek": 581652, "start": 5816.52, "end": 5822.200000000001, "text": " out with some harder questions so um yeah so that means maybe it's a kind of static", "tokens": [484, 365, 512, 6081, 1651, 370, 1105, 1338, 370, 300, 1355, 1310, 309, 311, 257, 733, 295, 13437], "temperature": 0.0, "avg_logprob": -0.34920585723150344, "compression_ratio": 1.7431192660550459, "no_speech_prob": 3.760881008929573e-05}, {"id": 886, "seek": 581652, "start": 5822.200000000001, "end": 5827.400000000001, "text": " data system of good enough to measure the progress so we actually really need some kind of dynamic", "tokens": [1412, 1185, 295, 665, 1547, 281, 3481, 264, 4205, 370, 321, 767, 534, 643, 512, 733, 295, 8546], "temperature": 0.0, "avg_logprob": -0.34920585723150344, "compression_ratio": 1.7431192660550459, "no_speech_prob": 3.760881008929573e-05}, {"id": 887, "seek": 581652, "start": 5827.400000000001, "end": 5833.8, "text": " evaleration and also introduce all more these kind of adverse examples or the um yeah harder questions", "tokens": [1073, 304, 5053, 293, 611, 5366, 439, 544, 613, 733, 295, 27590, 5110, 420, 264, 1105, 1338, 6081, 1651], "temperature": 0.0, "avg_logprob": -0.34920585723150344, "compression_ratio": 1.7431192660550459, "no_speech_prob": 3.760881008929573e-05}, {"id": 888, "seek": 581652, "start": 5833.8, "end": 5841.320000000001, "text": " or something like yeah hey still game for a couple more questions uh sure I don't only mention", "tokens": [420, 746, 411, 1338, 4177, 920, 1216, 337, 257, 1916, 544, 1651, 2232, 988, 286, 500, 380, 787, 2152], "temperature": 0.0, "avg_logprob": -0.34920585723150344, "compression_ratio": 1.7431192660550459, "no_speech_prob": 3.760881008929573e-05}, {"id": 889, "seek": 584132, "start": 5841.32, "end": 5848.679999999999, "text": " my 10 p.m uh yeah um only used cost yeah okay um so next is next is", "tokens": [452, 1266, 280, 13, 76, 2232, 1338, 1105, 787, 1143, 2063, 1338, 1392, 1105, 370, 958, 307, 958, 307], "temperature": 0.0, "avg_logprob": -0.5135313180776743, "compression_ratio": 1.5502958579881656, "no_speech_prob": 7.181610999396071e-05}, {"id": 890, "seek": 584132, "start": 5850.84, "end": 5858.36, "text": " hey yeah that's a much more much to the arm so in uh just 2020 there was this efficient open domain", "tokens": [4177, 1338, 300, 311, 257, 709, 544, 709, 281, 264, 3726, 370, 294, 2232, 445, 4808, 456, 390, 341, 7148, 1269, 9274], "temperature": 0.0, "avg_logprob": -0.5135313180776743, "compression_ratio": 1.5502958579881656, "no_speech_prob": 7.181610999396071e-05}, {"id": 891, "seek": 584132, "start": 5858.36, "end": 5865.32, "text": " question answering challenge um and from you know from performance to team what there was like", "tokens": [1168, 13430, 3430, 1105, 293, 490, 291, 458, 490, 3389, 281, 1469, 437, 456, 390, 411], "temperature": 0.0, "avg_logprob": -0.5135313180776743, "compression_ratio": 1.5502958579881656, "no_speech_prob": 7.181610999396071e-05}, {"id": 892, "seek": 586532, "start": 5865.32, "end": 5872.759999999999, "text": " quite substantial uh decreased versus human accuracy um probably like yeah well for", "tokens": [1596, 16726, 2232, 24436, 5717, 1952, 14170, 1105, 1391, 411, 1338, 731, 337], "temperature": 0.0, "avg_logprob": -0.4570671941193057, "compression_ratio": 1.7136150234741785, "no_speech_prob": 3.218549318262376e-05}, {"id": 893, "seek": 586532, "start": 5872.759999999999, "end": 5879.24, "text": " primarily dear quantization and um processing drift that occurred uh when they were quantizing", "tokens": [10029, 6875, 4426, 2144, 293, 1105, 9007, 19699, 300, 11068, 2232, 562, 436, 645, 4426, 3319], "temperature": 0.0, "avg_logprob": -0.4570671941193057, "compression_ratio": 1.7136150234741785, "no_speech_prob": 3.218549318262376e-05}, {"id": 894, "seek": 586532, "start": 5879.799999999999, "end": 5887.5599999999995, "text": " um so I I recently encountered this paper called uh random quantizers uh which is essentially", "tokens": [1105, 370, 286, 286, 3938, 20381, 341, 3035, 1219, 2232, 4974, 4426, 22525, 2232, 597, 307, 4476], "temperature": 0.0, "avg_logprob": -0.4570671941193057, "compression_ratio": 1.7136150234741785, "no_speech_prob": 3.218549318262376e-05}, {"id": 895, "seek": 586532, "start": 5888.2, "end": 5894.84, "text": " learned uh learns like basis representations for the quantizers like jointly with the weight", "tokens": [3264, 2232, 27152, 411, 5143, 33358, 337, 264, 4426, 22525, 411, 46557, 365, 264, 3364], "temperature": 0.0, "avg_logprob": -0.4570671941193057, "compression_ratio": 1.7136150234741785, "no_speech_prob": 3.218549318262376e-05}, {"id": 896, "seek": 589484, "start": 5894.84, "end": 5903.16, "text": " of the network um and while this would be like extremely effective if you were to just like say", "tokens": [295, 264, 3209, 1105, 293, 1339, 341, 576, 312, 411, 4664, 4942, 498, 291, 645, 281, 445, 411, 584], "temperature": 0.0, "avg_logprob": -0.2872950862830793, "compression_ratio": 1.5842696629213484, "no_speech_prob": 2.2821519451099448e-05}, {"id": 897, "seek": 589484, "start": 5903.16, "end": 5909.400000000001, "text": " change of scratch I was just really curious do you think it like such a uh there's a way to do", "tokens": [1319, 295, 8459, 286, 390, 445, 534, 6369, 360, 291, 519, 309, 411, 1270, 257, 2232, 456, 311, 257, 636, 281, 360], "temperature": 0.0, "avg_logprob": -0.2872950862830793, "compression_ratio": 1.5842696629213484, "no_speech_prob": 2.2821519451099448e-05}, {"id": 898, "seek": 589484, "start": 5909.400000000001, "end": 5915.72, "text": " this with say a pre-trained model or something like that um I had a few ideas with like the", "tokens": [341, 365, 584, 257, 659, 12, 17227, 2001, 2316, 420, 746, 411, 300, 1105, 286, 632, 257, 1326, 3487, 365, 411, 264], "temperature": 0.0, "avg_logprob": -0.2872950862830793, "compression_ratio": 1.5842696629213484, "no_speech_prob": 2.2821519451099448e-05}, {"id": 899, "seek": 591572, "start": 5915.72, "end": 5925.08, "text": " insurance insurance bloggers uh I don't think uh I don't see a very clear way of doing them um yeah I", "tokens": [7214, 7214, 6968, 9458, 2232, 286, 500, 380, 519, 2232, 286, 500, 380, 536, 257, 588, 1850, 636, 295, 884, 552, 1105, 1338, 286], "temperature": 0.0, "avg_logprob": -0.28258019350887686, "compression_ratio": 1.8, "no_speech_prob": 9.436591426492669e-06}, {"id": 900, "seek": 591572, "start": 5925.08, "end": 5931.400000000001, "text": " don't think I'm really experts to answer those questions um yeah I'm not sure if I really have", "tokens": [500, 380, 519, 286, 478, 534, 8572, 281, 1867, 729, 1651, 1105, 1338, 286, 478, 406, 988, 498, 286, 534, 362], "temperature": 0.0, "avg_logprob": -0.28258019350887686, "compression_ratio": 1.8, "no_speech_prob": 9.436591426492669e-06}, {"id": 901, "seek": 591572, "start": 5931.400000000001, "end": 5935.88, "text": " the answer but I also want to give you mentions that yeah quantization has been very useful", "tokens": [264, 1867, 457, 286, 611, 528, 281, 976, 291, 23844, 300, 1338, 4426, 2144, 575, 668, 588, 4420], "temperature": 0.0, "avg_logprob": -0.28258019350887686, "compression_ratio": 1.8, "no_speech_prob": 9.436591426492669e-06}, {"id": 902, "seek": 591572, "start": 5935.88, "end": 5940.92, "text": " technique to make the model smaller right so we have been also exploring the quantization", "tokens": [6532, 281, 652, 264, 2316, 4356, 558, 370, 321, 362, 668, 611, 12736, 264, 4426, 2144], "temperature": 0.0, "avg_logprob": -0.28258019350887686, "compression_ratio": 1.8, "no_speech_prob": 9.436591426492669e-06}, {"id": 903, "seek": 594092, "start": 5940.92, "end": 5947.96, "text": " in the dense forest's project recently because of the storage has been still very uh has been", "tokens": [294, 264, 18011, 6719, 311, 1716, 3938, 570, 295, 264, 6725, 575, 668, 920, 588, 2232, 575, 668], "temperature": 0.0, "avg_logprob": -0.4067844316071155, "compression_ratio": 1.8016877637130801, "no_speech_prob": 3.7539553886745125e-05}, {"id": 904, "seek": 594092, "start": 5947.96, "end": 5954.12, "text": " still very large so we are having trying to reduce that storage um yeah I'm not sure uh", "tokens": [920, 588, 2416, 370, 321, 366, 1419, 1382, 281, 5407, 300, 6725, 1105, 1338, 286, 478, 406, 988, 2232], "temperature": 0.0, "avg_logprob": -0.4067844316071155, "compression_ratio": 1.8016877637130801, "no_speech_prob": 3.7539553886745125e-05}, {"id": 905, "seek": 594092, "start": 5954.12, "end": 5959.72, "text": " about the question about the connection between conversation and also creating uh yeah I'm not", "tokens": [466, 264, 1168, 466, 264, 4984, 1296, 3761, 293, 611, 4084, 2232, 1338, 286, 478, 406], "temperature": 0.0, "avg_logprob": -0.4067844316071155, "compression_ratio": 1.8016877637130801, "no_speech_prob": 3.7539553886745125e-05}, {"id": 906, "seek": 594092, "start": 5959.72, "end": 5964.92, "text": " sure I'm I'm I have also to ask you sorry thank you", "tokens": [988, 286, 478, 286, 478, 286, 362, 611, 281, 1029, 291, 2597, 1309, 291], "temperature": 0.0, "avg_logprob": -0.4067844316071155, "compression_ratio": 1.8016877637130801, "no_speech_prob": 3.7539553886745125e-05}, {"id": 907, "seek": 596492, "start": 5964.92, "end": 5971.32, "text": " that don't you is too um modest to mention that she was one of the co-organizers of the efficient QA", "tokens": [300, 500, 380, 291, 307, 886, 1105, 25403, 281, 2152, 300, 750, 390, 472, 295, 264, 598, 12, 12372, 22525, 295, 264, 7148, 1249, 32], "temperature": 0.0, "avg_logprob": -0.2817648649215698, "compression_ratio": 1.608910891089109, "no_speech_prob": 4.8934158257907256e-05}, {"id": 908, "seek": 596492, "start": 5971.32, "end": 5974.76, "text": " um share task um okay next question is", "tokens": [1105, 2073, 5633, 1105, 1392, 958, 1168, 307], "temperature": 0.0, "avg_logprob": -0.2817648649215698, "compression_ratio": 1.608910891089109, "no_speech_prob": 4.8934158257907256e-05}, {"id": 909, "seek": 596492, "start": 5980.12, "end": 5986.92, "text": " I think she thinks so much for being here today um so my question is a bit different um so one", "tokens": [286, 519, 750, 7309, 370, 709, 337, 885, 510, 965, 1105, 370, 452, 1168, 307, 257, 857, 819, 1105, 370, 472], "temperature": 0.0, "avg_logprob": -0.2817648649215698, "compression_ratio": 1.608910891089109, "no_speech_prob": 4.8934158257907256e-05}, {"id": 910, "seek": 596492, "start": 5986.92, "end": 5992.84, "text": " example you gave the competition was this Alex Victoria example the checklist um and I was", "tokens": [1365, 291, 2729, 264, 6211, 390, 341, 5202, 16656, 1365, 264, 30357, 1105, 293, 286, 390], "temperature": 0.0, "avg_logprob": -0.2817648649215698, "compression_ratio": 1.608910891089109, "no_speech_prob": 4.8934158257907256e-05}, {"id": 911, "seek": 599284, "start": 5992.84, "end": 5997.400000000001, "text": " like in technically Alex was an evolved answer right it's gender neutral and there wasn't enough", "tokens": [411, 294, 12120, 5202, 390, 364, 14178, 1867, 558, 309, 311, 7898, 10598, 293, 456, 2067, 380, 1547], "temperature": 0.0, "avg_logprob": -0.20231290090651738, "compression_ratio": 1.6768558951965065, "no_speech_prob": 1.8627100871526636e-05}, {"id": 912, "seek": 599284, "start": 5997.400000000001, "end": 6004.52, "text": " context context in the question to determine who it's referring to so my question is how concerned", "tokens": [4319, 4319, 294, 264, 1168, 281, 6997, 567, 309, 311, 13761, 281, 370, 452, 1168, 307, 577, 5922], "temperature": 0.0, "avg_logprob": -0.20231290090651738, "compression_ratio": 1.6768558951965065, "no_speech_prob": 1.8627100871526636e-05}, {"id": 913, "seek": 599284, "start": 6004.52, "end": 6011.64, "text": " should we be about potentially including uh sort of biases into these or go labels or how we", "tokens": [820, 321, 312, 466, 7263, 3009, 2232, 1333, 295, 32152, 666, 613, 420, 352, 16949, 420, 577, 321], "temperature": 0.0, "avg_logprob": -0.20231290090651738, "compression_ratio": 1.6768558951965065, "no_speech_prob": 1.8627100871526636e-05}, {"id": 914, "seek": 599284, "start": 6011.64, "end": 6020.4400000000005, "text": " evaluate them or is that just more of a concern for more open and new questions um yeah this is", "tokens": [13059, 552, 420, 307, 300, 445, 544, 295, 257, 3136, 337, 544, 1269, 293, 777, 1651, 1105, 1338, 341, 307], "temperature": 0.0, "avg_logprob": -0.20231290090651738, "compression_ratio": 1.6768558951965065, "no_speech_prob": 1.8627100871526636e-05}, {"id": 915, "seek": 602044, "start": 6020.44, "end": 6025.879999999999, "text": " definitely a very important again a lot of people are trying to study okay how much bias have been", "tokens": [2138, 257, 588, 1021, 797, 257, 688, 295, 561, 366, 1382, 281, 2979, 1392, 577, 709, 12577, 362, 668], "temperature": 0.0, "avg_logprob": -0.3251836348553093, "compression_ratio": 1.71875, "no_speech_prob": 5.29517137692892e-06}, {"id": 916, "seek": 602044, "start": 6025.879999999999, "end": 6032.599999999999, "text": " coding this model time how we can yeah um yeah I'm not sure if I'm good on so to do that um", "tokens": [17720, 341, 2316, 565, 577, 321, 393, 1338, 1105, 1338, 286, 478, 406, 988, 498, 286, 478, 665, 322, 370, 281, 360, 300, 1105], "temperature": 0.0, "avg_logprob": -0.3251836348553093, "compression_ratio": 1.71875, "no_speech_prob": 5.29517137692892e-06}, {"id": 917, "seek": 602044, "start": 6034.36, "end": 6038.679999999999, "text": " again I like it just I want to say like talk to do the debiasing of the pre-tronement models all", "tokens": [797, 286, 411, 309, 445, 286, 528, 281, 584, 411, 751, 281, 360, 264, 3001, 72, 3349, 295, 264, 659, 12, 83, 2044, 1712, 5245, 439], "temperature": 0.0, "avg_logprob": -0.3251836348553093, "compression_ratio": 1.71875, "no_speech_prob": 5.29517137692892e-06}, {"id": 918, "seek": 602044, "start": 6038.679999999999, "end": 6045.879999999999, "text": " these things are very important and um yeah this is just one so you're talking about this example", "tokens": [613, 721, 366, 588, 1021, 293, 1105, 1338, 341, 307, 445, 472, 370, 291, 434, 1417, 466, 341, 1365], "temperature": 0.0, "avg_logprob": -0.3251836348553093, "compression_ratio": 1.71875, "no_speech_prob": 5.29517137692892e-06}, {"id": 919, "seek": 604588, "start": 6045.88, "end": 6055.0, "text": " right so this is just one test case um yeah um yeah yeah right yeah so I guess I'm just", "tokens": [558, 370, 341, 307, 445, 472, 1500, 1389, 1105, 1338, 1105, 1338, 1338, 558, 1338, 370, 286, 2041, 286, 478, 445], "temperature": 0.0, "avg_logprob": -0.3580492482040868, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.876535290852189e-05}, {"id": 920, "seek": 604588, "start": 6055.0, "end": 6066.6, "text": " wondering who comes up with a test case is we will have more discussion of toxicity and bias", "tokens": [6359, 567, 1487, 493, 365, 257, 1500, 1389, 307, 321, 486, 362, 544, 5017, 295, 45866, 293, 12577], "temperature": 0.0, "avg_logprob": -0.3580492482040868, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.876535290852189e-05}, {"id": 921, "seek": 604588, "start": 6066.6, "end": 6073.0, "text": " coming up very soon including actually first-day lecture as well as a later lecture um not specifically", "tokens": [1348, 493, 588, 2321, 3009, 767, 700, 12, 810, 7991, 382, 731, 382, 257, 1780, 7991, 1105, 406, 4682], "temperature": 0.0, "avg_logprob": -0.3580492482040868, "compression_ratio": 1.5777777777777777, "no_speech_prob": 2.876535290852189e-05}, {"id": 922, "seek": 607300, "start": 6073.0, "end": 6083.16, "text": " about QA though um okay next person is uh right thank you for the lecture um yeah and my question", "tokens": [466, 1249, 32, 1673, 1105, 1392, 958, 954, 307, 2232, 558, 1309, 291, 337, 264, 7991, 1105, 1338, 293, 452, 1168], "temperature": 0.0, "avg_logprob": -0.26585957407951355, "compression_ratio": 1.5932203389830508, "no_speech_prob": 2.0428014977369457e-05}, {"id": 923, "seek": 607300, "start": 6083.16, "end": 6091.0, "text": " is also related to the open domain question answering so um I was just wondering how much of like", "tokens": [307, 611, 4077, 281, 264, 1269, 9274, 1168, 13430, 370, 1105, 286, 390, 445, 6359, 577, 709, 295, 411], "temperature": 0.0, "avg_logprob": -0.26585957407951355, "compression_ratio": 1.5932203389830508, "no_speech_prob": 2.0428014977369457e-05}, {"id": 924, "seek": 607300, "start": 6092.2, "end": 6100.68, "text": " the learning side of um domain like sort of authorization or like domain alignments um", "tokens": [264, 2539, 1252, 295, 1105, 9274, 411, 1333, 295, 33697, 420, 411, 9274, 7975, 1117, 1105], "temperature": 0.0, "avg_logprob": -0.26585957407951355, "compression_ratio": 1.5932203389830508, "no_speech_prob": 2.0428014977369457e-05}, {"id": 925, "seek": 610068, "start": 6100.68, "end": 6109.56, "text": " techniques can be combined with like the language level like question answering like to what", "tokens": [7512, 393, 312, 9354, 365, 411, 264, 2856, 1496, 411, 1168, 13430, 411, 281, 437], "temperature": 0.0, "avg_logprob": -0.23666785892687345, "compression_ratio": 1.7826086956521738, "no_speech_prob": 8.602727029938251e-05}, {"id": 926, "seek": 610068, "start": 6109.56, "end": 6115.320000000001, "text": " extends where they work and like what kind of like the language specific design should be", "tokens": [26448, 689, 436, 589, 293, 411, 437, 733, 295, 411, 264, 2856, 2685, 1715, 820, 312], "temperature": 0.0, "avg_logprob": -0.23666785892687345, "compression_ratio": 1.7826086956521738, "no_speech_prob": 8.602727029938251e-05}, {"id": 927, "seek": 610068, "start": 6115.320000000001, "end": 6122.360000000001, "text": " leveraged to combine with those two two different um it's if if we want like higher performance", "tokens": [12451, 2980, 281, 10432, 365, 729, 732, 732, 819, 1105, 309, 311, 498, 498, 321, 528, 411, 2946, 3389], "temperature": 0.0, "avg_logprob": -0.23666785892687345, "compression_ratio": 1.7826086956521738, "no_speech_prob": 8.602727029938251e-05}, {"id": 928, "seek": 610068, "start": 6122.360000000001, "end": 6128.52, "text": " and stuff like that the question about how to generalize between different domains or like", "tokens": [293, 1507, 411, 300, 264, 1168, 466, 577, 281, 2674, 1125, 1296, 819, 25514, 420, 411], "temperature": 0.0, "avg_logprob": -0.23666785892687345, "compression_ratio": 1.7826086956521738, "no_speech_prob": 8.602727029938251e-05}, {"id": 929, "seek": 612852, "start": 6128.52, "end": 6133.080000000001, "text": " all about how to extend open domain to assist them for different languages I'm watching", "tokens": [439, 466, 577, 281, 10101, 1269, 9274, 281, 4255, 552, 337, 819, 8650, 286, 478, 1976], "temperature": 0.0, "avg_logprob": -0.5099664718385727, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.8617167370393872e-05}, {"id": 930, "seek": 612852, "start": 6133.080000000001, "end": 6141.400000000001, "text": " together yeah great so I was wondering so um so there's like um some some like many specific designs", "tokens": [1214, 1338, 869, 370, 286, 390, 6359, 370, 1105, 370, 456, 311, 411, 1105, 512, 512, 411, 867, 2685, 11347], "temperature": 0.0, "avg_logprob": -0.5099664718385727, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.8617167370393872e-05}, {"id": 931, "seek": 612852, "start": 6141.400000000001, "end": 6149.88, "text": " like um um uh domain alignments and like uh future level disentanglement techniques uh that have", "tokens": [411, 1105, 1105, 2232, 9274, 7975, 1117, 293, 411, 2232, 2027, 1496, 37313, 656, 3054, 7512, 2232, 300, 362], "temperature": 0.0, "avg_logprob": -0.5099664718385727, "compression_ratio": 1.5833333333333333, "no_speech_prob": 1.8617167370393872e-05}, {"id": 932, "seek": 614988, "start": 6149.88, "end": 6158.6, "text": " been that has shown some like like interesting performance and um other tasks um it may solve that", "tokens": [668, 300, 575, 4898, 512, 411, 411, 1880, 3389, 293, 1105, 661, 9608, 1105, 309, 815, 5039, 300], "temperature": 0.0, "avg_logprob": -0.2804823045606737, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.996455234300811e-06}, {"id": 933, "seek": 614988, "start": 6158.6, "end": 6164.76, "text": " like recently some people also like leverage some other things um like for for for question answering", "tokens": [411, 3938, 512, 561, 611, 411, 13982, 512, 661, 721, 1105, 411, 337, 337, 337, 1168, 13430], "temperature": 0.0, "avg_logprob": -0.2804823045606737, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.996455234300811e-06}, {"id": 934, "seek": 614988, "start": 6164.76, "end": 6172.2, "text": " so I was just wondering um like to what extent these kind of techniques come work on um like", "tokens": [370, 286, 390, 445, 6359, 1105, 411, 281, 437, 8396, 613, 733, 295, 7512, 808, 589, 322, 1105, 411], "temperature": 0.0, "avg_logprob": -0.2804823045606737, "compression_ratio": 1.8695652173913044, "no_speech_prob": 3.996455234300811e-06}, {"id": 935, "seek": 617220, "start": 6172.2, "end": 6179.0, "text": " uh what in group tasks modules limited question answering but like um mainly uh for question answering", "tokens": [2232, 437, 294, 1594, 9608, 16679, 5567, 1168, 13430, 457, 411, 1105, 8704, 2232, 337, 1168, 13430], "temperature": 0.0, "avg_logprob": -0.44709666182355184, "compression_ratio": 1.7109004739336493, "no_speech_prob": 1.2598372450156603e-05}, {"id": 936, "seek": 617220, "start": 6180.36, "end": 6185.24, "text": " sorry which which work I was talking about um not you know sure what do you mean by this", "tokens": [2597, 597, 597, 589, 286, 390, 1417, 466, 1105, 406, 291, 458, 988, 437, 360, 291, 914, 538, 341], "temperature": 0.0, "avg_logprob": -0.44709666182355184, "compression_ratio": 1.7109004739336493, "no_speech_prob": 1.2598372450156603e-05}, {"id": 937, "seek": 617220, "start": 6185.24, "end": 6191.8, "text": " in hand go to your school questions right so uh so basically um believe this is like a", "tokens": [294, 1011, 352, 281, 428, 1395, 1651, 558, 370, 2232, 370, 1936, 1105, 1697, 341, 307, 411, 257], "temperature": 0.0, "avg_logprob": -0.44709666182355184, "compression_ratio": 1.7109004739336493, "no_speech_prob": 1.2598372450156603e-05}, {"id": 938, "seek": 617220, "start": 6191.8, "end": 6199.32, "text": " little bit more specific for so um so there is this um paper called um um doing um", "tokens": [707, 857, 544, 2685, 337, 370, 1105, 370, 456, 307, 341, 1105, 3035, 1219, 1105, 1105, 884, 1105], "temperature": 0.0, "avg_logprob": -0.44709666182355184, "compression_ratio": 1.7109004739336493, "no_speech_prob": 1.2598372450156603e-05}, {"id": 939, "seek": 619932, "start": 6199.32, "end": 6207.88, "text": " um I forgot the exact name uh it's uh like sorry align true domains um by decent", "tokens": [1105, 286, 5298, 264, 1900, 1315, 2232, 309, 311, 2232, 411, 2597, 7975, 2074, 25514, 1105, 538, 8681], "temperature": 0.0, "avg_logprob": -0.4927051067352295, "compression_ratio": 1.544378698224852, "no_speech_prob": 1.1839908438560087e-05}, {"id": 940, "seek": 619932, "start": 6215.08, "end": 6218.92, "text": " okay so I have okay just want to make sure that we are on the same page so I have things", "tokens": [1392, 370, 286, 362, 1392, 445, 528, 281, 652, 988, 300, 321, 366, 322, 264, 912, 3028, 370, 286, 362, 721], "temperature": 0.0, "avg_logprob": -0.4927051067352295, "compression_ratio": 1.544378698224852, "no_speech_prob": 1.1839908438560087e-05}, {"id": 941, "seek": 619932, "start": 6218.92, "end": 6224.679999999999, "text": " of work that trying to learn some kind of in decent angle rotation second better generalize", "tokens": [295, 589, 300, 1382, 281, 1466, 512, 733, 295, 294, 8681, 5802, 12447, 1150, 1101, 2674, 1125], "temperature": 0.0, "avg_logprob": -0.4927051067352295, "compression_ratio": 1.544378698224852, "no_speech_prob": 1.1839908438560087e-05}, {"id": 942, "seek": 622468, "start": 6224.68, "end": 6231.400000000001, "text": " to the different domains or adverse examples is this what do you say yeah yeah and the question", "tokens": [281, 264, 819, 25514, 420, 27590, 5110, 307, 341, 437, 360, 291, 584, 1338, 1338, 293, 264, 1168], "temperature": 0.0, "avg_logprob": -0.2770000168039829, "compression_ratio": 1.76036866359447, "no_speech_prob": 1.1298978279228322e-05}, {"id": 943, "seek": 622468, "start": 6231.400000000001, "end": 6237.8, "text": " is whether this technique kind of general general general you apply to question answering oh yeah", "tokens": [307, 1968, 341, 6532, 733, 295, 2674, 2674, 2674, 291, 3079, 281, 1168, 13430, 1954, 1338], "temperature": 0.0, "avg_logprob": -0.2770000168039829, "compression_ratio": 1.76036866359447, "no_speech_prob": 1.1298978279228322e-05}, {"id": 944, "seek": 622468, "start": 6237.8, "end": 6244.52, "text": " you're just wondering how um to what extent would they work because um I think language has like", "tokens": [291, 434, 445, 6359, 577, 1105, 281, 437, 8396, 576, 436, 589, 570, 1105, 286, 519, 2856, 575, 411], "temperature": 0.0, "avg_logprob": -0.2770000168039829, "compression_ratio": 1.76036866359447, "no_speech_prob": 1.1298978279228322e-05}, {"id": 945, "seek": 622468, "start": 6245.16, "end": 6250.84, "text": " a lot of like specific things like dependency and to other stuff that like these techniques", "tokens": [257, 688, 295, 411, 2685, 721, 411, 33621, 293, 281, 661, 1507, 300, 411, 613, 7512], "temperature": 0.0, "avg_logprob": -0.2770000168039829, "compression_ratio": 1.76036866359447, "no_speech_prob": 1.1298978279228322e-05}, {"id": 946, "seek": 625084, "start": 6250.84, "end": 6262.52, "text": " does not like actually um take care of several languages um yeah um yeah um yeah I'm not sure uh I think", "tokens": [775, 406, 411, 767, 1105, 747, 1127, 295, 2940, 8650, 1105, 1338, 1105, 1338, 1105, 1338, 286, 478, 406, 988, 2232, 286, 519], "temperature": 0.0, "avg_logprob": -0.3608240127563477, "compression_ratio": 1.627027027027027, "no_speech_prob": 9.969137863663491e-06}, {"id": 947, "seek": 625084, "start": 6262.52, "end": 6268.04, "text": " we have to try that for the subtle interesting point yeah I don't know at least for the work that", "tokens": [321, 362, 281, 853, 300, 337, 264, 13743, 1880, 935, 1338, 286, 500, 380, 458, 412, 1935, 337, 264, 589, 300], "temperature": 0.0, "avg_logprob": -0.3608240127563477, "compression_ratio": 1.627027027027027, "no_speech_prob": 9.969137863663491e-06}, {"id": 948, "seek": 625084, "start": 6268.04, "end": 6274.76, "text": " I have since though fact all all of the title operated at a very simple uh sentence classification", "tokens": [286, 362, 1670, 1673, 1186, 439, 439, 295, 264, 4876, 20826, 412, 257, 588, 2199, 2232, 8174, 21538], "temperature": 0.0, "avg_logprob": -0.3608240127563477, "compression_ratio": 1.627027027027027, "no_speech_prob": 9.969137863663491e-06}, {"id": 949, "seek": 627476, "start": 6274.76, "end": 6281.64, "text": " task maybe I'm almost maybe that's not correct so my own thing is that's basically take a", "tokens": [5633, 1310, 286, 478, 1920, 1310, 300, 311, 406, 3006, 370, 452, 1065, 551, 307, 300, 311, 1936, 747, 257], "temperature": 0.0, "avg_logprob": -0.47671122934626436, "compression_ratio": 1.7399103139013452, "no_speech_prob": 5.953813342784997e-06}, {"id": 950, "seek": 627476, "start": 6281.64, "end": 6287.56, "text": " label encoder apply to a simple test of classification task and take a documentation do some time", "tokens": [7645, 2058, 19866, 3079, 281, 257, 2199, 1500, 295, 21538, 5633, 293, 747, 257, 14333, 360, 512, 565], "temperature": 0.0, "avg_logprob": -0.47671122934626436, "compression_ratio": 1.7399103139013452, "no_speech_prob": 5.953813342784997e-06}, {"id": 951, "seek": 627476, "start": 6287.56, "end": 6293.72, "text": " to solve or transformation and make sure that um yeah it can learn some kind of invariant features", "tokens": [281, 5039, 420, 9887, 293, 652, 988, 300, 1105, 1338, 309, 393, 1466, 512, 733, 295, 33270, 394, 4122], "temperature": 0.0, "avg_logprob": -0.47671122934626436, "compression_ratio": 1.7399103139013452, "no_speech_prob": 5.953813342784997e-06}, {"id": 952, "seek": 627476, "start": 6293.72, "end": 6301.4800000000005, "text": " about the header rotation something like that very yeah cool very input yeah I'm not sure I feel like", "tokens": [466, 264, 23117, 12447, 746, 411, 300, 588, 1338, 1627, 588, 4846, 1338, 286, 478, 406, 988, 286, 841, 411], "temperature": 0.0, "avg_logprob": -0.47671122934626436, "compression_ratio": 1.7399103139013452, "no_speech_prob": 5.953813342784997e-06}, {"id": 953, "seek": 630148, "start": 6301.48, "end": 6310.2, "text": " QA is a more structured task and also kind of longer uh balance sequences um yeah so I don't know", "tokens": [1249, 32, 307, 257, 544, 18519, 5633, 293, 611, 733, 295, 2854, 2232, 4772, 22978, 1105, 1338, 370, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.20831570696474902, "compression_ratio": 1.5027322404371584, "no_speech_prob": 1.4962502973503433e-05}, {"id": 954, "seek": 630148, "start": 6310.2, "end": 6317.32, "text": " if it works unless people have tried that yeah thank you thank you okay and then we've got", "tokens": [498, 309, 1985, 5969, 561, 362, 3031, 300, 1338, 1309, 291, 1309, 291, 1392, 293, 550, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.20831570696474902, "compression_ratio": 1.5027322404371584, "no_speech_prob": 1.4962502973503433e-05}, {"id": 955, "seek": 630148, "start": 6317.32, "end": 6323.879999999999, "text": " and maybe we should call this the last question um I'm just wondering what is like the", "tokens": [293, 1310, 321, 820, 818, 341, 264, 1036, 1168, 1105, 286, 478, 445, 6359, 437, 307, 411, 264], "temperature": 0.0, "avg_logprob": -0.20831570696474902, "compression_ratio": 1.5027322404371584, "no_speech_prob": 1.4962502973503433e-05}, {"id": 956, "seek": 632388, "start": 6323.88, "end": 6332.68, "text": " alternative difference between solving question answering with um dance models like T5 versus encoder", "tokens": [8535, 2649, 1296, 12606, 1168, 13430, 365, 1105, 4489, 5245, 411, 314, 20, 5717, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.34828412532806396, "compression_ratio": 1.5736842105263158, "no_speech_prob": 9.971623512683436e-06}, {"id": 957, "seek": 632388, "start": 6332.68, "end": 6342.84, "text": " swipeboard okay um that's good point uh okay so I so I skipped this slide so why does model work so", "tokens": [28170, 3787, 1392, 1105, 300, 311, 665, 935, 2232, 1392, 370, 286, 370, 286, 30193, 341, 4137, 370, 983, 775, 2316, 589, 370], "temperature": 0.0, "avg_logprob": -0.34828412532806396, "compression_ratio": 1.5736842105263158, "no_speech_prob": 9.971623512683436e-06}, {"id": 958, "seek": 632388, "start": 6342.84, "end": 6348.12, "text": " well uh the reason is actually it's not really about extracting model versus generating model the", "tokens": [731, 2232, 264, 1778, 307, 767, 309, 311, 406, 534, 466, 49844, 2316, 5717, 17746, 2316, 264], "temperature": 0.0, "avg_logprob": -0.34828412532806396, "compression_ratio": 1.5736842105263158, "no_speech_prob": 9.971623512683436e-06}, {"id": 959, "seek": 634812, "start": 6348.12, "end": 6355.8, "text": " reason is that they actually um for the extracting model so if the retrieval returns let's say", "tokens": [1778, 307, 300, 436, 767, 1105, 337, 264, 49844, 2316, 370, 498, 264, 19817, 3337, 11247, 718, 311, 584], "temperature": 0.0, "avg_logprob": -0.28525978181420303, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.0136595746153034e-06}, {"id": 960, "seek": 634812, "start": 6355.8, "end": 6362.12, "text": " 100 passages so they have to extract also from each of the passages and finally figure out which", "tokens": [2319, 31589, 370, 436, 362, 281, 8947, 611, 490, 1184, 295, 264, 31589, 293, 2721, 2573, 484, 597], "temperature": 0.0, "avg_logprob": -0.28525978181420303, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.0136595746153034e-06}, {"id": 961, "seek": 634812, "start": 6362.12, "end": 6367.4, "text": " one that has a might hide score but for the generation model essentially they're trying to aggregate", "tokens": [472, 300, 575, 257, 1062, 6479, 6175, 457, 337, 264, 5125, 2316, 4476, 436, 434, 1382, 281, 26118], "temperature": 0.0, "avg_logprob": -0.28525978181420303, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.0136595746153034e-06}, {"id": 962, "seek": 634812, "start": 6367.4, "end": 6374.36, "text": " all the 100 passages and the direct stations together and do their generation um jointly do you", "tokens": [439, 264, 2319, 31589, 293, 264, 2047, 13390, 1214, 293, 360, 641, 5125, 1105, 46557, 360, 291], "temperature": 0.0, "avg_logprob": -0.28525978181420303, "compression_ratio": 1.7716894977168949, "no_speech_prob": 5.0136595746153034e-06}, {"id": 963, "seek": 637436, "start": 6374.36, "end": 6380.759999999999, "text": " understand so essentially taking the 100 stations together to the joint generation instead of only", "tokens": [1223, 370, 4476, 1940, 264, 2319, 13390, 1214, 281, 264, 7225, 5125, 2602, 295, 787], "temperature": 0.0, "avg_logprob": -0.29437168345731846, "compression_ratio": 1.7066666666666668, "no_speech_prob": 5.41972531209467e-06}, {"id": 964, "seek": 637436, "start": 6380.759999999999, "end": 6386.599999999999, "text": " do the extraction from each of the passages so I think that's actually the T difference so that's", "tokens": [360, 264, 30197, 490, 1184, 295, 264, 31589, 370, 286, 519, 300, 311, 767, 264, 314, 2649, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.29437168345731846, "compression_ratio": 1.7066666666666668, "no_speech_prob": 5.41972531209467e-06}, {"id": 965, "seek": 637436, "start": 6386.599999999999, "end": 6393.08, "text": " why this generating model can do really well compared to the extracting models so also I'll", "tokens": [983, 341, 17746, 2316, 393, 360, 534, 731, 5347, 281, 264, 49844, 5245, 370, 611, 286, 603], "temperature": 0.0, "avg_logprob": -0.29437168345731846, "compression_ratio": 1.7066666666666668, "no_speech_prob": 5.41972531209467e-06}, {"id": 966, "seek": 637436, "start": 6393.08, "end": 6400.2, "text": " dimension that okay so if you look at this i.g model it's actually um like uh compress this DPR", "tokens": [10139, 300, 1392, 370, 498, 291, 574, 412, 341, 741, 13, 70, 2316, 309, 311, 767, 1105, 411, 2232, 14778, 341, 413, 15958], "temperature": 0.0, "avg_logprob": -0.29437168345731846, "compression_ratio": 1.7066666666666668, "no_speech_prob": 5.41972531209467e-06}, {"id": 967, "seek": 640020, "start": 6400.2, "end": 6405.08, "text": " and i.g model the i.g model is always doing the generating model but they're not doing this kind", "tokens": [293, 741, 13, 70, 2316, 264, 741, 13, 70, 2316, 307, 1009, 884, 264, 17746, 2316, 457, 436, 434, 406, 884, 341, 733], "temperature": 0.0, "avg_logprob": -0.19489355254591556, "compression_ratio": 2.0327868852459017, "no_speech_prob": 3.7039757444290444e-05}, {"id": 968, "seek": 640020, "start": 6405.08, "end": 6411.48, "text": " of aggregation they're just trying to take a single passage and doing the generation so the i.g model", "tokens": [295, 16743, 399, 436, 434, 445, 1382, 281, 747, 257, 2167, 11497, 293, 884, 264, 5125, 370, 264, 741, 13, 70, 2316], "temperature": 0.0, "avg_logprob": -0.19489355254591556, "compression_ratio": 2.0327868852459017, "no_speech_prob": 3.7039757444290444e-05}, {"id": 969, "seek": 640020, "start": 6411.48, "end": 6416.92, "text": " actually um doesn't perform as well as this model but we also animation i.g model is actually not", "tokens": [767, 1105, 1177, 380, 2042, 382, 731, 382, 341, 2316, 457, 321, 611, 9603, 741, 13, 70, 2316, 307, 767, 406], "temperature": 0.0, "avg_logprob": -0.19489355254591556, "compression_ratio": 2.0327868852459017, "no_speech_prob": 3.7039757444290444e-05}, {"id": 970, "seek": 640020, "start": 6416.92, "end": 6422.12, "text": " doing better than DPR because this base model is this large model so this this numbers are a little", "tokens": [884, 1101, 813, 413, 15958, 570, 341, 3096, 2316, 307, 341, 2416, 2316, 370, 341, 341, 3547, 366, 257, 707], "temperature": 0.0, "avg_logprob": -0.19489355254591556, "compression_ratio": 2.0327868852459017, "no_speech_prob": 3.7039757444290444e-05}, {"id": 971, "seek": 640020, "start": 6422.12, "end": 6426.44, "text": " bit confusing so they're actually basically a really own part they're basically performed similarly", "tokens": [857, 13181, 370, 436, 434, 767, 1936, 257, 534, 1065, 644, 436, 434, 1936, 10332, 14138], "temperature": 0.0, "avg_logprob": -0.19489355254591556, "compression_ratio": 2.0327868852459017, "no_speech_prob": 3.7039757444290444e-05}, {"id": 972, "seek": 642644, "start": 6426.44, "end": 6431.799999999999, "text": " but um so the the key difference between the generating model and extracting model is that for", "tokens": [457, 1105, 370, 264, 264, 2141, 2649, 1296, 264, 17746, 2316, 293, 49844, 2316, 307, 300, 337], "temperature": 0.0, "avg_logprob": -0.36079032332808886, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.3005601431359537e-05}, {"id": 973, "seek": 642644, "start": 6431.799999999999, "end": 6437.0, "text": " general models you can actually leverage more on input passage together and do the generation", "tokens": [2674, 5245, 291, 393, 767, 13982, 544, 322, 4846, 11497, 1214, 293, 360, 264, 5125], "temperature": 0.0, "avg_logprob": -0.36079032332808886, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.3005601431359537e-05}, {"id": 974, "seek": 642644, "start": 6437.0, "end": 6448.599999999999, "text": " look does this is that clear or not um yes thanks yeah i know that's usually just check out this", "tokens": [574, 775, 341, 307, 300, 1850, 420, 406, 1105, 2086, 3231, 1338, 741, 458, 300, 311, 2673, 445, 1520, 484, 341], "temperature": 0.0, "avg_logprob": -0.36079032332808886, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.3005601431359537e-05}, {"id": 975, "seek": 642644, "start": 6448.599999999999, "end": 6453.48, "text": " paper yeah so this paper asks the query well then why this model has worked better than the", "tokens": [3035, 1338, 370, 341, 3035, 8962, 264, 14581, 731, 550, 983, 341, 2316, 575, 2732, 1101, 813, 264], "temperature": 0.0, "avg_logprob": -0.36079032332808886, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.3005601431359537e-05}, {"id": 976, "seek": 645348, "start": 6453.48, "end": 6461.32, "text": " previous generating model actually uh follow up this page is janity models to do like wait um", "tokens": [3894, 17746, 2316, 767, 2232, 1524, 493, 341, 3028, 307, 25442, 507, 5245, 281, 360, 411, 1699, 1105], "temperature": 0.0, "avg_logprob": -0.6729129392709305, "compression_ratio": 1.64, "no_speech_prob": 1.3948888408776838e-05}, {"id": 977, "seek": 645348, "start": 6462.36, "end": 6469.4, "text": " simple fashion and you know when you have the documents and vice-fabriccial page janity models to", "tokens": [2199, 6700, 293, 291, 458, 562, 291, 362, 264, 8512, 293, 11964, 12, 69, 455, 1341, 1013, 3028, 25442, 507, 5245, 281], "temperature": 0.0, "avg_logprob": -0.6729129392709305, "compression_ratio": 1.64, "no_speech_prob": 1.3948888408776838e-05}, {"id": 978, "seek": 645348, "start": 6469.4, "end": 6479.08, "text": " generate the span wait the yeah you you can definitely do that here oh so you are talking about", "tokens": [8460, 264, 16174, 1699, 264, 1338, 291, 291, 393, 2138, 360, 300, 510, 1954, 370, 291, 366, 1417, 466], "temperature": 0.0, "avg_logprob": -0.6729129392709305, "compression_ratio": 1.64, "no_speech_prob": 1.3948888408776838e-05}, {"id": 979, "seek": 647908, "start": 6479.08, "end": 6486.36, "text": " this tip i'm not this yeah so i'm just wondering about like if you use encoders is like", "tokens": [341, 4125, 741, 478, 406, 341, 1338, 370, 741, 478, 445, 6359, 466, 411, 498, 291, 764, 2058, 378, 433, 307, 411], "temperature": 0.0, "avg_logprob": -0.4127353073714615, "compression_ratio": 1.706806282722513, "no_speech_prob": 1.0362905413785484e-05}, {"id": 980, "seek": 647908, "start": 6486.92, "end": 6492.44, "text": " you're finding similarities between very encodings uh and then generating models are you", "tokens": [291, 434, 5006, 24197, 1296, 588, 2058, 378, 1109, 2232, 293, 550, 17746, 5245, 366, 291], "temperature": 0.0, "avg_logprob": -0.4127353073714615, "compression_ratio": 1.706806282722513, "no_speech_prob": 1.0362905413785484e-05}, {"id": 981, "seek": 647908, "start": 6492.44, "end": 6500.44, "text": " like remembering a whole question and you try to retrieve them in a way", "tokens": [411, 20719, 257, 1379, 1168, 293, 291, 853, 281, 30254, 552, 294, 257, 636], "temperature": 0.0, "avg_logprob": -0.4127353073714615, "compression_ratio": 1.706806282722513, "no_speech_prob": 1.0362905413785484e-05}, {"id": 982, "seek": 650044, "start": 6500.44, "end": 6511.639999999999, "text": " okay so for this model there is an any retrieval so you can't really find the answer from the", "tokens": [1392, 370, 337, 341, 2316, 456, 307, 364, 604, 19817, 3337, 370, 291, 393, 380, 534, 915, 264, 1867, 490, 264], "temperature": 0.0, "avg_logprob": -0.2741573333740234, "compression_ratio": 1.83, "no_speech_prob": 6.6401671574567445e-06}, {"id": 983, "seek": 650044, "start": 6511.639999999999, "end": 6516.28, "text": " question right so this model really has to rely on all the parameters you remember", "tokens": [1168, 558, 370, 341, 2316, 534, 575, 281, 10687, 322, 439, 264, 9834, 291, 1604], "temperature": 0.0, "avg_logprob": -0.2741573333740234, "compression_ratio": 1.83, "no_speech_prob": 6.6401671574567445e-06}, {"id": 984, "seek": 650044, "start": 6516.839999999999, "end": 6521.799999999999, "text": " all the information so by just taking the scene for the task to just rely on the parameters", "tokens": [439, 264, 1589, 370, 538, 445, 1940, 264, 4145, 337, 264, 5633, 281, 445, 10687, 322, 264, 9834], "temperature": 0.0, "avg_logprob": -0.2741573333740234, "compression_ratio": 1.83, "no_speech_prob": 6.6401671574567445e-06}, {"id": 985, "seek": 650044, "start": 6521.799999999999, "end": 6527.879999999999, "text": " to infer this answer so it's actually very hard to so it's yeah it's definitely a balance between", "tokens": [281, 13596, 341, 1867, 370, 309, 311, 767, 588, 1152, 281, 370, 309, 311, 1338, 309, 311, 2138, 257, 4772, 1296], "temperature": 0.0, "avg_logprob": -0.2741573333740234, "compression_ratio": 1.83, "no_speech_prob": 6.6401671574567445e-06}, {"id": 986, "seek": 652788, "start": 6527.88, "end": 6537.400000000001, "text": " all the yeah it's a memory and a generalization problem yeah i see so um i'm just gonna say what i", "tokens": [439, 264, 1338, 309, 311, 257, 4675, 293, 257, 2674, 2144, 1154, 1338, 741, 536, 370, 1105, 741, 478, 445, 799, 584, 437, 741], "temperature": 0.0, "avg_logprob": -0.4745904032389323, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.590974978986196e-05}, {"id": 987, "seek": 652788, "start": 6537.400000000001, "end": 6542.84, "text": " like is it but when you've got this question because you're adding it in some space and then", "tokens": [411, 307, 309, 457, 562, 291, 600, 658, 341, 1168, 570, 291, 434, 5127, 309, 294, 512, 1901, 293, 550], "temperature": 0.0, "avg_logprob": -0.4745904032389323, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.590974978986196e-05}, {"id": 988, "seek": 652788, "start": 6543.64, "end": 6552.28, "text": " using that you're adding the generator matches that to 18, 18, 18, is that what is going on either", "tokens": [1228, 300, 291, 434, 5127, 264, 19265, 10676, 300, 281, 2443, 11, 2443, 11, 2443, 11, 307, 300, 437, 307, 516, 322, 2139], "temperature": 0.0, "avg_logprob": -0.4745904032389323, "compression_ratio": 1.6111111111111112, "no_speech_prob": 5.590974978986196e-05}, {"id": 989, "seek": 655228, "start": 6552.28, "end": 6560.04, "text": " yeah exactly yes the model has to like it's very large like 11 billion parameters so all this", "tokens": [1338, 2293, 2086, 264, 2316, 575, 281, 411, 309, 311, 588, 2416, 411, 2975, 5218, 9834, 370, 439, 341], "temperature": 0.0, "avg_logprob": -0.2653802378793781, "compression_ratio": 1.9343434343434343, "no_speech_prob": 1.7764579752110876e-05}, {"id": 990, "seek": 655228, "start": 6560.04, "end": 6565.88, "text": " so parameters visit trying to uh yeah memorize a lot of information that has been because the", "tokens": [370, 9834, 3441, 1382, 281, 2232, 1338, 27478, 257, 688, 295, 1589, 300, 575, 668, 570, 264], "temperature": 0.0, "avg_logprob": -0.2653802378793781, "compression_ratio": 1.9343434343434343, "no_speech_prob": 1.7764579752110876e-05}, {"id": 991, "seek": 655228, "start": 6565.88, "end": 6571.4, "text": " model has been retrieved from the text and also has been funky so the model has been trying to", "tokens": [2316, 575, 668, 19817, 937, 490, 264, 2487, 293, 611, 575, 668, 33499, 370, 264, 2316, 575, 668, 1382, 281], "temperature": 0.0, "avg_logprob": -0.2653802378793781, "compression_ratio": 1.9343434343434343, "no_speech_prob": 1.7764579752110876e-05}, {"id": 992, "seek": 655228, "start": 6571.4, "end": 6578.679999999999, "text": " memorize a lot of information about the text here um do you want to call it an id or do you want one", "tokens": [27478, 257, 688, 295, 1589, 466, 264, 2487, 510, 1105, 360, 291, 528, 281, 818, 309, 364, 4496, 420, 360, 291, 528, 472], "temperature": 0.0, "avg_logprob": -0.2653802378793781, "compression_ratio": 1.9343434343434343, "no_speech_prob": 1.7764579752110876e-05}, {"id": 993, "seek": 657868, "start": 6578.68, "end": 6588.68, "text": " more question uh either way yeah i'm sorry you just show around how do you take one more question", "tokens": [544, 1168, 2232, 2139, 636, 1338, 741, 478, 2597, 291, 445, 855, 926, 577, 360, 291, 747, 472, 544, 1168], "temperature": 0.0, "avg_logprob": -0.45593433692807056, "compression_ratio": 1.7454545454545454, "no_speech_prob": 9.73316709860228e-05}, {"id": 994, "seek": 657868, "start": 6588.68, "end": 6597.72, "text": " okay let me just do okay one more question okay the first question is about how how you really are", "tokens": [1392, 718, 385, 445, 360, 1392, 472, 544, 1168, 1392, 264, 700, 1168, 307, 466, 577, 577, 291, 534, 366], "temperature": 0.0, "avg_logprob": -0.45593433692807056, "compression_ratio": 1.7454545454545454, "no_speech_prob": 9.73316709860228e-05}, {"id": 995, "seek": 657868, "start": 6597.72, "end": 6604.12, "text": " these techniques generalized to other languages like say languages that are quite different", "tokens": [613, 7512, 44498, 281, 661, 8650, 411, 584, 8650, 300, 366, 1596, 819], "temperature": 0.0, "avg_logprob": -0.45593433692807056, "compression_ratio": 1.7454545454545454, "no_speech_prob": 9.73316709860228e-05}, {"id": 996, "seek": 660412, "start": 6604.12, "end": 6610.599999999999, "text": " or quite different grammatical rules which Chinese um Japanese or Arabic or some other languages", "tokens": [420, 1596, 819, 17570, 267, 804, 4474, 597, 4649, 1105, 5433, 420, 19938, 420, 512, 661, 8650], "temperature": 0.0, "avg_logprob": -0.3046693439725079, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.085380250122398e-05}, {"id": 997, "seek": 660412, "start": 6611.32, "end": 6618.12, "text": " sort of wonderful question maybe not right exactly your domain expertise is there's a lot of", "tokens": [1333, 295, 3715, 1168, 1310, 406, 558, 2293, 428, 9274, 11769, 307, 456, 311, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.3046693439725079, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.085380250122398e-05}, {"id": 998, "seek": 660412, "start": 6618.12, "end": 6625.24, "text": " interest in modeling user behavior say online searching behavior browsing behavior as a sequence", "tokens": [1179, 294, 15983, 4195, 5223, 584, 2950, 10808, 5223, 38602, 5223, 382, 257, 8310], "temperature": 0.0, "avg_logprob": -0.3046693439725079, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.085380250122398e-05}, {"id": 999, "seek": 660412, "start": 6625.24, "end": 6631.48, "text": " using state transformers self-attention um and then you can use that to predict how user or can", "tokens": [1228, 1785, 4088, 433, 2698, 12, 1591, 1251, 1105, 293, 550, 291, 393, 764, 300, 281, 6069, 577, 4195, 420, 393], "temperature": 0.0, "avg_logprob": -0.3046693439725079, "compression_ratio": 1.6118143459915613, "no_speech_prob": 8.085380250122398e-05}, {"id": 1000, "seek": 663148, "start": 6631.48, "end": 6637.959999999999, "text": " be like an user or a selector and then predict user's actions hope promising do you think that", "tokens": [312, 411, 364, 4195, 420, 257, 23264, 1672, 293, 550, 6069, 4195, 311, 5909, 1454, 20257, 360, 291, 519, 300], "temperature": 0.0, "avg_logprob": -0.3451465009206749, "compression_ratio": 1.7716894977168949, "no_speech_prob": 3.534971983754076e-05}, {"id": 1001, "seek": 663148, "start": 6637.959999999999, "end": 6643.16, "text": " would be i know this may not be your domain expertise but there is a lot of interest in extending", "tokens": [576, 312, 741, 458, 341, 815, 406, 312, 428, 9274, 11769, 457, 456, 307, 257, 688, 295, 1179, 294, 24360], "temperature": 0.0, "avg_logprob": -0.3451465009206749, "compression_ratio": 1.7716894977168949, "no_speech_prob": 3.534971983754076e-05}, {"id": 1002, "seek": 663148, "start": 6643.16, "end": 6649.16, "text": " these uh questions from techniques or just encoding techniques embedding techniques to recommend", "tokens": [613, 2232, 1651, 490, 7512, 420, 445, 43430, 7512, 12240, 3584, 7512, 281, 2748], "temperature": 0.0, "avg_logprob": -0.3451465009206749, "compression_ratio": 1.7716894977168949, "no_speech_prob": 3.534971983754076e-05}, {"id": 1003, "seek": 663148, "start": 6649.16, "end": 6659.08, "text": " their systems um just want to get your thoughts on your mind okay um the first question is whether", "tokens": [641, 3652, 1105, 445, 528, 281, 483, 428, 4598, 322, 428, 1575, 1392, 1105, 264, 700, 1168, 307, 1968], "temperature": 0.0, "avg_logprob": -0.3451465009206749, "compression_ratio": 1.7716894977168949, "no_speech_prob": 3.534971983754076e-05}, {"id": 1004, "seek": 665908, "start": 6659.08, "end": 6665.5599999999995, "text": " the these techniques can be generalized to other languages i think it's also yes and that has", "tokens": [264, 613, 7512, 393, 312, 44498, 281, 661, 8650, 741, 519, 309, 311, 611, 2086, 293, 300, 575], "temperature": 0.0, "avg_logprob": -0.25502863797274505, "compression_ratio": 1.7092511013215859, "no_speech_prob": 1.749092189129442e-05}, {"id": 1005, "seek": 665908, "start": 6665.5599999999995, "end": 6671.8, "text": " been a lot of active research in this version but there has no concerns that um as a lot of models", "tokens": [668, 257, 688, 295, 4967, 2132, 294, 341, 3037, 457, 456, 575, 572, 7389, 300, 1105, 382, 257, 688, 295, 5245], "temperature": 0.0, "avg_logprob": -0.25502863797274505, "compression_ratio": 1.7092511013215859, "no_speech_prob": 1.749092189129442e-05}, {"id": 1006, "seek": 665908, "start": 6671.8, "end": 6678.12, "text": " of systems i described here actually require a lot of them require very strong like a pre-term", "tokens": [295, 3652, 741, 7619, 510, 767, 3651, 257, 688, 295, 552, 3651, 588, 2068, 411, 257, 659, 12, 7039], "temperature": 0.0, "avg_logprob": -0.25502863797274505, "compression_ratio": 1.7092511013215859, "no_speech_prob": 1.749092189129442e-05}, {"id": 1007, "seek": 665908, "start": 6678.12, "end": 6685.48, "text": " language model and also require the lots of training examples for the QDSS so that would be actually", "tokens": [2856, 2316, 293, 611, 3651, 264, 3195, 295, 3097, 5110, 337, 264, 1249, 11844, 50, 370, 300, 576, 312, 767], "temperature": 0.0, "avg_logprob": -0.25502863797274505, "compression_ratio": 1.7092511013215859, "no_speech_prob": 1.749092189129442e-05}, {"id": 1008, "seek": 668548, "start": 6685.48, "end": 6693.08, "text": " um i would say a bottleneck for many low resource languages right so so it's very hard to collect so", "tokens": [1105, 741, 576, 584, 257, 44641, 547, 337, 867, 2295, 7684, 8650, 558, 370, 370, 309, 311, 588, 1152, 281, 2500, 370], "temperature": 0.0, "avg_logprob": -0.21562870936607248, "compression_ratio": 1.6318681318681318, "no_speech_prob": 1.416029772371985e-05}, {"id": 1009, "seek": 668548, "start": 6693.08, "end": 6699.799999999999, "text": " many examples called other languages as if we have actually i see the techniques can can be generalized", "tokens": [867, 5110, 1219, 661, 8650, 382, 498, 321, 362, 767, 741, 536, 264, 7512, 393, 393, 312, 44498], "temperature": 0.0, "avg_logprob": -0.21562870936607248, "compression_ratio": 1.6318681318681318, "no_speech_prob": 1.416029772371985e-05}, {"id": 1010, "seek": 668548, "start": 6699.799999999999, "end": 6704.5199999999995, "text": " generally applied to other languages as and i there has been also a lot of work trying to do", "tokens": [5101, 6456, 281, 661, 8650, 382, 293, 741, 456, 575, 668, 611, 257, 688, 295, 589, 1382, 281, 360], "temperature": 0.0, "avg_logprob": -0.21562870936607248, "compression_ratio": 1.6318681318681318, "no_speech_prob": 1.416029772371985e-05}, {"id": 1011, "seek": 670452, "start": 6704.52, "end": 6718.4400000000005, "text": " to cross symbol questions so that's it there", "tokens": [50364, 281, 3278, 5986, 1651, 370, 300, 311, 309, 456, 51060], "temperature": 1.0, "avg_logprob": -1.7770811716715496, "compression_ratio": 0.8979591836734694, "no_speech_prob": 3.8221769500523806e-05}], "language": "en"}