{"text": " Hello, everyone. Welcome back to CS224N. And today I'm delighted to introduce our final guest speaker, Julia Svetkov. So Julia is currently a professor at Carnegie Mellon University, but actually starting from next year, she's going to be a professor at the University of Washington, as you can already see updated in her email address. Julia's research focuses on extending the capabilities of human language technology beyond individual cultures and across language boundaries. So lots of work that considers the roles of human beings and different multilingual situations. And today she's going to be giving a talk to us on social and ethical considerations and NLP systems. Just one more note on the way things are going to run. So Julia has some interactive exercises. So what we're going to do is, for the interactive exercises, you'll be asked to put something into the Zoom comments. So that's our, sorry, the Zoom chats. That means really using the chat. And you might want to say who the two to the chat is to, I think it's by default panelists, which is okay or to Julia. So goes to her panelists is good, but probably not all attendees. So that'll be a bit overwhelming. And then if you have questions, put them in the Q&A as usual, because they'll keep the two streams separate. And as for our other invited lectures, if you've got some questions that you'd like to ask, you'll hear at the end, stay on the line and raise your hand and we can promote people to be panelists and have a chat with you. Okay, so now further ado on the light that happy you, Julia. Thank you very much, Chris. I'm very excited to speak to you all today despite, unfortunately, I cannot see you, but I'm excited. And so this lecture is structured as follows. We'll have three parts. The first part will be primarily a discussion in which I will ask questions. It's supposed to be interactive, but I realize we are very limited in ways we can interact now. So this is when please put responses if you want in the chat window. And I will answer my own questions following also your responses. And maybe read some of your responses. So this will be the first part. And the goal of this part is to provide you some practical tools when you have a new problem to work on in AI in your field. How would you assess this problem in terms of how ethical it is to solve it? What kind of biases it may incorporate and so on. So in the second part, I will try to generalize to give a review of what are overall topics in the intersection of ethics and NLP because it's actually a very big field. And what I will talk about today is just a motivational lecture, but there is a lot of technical, interesting technical content and a lot of subfields of this field. And I will dive a little deeper in one topic in this field, specifically focusing on algorithmic bias. And if time is left, which I'm not sure about, I will talk about one or two projects in my lab. So specific research projects, but if we don't have time to cover it, then you can always read the paper. So the first first two parts are more important for the purpose of this lecture. So let's start. This is as far as I understand, this is a course on deep learning and natural language processing. So you've probably covered various deep learning architectures and their applications to various NLP tasks, like machine translation, dialogue systems, question answering, and there is an obvious question, what does it all has to do with ethics? What does syntactic parsing or part of speech tagging has to do with ethics? And the answer, which I want to suggest is this quote, that it's a simple answer that the common misconception is that language has to do with words, but it doesn't. It has to do with people. So every word, every sentence that we produced, language is produced by people. It is directed towards other people and everything that is related to language necessarily involved people. And it has social meaning and incorporates human biases. And this is why also models that we build, which will be used by other people, may incorporate social biases. So this is why decisions that we make about our data, some kind of considerations that we incorporate into our model, may have direct people, direct impact on people, maybe societies. And to start this lecture, we need to start with understanding what is ethics. So what is ethics? Here is a definition from a textbook on ethics. Ethics is a study of what a good and bad ends to pursue in life. And what is right and wrong to do in the conduct of life? So it is a practical discipline. And the primary goal is to determine how one ought to live and what actions one ought to do in the conduct of one's life. So to summarize, it is very practical and it's simple. It's just doing the good things and doing the right things. Then my question to you is how simple it is to divide, to define what is good and what is right. So let's start discussion by diving into various problems. And we start with a boring theoretical problem, which everybody knows about, which is a trolley dilemma. And we won't spend too much time on it. So just to, I'm sure all of you know about it. So it's a classical problem in ethics in which so this is you standing near the lever. And here is a trolley coming and there are several people. So the trolley cannot see the people and the people cannot see the trolley. And you are the only one in control, in charge. You can save people and maybe you need to make decisions about people's life. Do you ask yourself why me? But the point here is that imagine that there are five people on one side and no one on the other side. And then I would ask you, would you pull the lever to save five people if the trolley is supposed to go straight? And if I would ask you, interactively, everybody would say, yes, I will pull the lever. And then I will follow up with next question, okay, what about if five people on one side and only one person on the other side? So would you pull the lever to minimize the number of lives that will be sacrificed? And some people will not answer, some people will say, yes, some people will say no. And those who say, yes, I will ask them, what if this one person is your brother? And on the other side, just five random people, what would be your answer? And I can go on and on and on to make this problem harder and harder. And as you can imagine, the answers are difficult. And also, we don't know what the answer will be in the actual situation. And while this problem is theoretical, it is in part becoming relevant now when we talk about self-driving cars. So I am now moving to closer to the topics that we will discuss today. And I want to introduce a new problem, which I call the chicken dilemma. So in this dilemma, let's train a classifier. And this will be a simple CNN classifier. And the input to the classifier is an egg. And the classifier needs to define the gender of the chicken, of the chick. So and decides if it's a ham, it will go to egg laying farm. And if it's a rooster, it will go to meat farm. So first of all, do you think you can build such a classifier? I'm sure every student in this course will easily build a classifier. And I'm sure it will have quite a good accuracy. And then the question to you is, do you think it is ethical? And I invite you to type your responses in the chat. Yes, no. I mean, you can justify a little bit. Thank you for participating. So could you repeat the question? So the question is, there is an egg. And you need to determine the gender of the chick. And if it's a rooster, it will go to a meat farm. And if it's a ham, it will go to an egg laying farm. And the question is, is this ethical? So there are kinds of responses. Let's see. So yes and no. But you can use exact same thing to target ethnic groups instead. So yes, thank you. And I see there are many interesting responses here. And just the amount of responses, I cannot even have time to read them. So anyway, so based on this question, I can tell you what my thoughts are. So as a vegetarian, I maybe think it's unethical. But as a mother, I actually want my kids to eat meat. And whether I think it's ethical or not, we are doing this anyway today. And there are all kinds of considerations, pro and cons. For example, this is what already is done today. And then maybe such classifier will minimize the suffering the animal. But on the other hand, we hope that in the future, society, the life of a chicken will be as valuable as the life of a person. And I can continue on and on. But from this example, I also don't want to stay on it too long. You can see that the questions of ethics are difficult. That whether like you don't know too much about this field, but you can feel what is the right answer. So ethics is inner guiding. It's moral principles. And there are often no easy answers. So there are many gray areas. And importantly, ethics changes over time with values and beliefs of people. So whatever we discuss today, we can think it's ethical or not ethical, but it may change in a hundred of years. And maybe a hundred years ago, this would not even be a question why this would be unethical. And another important point is that this is what we are doing today. So what is ethical is what is legal is not necessarily aligned. We can do legal things that will still be unethical. And now having this primer, I want to move to the actual problems, the actual problems that we can kind of be asked to build and decide whether we want to build them or not. And the way I will guide this discussion is I will ask you a specific questions. We will ask you for your answers. And I realize it's very difficult to read the chat, specific answers. But the point is that the types of questions that I will ask you, this would be the questions that you could ask yourself when you need to build a technology. And maybe the question whether something is ethical or not is a difficult question, but let's try to break down analysis of a specific application of a specific model to derive an answer which will give us some tools to derive an answer in an easier way. So here is an exclacifier that we want to build. We want to build an IQ classifier. So we will be talking about predictive technology. So based on people's personal data, for example, facial images, and maybe we can collect the text of these people on social media, media, let's predict the IQ of the person. So if you don't know what IQ is, IQ is a general capacity of an individual to consciously adjust the thinking to new requirements. So it's basically how intelligent a person is. So this is already not a hypothetical problem. You can collect the individual's data, you can collect the text online, and you can collect training data to predict people's IQ. But when I will ask you, is this an ethical question or not, it might be a difficult question to answer immediately. Thank you very much for participating. I really appreciate I hope I can save this chat later to read the answers. Okay, so let's start with the first question. We need to predict people's IQ from their photos and text. And then the first question that if I ask you, is it ethical or not, I don't know. And then you can ask yourself first, who would benefit from such a technology? So can you think who would benefit from a technology that predicts an IQ of a person? Hiring employers, schools, universities, so I see your answers. Right. So overall, it can be a useful technology. Immigration services can benefit from it. And invite only smart people to immigrate to a country. Even individuals with high IQ can benefit from this, right? Because they would maybe not need to do GRE and SAT. They will not need to write essays. They will just need to show the IQ. Okay, so this technology can potentially be useful. And then the next question is, let's assume we can build such a technology. I will show you later that we actually cannot. But even if we can build such a technology, let's think about corner cases and understand who can be harm-based technology. So basically, what is the potential for dual use? How these technology can be misused? So assume that the classifier is 100% accurate now for a second. And please think about it and type, what do you think who can be harm from such a classifier? And how this classifier can be misused? Right, so I can see answers. And I wish we can have this interactive, but I can try to summarize what I have read so far. So first of all, one of you wrote that IQ is, let me just answer my question because it's difficult to summarize that chat. The interactive feature is difficult. So I would think about it in this way. First of all, why would we want to build such a classifier? So to build a classifier, to predict an IQ, companies, universities, they don't really need to know your IQ. What they are trying to predict is your future success. The way you will succeed in a job or the way you will study at school. And then the question is, is IQ is the right proxy for future success? And the answer is no, IQ correlates with future success, but it's not necessarily the right proxy for future success. And then who are people who could be harmed? For example, people who have lower IQ, but very hard working people. People who have lower IQ, but have good soft skills. So assuming that, so first of all, the IQ is a proxy of future success is incorrect bias proxy. And this kind of problem of using a proxy to the actual label because we cannot have the actual label, the future success. We cannot have this label is actually very common in other types of predicting technology. If you think about parole decisions, technology that uses the decides on parole decisions, what they want to predict is whether the individual recommit the crime. But this is a label that is very hard to obtain. And this is why they might resort to another label, whether they whether this individual will be convicted of a crime again, and build this technology to predict future conviction. But conviction of a crime is a biased proxy of where the actual objective that we want to have, the likelihood to make the crime. And this is one example of a biased proxy, which does not allow us us to build the right application for the goals that we have. So this is one problem. The second problem is IQ test itself. It is a biased test. So actually we cannot build a proc- and accurate classifier for future for the right IQ. And also if we look at the data that we use, this data, picture photos or social media, this data is biased itself. So there are all kinds of biases because of which we cannot actually build the right model. And this is why this classifier will not be 100 percent accurate. But there will be many individuals who can be harmed. And then there will be questions. For example, assume our classifier that we build is not actually accurate. But it has high accuracy. For example, 90 percent or 95 percent. And then I would ask you, is 95 percent a good accuracy or 99 percent a good accuracy? And then the questions to think about is whether what would happen with misclassification? What would would be an impact on individual lives if the classifiers makes mistakes? And in this case, the important point is that the cost of misclassification is very high. It has effect on people's life. So accuracy may be not the right evaluation measure for this classifier. And another question that I could ask is, for example, this condition on this slide that we find out that white females have 99 percent accuracy. But people with blonde hair under age 25 have only 60 percent accuracy. So what does it tell you about this classifier? Right. So the data set itself is biased. This means basically that people with blonde hair under the age of 25 are underrepresented in your data set. So there are all kinds of questions and all kinds of probing questions that you can ask about the classifier to understand, is this the right problem to solve? Who can be harmed? Am I optimizing towards the right objective? Is my data biased? And what is the cost of misclassification? How do I assess the potential for dole use and how much harm this technology can bring in addition to how useful it can be? And the one last question, which is a hard one, I want to ask you who is responsible. So I'm your manager in Google, you're working in a country and I ask you please build a Q classifier. And you build a Q classifier and you publish a paper about the Q classifier. And this paper is publicized on media. So and then the question is who is responsible? Is it the researcher or developer? Is it the manager? Is it a reviewer who didn't catch the problems with IQ classifier? Is it university or company? Or is it society? Yeah, so there is one nice answer that I want to read is that all of us should be responsible. So in practice, there is very little awareness about understanding what problems are ethical or not. And there is no clear policies here. This is a complicated issue and it's not clear who is responsible. This is why assuming that whoever is aware of such dangers should be responsible. So I don't know what is the right answer to this question. So now what is the difference between the chicken classifier and the Q classifier? Right, so one of you answers is that one is affect people and one does not, right? And while chicken classifier actually affects chicken lives and IQ classifier will not kill anyone. It can harm but will not kill. We do feel that IQ classifier currently can have potentially worse impacts. So AI systems are pervasive in our world and the question about ethics are specifically raised commonly about people centered AI systems. And these systems are really pervasive. So they interact with people like conversational agents. They reason about people such as profiling applications or recommendation systems. They affect people in other lives like parole decision applications that I mentioned, face recognition, voice recognition, all of these actually have this component of predictive technology and the human center technology. And this is why ethics is critical here. So I want to move to a next study. The next study is a study of detecting, so we again build a classifier and we want to identify the ability to accurately identify one's sexual orientation from mere observation. So this study is called AI GADAR. So as I mentioned many similar studies, studies that predict potential for terrorist attacks, studies that predict predictive policing. And also if you heard about Cambridge Analytica, all of them incorporate very similar technology. So let's talk about AI GADAR study and the goal is to understand again what kind of questions we could ask about this study and what kind of pitfalls we could prevent if we would ask these questions. So to summarize this study, the recent question is we need to identify the sexual orientation from people's images. And the data collection process is that we can download photos from a popular American dating website. And there are 35,000 pictures, all white, equally represent equally representation for gay and straight, for male and female. Everybody is represented evenly. The method that was used is a deep learning model to extract facial features and grooming features. And then a logistic regression classifier applied to classify the final label, gay or straight. And the accuracy of this classifier is 81% for men and 74% for women. So this is a summary of the study. And I see rightfully asked questions why would we ever need such a nice system? This is a good question. But I don't want to publish this study or disparage specific researcher, but this is a good study to present as an example of what could go wrong at all levels of the study. So this is why I am discussing it now. So what went wrong here? So let's start with an ethics of the research question. So is it ethical at all to predict sexual orientation from any kind of features? And I see a lot of comments and thank you for the comments. So first of all, this is not a new research question. From 19th century there were multiple studies to correlate sexual identity with some external features. People were and then with genetics, people were looking for gay genes, gay brains, gay ring fingers and so on. So maybe moving from 19th century to 21 century, we can again ask who can benefit from such a classifier and who can be harmed by such a classifier. So what do I think? Who benefit from such a classifier? So autocratic governments, right? But also maybe dating apps, advertisers, conservative religious groups and so on. So we could think who would want to use such a classifier? Then maybe who can be harmed by such a classifier? Now again, assuming we are not thinking if it's possible at all builds such a classifier and as you can guess we will see that it's not possible. But what would stop you from building such a classifier? What do you think could be harmful in this classifier? So yeah, thank you for your answers. I will summarize them. So many people can be harmed by such a classifier and I summarized basically many of your answers here in this slide. So this potentially can be a dangerous technology. So in many countries being gay person is prosecutable by law and it can lead even to despenalty. It might affect people's employment, relationships, health opportunities, right? Importantly, this is not only about sexual orientation. So there are many attributes including sexual identity that are private for people, right? They are protected attributes and they can be non-binary, they can be intimate and not visible publicly. And most importantly is that these attributes are specifically those attributes against which people are discriminated against. And this is why it is basically dangerous to build such a technology. So in the paper, in the published paper, the argument for building this, for it's called presenting this study was that the study isn't alert how easy it is to build such a classifier. And basically, it is alert for to expose their thread to the privacy and safety of people. And then I would be interested to hear if you have counter arguments. So basically, there can be many counter arguments. One of them is that this is a classifier, this technology. So like a knife is a technology and with a knife you can kill people and you can cook food, right? And you don't, you're not necessarily need to kill people with a knife to expose the dangers and harms of this technology, right? And another issue is that this is actually not possible to build such a classifier with the data that researchers had. And this is what we will see when we will discuss additional details about the data. And another comment is, as I said, this is only one instance of such a technology. Here is another instance, which is a successful startup called Faceception, which has drawn a lot of funding and its goal is to identify terrorists based on facial features. And unlike in the previous study, the startup doesn't show how the bot technology they developed, but you can guess that it can have similar dangers. So in general, building predictive technology is very pervasive, as ubiquitous, but it is always, and sometimes it's not as kind of clear cut on ethical. For example, many people in NLP published papers on predicting gender from comments. And it is not clear basically when the technology is clearly harmful and unethical and where it can be actually used in good ways. For example, we all want our search to work, right? And to work well, and to be personalized, the algorithm actually needs to know something about us. So again, this is not an easy question, but in the case of the dark classifier, maybe it's already on the extreme. Okay, let's move to the data. Again, to discuss basically what questions could we ask about the data? So here's how the data was collected. So photos were downloaded from a popular American dating website. They were public. And there were a few thousands of images all white and the balance data set. And my first question is what can you say about the data? So is it okay to use this data if there is no robots to exceed the photos are public? What can be counter arguments to using people's photos from a dating website? There is a hint on the slide. A lack of consent. People did not intend for their photos to be used to build a classifier, private information, right? So thank you for your answers. So the points that I wanted to emphasize here is that it was legal to collect this data. But again, it's not clear whether it was ethical to collect this data because as you said, people did not provide 35,000 people did not provide consent for using specifically this data. And there are more important and global issue here is that there is a difference between data that is public and data, which is public size. So public, it's fine because these people want to be found by the social circle that they are targeting when they publish their photo on the dating website. But this does not mean necessarily that they want to be found by broader social circle, by their families, by their colleagues and so on. So there is a big difference between the data that is public and the data which is public size. So overall, even if they did not violate state terms of service, I don't know about it. I didn't read in depth actually, but they did violate the social contract because this was not the intent of the user for their data to be used in this way. Next question about data. So what do you think about this data set? 35,000 of pictures, all white, balanced in terms of sexual orientation and balanced in terms of gender. It's white. Okay, so does not represent the population. So right, so basically, you can guess that this data set has many, many biases incorporated. It contains only white people, only people who have self disclosed their sexual identity. It represents very certain social groups, people who put their photos on the dating website, specific age, specific ethnicity. And basically, these are photos that were carefully selected to be attractive to that target audience. So this data set contains many types of biases. And also, as one of you mentioned, and as reading on this slide, that the data set is balanced, which does not represent the whole the true distribution of the population. So what does it mean? This means that this model is built on a very biased data set. And you, as students at Stanford, you understand that it cannot be used, for example, on a non-white population. It cannot be used on photos of people, not from the dating website. We don't actually know how this what this classifier learned. Maybe the most important features were the watermark of this specific website. I don't know or some other of confounds, curious confounds. But the point is that once the classifier is out, those who want to use it maliciously, they don't know that this technology is actually not applicable for any other data set except for this specific data set. So this technology is biased, and it also shows that it's basically not a credible result. Okay, so let's move on. And final question is that, basically this is a deep learning model, it's a black box model, and then there is a question of how to analyze errors in the learning model. Specifically, when we work on such a critical, such a sensitive topics like predictive technology, not necessarily predicting sexual orientation, but for example, predicting gender, which is again used in many companies. So it is very difficult to understand whether it is okay to use this technology. But the point is that we need to be able to analyze it and evaluate it properly. And the last point is about the accuracy. Again, and I'm going back to the points that I mentioned also in the IQ classifier. So the accuracy of this classifier is 80% 1% for men and 74% for women. Is it a good accuracy or is it a bad accuracy? So the numbers are okay for some tasks, but not for others. But importantly for this type of problem, it's important to understand that the cost of misclassification is not equal to the cost of correct prediction. And here is kind of visual examples. So if my algorithm misclassifies by doc as a cookie, is the cost misclassification of this misclassification as high or low? So I guess it's just funny, right? It's funny. There is nothing offensive here. And then the next question, if my algorithm misclassifies me with my doc, is the cost of misclassification high or low? It can be funny, but maybe not for everyone. We already don't know. And then the photo that I don't put here, but the one that is maybe many of you have heard is the gorilla incident that happened in Google in 2016. So in this case, there was a misclassification of African women with a gorilla and to understand how expensive is the cost of this misclassification. We need to understand the whole history of dehumanization of black people in the US and so on. So we can see the difference for the same algorithm and same types of errors. There are different types of errors that are more expensive than others. So this is why it is important to assess AI systems adversarially. And now I just want to reiterate the types of questions that I asked because these are the kinds of questions that you might want to ask yourself next time when you need to build another predictive technology. And the first is to understand the ethics of the research question. And sometimes it's not very easy to understand, but just ask yourself these most specific questions. If I build this technology, who could benefit from such a technology? And who can be harmed by it? So try to see the corner cases. And also what about the data? Could sharing this data have major effect on people's lives, like in the case of AI, gay, dark, classifier? The next question that you can ask is about privacy. What we discuss, who owns the data? And is this data not only public or legal to use, but also is it are we violating the social, basically, circles to which the data is publicized? Are we violating a social contract in the way that the public data is expected to be used? And user consent is not always possible to obtain. But we need to understand implicit assumptions of people who put their data online. Now, the next question is what are possible biases in data? What are artifacts in data? What are distributions for specific populations and subpopulations? How representative or what kind of misrepresentations are in my data? And next is what is basically potential bias in these models? Do I when I build this model do I control for confined variables? And do I optimize for the right objective? Like in the case of the IQ classifier? And also if I have biases, does my system amplify biases? And finally, it is not enough to measure accuracy because the kind of semantics of false positives and false negatives can be different. Sometimes the cost of misclassification is much higher than the cost of correct prediction. So, also need to understand how to evaluate the model's property. And why is it especially relevant now? Is because as you all know, there is an exponential growth of user-generated data. And it's really easy to build the tools. Each of us can build the IQ classifier or GADAR. But the question is what kind of technology we will produce? So, in this I finished the first part of discussion and I put in this slide some recommended papers on talks specifically on the introductory topics on the impact of NLP. And I think these are there are hundreds or thousands of similar talks, but these are my favorites. So, if you want to read more, please take a look. Should I stop for questions? So, should we move to the next part? Right at the moment there aren't any outstanding questions. So, maybe it's okay to move on unless anyone is desperately typing. Okay, and the chat window is really nice. There are so many responses. Thank you all. And I hope to get later this chat file to read it. I think we can save it, yeah. Thank you. Okay, we can move on to the second part about algorithmic bias. So, what are the topics in the intersection of ethics in NLP? So, the first one is algorithmic bias, and this is about bias in data and NLP models, and this is something that I will talk more in the second part. But important to understand the field is much broader. So, the next topic is incivility. So, ability to develop NLP tools, to identify, to actually data analytics, to understand the hate speech, toxicity, incivility online, and this is a very complicated field, because it's not only about building the right classifiers. There are many, many questions such as if I post hateful comment, who does this comment belong to? Does it belong to a company? Does it belong to me? Should it be removed or not? Because it is not clear where is the boundary between the free speech and moderated speech, how to minimize the harms, but defend the democracy. These questions are very subjective, and they are not regulated. So, this is a big difficult field. The next field is about privacy. So, again, who does this data belong to, and how to protect privacy? This is field of privacy is actually very, very under-explored NLP. On other fields, I think there is some research on incivility, or no gripping bias, on other fields, but very little research on privacy. Misinformation, so information manipulation, opinion manipulation, fake news. So, there is a whole range of ways that data can be manipulated from generated texts and disinformation to just advertisement and the most subtle or propaganda and subtle opinion manipulation. And there are many, many interesting research projects that can be done really with focus on the language of manipulation. I think it's just an interesting topic to explore. And finally, the technological divide. So, when we build our tools, even if it's a part of speech tagger, what are populations for which are served by these tools? So, there is a certain divide. The technologies are built unequally. There is no one language, no two languages. There are six thousand languages in the world, and there are many populations. And there are certain areas of NLP, which are completely under-explored. For example, language varieties. We think we can solve a problem for English, the problem of dependency parsing, but we don't account for different varieties of English. What about Nigerian English, what about African-American English, what about Indian English? So, there are the technological divide that is currently present. And as you see in the bottom, this picture shows that they feel this highly interdisciplinary. So, AI researchers cannot actually solve the problem of misinformation alone. To be able to address the problem of misinformation or hate speech, we need to have not only engineers, but also ethicists and social scientists and activists, and politicians who actually are responsible for policies and linguists, because many of these phenomena are interesting phenomena, which are not in words, but more in pragmatics. So, this is a very interesting field, scientifically, but also very challenging to work on. And there are some recommended resources, and in particular, the one that might be interesting to you all is CS384, is a seminar by Dan Jaravsky, that is an amazing also list. So, if you want to take a look on specific subfields. So, this is a general overview, and now I want to talk about one of these subfields, and give some of explanation, why do we have algorithmic bias in our models. So, let's start again with interaction. I know you have the slides, but don't look forward. I want to ask you questions, and please type, which word is more likely to be used by a female? Giggle or laugh? Just type quickly, don't think too much. So, please look at the chat and see the majority response. It is absolutely giggle. You're right. Next question, which word is more likely to be used by a female? Brutal or fierce? Oh, lovely. Like 99% fierce. Thank you. Next question, which word is more likely to be used by an older person? Impressive or amazing? So, actually from what I see, it's 100% impressive. Very impressive. Thank you for answers, which word is more likely to be used by a person of a higher occupational class. Suggestions or proposals. Do you see how correctly you answer my questions? Next question, why do we intuitively recognize the default social? Why do we all know the right answer? Right, our brains are biased. So, this is about implicit biases in our brains. And this is a very good example. And you can also see how language perpetuates and propagates biases. Right? It's all in the language. If you could know from one word who is the person who said it, you can imagine what kind of biases we can extract from a longer text. So, to understand what's happening with biases, we need to understand how cognition works. So, we have, this is was introduced by Kaliman and Tversky. So, conceptually our brain is divided into system 1, system 2. So, system 1 is our autopilot. It is used to make decisions without thinking. It is very fast parallel effortless and so on. System 2 is our logical part. It knows how to analyze and make decisions that are unusual for us. So, it is not automatic and it is slow, serial, controlled. It requires a lot of mental energy. So, our brain constantly receives signals, all kinds of from through all the sensors, through eyes, ears. There is a lot of incoming data. There is a lot of pixels here around me. But the actual part of system 2 is only able to produce a very small portion of the signals that we received. So, system 1 is automatic. System 2 is effortful, but in practice over 95% of the signals that we received from the world is relegated to system 1. And the funny thing is Kaliman wrote is that we identify ourselves with system 2. We believe that we are conscious and reasonable beings, but in practice most of our decisions are made by system 1. So, since we are using autopilot most of our time, our brain gets all the information that we perceive, get categorized clusters and labeled automatically. And this is how cognitive stereotypes are created. So, and there are multiple cognitive stereotypes that are aimed to fill the gaps if we don't have enough meaning or reduce information, generalize if you have too much information, or to complete the facts if we are missing facts and so on. And this leads all kinds of cognitive biases. So, examples of biases would be in group favoritism. So, we grow up seeing the majority of specific people and we tend to like those people more than the minorities. How the effect that we know very little about the person or a specific social group, but we tend to generalize based on one trait we could generalize about the whole group and to other traits. And so, there are many biases. And thanks to these stereotypes, if I asked you the questions the words, or if I show you these pictures you immediately know this is a calming, cute, tasty, and if I show these pictures you will know that maybe it's dangerous and pleasant and automatically when we see a snake we will not, will automatically step down right step back. And we only need to make system to invoke system two for example, if we decide to touch it. But most of our decisions are automatic and this is the same exactly mechanism that creates social stereotypes in our brains. So, we exactly in the same mechanism internalize these associations and make generalizations about specific groups. So, and this is why when I ask you which word is more likely to be spent older person, 100% of you typed that the word impressive would be used. And importantly, these implicit biases are very pervasive and they operate unconsciously. And one important property that they are transitive. So, basically we are seeing that a black person is playing basketball and in a movie we see that a black person uses drugs and we immediately connected and reinforce the, for example, the associations with specific groups. And the social stereotypes are not necessarily all negative. I can name some on the surface positive stereotypes. For example, Asians are good and math. Importantly, they all have negative effect even seemingly positive stereotypes because they pigeonhole with the individuals and put expectations of them or they can just be harmful. And then how do these biases manifest? They manifest in language. And for example, they manifest in subtle microaggressions. And importantly, microaggressions should not correlate necessarily with sentiment. So, sentiment analysis tools would not detect them. On the surface level, microaggressions can be negative, neutral, or positive, like in these examples. But they actually bring prolonged harms, even if they are meant as a compliment. And there was a lot of research in social sciences that showed that they can even bring more harms that overt hate speech. Because they, in place, they kind of bring a significant emotional harm and reinforce problematic stereotypes. So, if I will collect these conversations from Twitter, do I look okay? You are so pretty. Is this a positive or negative? It's probably positive interaction. And then the next interaction, check out my new physics paper. Is it positive or negative interaction? Why physics? You're so pretty. So, we don't know. We don't have the right context. And then for this question, do I look okay? And all kinds of responses, for example, you are so pretty for your age. In this case, these are negative. These are microaggressions. They make us cringe, right? And then the problem is that all of these human generated data, which is necessarily and incorporates a lot of microaggressions. And just stereotypes that we all have, we are not aware of them, it is fed to our systems. So, there is a lot of bias in language, stereotypes or historical biases that are perpetuated. For example, there are more photos of men, doctors than female doctors on the web, or human reporting biases. And later, there are biases also in our data sets. So, for example, what kind of data is sample for annotation from which kind of populations, from which language varieties, from which locations? And then who are we choosing as annotators? So, there is a bias in who are annotators that will annotate our data. And then there is bias, cognitive biases of annotators themselves, how they treat, what is the microaggression and what is not, or and other questions. And all these types of biases later propagate into our computational systems. And this is how we get from cognitive bias, social cognitive biases to algorithmic biases. Because if you remember system one, a system two, currently the way we develop systems, AI is only system one. And why is that? Because currently the way we develop our tools, the dominant paradigm, is a data-centric approach. So, we need a lot of data to train good models. And we do know well how to leverage a lot of data. But again, language is about people. It is produced by people. But our existing systems, they do not leverage social cultural context. We don't know how to incorporate and we don't do it usually. We don't incorporate which social biases are positive and which inductive biases are good and which inductive biases are bad to have. So overall, our models are really powerful and they are powerful at making generalizations. But we don't know how to control for the right inductive biases, which biases are good and which inductive biases are not. And then going to the next point is that these models are opaque. We don't also know how to interpret well deep learning networks, which means it's not easy to analyze them and spot the problems. So, and as you can guess, this is not only related to the field of ethical and LPE. These are just interesting research questions. How to incorporate social and cultural knowledge into deep learning models or how to develop interpretation approaches. So, what is missing? Today, what is missing, for example, is that existing classifiers for toxicity detection, if we want to build data analytics to clean up our data before it propagates to the models, we know only how to detect overt toxic language, such as hate speech. Because we are primarily sampling our data and training our data based on lexicans. And there is almost no focus on actual microaggressions and most subtle biases, which are often not in words, but in pragmatics, the conversation and understanding who are the people involved in the conversation. So, today's tools that could be applied to this kind of microaggressions or hate speech detection or sentiment analysis, but they will necessarily fail. The next point is that, again, our models do not incorporate social cultural knowledge. And basically, the same comment can be toxic or non-toxic depending on who are the people involved in the conversation. But our models are data-centric and not people-centric. And the more general problem is that the deep learning models really go to the picking spurious correlations. And this is why, for example, in this paper, the three comments, which change, which the only difference in these three sentences is the name and probably the association of this name with race or ethnicity. So our models do pick up on spurious confounds. So we think we predict sentiment, but we also predict all kinds of labels that correlate with sentiment, but not necessarily a true predictor of sentiment, for example, gender or race. And this is something very pervasive. So, and finally, the models are not explainable. So we kind of have these deficiencies just in core approaches to deep learning. And we have all these data with these data with trained conversational agents, personal systems, all kinds of systems. And why do we care now? Because it can bring harms. So what kind of unintended harms it can bring? Here is an example of an image search. If you search for three black teenagers, and this is I searched for it when I prepared this talk for the first time. So it was fixed, I guess, but this is how it was in June 2017. And then when you search for a doctor, you get primarily male doctors, right? And primarily white. And if you search for a nurse, this is a stereotypical image of a nurse. And if you search for a homemaker, this is just top search results for this query world. If you search for CEO, it's a very specific stereotypical image of CEO. And if you search for a professor, this one is my personal favorite. So you can see all images. And there is only one woman. But if you look at her background, you can see this is a simple aspect. So it's just an error in search. She is not a professor. And this is a result of, for example, speech, face recognition. So these are two examples. One camera does not recognize Asian faces and things they blinked on the right. The camera, this is a video of face tracking camera that is able to track white faces, but immediately shuts down when black face. So it is not able to track black faces. So these are all consequences of bias data that propagates into models that do not incorporate intentionally, basically safeguards against very specific biases. Now what's going on with natural language processing? So this is a slide from the very beginning that just leads all possible applications that I could think about. As you can guess, since 2016, there are many, many papers that just I don't think there is any application or court technologies of NLP left, which did not expose biases in NLP technologies. So here is an example of bias in machine translation. So this is visual. This is why I'm showing it. So there are languages that mark third person, third person pronouns with gender and other languages that do not mark third person pronouns with gender. So if you translate from a language such as Hungarian or from Estonia that don't mark third person pronoun with gender into English, which does mark third person pronoun with a gender, you might see similar results. You will not see them now. This is what was exposed maybe year or two ago. So basically from translation from they are the nurse. This would be she's a nurse, but they are the scientists. The translations would be he is a scientist. In saying for engineer, baker, teacher, so all the stereotypes, just historical stereotypes that you could think about. So what are possibilities to fix it? One way to fix it is actually simple. You could treat the target gender just as a target language is multilingual NMT. So I don't know, but I suspect you did look at this paper on multilingual neural machine translation. And basically you can add another token, for example, and you can controllably generate into female or male translation. So the fix is not difficult, but you need to be aware of potential dangers to be able to fix model. And importantly, this is not about only about fixing the model itself, but also about fixing the user interface. So the way Google fix this interface is they provided different translations, so basically all possible translations for different jenders. So in the similar kind of harms were shown especially in dialogue systems. So occasionally such models make big headings and news like Microsoft's Tire Chatbot that became very racist and sexist overnight. And the GPT-3 based models that was offering a suicide advice. And about two weeks ago there was a Korean chatbot that became extremely homophobic very quickly and had to be removed. So these titles come up again, these headlines coming again and again and again. And I guess the point here is that what we do in LP today, I call it a reactive approach. So we have a we expose specific problem, a problem in search, a problem in chatbots, racist chatbots, or a problem in machine translation. And then it gives creates bad publicity and then we start debicing the models. But it's not necessarily that we need to develop the tools in this way, right? So I hope that kind of in the future we make a paradigm shift, talk the more proactive approach. And the specific, what would proactive approach require is, for example, building new data analytics. So basically, rather than exposing biases, going further up the pipeline and starting actually with the data and building automatic moderators and data analytics that can identify problematic texts, dramatic images beyond the workly hate speech. And then incorporating the right inductive biases into the models and understanding what, of understanding, moving from the data center approaches to people center approaches, incorporating social cultural and pragmatic knowledge. And in modeling, there are interesting research questions on how to the most furious confounds, but predict only the target label, not necessarily the picking up on spurious correlations. And finally, on building more interpretable models. And importantly, these are not orthogonal research directions. So for example, to build good data analytics, you necessarily need to maybe have an interpretable model and also be able to incorporate the right social cultural knowledge, because again, the microaggressions, the text is not necessarily in words. So what I was going to do, if I had time, is I was going to show to who case studies, from research studies, from my group that specifically focus on these data analytics, identifying unsupervised bias or an interpretable model for making hate speech classifiers more robust. But I will skip it because we are out of time. Good show what you've stored a few minutes, you could show one quickly for five minutes. So basically, we are trying to build this green boxes is what we have today, hate speech or sentiment analysis. But we are trying to build a new kind of class of models specifically for social bias analysis. And in these models, we would want to detect who are the people involved, so who the comment, for example, is directed to if it's a conversational domain. And also to understand what kinds of microaggressions these are. And also to maybe generate explanations or interpretations through building more interpretable models. And these are the two papers that I was going to talk about. One is an unsupervised approach to detection gender bias. And one is on if we have just a few examples of microaggressions and the classifier of hate speech, these examples of microaggressions are adversarial examples to the classifier. The classifier is not able to deal with them. But what we can do, we can focus on interpretability of the classifier and specifically making the classifier understanding for each probing example, which examples in the training data influenced the classifier's decision. So changing the approach to interpretability from interpreting specific salient words in the input of the classifier into looking at the training data, sorting the training data and identifying which examples were most influential for classifier predictions. So using influence functions from example, this is a paper that Percy published in 2017. And through this classifier we are able to surface microaggressions despite that the classifier makes the wrong prediction. So this is just a high level, very high level summary without talking about the actual studies. So I will skip, let me just skip the actual papers. And the slides are there. I'm happy to discuss later. I just don't want to go over time. So to summarize, the field of computational ethics is super interesting and there are interesting problems that are technically interesting, challenging. So you don't need to have separate kind of important problems and the technical interesting problems. We can work on important problems, which are also technically interesting and focus on important things like building better, deploring models. And these are interesting subfields. And if some of you are interested in specific projects, so it's just in our course we just put together a presentation that just summarizes all kinds of possible projects. Thank you very much. I wish I could see the audience. This is so weird. Thank you, Julia, for that great talk. Yeah, so if people would like to ask some questions to Julia, if you raise your hand, we can promote you to be panelists. And I think then we can even have you turn on your cameras if you want to show you're a real human being. But you know, if we're waiting to see if there are people who would like to do that, I mean, there is one question that's outstanding at the moment, which is, do these bots become racist sexists so quickly after exposure to the public, due to the public intentionally trying to bias them, or is it that common talk among the public is racist sexists enough to bias any model upon exposure? So I think this is both. But in the case, for example, of tie-bought, the way it was built is it's a continual learning system. So it collects inputs from people and then uses them as training examples to generate forward answers. And people, as usually people, pick up on such things very quickly. And then they intentionally became racist and the sexist again against the bot. And the bot very quickly learned to just meaning the people's behavior. So it was some malicious attempt to turn this bot into racist and sexist. But this is how the model was designed to collect inputs from people, but not monitor the kind of sentences that are used or not used in the training data. So this is again going back to the discussion of that we actually don't have good analytics. Many of these analytics are just at least so, whitelist, they are very, very primitive. It's not very easy to incorporate such constraints into generation or the automatic filtering of data. There is another question. Do you want to ask my question? Yes. So I guess you got both questions. Thank you and I end live people. So you get it. Let's ask the question. Yeah. I'm curious. I may be going to a little bit more about how you measure your model's performance. Are there actually public benchmarked data sets? Are any sort of well defined metrics that you can sort of objectively measure your model's improvements? Are you talking about specifically our papers that I skipped? I mean, I know it's a very new field and maybe it's harder to define real objective measure of bias. So how do you measure progress in general? Are you also just mentioning this? This is a good question. It's very difficult. There is growing body of data sets. For example, in Yodav, in the Joyce group created the social bias inference corpus. I don't remember what I said as BIC. Overall, the problem of evaluation is actually very difficult. And there are some problems in which there are existing evaluation data sets. If you think about hate speech, for example, there are many data sets for training and evaluating performance of hate speech classifiers. But when we think about biases, there are much less. And the big problem here is not easy to collect such a data set. So if you think, let me actually show why it is difficult to collect a data set of say of microaggressions. So an if solution would be to so if you think about the standard way of data collection, so we would sample some data from the internet and we give it to mechanical token, annotators. And then they would analyze is it bias or not? And we build a supervised specifier. So this is what we cannot do in the case of most subtle biases. First, because we don't have a strong lexical C to sample the right data. Because again, these biases are not in words. What like you just sample from the whole reddit corpus, it's not clear how to annotate to make it feasible, not too expensive. But more importantly that every annotator when you incorporate their own biases, so you actually need very well trained annotators and multiple annotations per sample. So the question of how to create such a data set is very, very difficult. In our study, we collected the so there is a corp website called microaggressions.com that has self-reported microaggressions. When people actually recall experiences of microaggressions against them and they quote them. And this is what we use to evaluate our data. But the data collection is as a big problem currently as just modeling. Do you want to ask any questions? Yeah. I don't have any other questions to ask. Oh, sorry. Okay, so I should go on. Will that pay for me? Yes. Yeah, things for the green lecture. It's a very appropriate topic for the sketch lecture. So I took the course CS 182 which introduced many notions of theirness through case studies and like assignments. I've been thinking a lot about how to use notions. And of course there was like mentioned for the research done by timeberg who showed three different notions of theirness can be simultaneously satisfied. The calibration which is like the probability of outcome given risk scores, the false positive rate, the false negative rate and not all be like completely independent across protective traits. So if like pass a certain point, you know, these metrics just become direct trade-offs. Is it the case that fairness becomes subjective after that? And I guess like more generally, you know, in ethics research has there been frameworks of creating sort of upper bounds or constraints among these different metrics. So we sort of measure how close we get to the ideal. This is a very difficult question. So right, the fairness research, there actually proves that you cannot satisfy both the measures of performance and inclusivity. And this is why they are measured separately, false positives, false negatives. And the question is whether it because of this issue, whether it becomes subjective, it's even bigger, I guess. The question here is even bigger because the question of inclusivity, so it competes with a question of monetization. If you think who are the main owners of data and how they train algorithms, the goal is to basically have better monetization, like who will see this advertisement. But there is a competing objective of inclusivity, who will this advertisement reach out to all kinds of populations. And it's not only subjective, there is a kind of clear incentive, for example, in companies, to maximize monetization rather than inclusivity, right, because it's also internal. I don't have an easy answer to this. I agree, it can be subjective or can be more than subjective because these objectives are compete. So would you say it's sort of more the field of ethics research overall is more interdisciplinary and long these answers to these questions are more context dependent. It's very context dependent. Right, it is very context dependent. As I mentioned, for example, the same application in different contexts can be used for good and for bad, right, and different thresholds on performance can be applied for different types of different settings. And also, I really think, like I'm not qualified even to answer this question, right, we should ask maybe philosopher or expert in policy, right, because eventually I'm I know how to build the tools and I'm trying to like I'm trying to make technologies kind of more ethical, but the question of how they are deployed and what are specific decisions it's it's very difficult to control for and to kind of give definite answers about this. Well, here's a number of good gold question for you from and you can't use that copy of our answer. So I said earlier you showed the example of AI GADA with the question of why would we want to study this? The author is justified by claiming that given the widespread use of facial recognition, our findings have critical implications for the protection of civil liberties. Given that some unscrupulous governments may indeed implement such technology to oppress minorities based on such things as orientation, do social scientists have an obligation to get ahead of this threat by understanding the properties of such models? How do we weigh the ethical trade-offs? Oh gosh, now I need to respond from the point of view of all social scientists. I don't want to answer philosophical questions, but I kind of I have the answer about maybe a simple answer to why I don't agree with the claim of researchers that we need to expose this technology to publish actually this paper to expose the dangers of this technology. One of them like the knife analogies that I gave is one of the answers. So if you think about similar field, not a similar field, but a similar type of interaction in security, right, in cyber security. So it's very common to kind of break the algorithm to show its vulnerabilities and then to iteratively fix it. So this is like the approach that researchers took. Let's show the vulnerabilities that we are able to build this technology to expose its threats. But unlike with security field, here the kind of exposure of this technology, publication of this technology can have real implications of human lives. So again, the cost of misclassification. And if you think about other problems, like similar problems, like the problems of, I can give many other similar problems in which we can expose this technology which will harm people. Like let's create deep fakes video, a poor video with professors. We can do it right to expose the danger of technology of deep fakes. But what kind of harm it will bring to specific people who were involved in this kind of exposure of the harm of this technology? So I have the answer of why it was wrong to publish this study in the first place. And why it's not productive, not helpful, but it's very difficult to answer the question, what should social scientists do? I don't know. Okay, well I can ask the question next. I was just disappointed. Oh wait, I think can you hear me now? Yeah. Okay, thank you so much for the talk. That's really interesting. And as we've just seen really challenging stuff. I guess my question is a little bit more practical. So maybe that's a reprieve for you. But I like unfortunately it seems like in a lot of NLP and AI more broadly. Some of this ethics and bias stuff is like kind of an afterthought. A lot of projects don't really necessarily take it into account from the outset and it's more sort of incidental. So my question is like, you know, as we're working on NLP projects maybe even our final course project, what are some kind of concrete steps that we can take or like a systematic approach that we can use to sort of incorporating some of this ethics knowledge in things that might not explicitly seem like they have a lot to do without the X. So it depends on the project, right? It's like I cannot answer generally if the first part of the lecture was exactly about this. If I build my project, what kind of questions I can ask to know if there are some pitfalls. If it's a different project, I think overall, these are important questions which are general for deep learning models, which could be later used to create better technology. So how to incorporate understanding who are the people who produce the language or who are the users incorporating the right inductive biases or a technology for demoting confounds. It doesn't have to be really like specifically on ethics related problems or interpretability of deep learning. It doesn't have to be ethics and of project, but the kind of technology, if you can develop such a technology, eventually it can be useful for building such better like productively models that proactively prevent unintended harms. So I guess the strategy would be sort of to just lay out these general topics, pose these questions to yourself, maybe write them down or just think about them and then proceed as such. Yeah, so I think about it. I think about potential, like if this technology would be deployed, what could be corner cases? What is potential for dual use? If it works, how can it be misused? And also when it doesn't work, what kind of errors can be harmful? So this is, if you go back to in the slides to the beginning of the lecture, these questions, they could be applied to many kinds of technology and they give a more clear kind of guidelines to what things, how bad outcomes could be prevented. I'm sure maybe I'm missing something totally, right? It's all of this content, much of this content is just I came up with it by reading a lot of different papers, but there is no clear guidelines. It's such a new field. So maybe there are things that I am also missing. Here's another question from our models wrong for being biased. In the end, they just learn what they're designed to learn and isn't our intervention to correct this behavior actually cause a bias. This is a good question. So it is kind of it is a question that I also have been thinking about, like a model is wrong for for example reflecting accurately the real world, right? Do we need to de-bias actively models to make them fair when the world is not fair, when our data is not fair? So first of all, the way we train models today, they don't only perpetuate biases, they amplify biases. So this is a natural behavior of a machine learning model that basically when you have an input example for which the confidence is lower, it will default to a majority class. This is why if your data contains biases, these biases will be amplified in a machine learning model trained on this data. And this is clearly wrong. Whether it is wrong to to build models that do not reflect the actual true distribution in the data, it's a much more difficult questions. But there are clear, there are clear kind of cases in which I would say it is wrong to build a model that in the search for CEO shows only male CEOs why why kind of because it amplifies biases. But yeah, this is already a subjective kind of answer. It's just my personal opinion because not much to do is research that we are doing, right? So, would you like to ask your question? Oh yeah, one said, okay, I was trying to start my video but it's saying I can't, so I guess I'm just going to ask. By the way, I don't see people anyway. Well, you'd like to start with DDR, but yeah, anyway, let's just go. Yeah, I'll just ask. Oh yes, thank you so much for talking. I have a question, microaggressions. So who is to decide what is what is contrary to microaggression? Is it the people who microaggressions are potentially targeted against? Is it philosophers or social scientists or just people in education? And in case opinions differ, you know, do we just listen to majority? It all seems culturally important to me, but very, very difficult to standardize and kind of reach consensus, especially because your time culture is perceived differently. Thank you very much for these questions. These are amazing questions. These are difficult questions. This is why we did not create our own corpus of microaggressions, right? Because it's cultural dependent. It's very, very personal subjective. This is why we were focused on a corpus of perceived microaggressions, people that actually felt that the interactions were negative, because they knew that these were microaggressions. It's who is to decide whether something is microaggression? Yeah, I don't know. This is very difficult. What I can think about like practical solutions about it, I would say, have a very well trained annotators who understand what microaggression is. So kind of we explain what is microaggression. They see many examples. They understand, for example, a sentence that targets a minority group and other things, and then have many annotators pair one sentence. So like in say the study, like this is about other social concepts that are abstract. For example, in Dandrovsky's study on respect in police interactions, respect is also a subjective thing, right? So what they did, they took every utterance and they had multiple annotators, multiple trained annotators for each utterance and just increased the number of voters, whether an utterance is respectful or not. So like practically, I think this should be the procedure for creating corpus of microaggressions. But more philosophical questions, who is to decide, it's a more difficult one. So there's still more questions, but you're allowed to say that you're worn out at any point you're liar. And if you're not the next question, I feel bad about not being able to answer big questions about the society and yeah, I'm happy to answer the next question from here's, can you talk a bit more about the unsupervised approach to identifying implicit bias? I can, it's, I just need to think how to talk about it in a few words. So intuitively, we cannot create a corpus of like which has utterance, which has an utterance and then a label is it sentence bias or not. So we create a causal framework in which our target label is more objective, who is, who is the sentence directed to a man or to a woman. So our labels are general labels. And in a naive way, if given a sentence towards a person without looking at the actual person and their comment, just by this comment towards a person, we can predict if it's directed to a woman, we can say that kind of there is some bias in the sentence, but it's an naive approach because there are other ways in which we can predict the target gender, but which will not be associated with bias. For example, it's the context of the current station, the traits of the person that we are writing to and so on. So the crux of our technology is that we predict gender, but the mode all kinds of confounds in the task of detecting bias. So we demote the signals of the source sentence. We demote the latent traits of the target person. And so we make this task very difficult to detect who is a target, what is a target gender. And if after all these demotions of the confounds given utterance that is directed to a specific person, we can still classify this utterance that is clearly directed to a woman, it is likely that this utterance contains bias. So what I was going to talk is all kinds of demotion approaches that we develop, but once we demote this approach and we can still predict the utterance, what is a gender of the target and receive, we actually can surface some bias sentences. So these are the main findings. For example, if we look at comments directed to politicians, after all these demotions and we see comments that kind of clearly predict the target gender, we can see that comments or politicians talk about their spouses, about their family love and also about their competence, maybe question their confidence. And if we look at comments towards public figures like actresses, we can see a lot of words that are just related to objectification, sexualization, regardless of their source content. So they can talk about their movie, but the comments will always be about that she is sexy. And this is what our model is able to surface, but it's again, it's just initial study in the local work. Okay, so around here also, if microaggressions are pulled from a site where people can list what they have experienced, isn't that data very vulnerable to social engineering? Yes, this data is vulnerable. So in our case, we anonymized this data. We extract only quotes from the data. We removed actual users who published it. We remove all the text around these quotes. And this is a good question. Maybe we should also not make this data public, even, yeah. Hey, there are more questions. Thank you. More questions. So by the way, like if Chris John and I'm saying Chris John because they said the two faces, the faces that I see on my screen. Please let me know when we need to finish. I'm happy to continue answering. We've often gone on for a few more minutes. I can ask a couple more questions. So here's one that's very prominent in AI right at the moment. Do you think of this fair for AI scientists and tech and academia who are definitely not representative of the general population to decide what is biased and what is not? IE, the act of devising itself might be biased. Yeah, this is one problem also that this is like more general problem. The researchers, even those who work on the bias and can incorporate their own biases. We currently don't have any other alternative, right? We don't have a training how to do it. We don't have, I think it's a good thing to work on these topics to try to promote these topics as much as possible. We're in a awareness that we as researchers can incorporate our own biases. So this is what we also write in ethical implications sections in the paper that we try to identify bias, we try to de-bias, but there are limitations to this study because we could incorporate our own biases into our analysis, right? This is how we interpret this results. Maybe this is what we were looking for and this is a confirmation bias. Yeah. Okay, maybe should just have one more, oh no, a new question just turned up. Maybe we'll have to be two questions more. Now, maybe I should do that one immediately because it directly relates to that answer, which was from how are the perspectives of community stakeholders, are people from minoritized groups included when these systems are being built? This is a wonderful question to, yeah. Currently not very good. Actually, we have, currently have a paper also in submission about analysis of what kind of how race have been treated in LP systems, starting from data sets to models to potential users. And one of the things that we found is that even people who work on identifying racism, they don't involve actually in group members. This is, yeah, you identified yet another problem in the community that perspectives of community are not often incorporated. In our acquisition paper, we try to advocate for its importance, but like all these questions are very good, but like, there are maybe first, somebody like Chris who has a lot of influence could make changes in the community. It's very difficult to make such changes. Yes, I'm hopeful that there's actually starting to be a bit of change right now. I mean, you know, like one can be pessimistic given the history and one can be pessimistic given the current statistics, but you know, I actually believe that, you know, through recent events of like lives matter and other things that there's actually just more genuine attempts to create change around, well, certainly the standard computer science department, but I think more generally around the field of AI than there's been at any time in the past 30 years when I've been watching it. Right, even when I did my postdoc, it's 1014, 2016, we started working on the problem of gender bias and it was total outlier. Like, I didn't know if what I'm doing will be relevant to anyone. And then now look, we discussed this is a kind of relevant question. This is already an amazing change in the community. And if there will be more focus also on the right hiring, which clearly now has more awareness than ever. Right, I'm also more optimistic now than say three years ago. Okay, well, maybe we'll give you some wonderful questions for which there are no good answers yet. Maybe you can do this as the last question. And let's just say something that really I could listen a lot more. What do you think the social and ethics space might look like say five to 10 years down the line? Do you think the industry might come down to a unified standard for ethics for AI systems? Given that a lot of the challenges come from the fact that social and ethics discussions are often subjective. Yeah, I'm also optimistic about the field of ethics in five years. These are difficult problems. The field of ethics, by the way, itself is 2,000 years old, right? Aristotle already asked these questions. Now we're asking these questions about AI, but giving the current awareness and the bad publicity that currently companies are the main players, right? It's more even than governments. And there is a big incentive with companies to fix things because of the bad publicity. And for example, today, I read an article about that Google will stop advertising the truck, the profile users, like these plugins. So overall, I do see a very positive trajectory. It's very difficult to predict what exactly will be like in five years. I don't think all the problems will be resolved, but overall, I'm optimistic also about the new policies will be already not entirely in hands of decisions of companies. And definitely about research, because I see how many students now are interested in these topics, which is totally amazing. Okay. And another comment that I want to make is actually a no P is important in all this, which was much less say the field of fairness very much focused on image recognition, but I think more and more is we will see more and more research on and there'll be in language, which is also exciting. Okay. Well, maybe we should call a quicks at that point. Thank you so much, Julia.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.280000000000001, "text": " Hello, everyone. Welcome back to CS224N. And today I'm delighted to introduce our final guest speaker,", "tokens": [2425, 11, 1518, 13, 4027, 646, 281, 9460, 17, 7911, 45, 13, 400, 965, 286, 478, 18783, 281, 5366, 527, 2572, 8341, 8145, 11], "temperature": 0.0, "avg_logprob": -0.2933299090411212, "compression_ratio": 1.438095238095238, "no_speech_prob": 0.060274142771959305}, {"id": 1, "seek": 0, "start": 13.280000000000001, "end": 20.28, "text": " Julia Svetkov. So Julia is currently a professor at Carnegie Mellon University, but actually starting", "tokens": [18551, 318, 9771, 33516, 13, 407, 18551, 307, 4362, 257, 8304, 412, 47301, 376, 898, 266, 3535, 11, 457, 767, 2891], "temperature": 0.0, "avg_logprob": -0.2933299090411212, "compression_ratio": 1.438095238095238, "no_speech_prob": 0.060274142771959305}, {"id": 2, "seek": 0, "start": 20.28, "end": 25.32, "text": " from next year, she's going to be a professor at the University of Washington, as you can already", "tokens": [490, 958, 1064, 11, 750, 311, 516, 281, 312, 257, 8304, 412, 264, 3535, 295, 6149, 11, 382, 291, 393, 1217], "temperature": 0.0, "avg_logprob": -0.2933299090411212, "compression_ratio": 1.438095238095238, "no_speech_prob": 0.060274142771959305}, {"id": 3, "seek": 2532, "start": 25.32, "end": 31.96, "text": " see updated in her email address. Julia's research focuses on extending the capabilities of human", "tokens": [536, 10588, 294, 720, 3796, 2985, 13, 18551, 311, 2132, 16109, 322, 24360, 264, 10862, 295, 1952], "temperature": 0.0, "avg_logprob": -0.10890436172485352, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.0019078736659139395}, {"id": 4, "seek": 2532, "start": 31.96, "end": 38.44, "text": " language technology beyond individual cultures and across language boundaries. So lots of work that", "tokens": [2856, 2899, 4399, 2609, 12951, 293, 2108, 2856, 13180, 13, 407, 3195, 295, 589, 300], "temperature": 0.0, "avg_logprob": -0.10890436172485352, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.0019078736659139395}, {"id": 5, "seek": 2532, "start": 38.44, "end": 44.92, "text": " considers the roles of human beings and different multilingual situations. And today she's going to be", "tokens": [33095, 264, 9604, 295, 1952, 8958, 293, 819, 2120, 38219, 6851, 13, 400, 965, 750, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.10890436172485352, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.0019078736659139395}, {"id": 6, "seek": 2532, "start": 44.92, "end": 52.04, "text": " giving a talk to us on social and ethical considerations and NLP systems. Just one more note on the", "tokens": [2902, 257, 751, 281, 505, 322, 2093, 293, 18890, 24070, 293, 426, 45196, 3652, 13, 1449, 472, 544, 3637, 322, 264], "temperature": 0.0, "avg_logprob": -0.10890436172485352, "compression_ratio": 1.6194331983805668, "no_speech_prob": 0.0019078736659139395}, {"id": 7, "seek": 5204, "start": 52.04, "end": 58.28, "text": " way things are going to run. So Julia has some interactive exercises. So what we're going to do is,", "tokens": [636, 721, 366, 516, 281, 1190, 13, 407, 18551, 575, 512, 15141, 11900, 13, 407, 437, 321, 434, 516, 281, 360, 307, 11], "temperature": 0.0, "avg_logprob": -0.21495131690903466, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.001340407063253224}, {"id": 8, "seek": 5204, "start": 58.28, "end": 66.68, "text": " for the interactive exercises, you'll be asked to put something into the Zoom comments. So that's", "tokens": [337, 264, 15141, 11900, 11, 291, 603, 312, 2351, 281, 829, 746, 666, 264, 13453, 3053, 13, 407, 300, 311], "temperature": 0.0, "avg_logprob": -0.21495131690903466, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.001340407063253224}, {"id": 9, "seek": 5204, "start": 66.68, "end": 72.03999999999999, "text": " our, sorry, the Zoom chats. That means really using the chat. And you might want to say who the two", "tokens": [527, 11, 2597, 11, 264, 13453, 38057, 13, 663, 1355, 534, 1228, 264, 5081, 13, 400, 291, 1062, 528, 281, 584, 567, 264, 732], "temperature": 0.0, "avg_logprob": -0.21495131690903466, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.001340407063253224}, {"id": 10, "seek": 5204, "start": 72.03999999999999, "end": 77.48, "text": " to the chat is to, I think it's by default panelists, which is okay or to Julia. So goes to", "tokens": [281, 264, 5081, 307, 281, 11, 286, 519, 309, 311, 538, 7576, 20162, 11, 597, 307, 1392, 420, 281, 18551, 13, 407, 1709, 281], "temperature": 0.0, "avg_logprob": -0.21495131690903466, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.001340407063253224}, {"id": 11, "seek": 7748, "start": 77.48, "end": 83.48, "text": " her panelists is good, but probably not all attendees. So that'll be a bit overwhelming. And then if", "tokens": [720, 20162, 307, 665, 11, 457, 1391, 406, 439, 34826, 13, 407, 300, 603, 312, 257, 857, 13373, 13, 400, 550, 498], "temperature": 0.0, "avg_logprob": -0.17398859274507772, "compression_ratio": 1.6, "no_speech_prob": 8.072052150964737e-05}, {"id": 12, "seek": 7748, "start": 83.48, "end": 91.08, "text": " you have questions, put them in the Q&A as usual, because they'll keep the two streams separate. And", "tokens": [291, 362, 1651, 11, 829, 552, 294, 264, 1249, 5, 32, 382, 7713, 11, 570, 436, 603, 1066, 264, 732, 15842, 4994, 13, 400], "temperature": 0.0, "avg_logprob": -0.17398859274507772, "compression_ratio": 1.6, "no_speech_prob": 8.072052150964737e-05}, {"id": 13, "seek": 7748, "start": 91.08, "end": 97.72, "text": " as for our other invited lectures, if you've got some questions that you'd like to ask, you'll", "tokens": [382, 337, 527, 661, 9185, 16564, 11, 498, 291, 600, 658, 512, 1651, 300, 291, 1116, 411, 281, 1029, 11, 291, 603], "temperature": 0.0, "avg_logprob": -0.17398859274507772, "compression_ratio": 1.6, "no_speech_prob": 8.072052150964737e-05}, {"id": 14, "seek": 7748, "start": 97.72, "end": 104.04, "text": " hear at the end, stay on the line and raise your hand and we can promote people to be panelists", "tokens": [1568, 412, 264, 917, 11, 1754, 322, 264, 1622, 293, 5300, 428, 1011, 293, 321, 393, 9773, 561, 281, 312, 20162], "temperature": 0.0, "avg_logprob": -0.17398859274507772, "compression_ratio": 1.6, "no_speech_prob": 8.072052150964737e-05}, {"id": 15, "seek": 10404, "start": 104.04, "end": 109.16000000000001, "text": " and have a chat with you. Okay, so now further ado on the light that happy you, Julia.", "tokens": [293, 362, 257, 5081, 365, 291, 13, 1033, 11, 370, 586, 3052, 22450, 322, 264, 1442, 300, 2055, 291, 11, 18551, 13], "temperature": 0.0, "avg_logprob": -0.17766996684827302, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.001016290858387947}, {"id": 16, "seek": 10404, "start": 109.80000000000001, "end": 115.72, "text": " Thank you very much, Chris. I'm very excited to speak to you all today despite, unfortunately,", "tokens": [1044, 291, 588, 709, 11, 6688, 13, 286, 478, 588, 2919, 281, 1710, 281, 291, 439, 965, 7228, 11, 7015, 11], "temperature": 0.0, "avg_logprob": -0.17766996684827302, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.001016290858387947}, {"id": 17, "seek": 10404, "start": 115.72, "end": 125.08000000000001, "text": " I cannot see you, but I'm excited. And so this lecture is structured as follows. We'll have", "tokens": [286, 2644, 536, 291, 11, 457, 286, 478, 2919, 13, 400, 370, 341, 7991, 307, 18519, 382, 10002, 13, 492, 603, 362], "temperature": 0.0, "avg_logprob": -0.17766996684827302, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.001016290858387947}, {"id": 18, "seek": 10404, "start": 125.08000000000001, "end": 131.48000000000002, "text": " three parts. The first part will be primarily a discussion in which I will ask questions. It's", "tokens": [1045, 3166, 13, 440, 700, 644, 486, 312, 10029, 257, 5017, 294, 597, 286, 486, 1029, 1651, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.17766996684827302, "compression_ratio": 1.5206611570247934, "no_speech_prob": 0.001016290858387947}, {"id": 19, "seek": 13148, "start": 131.48, "end": 138.12, "text": " supposed to be interactive, but I realize we are very limited in ways we can interact now.", "tokens": [3442, 281, 312, 15141, 11, 457, 286, 4325, 321, 366, 588, 5567, 294, 2098, 321, 393, 4648, 586, 13], "temperature": 0.0, "avg_logprob": -0.10354923672146267, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0024497988633811474}, {"id": 20, "seek": 13148, "start": 138.12, "end": 145.39999999999998, "text": " So this is when please put responses if you want in the chat window. And I will answer my own", "tokens": [407, 341, 307, 562, 1767, 829, 13019, 498, 291, 528, 294, 264, 5081, 4910, 13, 400, 286, 486, 1867, 452, 1065], "temperature": 0.0, "avg_logprob": -0.10354923672146267, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0024497988633811474}, {"id": 21, "seek": 13148, "start": 145.39999999999998, "end": 151.0, "text": " questions following also your responses. And maybe read some of your responses. So this will be the", "tokens": [1651, 3480, 611, 428, 13019, 13, 400, 1310, 1401, 512, 295, 428, 13019, 13, 407, 341, 486, 312, 264], "temperature": 0.0, "avg_logprob": -0.10354923672146267, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0024497988633811474}, {"id": 22, "seek": 13148, "start": 151.0, "end": 156.04, "text": " first part. And the goal of this part is to provide you some practical tools when you have a new", "tokens": [700, 644, 13, 400, 264, 3387, 295, 341, 644, 307, 281, 2893, 291, 512, 8496, 3873, 562, 291, 362, 257, 777], "temperature": 0.0, "avg_logprob": -0.10354923672146267, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0024497988633811474}, {"id": 23, "seek": 15604, "start": 156.04, "end": 164.35999999999999, "text": " problem to work on in AI in your field. How would you assess this problem in terms of how", "tokens": [1154, 281, 589, 322, 294, 7318, 294, 428, 2519, 13, 1012, 576, 291, 5877, 341, 1154, 294, 2115, 295, 577], "temperature": 0.0, "avg_logprob": -0.12106703602990439, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0021084498148411512}, {"id": 24, "seek": 15604, "start": 164.35999999999999, "end": 168.6, "text": " ethical it is to solve it? What kind of biases it may incorporate and so on.", "tokens": [18890, 309, 307, 281, 5039, 309, 30, 708, 733, 295, 32152, 309, 815, 16091, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.12106703602990439, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0021084498148411512}, {"id": 25, "seek": 15604, "start": 169.72, "end": 174.51999999999998, "text": " So in the second part, I will try to generalize to give a review of what are", "tokens": [407, 294, 264, 1150, 644, 11, 286, 486, 853, 281, 2674, 1125, 281, 976, 257, 3131, 295, 437, 366], "temperature": 0.0, "avg_logprob": -0.12106703602990439, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0021084498148411512}, {"id": 26, "seek": 15604, "start": 176.44, "end": 182.12, "text": " overall topics in the intersection of ethics and NLP because it's actually a very big field.", "tokens": [4787, 8378, 294, 264, 15236, 295, 19769, 293, 426, 45196, 570, 309, 311, 767, 257, 588, 955, 2519, 13], "temperature": 0.0, "avg_logprob": -0.12106703602990439, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.0021084498148411512}, {"id": 27, "seek": 18212, "start": 182.12, "end": 187.96, "text": " And what I will talk about today is just a motivational lecture, but there is a lot of technical,", "tokens": [400, 437, 286, 486, 751, 466, 965, 307, 445, 257, 48186, 7991, 11, 457, 456, 307, 257, 688, 295, 6191, 11], "temperature": 0.0, "avg_logprob": -0.07911189058993726, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.003072046209126711}, {"id": 28, "seek": 18212, "start": 187.96, "end": 194.12, "text": " interesting technical content and a lot of subfields of this field. And I will dive a little deeper", "tokens": [1880, 6191, 2701, 293, 257, 688, 295, 1422, 7610, 82, 295, 341, 2519, 13, 400, 286, 486, 9192, 257, 707, 7731], "temperature": 0.0, "avg_logprob": -0.07911189058993726, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.003072046209126711}, {"id": 29, "seek": 18212, "start": 194.12, "end": 202.04000000000002, "text": " in one topic in this field, specifically focusing on algorithmic bias. And if time is left,", "tokens": [294, 472, 4829, 294, 341, 2519, 11, 4682, 8416, 322, 9284, 299, 12577, 13, 400, 498, 565, 307, 1411, 11], "temperature": 0.0, "avg_logprob": -0.07911189058993726, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.003072046209126711}, {"id": 30, "seek": 18212, "start": 202.04000000000002, "end": 208.68, "text": " which I'm not sure about, I will talk about one or two projects in my lab. So specific research", "tokens": [597, 286, 478, 406, 988, 466, 11, 286, 486, 751, 466, 472, 420, 732, 4455, 294, 452, 2715, 13, 407, 2685, 2132], "temperature": 0.0, "avg_logprob": -0.07911189058993726, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.003072046209126711}, {"id": 31, "seek": 20868, "start": 208.68, "end": 215.0, "text": " projects, but if we don't have time to cover it, then you can always read the paper. So the first", "tokens": [4455, 11, 457, 498, 321, 500, 380, 362, 565, 281, 2060, 309, 11, 550, 291, 393, 1009, 1401, 264, 3035, 13, 407, 264, 700], "temperature": 0.0, "avg_logprob": -0.10362850934609599, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.0007134084007702768}, {"id": 32, "seek": 20868, "start": 215.0, "end": 220.6, "text": " first two parts are more important for the purpose of this lecture. So let's start.", "tokens": [700, 732, 3166, 366, 544, 1021, 337, 264, 4334, 295, 341, 7991, 13, 407, 718, 311, 722, 13], "temperature": 0.0, "avg_logprob": -0.10362850934609599, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.0007134084007702768}, {"id": 33, "seek": 20868, "start": 222.28, "end": 227.64000000000001, "text": " This is as far as I understand, this is a course on deep learning and natural language processing.", "tokens": [639, 307, 382, 1400, 382, 286, 1223, 11, 341, 307, 257, 1164, 322, 2452, 2539, 293, 3303, 2856, 9007, 13], "temperature": 0.0, "avg_logprob": -0.10362850934609599, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.0007134084007702768}, {"id": 34, "seek": 20868, "start": 227.64000000000001, "end": 233.16, "text": " So you've probably covered various deep learning architectures and their applications to various", "tokens": [407, 291, 600, 1391, 5343, 3683, 2452, 2539, 6331, 1303, 293, 641, 5821, 281, 3683], "temperature": 0.0, "avg_logprob": -0.10362850934609599, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.0007134084007702768}, {"id": 35, "seek": 23316, "start": 233.16, "end": 238.84, "text": " NLP tasks, like machine translation, dialogue systems, question answering, and there is an obvious", "tokens": [426, 45196, 9608, 11, 411, 3479, 12853, 11, 10221, 3652, 11, 1168, 13430, 11, 293, 456, 307, 364, 6322], "temperature": 0.0, "avg_logprob": -0.15126462884851405, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.001923477160744369}, {"id": 36, "seek": 23316, "start": 238.84, "end": 246.04, "text": " question, what does it all has to do with ethics? What does syntactic parsing or part of speech", "tokens": [1168, 11, 437, 775, 309, 439, 575, 281, 360, 365, 19769, 30, 708, 775, 23980, 19892, 21156, 278, 420, 644, 295, 6218], "temperature": 0.0, "avg_logprob": -0.15126462884851405, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.001923477160744369}, {"id": 37, "seek": 23316, "start": 246.04, "end": 257.08, "text": " tagging has to do with ethics? And the answer, which I want to suggest is this quote, that it's a", "tokens": [6162, 3249, 575, 281, 360, 365, 19769, 30, 400, 264, 1867, 11, 597, 286, 528, 281, 3402, 307, 341, 6513, 11, 300, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.15126462884851405, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.001923477160744369}, {"id": 38, "seek": 25708, "start": 257.08, "end": 264.44, "text": " simple answer that the common misconception is that language has to do with words, but it doesn't.", "tokens": [2199, 1867, 300, 264, 2689, 41350, 307, 300, 2856, 575, 281, 360, 365, 2283, 11, 457, 309, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.10669417661779067, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.0006306325667537749}, {"id": 39, "seek": 25708, "start": 264.44, "end": 272.03999999999996, "text": " It has to do with people. So every word, every sentence that we produced, language is produced", "tokens": [467, 575, 281, 360, 365, 561, 13, 407, 633, 1349, 11, 633, 8174, 300, 321, 7126, 11, 2856, 307, 7126], "temperature": 0.0, "avg_logprob": -0.10669417661779067, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.0006306325667537749}, {"id": 40, "seek": 25708, "start": 272.03999999999996, "end": 278.52, "text": " by people. It is directed towards other people and everything that is related to language necessarily", "tokens": [538, 561, 13, 467, 307, 12898, 3030, 661, 561, 293, 1203, 300, 307, 4077, 281, 2856, 4725], "temperature": 0.0, "avg_logprob": -0.10669417661779067, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.0006306325667537749}, {"id": 41, "seek": 25708, "start": 278.52, "end": 286.12, "text": " involved people. And it has social meaning and incorporates human biases. And this is why also", "tokens": [3288, 561, 13, 400, 309, 575, 2093, 3620, 293, 50193, 1952, 32152, 13, 400, 341, 307, 983, 611], "temperature": 0.0, "avg_logprob": -0.10669417661779067, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.0006306325667537749}, {"id": 42, "seek": 28612, "start": 286.12, "end": 293.48, "text": " models that we build, which will be used by other people, may incorporate social biases.", "tokens": [5245, 300, 321, 1322, 11, 597, 486, 312, 1143, 538, 661, 561, 11, 815, 16091, 2093, 32152, 13], "temperature": 0.0, "avg_logprob": -0.15448064188803395, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.0015323361149057746}, {"id": 43, "seek": 28612, "start": 295.24, "end": 301.72, "text": " So this is why decisions that we make about our data, some kind of considerations that we incorporate", "tokens": [407, 341, 307, 983, 5327, 300, 321, 652, 466, 527, 1412, 11, 512, 733, 295, 24070, 300, 321, 16091], "temperature": 0.0, "avg_logprob": -0.15448064188803395, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.0015323361149057746}, {"id": 44, "seek": 28612, "start": 302.52, "end": 309.16, "text": " into our model, may have direct people, direct impact on people, maybe societies.", "tokens": [666, 527, 2316, 11, 815, 362, 2047, 561, 11, 2047, 2712, 322, 561, 11, 1310, 19329, 13], "temperature": 0.0, "avg_logprob": -0.15448064188803395, "compression_ratio": 1.6790123456790123, "no_speech_prob": 0.0015323361149057746}, {"id": 45, "seek": 30916, "start": 309.16, "end": 318.6, "text": " And to start this lecture, we need to start with understanding what is ethics. So what is ethics?", "tokens": [400, 281, 722, 341, 7991, 11, 321, 643, 281, 722, 365, 3701, 437, 307, 19769, 13, 407, 437, 307, 19769, 30], "temperature": 0.0, "avg_logprob": -0.088773619333903, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.002078024437651038}, {"id": 46, "seek": 30916, "start": 318.6, "end": 324.6, "text": " Here is a definition from a textbook on ethics. Ethics is a study of what a good and bad ends to", "tokens": [1692, 307, 257, 7123, 490, 257, 25591, 322, 19769, 13, 10540, 1167, 307, 257, 2979, 295, 437, 257, 665, 293, 1578, 5314, 281], "temperature": 0.0, "avg_logprob": -0.088773619333903, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.002078024437651038}, {"id": 47, "seek": 30916, "start": 324.6, "end": 331.88, "text": " pursue in life. And what is right and wrong to do in the conduct of life? So it is a practical", "tokens": [12392, 294, 993, 13, 400, 437, 307, 558, 293, 2085, 281, 360, 294, 264, 6018, 295, 993, 30, 407, 309, 307, 257, 8496], "temperature": 0.0, "avg_logprob": -0.088773619333903, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.002078024437651038}, {"id": 48, "seek": 33188, "start": 331.88, "end": 339.8, "text": " discipline. And the primary goal is to determine how one ought to live and what actions one ought to do", "tokens": [13635, 13, 400, 264, 6194, 3387, 307, 281, 6997, 577, 472, 13416, 281, 1621, 293, 437, 5909, 472, 13416, 281, 360], "temperature": 0.0, "avg_logprob": -0.1197792388297416, "compression_ratio": 1.586021505376344, "no_speech_prob": 0.0016800547018647194}, {"id": 49, "seek": 33188, "start": 339.8, "end": 347.88, "text": " in the conduct of one's life. So to summarize, it is very practical and it's simple. It's just", "tokens": [294, 264, 6018, 295, 472, 311, 993, 13, 407, 281, 20858, 11, 309, 307, 588, 8496, 293, 309, 311, 2199, 13, 467, 311, 445], "temperature": 0.0, "avg_logprob": -0.1197792388297416, "compression_ratio": 1.586021505376344, "no_speech_prob": 0.0016800547018647194}, {"id": 50, "seek": 33188, "start": 347.88, "end": 355.88, "text": " doing the good things and doing the right things. Then my question to you is how simple it is to", "tokens": [884, 264, 665, 721, 293, 884, 264, 558, 721, 13, 1396, 452, 1168, 281, 291, 307, 577, 2199, 309, 307, 281], "temperature": 0.0, "avg_logprob": -0.1197792388297416, "compression_ratio": 1.586021505376344, "no_speech_prob": 0.0016800547018647194}, {"id": 51, "seek": 35588, "start": 355.88, "end": 364.12, "text": " divide, to define what is good and what is right. So let's start discussion by diving into various", "tokens": [9845, 11, 281, 6964, 437, 307, 665, 293, 437, 307, 558, 13, 407, 718, 311, 722, 5017, 538, 20241, 666, 3683], "temperature": 0.0, "avg_logprob": -0.1381409454345703, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.002103672595694661}, {"id": 52, "seek": 35588, "start": 364.12, "end": 371.08, "text": " problems. And we start with a boring theoretical problem, which everybody knows about, which is a", "tokens": [2740, 13, 400, 321, 722, 365, 257, 9989, 20864, 1154, 11, 597, 2201, 3255, 466, 11, 597, 307, 257], "temperature": 0.0, "avg_logprob": -0.1381409454345703, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.002103672595694661}, {"id": 53, "seek": 35588, "start": 371.08, "end": 377.48, "text": " trolley dilemma. And we won't spend too much time on it. So just to, I'm sure all of you know about", "tokens": [20680, 2030, 34312, 13, 400, 321, 1582, 380, 3496, 886, 709, 565, 322, 309, 13, 407, 445, 281, 11, 286, 478, 988, 439, 295, 291, 458, 466], "temperature": 0.0, "avg_logprob": -0.1381409454345703, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.002103672595694661}, {"id": 54, "seek": 37748, "start": 377.48, "end": 387.40000000000003, "text": " it. So it's a classical problem in ethics in which so this is you standing near the lever.", "tokens": [309, 13, 407, 309, 311, 257, 13735, 1154, 294, 19769, 294, 597, 370, 341, 307, 291, 4877, 2651, 264, 12451, 13], "temperature": 0.0, "avg_logprob": -0.11867298618439705, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0012679415522143245}, {"id": 55, "seek": 37748, "start": 387.40000000000003, "end": 392.76, "text": " And here is a trolley coming and there are several people. So the trolley cannot see the people", "tokens": [400, 510, 307, 257, 20680, 2030, 1348, 293, 456, 366, 2940, 561, 13, 407, 264, 20680, 2030, 2644, 536, 264, 561], "temperature": 0.0, "avg_logprob": -0.11867298618439705, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0012679415522143245}, {"id": 56, "seek": 37748, "start": 392.76, "end": 398.52000000000004, "text": " and the people cannot see the trolley. And you are the only one in control, in charge.", "tokens": [293, 264, 561, 2644, 536, 264, 20680, 2030, 13, 400, 291, 366, 264, 787, 472, 294, 1969, 11, 294, 4602, 13], "temperature": 0.0, "avg_logprob": -0.11867298618439705, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0012679415522143245}, {"id": 57, "seek": 37748, "start": 399.32, "end": 405.72, "text": " You can save people and maybe you need to make decisions about people's life. Do you ask yourself", "tokens": [509, 393, 3155, 561, 293, 1310, 291, 643, 281, 652, 5327, 466, 561, 311, 993, 13, 1144, 291, 1029, 1803], "temperature": 0.0, "avg_logprob": -0.11867298618439705, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.0012679415522143245}, {"id": 58, "seek": 40572, "start": 405.72, "end": 417.64000000000004, "text": " why me? But the point here is that imagine that there are five people on one side and no one on", "tokens": [983, 385, 30, 583, 264, 935, 510, 307, 300, 3811, 300, 456, 366, 1732, 561, 322, 472, 1252, 293, 572, 472, 322], "temperature": 0.0, "avg_logprob": -0.1498410325301321, "compression_ratio": 1.676300578034682, "no_speech_prob": 0.001254510716535151}, {"id": 59, "seek": 40572, "start": 417.64000000000004, "end": 423.0, "text": " the other side. And then I would ask you, would you pull the lever to save five people if the trolley", "tokens": [264, 661, 1252, 13, 400, 550, 286, 576, 1029, 291, 11, 576, 291, 2235, 264, 12451, 281, 3155, 1732, 561, 498, 264, 20680, 2030], "temperature": 0.0, "avg_logprob": -0.1498410325301321, "compression_ratio": 1.676300578034682, "no_speech_prob": 0.001254510716535151}, {"id": 60, "seek": 40572, "start": 423.0, "end": 435.0, "text": " is supposed to go straight? And if I would ask you, interactively, everybody would say, yes,", "tokens": [307, 3442, 281, 352, 2997, 30, 400, 498, 286, 576, 1029, 291, 11, 4648, 3413, 11, 2201, 576, 584, 11, 2086, 11], "temperature": 0.0, "avg_logprob": -0.1498410325301321, "compression_ratio": 1.676300578034682, "no_speech_prob": 0.001254510716535151}, {"id": 61, "seek": 43500, "start": 435.0, "end": 441.08, "text": " I will pull the lever. And then I will follow up with next question, okay, what about if five people", "tokens": [286, 486, 2235, 264, 12451, 13, 400, 550, 286, 486, 1524, 493, 365, 958, 1168, 11, 1392, 11, 437, 466, 498, 1732, 561], "temperature": 0.0, "avg_logprob": -0.1209695411450935, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.002046968089416623}, {"id": 62, "seek": 43500, "start": 441.08, "end": 447.08, "text": " on one side and only one person on the other side? So would you pull the lever to minimize the number", "tokens": [322, 472, 1252, 293, 787, 472, 954, 322, 264, 661, 1252, 30, 407, 576, 291, 2235, 264, 12451, 281, 17522, 264, 1230], "temperature": 0.0, "avg_logprob": -0.1209695411450935, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.002046968089416623}, {"id": 63, "seek": 43500, "start": 448.04, "end": 453.72, "text": " of lives that will be sacrificed? And some people will not answer, some people will say, yes,", "tokens": [295, 2909, 300, 486, 312, 32021, 30, 400, 512, 561, 486, 406, 1867, 11, 512, 561, 486, 584, 11, 2086, 11], "temperature": 0.0, "avg_logprob": -0.1209695411450935, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.002046968089416623}, {"id": 64, "seek": 43500, "start": 454.52, "end": 459.56, "text": " some people will say no. And those who say, yes, I will ask them, what if this one person", "tokens": [512, 561, 486, 584, 572, 13, 400, 729, 567, 584, 11, 2086, 11, 286, 486, 1029, 552, 11, 437, 498, 341, 472, 954], "temperature": 0.0, "avg_logprob": -0.1209695411450935, "compression_ratio": 1.8293838862559242, "no_speech_prob": 0.002046968089416623}, {"id": 65, "seek": 45956, "start": 459.56, "end": 466.04, "text": " is your brother? And on the other side, just five random people, what would be your answer?", "tokens": [307, 428, 3708, 30, 400, 322, 264, 661, 1252, 11, 445, 1732, 4974, 561, 11, 437, 576, 312, 428, 1867, 30], "temperature": 0.0, "avg_logprob": -0.126790291554219, "compression_ratio": 1.6193181818181819, "no_speech_prob": 0.001654937630519271}, {"id": 66, "seek": 45956, "start": 466.04, "end": 473.48, "text": " And I can go on and on and on to make this problem harder and harder. And as you can imagine,", "tokens": [400, 286, 393, 352, 322, 293, 322, 293, 322, 281, 652, 341, 1154, 6081, 293, 6081, 13, 400, 382, 291, 393, 3811, 11], "temperature": 0.0, "avg_logprob": -0.126790291554219, "compression_ratio": 1.6193181818181819, "no_speech_prob": 0.001654937630519271}, {"id": 67, "seek": 45956, "start": 475.64, "end": 483.08, "text": " the answers are difficult. And also, we don't know what the answer will be in the actual situation.", "tokens": [264, 6338, 366, 2252, 13, 400, 611, 11, 321, 500, 380, 458, 437, 264, 1867, 486, 312, 294, 264, 3539, 2590, 13], "temperature": 0.0, "avg_logprob": -0.126790291554219, "compression_ratio": 1.6193181818181819, "no_speech_prob": 0.001654937630519271}, {"id": 68, "seek": 48308, "start": 483.08, "end": 489.96, "text": " And while this problem is theoretical, it is in part becoming relevant now when we talk about", "tokens": [400, 1339, 341, 1154, 307, 20864, 11, 309, 307, 294, 644, 5617, 7340, 586, 562, 321, 751, 466], "temperature": 0.0, "avg_logprob": -0.1017829245991177, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.002143226098269224}, {"id": 69, "seek": 48308, "start": 489.96, "end": 498.76, "text": " self-driving cars. So I am now moving to closer to the topics that we will discuss today. And I want", "tokens": [2698, 12, 47094, 5163, 13, 407, 286, 669, 586, 2684, 281, 4966, 281, 264, 8378, 300, 321, 486, 2248, 965, 13, 400, 286, 528], "temperature": 0.0, "avg_logprob": -0.1017829245991177, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.002143226098269224}, {"id": 70, "seek": 48308, "start": 498.76, "end": 507.64, "text": " to introduce a new problem, which I call the chicken dilemma. So in this dilemma, let's train a", "tokens": [281, 5366, 257, 777, 1154, 11, 597, 286, 818, 264, 4662, 34312, 13, 407, 294, 341, 34312, 11, 718, 311, 3847, 257], "temperature": 0.0, "avg_logprob": -0.1017829245991177, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.002143226098269224}, {"id": 71, "seek": 50764, "start": 507.64, "end": 518.84, "text": " classifier. And this will be a simple CNN classifier. And the input to the classifier is an egg.", "tokens": [1508, 9902, 13, 400, 341, 486, 312, 257, 2199, 24859, 1508, 9902, 13, 400, 264, 4846, 281, 264, 1508, 9902, 307, 364, 3777, 13], "temperature": 0.0, "avg_logprob": -0.1930842032799354, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0035162491258233786}, {"id": 72, "seek": 50764, "start": 519.8, "end": 524.68, "text": " And the classifier needs to define the gender of the chicken, of the chick.", "tokens": [400, 264, 1508, 9902, 2203, 281, 6964, 264, 7898, 295, 264, 4662, 11, 295, 264, 14371, 13], "temperature": 0.0, "avg_logprob": -0.1930842032799354, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0035162491258233786}, {"id": 73, "seek": 50764, "start": 525.4, "end": 534.28, "text": " So and decides if it's a ham, it will go to egg laying farm. And if it's a rooster, it will go to", "tokens": [407, 293, 14898, 498, 309, 311, 257, 7852, 11, 309, 486, 352, 281, 3777, 14903, 5421, 13, 400, 498, 309, 311, 257, 744, 7096, 11, 309, 486, 352, 281], "temperature": 0.0, "avg_logprob": -0.1930842032799354, "compression_ratio": 1.7763157894736843, "no_speech_prob": 0.0035162491258233786}, {"id": 74, "seek": 53428, "start": 534.28, "end": 544.04, "text": " meat farm. So first of all, do you think you can build such a classifier? I'm sure every student", "tokens": [4615, 5421, 13, 407, 700, 295, 439, 11, 360, 291, 519, 291, 393, 1322, 1270, 257, 1508, 9902, 30, 286, 478, 988, 633, 3107], "temperature": 0.0, "avg_logprob": -0.11848914914014863, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.0009210087591782212}, {"id": 75, "seek": 53428, "start": 544.04, "end": 549.9599999999999, "text": " in this course will easily build a classifier. And I'm sure it will have quite a good accuracy.", "tokens": [294, 341, 1164, 486, 3612, 1322, 257, 1508, 9902, 13, 400, 286, 478, 988, 309, 486, 362, 1596, 257, 665, 14170, 13], "temperature": 0.0, "avg_logprob": -0.11848914914014863, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.0009210087591782212}, {"id": 76, "seek": 53428, "start": 551.4, "end": 553.9599999999999, "text": " And then the question to you is, do you think it is ethical?", "tokens": [400, 550, 264, 1168, 281, 291, 307, 11, 360, 291, 519, 309, 307, 18890, 30], "temperature": 0.0, "avg_logprob": -0.11848914914014863, "compression_ratio": 1.627659574468085, "no_speech_prob": 0.0009210087591782212}, {"id": 77, "seek": 55396, "start": 553.96, "end": 571.1600000000001, "text": " And I invite you to type your responses in the chat. Yes, no. I mean, you can justify a little bit.", "tokens": [400, 286, 7980, 291, 281, 2010, 428, 13019, 294, 264, 5081, 13, 1079, 11, 572, 13, 286, 914, 11, 291, 393, 20833, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.16286677730326748, "compression_ratio": 1.368421052631579, "no_speech_prob": 0.0009339118842035532}, {"id": 78, "seek": 55396, "start": 574.84, "end": 580.36, "text": " Thank you for participating. So could you repeat the question? So the question is,", "tokens": [1044, 291, 337, 13950, 13, 407, 727, 291, 7149, 264, 1168, 30, 407, 264, 1168, 307, 11], "temperature": 0.0, "avg_logprob": -0.16286677730326748, "compression_ratio": 1.368421052631579, "no_speech_prob": 0.0009339118842035532}, {"id": 79, "seek": 58036, "start": 580.36, "end": 586.36, "text": " there is an egg. And you need to determine the gender of the chick. And if it's a rooster,", "tokens": [456, 307, 364, 3777, 13, 400, 291, 643, 281, 6997, 264, 7898, 295, 264, 14371, 13, 400, 498, 309, 311, 257, 744, 7096, 11], "temperature": 0.0, "avg_logprob": -0.13963937759399414, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.002216777065768838}, {"id": 80, "seek": 58036, "start": 586.36, "end": 594.92, "text": " it will go to a meat farm. And if it's a ham, it will go to an egg laying farm. And the question is,", "tokens": [309, 486, 352, 281, 257, 4615, 5421, 13, 400, 498, 309, 311, 257, 7852, 11, 309, 486, 352, 281, 364, 3777, 14903, 5421, 13, 400, 264, 1168, 307, 11], "temperature": 0.0, "avg_logprob": -0.13963937759399414, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.002216777065768838}, {"id": 81, "seek": 58036, "start": 594.92, "end": 603.24, "text": " is this ethical? So there are kinds of responses. Let's see. So yes and no. But you can use exact", "tokens": [307, 341, 18890, 30, 407, 456, 366, 3685, 295, 13019, 13, 961, 311, 536, 13, 407, 2086, 293, 572, 13, 583, 291, 393, 764, 1900], "temperature": 0.0, "avg_logprob": -0.13963937759399414, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.002216777065768838}, {"id": 82, "seek": 60324, "start": 603.24, "end": 613.48, "text": " same thing to target ethnic groups instead. So yes, thank you. And I see there are many interesting", "tokens": [912, 551, 281, 3779, 14363, 3935, 2602, 13, 407, 2086, 11, 1309, 291, 13, 400, 286, 536, 456, 366, 867, 1880], "temperature": 0.0, "avg_logprob": -0.17219061464876742, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.003168797353282571}, {"id": 83, "seek": 60324, "start": 613.48, "end": 621.4, "text": " responses here. And just the amount of responses, I cannot even have time to read them. So anyway,", "tokens": [13019, 510, 13, 400, 445, 264, 2372, 295, 13019, 11, 286, 2644, 754, 362, 565, 281, 1401, 552, 13, 407, 4033, 11], "temperature": 0.0, "avg_logprob": -0.17219061464876742, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.003168797353282571}, {"id": 84, "seek": 60324, "start": 623.08, "end": 631.96, "text": " so based on this question, I can tell you what my thoughts are. So as a vegetarian, I maybe think", "tokens": [370, 2361, 322, 341, 1168, 11, 286, 393, 980, 291, 437, 452, 4598, 366, 13, 407, 382, 257, 25739, 11, 286, 1310, 519], "temperature": 0.0, "avg_logprob": -0.17219061464876742, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.003168797353282571}, {"id": 85, "seek": 63196, "start": 631.96, "end": 638.6, "text": " it's unethical. But as a mother, I actually want my kids to eat meat. And whether I think it's", "tokens": [309, 311, 517, 3293, 804, 13, 583, 382, 257, 2895, 11, 286, 767, 528, 452, 2301, 281, 1862, 4615, 13, 400, 1968, 286, 519, 309, 311], "temperature": 0.0, "avg_logprob": -0.11017146706581116, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.002844935981556773}, {"id": 86, "seek": 63196, "start": 638.6, "end": 643.96, "text": " ethical or not, we are doing this anyway today. And there are all kinds of considerations,", "tokens": [18890, 420, 406, 11, 321, 366, 884, 341, 4033, 965, 13, 400, 456, 366, 439, 3685, 295, 24070, 11], "temperature": 0.0, "avg_logprob": -0.11017146706581116, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.002844935981556773}, {"id": 87, "seek": 63196, "start": 644.6800000000001, "end": 651.32, "text": " pro and cons. For example, this is what already is done today. And then maybe such classifier will", "tokens": [447, 293, 1014, 13, 1171, 1365, 11, 341, 307, 437, 1217, 307, 1096, 965, 13, 400, 550, 1310, 1270, 1508, 9902, 486], "temperature": 0.0, "avg_logprob": -0.11017146706581116, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.002844935981556773}, {"id": 88, "seek": 63196, "start": 651.32, "end": 658.0400000000001, "text": " minimize the suffering the animal. But on the other hand, we hope that in the future,", "tokens": [17522, 264, 7755, 264, 5496, 13, 583, 322, 264, 661, 1011, 11, 321, 1454, 300, 294, 264, 2027, 11], "temperature": 0.0, "avg_logprob": -0.11017146706581116, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.002844935981556773}, {"id": 89, "seek": 65804, "start": 658.04, "end": 666.36, "text": " society, the life of a chicken will be as valuable as the life of a person. And I can continue on and", "tokens": [4086, 11, 264, 993, 295, 257, 4662, 486, 312, 382, 8263, 382, 264, 993, 295, 257, 954, 13, 400, 286, 393, 2354, 322, 293], "temperature": 0.0, "avg_logprob": -0.13459763844807943, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.001784489955753088}, {"id": 90, "seek": 65804, "start": 666.36, "end": 672.5999999999999, "text": " on. But from this example, I also don't want to stay on it too long. You can see that the", "tokens": [322, 13, 583, 490, 341, 1365, 11, 286, 611, 500, 380, 528, 281, 1754, 322, 309, 886, 938, 13, 509, 393, 536, 300, 264], "temperature": 0.0, "avg_logprob": -0.13459763844807943, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.001784489955753088}, {"id": 91, "seek": 65804, "start": 672.5999999999999, "end": 681.64, "text": " questions of ethics are difficult. That whether like you don't know too much about this field,", "tokens": [1651, 295, 19769, 366, 2252, 13, 663, 1968, 411, 291, 500, 380, 458, 886, 709, 466, 341, 2519, 11], "temperature": 0.0, "avg_logprob": -0.13459763844807943, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.001784489955753088}, {"id": 92, "seek": 68164, "start": 681.64, "end": 689.96, "text": " but you can feel what is the right answer. So ethics is inner guiding. It's moral principles.", "tokens": [457, 291, 393, 841, 437, 307, 264, 558, 1867, 13, 407, 19769, 307, 7284, 25061, 13, 467, 311, 9723, 9156, 13], "temperature": 0.0, "avg_logprob": -0.11456881398740022, "compression_ratio": 1.5, "no_speech_prob": 0.0010249674087390304}, {"id": 93, "seek": 68164, "start": 690.68, "end": 699.8, "text": " And there are often no easy answers. So there are many gray areas. And importantly, ethics changes", "tokens": [400, 456, 366, 2049, 572, 1858, 6338, 13, 407, 456, 366, 867, 10855, 3179, 13, 400, 8906, 11, 19769, 2962], "temperature": 0.0, "avg_logprob": -0.11456881398740022, "compression_ratio": 1.5, "no_speech_prob": 0.0010249674087390304}, {"id": 94, "seek": 68164, "start": 699.8, "end": 705.8, "text": " over time with values and beliefs of people. So whatever we discuss today, we can think it's", "tokens": [670, 565, 365, 4190, 293, 13585, 295, 561, 13, 407, 2035, 321, 2248, 965, 11, 321, 393, 519, 309, 311], "temperature": 0.0, "avg_logprob": -0.11456881398740022, "compression_ratio": 1.5, "no_speech_prob": 0.0010249674087390304}, {"id": 95, "seek": 70580, "start": 705.8, "end": 712.3599999999999, "text": " ethical or not ethical, but it may change in a hundred of years. And maybe a hundred years ago,", "tokens": [18890, 420, 406, 18890, 11, 457, 309, 815, 1319, 294, 257, 3262, 295, 924, 13, 400, 1310, 257, 3262, 924, 2057, 11], "temperature": 0.0, "avg_logprob": -0.08190428841974318, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0011492448393255472}, {"id": 96, "seek": 70580, "start": 712.3599999999999, "end": 719.4, "text": " this would not even be a question why this would be unethical. And another important point is that", "tokens": [341, 576, 406, 754, 312, 257, 1168, 983, 341, 576, 312, 517, 3293, 804, 13, 400, 1071, 1021, 935, 307, 300], "temperature": 0.0, "avg_logprob": -0.08190428841974318, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0011492448393255472}, {"id": 97, "seek": 70580, "start": 721.3199999999999, "end": 727.56, "text": " this is what we are doing today. So what is ethical is what is legal is not necessarily aligned.", "tokens": [341, 307, 437, 321, 366, 884, 965, 13, 407, 437, 307, 18890, 307, 437, 307, 5089, 307, 406, 4725, 17962, 13], "temperature": 0.0, "avg_logprob": -0.08190428841974318, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0011492448393255472}, {"id": 98, "seek": 70580, "start": 727.56, "end": 734.3599999999999, "text": " We can do legal things that will still be unethical. And now having this primer, I want to move", "tokens": [492, 393, 360, 5089, 721, 300, 486, 920, 312, 517, 3293, 804, 13, 400, 586, 1419, 341, 12595, 11, 286, 528, 281, 1286], "temperature": 0.0, "avg_logprob": -0.08190428841974318, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.0011492448393255472}, {"id": 99, "seek": 73436, "start": 734.36, "end": 741.64, "text": " to the actual problems, the actual problems that we can kind of be asked to build and decide whether", "tokens": [281, 264, 3539, 2740, 11, 264, 3539, 2740, 300, 321, 393, 733, 295, 312, 2351, 281, 1322, 293, 4536, 1968], "temperature": 0.0, "avg_logprob": -0.10457217282262342, "compression_ratio": 1.9365079365079365, "no_speech_prob": 0.0018845004960894585}, {"id": 100, "seek": 73436, "start": 741.64, "end": 747.32, "text": " we want to build them or not. And the way I will guide this discussion is I will ask you a specific", "tokens": [321, 528, 281, 1322, 552, 420, 406, 13, 400, 264, 636, 286, 486, 5934, 341, 5017, 307, 286, 486, 1029, 291, 257, 2685], "temperature": 0.0, "avg_logprob": -0.10457217282262342, "compression_ratio": 1.9365079365079365, "no_speech_prob": 0.0018845004960894585}, {"id": 101, "seek": 73436, "start": 747.32, "end": 753.16, "text": " questions. We will ask you for your answers. And I realize it's very difficult to read the chat,", "tokens": [1651, 13, 492, 486, 1029, 291, 337, 428, 6338, 13, 400, 286, 4325, 309, 311, 588, 2252, 281, 1401, 264, 5081, 11], "temperature": 0.0, "avg_logprob": -0.10457217282262342, "compression_ratio": 1.9365079365079365, "no_speech_prob": 0.0018845004960894585}, {"id": 102, "seek": 73436, "start": 753.16, "end": 759.16, "text": " specific answers. But the point is that the types of questions that I will ask you, this would be", "tokens": [2685, 6338, 13, 583, 264, 935, 307, 300, 264, 3467, 295, 1651, 300, 286, 486, 1029, 291, 11, 341, 576, 312], "temperature": 0.0, "avg_logprob": -0.10457217282262342, "compression_ratio": 1.9365079365079365, "no_speech_prob": 0.0018845004960894585}, {"id": 103, "seek": 73436, "start": 759.16, "end": 764.28, "text": " the questions that you could ask yourself when you need to build a technology. And maybe the", "tokens": [264, 1651, 300, 291, 727, 1029, 1803, 562, 291, 643, 281, 1322, 257, 2899, 13, 400, 1310, 264], "temperature": 0.0, "avg_logprob": -0.10457217282262342, "compression_ratio": 1.9365079365079365, "no_speech_prob": 0.0018845004960894585}, {"id": 104, "seek": 76428, "start": 764.28, "end": 770.28, "text": " question whether something is ethical or not is a difficult question, but let's try to break down", "tokens": [1168, 1968, 746, 307, 18890, 420, 406, 307, 257, 2252, 1168, 11, 457, 718, 311, 853, 281, 1821, 760], "temperature": 0.0, "avg_logprob": -0.19226609468460082, "compression_ratio": 1.6787564766839378, "no_speech_prob": 0.0011261370964348316}, {"id": 105, "seek": 76428, "start": 770.76, "end": 777.56, "text": " analysis of a specific application of a specific model to derive an answer which", "tokens": [5215, 295, 257, 2685, 3861, 295, 257, 2685, 2316, 281, 28446, 364, 1867, 597], "temperature": 0.0, "avg_logprob": -0.19226609468460082, "compression_ratio": 1.6787564766839378, "no_speech_prob": 0.0011261370964348316}, {"id": 106, "seek": 76428, "start": 779.0, "end": 783.0799999999999, "text": " will give us some tools to derive an answer in an easier way.", "tokens": [486, 976, 505, 512, 3873, 281, 28446, 364, 1867, 294, 364, 3571, 636, 13], "temperature": 0.0, "avg_logprob": -0.19226609468460082, "compression_ratio": 1.6787564766839378, "no_speech_prob": 0.0011261370964348316}, {"id": 107, "seek": 76428, "start": 785.9599999999999, "end": 792.92, "text": " So here is an exclacifier that we want to build. We want to build an IQ classifier.", "tokens": [407, 510, 307, 364, 1624, 75, 326, 9902, 300, 321, 528, 281, 1322, 13, 492, 528, 281, 1322, 364, 28921, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.19226609468460082, "compression_ratio": 1.6787564766839378, "no_speech_prob": 0.0011261370964348316}, {"id": 108, "seek": 79292, "start": 792.92, "end": 800.92, "text": " So we will be talking about predictive technology. So based on people's personal data, for example,", "tokens": [407, 321, 486, 312, 1417, 466, 35521, 2899, 13, 407, 2361, 322, 561, 311, 2973, 1412, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.14024452313984911, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0025091315619647503}, {"id": 109, "seek": 79292, "start": 800.92, "end": 807.3199999999999, "text": " facial images, and maybe we can collect the text of these people on social media, media,", "tokens": [15642, 5267, 11, 293, 1310, 321, 393, 2500, 264, 2487, 295, 613, 561, 322, 2093, 3021, 11, 3021, 11], "temperature": 0.0, "avg_logprob": -0.14024452313984911, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0025091315619647503}, {"id": 110, "seek": 79292, "start": 808.92, "end": 822.4399999999999, "text": " let's predict the IQ of the person. So if you don't know what IQ is, IQ is a general capacity of an", "tokens": [718, 311, 6069, 264, 28921, 295, 264, 954, 13, 407, 498, 291, 500, 380, 458, 437, 28921, 307, 11, 28921, 307, 257, 2674, 6042, 295, 364], "temperature": 0.0, "avg_logprob": -0.14024452313984911, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0025091315619647503}, {"id": 111, "seek": 82244, "start": 822.44, "end": 830.44, "text": " individual to consciously adjust the thinking to new requirements. So it's basically how intelligent", "tokens": [2609, 281, 32538, 4369, 264, 1953, 281, 777, 7728, 13, 407, 309, 311, 1936, 577, 13232], "temperature": 0.0, "avg_logprob": -0.1254962899468162, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.001386492047458887}, {"id": 112, "seek": 82244, "start": 830.44, "end": 837.96, "text": " a person is. So this is already not a hypothetical problem. You can collect the individual's data,", "tokens": [257, 954, 307, 13, 407, 341, 307, 1217, 406, 257, 33053, 1154, 13, 509, 393, 2500, 264, 2609, 311, 1412, 11], "temperature": 0.0, "avg_logprob": -0.1254962899468162, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.001386492047458887}, {"id": 113, "seek": 82244, "start": 837.96, "end": 845.08, "text": " you can collect the text online, and you can collect training data to predict people's IQ.", "tokens": [291, 393, 2500, 264, 2487, 2950, 11, 293, 291, 393, 2500, 3097, 1412, 281, 6069, 561, 311, 28921, 13], "temperature": 0.0, "avg_logprob": -0.1254962899468162, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.001386492047458887}, {"id": 114, "seek": 82244, "start": 845.8000000000001, "end": 851.48, "text": " But when I will ask you, is this an ethical question or not, it might be a difficult question to", "tokens": [583, 562, 286, 486, 1029, 291, 11, 307, 341, 364, 18890, 1168, 420, 406, 11, 309, 1062, 312, 257, 2252, 1168, 281], "temperature": 0.0, "avg_logprob": -0.1254962899468162, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.001386492047458887}, {"id": 115, "seek": 85148, "start": 851.48, "end": 858.28, "text": " answer immediately. Thank you very much for participating. I really appreciate", "tokens": [1867, 4258, 13, 1044, 291, 588, 709, 337, 13950, 13, 286, 534, 4449], "temperature": 0.0, "avg_logprob": -0.16669895384046768, "compression_ratio": 1.3161764705882353, "no_speech_prob": 0.0012246264377608895}, {"id": 116, "seek": 85148, "start": 858.28, "end": 862.2, "text": " I hope I can save this chat later to read the answers.", "tokens": [286, 1454, 286, 393, 3155, 341, 5081, 1780, 281, 1401, 264, 6338, 13], "temperature": 0.0, "avg_logprob": -0.16669895384046768, "compression_ratio": 1.3161764705882353, "no_speech_prob": 0.0012246264377608895}, {"id": 117, "seek": 85148, "start": 871.0, "end": 873.8000000000001, "text": " Okay, so let's start with the first question.", "tokens": [1033, 11, 370, 718, 311, 722, 365, 264, 700, 1168, 13], "temperature": 0.0, "avg_logprob": -0.16669895384046768, "compression_ratio": 1.3161764705882353, "no_speech_prob": 0.0012246264377608895}, {"id": 118, "seek": 87380, "start": 873.8, "end": 882.52, "text": " We need to predict people's IQ from their photos and text. And then the first question that", "tokens": [492, 643, 281, 6069, 561, 311, 28921, 490, 641, 5787, 293, 2487, 13, 400, 550, 264, 700, 1168, 300], "temperature": 0.0, "avg_logprob": -0.16264963841092758, "compression_ratio": 1.5872093023255813, "no_speech_prob": 0.00114236562512815}, {"id": 119, "seek": 87380, "start": 882.52, "end": 888.28, "text": " if I ask you, is it ethical or not, I don't know. And then you can ask yourself first,", "tokens": [498, 286, 1029, 291, 11, 307, 309, 18890, 420, 406, 11, 286, 500, 380, 458, 13, 400, 550, 291, 393, 1029, 1803, 700, 11], "temperature": 0.0, "avg_logprob": -0.16264963841092758, "compression_ratio": 1.5872093023255813, "no_speech_prob": 0.00114236562512815}, {"id": 120, "seek": 87380, "start": 889.0, "end": 895.0799999999999, "text": " who would benefit from such a technology? So can you think who would benefit from a technology", "tokens": [567, 576, 5121, 490, 1270, 257, 2899, 30, 407, 393, 291, 519, 567, 576, 5121, 490, 257, 2899], "temperature": 0.0, "avg_logprob": -0.16264963841092758, "compression_ratio": 1.5872093023255813, "no_speech_prob": 0.00114236562512815}, {"id": 121, "seek": 89508, "start": 895.08, "end": 911.24, "text": " that predicts an IQ of a person? Hiring employers, schools, universities, so I see your answers.", "tokens": [300, 6069, 82, 364, 28921, 295, 257, 954, 30, 389, 5057, 16744, 11, 4656, 11, 11779, 11, 370, 286, 536, 428, 6338, 13], "temperature": 0.0, "avg_logprob": -0.20817449141521843, "compression_ratio": 1.3146853146853146, "no_speech_prob": 0.0009898069547489285}, {"id": 122, "seek": 89508, "start": 911.24, "end": 920.12, "text": " Right. So overall, it can be a useful technology. Immigration services can benefit from it.", "tokens": [1779, 13, 407, 4787, 11, 309, 393, 312, 257, 4420, 2899, 13, 17322, 36045, 3328, 393, 5121, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.20817449141521843, "compression_ratio": 1.3146853146853146, "no_speech_prob": 0.0009898069547489285}, {"id": 123, "seek": 92012, "start": 920.12, "end": 930.36, "text": " And invite only smart people to immigrate to a country. Even individuals with high IQ can", "tokens": [400, 7980, 787, 4069, 561, 281, 7730, 4404, 281, 257, 1941, 13, 2754, 5346, 365, 1090, 28921, 393], "temperature": 0.0, "avg_logprob": -0.1944281578063965, "compression_ratio": 1.4639175257731958, "no_speech_prob": 0.00292324204929173}, {"id": 124, "seek": 92012, "start": 930.36, "end": 937.24, "text": " benefit from this, right? Because they would maybe not need to do GRE and SAT. They will not need", "tokens": [5121, 490, 341, 11, 558, 30, 1436, 436, 576, 1310, 406, 643, 281, 360, 10903, 36, 293, 31536, 13, 814, 486, 406, 643], "temperature": 0.0, "avg_logprob": -0.1944281578063965, "compression_ratio": 1.4639175257731958, "no_speech_prob": 0.00292324204929173}, {"id": 125, "seek": 92012, "start": 937.24, "end": 944.84, "text": " to write essays. They will just need to show the IQ. Okay, so this technology can potentially be", "tokens": [281, 2464, 35123, 13, 814, 486, 445, 643, 281, 855, 264, 28921, 13, 1033, 11, 370, 341, 2899, 393, 7263, 312], "temperature": 0.0, "avg_logprob": -0.1944281578063965, "compression_ratio": 1.4639175257731958, "no_speech_prob": 0.00292324204929173}, {"id": 126, "seek": 94484, "start": 944.84, "end": 953.96, "text": " useful. And then the next question is, let's assume we can build such a technology.", "tokens": [4420, 13, 400, 550, 264, 958, 1168, 307, 11, 718, 311, 6552, 321, 393, 1322, 1270, 257, 2899, 13], "temperature": 0.0, "avg_logprob": -0.12329847162420099, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0020234144758433104}, {"id": 127, "seek": 94484, "start": 953.96, "end": 959.0, "text": " I will show you later that we actually cannot. But even if we can build such a technology,", "tokens": [286, 486, 855, 291, 1780, 300, 321, 767, 2644, 13, 583, 754, 498, 321, 393, 1322, 1270, 257, 2899, 11], "temperature": 0.0, "avg_logprob": -0.12329847162420099, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0020234144758433104}, {"id": 128, "seek": 94484, "start": 959.72, "end": 966.76, "text": " let's think about corner cases and understand who can be harm-based technology. So basically,", "tokens": [718, 311, 519, 466, 4538, 3331, 293, 1223, 567, 393, 312, 6491, 12, 6032, 2899, 13, 407, 1936, 11], "temperature": 0.0, "avg_logprob": -0.12329847162420099, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0020234144758433104}, {"id": 129, "seek": 94484, "start": 966.76, "end": 974.12, "text": " what is the potential for dual use? How these technology can be misused? So assume that the", "tokens": [437, 307, 264, 3995, 337, 11848, 764, 30, 1012, 613, 2899, 393, 312, 3346, 4717, 30, 407, 6552, 300, 264], "temperature": 0.0, "avg_logprob": -0.12329847162420099, "compression_ratio": 1.6744186046511629, "no_speech_prob": 0.0020234144758433104}, {"id": 130, "seek": 97412, "start": 974.12, "end": 980.04, "text": " classifier is 100% accurate now for a second. And please think about it and type,", "tokens": [1508, 9902, 307, 2319, 4, 8559, 586, 337, 257, 1150, 13, 400, 1767, 519, 466, 309, 293, 2010, 11], "temperature": 0.0, "avg_logprob": -0.20098771854322783, "compression_ratio": 1.4094488188976377, "no_speech_prob": 0.0006541367038153112}, {"id": 131, "seek": 97412, "start": 980.92, "end": 990.04, "text": " what do you think who can be harm from such a classifier? And how this classifier can be misused?", "tokens": [437, 360, 291, 519, 567, 393, 312, 6491, 490, 1270, 257, 1508, 9902, 30, 400, 577, 341, 1508, 9902, 393, 312, 3346, 4717, 30], "temperature": 0.0, "avg_logprob": -0.20098771854322783, "compression_ratio": 1.4094488188976377, "no_speech_prob": 0.0006541367038153112}, {"id": 132, "seek": 99004, "start": 990.04, "end": 1008.1999999999999, "text": " Right, so I can see answers. And I wish we can have this interactive, but I can try to summarize", "tokens": [1779, 11, 370, 286, 393, 536, 6338, 13, 400, 286, 3172, 321, 393, 362, 341, 15141, 11, 457, 286, 393, 853, 281, 20858], "temperature": 0.0, "avg_logprob": -0.21733142711498118, "compression_ratio": 1.103448275862069, "no_speech_prob": 0.0006869198987260461}, {"id": 133, "seek": 100820, "start": 1008.2, "end": 1022.44, "text": " what I have read so far. So first of all, one of you wrote that IQ is, let me just answer my question", "tokens": [437, 286, 362, 1401, 370, 1400, 13, 407, 700, 295, 439, 11, 472, 295, 291, 4114, 300, 28921, 307, 11, 718, 385, 445, 1867, 452, 1168], "temperature": 0.0, "avg_logprob": -0.1422833743160718, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.00032288048532791436}, {"id": 134, "seek": 100820, "start": 1022.44, "end": 1028.76, "text": " because it's difficult to summarize that chat. The interactive feature is difficult. So", "tokens": [570, 309, 311, 2252, 281, 20858, 300, 5081, 13, 440, 15141, 4111, 307, 2252, 13, 407], "temperature": 0.0, "avg_logprob": -0.1422833743160718, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.00032288048532791436}, {"id": 135, "seek": 102876, "start": 1028.76, "end": 1037.72, "text": " I would think about it in this way. First of all, why would we want to build such a classifier?", "tokens": [286, 576, 519, 466, 309, 294, 341, 636, 13, 2386, 295, 439, 11, 983, 576, 321, 528, 281, 1322, 1270, 257, 1508, 9902, 30], "temperature": 0.0, "avg_logprob": -0.1124338852731805, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.0015517957508563995}, {"id": 136, "seek": 102876, "start": 1038.84, "end": 1046.68, "text": " So to build a classifier, to predict an IQ, companies, universities, they don't really need to", "tokens": [407, 281, 1322, 257, 1508, 9902, 11, 281, 6069, 364, 28921, 11, 3431, 11, 11779, 11, 436, 500, 380, 534, 643, 281], "temperature": 0.0, "avg_logprob": -0.1124338852731805, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.0015517957508563995}, {"id": 137, "seek": 102876, "start": 1046.68, "end": 1055.24, "text": " know your IQ. What they are trying to predict is your future success. The way you will succeed in a", "tokens": [458, 428, 28921, 13, 708, 436, 366, 1382, 281, 6069, 307, 428, 2027, 2245, 13, 440, 636, 291, 486, 7754, 294, 257], "temperature": 0.0, "avg_logprob": -0.1124338852731805, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.0015517957508563995}, {"id": 138, "seek": 105524, "start": 1055.24, "end": 1063.24, "text": " job or the way you will study at school. And then the question is, is IQ is the right proxy for", "tokens": [1691, 420, 264, 636, 291, 486, 2979, 412, 1395, 13, 400, 550, 264, 1168, 307, 11, 307, 28921, 307, 264, 558, 29690, 337], "temperature": 0.0, "avg_logprob": -0.09573853505800849, "compression_ratio": 1.68, "no_speech_prob": 0.0016315762186422944}, {"id": 139, "seek": 105524, "start": 1063.24, "end": 1072.68, "text": " future success? And the answer is no, IQ correlates with future success, but it's not necessarily the", "tokens": [2027, 2245, 30, 400, 264, 1867, 307, 572, 11, 28921, 13983, 1024, 365, 2027, 2245, 11, 457, 309, 311, 406, 4725, 264], "temperature": 0.0, "avg_logprob": -0.09573853505800849, "compression_ratio": 1.68, "no_speech_prob": 0.0016315762186422944}, {"id": 140, "seek": 105524, "start": 1072.68, "end": 1079.8, "text": " right proxy for future success. And then who are people who could be harmed? For example, people", "tokens": [558, 29690, 337, 2027, 2245, 13, 400, 550, 567, 366, 561, 567, 727, 312, 41478, 30, 1171, 1365, 11, 561], "temperature": 0.0, "avg_logprob": -0.09573853505800849, "compression_ratio": 1.68, "no_speech_prob": 0.0016315762186422944}, {"id": 141, "seek": 107980, "start": 1079.8, "end": 1087.8799999999999, "text": " who have lower IQ, but very hard working people. People who have lower IQ, but have good", "tokens": [567, 362, 3126, 28921, 11, 457, 588, 1152, 1364, 561, 13, 3432, 567, 362, 3126, 28921, 11, 457, 362, 665], "temperature": 0.0, "avg_logprob": -0.1548638827558877, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.0013734728563576937}, {"id": 142, "seek": 107980, "start": 1087.8799999999999, "end": 1098.28, "text": " soft skills. So assuming that, so first of all, the IQ is a proxy of future success is", "tokens": [2787, 3942, 13, 407, 11926, 300, 11, 370, 700, 295, 439, 11, 264, 28921, 307, 257, 29690, 295, 2027, 2245, 307], "temperature": 0.0, "avg_logprob": -0.1548638827558877, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.0013734728563576937}, {"id": 143, "seek": 107980, "start": 1098.28, "end": 1106.84, "text": " incorrect bias proxy. And this kind of problem of using a proxy to the actual label because we cannot", "tokens": [18424, 12577, 29690, 13, 400, 341, 733, 295, 1154, 295, 1228, 257, 29690, 281, 264, 3539, 7645, 570, 321, 2644], "temperature": 0.0, "avg_logprob": -0.1548638827558877, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.0013734728563576937}, {"id": 144, "seek": 110684, "start": 1106.84, "end": 1113.6399999999999, "text": " have the actual label, the future success. We cannot have this label is actually very common in", "tokens": [362, 264, 3539, 7645, 11, 264, 2027, 2245, 13, 492, 2644, 362, 341, 7645, 307, 767, 588, 2689, 294], "temperature": 0.0, "avg_logprob": -0.11187674051307771, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0018035235116258264}, {"id": 145, "seek": 110684, "start": 1113.6399999999999, "end": 1119.3999999999999, "text": " other types of predicting technology. If you think about parole decisions, technology that", "tokens": [661, 3467, 295, 32884, 2899, 13, 759, 291, 519, 466, 26783, 5327, 11, 2899, 300], "temperature": 0.0, "avg_logprob": -0.11187674051307771, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0018035235116258264}, {"id": 146, "seek": 110684, "start": 1119.3999999999999, "end": 1124.6799999999998, "text": " uses the decides on parole decisions, what they want to predict is whether the individual", "tokens": [4960, 264, 14898, 322, 26783, 5327, 11, 437, 436, 528, 281, 6069, 307, 1968, 264, 2609], "temperature": 0.0, "avg_logprob": -0.11187674051307771, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0018035235116258264}, {"id": 147, "seek": 110684, "start": 1124.6799999999998, "end": 1130.9199999999998, "text": " recommit the crime. But this is a label that is very hard to obtain. And this is why they might", "tokens": [2616, 270, 264, 7206, 13, 583, 341, 307, 257, 7645, 300, 307, 588, 1152, 281, 12701, 13, 400, 341, 307, 983, 436, 1062], "temperature": 0.0, "avg_logprob": -0.11187674051307771, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0018035235116258264}, {"id": 148, "seek": 113092, "start": 1130.92, "end": 1141.0, "text": " resort to another label, whether they whether this individual will be convicted of a crime again,", "tokens": [19606, 281, 1071, 7645, 11, 1968, 436, 1968, 341, 2609, 486, 312, 26942, 295, 257, 7206, 797, 11], "temperature": 0.0, "avg_logprob": -0.1783487026508038, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0013606732245534658}, {"id": 149, "seek": 113092, "start": 1141.0, "end": 1147.64, "text": " and build this technology to predict future conviction. But conviction of a crime is a biased proxy", "tokens": [293, 1322, 341, 2899, 281, 6069, 2027, 24837, 13, 583, 24837, 295, 257, 7206, 307, 257, 28035, 29690], "temperature": 0.0, "avg_logprob": -0.1783487026508038, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0013606732245534658}, {"id": 150, "seek": 113092, "start": 1147.64, "end": 1155.48, "text": " of where the actual objective that we want to have, the likelihood to make the crime. And this is", "tokens": [295, 689, 264, 3539, 10024, 300, 321, 528, 281, 362, 11, 264, 22119, 281, 652, 264, 7206, 13, 400, 341, 307], "temperature": 0.0, "avg_logprob": -0.1783487026508038, "compression_ratio": 1.6480446927374302, "no_speech_prob": 0.0013606732245534658}, {"id": 151, "seek": 115548, "start": 1155.48, "end": 1162.28, "text": " one example of a biased proxy, which does not allow us us to build the right application for the", "tokens": [472, 1365, 295, 257, 28035, 29690, 11, 597, 775, 406, 2089, 505, 505, 281, 1322, 264, 558, 3861, 337, 264], "temperature": 0.0, "avg_logprob": -0.20855161878797743, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.000936827389523387}, {"id": 152, "seek": 115548, "start": 1162.28, "end": 1171.4, "text": " goals that we have. So this is one problem. The second problem is IQ test itself. It is a biased", "tokens": [5493, 300, 321, 362, 13, 407, 341, 307, 472, 1154, 13, 440, 1150, 1154, 307, 28921, 1500, 2564, 13, 467, 307, 257, 28035], "temperature": 0.0, "avg_logprob": -0.20855161878797743, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.000936827389523387}, {"id": 153, "seek": 115548, "start": 1171.4, "end": 1182.84, "text": " test. So actually we cannot build a proc- and accurate classifier for future for the right IQ.", "tokens": [1500, 13, 407, 767, 321, 2644, 1322, 257, 9510, 12, 293, 8559, 1508, 9902, 337, 2027, 337, 264, 558, 28921, 13], "temperature": 0.0, "avg_logprob": -0.20855161878797743, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.000936827389523387}, {"id": 154, "seek": 118284, "start": 1182.84, "end": 1192.84, "text": " And also if we look at the data that we use, this data, picture photos or social media,", "tokens": [400, 611, 498, 321, 574, 412, 264, 1412, 300, 321, 764, 11, 341, 1412, 11, 3036, 5787, 420, 2093, 3021, 11], "temperature": 0.0, "avg_logprob": -0.14772141498068106, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0013008926762267947}, {"id": 155, "seek": 118284, "start": 1192.84, "end": 1199.48, "text": " this data is biased itself. So there are all kinds of biases because of which we cannot actually", "tokens": [341, 1412, 307, 28035, 2564, 13, 407, 456, 366, 439, 3685, 295, 32152, 570, 295, 597, 321, 2644, 767], "temperature": 0.0, "avg_logprob": -0.14772141498068106, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0013008926762267947}, {"id": 156, "seek": 118284, "start": 1199.48, "end": 1205.56, "text": " build the right model. And this is why this classifier will not be 100 percent accurate. But there", "tokens": [1322, 264, 558, 2316, 13, 400, 341, 307, 983, 341, 1508, 9902, 486, 406, 312, 2319, 3043, 8559, 13, 583, 456], "temperature": 0.0, "avg_logprob": -0.14772141498068106, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0013008926762267947}, {"id": 157, "seek": 120556, "start": 1205.56, "end": 1213.48, "text": " will be many individuals who can be harmed. And then there will be questions. For example,", "tokens": [486, 312, 867, 5346, 567, 393, 312, 41478, 13, 400, 550, 456, 486, 312, 1651, 13, 1171, 1365, 11], "temperature": 0.0, "avg_logprob": -0.18114111457072513, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.00046369730262085795}, {"id": 158, "seek": 120556, "start": 1214.6, "end": 1223.48, "text": " assume our classifier that we build is not actually accurate. But it has high accuracy. For example,", "tokens": [6552, 527, 1508, 9902, 300, 321, 1322, 307, 406, 767, 8559, 13, 583, 309, 575, 1090, 14170, 13, 1171, 1365, 11], "temperature": 0.0, "avg_logprob": -0.18114111457072513, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.00046369730262085795}, {"id": 159, "seek": 120556, "start": 1224.28, "end": 1234.04, "text": " 90 percent or 95 percent. And then I would ask you, is 95 percent a good accuracy or 99 percent a", "tokens": [4289, 3043, 420, 13420, 3043, 13, 400, 550, 286, 576, 1029, 291, 11, 307, 13420, 3043, 257, 665, 14170, 420, 11803, 3043, 257], "temperature": 0.0, "avg_logprob": -0.18114111457072513, "compression_ratio": 1.5879120879120878, "no_speech_prob": 0.00046369730262085795}, {"id": 160, "seek": 123404, "start": 1234.04, "end": 1244.92, "text": " good accuracy? And then the questions to think about is whether what would happen with misclassification?", "tokens": [665, 14170, 30, 400, 550, 264, 1651, 281, 519, 466, 307, 1968, 437, 576, 1051, 365, 3346, 11665, 3774, 30], "temperature": 0.0, "avg_logprob": -0.1299324596629423, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0012006424367427826}, {"id": 161, "seek": 123404, "start": 1245.56, "end": 1252.44, "text": " What would would be an impact on individual lives if the classifiers makes mistakes? And in this", "tokens": [708, 576, 576, 312, 364, 2712, 322, 2609, 2909, 498, 264, 1508, 23463, 1669, 8038, 30, 400, 294, 341], "temperature": 0.0, "avg_logprob": -0.1299324596629423, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0012006424367427826}, {"id": 162, "seek": 123404, "start": 1252.44, "end": 1261.48, "text": " case, the important point is that the cost of misclassification is very high. It has effect on", "tokens": [1389, 11, 264, 1021, 935, 307, 300, 264, 2063, 295, 3346, 11665, 3774, 307, 588, 1090, 13, 467, 575, 1802, 322], "temperature": 0.0, "avg_logprob": -0.1299324596629423, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0012006424367427826}, {"id": 163, "seek": 126148, "start": 1261.48, "end": 1268.04, "text": " people's life. So accuracy may be not the right evaluation measure for this classifier.", "tokens": [561, 311, 993, 13, 407, 14170, 815, 312, 406, 264, 558, 13344, 3481, 337, 341, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.11920887231826782, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.0001924654934555292}, {"id": 164, "seek": 126148, "start": 1269.4, "end": 1277.48, "text": " And another question that I could ask is, for example, this condition on this slide that we find", "tokens": [400, 1071, 1168, 300, 286, 727, 1029, 307, 11, 337, 1365, 11, 341, 4188, 322, 341, 4137, 300, 321, 915], "temperature": 0.0, "avg_logprob": -0.11920887231826782, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.0001924654934555292}, {"id": 165, "seek": 126148, "start": 1277.48, "end": 1284.68, "text": " out that white females have 99 percent accuracy. But people with blonde hair under age 25 have", "tokens": [484, 300, 2418, 21529, 362, 11803, 3043, 14170, 13, 583, 561, 365, 30043, 2578, 833, 3205, 3552, 362], "temperature": 0.0, "avg_logprob": -0.11920887231826782, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.0001924654934555292}, {"id": 166, "seek": 128468, "start": 1284.68, "end": 1300.6000000000001, "text": " only 60 percent accuracy. So what does it tell you about this classifier?", "tokens": [787, 4060, 3043, 14170, 13, 407, 437, 775, 309, 980, 291, 466, 341, 1508, 9902, 30], "temperature": 0.0, "avg_logprob": -0.2023390769958496, "compression_ratio": 1.2713178294573644, "no_speech_prob": 0.0003131167031824589}, {"id": 167, "seek": 128468, "start": 1304.1200000000001, "end": 1311.88, "text": " Right. So the data set itself is biased. This means basically that people with blonde hair", "tokens": [1779, 13, 407, 264, 1412, 992, 2564, 307, 28035, 13, 639, 1355, 1936, 300, 561, 365, 30043, 2578], "temperature": 0.0, "avg_logprob": -0.2023390769958496, "compression_ratio": 1.2713178294573644, "no_speech_prob": 0.0003131167031824589}, {"id": 168, "seek": 131188, "start": 1311.88, "end": 1319.24, "text": " under the age of 25 are underrepresented in your data set. So there are all kinds of questions", "tokens": [833, 264, 3205, 295, 3552, 366, 833, 38293, 294, 428, 1412, 992, 13, 407, 456, 366, 439, 3685, 295, 1651], "temperature": 0.0, "avg_logprob": -0.07554004480550577, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0011279438622295856}, {"id": 169, "seek": 131188, "start": 1319.24, "end": 1324.5200000000002, "text": " and all kinds of probing questions that you can ask about the classifier to understand, is this", "tokens": [293, 439, 3685, 295, 1239, 278, 1651, 300, 291, 393, 1029, 466, 264, 1508, 9902, 281, 1223, 11, 307, 341], "temperature": 0.0, "avg_logprob": -0.07554004480550577, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0011279438622295856}, {"id": 170, "seek": 131188, "start": 1324.5200000000002, "end": 1331.0800000000002, "text": " the right problem to solve? Who can be harmed? Am I optimizing towards the right objective?", "tokens": [264, 558, 1154, 281, 5039, 30, 2102, 393, 312, 41478, 30, 2012, 286, 40425, 3030, 264, 558, 10024, 30], "temperature": 0.0, "avg_logprob": -0.07554004480550577, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0011279438622295856}, {"id": 171, "seek": 131188, "start": 1331.0800000000002, "end": 1339.0, "text": " Is my data biased? And what is the cost of misclassification? How do I assess the potential for", "tokens": [1119, 452, 1412, 28035, 30, 400, 437, 307, 264, 2063, 295, 3346, 11665, 3774, 30, 1012, 360, 286, 5877, 264, 3995, 337], "temperature": 0.0, "avg_logprob": -0.07554004480550577, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0011279438622295856}, {"id": 172, "seek": 133900, "start": 1339.0, "end": 1344.76, "text": " dole use and how much harm this technology can bring in addition to how useful it can be?", "tokens": [360, 306, 764, 293, 577, 709, 6491, 341, 2899, 393, 1565, 294, 4500, 281, 577, 4420, 309, 393, 312, 30], "temperature": 0.0, "avg_logprob": -0.20214874186414353, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0005859215743839741}, {"id": 173, "seek": 133900, "start": 1346.44, "end": 1351.88, "text": " And the one last question, which is a hard one, I want to ask you who is responsible.", "tokens": [400, 264, 472, 1036, 1168, 11, 597, 307, 257, 1152, 472, 11, 286, 528, 281, 1029, 291, 567, 307, 6250, 13], "temperature": 0.0, "avg_logprob": -0.20214874186414353, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0005859215743839741}, {"id": 174, "seek": 133900, "start": 1352.52, "end": 1357.96, "text": " So I'm your manager in Google, you're working in a country and I ask you please build a", "tokens": [407, 286, 478, 428, 6598, 294, 3329, 11, 291, 434, 1364, 294, 257, 1941, 293, 286, 1029, 291, 1767, 1322, 257], "temperature": 0.0, "avg_logprob": -0.20214874186414353, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0005859215743839741}, {"id": 175, "seek": 133900, "start": 1357.96, "end": 1364.04, "text": " Q classifier. And you build a Q classifier and you publish a paper about the Q classifier.", "tokens": [1249, 1508, 9902, 13, 400, 291, 1322, 257, 1249, 1508, 9902, 293, 291, 11374, 257, 3035, 466, 264, 1249, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.20214874186414353, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.0005859215743839741}, {"id": 176, "seek": 136404, "start": 1364.04, "end": 1370.76, "text": " And this paper is publicized on media. So and then the question is who is responsible?", "tokens": [400, 341, 3035, 307, 1908, 1602, 322, 3021, 13, 407, 293, 550, 264, 1168, 307, 567, 307, 6250, 30], "temperature": 0.0, "avg_logprob": -0.15065124299791124, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0008873248589225113}, {"id": 177, "seek": 136404, "start": 1370.76, "end": 1376.2, "text": " Is it the researcher or developer? Is it the manager? Is it a reviewer who didn't catch the", "tokens": [1119, 309, 264, 21751, 420, 10754, 30, 1119, 309, 264, 6598, 30, 1119, 309, 257, 3131, 260, 567, 994, 380, 3745, 264], "temperature": 0.0, "avg_logprob": -0.15065124299791124, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0008873248589225113}, {"id": 178, "seek": 136404, "start": 1376.2, "end": 1382.04, "text": " problems with IQ classifier? Is it university or company? Or is it society?", "tokens": [2740, 365, 28921, 1508, 9902, 30, 1119, 309, 5454, 420, 2237, 30, 1610, 307, 309, 4086, 30], "temperature": 0.0, "avg_logprob": -0.15065124299791124, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0008873248589225113}, {"id": 179, "seek": 136404, "start": 1387.1599999999999, "end": 1393.0, "text": " Yeah, so there is one nice answer that I want to read is that all of us should be responsible.", "tokens": [865, 11, 370, 456, 307, 472, 1481, 1867, 300, 286, 528, 281, 1401, 307, 300, 439, 295, 505, 820, 312, 6250, 13], "temperature": 0.0, "avg_logprob": -0.15065124299791124, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.0008873248589225113}, {"id": 180, "seek": 139300, "start": 1393.0, "end": 1399.88, "text": " So in practice, there is very little awareness about understanding what problems are ethical or not.", "tokens": [407, 294, 3124, 11, 456, 307, 588, 707, 8888, 466, 3701, 437, 2740, 366, 18890, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.17544293705421157, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.0013206942239776254}, {"id": 181, "seek": 139300, "start": 1399.88, "end": 1407.4, "text": " And there is no clear policies here. This is a complicated issue and it's not clear who is responsible.", "tokens": [400, 456, 307, 572, 1850, 7657, 510, 13, 639, 307, 257, 6179, 2734, 293, 309, 311, 406, 1850, 567, 307, 6250, 13], "temperature": 0.0, "avg_logprob": -0.17544293705421157, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.0013206942239776254}, {"id": 182, "seek": 139300, "start": 1407.4, "end": 1413.96, "text": " This is why assuming that whoever is aware of such dangers should be responsible.", "tokens": [639, 307, 983, 11926, 300, 11387, 307, 3650, 295, 1270, 27701, 820, 312, 6250, 13], "temperature": 0.0, "avg_logprob": -0.17544293705421157, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.0013206942239776254}, {"id": 183, "seek": 139300, "start": 1413.96, "end": 1417.72, "text": " So I don't know what is the right answer to this question.", "tokens": [407, 286, 500, 380, 458, 437, 307, 264, 558, 1867, 281, 341, 1168, 13], "temperature": 0.0, "avg_logprob": -0.17544293705421157, "compression_ratio": 1.5972222222222223, "no_speech_prob": 0.0013206942239776254}, {"id": 184, "seek": 141772, "start": 1417.72, "end": 1430.1200000000001, "text": " So now what is the difference between the chicken classifier and the Q classifier?", "tokens": [407, 586, 437, 307, 264, 2649, 1296, 264, 4662, 1508, 9902, 293, 264, 1249, 1508, 9902, 30], "temperature": 0.0, "avg_logprob": -0.22473129006319267, "compression_ratio": 1.375, "no_speech_prob": 0.000862836663145572}, {"id": 185, "seek": 141772, "start": 1435.0, "end": 1441.88, "text": " Right, so one of you answers is that one is affect people and one does not, right?", "tokens": [1779, 11, 370, 472, 295, 291, 6338, 307, 300, 472, 307, 3345, 561, 293, 472, 775, 406, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22473129006319267, "compression_ratio": 1.375, "no_speech_prob": 0.000862836663145572}, {"id": 186, "seek": 144188, "start": 1441.88, "end": 1448.2, "text": " And while chicken classifier actually affects chicken lives and IQ classifier will not kill anyone.", "tokens": [400, 1339, 4662, 1508, 9902, 767, 11807, 4662, 2909, 293, 28921, 1508, 9902, 486, 406, 1961, 2878, 13], "temperature": 0.0, "avg_logprob": -0.10721649662140877, "compression_ratio": 1.5649717514124293, "no_speech_prob": 0.0013542929664254189}, {"id": 187, "seek": 144188, "start": 1448.2, "end": 1457.5600000000002, "text": " It can harm but will not kill. We do feel that IQ classifier currently can have potentially", "tokens": [467, 393, 6491, 457, 486, 406, 1961, 13, 492, 360, 841, 300, 28921, 1508, 9902, 4362, 393, 362, 7263], "temperature": 0.0, "avg_logprob": -0.10721649662140877, "compression_ratio": 1.5649717514124293, "no_speech_prob": 0.0013542929664254189}, {"id": 188, "seek": 144188, "start": 1457.5600000000002, "end": 1465.72, "text": " worse impacts. So AI systems are pervasive in our world and the question about ethics", "tokens": [5324, 11606, 13, 407, 7318, 3652, 366, 680, 39211, 294, 527, 1002, 293, 264, 1168, 466, 19769], "temperature": 0.0, "avg_logprob": -0.10721649662140877, "compression_ratio": 1.5649717514124293, "no_speech_prob": 0.0013542929664254189}, {"id": 189, "seek": 146572, "start": 1465.72, "end": 1472.68, "text": " are specifically raised commonly about people centered AI systems. And these systems are really", "tokens": [366, 4682, 6005, 12719, 466, 561, 18988, 7318, 3652, 13, 400, 613, 3652, 366, 534], "temperature": 0.0, "avg_logprob": -0.14532684272443744, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.0006644095410592854}, {"id": 190, "seek": 146572, "start": 1472.68, "end": 1478.76, "text": " pervasive. So they interact with people like conversational agents. They reason about people", "tokens": [680, 39211, 13, 407, 436, 4648, 365, 561, 411, 2615, 1478, 12554, 13, 814, 1778, 466, 561], "temperature": 0.0, "avg_logprob": -0.14532684272443744, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.0006644095410592854}, {"id": 191, "seek": 146572, "start": 1479.48, "end": 1486.44, "text": " such as profiling applications or recommendation systems. They affect people in other lives", "tokens": [1270, 382, 1740, 4883, 5821, 420, 11879, 3652, 13, 814, 3345, 561, 294, 661, 2909], "temperature": 0.0, "avg_logprob": -0.14532684272443744, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.0006644095410592854}, {"id": 192, "seek": 146572, "start": 1486.44, "end": 1492.6000000000001, "text": " like parole decision applications that I mentioned, face recognition, voice recognition,", "tokens": [411, 26783, 3537, 5821, 300, 286, 2835, 11, 1851, 11150, 11, 3177, 11150, 11], "temperature": 0.0, "avg_logprob": -0.14532684272443744, "compression_ratio": 1.740566037735849, "no_speech_prob": 0.0006644095410592854}, {"id": 193, "seek": 149260, "start": 1492.6, "end": 1499.6399999999999, "text": " all of these actually have this component of predictive technology and the human center", "tokens": [439, 295, 613, 767, 362, 341, 6542, 295, 35521, 2899, 293, 264, 1952, 3056], "temperature": 0.0, "avg_logprob": -0.17967258393764496, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0015448285266757011}, {"id": 194, "seek": 149260, "start": 1499.6399999999999, "end": 1506.1999999999998, "text": " technology. And this is why ethics is critical here. So I want to move to a next study.", "tokens": [2899, 13, 400, 341, 307, 983, 19769, 307, 4924, 510, 13, 407, 286, 528, 281, 1286, 281, 257, 958, 2979, 13], "temperature": 0.0, "avg_logprob": -0.17967258393764496, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0015448285266757011}, {"id": 195, "seek": 149260, "start": 1507.32, "end": 1520.36, "text": " The next study is a study of detecting, so we again build a classifier and we want to identify", "tokens": [440, 958, 2979, 307, 257, 2979, 295, 40237, 11, 370, 321, 797, 1322, 257, 1508, 9902, 293, 321, 528, 281, 5876], "temperature": 0.0, "avg_logprob": -0.17967258393764496, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0015448285266757011}, {"id": 196, "seek": 152036, "start": 1520.36, "end": 1527.0, "text": " the ability to accurately identify one's sexual orientation from mere observation.", "tokens": [264, 3485, 281, 20095, 5876, 472, 311, 6701, 14764, 490, 8401, 14816, 13], "temperature": 0.0, "avg_logprob": -0.19188965691460502, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.0008020654204301536}, {"id": 197, "seek": 152036, "start": 1527.8, "end": 1536.6, "text": " So this study is called AI GADAR. So as I mentioned many similar studies, studies that predict", "tokens": [407, 341, 2979, 307, 1219, 7318, 460, 6112, 1899, 13, 407, 382, 286, 2835, 867, 2531, 5313, 11, 5313, 300, 6069], "temperature": 0.0, "avg_logprob": -0.19188965691460502, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.0008020654204301536}, {"id": 198, "seek": 152036, "start": 1536.6, "end": 1543.08, "text": " potential for terrorist attacks, studies that predict predictive policing.", "tokens": [3995, 337, 20342, 8122, 11, 5313, 300, 6069, 35521, 28799, 13], "temperature": 0.0, "avg_logprob": -0.19188965691460502, "compression_ratio": 1.583710407239819, "no_speech_prob": 0.0008020654204301536}, {"id": 199, "seek": 154308, "start": 1543.08, "end": 1549.32, "text": " And also if you heard about Cambridge Analytica, all of them incorporate very similar technology.", "tokens": [400, 611, 498, 291, 2198, 466, 24876, 23688, 2262, 11, 439, 295, 552, 16091, 588, 2531, 2899, 13], "temperature": 0.0, "avg_logprob": -0.11393463789527096, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0011318273609504104}, {"id": 200, "seek": 154308, "start": 1550.1999999999998, "end": 1559.1599999999999, "text": " So let's talk about AI GADAR study and the goal is to understand again what kind of questions", "tokens": [407, 718, 311, 751, 466, 7318, 460, 6112, 1899, 2979, 293, 264, 3387, 307, 281, 1223, 797, 437, 733, 295, 1651], "temperature": 0.0, "avg_logprob": -0.11393463789527096, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0011318273609504104}, {"id": 201, "seek": 154308, "start": 1559.1599999999999, "end": 1567.1599999999999, "text": " we could ask about this study and what kind of pitfalls we could prevent if we would ask these", "tokens": [321, 727, 1029, 466, 341, 2979, 293, 437, 733, 295, 10147, 18542, 321, 727, 4871, 498, 321, 576, 1029, 613], "temperature": 0.0, "avg_logprob": -0.11393463789527096, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.0011318273609504104}, {"id": 202, "seek": 156716, "start": 1567.16, "end": 1577.16, "text": " questions. So to summarize this study, the recent question is we need to identify the sexual", "tokens": [1651, 13, 407, 281, 20858, 341, 2979, 11, 264, 5162, 1168, 307, 321, 643, 281, 5876, 264, 6701], "temperature": 0.0, "avg_logprob": -0.13055050373077393, "compression_ratio": 1.4648648648648648, "no_speech_prob": 0.0007897596224211156}, {"id": 203, "seek": 156716, "start": 1577.16, "end": 1585.4, "text": " orientation from people's images. And the data collection process is that we can download photos", "tokens": [14764, 490, 561, 311, 5267, 13, 400, 264, 1412, 5765, 1399, 307, 300, 321, 393, 5484, 5787], "temperature": 0.0, "avg_logprob": -0.13055050373077393, "compression_ratio": 1.4648648648648648, "no_speech_prob": 0.0007897596224211156}, {"id": 204, "seek": 158540, "start": 1585.4, "end": 1598.2, "text": " from a popular American dating website. And there are 35,000 pictures, all white, equally", "tokens": [490, 257, 3743, 2665, 10689, 3144, 13, 400, 456, 366, 6976, 11, 1360, 5242, 11, 439, 2418, 11, 12309], "temperature": 0.0, "avg_logprob": -0.15865768370081168, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.00040855922270566225}, {"id": 205, "seek": 158540, "start": 1598.2, "end": 1603.96, "text": " represent equally representation for gay and straight, for male and female. Everybody is represented", "tokens": [2906, 12309, 10290, 337, 9049, 293, 2997, 11, 337, 7133, 293, 6556, 13, 7646, 307, 10379], "temperature": 0.0, "avg_logprob": -0.15865768370081168, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.00040855922270566225}, {"id": 206, "seek": 158540, "start": 1604.76, "end": 1614.1200000000001, "text": " evenly. The method that was used is a deep learning model to extract facial features and grooming", "tokens": [17658, 13, 440, 3170, 300, 390, 1143, 307, 257, 2452, 2539, 2316, 281, 8947, 15642, 4122, 293, 49700], "temperature": 0.0, "avg_logprob": -0.15865768370081168, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.00040855922270566225}, {"id": 207, "seek": 161412, "start": 1614.12, "end": 1620.76, "text": " features. And then a logistic regression classifier applied to classify the final label,", "tokens": [4122, 13, 400, 550, 257, 3565, 3142, 24590, 1508, 9902, 6456, 281, 33872, 264, 2572, 7645, 11], "temperature": 0.0, "avg_logprob": -0.11518322903177013, "compression_ratio": 1.510989010989011, "no_speech_prob": 0.0008692063856869936}, {"id": 208, "seek": 161412, "start": 1620.76, "end": 1628.52, "text": " gay or straight. And the accuracy of this classifier is 81% for men and 74% for women.", "tokens": [9049, 420, 2997, 13, 400, 264, 14170, 295, 341, 1508, 9902, 307, 30827, 4, 337, 1706, 293, 28868, 4, 337, 2266, 13], "temperature": 0.0, "avg_logprob": -0.11518322903177013, "compression_ratio": 1.510989010989011, "no_speech_prob": 0.0008692063856869936}, {"id": 209, "seek": 161412, "start": 1628.52, "end": 1635.9599999999998, "text": " So this is a summary of the study. And I see rightfully asked questions why would we ever need such", "tokens": [407, 341, 307, 257, 12691, 295, 264, 2979, 13, 400, 286, 536, 558, 2277, 2351, 1651, 983, 576, 321, 1562, 643, 1270], "temperature": 0.0, "avg_logprob": -0.11518322903177013, "compression_ratio": 1.510989010989011, "no_speech_prob": 0.0008692063856869936}, {"id": 210, "seek": 163596, "start": 1635.96, "end": 1644.76, "text": " a nice system? This is a good question. But I don't want to publish this study or disparage", "tokens": [257, 1481, 1185, 30, 639, 307, 257, 665, 1168, 13, 583, 286, 500, 380, 528, 281, 11374, 341, 2979, 420, 14548, 609], "temperature": 0.0, "avg_logprob": -0.11737594339582655, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.001698829117231071}, {"id": 211, "seek": 163596, "start": 1644.76, "end": 1650.68, "text": " specific researcher, but this is a good study to present as an example of what could go wrong", "tokens": [2685, 21751, 11, 457, 341, 307, 257, 665, 2979, 281, 1974, 382, 364, 1365, 295, 437, 727, 352, 2085], "temperature": 0.0, "avg_logprob": -0.11737594339582655, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.001698829117231071}, {"id": 212, "seek": 163596, "start": 1650.68, "end": 1658.44, "text": " at all levels of the study. So this is why I am discussing it now. So what went wrong here?", "tokens": [412, 439, 4358, 295, 264, 2979, 13, 407, 341, 307, 983, 286, 669, 10850, 309, 586, 13, 407, 437, 1437, 2085, 510, 30], "temperature": 0.0, "avg_logprob": -0.11737594339582655, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.001698829117231071}, {"id": 213, "seek": 165844, "start": 1658.44, "end": 1667.48, "text": " So let's start with an ethics of the research question. So is it ethical at all to predict", "tokens": [407, 718, 311, 722, 365, 364, 19769, 295, 264, 2132, 1168, 13, 407, 307, 309, 18890, 412, 439, 281, 6069], "temperature": 0.0, "avg_logprob": -0.1342582901318868, "compression_ratio": 1.4, "no_speech_prob": 0.0012989930110052228}, {"id": 214, "seek": 165844, "start": 1667.48, "end": 1677.8, "text": " sexual orientation from any kind of features? And I see a lot of comments and thank you for the comments.", "tokens": [6701, 14764, 490, 604, 733, 295, 4122, 30, 400, 286, 536, 257, 688, 295, 3053, 293, 1309, 291, 337, 264, 3053, 13], "temperature": 0.0, "avg_logprob": -0.1342582901318868, "compression_ratio": 1.4, "no_speech_prob": 0.0012989930110052228}, {"id": 215, "seek": 167780, "start": 1677.8, "end": 1688.68, "text": " So first of all, this is not a new research question. From 19th century there were multiple studies", "tokens": [407, 700, 295, 439, 11, 341, 307, 406, 257, 777, 2132, 1168, 13, 3358, 1294, 392, 4901, 456, 645, 3866, 5313], "temperature": 0.0, "avg_logprob": -0.11351892442414255, "compression_ratio": 1.5052083333333333, "no_speech_prob": 0.0012291277525946498}, {"id": 216, "seek": 167780, "start": 1688.68, "end": 1695.32, "text": " to correlate sexual identity with some external features. People were and then with genetics,", "tokens": [281, 48742, 6701, 6575, 365, 512, 8320, 4122, 13, 3432, 645, 293, 550, 365, 26516, 11], "temperature": 0.0, "avg_logprob": -0.11351892442414255, "compression_ratio": 1.5052083333333333, "no_speech_prob": 0.0012291277525946498}, {"id": 217, "seek": 167780, "start": 1695.32, "end": 1702.76, "text": " people were looking for gay genes, gay brains, gay ring fingers and so on. So maybe moving from", "tokens": [561, 645, 1237, 337, 9049, 14424, 11, 9049, 15442, 11, 9049, 4875, 7350, 293, 370, 322, 13, 407, 1310, 2684, 490], "temperature": 0.0, "avg_logprob": -0.11351892442414255, "compression_ratio": 1.5052083333333333, "no_speech_prob": 0.0012291277525946498}, {"id": 218, "seek": 170276, "start": 1702.76, "end": 1715.32, "text": " 19th century to 21 century, we can again ask who can benefit from such a classifier and who", "tokens": [1294, 392, 4901, 281, 5080, 4901, 11, 321, 393, 797, 1029, 567, 393, 5121, 490, 1270, 257, 1508, 9902, 293, 567], "temperature": 0.0, "avg_logprob": -0.21095321655273438, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.001282157958485186}, {"id": 219, "seek": 170276, "start": 1715.32, "end": 1722.28, "text": " can be harmed by such a classifier. So what do I think? Who benefit from such a classifier?", "tokens": [393, 312, 41478, 538, 1270, 257, 1508, 9902, 13, 407, 437, 360, 286, 519, 30, 2102, 5121, 490, 1270, 257, 1508, 9902, 30], "temperature": 0.0, "avg_logprob": -0.21095321655273438, "compression_ratio": 1.5508474576271187, "no_speech_prob": 0.001282157958485186}, {"id": 220, "seek": 172228, "start": 1722.28, "end": 1740.2, "text": " So autocratic governments, right? But also maybe dating apps, advertisers, conservative", "tokens": [407, 1476, 38883, 11280, 11, 558, 30, 583, 611, 1310, 10689, 7733, 11, 42679, 11, 13780], "temperature": 0.0, "avg_logprob": -0.24608973094395228, "compression_ratio": 1.3082706766917294, "no_speech_prob": 0.0003686177369672805}, {"id": 221, "seek": 172228, "start": 1740.2, "end": 1746.6, "text": " religious groups and so on. So we could think who would want to use such a classifier?", "tokens": [7185, 3935, 293, 370, 322, 13, 407, 321, 727, 519, 567, 576, 528, 281, 764, 1270, 257, 1508, 9902, 30], "temperature": 0.0, "avg_logprob": -0.24608973094395228, "compression_ratio": 1.3082706766917294, "no_speech_prob": 0.0003686177369672805}, {"id": 222, "seek": 174660, "start": 1746.6, "end": 1759.56, "text": " Then maybe who can be harmed by such a classifier? Now again, assuming we are not thinking if it's", "tokens": [1396, 1310, 567, 393, 312, 41478, 538, 1270, 257, 1508, 9902, 30, 823, 797, 11, 11926, 321, 366, 406, 1953, 498, 309, 311], "temperature": 0.0, "avg_logprob": -0.14621192461823765, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.00018522178288549185}, {"id": 223, "seek": 174660, "start": 1759.56, "end": 1765.0, "text": " possible at all builds such a classifier and as you can guess we will see that it's not possible.", "tokens": [1944, 412, 439, 15182, 1270, 257, 1508, 9902, 293, 382, 291, 393, 2041, 321, 486, 536, 300, 309, 311, 406, 1944, 13], "temperature": 0.0, "avg_logprob": -0.14621192461823765, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.00018522178288549185}, {"id": 224, "seek": 174660, "start": 1765.0, "end": 1773.1599999999999, "text": " But what would stop you from building such a classifier? What do you think could be harmful in", "tokens": [583, 437, 576, 1590, 291, 490, 2390, 1270, 257, 1508, 9902, 30, 708, 360, 291, 519, 727, 312, 19727, 294], "temperature": 0.0, "avg_logprob": -0.14621192461823765, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.00018522178288549185}, {"id": 225, "seek": 177316, "start": 1773.16, "end": 1789.8000000000002, "text": " this classifier? So yeah, thank you for your answers. I will summarize them. So", "tokens": [341, 1508, 9902, 30, 407, 1338, 11, 1309, 291, 337, 428, 6338, 13, 286, 486, 20858, 552, 13, 407], "temperature": 0.0, "avg_logprob": -0.14423842863603073, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.0002558559353929013}, {"id": 226, "seek": 177316, "start": 1793.5600000000002, "end": 1799.5600000000002, "text": " many people can be harmed by such a classifier and I summarized basically many of your answers", "tokens": [867, 561, 393, 312, 41478, 538, 1270, 257, 1508, 9902, 293, 286, 14611, 1602, 1936, 867, 295, 428, 6338], "temperature": 0.0, "avg_logprob": -0.14423842863603073, "compression_ratio": 1.403225806451613, "no_speech_prob": 0.0002558559353929013}, {"id": 227, "seek": 179956, "start": 1799.56, "end": 1805.96, "text": " here in this slide. So this potentially can be a dangerous technology. So in many countries", "tokens": [510, 294, 341, 4137, 13, 407, 341, 7263, 393, 312, 257, 5795, 2899, 13, 407, 294, 867, 3517], "temperature": 0.0, "avg_logprob": -0.154395883733576, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.0020016501657664776}, {"id": 228, "seek": 179956, "start": 1805.96, "end": 1814.04, "text": " being gay person is prosecutable by law and it can lead even to despenalty. It might affect", "tokens": [885, 9049, 954, 307, 22382, 32148, 538, 2101, 293, 309, 393, 1477, 754, 281, 730, 5200, 304, 874, 13, 467, 1062, 3345], "temperature": 0.0, "avg_logprob": -0.154395883733576, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.0020016501657664776}, {"id": 229, "seek": 179956, "start": 1814.04, "end": 1823.1599999999999, "text": " people's employment, relationships, health opportunities, right? Importantly, this is not only", "tokens": [561, 311, 11949, 11, 6159, 11, 1585, 4786, 11, 558, 30, 26391, 3627, 11, 341, 307, 406, 787], "temperature": 0.0, "avg_logprob": -0.154395883733576, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.0020016501657664776}, {"id": 230, "seek": 182316, "start": 1823.16, "end": 1831.64, "text": " about sexual orientation. So there are many attributes including sexual identity that are private", "tokens": [466, 6701, 14764, 13, 407, 456, 366, 867, 17212, 3009, 6701, 6575, 300, 366, 4551], "temperature": 0.0, "avg_logprob": -0.15381203026607118, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0007010861882008612}, {"id": 231, "seek": 182316, "start": 1832.8400000000001, "end": 1839.88, "text": " for people, right? They are protected attributes and they can be non-binary, they can be", "tokens": [337, 561, 11, 558, 30, 814, 366, 10594, 17212, 293, 436, 393, 312, 2107, 12, 48621, 11, 436, 393, 312], "temperature": 0.0, "avg_logprob": -0.15381203026607118, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0007010861882008612}, {"id": 232, "seek": 182316, "start": 1839.88, "end": 1848.52, "text": " intimate and not visible publicly. And most importantly is that these attributes are specifically", "tokens": [20215, 293, 406, 8974, 14843, 13, 400, 881, 8906, 307, 300, 613, 17212, 366, 4682], "temperature": 0.0, "avg_logprob": -0.15381203026607118, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.0007010861882008612}, {"id": 233, "seek": 184852, "start": 1848.52, "end": 1854.52, "text": " those attributes against which people are discriminated against. And this is why it is", "tokens": [729, 17212, 1970, 597, 561, 366, 20828, 770, 1970, 13, 400, 341, 307, 983, 309, 307], "temperature": 0.0, "avg_logprob": -0.18104586283365887, "compression_ratio": 1.6528497409326426, "no_speech_prob": 0.0005272346897982061}, {"id": 234, "seek": 184852, "start": 1854.52, "end": 1861.0, "text": " basically dangerous to build such a technology. So in the paper, in the published paper,", "tokens": [1936, 5795, 281, 1322, 1270, 257, 2899, 13, 407, 294, 264, 3035, 11, 294, 264, 6572, 3035, 11], "temperature": 0.0, "avg_logprob": -0.18104586283365887, "compression_ratio": 1.6528497409326426, "no_speech_prob": 0.0005272346897982061}, {"id": 235, "seek": 184852, "start": 1861.6399999999999, "end": 1869.4, "text": " the argument for building this, for it's called presenting this study was that", "tokens": [264, 6770, 337, 2390, 341, 11, 337, 309, 311, 1219, 15578, 341, 2979, 390, 300], "temperature": 0.0, "avg_logprob": -0.18104586283365887, "compression_ratio": 1.6528497409326426, "no_speech_prob": 0.0005272346897982061}, {"id": 236, "seek": 184852, "start": 1871.72, "end": 1875.96, "text": " the study isn't alert how easy it is to build such a classifier.", "tokens": [264, 2979, 1943, 380, 9615, 577, 1858, 309, 307, 281, 1322, 1270, 257, 1508, 9902, 13], "temperature": 0.0, "avg_logprob": -0.18104586283365887, "compression_ratio": 1.6528497409326426, "no_speech_prob": 0.0005272346897982061}, {"id": 237, "seek": 187596, "start": 1875.96, "end": 1887.08, "text": " And basically, it is alert for to expose their thread to the privacy and safety of people.", "tokens": [400, 1936, 11, 309, 307, 9615, 337, 281, 19219, 641, 7207, 281, 264, 11427, 293, 4514, 295, 561, 13], "temperature": 0.0, "avg_logprob": -0.35383719932742236, "compression_ratio": 1.3565891472868217, "no_speech_prob": 0.0009880855213850737}, {"id": 238, "seek": 188708, "start": 1887.08, "end": 1909.8799999999999, "text": " And then I would be interested to hear if you have counter arguments. So basically,", "tokens": [400, 550, 286, 576, 312, 3102, 281, 1568, 498, 291, 362, 5682, 12869, 13, 407, 1936, 11], "temperature": 0.0, "avg_logprob": -0.2401363736107236, "compression_ratio": 1.0246913580246915, "no_speech_prob": 0.00033121128217317164}, {"id": 239, "seek": 190988, "start": 1909.88, "end": 1917.8000000000002, "text": " there can be many counter arguments. One of them is that this is a classifier, this technology.", "tokens": [456, 393, 312, 867, 5682, 12869, 13, 1485, 295, 552, 307, 300, 341, 307, 257, 1508, 9902, 11, 341, 2899, 13], "temperature": 0.0, "avg_logprob": -0.13145249465416217, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.0009866298642009497}, {"id": 240, "seek": 190988, "start": 1917.8000000000002, "end": 1923.0800000000002, "text": " So like a knife is a technology and with a knife you can kill people and you can", "tokens": [407, 411, 257, 7976, 307, 257, 2899, 293, 365, 257, 7976, 291, 393, 1961, 561, 293, 291, 393], "temperature": 0.0, "avg_logprob": -0.13145249465416217, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.0009866298642009497}, {"id": 241, "seek": 190988, "start": 1923.96, "end": 1930.3600000000001, "text": " cook food, right? And you don't, you're not necessarily need to kill people with a knife to", "tokens": [2543, 1755, 11, 558, 30, 400, 291, 500, 380, 11, 291, 434, 406, 4725, 643, 281, 1961, 561, 365, 257, 7976, 281], "temperature": 0.0, "avg_logprob": -0.13145249465416217, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.0009866298642009497}, {"id": 242, "seek": 190988, "start": 1930.3600000000001, "end": 1937.8000000000002, "text": " expose the dangers and harms of this technology, right? And another issue is that", "tokens": [19219, 264, 27701, 293, 48505, 295, 341, 2899, 11, 558, 30, 400, 1071, 2734, 307, 300], "temperature": 0.0, "avg_logprob": -0.13145249465416217, "compression_ratio": 1.7412935323383085, "no_speech_prob": 0.0009866298642009497}, {"id": 243, "seek": 193780, "start": 1937.8, "end": 1945.32, "text": " this is actually not possible to build such a classifier with the data that researchers had.", "tokens": [341, 307, 767, 406, 1944, 281, 1322, 1270, 257, 1508, 9902, 365, 264, 1412, 300, 10309, 632, 13], "temperature": 0.0, "avg_logprob": -0.11000968515872955, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0011158662382513285}, {"id": 244, "seek": 193780, "start": 1945.32, "end": 1952.36, "text": " And this is what we will see when we will discuss additional details about the data.", "tokens": [400, 341, 307, 437, 321, 486, 536, 562, 321, 486, 2248, 4497, 4365, 466, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11000968515872955, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0011158662382513285}, {"id": 245, "seek": 193780, "start": 1953.48, "end": 1959.1599999999999, "text": " And another comment is, as I said, this is only one instance of such a technology. Here is", "tokens": [400, 1071, 2871, 307, 11, 382, 286, 848, 11, 341, 307, 787, 472, 5197, 295, 1270, 257, 2899, 13, 1692, 307], "temperature": 0.0, "avg_logprob": -0.11000968515872955, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0011158662382513285}, {"id": 246, "seek": 195916, "start": 1959.16, "end": 1968.52, "text": " another instance, which is a successful startup called Faceception, which has drawn a lot of", "tokens": [1071, 5197, 11, 597, 307, 257, 4406, 18578, 1219, 4047, 7311, 11, 597, 575, 10117, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.15161269710909936, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.0008023020927794278}, {"id": 247, "seek": 195916, "start": 1968.52, "end": 1975.8000000000002, "text": " funding and its goal is to identify terrorists based on facial features. And unlike in the", "tokens": [6137, 293, 1080, 3387, 307, 281, 5876, 28330, 2361, 322, 15642, 4122, 13, 400, 8343, 294, 264], "temperature": 0.0, "avg_logprob": -0.15161269710909936, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.0008023020927794278}, {"id": 248, "seek": 195916, "start": 1975.8000000000002, "end": 1983.16, "text": " previous study, the startup doesn't show how the bot technology they developed, but you can guess", "tokens": [3894, 2979, 11, 264, 18578, 1177, 380, 855, 577, 264, 10592, 2899, 436, 4743, 11, 457, 291, 393, 2041], "temperature": 0.0, "avg_logprob": -0.15161269710909936, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.0008023020927794278}, {"id": 249, "seek": 198316, "start": 1983.16, "end": 1991.5600000000002, "text": " that it can have similar dangers. So in general, building predictive technology is very pervasive,", "tokens": [300, 309, 393, 362, 2531, 27701, 13, 407, 294, 2674, 11, 2390, 35521, 2899, 307, 588, 680, 39211, 11], "temperature": 0.0, "avg_logprob": -0.1919141622690054, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.0008953950018621981}, {"id": 250, "seek": 198316, "start": 1991.5600000000002, "end": 2001.0800000000002, "text": " as ubiquitous, but it is always, and sometimes it's not as kind of clear cut on ethical. For example,", "tokens": [382, 43868, 39831, 11, 457, 309, 307, 1009, 11, 293, 2171, 309, 311, 406, 382, 733, 295, 1850, 1723, 322, 18890, 13, 1171, 1365, 11], "temperature": 0.0, "avg_logprob": -0.1919141622690054, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.0008953950018621981}, {"id": 251, "seek": 198316, "start": 2003.96, "end": 2008.68, "text": " many people in NLP published papers on predicting gender from comments.", "tokens": [867, 561, 294, 426, 45196, 6572, 10577, 322, 32884, 7898, 490, 3053, 13], "temperature": 0.0, "avg_logprob": -0.1919141622690054, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.0008953950018621981}, {"id": 252, "seek": 200868, "start": 2008.68, "end": 2018.44, "text": " And it is not clear basically when the technology is clearly harmful and unethical and where it can", "tokens": [400, 309, 307, 406, 1850, 1936, 562, 264, 2899, 307, 4448, 19727, 293, 517, 3293, 804, 293, 689, 309, 393], "temperature": 0.0, "avg_logprob": -0.13592676920433566, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.0018905176548287272}, {"id": 253, "seek": 200868, "start": 2018.44, "end": 2028.44, "text": " be actually used in good ways. For example, we all want our search to work, right? And to work well,", "tokens": [312, 767, 1143, 294, 665, 2098, 13, 1171, 1365, 11, 321, 439, 528, 527, 3164, 281, 589, 11, 558, 30, 400, 281, 589, 731, 11], "temperature": 0.0, "avg_logprob": -0.13592676920433566, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.0018905176548287272}, {"id": 254, "seek": 200868, "start": 2028.44, "end": 2035.3200000000002, "text": " and to be personalized, the algorithm actually needs to know something about us. So again, this is", "tokens": [293, 281, 312, 28415, 11, 264, 9284, 767, 2203, 281, 458, 746, 466, 505, 13, 407, 797, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.13592676920433566, "compression_ratio": 1.517766497461929, "no_speech_prob": 0.0018905176548287272}, {"id": 255, "seek": 203532, "start": 2035.32, "end": 2043.48, "text": " not an easy question, but in the case of the dark classifier, maybe it's already on the extreme.", "tokens": [406, 364, 1858, 1168, 11, 457, 294, 264, 1389, 295, 264, 2877, 1508, 9902, 11, 1310, 309, 311, 1217, 322, 264, 8084, 13], "temperature": 0.0, "avg_logprob": -0.16323245896233451, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.0012315497733652592}, {"id": 256, "seek": 203532, "start": 2043.48, "end": 2055.24, "text": " Okay, let's move to the data. Again, to discuss basically what questions could we ask about the data?", "tokens": [1033, 11, 718, 311, 1286, 281, 264, 1412, 13, 3764, 11, 281, 2248, 1936, 437, 1651, 727, 321, 1029, 466, 264, 1412, 30], "temperature": 0.0, "avg_logprob": -0.16323245896233451, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.0012315497733652592}, {"id": 257, "seek": 203532, "start": 2055.24, "end": 2060.36, "text": " So here's how the data was collected. So photos were downloaded from a popular American dating", "tokens": [407, 510, 311, 577, 264, 1412, 390, 11087, 13, 407, 5787, 645, 21748, 490, 257, 3743, 2665, 10689], "temperature": 0.0, "avg_logprob": -0.16323245896233451, "compression_ratio": 1.5025641025641026, "no_speech_prob": 0.0012315497733652592}, {"id": 258, "seek": 206036, "start": 2060.36, "end": 2070.76, "text": " website. They were public. And there were a few thousands of images all white and the balance data", "tokens": [3144, 13, 814, 645, 1908, 13, 400, 456, 645, 257, 1326, 5383, 295, 5267, 439, 2418, 293, 264, 4772, 1412], "temperature": 0.0, "avg_logprob": -0.23443534639146593, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.001158610568381846}, {"id": 259, "seek": 206036, "start": 2070.76, "end": 2080.84, "text": " set. And my first question is what can you say about the data? So is it okay to use this data if", "tokens": [992, 13, 400, 452, 700, 1168, 307, 437, 393, 291, 584, 466, 264, 1412, 30, 407, 307, 309, 1392, 281, 764, 341, 1412, 498], "temperature": 0.0, "avg_logprob": -0.23443534639146593, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.001158610568381846}, {"id": 260, "seek": 206036, "start": 2080.84, "end": 2087.56, "text": " there is no robots to exceed the photos are public? What can be counter arguments to using people's", "tokens": [456, 307, 572, 14733, 281, 14048, 264, 5787, 366, 1908, 30, 708, 393, 312, 5682, 12869, 281, 1228, 561, 311], "temperature": 0.0, "avg_logprob": -0.23443534639146593, "compression_ratio": 1.5691489361702127, "no_speech_prob": 0.001158610568381846}, {"id": 261, "seek": 208756, "start": 2087.56, "end": 2090.44, "text": " photos from a dating website?", "tokens": [5787, 490, 257, 10689, 3144, 30], "temperature": 0.0, "avg_logprob": -0.2627760410308838, "compression_ratio": 0.9076923076923077, "no_speech_prob": 0.0011417438508942723}, {"id": 262, "seek": 208756, "start": 2103.72, "end": 2107.48, "text": " There is a hint on the slide.", "tokens": [821, 307, 257, 12075, 322, 264, 4137, 13], "temperature": 0.0, "avg_logprob": -0.2627760410308838, "compression_ratio": 0.9076923076923077, "no_speech_prob": 0.0011417438508942723}, {"id": 263, "seek": 210748, "start": 2107.48, "end": 2119.08, "text": " A lack of consent. People did not intend for their photos to be used to build a classifier,", "tokens": [316, 5011, 295, 14546, 13, 3432, 630, 406, 19759, 337, 641, 5787, 281, 312, 1143, 281, 1322, 257, 1508, 9902, 11], "temperature": 0.0, "avg_logprob": -0.17052712705400255, "compression_ratio": 1.4923076923076923, "no_speech_prob": 0.0014818067429587245}, {"id": 264, "seek": 210748, "start": 2119.96, "end": 2125.72, "text": " private information, right? So thank you for your answers. So the points that I wanted to emphasize", "tokens": [4551, 1589, 11, 558, 30, 407, 1309, 291, 337, 428, 6338, 13, 407, 264, 2793, 300, 286, 1415, 281, 16078], "temperature": 0.0, "avg_logprob": -0.17052712705400255, "compression_ratio": 1.4923076923076923, "no_speech_prob": 0.0014818067429587245}, {"id": 265, "seek": 210748, "start": 2125.72, "end": 2133.0, "text": " here is that it was legal to collect this data. But again, it's not clear whether it was ethical to", "tokens": [510, 307, 300, 309, 390, 5089, 281, 2500, 341, 1412, 13, 583, 797, 11, 309, 311, 406, 1850, 1968, 309, 390, 18890, 281], "temperature": 0.0, "avg_logprob": -0.17052712705400255, "compression_ratio": 1.4923076923076923, "no_speech_prob": 0.0014818067429587245}, {"id": 266, "seek": 213300, "start": 2133.0, "end": 2140.36, "text": " collect this data because as you said, people did not provide 35,000 people did not provide consent", "tokens": [2500, 341, 1412, 570, 382, 291, 848, 11, 561, 630, 406, 2893, 6976, 11, 1360, 561, 630, 406, 2893, 14546], "temperature": 0.0, "avg_logprob": -0.11332594115158608, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0015665070386603475}, {"id": 267, "seek": 213300, "start": 2140.36, "end": 2147.16, "text": " for using specifically this data. And there are more important and global issue here is that there", "tokens": [337, 1228, 4682, 341, 1412, 13, 400, 456, 366, 544, 1021, 293, 4338, 2734, 510, 307, 300, 456], "temperature": 0.0, "avg_logprob": -0.11332594115158608, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0015665070386603475}, {"id": 268, "seek": 213300, "start": 2147.16, "end": 2154.92, "text": " is a difference between data that is public and data, which is public size. So public, it's fine", "tokens": [307, 257, 2649, 1296, 1412, 300, 307, 1908, 293, 1412, 11, 597, 307, 1908, 2744, 13, 407, 1908, 11, 309, 311, 2489], "temperature": 0.0, "avg_logprob": -0.11332594115158608, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0015665070386603475}, {"id": 269, "seek": 213300, "start": 2154.92, "end": 2161.0, "text": " because these people want to be found by the social circle that they are targeting when they", "tokens": [570, 613, 561, 528, 281, 312, 1352, 538, 264, 2093, 6329, 300, 436, 366, 17918, 562, 436], "temperature": 0.0, "avg_logprob": -0.11332594115158608, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0015665070386603475}, {"id": 270, "seek": 216100, "start": 2161.0, "end": 2167.24, "text": " publish their photo on the dating website. But this does not mean necessarily that they want to be", "tokens": [11374, 641, 5052, 322, 264, 10689, 3144, 13, 583, 341, 775, 406, 914, 4725, 300, 436, 528, 281, 312], "temperature": 0.0, "avg_logprob": -0.12968759394403714, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.0009424928575754166}, {"id": 271, "seek": 216100, "start": 2167.24, "end": 2175.24, "text": " found by broader social circle, by their families, by their colleagues and so on. So there is a", "tokens": [1352, 538, 13227, 2093, 6329, 11, 538, 641, 4466, 11, 538, 641, 7734, 293, 370, 322, 13, 407, 456, 307, 257], "temperature": 0.0, "avg_logprob": -0.12968759394403714, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.0009424928575754166}, {"id": 272, "seek": 216100, "start": 2175.24, "end": 2183.24, "text": " big difference between the data that is public and the data which is public size. So overall,", "tokens": [955, 2649, 1296, 264, 1412, 300, 307, 1908, 293, 264, 1412, 597, 307, 1908, 2744, 13, 407, 4787, 11], "temperature": 0.0, "avg_logprob": -0.12968759394403714, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.0009424928575754166}, {"id": 273, "seek": 218324, "start": 2183.24, "end": 2190.9199999999996, "text": " even if they did not violate state terms of service, I don't know about it. I didn't read", "tokens": [754, 498, 436, 630, 406, 37478, 1785, 2115, 295, 2643, 11, 286, 500, 380, 458, 466, 309, 13, 286, 994, 380, 1401], "temperature": 0.0, "avg_logprob": -0.10747657695286711, "compression_ratio": 1.5921787709497206, "no_speech_prob": 0.0008388716960325837}, {"id": 274, "seek": 218324, "start": 2191.56, "end": 2199.56, "text": " in depth actually, but they did violate the social contract because this was not the intent of", "tokens": [294, 7161, 767, 11, 457, 436, 630, 37478, 264, 2093, 4364, 570, 341, 390, 406, 264, 8446, 295], "temperature": 0.0, "avg_logprob": -0.10747657695286711, "compression_ratio": 1.5921787709497206, "no_speech_prob": 0.0008388716960325837}, {"id": 275, "seek": 218324, "start": 2199.56, "end": 2210.3599999999997, "text": " the user for their data to be used in this way. Next question about data. So what do you think about", "tokens": [264, 4195, 337, 641, 1412, 281, 312, 1143, 294, 341, 636, 13, 3087, 1168, 466, 1412, 13, 407, 437, 360, 291, 519, 466], "temperature": 0.0, "avg_logprob": -0.10747657695286711, "compression_ratio": 1.5921787709497206, "no_speech_prob": 0.0008388716960325837}, {"id": 276, "seek": 221036, "start": 2210.36, "end": 2218.6800000000003, "text": " this data set? 35,000 of pictures, all white, balanced in terms of sexual orientation and balanced", "tokens": [341, 1412, 992, 30, 6976, 11, 1360, 295, 5242, 11, 439, 2418, 11, 13902, 294, 2115, 295, 6701, 14764, 293, 13902], "temperature": 0.0, "avg_logprob": -0.18886811125512218, "compression_ratio": 1.3943661971830985, "no_speech_prob": 0.0005849804147146642}, {"id": 277, "seek": 221036, "start": 2218.6800000000003, "end": 2238.36, "text": " in terms of gender. It's white. Okay, so does not represent the population. So right, so basically,", "tokens": [294, 2115, 295, 7898, 13, 467, 311, 2418, 13, 1033, 11, 370, 775, 406, 2906, 264, 4415, 13, 407, 558, 11, 370, 1936, 11], "temperature": 0.0, "avg_logprob": -0.18886811125512218, "compression_ratio": 1.3943661971830985, "no_speech_prob": 0.0005849804147146642}, {"id": 278, "seek": 223836, "start": 2238.36, "end": 2245.56, "text": " you can guess that this data set has many, many biases incorporated. It contains only white people,", "tokens": [291, 393, 2041, 300, 341, 1412, 992, 575, 867, 11, 867, 32152, 21654, 13, 467, 8306, 787, 2418, 561, 11], "temperature": 0.0, "avg_logprob": -0.1427026564075101, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.0009170129778794944}, {"id": 279, "seek": 223836, "start": 2245.56, "end": 2253.88, "text": " only people who have self disclosed their sexual identity. It represents very certain social", "tokens": [787, 561, 567, 362, 2698, 17092, 1744, 641, 6701, 6575, 13, 467, 8855, 588, 1629, 2093], "temperature": 0.0, "avg_logprob": -0.1427026564075101, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.0009170129778794944}, {"id": 280, "seek": 223836, "start": 2253.88, "end": 2260.52, "text": " groups, people who put their photos on the dating website, specific age, specific ethnicity.", "tokens": [3935, 11, 561, 567, 829, 641, 5787, 322, 264, 10689, 3144, 11, 2685, 3205, 11, 2685, 33774, 13], "temperature": 0.0, "avg_logprob": -0.1427026564075101, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.0009170129778794944}, {"id": 281, "seek": 226052, "start": 2260.52, "end": 2269.88, "text": " And basically, these are photos that were carefully selected to be attractive to that target audience.", "tokens": [400, 1936, 11, 613, 366, 5787, 300, 645, 7500, 8209, 281, 312, 12609, 281, 300, 3779, 4034, 13], "temperature": 0.0, "avg_logprob": -0.2251849834735577, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0007275686948560178}, {"id": 282, "seek": 226052, "start": 2270.52, "end": 2277.4, "text": " So this data set contains many types of biases. And also, as one of you mentioned,", "tokens": [407, 341, 1412, 992, 8306, 867, 3467, 295, 32152, 13, 400, 611, 11, 382, 472, 295, 291, 2835, 11], "temperature": 0.0, "avg_logprob": -0.2251849834735577, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0007275686948560178}, {"id": 283, "seek": 226052, "start": 2277.4, "end": 2283.24, "text": " and as reading on this slide, that the data set is balanced, which does not represent the whole", "tokens": [293, 382, 3760, 322, 341, 4137, 11, 300, 264, 1412, 992, 307, 13902, 11, 597, 775, 406, 2906, 264, 1379], "temperature": 0.0, "avg_logprob": -0.2251849834735577, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0007275686948560178}, {"id": 284, "seek": 228324, "start": 2283.24, "end": 2291.24, "text": " the true distribution of the population. So what does it mean? This means that this model", "tokens": [264, 2074, 7316, 295, 264, 4415, 13, 407, 437, 775, 309, 914, 30, 639, 1355, 300, 341, 2316], "temperature": 0.0, "avg_logprob": -0.12044121878487724, "compression_ratio": 1.582857142857143, "no_speech_prob": 0.0009601183119229972}, {"id": 285, "seek": 228324, "start": 2292.4399999999996, "end": 2303.64, "text": " is built on a very biased data set. And you, as students at Stanford, you understand that", "tokens": [307, 3094, 322, 257, 588, 28035, 1412, 992, 13, 400, 291, 11, 382, 1731, 412, 20374, 11, 291, 1223, 300], "temperature": 0.0, "avg_logprob": -0.12044121878487724, "compression_ratio": 1.582857142857143, "no_speech_prob": 0.0009601183119229972}, {"id": 286, "seek": 228324, "start": 2304.4399999999996, "end": 2311.72, "text": " it cannot be used, for example, on a non-white population. It cannot be used on photos of people,", "tokens": [309, 2644, 312, 1143, 11, 337, 1365, 11, 322, 257, 2107, 12, 28865, 4415, 13, 467, 2644, 312, 1143, 322, 5787, 295, 561, 11], "temperature": 0.0, "avg_logprob": -0.12044121878487724, "compression_ratio": 1.582857142857143, "no_speech_prob": 0.0009601183119229972}, {"id": 287, "seek": 231172, "start": 2311.72, "end": 2318.6, "text": " not from the dating website. We don't actually know how this what this classifier learned.", "tokens": [406, 490, 264, 10689, 3144, 13, 492, 500, 380, 767, 458, 577, 341, 437, 341, 1508, 9902, 3264, 13], "temperature": 0.0, "avg_logprob": -0.1527761459350586, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.003139011561870575}, {"id": 288, "seek": 231172, "start": 2318.6, "end": 2324.8399999999997, "text": " Maybe the most important features were the watermark of this specific website. I don't know", "tokens": [2704, 264, 881, 1021, 4122, 645, 264, 1281, 5638, 295, 341, 2685, 3144, 13, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.1527761459350586, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.003139011561870575}, {"id": 289, "seek": 231172, "start": 2324.8399999999997, "end": 2333.7999999999997, "text": " or some other of confounds, curious confounds. But the point is that once the classifier is out,", "tokens": [420, 512, 661, 295, 1497, 4432, 11, 6369, 1497, 4432, 13, 583, 264, 935, 307, 300, 1564, 264, 1508, 9902, 307, 484, 11], "temperature": 0.0, "avg_logprob": -0.1527761459350586, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.003139011561870575}, {"id": 290, "seek": 231172, "start": 2333.7999999999997, "end": 2340.3599999999997, "text": " those who want to use it maliciously, they don't know that this technology is actually not applicable", "tokens": [729, 567, 528, 281, 764, 309, 33496, 356, 11, 436, 500, 380, 458, 300, 341, 2899, 307, 767, 406, 21142], "temperature": 0.0, "avg_logprob": -0.1527761459350586, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.003139011561870575}, {"id": 291, "seek": 234036, "start": 2340.36, "end": 2346.6800000000003, "text": " for any other data set except for this specific data set. So this technology is biased,", "tokens": [337, 604, 661, 1412, 992, 3993, 337, 341, 2685, 1412, 992, 13, 407, 341, 2899, 307, 28035, 11], "temperature": 0.0, "avg_logprob": -0.17381039603811796, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.0009558060555718839}, {"id": 292, "seek": 234036, "start": 2347.1600000000003, "end": 2354.36, "text": " and it also shows that it's basically not a credible result.", "tokens": [293, 309, 611, 3110, 300, 309, 311, 1936, 406, 257, 32757, 1874, 13], "temperature": 0.0, "avg_logprob": -0.17381039603811796, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.0009558060555718839}, {"id": 293, "seek": 234036, "start": 2356.52, "end": 2363.56, "text": " Okay, so let's move on. And final question is that, basically this is a deep learning model,", "tokens": [1033, 11, 370, 718, 311, 1286, 322, 13, 400, 2572, 1168, 307, 300, 11, 1936, 341, 307, 257, 2452, 2539, 2316, 11], "temperature": 0.0, "avg_logprob": -0.17381039603811796, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.0009558060555718839}, {"id": 294, "seek": 236356, "start": 2363.56, "end": 2372.36, "text": " it's a black box model, and then there is a question of how to analyze errors in the learning", "tokens": [309, 311, 257, 2211, 2424, 2316, 11, 293, 550, 456, 307, 257, 1168, 295, 577, 281, 12477, 13603, 294, 264, 2539], "temperature": 0.0, "avg_logprob": -0.15269547124062816, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0015487567288801074}, {"id": 295, "seek": 236356, "start": 2372.36, "end": 2380.6, "text": " model. Specifically, when we work on such a critical, such a sensitive topics like predictive", "tokens": [2316, 13, 26058, 11, 562, 321, 589, 322, 1270, 257, 4924, 11, 1270, 257, 9477, 8378, 411, 35521], "temperature": 0.0, "avg_logprob": -0.15269547124062816, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0015487567288801074}, {"id": 296, "seek": 236356, "start": 2380.6, "end": 2387.16, "text": " technology, not necessarily predicting sexual orientation, but for example, predicting gender,", "tokens": [2899, 11, 406, 4725, 32884, 6701, 14764, 11, 457, 337, 1365, 11, 32884, 7898, 11], "temperature": 0.0, "avg_logprob": -0.15269547124062816, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0015487567288801074}, {"id": 297, "seek": 238716, "start": 2387.16, "end": 2395.56, "text": " which is again used in many companies. So it is very difficult to understand whether it is okay", "tokens": [597, 307, 797, 1143, 294, 867, 3431, 13, 407, 309, 307, 588, 2252, 281, 1223, 1968, 309, 307, 1392], "temperature": 0.0, "avg_logprob": -0.11847608229693245, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.0005471415352076292}, {"id": 298, "seek": 238716, "start": 2395.56, "end": 2401.08, "text": " to use this technology. But the point is that we need to be able to analyze it and", "tokens": [281, 764, 341, 2899, 13, 583, 264, 935, 307, 300, 321, 643, 281, 312, 1075, 281, 12477, 309, 293], "temperature": 0.0, "avg_logprob": -0.11847608229693245, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.0005471415352076292}, {"id": 299, "seek": 238716, "start": 2403.0, "end": 2411.64, "text": " evaluate it properly. And the last point is about the accuracy. Again, and I'm going back to the", "tokens": [13059, 309, 6108, 13, 400, 264, 1036, 935, 307, 466, 264, 14170, 13, 3764, 11, 293, 286, 478, 516, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.11847608229693245, "compression_ratio": 1.5027322404371584, "no_speech_prob": 0.0005471415352076292}, {"id": 300, "seek": 241164, "start": 2411.64, "end": 2418.44, "text": " points that I mentioned also in the IQ classifier. So the accuracy of this classifier is 80%", "tokens": [2793, 300, 286, 2835, 611, 294, 264, 28921, 1508, 9902, 13, 407, 264, 14170, 295, 341, 1508, 9902, 307, 4688, 4], "temperature": 0.0, "avg_logprob": -0.142638624530949, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0005206561763770878}, {"id": 301, "seek": 241164, "start": 2418.44, "end": 2426.2, "text": " 1% for men and 74% for women. Is it a good accuracy or is it a bad accuracy?", "tokens": [502, 4, 337, 1706, 293, 28868, 4, 337, 2266, 13, 1119, 309, 257, 665, 14170, 420, 307, 309, 257, 1578, 14170, 30], "temperature": 0.0, "avg_logprob": -0.142638624530949, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0005206561763770878}, {"id": 302, "seek": 241164, "start": 2428.52, "end": 2436.68, "text": " So the numbers are okay for some tasks, but not for others. But importantly for this type of problem,", "tokens": [407, 264, 3547, 366, 1392, 337, 512, 9608, 11, 457, 406, 337, 2357, 13, 583, 8906, 337, 341, 2010, 295, 1154, 11], "temperature": 0.0, "avg_logprob": -0.142638624530949, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.0005206561763770878}, {"id": 303, "seek": 243668, "start": 2436.68, "end": 2443.0, "text": " it's important to understand that the cost of misclassification is not equal to the cost of", "tokens": [309, 311, 1021, 281, 1223, 300, 264, 2063, 295, 3346, 11665, 3774, 307, 406, 2681, 281, 264, 2063, 295], "temperature": 0.0, "avg_logprob": -0.12721270945534777, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.002051666146144271}, {"id": 304, "seek": 243668, "start": 2443.0, "end": 2453.48, "text": " correct prediction. And here is kind of visual examples. So if my algorithm misclassifies by doc", "tokens": [3006, 17630, 13, 400, 510, 307, 733, 295, 5056, 5110, 13, 407, 498, 452, 9284, 3346, 11665, 11221, 538, 3211], "temperature": 0.0, "avg_logprob": -0.12721270945534777, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.002051666146144271}, {"id": 305, "seek": 243668, "start": 2453.48, "end": 2461.0, "text": " as a cookie, is the cost misclassification of this misclassification as high or low?", "tokens": [382, 257, 14417, 11, 307, 264, 2063, 3346, 11665, 3774, 295, 341, 3346, 11665, 3774, 382, 1090, 420, 2295, 30], "temperature": 0.0, "avg_logprob": -0.12721270945534777, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.002051666146144271}, {"id": 306, "seek": 246100, "start": 2461.0, "end": 2470.44, "text": " So I guess it's just funny, right? It's funny. There is nothing offensive here. And then the next", "tokens": [407, 286, 2041, 309, 311, 445, 4074, 11, 558, 30, 467, 311, 4074, 13, 821, 307, 1825, 15710, 510, 13, 400, 550, 264, 958], "temperature": 0.0, "avg_logprob": -0.13514803632905212, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.0032234867103397846}, {"id": 307, "seek": 246100, "start": 2470.44, "end": 2479.64, "text": " question, if my algorithm misclassifies me with my doc, is the cost of misclassification high or low?", "tokens": [1168, 11, 498, 452, 9284, 3346, 11665, 11221, 385, 365, 452, 3211, 11, 307, 264, 2063, 295, 3346, 11665, 3774, 1090, 420, 2295, 30], "temperature": 0.0, "avg_logprob": -0.13514803632905212, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.0032234867103397846}, {"id": 308, "seek": 246100, "start": 2481.08, "end": 2486.6, "text": " It can be funny, but maybe not for everyone. We already don't know. And then the photo that I", "tokens": [467, 393, 312, 4074, 11, 457, 1310, 406, 337, 1518, 13, 492, 1217, 500, 380, 458, 13, 400, 550, 264, 5052, 300, 286], "temperature": 0.0, "avg_logprob": -0.13514803632905212, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.0032234867103397846}, {"id": 309, "seek": 248660, "start": 2486.6, "end": 2493.16, "text": " don't put here, but the one that is maybe many of you have heard is the gorilla incident that", "tokens": [500, 380, 829, 510, 11, 457, 264, 472, 300, 307, 1310, 867, 295, 291, 362, 2198, 307, 264, 45066, 9348, 300], "temperature": 0.0, "avg_logprob": -0.12449637703273607, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00042307432158850133}, {"id": 310, "seek": 248660, "start": 2493.16, "end": 2502.7599999999998, "text": " happened in Google in 2016. So in this case, there was a misclassification of African women", "tokens": [2011, 294, 3329, 294, 6549, 13, 407, 294, 341, 1389, 11, 456, 390, 257, 3346, 11665, 3774, 295, 7312, 2266], "temperature": 0.0, "avg_logprob": -0.12449637703273607, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00042307432158850133}, {"id": 311, "seek": 248660, "start": 2502.7599999999998, "end": 2508.92, "text": " with a gorilla and to understand how expensive is the cost of this misclassification. We need to", "tokens": [365, 257, 45066, 293, 281, 1223, 577, 5124, 307, 264, 2063, 295, 341, 3346, 11665, 3774, 13, 492, 643, 281], "temperature": 0.0, "avg_logprob": -0.12449637703273607, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.00042307432158850133}, {"id": 312, "seek": 250892, "start": 2508.92, "end": 2518.12, "text": " understand the whole history of dehumanization of black people in the US and so on. So we can see", "tokens": [1223, 264, 1379, 2503, 295, 368, 18796, 2144, 295, 2211, 561, 294, 264, 2546, 293, 370, 322, 13, 407, 321, 393, 536], "temperature": 0.0, "avg_logprob": -0.12383794784545898, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.0005467299488373101}, {"id": 313, "seek": 250892, "start": 2518.12, "end": 2525.2400000000002, "text": " the difference for the same algorithm and same types of errors. There are different types of errors", "tokens": [264, 2649, 337, 264, 912, 9284, 293, 912, 3467, 295, 13603, 13, 821, 366, 819, 3467, 295, 13603], "temperature": 0.0, "avg_logprob": -0.12383794784545898, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.0005467299488373101}, {"id": 314, "seek": 250892, "start": 2525.2400000000002, "end": 2535.88, "text": " that are more expensive than others. So this is why it is important to assess AI systems adversarially.", "tokens": [300, 366, 544, 5124, 813, 2357, 13, 407, 341, 307, 983, 309, 307, 1021, 281, 5877, 7318, 3652, 17641, 289, 2270, 13], "temperature": 0.0, "avg_logprob": -0.12383794784545898, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.0005467299488373101}, {"id": 315, "seek": 253588, "start": 2535.88, "end": 2541.08, "text": " And now I just want to reiterate the types of questions that I asked because these are the kinds", "tokens": [400, 586, 286, 445, 528, 281, 33528, 264, 3467, 295, 1651, 300, 286, 2351, 570, 613, 366, 264, 3685], "temperature": 0.0, "avg_logprob": -0.10675422444063075, "compression_ratio": 1.7668161434977578, "no_speech_prob": 0.0010180018143728375}, {"id": 316, "seek": 253588, "start": 2541.08, "end": 2546.52, "text": " of questions that you might want to ask yourself next time when you need to build another predictive", "tokens": [295, 1651, 300, 291, 1062, 528, 281, 1029, 1803, 958, 565, 562, 291, 643, 281, 1322, 1071, 35521], "temperature": 0.0, "avg_logprob": -0.10675422444063075, "compression_ratio": 1.7668161434977578, "no_speech_prob": 0.0010180018143728375}, {"id": 317, "seek": 253588, "start": 2546.52, "end": 2553.48, "text": " technology. And the first is to understand the ethics of the research question. And sometimes it's", "tokens": [2899, 13, 400, 264, 700, 307, 281, 1223, 264, 19769, 295, 264, 2132, 1168, 13, 400, 2171, 309, 311], "temperature": 0.0, "avg_logprob": -0.10675422444063075, "compression_ratio": 1.7668161434977578, "no_speech_prob": 0.0010180018143728375}, {"id": 318, "seek": 253588, "start": 2553.48, "end": 2561.0, "text": " not very easy to understand, but just ask yourself these most specific questions. If I build this", "tokens": [406, 588, 1858, 281, 1223, 11, 457, 445, 1029, 1803, 613, 881, 2685, 1651, 13, 759, 286, 1322, 341], "temperature": 0.0, "avg_logprob": -0.10675422444063075, "compression_ratio": 1.7668161434977578, "no_speech_prob": 0.0010180018143728375}, {"id": 319, "seek": 256100, "start": 2561.0, "end": 2567.32, "text": " technology, who could benefit from such a technology? And who can be harmed by it? So try to see the", "tokens": [2899, 11, 567, 727, 5121, 490, 1270, 257, 2899, 30, 400, 567, 393, 312, 41478, 538, 309, 30, 407, 853, 281, 536, 264], "temperature": 0.0, "avg_logprob": -0.16762893627851438, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0015157361049205065}, {"id": 320, "seek": 256100, "start": 2567.32, "end": 2575.08, "text": " corner cases. And also what about the data? Could sharing this data have major effect on people's", "tokens": [4538, 3331, 13, 400, 611, 437, 466, 264, 1412, 30, 7497, 5414, 341, 1412, 362, 2563, 1802, 322, 561, 311], "temperature": 0.0, "avg_logprob": -0.16762893627851438, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0015157361049205065}, {"id": 321, "seek": 256100, "start": 2575.08, "end": 2584.04, "text": " lives, like in the case of AI, gay, dark, classifier? The next question that you can ask is about privacy.", "tokens": [2909, 11, 411, 294, 264, 1389, 295, 7318, 11, 9049, 11, 2877, 11, 1508, 9902, 30, 440, 958, 1168, 300, 291, 393, 1029, 307, 466, 11427, 13], "temperature": 0.0, "avg_logprob": -0.16762893627851438, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.0015157361049205065}, {"id": 322, "seek": 258404, "start": 2584.04, "end": 2591.4, "text": " What we discuss, who owns the data? And is this data not only public or legal to use, but also", "tokens": [708, 321, 2248, 11, 567, 19143, 264, 1412, 30, 400, 307, 341, 1412, 406, 787, 1908, 420, 5089, 281, 764, 11, 457, 611], "temperature": 0.0, "avg_logprob": -0.11561528058119223, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.000721702235750854}, {"id": 323, "seek": 258404, "start": 2591.4, "end": 2599.0, "text": " is it are we violating the social, basically, circles to which the data is publicized? Are we", "tokens": [307, 309, 366, 321, 42201, 264, 2093, 11, 1936, 11, 13040, 281, 597, 264, 1412, 307, 1908, 1602, 30, 2014, 321], "temperature": 0.0, "avg_logprob": -0.11561528058119223, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.000721702235750854}, {"id": 324, "seek": 258404, "start": 2599.0, "end": 2606.44, "text": " violating a social contract in the way that the public data is expected to be used? And user", "tokens": [42201, 257, 2093, 4364, 294, 264, 636, 300, 264, 1908, 1412, 307, 5176, 281, 312, 1143, 30, 400, 4195], "temperature": 0.0, "avg_logprob": -0.11561528058119223, "compression_ratio": 1.6337209302325582, "no_speech_prob": 0.000721702235750854}, {"id": 325, "seek": 260644, "start": 2606.44, "end": 2616.04, "text": " consent is not always possible to obtain. But we need to understand implicit assumptions of people", "tokens": [14546, 307, 406, 1009, 1944, 281, 12701, 13, 583, 321, 643, 281, 1223, 26947, 17695, 295, 561], "temperature": 0.0, "avg_logprob": -0.09631749450183305, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.001489700167439878}, {"id": 326, "seek": 260644, "start": 2616.04, "end": 2624.36, "text": " who put their data online. Now, the next question is what are possible biases in data? What are", "tokens": [567, 829, 641, 1412, 2950, 13, 823, 11, 264, 958, 1168, 307, 437, 366, 1944, 32152, 294, 1412, 30, 708, 366], "temperature": 0.0, "avg_logprob": -0.09631749450183305, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.001489700167439878}, {"id": 327, "seek": 260644, "start": 2624.36, "end": 2631.48, "text": " artifacts in data? What are distributions for specific populations and subpopulations?", "tokens": [24617, 294, 1412, 30, 708, 366, 37870, 337, 2685, 12822, 293, 1422, 13872, 4136, 30], "temperature": 0.0, "avg_logprob": -0.09631749450183305, "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.001489700167439878}, {"id": 328, "seek": 263148, "start": 2631.48, "end": 2640.12, "text": " How representative or what kind of misrepresentations are in my data? And next is what is", "tokens": [1012, 12424, 420, 437, 733, 295, 3346, 19919, 11662, 763, 366, 294, 452, 1412, 30, 400, 958, 307, 437, 307], "temperature": 0.0, "avg_logprob": -0.17387451664094003, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0016843428602442145}, {"id": 329, "seek": 263148, "start": 2640.6, "end": 2647.2400000000002, "text": " basically potential bias in these models? Do I when I build this model do I control for confined", "tokens": [1936, 3995, 12577, 294, 613, 5245, 30, 1144, 286, 562, 286, 1322, 341, 2316, 360, 286, 1969, 337, 1497, 2001], "temperature": 0.0, "avg_logprob": -0.17387451664094003, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0016843428602442145}, {"id": 330, "seek": 263148, "start": 2647.2400000000002, "end": 2653.72, "text": " variables? And do I optimize for the right objective? Like in the case of the IQ classifier?", "tokens": [9102, 30, 400, 360, 286, 19719, 337, 264, 558, 10024, 30, 1743, 294, 264, 1389, 295, 264, 28921, 1508, 9902, 30], "temperature": 0.0, "avg_logprob": -0.17387451664094003, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0016843428602442145}, {"id": 331, "seek": 263148, "start": 2654.44, "end": 2661.4, "text": " And also if I have biases, does my system amplify biases? And finally, it is not enough to measure", "tokens": [400, 611, 498, 286, 362, 32152, 11, 775, 452, 1185, 41174, 32152, 30, 400, 2721, 11, 309, 307, 406, 1547, 281, 3481], "temperature": 0.0, "avg_logprob": -0.17387451664094003, "compression_ratio": 1.6016949152542372, "no_speech_prob": 0.0016843428602442145}, {"id": 332, "seek": 266140, "start": 2661.4, "end": 2670.28, "text": " accuracy because the kind of semantics of false positives and false negatives can be different.", "tokens": [14170, 570, 264, 733, 295, 4361, 45298, 295, 7908, 35127, 293, 7908, 40019, 393, 312, 819, 13], "temperature": 0.0, "avg_logprob": -0.1520365925578328, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.001456430647522211}, {"id": 333, "seek": 266140, "start": 2670.28, "end": 2676.12, "text": " Sometimes the cost of misclassification is much higher than the cost of correct prediction.", "tokens": [4803, 264, 2063, 295, 3346, 11665, 3774, 307, 709, 2946, 813, 264, 2063, 295, 3006, 17630, 13], "temperature": 0.0, "avg_logprob": -0.1520365925578328, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.001456430647522211}, {"id": 334, "seek": 266140, "start": 2676.12, "end": 2681.0, "text": " So, also need to understand how to evaluate the model's property.", "tokens": [407, 11, 611, 643, 281, 1223, 577, 281, 13059, 264, 2316, 311, 4707, 13], "temperature": 0.0, "avg_logprob": -0.1520365925578328, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.001456430647522211}, {"id": 335, "seek": 266140, "start": 2683.2400000000002, "end": 2690.36, "text": " And why is it especially relevant now? Is because as you all know, there is an exponential", "tokens": [400, 983, 307, 309, 2318, 7340, 586, 30, 1119, 570, 382, 291, 439, 458, 11, 456, 307, 364, 21510], "temperature": 0.0, "avg_logprob": -0.1520365925578328, "compression_ratio": 1.5636363636363637, "no_speech_prob": 0.001456430647522211}, {"id": 336, "seek": 269036, "start": 2690.36, "end": 2696.52, "text": " growth of user-generated data. And it's really easy to build the tools. Each of us can build the", "tokens": [4599, 295, 4195, 12, 21848, 770, 1412, 13, 400, 309, 311, 534, 1858, 281, 1322, 264, 3873, 13, 6947, 295, 505, 393, 1322, 264], "temperature": 0.0, "avg_logprob": -0.19426044341056578, "compression_ratio": 1.5327868852459017, "no_speech_prob": 0.0005263944040052593}, {"id": 337, "seek": 269036, "start": 2696.52, "end": 2703.8, "text": " IQ classifier or GADAR. But the question is what kind of technology we will produce?", "tokens": [28921, 1508, 9902, 420, 460, 6112, 1899, 13, 583, 264, 1168, 307, 437, 733, 295, 2899, 321, 486, 5258, 30], "temperature": 0.0, "avg_logprob": -0.19426044341056578, "compression_ratio": 1.5327868852459017, "no_speech_prob": 0.0005263944040052593}, {"id": 338, "seek": 269036, "start": 2705.0, "end": 2712.76, "text": " So, in this I finished the first part of discussion and I put in this slide some recommended papers", "tokens": [407, 11, 294, 341, 286, 4335, 264, 700, 644, 295, 5017, 293, 286, 829, 294, 341, 4137, 512, 9628, 10577], "temperature": 0.0, "avg_logprob": -0.19426044341056578, "compression_ratio": 1.5327868852459017, "no_speech_prob": 0.0005263944040052593}, {"id": 339, "seek": 269036, "start": 2712.76, "end": 2720.2000000000003, "text": " on talks specifically on the introductory topics on the impact of NLP. And I think these are", "tokens": [322, 6686, 4682, 322, 264, 39048, 8378, 322, 264, 2712, 295, 426, 45196, 13, 400, 286, 519, 613, 366], "temperature": 0.0, "avg_logprob": -0.19426044341056578, "compression_ratio": 1.5327868852459017, "no_speech_prob": 0.0005263944040052593}, {"id": 340, "seek": 272020, "start": 2720.2, "end": 2726.2, "text": " there are hundreds or thousands of similar talks, but these are my favorites. So, if you want to read", "tokens": [456, 366, 6779, 420, 5383, 295, 2531, 6686, 11, 457, 613, 366, 452, 16907, 13, 407, 11, 498, 291, 528, 281, 1401], "temperature": 0.0, "avg_logprob": -0.17526715422329836, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.0003229753638152033}, {"id": 341, "seek": 272020, "start": 2726.2, "end": 2735.64, "text": " more, please take a look. Should I stop for questions? So, should we move to the next part?", "tokens": [544, 11, 1767, 747, 257, 574, 13, 6454, 286, 1590, 337, 1651, 30, 407, 11, 820, 321, 1286, 281, 264, 958, 644, 30], "temperature": 0.0, "avg_logprob": -0.17526715422329836, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.0003229753638152033}, {"id": 342, "seek": 272020, "start": 2738.8399999999997, "end": 2743.56, "text": " Right at the moment there aren't any outstanding questions. So, maybe it's okay to move on", "tokens": [1779, 412, 264, 1623, 456, 3212, 380, 604, 14485, 1651, 13, 407, 11, 1310, 309, 311, 1392, 281, 1286, 322], "temperature": 0.0, "avg_logprob": -0.17526715422329836, "compression_ratio": 1.518716577540107, "no_speech_prob": 0.0003229753638152033}, {"id": 343, "seek": 274356, "start": 2743.56, "end": 2751.7999999999997, "text": " unless anyone is desperately typing. Okay, and the chat window is really nice. There are so", "tokens": [5969, 2878, 307, 23726, 18444, 13, 1033, 11, 293, 264, 5081, 4910, 307, 534, 1481, 13, 821, 366, 370], "temperature": 0.0, "avg_logprob": -0.13658812840779622, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0004737227864097804}, {"id": 344, "seek": 274356, "start": 2751.7999999999997, "end": 2758.84, "text": " many responses. Thank you all. And I hope to get later this chat file to read it.", "tokens": [867, 13019, 13, 1044, 291, 439, 13, 400, 286, 1454, 281, 483, 1780, 341, 5081, 3991, 281, 1401, 309, 13], "temperature": 0.0, "avg_logprob": -0.13658812840779622, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0004737227864097804}, {"id": 345, "seek": 274356, "start": 2758.84, "end": 2765.48, "text": " I think we can save it, yeah. Thank you. Okay, we can move on to the second part about algorithmic bias.", "tokens": [286, 519, 321, 393, 3155, 309, 11, 1338, 13, 1044, 291, 13, 1033, 11, 321, 393, 1286, 322, 281, 264, 1150, 644, 466, 9284, 299, 12577, 13], "temperature": 0.0, "avg_logprob": -0.13658812840779622, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0004737227864097804}, {"id": 346, "seek": 276548, "start": 2765.48, "end": 2776.84, "text": " So, what are the topics in the intersection of ethics in NLP? So, the first one is algorithmic bias,", "tokens": [407, 11, 437, 366, 264, 8378, 294, 264, 15236, 295, 19769, 294, 426, 45196, 30, 407, 11, 264, 700, 472, 307, 9284, 299, 12577, 11], "temperature": 0.0, "avg_logprob": -0.12844778696695963, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0012431020149961114}, {"id": 347, "seek": 276548, "start": 2776.84, "end": 2781.96, "text": " and this is about bias in data and NLP models, and this is something that I will talk more in the", "tokens": [293, 341, 307, 466, 12577, 294, 1412, 293, 426, 45196, 5245, 11, 293, 341, 307, 746, 300, 286, 486, 751, 544, 294, 264], "temperature": 0.0, "avg_logprob": -0.12844778696695963, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0012431020149961114}, {"id": 348, "seek": 276548, "start": 2781.96, "end": 2788.28, "text": " second part. But important to understand the field is much broader. So, the next topic is", "tokens": [1150, 644, 13, 583, 1021, 281, 1223, 264, 2519, 307, 709, 13227, 13, 407, 11, 264, 958, 4829, 307], "temperature": 0.0, "avg_logprob": -0.12844778696695963, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0012431020149961114}, {"id": 349, "seek": 278828, "start": 2788.28, "end": 2796.6000000000004, "text": " incivility. So, ability to develop NLP tools, to identify, to actually data analytics, to understand", "tokens": [294, 537, 85, 1140, 13, 407, 11, 3485, 281, 1499, 426, 45196, 3873, 11, 281, 5876, 11, 281, 767, 1412, 15370, 11, 281, 1223], "temperature": 0.0, "avg_logprob": -0.18471242235852525, "compression_ratio": 1.49009900990099, "no_speech_prob": 0.002144248690456152}, {"id": 350, "seek": 278828, "start": 2796.6000000000004, "end": 2803.88, "text": " the hate speech, toxicity, incivility online, and this is a very complicated field, because it's not", "tokens": [264, 4700, 6218, 11, 45866, 11, 294, 537, 85, 1140, 2950, 11, 293, 341, 307, 257, 588, 6179, 2519, 11, 570, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.18471242235852525, "compression_ratio": 1.49009900990099, "no_speech_prob": 0.002144248690456152}, {"id": 351, "seek": 278828, "start": 2803.88, "end": 2810.44, "text": " only about building the right classifiers. There are many, many questions such as if I post hateful", "tokens": [787, 466, 2390, 264, 558, 1508, 23463, 13, 821, 366, 867, 11, 867, 1651, 1270, 382, 498, 286, 2183, 4700, 906], "temperature": 0.0, "avg_logprob": -0.18471242235852525, "compression_ratio": 1.49009900990099, "no_speech_prob": 0.002144248690456152}, {"id": 352, "seek": 281044, "start": 2810.44, "end": 2819.08, "text": " comment, who does this comment belong to? Does it belong to a company? Does it belong to me? Should", "tokens": [2871, 11, 567, 775, 341, 2871, 5784, 281, 30, 4402, 309, 5784, 281, 257, 2237, 30, 4402, 309, 5784, 281, 385, 30, 6454], "temperature": 0.0, "avg_logprob": -0.12195432358893796, "compression_ratio": 1.5977653631284916, "no_speech_prob": 0.0027615674771368504}, {"id": 353, "seek": 281044, "start": 2819.08, "end": 2824.52, "text": " it be removed or not? Because it is not clear where is the boundary between the free speech and", "tokens": [309, 312, 7261, 420, 406, 30, 1436, 309, 307, 406, 1850, 689, 307, 264, 12866, 1296, 264, 1737, 6218, 293], "temperature": 0.0, "avg_logprob": -0.12195432358893796, "compression_ratio": 1.5977653631284916, "no_speech_prob": 0.0027615674771368504}, {"id": 354, "seek": 281044, "start": 2826.52, "end": 2833.16, "text": " moderated speech, how to minimize the harms, but defend the democracy. These questions are", "tokens": [10494, 770, 6218, 11, 577, 281, 17522, 264, 48505, 11, 457, 8602, 264, 10528, 13, 1981, 1651, 366], "temperature": 0.0, "avg_logprob": -0.12195432358893796, "compression_ratio": 1.5977653631284916, "no_speech_prob": 0.0027615674771368504}, {"id": 355, "seek": 283316, "start": 2833.16, "end": 2843.08, "text": " very subjective, and they are not regulated. So, this is a big difficult field. The next field is", "tokens": [588, 25972, 11, 293, 436, 366, 406, 26243, 13, 407, 11, 341, 307, 257, 955, 2252, 2519, 13, 440, 958, 2519, 307], "temperature": 0.0, "avg_logprob": -0.18822093963623046, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.0007055687601678073}, {"id": 356, "seek": 283316, "start": 2843.08, "end": 2849.3999999999996, "text": " about privacy. So, again, who does this data belong to, and how to protect privacy? This is", "tokens": [466, 11427, 13, 407, 11, 797, 11, 567, 775, 341, 1412, 5784, 281, 11, 293, 577, 281, 2371, 11427, 30, 639, 307], "temperature": 0.0, "avg_logprob": -0.18822093963623046, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.0007055687601678073}, {"id": 357, "seek": 283316, "start": 2850.2799999999997, "end": 2856.2, "text": " field of privacy is actually very, very under-explored NLP. On other fields, I think there is", "tokens": [2519, 295, 11427, 307, 767, 588, 11, 588, 833, 12, 23040, 2769, 426, 45196, 13, 1282, 661, 7909, 11, 286, 519, 456, 307], "temperature": 0.0, "avg_logprob": -0.18822093963623046, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.0007055687601678073}, {"id": 358, "seek": 283316, "start": 2856.2, "end": 2862.2, "text": " some research on incivility, or no gripping bias, on other fields, but very little research on", "tokens": [512, 2132, 322, 294, 537, 85, 1140, 11, 420, 572, 17865, 3759, 12577, 11, 322, 661, 7909, 11, 457, 588, 707, 2132, 322], "temperature": 0.0, "avg_logprob": -0.18822093963623046, "compression_ratio": 1.6293103448275863, "no_speech_prob": 0.0007055687601678073}, {"id": 359, "seek": 286220, "start": 2862.2, "end": 2873.72, "text": " privacy. Misinformation, so information manipulation, opinion manipulation, fake news. So, there is", "tokens": [11427, 13, 23240, 20941, 11, 370, 1589, 26475, 11, 4800, 26475, 11, 7592, 2583, 13, 407, 11, 456, 307], "temperature": 0.0, "avg_logprob": -0.19354880133340524, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0038431580178439617}, {"id": 360, "seek": 286220, "start": 2873.72, "end": 2883.8799999999997, "text": " a whole range of ways that data can be manipulated from generated texts and disinformation to just", "tokens": [257, 1379, 3613, 295, 2098, 300, 1412, 393, 312, 37161, 490, 10833, 15765, 293, 717, 20941, 281, 445], "temperature": 0.0, "avg_logprob": -0.19354880133340524, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0038431580178439617}, {"id": 361, "seek": 288388, "start": 2883.88, "end": 2892.6, "text": " advertisement and the most subtle or propaganda and subtle opinion manipulation. And there are", "tokens": [31370, 293, 264, 881, 13743, 420, 22968, 293, 13743, 4800, 26475, 13, 400, 456, 366], "temperature": 0.0, "avg_logprob": -0.17925217946370442, "compression_ratio": 1.575268817204301, "no_speech_prob": 0.0007710880599915981}, {"id": 362, "seek": 288388, "start": 2892.6, "end": 2899.08, "text": " many, many interesting research projects that can be done really with focus on the language of", "tokens": [867, 11, 867, 1880, 2132, 4455, 300, 393, 312, 1096, 534, 365, 1879, 322, 264, 2856, 295], "temperature": 0.0, "avg_logprob": -0.17925217946370442, "compression_ratio": 1.575268817204301, "no_speech_prob": 0.0007710880599915981}, {"id": 363, "seek": 288388, "start": 2899.08, "end": 2908.04, "text": " manipulation. I think it's just an interesting topic to explore. And finally, the technological divide.", "tokens": [26475, 13, 286, 519, 309, 311, 445, 364, 1880, 4829, 281, 6839, 13, 400, 2721, 11, 264, 18439, 9845, 13], "temperature": 0.0, "avg_logprob": -0.17925217946370442, "compression_ratio": 1.575268817204301, "no_speech_prob": 0.0007710880599915981}, {"id": 364, "seek": 290804, "start": 2908.04, "end": 2915.08, "text": " So, when we build our tools, even if it's a part of speech tagger, what are populations for which", "tokens": [407, 11, 562, 321, 1322, 527, 3873, 11, 754, 498, 309, 311, 257, 644, 295, 6218, 6162, 1321, 11, 437, 366, 12822, 337, 597], "temperature": 0.0, "avg_logprob": -0.16930844085385102, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0010463384678587317}, {"id": 365, "seek": 290804, "start": 2915.96, "end": 2922.12, "text": " are served by these tools? So, there is a certain divide. The technologies are built unequally. There", "tokens": [366, 7584, 538, 613, 3873, 30, 407, 11, 456, 307, 257, 1629, 9845, 13, 440, 7943, 366, 3094, 2251, 358, 379, 13, 821], "temperature": 0.0, "avg_logprob": -0.16930844085385102, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0010463384678587317}, {"id": 366, "seek": 290804, "start": 2922.12, "end": 2928.52, "text": " is no one language, no two languages. There are six thousand languages in the world, and there are", "tokens": [307, 572, 472, 2856, 11, 572, 732, 8650, 13, 821, 366, 2309, 4714, 8650, 294, 264, 1002, 11, 293, 456, 366], "temperature": 0.0, "avg_logprob": -0.16930844085385102, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0010463384678587317}, {"id": 367, "seek": 290804, "start": 2928.52, "end": 2936.52, "text": " many populations. And there are certain areas of NLP, which are completely under-explored. For", "tokens": [867, 12822, 13, 400, 456, 366, 1629, 3179, 295, 426, 45196, 11, 597, 366, 2584, 833, 12, 23040, 2769, 13, 1171], "temperature": 0.0, "avg_logprob": -0.16930844085385102, "compression_ratio": 1.6652542372881356, "no_speech_prob": 0.0010463384678587317}, {"id": 368, "seek": 293652, "start": 2936.52, "end": 2942.7599999999998, "text": " example, language varieties. We think we can solve a problem for English, the problem of dependency", "tokens": [1365, 11, 2856, 22092, 13, 492, 519, 321, 393, 5039, 257, 1154, 337, 3669, 11, 264, 1154, 295, 33621], "temperature": 0.0, "avg_logprob": -0.15944886207580566, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0019526851829141378}, {"id": 369, "seek": 293652, "start": 2942.7599999999998, "end": 2948.2, "text": " parsing, but we don't account for different varieties of English. What about Nigerian English,", "tokens": [21156, 278, 11, 457, 321, 500, 380, 2696, 337, 819, 22092, 295, 3669, 13, 708, 466, 21489, 952, 3669, 11], "temperature": 0.0, "avg_logprob": -0.15944886207580566, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0019526851829141378}, {"id": 370, "seek": 293652, "start": 2948.2, "end": 2955.64, "text": " what about African-American English, what about Indian English? So, there are the", "tokens": [437, 466, 7312, 12, 14604, 3669, 11, 437, 466, 6427, 3669, 30, 407, 11, 456, 366, 264], "temperature": 0.0, "avg_logprob": -0.15944886207580566, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0019526851829141378}, {"id": 371, "seek": 295564, "start": 2955.64, "end": 2966.44, "text": " technological divide that is currently present. And as you see in the bottom, this picture shows that", "tokens": [18439, 9845, 300, 307, 4362, 1974, 13, 400, 382, 291, 536, 294, 264, 2767, 11, 341, 3036, 3110, 300], "temperature": 0.0, "avg_logprob": -0.18879824169611525, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0005847453139722347}, {"id": 372, "seek": 295564, "start": 2966.44, "end": 2973.3199999999997, "text": " they feel this highly interdisciplinary. So, AI researchers cannot actually solve the problem of", "tokens": [436, 841, 341, 5405, 38280, 13, 407, 11, 7318, 10309, 2644, 767, 5039, 264, 1154, 295], "temperature": 0.0, "avg_logprob": -0.18879824169611525, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0005847453139722347}, {"id": 373, "seek": 295564, "start": 2974.04, "end": 2981.0, "text": " misinformation alone. To be able to address the problem of misinformation or hate speech,", "tokens": [34238, 3312, 13, 1407, 312, 1075, 281, 2985, 264, 1154, 295, 34238, 420, 4700, 6218, 11], "temperature": 0.0, "avg_logprob": -0.18879824169611525, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0005847453139722347}, {"id": 374, "seek": 298100, "start": 2981.0, "end": 2989.48, "text": " we need to have not only engineers, but also ethicists and social scientists and activists,", "tokens": [321, 643, 281, 362, 406, 787, 11955, 11, 457, 611, 37820, 1751, 293, 2093, 7708, 293, 23042, 11], "temperature": 0.0, "avg_logprob": -0.23802275811472245, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.0026353809516876936}, {"id": 375, "seek": 298100, "start": 2993.4, "end": 3001.56, "text": " and politicians who actually are responsible for policies and linguists, because many of these", "tokens": [293, 14756, 567, 767, 366, 6250, 337, 7657, 293, 21766, 1751, 11, 570, 867, 295, 613], "temperature": 0.0, "avg_logprob": -0.23802275811472245, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.0026353809516876936}, {"id": 376, "seek": 298100, "start": 3001.56, "end": 3008.52, "text": " phenomena are interesting phenomena, which are not in words, but more in pragmatics. So,", "tokens": [22004, 366, 1880, 22004, 11, 597, 366, 406, 294, 2283, 11, 457, 544, 294, 33394, 15677, 1167, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.23802275811472245, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.0026353809516876936}, {"id": 377, "seek": 300852, "start": 3008.52, "end": 3013.64, "text": " this is a very interesting field, scientifically, but also very challenging to work on.", "tokens": [341, 307, 257, 588, 1880, 2519, 11, 39719, 11, 457, 611, 588, 7595, 281, 589, 322, 13], "temperature": 0.0, "avg_logprob": -0.20283506354507136, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.003148505697026849}, {"id": 378, "seek": 300852, "start": 3015.24, "end": 3019.8, "text": " And there are some recommended resources, and in particular, the one that might be interesting to", "tokens": [400, 456, 366, 512, 9628, 3593, 11, 293, 294, 1729, 11, 264, 472, 300, 1062, 312, 1880, 281], "temperature": 0.0, "avg_logprob": -0.20283506354507136, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.003148505697026849}, {"id": 379, "seek": 300852, "start": 3019.8, "end": 3029.48, "text": " you all is CS384, is a seminar by Dan Jaravsky, that is an amazing also list. So, if you want to", "tokens": [291, 439, 307, 9460, 18, 25494, 11, 307, 257, 29235, 538, 3394, 23941, 706, 25810, 11, 300, 307, 364, 2243, 611, 1329, 13, 407, 11, 498, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.20283506354507136, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.003148505697026849}, {"id": 380, "seek": 300852, "start": 3029.48, "end": 3037.4, "text": " take a look on specific subfields. So, this is a general overview, and now I want to talk about", "tokens": [747, 257, 574, 322, 2685, 1422, 7610, 82, 13, 407, 11, 341, 307, 257, 2674, 12492, 11, 293, 586, 286, 528, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.20283506354507136, "compression_ratio": 1.5684647302904564, "no_speech_prob": 0.003148505697026849}, {"id": 381, "seek": 303740, "start": 3037.4, "end": 3044.84, "text": " one of these subfields, and give some of explanation, why do we have algorithmic bias in our models.", "tokens": [472, 295, 613, 1422, 7610, 82, 11, 293, 976, 512, 295, 10835, 11, 983, 360, 321, 362, 9284, 299, 12577, 294, 527, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16593596223112825, "compression_ratio": 1.4619289340101522, "no_speech_prob": 0.0014728668611496687}, {"id": 382, "seek": 303740, "start": 3045.7200000000003, "end": 3053.64, "text": " So, let's start again with interaction. I know you have the slides, but don't look forward.", "tokens": [407, 11, 718, 311, 722, 797, 365, 9285, 13, 286, 458, 291, 362, 264, 9788, 11, 457, 500, 380, 574, 2128, 13], "temperature": 0.0, "avg_logprob": -0.16593596223112825, "compression_ratio": 1.4619289340101522, "no_speech_prob": 0.0014728668611496687}, {"id": 383, "seek": 303740, "start": 3054.52, "end": 3059.7200000000003, "text": " I want to ask you questions, and please type, which word is more likely to be used by a female?", "tokens": [286, 528, 281, 1029, 291, 1651, 11, 293, 1767, 2010, 11, 597, 1349, 307, 544, 3700, 281, 312, 1143, 538, 257, 6556, 30], "temperature": 0.0, "avg_logprob": -0.16593596223112825, "compression_ratio": 1.4619289340101522, "no_speech_prob": 0.0014728668611496687}, {"id": 384, "seek": 305972, "start": 3059.72, "end": 3071.64, "text": " Giggle or laugh? Just type quickly, don't think too much. So, please look at the chat and see", "tokens": [460, 19694, 420, 5801, 30, 1449, 2010, 2661, 11, 500, 380, 519, 886, 709, 13, 407, 11, 1767, 574, 412, 264, 5081, 293, 536], "temperature": 0.0, "avg_logprob": -0.1722868589254526, "compression_ratio": 1.2810457516339868, "no_speech_prob": 0.00042207265505567193}, {"id": 385, "seek": 305972, "start": 3071.64, "end": 3083.9599999999996, "text": " the majority response. It is absolutely giggle. You're right. Next question, which word is more likely", "tokens": [264, 6286, 4134, 13, 467, 307, 3122, 290, 19694, 13, 509, 434, 558, 13, 3087, 1168, 11, 597, 1349, 307, 544, 3700], "temperature": 0.0, "avg_logprob": -0.1722868589254526, "compression_ratio": 1.2810457516339868, "no_speech_prob": 0.00042207265505567193}, {"id": 386, "seek": 308396, "start": 3083.96, "end": 3098.84, "text": " to be used by a female? Brutal or fierce? Oh, lovely. Like 99% fierce. Thank you. Next question,", "tokens": [281, 312, 1143, 538, 257, 6556, 30, 1603, 325, 304, 420, 25341, 30, 876, 11, 7496, 13, 1743, 11803, 4, 25341, 13, 1044, 291, 13, 3087, 1168, 11], "temperature": 0.0, "avg_logprob": -0.16892163570110613, "compression_ratio": 1.2571428571428571, "no_speech_prob": 0.0010918315965682268}, {"id": 387, "seek": 308396, "start": 3099.96, "end": 3103.88, "text": " which word is more likely to be used by an older person? Impressive or amazing?", "tokens": [597, 1349, 307, 544, 3700, 281, 312, 1143, 538, 364, 4906, 954, 30, 8270, 22733, 420, 2243, 30], "temperature": 0.0, "avg_logprob": -0.16892163570110613, "compression_ratio": 1.2571428571428571, "no_speech_prob": 0.0010918315965682268}, {"id": 388, "seek": 310388, "start": 3103.88, "end": 3114.28, "text": " So, actually from what I see, it's 100% impressive. Very impressive. Thank you for answers,", "tokens": [407, 11, 767, 490, 437, 286, 536, 11, 309, 311, 2319, 4, 8992, 13, 4372, 8992, 13, 1044, 291, 337, 6338, 11], "temperature": 0.0, "avg_logprob": -0.20229486435178726, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.0023457396309822798}, {"id": 389, "seek": 310388, "start": 3115.0, "end": 3121.6400000000003, "text": " which word is more likely to be used by a person of a higher occupational class. Suggestions or proposals.", "tokens": [597, 1349, 307, 544, 3700, 281, 312, 1143, 538, 257, 954, 295, 257, 2946, 43544, 1508, 13, 39131, 2629, 626, 420, 20198, 13], "temperature": 0.0, "avg_logprob": -0.20229486435178726, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.0023457396309822798}, {"id": 390, "seek": 310388, "start": 3125.0, "end": 3127.6400000000003, "text": " Do you see how correctly you answer my questions?", "tokens": [1144, 291, 536, 577, 8944, 291, 1867, 452, 1651, 30], "temperature": 0.0, "avg_logprob": -0.20229486435178726, "compression_ratio": 1.4011299435028248, "no_speech_prob": 0.0023457396309822798}, {"id": 391, "seek": 312764, "start": 3127.64, "end": 3136.04, "text": " Next question, why do we intuitively recognize the default social? Why do we all know the right answer?", "tokens": [3087, 1168, 11, 983, 360, 321, 46506, 5521, 264, 7576, 2093, 30, 1545, 360, 321, 439, 458, 264, 558, 1867, 30], "temperature": 0.0, "avg_logprob": -0.21387544318811216, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.0021747821010649204}, {"id": 392, "seek": 312764, "start": 3141.24, "end": 3147.3199999999997, "text": " Right, our brains are biased. So, this is about implicit biases in our brains.", "tokens": [1779, 11, 527, 15442, 366, 28035, 13, 407, 11, 341, 307, 466, 26947, 32152, 294, 527, 15442, 13], "temperature": 0.0, "avg_logprob": -0.21387544318811216, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.0021747821010649204}, {"id": 393, "seek": 312764, "start": 3148.12, "end": 3153.0, "text": " And this is a very good example. And you can also see how language perpetuates and propagates", "tokens": [400, 341, 307, 257, 588, 665, 1365, 13, 400, 291, 393, 611, 536, 577, 2856, 16211, 27710, 293, 12425, 1024], "temperature": 0.0, "avg_logprob": -0.21387544318811216, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.0021747821010649204}, {"id": 394, "seek": 315300, "start": 3153.0, "end": 3158.6, "text": " biases. Right? It's all in the language. If you could know from one word who is the person who", "tokens": [32152, 13, 1779, 30, 467, 311, 439, 294, 264, 2856, 13, 759, 291, 727, 458, 490, 472, 1349, 567, 307, 264, 954, 567], "temperature": 0.0, "avg_logprob": -0.18743836149877433, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.001113350735977292}, {"id": 395, "seek": 315300, "start": 3158.6, "end": 3166.52, "text": " said it, you can imagine what kind of biases we can extract from a longer text. So, to understand", "tokens": [848, 309, 11, 291, 393, 3811, 437, 733, 295, 32152, 321, 393, 8947, 490, 257, 2854, 2487, 13, 407, 11, 281, 1223], "temperature": 0.0, "avg_logprob": -0.18743836149877433, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.001113350735977292}, {"id": 396, "seek": 315300, "start": 3166.52, "end": 3172.52, "text": " what's happening with biases, we need to understand how cognition works. So, we have, this is", "tokens": [437, 311, 2737, 365, 32152, 11, 321, 643, 281, 1223, 577, 46905, 1985, 13, 407, 11, 321, 362, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.18743836149877433, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.001113350735977292}, {"id": 397, "seek": 315300, "start": 3172.52, "end": 3178.68, "text": " was introduced by Kaliman and Tversky. So, conceptually our brain is divided into system 1,", "tokens": [390, 7268, 538, 12655, 25504, 293, 314, 840, 4133, 13, 407, 11, 3410, 671, 527, 3567, 307, 6666, 666, 1185, 502, 11], "temperature": 0.0, "avg_logprob": -0.18743836149877433, "compression_ratio": 1.56198347107438, "no_speech_prob": 0.001113350735977292}, {"id": 398, "seek": 317868, "start": 3178.68, "end": 3186.3599999999997, "text": " system 2. So, system 1 is our autopilot. It is used to make decisions without thinking. It is", "tokens": [1185, 568, 13, 407, 11, 1185, 502, 307, 527, 31090, 31516, 13, 467, 307, 1143, 281, 652, 5327, 1553, 1953, 13, 467, 307], "temperature": 0.0, "avg_logprob": -0.11972753625167043, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.002923262771219015}, {"id": 399, "seek": 317868, "start": 3186.3599999999997, "end": 3194.04, "text": " very fast parallel effortless and so on. System 2 is our logical part. It knows how to analyze and", "tokens": [588, 2370, 8952, 4630, 1832, 293, 370, 322, 13, 8910, 568, 307, 527, 14978, 644, 13, 467, 3255, 577, 281, 12477, 293], "temperature": 0.0, "avg_logprob": -0.11972753625167043, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.002923262771219015}, {"id": 400, "seek": 317868, "start": 3194.04, "end": 3200.04, "text": " make decisions that are unusual for us. So, it is not automatic and it is slow, serial, controlled.", "tokens": [652, 5327, 300, 366, 10901, 337, 505, 13, 407, 11, 309, 307, 406, 12509, 293, 309, 307, 2964, 11, 17436, 11, 10164, 13], "temperature": 0.0, "avg_logprob": -0.11972753625167043, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.002923262771219015}, {"id": 401, "seek": 320004, "start": 3200.04, "end": 3208.84, "text": " It requires a lot of mental energy. So, our brain constantly receives signals, all kinds of", "tokens": [467, 7029, 257, 688, 295, 4973, 2281, 13, 407, 11, 527, 3567, 6460, 20717, 12354, 11, 439, 3685, 295], "temperature": 0.0, "avg_logprob": -0.15297538704342312, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0013081826036795974}, {"id": 402, "seek": 320004, "start": 3208.84, "end": 3215.32, "text": " from through all the sensors, through eyes, ears. There is a lot of incoming data. There is a lot of", "tokens": [490, 807, 439, 264, 14840, 11, 807, 2575, 11, 8798, 13, 821, 307, 257, 688, 295, 22341, 1412, 13, 821, 307, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.15297538704342312, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0013081826036795974}, {"id": 403, "seek": 320004, "start": 3215.32, "end": 3222.84, "text": " pixels here around me. But the actual part of system 2 is only able to produce a very small portion", "tokens": [18668, 510, 926, 385, 13, 583, 264, 3539, 644, 295, 1185, 568, 307, 787, 1075, 281, 5258, 257, 588, 1359, 8044], "temperature": 0.0, "avg_logprob": -0.15297538704342312, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0013081826036795974}, {"id": 404, "seek": 322284, "start": 3222.84, "end": 3231.4, "text": " of the signals that we received. So, system 1 is automatic. System 2 is effortful, but in practice", "tokens": [295, 264, 12354, 300, 321, 4613, 13, 407, 11, 1185, 502, 307, 12509, 13, 8910, 568, 307, 4630, 906, 11, 457, 294, 3124], "temperature": 0.0, "avg_logprob": -0.12687369939443227, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.0021771022584289312}, {"id": 405, "seek": 322284, "start": 3231.4, "end": 3241.8, "text": " over 95% of the signals that we received from the world is relegated to system 1. And the funny", "tokens": [670, 13420, 4, 295, 264, 12354, 300, 321, 4613, 490, 264, 1002, 307, 2951, 70, 770, 281, 1185, 502, 13, 400, 264, 4074], "temperature": 0.0, "avg_logprob": -0.12687369939443227, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.0021771022584289312}, {"id": 406, "seek": 322284, "start": 3241.8, "end": 3249.56, "text": " thing is Kaliman wrote is that we identify ourselves with system 2. We believe that we are conscious", "tokens": [551, 307, 12655, 25504, 4114, 307, 300, 321, 5876, 4175, 365, 1185, 568, 13, 492, 1697, 300, 321, 366, 6648], "temperature": 0.0, "avg_logprob": -0.12687369939443227, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.0021771022584289312}, {"id": 407, "seek": 324956, "start": 3249.56, "end": 3255.32, "text": " and reasonable beings, but in practice most of our decisions are made by system 1.", "tokens": [293, 10585, 8958, 11, 457, 294, 3124, 881, 295, 527, 5327, 366, 1027, 538, 1185, 502, 13], "temperature": 0.0, "avg_logprob": -0.17408678608555947, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.00035545678110793233}, {"id": 408, "seek": 324956, "start": 3257.96, "end": 3269.0, "text": " So, since we are using autopilot most of our time, our brain gets all the information that we", "tokens": [407, 11, 1670, 321, 366, 1228, 31090, 31516, 881, 295, 527, 565, 11, 527, 3567, 2170, 439, 264, 1589, 300, 321], "temperature": 0.0, "avg_logprob": -0.17408678608555947, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.00035545678110793233}, {"id": 409, "seek": 324956, "start": 3269.0, "end": 3277.48, "text": " perceive, get categorized clusters and labeled automatically. And this is how cognitive stereotypes", "tokens": [20281, 11, 483, 19250, 1602, 23313, 293, 21335, 6772, 13, 400, 341, 307, 577, 15605, 30853], "temperature": 0.0, "avg_logprob": -0.17408678608555947, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.00035545678110793233}, {"id": 410, "seek": 327748, "start": 3277.48, "end": 3287.0, "text": " are created. So, and there are multiple cognitive stereotypes that are aimed to fill the gaps if", "tokens": [366, 2942, 13, 407, 11, 293, 456, 366, 3866, 15605, 30853, 300, 366, 20540, 281, 2836, 264, 15031, 498], "temperature": 0.0, "avg_logprob": -0.14662106357403656, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0016179033555090427}, {"id": 411, "seek": 327748, "start": 3287.0, "end": 3293.56, "text": " we don't have enough meaning or reduce information, generalize if you have too much information,", "tokens": [321, 500, 380, 362, 1547, 3620, 420, 5407, 1589, 11, 2674, 1125, 498, 291, 362, 886, 709, 1589, 11], "temperature": 0.0, "avg_logprob": -0.14662106357403656, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0016179033555090427}, {"id": 412, "seek": 327748, "start": 3294.44, "end": 3301.8, "text": " or to complete the facts if we are missing facts and so on. And this leads all kinds of cognitive", "tokens": [420, 281, 3566, 264, 9130, 498, 321, 366, 5361, 9130, 293, 370, 322, 13, 400, 341, 6689, 439, 3685, 295, 15605], "temperature": 0.0, "avg_logprob": -0.14662106357403656, "compression_ratio": 1.5815217391304348, "no_speech_prob": 0.0016179033555090427}, {"id": 413, "seek": 330180, "start": 3301.8, "end": 3311.32, "text": " biases. So, examples of biases would be in group favoritism. So, we grow up seeing the majority", "tokens": [32152, 13, 407, 11, 5110, 295, 32152, 576, 312, 294, 1594, 2294, 270, 1434, 13, 407, 11, 321, 1852, 493, 2577, 264, 6286], "temperature": 0.0, "avg_logprob": -0.1234483382281135, "compression_ratio": 1.7339901477832513, "no_speech_prob": 0.0026751430705189705}, {"id": 414, "seek": 330180, "start": 3311.32, "end": 3315.4, "text": " of specific people and we tend to like those people more than the minorities.", "tokens": [295, 2685, 561, 293, 321, 3928, 281, 411, 729, 561, 544, 813, 264, 30373, 13], "temperature": 0.0, "avg_logprob": -0.1234483382281135, "compression_ratio": 1.7339901477832513, "no_speech_prob": 0.0026751430705189705}, {"id": 415, "seek": 330180, "start": 3317.0, "end": 3323.0, "text": " How the effect that we know very little about the person or a specific social group,", "tokens": [1012, 264, 1802, 300, 321, 458, 588, 707, 466, 264, 954, 420, 257, 2685, 2093, 1594, 11], "temperature": 0.0, "avg_logprob": -0.1234483382281135, "compression_ratio": 1.7339901477832513, "no_speech_prob": 0.0026751430705189705}, {"id": 416, "seek": 330180, "start": 3323.0, "end": 3329.2400000000002, "text": " but we tend to generalize based on one trait we could generalize about the whole group and to", "tokens": [457, 321, 3928, 281, 2674, 1125, 2361, 322, 472, 22538, 321, 727, 2674, 1125, 466, 264, 1379, 1594, 293, 281], "temperature": 0.0, "avg_logprob": -0.1234483382281135, "compression_ratio": 1.7339901477832513, "no_speech_prob": 0.0026751430705189705}, {"id": 417, "seek": 332924, "start": 3329.24, "end": 3340.68, "text": " other traits. And so, there are many biases. And thanks to these stereotypes, if I", "tokens": [661, 19526, 13, 400, 370, 11, 456, 366, 867, 32152, 13, 400, 3231, 281, 613, 30853, 11, 498, 286], "temperature": 0.0, "avg_logprob": -0.1708619286032284, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.002101101214066148}, {"id": 418, "seek": 332924, "start": 3340.68, "end": 3345.24, "text": " asked you the questions the words, or if I show you these pictures you immediately know", "tokens": [2351, 291, 264, 1651, 264, 2283, 11, 420, 498, 286, 855, 291, 613, 5242, 291, 4258, 458], "temperature": 0.0, "avg_logprob": -0.1708619286032284, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.002101101214066148}, {"id": 419, "seek": 332924, "start": 3345.24, "end": 3351.9599999999996, "text": " this is a calming, cute, tasty, and if I show these pictures you will know that maybe it's", "tokens": [341, 307, 257, 39723, 11, 4052, 11, 11535, 11, 293, 498, 286, 855, 613, 5242, 291, 486, 458, 300, 1310, 309, 311], "temperature": 0.0, "avg_logprob": -0.1708619286032284, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.002101101214066148}, {"id": 420, "seek": 332924, "start": 3351.9599999999996, "end": 3358.6, "text": " dangerous and pleasant and automatically when we see a snake we will not, will automatically step", "tokens": [5795, 293, 16232, 293, 6772, 562, 321, 536, 257, 12650, 321, 486, 406, 11, 486, 6772, 1823], "temperature": 0.0, "avg_logprob": -0.1708619286032284, "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.002101101214066148}, {"id": 421, "seek": 335860, "start": 3358.6, "end": 3367.7999999999997, "text": " down right step back. And we only need to make system to invoke system two for example,", "tokens": [760, 558, 1823, 646, 13, 400, 321, 787, 643, 281, 652, 1185, 281, 41117, 1185, 732, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.13685251772403717, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0016418242594227195}, {"id": 422, "seek": 335860, "start": 3367.7999999999997, "end": 3374.04, "text": " if we decide to touch it. But most of our decisions are automatic and this is the same exactly", "tokens": [498, 321, 4536, 281, 2557, 309, 13, 583, 881, 295, 527, 5327, 366, 12509, 293, 341, 307, 264, 912, 2293], "temperature": 0.0, "avg_logprob": -0.13685251772403717, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0016418242594227195}, {"id": 423, "seek": 335860, "start": 3374.04, "end": 3382.36, "text": " mechanism that creates social stereotypes in our brains. So, we exactly in the same mechanism", "tokens": [7513, 300, 7829, 2093, 30853, 294, 527, 15442, 13, 407, 11, 321, 2293, 294, 264, 912, 7513], "temperature": 0.0, "avg_logprob": -0.13685251772403717, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.0016418242594227195}, {"id": 424, "seek": 338236, "start": 3382.36, "end": 3390.6, "text": " internalize these associations and make generalizations about specific groups.", "tokens": [6920, 1125, 613, 26597, 293, 652, 2674, 14455, 466, 2685, 3935, 13], "temperature": 0.0, "avg_logprob": -0.21512162057976975, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.0004962886450812221}, {"id": 425, "seek": 338236, "start": 3392.1200000000003, "end": 3397.1600000000003, "text": " So, and this is why when I ask you which word is more likely to be spent older person,", "tokens": [407, 11, 293, 341, 307, 983, 562, 286, 1029, 291, 597, 1349, 307, 544, 3700, 281, 312, 4418, 4906, 954, 11], "temperature": 0.0, "avg_logprob": -0.21512162057976975, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.0004962886450812221}, {"id": 426, "seek": 338236, "start": 3397.1600000000003, "end": 3405.6400000000003, "text": " 100% of you typed that the word impressive would be used. And importantly,", "tokens": [2319, 4, 295, 291, 33941, 300, 264, 1349, 8992, 576, 312, 1143, 13, 400, 8906, 11], "temperature": 0.0, "avg_logprob": -0.21512162057976975, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.0004962886450812221}, {"id": 427, "seek": 340564, "start": 3405.64, "end": 3412.92, "text": " these implicit biases are very pervasive and they operate unconsciously. And one important", "tokens": [613, 26947, 32152, 366, 588, 680, 39211, 293, 436, 9651, 18900, 356, 13, 400, 472, 1021], "temperature": 0.0, "avg_logprob": -0.18329602479934692, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.000963912287261337}, {"id": 428, "seek": 340564, "start": 3412.92, "end": 3418.68, "text": " property that they are transitive. So, basically we are seeing that a black person is playing", "tokens": [4707, 300, 436, 366, 1145, 2187, 13, 407, 11, 1936, 321, 366, 2577, 300, 257, 2211, 954, 307, 2433], "temperature": 0.0, "avg_logprob": -0.18329602479934692, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.000963912287261337}, {"id": 429, "seek": 340564, "start": 3418.68, "end": 3424.3599999999997, "text": " basketball and in a movie we see that a black person uses drugs and we immediately connected and", "tokens": [11767, 293, 294, 257, 3169, 321, 536, 300, 257, 2211, 954, 4960, 7766, 293, 321, 4258, 4582, 293], "temperature": 0.0, "avg_logprob": -0.18329602479934692, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.000963912287261337}, {"id": 430, "seek": 340564, "start": 3424.3599999999997, "end": 3433.8799999999997, "text": " reinforce the, for example, the associations with specific groups. And the social stereotypes are", "tokens": [22634, 264, 11, 337, 1365, 11, 264, 26597, 365, 2685, 3935, 13, 400, 264, 2093, 30853, 366], "temperature": 0.0, "avg_logprob": -0.18329602479934692, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.000963912287261337}, {"id": 431, "seek": 343388, "start": 3433.88, "end": 3442.28, "text": " not necessarily all negative. I can name some on the surface positive stereotypes. For example,", "tokens": [406, 4725, 439, 3671, 13, 286, 393, 1315, 512, 322, 264, 3753, 3353, 30853, 13, 1171, 1365, 11], "temperature": 0.0, "avg_logprob": -0.24711151446326304, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0005816429620608687}, {"id": 432, "seek": 343388, "start": 3442.28, "end": 3448.84, "text": " Asians are good and math. Importantly, they all have negative effect even seemingly positive", "tokens": [47724, 366, 665, 293, 5221, 13, 26391, 3627, 11, 436, 439, 362, 3671, 1802, 754, 18709, 3353], "temperature": 0.0, "avg_logprob": -0.24711151446326304, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0005816429620608687}, {"id": 433, "seek": 343388, "start": 3448.84, "end": 3456.52, "text": " stereotypes because they pigeonhole with the individuals and put expectations of them or they can", "tokens": [30853, 570, 436, 37886, 14094, 365, 264, 5346, 293, 829, 9843, 295, 552, 420, 436, 393], "temperature": 0.0, "avg_logprob": -0.24711151446326304, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.0005816429620608687}, {"id": 434, "seek": 345652, "start": 3456.52, "end": 3464.84, "text": " just be harmful. And then how do these biases manifest? They manifest in language. And for example,", "tokens": [445, 312, 19727, 13, 400, 550, 577, 360, 613, 32152, 10067, 30, 814, 10067, 294, 2856, 13, 400, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.10364323231711317, "compression_ratio": 1.6021505376344085, "no_speech_prob": 0.0004898744518868625}, {"id": 435, "seek": 345652, "start": 3464.84, "end": 3474.68, "text": " they manifest in subtle microaggressions. And importantly, microaggressions should not correlate", "tokens": [436, 10067, 294, 13743, 4532, 559, 3091, 626, 13, 400, 8906, 11, 4532, 559, 3091, 626, 820, 406, 48742], "temperature": 0.0, "avg_logprob": -0.10364323231711317, "compression_ratio": 1.6021505376344085, "no_speech_prob": 0.0004898744518868625}, {"id": 436, "seek": 345652, "start": 3474.68, "end": 3482.36, "text": " necessarily with sentiment. So, sentiment analysis tools would not detect them. On the surface level,", "tokens": [4725, 365, 16149, 13, 407, 11, 16149, 5215, 3873, 576, 406, 5531, 552, 13, 1282, 264, 3753, 1496, 11], "temperature": 0.0, "avg_logprob": -0.10364323231711317, "compression_ratio": 1.6021505376344085, "no_speech_prob": 0.0004898744518868625}, {"id": 437, "seek": 348236, "start": 3482.36, "end": 3489.96, "text": " microaggressions can be negative, neutral, or positive, like in these examples. But they actually", "tokens": [4532, 559, 3091, 626, 393, 312, 3671, 11, 10598, 11, 420, 3353, 11, 411, 294, 613, 5110, 13, 583, 436, 767], "temperature": 0.0, "avg_logprob": -0.1596557753426688, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0009456273983232677}, {"id": 438, "seek": 348236, "start": 3489.96, "end": 3496.76, "text": " bring prolonged harms, even if they are meant as a compliment. And there was a lot of research", "tokens": [1565, 41237, 48505, 11, 754, 498, 436, 366, 4140, 382, 257, 16250, 13, 400, 456, 390, 257, 688, 295, 2132], "temperature": 0.0, "avg_logprob": -0.1596557753426688, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0009456273983232677}, {"id": 439, "seek": 348236, "start": 3496.76, "end": 3504.1200000000003, "text": " in social sciences that showed that they can even bring more harms that overt hate speech.", "tokens": [294, 2093, 17677, 300, 4712, 300, 436, 393, 754, 1565, 544, 48505, 300, 17038, 4700, 6218, 13], "temperature": 0.0, "avg_logprob": -0.1596557753426688, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0009456273983232677}, {"id": 440, "seek": 348236, "start": 3504.1200000000003, "end": 3510.28, "text": " Because they, in place, they kind of bring a significant emotional harm and reinforce", "tokens": [1436, 436, 11, 294, 1081, 11, 436, 733, 295, 1565, 257, 4776, 6863, 6491, 293, 22634], "temperature": 0.0, "avg_logprob": -0.1596557753426688, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.0009456273983232677}, {"id": 441, "seek": 351028, "start": 3510.28, "end": 3517.2400000000002, "text": " problematic stereotypes. So, if I will collect these conversations from Twitter, do I look okay?", "tokens": [19011, 30853, 13, 407, 11, 498, 286, 486, 2500, 613, 7315, 490, 5794, 11, 360, 286, 574, 1392, 30], "temperature": 0.0, "avg_logprob": -0.15909742779201932, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.002408684929832816}, {"id": 442, "seek": 351028, "start": 3517.2400000000002, "end": 3522.84, "text": " You are so pretty. Is this a positive or negative? It's probably positive interaction.", "tokens": [509, 366, 370, 1238, 13, 1119, 341, 257, 3353, 420, 3671, 30, 467, 311, 1391, 3353, 9285, 13], "temperature": 0.0, "avg_logprob": -0.15909742779201932, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.002408684929832816}, {"id": 443, "seek": 351028, "start": 3523.6400000000003, "end": 3529.96, "text": " And then the next interaction, check out my new physics paper. Is it positive or negative interaction?", "tokens": [400, 550, 264, 958, 9285, 11, 1520, 484, 452, 777, 10649, 3035, 13, 1119, 309, 3353, 420, 3671, 9285, 30], "temperature": 0.0, "avg_logprob": -0.15909742779201932, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.002408684929832816}, {"id": 444, "seek": 351028, "start": 3531.2400000000002, "end": 3536.44, "text": " Why physics? You're so pretty. So, we don't know. We don't have the right context.", "tokens": [1545, 10649, 30, 509, 434, 370, 1238, 13, 407, 11, 321, 500, 380, 458, 13, 492, 500, 380, 362, 264, 558, 4319, 13], "temperature": 0.0, "avg_logprob": -0.15909742779201932, "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.002408684929832816}, {"id": 445, "seek": 353644, "start": 3536.44, "end": 3543.08, "text": " And then for this question, do I look okay? And all kinds of responses, for example, you are so", "tokens": [400, 550, 337, 341, 1168, 11, 360, 286, 574, 1392, 30, 400, 439, 3685, 295, 13019, 11, 337, 1365, 11, 291, 366, 370], "temperature": 0.0, "avg_logprob": -0.1138175497663782, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0005830663139931858}, {"id": 446, "seek": 353644, "start": 3543.08, "end": 3548.2000000000003, "text": " pretty for your age. In this case, these are negative. These are microaggressions. They make us", "tokens": [1238, 337, 428, 3205, 13, 682, 341, 1389, 11, 613, 366, 3671, 13, 1981, 366, 4532, 559, 3091, 626, 13, 814, 652, 505], "temperature": 0.0, "avg_logprob": -0.1138175497663782, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0005830663139931858}, {"id": 447, "seek": 353644, "start": 3548.2000000000003, "end": 3554.12, "text": " cringe, right? And then the problem is that all of these human generated data, which is", "tokens": [47081, 11, 558, 30, 400, 550, 264, 1154, 307, 300, 439, 295, 613, 1952, 10833, 1412, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.1138175497663782, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0005830663139931858}, {"id": 448, "seek": 353644, "start": 3554.12, "end": 3560.36, "text": " necessarily and incorporates a lot of microaggressions. And just stereotypes that we all have,", "tokens": [4725, 293, 50193, 257, 688, 295, 4532, 559, 3091, 626, 13, 400, 445, 30853, 300, 321, 439, 362, 11], "temperature": 0.0, "avg_logprob": -0.1138175497663782, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0005830663139931858}, {"id": 449, "seek": 356036, "start": 3560.36, "end": 3570.04, "text": " we are not aware of them, it is fed to our systems. So, there is a lot of bias in language,", "tokens": [321, 366, 406, 3650, 295, 552, 11, 309, 307, 4636, 281, 527, 3652, 13, 407, 11, 456, 307, 257, 688, 295, 12577, 294, 2856, 11], "temperature": 0.0, "avg_logprob": -0.1652056750129251, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.000738501432351768}, {"id": 450, "seek": 356036, "start": 3570.92, "end": 3576.92, "text": " stereotypes or historical biases that are perpetuated. For example, there are more photos of", "tokens": [30853, 420, 8584, 32152, 300, 366, 16211, 27275, 13, 1171, 1365, 11, 456, 366, 544, 5787, 295], "temperature": 0.0, "avg_logprob": -0.1652056750129251, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.000738501432351768}, {"id": 451, "seek": 356036, "start": 3577.6400000000003, "end": 3585.96, "text": " men, doctors than female doctors on the web, or human reporting biases. And later,", "tokens": [1706, 11, 8778, 813, 6556, 8778, 322, 264, 3670, 11, 420, 1952, 10031, 32152, 13, 400, 1780, 11], "temperature": 0.0, "avg_logprob": -0.1652056750129251, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.000738501432351768}, {"id": 452, "seek": 358596, "start": 3585.96, "end": 3592.28, "text": " there are biases also in our data sets. So, for example, what kind of data is sample for annotation", "tokens": [456, 366, 32152, 611, 294, 527, 1412, 6352, 13, 407, 11, 337, 1365, 11, 437, 733, 295, 1412, 307, 6889, 337, 48654], "temperature": 0.0, "avg_logprob": -0.12688779830932617, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.0004754268447868526}, {"id": 453, "seek": 358596, "start": 3592.28, "end": 3597.8, "text": " from which kind of populations, from which language varieties, from which locations?", "tokens": [490, 597, 733, 295, 12822, 11, 490, 597, 2856, 22092, 11, 490, 597, 9253, 30], "temperature": 0.0, "avg_logprob": -0.12688779830932617, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.0004754268447868526}, {"id": 454, "seek": 358596, "start": 3597.8, "end": 3603.56, "text": " And then who are we choosing as annotators? So, there is a bias in who are annotators that will", "tokens": [400, 550, 567, 366, 321, 10875, 382, 25339, 3391, 30, 407, 11, 456, 307, 257, 12577, 294, 567, 366, 25339, 3391, 300, 486], "temperature": 0.0, "avg_logprob": -0.12688779830932617, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.0004754268447868526}, {"id": 455, "seek": 358596, "start": 3603.56, "end": 3610.04, "text": " annotate our data. And then there is bias, cognitive biases of annotators themselves, how they treat,", "tokens": [25339, 473, 527, 1412, 13, 400, 550, 456, 307, 12577, 11, 15605, 32152, 295, 25339, 3391, 2969, 11, 577, 436, 2387, 11], "temperature": 0.0, "avg_logprob": -0.12688779830932617, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.0004754268447868526}, {"id": 456, "seek": 361004, "start": 3610.04, "end": 3617.4, "text": " what is the microaggression and what is not, or and other questions. And all these types of", "tokens": [437, 307, 264, 4532, 559, 13338, 293, 437, 307, 406, 11, 420, 293, 661, 1651, 13, 400, 439, 613, 3467, 295], "temperature": 0.0, "avg_logprob": -0.1281641583110011, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.000749797560274601}, {"id": 457, "seek": 361004, "start": 3617.4, "end": 3624.7599999999998, "text": " biases later propagate into our computational systems. And this is how we get from cognitive bias,", "tokens": [32152, 1780, 48256, 666, 527, 28270, 3652, 13, 400, 341, 307, 577, 321, 483, 490, 15605, 12577, 11], "temperature": 0.0, "avg_logprob": -0.1281641583110011, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.000749797560274601}, {"id": 458, "seek": 361004, "start": 3624.7599999999998, "end": 3631.48, "text": " social cognitive biases to algorithmic biases. Because if you remember system one, a system two,", "tokens": [2093, 15605, 32152, 281, 9284, 299, 32152, 13, 1436, 498, 291, 1604, 1185, 472, 11, 257, 1185, 732, 11], "temperature": 0.0, "avg_logprob": -0.1281641583110011, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.000749797560274601}, {"id": 459, "seek": 363148, "start": 3631.48, "end": 3639.16, "text": " currently the way we develop systems, AI is only system one. And why is that? Because", "tokens": [4362, 264, 636, 321, 1499, 3652, 11, 7318, 307, 787, 1185, 472, 13, 400, 983, 307, 300, 30, 1436], "temperature": 0.0, "avg_logprob": -0.1284933884938558, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.0004826442454941571}, {"id": 460, "seek": 363148, "start": 3641.32, "end": 3648.84, "text": " currently the way we develop our tools, the dominant paradigm, is a data-centric approach.", "tokens": [4362, 264, 636, 321, 1499, 527, 3873, 11, 264, 15657, 24709, 11, 307, 257, 1412, 12, 45300, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1284933884938558, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.0004826442454941571}, {"id": 461, "seek": 363148, "start": 3648.84, "end": 3655.64, "text": " So, we need a lot of data to train good models. And we do know well how to leverage a lot of data.", "tokens": [407, 11, 321, 643, 257, 688, 295, 1412, 281, 3847, 665, 5245, 13, 400, 321, 360, 458, 731, 577, 281, 13982, 257, 688, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1284933884938558, "compression_ratio": 1.608187134502924, "no_speech_prob": 0.0004826442454941571}, {"id": 462, "seek": 365564, "start": 3655.64, "end": 3664.6, "text": " But again, language is about people. It is produced by people. But our existing systems, they", "tokens": [583, 797, 11, 2856, 307, 466, 561, 13, 467, 307, 7126, 538, 561, 13, 583, 527, 6741, 3652, 11, 436], "temperature": 0.0, "avg_logprob": -0.2001183032989502, "compression_ratio": 1.5149700598802396, "no_speech_prob": 0.0022714617662131786}, {"id": 463, "seek": 365564, "start": 3667.4, "end": 3673.0, "text": " do not leverage social cultural context. We don't know how to incorporate", "tokens": [360, 406, 13982, 2093, 6988, 4319, 13, 492, 500, 380, 458, 577, 281, 16091], "temperature": 0.0, "avg_logprob": -0.2001183032989502, "compression_ratio": 1.5149700598802396, "no_speech_prob": 0.0022714617662131786}, {"id": 464, "seek": 365564, "start": 3674.2, "end": 3681.0, "text": " and we don't do it usually. We don't incorporate which social biases are positive and", "tokens": [293, 321, 500, 380, 360, 309, 2673, 13, 492, 500, 380, 16091, 597, 2093, 32152, 366, 3353, 293], "temperature": 0.0, "avg_logprob": -0.2001183032989502, "compression_ratio": 1.5149700598802396, "no_speech_prob": 0.0022714617662131786}, {"id": 465, "seek": 368100, "start": 3681.0, "end": 3686.68, "text": " which inductive biases are good and which inductive biases are bad to have. So overall,", "tokens": [597, 31612, 488, 32152, 366, 665, 293, 597, 31612, 488, 32152, 366, 1578, 281, 362, 13, 407, 4787, 11], "temperature": 0.0, "avg_logprob": -0.17222243088942307, "compression_ratio": 1.9292929292929293, "no_speech_prob": 0.005606716498732567}, {"id": 466, "seek": 368100, "start": 3688.28, "end": 3694.04, "text": " our models are really powerful and they are powerful at making generalizations. But we don't know", "tokens": [527, 5245, 366, 534, 4005, 293, 436, 366, 4005, 412, 1455, 2674, 14455, 13, 583, 321, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.17222243088942307, "compression_ratio": 1.9292929292929293, "no_speech_prob": 0.005606716498732567}, {"id": 467, "seek": 368100, "start": 3694.04, "end": 3701.16, "text": " how to control for the right inductive biases, which biases are good and which inductive biases are not.", "tokens": [577, 281, 1969, 337, 264, 558, 31612, 488, 32152, 11, 597, 32152, 366, 665, 293, 597, 31612, 488, 32152, 366, 406, 13], "temperature": 0.0, "avg_logprob": -0.17222243088942307, "compression_ratio": 1.9292929292929293, "no_speech_prob": 0.005606716498732567}, {"id": 468, "seek": 368100, "start": 3701.72, "end": 3708.28, "text": " And then going to the next point is that these models are opaque. We don't also know how to", "tokens": [400, 550, 516, 281, 264, 958, 935, 307, 300, 613, 5245, 366, 42687, 13, 492, 500, 380, 611, 458, 577, 281], "temperature": 0.0, "avg_logprob": -0.17222243088942307, "compression_ratio": 1.9292929292929293, "no_speech_prob": 0.005606716498732567}, {"id": 469, "seek": 370828, "start": 3708.28, "end": 3715.0800000000004, "text": " interpret well deep learning networks, which means it's not easy to analyze them and spot the", "tokens": [7302, 731, 2452, 2539, 9590, 11, 597, 1355, 309, 311, 406, 1858, 281, 12477, 552, 293, 4008, 264], "temperature": 0.0, "avg_logprob": -0.16516690439992138, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0013230547774583101}, {"id": 470, "seek": 370828, "start": 3715.0800000000004, "end": 3722.6800000000003, "text": " problems. So, and as you can guess, this is not only related to the field of ethical and LPE.", "tokens": [2740, 13, 407, 11, 293, 382, 291, 393, 2041, 11, 341, 307, 406, 787, 4077, 281, 264, 2519, 295, 18890, 293, 441, 5208, 13], "temperature": 0.0, "avg_logprob": -0.16516690439992138, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0013230547774583101}, {"id": 471, "seek": 370828, "start": 3722.6800000000003, "end": 3727.88, "text": " These are just interesting research questions. How to incorporate social and cultural", "tokens": [1981, 366, 445, 1880, 2132, 1651, 13, 1012, 281, 16091, 2093, 293, 6988], "temperature": 0.0, "avg_logprob": -0.16516690439992138, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0013230547774583101}, {"id": 472, "seek": 370828, "start": 3727.88, "end": 3734.36, "text": " knowledge into deep learning models or how to develop interpretation approaches.", "tokens": [3601, 666, 2452, 2539, 5245, 420, 577, 281, 1499, 14174, 11587, 13], "temperature": 0.0, "avg_logprob": -0.16516690439992138, "compression_ratio": 1.5526315789473684, "no_speech_prob": 0.0013230547774583101}, {"id": 473, "seek": 373436, "start": 3734.36, "end": 3747.4, "text": " So, what is missing? Today, what is missing, for example, is that existing classifiers for", "tokens": [407, 11, 437, 307, 5361, 30, 2692, 11, 437, 307, 5361, 11, 337, 1365, 11, 307, 300, 6741, 1508, 23463, 337], "temperature": 0.0, "avg_logprob": -0.14718674922334976, "compression_ratio": 1.521505376344086, "no_speech_prob": 0.0020622932352125645}, {"id": 474, "seek": 373436, "start": 3747.4, "end": 3754.84, "text": " toxicity detection, if we want to build data analytics to clean up our data before it propagates", "tokens": [45866, 17784, 11, 498, 321, 528, 281, 1322, 1412, 15370, 281, 2541, 493, 527, 1412, 949, 309, 12425, 1024], "temperature": 0.0, "avg_logprob": -0.14718674922334976, "compression_ratio": 1.521505376344086, "no_speech_prob": 0.0020622932352125645}, {"id": 475, "seek": 373436, "start": 3754.84, "end": 3762.52, "text": " to the models, we know only how to detect overt toxic language, such as hate speech. Because we", "tokens": [281, 264, 5245, 11, 321, 458, 787, 577, 281, 5531, 17038, 12786, 2856, 11, 1270, 382, 4700, 6218, 13, 1436, 321], "temperature": 0.0, "avg_logprob": -0.14718674922334976, "compression_ratio": 1.521505376344086, "no_speech_prob": 0.0020622932352125645}, {"id": 476, "seek": 376252, "start": 3762.52, "end": 3769.4, "text": " are primarily sampling our data and training our data based on lexicans. And there is almost no", "tokens": [366, 10029, 21179, 527, 1412, 293, 3097, 527, 1412, 2361, 322, 476, 87, 34332, 13, 400, 456, 307, 1920, 572], "temperature": 0.0, "avg_logprob": -0.1904261473453406, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.000588484457693994}, {"id": 477, "seek": 376252, "start": 3769.4, "end": 3778.68, "text": " focus on actual microaggressions and most subtle biases, which are often not in words, but in", "tokens": [1879, 322, 3539, 4532, 559, 3091, 626, 293, 881, 13743, 32152, 11, 597, 366, 2049, 406, 294, 2283, 11, 457, 294], "temperature": 0.0, "avg_logprob": -0.1904261473453406, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.000588484457693994}, {"id": 478, "seek": 376252, "start": 3778.68, "end": 3784.52, "text": " pragmatics, the conversation and understanding who are the people involved in the conversation.", "tokens": [33394, 15677, 1167, 11, 264, 3761, 293, 3701, 567, 366, 264, 561, 3288, 294, 264, 3761, 13], "temperature": 0.0, "avg_logprob": -0.1904261473453406, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.000588484457693994}, {"id": 479, "seek": 378452, "start": 3784.52, "end": 3793.4, "text": " So, today's tools that could be applied to this kind of microaggressions or hate speech detection", "tokens": [407, 11, 965, 311, 3873, 300, 727, 312, 6456, 281, 341, 733, 295, 4532, 559, 3091, 626, 420, 4700, 6218, 17784], "temperature": 0.0, "avg_logprob": -0.14881270481989933, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.001506095752120018}, {"id": 480, "seek": 378452, "start": 3793.4, "end": 3801.64, "text": " or sentiment analysis, but they will necessarily fail. The next point is that, again, our", "tokens": [420, 16149, 5215, 11, 457, 436, 486, 4725, 3061, 13, 440, 958, 935, 307, 300, 11, 797, 11, 527], "temperature": 0.0, "avg_logprob": -0.14881270481989933, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.001506095752120018}, {"id": 481, "seek": 378452, "start": 3801.64, "end": 3810.12, "text": " models do not incorporate social cultural knowledge. And basically, the same comment can be toxic", "tokens": [5245, 360, 406, 16091, 2093, 6988, 3601, 13, 400, 1936, 11, 264, 912, 2871, 393, 312, 12786], "temperature": 0.0, "avg_logprob": -0.14881270481989933, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.001506095752120018}, {"id": 482, "seek": 381012, "start": 3810.12, "end": 3816.7599999999998, "text": " or non-toxic depending on who are the people involved in the conversation. But our models are", "tokens": [420, 2107, 12, 1353, 47228, 5413, 322, 567, 366, 264, 561, 3288, 294, 264, 3761, 13, 583, 527, 5245, 366], "temperature": 0.0, "avg_logprob": -0.1381537709917341, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.0015199912013486028}, {"id": 483, "seek": 381012, "start": 3816.7599999999998, "end": 3826.2799999999997, "text": " data-centric and not people-centric. And the more general problem is that the deep learning models", "tokens": [1412, 12, 45300, 293, 406, 561, 12, 45300, 13, 400, 264, 544, 2674, 1154, 307, 300, 264, 2452, 2539, 5245], "temperature": 0.0, "avg_logprob": -0.1381537709917341, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.0015199912013486028}, {"id": 484, "seek": 381012, "start": 3827.3199999999997, "end": 3834.8399999999997, "text": " really go to the picking spurious correlations. And this is why, for example, in this paper,", "tokens": [534, 352, 281, 264, 8867, 637, 24274, 13983, 763, 13, 400, 341, 307, 983, 11, 337, 1365, 11, 294, 341, 3035, 11], "temperature": 0.0, "avg_logprob": -0.1381537709917341, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.0015199912013486028}, {"id": 485, "seek": 383484, "start": 3834.84, "end": 3842.92, "text": " the three comments, which change, which the only difference in these three sentences is the name", "tokens": [264, 1045, 3053, 11, 597, 1319, 11, 597, 264, 787, 2649, 294, 613, 1045, 16579, 307, 264, 1315], "temperature": 0.0, "avg_logprob": -0.20260738191150485, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.0014565637102350593}, {"id": 486, "seek": 383484, "start": 3842.92, "end": 3852.1200000000003, "text": " and probably the association of this name with race or ethnicity. So our models do", "tokens": [293, 1391, 264, 14598, 295, 341, 1315, 365, 4569, 420, 33774, 13, 407, 527, 5245, 360], "temperature": 0.0, "avg_logprob": -0.20260738191150485, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.0014565637102350593}, {"id": 487, "seek": 383484, "start": 3852.1200000000003, "end": 3859.1600000000003, "text": " pick up on spurious confounds. So we think we predict sentiment, but we also predict all kinds", "tokens": [1888, 493, 322, 637, 24274, 1497, 4432, 13, 407, 321, 519, 321, 6069, 16149, 11, 457, 321, 611, 6069, 439, 3685], "temperature": 0.0, "avg_logprob": -0.20260738191150485, "compression_ratio": 1.583815028901734, "no_speech_prob": 0.0014565637102350593}, {"id": 488, "seek": 385916, "start": 3859.16, "end": 3865.0, "text": " of labels that correlate with sentiment, but not necessarily a true predictor of sentiment,", "tokens": [295, 16949, 300, 48742, 365, 16149, 11, 457, 406, 4725, 257, 2074, 6069, 284, 295, 16149, 11], "temperature": 0.0, "avg_logprob": -0.15858464182159046, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.00041639679693616927}, {"id": 489, "seek": 385916, "start": 3865.0, "end": 3869.24, "text": " for example, gender or race. And this is something very pervasive.", "tokens": [337, 1365, 11, 7898, 420, 4569, 13, 400, 341, 307, 746, 588, 680, 39211, 13], "temperature": 0.0, "avg_logprob": -0.15858464182159046, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.00041639679693616927}, {"id": 490, "seek": 385916, "start": 3871.3999999999996, "end": 3878.12, "text": " So, and finally, the models are not explainable. So we kind of have these deficiencies just", "tokens": [407, 11, 293, 2721, 11, 264, 5245, 366, 406, 2903, 712, 13, 407, 321, 733, 295, 362, 613, 19248, 31294, 445], "temperature": 0.0, "avg_logprob": -0.15858464182159046, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.00041639679693616927}, {"id": 491, "seek": 385916, "start": 3878.12, "end": 3884.2799999999997, "text": " in core approaches to deep learning. And we have all these data with these data with trained", "tokens": [294, 4965, 11587, 281, 2452, 2539, 13, 400, 321, 362, 439, 613, 1412, 365, 613, 1412, 365, 8895], "temperature": 0.0, "avg_logprob": -0.15858464182159046, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.00041639679693616927}, {"id": 492, "seek": 388428, "start": 3884.28, "end": 3891.0, "text": " conversational agents, personal systems, all kinds of systems. And why do we care now? Because", "tokens": [2615, 1478, 12554, 11, 2973, 3652, 11, 439, 3685, 295, 3652, 13, 400, 983, 360, 321, 1127, 586, 30, 1436], "temperature": 0.0, "avg_logprob": -0.11703696693341757, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00042597472202032804}, {"id": 493, "seek": 388428, "start": 3891.0, "end": 3899.88, "text": " it can bring harms. So what kind of unintended harms it can bring? Here is an example of an", "tokens": [309, 393, 1565, 48505, 13, 407, 437, 733, 295, 49902, 48505, 309, 393, 1565, 30, 1692, 307, 364, 1365, 295, 364], "temperature": 0.0, "avg_logprob": -0.11703696693341757, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00042597472202032804}, {"id": 494, "seek": 388428, "start": 3899.88, "end": 3905.2400000000002, "text": " image search. If you search for three black teenagers, and this is I searched for it when I prepared", "tokens": [3256, 3164, 13, 759, 291, 3164, 337, 1045, 2211, 23618, 11, 293, 341, 307, 286, 22961, 337, 309, 562, 286, 4927], "temperature": 0.0, "avg_logprob": -0.11703696693341757, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00042597472202032804}, {"id": 495, "seek": 388428, "start": 3905.2400000000002, "end": 3911.8, "text": " this talk for the first time. So it was fixed, I guess, but this is how it was in June 2017.", "tokens": [341, 751, 337, 264, 700, 565, 13, 407, 309, 390, 6806, 11, 286, 2041, 11, 457, 341, 307, 577, 309, 390, 294, 6928, 6591, 13], "temperature": 0.0, "avg_logprob": -0.11703696693341757, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.00042597472202032804}, {"id": 496, "seek": 391180, "start": 3911.8, "end": 3920.44, "text": " And then when you search for a doctor, you get primarily male doctors, right? And", "tokens": [400, 550, 562, 291, 3164, 337, 257, 4631, 11, 291, 483, 10029, 7133, 8778, 11, 558, 30, 400], "temperature": 0.0, "avg_logprob": -0.17023267318953328, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.001105575356632471}, {"id": 497, "seek": 391180, "start": 3920.44, "end": 3926.92, "text": " primarily white. And if you search for a nurse, this is a stereotypical image of a nurse.", "tokens": [10029, 2418, 13, 400, 498, 291, 3164, 337, 257, 14012, 11, 341, 307, 257, 41182, 34061, 3256, 295, 257, 14012, 13], "temperature": 0.0, "avg_logprob": -0.17023267318953328, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.001105575356632471}, {"id": 498, "seek": 391180, "start": 3927.96, "end": 3935.5600000000004, "text": " And if you search for a homemaker, this is just top search results for this query world.", "tokens": [400, 498, 291, 3164, 337, 257, 1280, 18821, 11, 341, 307, 445, 1192, 3164, 3542, 337, 341, 14581, 1002, 13], "temperature": 0.0, "avg_logprob": -0.17023267318953328, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.001105575356632471}, {"id": 499, "seek": 393556, "start": 3935.56, "end": 3942.84, "text": " If you search for CEO, it's a very specific stereotypical image of CEO.", "tokens": [759, 291, 3164, 337, 9282, 11, 309, 311, 257, 588, 2685, 41182, 34061, 3256, 295, 9282, 13], "temperature": 0.0, "avg_logprob": -0.14585028272686582, "compression_ratio": 1.521472392638037, "no_speech_prob": 0.00030626662191934884}, {"id": 500, "seek": 393556, "start": 3944.2799999999997, "end": 3949.64, "text": " And if you search for a professor, this one is my personal favorite. So you can see all", "tokens": [400, 498, 291, 3164, 337, 257, 8304, 11, 341, 472, 307, 452, 2973, 2954, 13, 407, 291, 393, 536, 439], "temperature": 0.0, "avg_logprob": -0.14585028272686582, "compression_ratio": 1.521472392638037, "no_speech_prob": 0.00030626662191934884}, {"id": 501, "seek": 393556, "start": 3951.0, "end": 3956.7599999999998, "text": " images. And there is only one woman. But if you look at her background, you can see this", "tokens": [5267, 13, 400, 456, 307, 787, 472, 3059, 13, 583, 498, 291, 574, 412, 720, 3678, 11, 291, 393, 536, 341], "temperature": 0.0, "avg_logprob": -0.14585028272686582, "compression_ratio": 1.521472392638037, "no_speech_prob": 0.00030626662191934884}, {"id": 502, "seek": 395676, "start": 3956.76, "end": 3965.2400000000002, "text": " is a simple aspect. So it's just an error in search. She is not a professor. And this is a result of,", "tokens": [307, 257, 2199, 4171, 13, 407, 309, 311, 445, 364, 6713, 294, 3164, 13, 1240, 307, 406, 257, 8304, 13, 400, 341, 307, 257, 1874, 295, 11], "temperature": 0.0, "avg_logprob": -0.185695432027181, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0013283300213515759}, {"id": 503, "seek": 395676, "start": 3965.2400000000002, "end": 3974.92, "text": " for example, speech, face recognition. So these are two examples. One camera does not recognize", "tokens": [337, 1365, 11, 6218, 11, 1851, 11150, 13, 407, 613, 366, 732, 5110, 13, 1485, 2799, 775, 406, 5521], "temperature": 0.0, "avg_logprob": -0.185695432027181, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0013283300213515759}, {"id": 504, "seek": 395676, "start": 3974.92, "end": 3982.2000000000003, "text": " Asian faces and things they blinked on the right. The camera, this is a video of face tracking", "tokens": [10645, 8475, 293, 721, 436, 24667, 292, 322, 264, 558, 13, 440, 2799, 11, 341, 307, 257, 960, 295, 1851, 11603], "temperature": 0.0, "avg_logprob": -0.185695432027181, "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.0013283300213515759}, {"id": 505, "seek": 398220, "start": 3982.2, "end": 3989.3199999999997, "text": " camera that is able to track white faces, but immediately shuts down when black face. So it is", "tokens": [2799, 300, 307, 1075, 281, 2837, 2418, 8475, 11, 457, 4258, 48590, 760, 562, 2211, 1851, 13, 407, 309, 307], "temperature": 0.0, "avg_logprob": -0.10909040768941243, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.001710970071144402}, {"id": 506, "seek": 398220, "start": 3989.3199999999997, "end": 3996.9199999999996, "text": " not able to track black faces. So these are all consequences of bias data that propagates into", "tokens": [406, 1075, 281, 2837, 2211, 8475, 13, 407, 613, 366, 439, 10098, 295, 12577, 1412, 300, 12425, 1024, 666], "temperature": 0.0, "avg_logprob": -0.10909040768941243, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.001710970071144402}, {"id": 507, "seek": 398220, "start": 3996.9199999999996, "end": 4004.68, "text": " models that do not incorporate intentionally, basically safeguards against very specific biases.", "tokens": [5245, 300, 360, 406, 16091, 22062, 11, 1936, 32358, 84, 2287, 1970, 588, 2685, 32152, 13], "temperature": 0.0, "avg_logprob": -0.10909040768941243, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.001710970071144402}, {"id": 508, "seek": 398220, "start": 4005.3199999999997, "end": 4010.2799999999997, "text": " Now what's going on with natural language processing? So this is a slide from the very beginning", "tokens": [823, 437, 311, 516, 322, 365, 3303, 2856, 9007, 30, 407, 341, 307, 257, 4137, 490, 264, 588, 2863], "temperature": 0.0, "avg_logprob": -0.10909040768941243, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.001710970071144402}, {"id": 509, "seek": 401028, "start": 4010.28, "end": 4017.4, "text": " that just leads all possible applications that I could think about. As you can guess, since 2016,", "tokens": [300, 445, 6689, 439, 1944, 5821, 300, 286, 727, 519, 466, 13, 1018, 291, 393, 2041, 11, 1670, 6549, 11], "temperature": 0.0, "avg_logprob": -0.1290000978406969, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.0006553024286404252}, {"id": 510, "seek": 401028, "start": 4017.4, "end": 4023.6400000000003, "text": " there are many, many papers that just I don't think there is any application or court", "tokens": [456, 366, 867, 11, 867, 10577, 300, 445, 286, 500, 380, 519, 456, 307, 604, 3861, 420, 4753], "temperature": 0.0, "avg_logprob": -0.1290000978406969, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.0006553024286404252}, {"id": 511, "seek": 401028, "start": 4023.6400000000003, "end": 4031.8, "text": " technologies of NLP left, which did not expose biases in NLP technologies. So here is an example", "tokens": [7943, 295, 426, 45196, 1411, 11, 597, 630, 406, 19219, 32152, 294, 426, 45196, 7943, 13, 407, 510, 307, 364, 1365], "temperature": 0.0, "avg_logprob": -0.1290000978406969, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.0006553024286404252}, {"id": 512, "seek": 401028, "start": 4031.8, "end": 4038.76, "text": " of bias in machine translation. So this is visual. This is why I'm showing it. So there are", "tokens": [295, 12577, 294, 3479, 12853, 13, 407, 341, 307, 5056, 13, 639, 307, 983, 286, 478, 4099, 309, 13, 407, 456, 366], "temperature": 0.0, "avg_logprob": -0.1290000978406969, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.0006553024286404252}, {"id": 513, "seek": 403876, "start": 4038.76, "end": 4046.5200000000004, "text": " languages that mark third person, third person pronouns with gender and other languages that do not", "tokens": [8650, 300, 1491, 2636, 954, 11, 2636, 954, 35883, 365, 7898, 293, 661, 8650, 300, 360, 406], "temperature": 0.0, "avg_logprob": -0.14629378499864024, "compression_ratio": 2.0558659217877095, "no_speech_prob": 0.0011590382782742381}, {"id": 514, "seek": 403876, "start": 4046.5200000000004, "end": 4051.48, "text": " mark third person pronouns with gender. So if you translate from a language such as", "tokens": [1491, 2636, 954, 35883, 365, 7898, 13, 407, 498, 291, 13799, 490, 257, 2856, 1270, 382], "temperature": 0.0, "avg_logprob": -0.14629378499864024, "compression_ratio": 2.0558659217877095, "no_speech_prob": 0.0011590382782742381}, {"id": 515, "seek": 403876, "start": 4052.2000000000003, "end": 4061.0800000000004, "text": " Hungarian or from Estonia that don't mark third person pronoun with gender into English,", "tokens": [38034, 420, 490, 4410, 16999, 300, 500, 380, 1491, 2636, 954, 14144, 365, 7898, 666, 3669, 11], "temperature": 0.0, "avg_logprob": -0.14629378499864024, "compression_ratio": 2.0558659217877095, "no_speech_prob": 0.0011590382782742381}, {"id": 516, "seek": 403876, "start": 4061.0800000000004, "end": 4066.44, "text": " which does mark third person pronoun with a gender, you might see similar results. You will not", "tokens": [597, 775, 1491, 2636, 954, 14144, 365, 257, 7898, 11, 291, 1062, 536, 2531, 3542, 13, 509, 486, 406], "temperature": 0.0, "avg_logprob": -0.14629378499864024, "compression_ratio": 2.0558659217877095, "no_speech_prob": 0.0011590382782742381}, {"id": 517, "seek": 406644, "start": 4066.44, "end": 4075.32, "text": " see them now. This is what was exposed maybe year or two ago. So basically from translation from", "tokens": [536, 552, 586, 13, 639, 307, 437, 390, 9495, 1310, 1064, 420, 732, 2057, 13, 407, 1936, 490, 12853, 490], "temperature": 0.0, "avg_logprob": -0.20368368831681616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.001023853779770434}, {"id": 518, "seek": 406644, "start": 4075.32, "end": 4081.16, "text": " they are the nurse. This would be she's a nurse, but they are the scientists. The translations", "tokens": [436, 366, 264, 14012, 13, 639, 576, 312, 750, 311, 257, 14012, 11, 457, 436, 366, 264, 7708, 13, 440, 37578], "temperature": 0.0, "avg_logprob": -0.20368368831681616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.001023853779770434}, {"id": 519, "seek": 406644, "start": 4081.16, "end": 4088.68, "text": " would be he is a scientist. In saying for engineer, baker, teacher, so all the stereotypes,", "tokens": [576, 312, 415, 307, 257, 12662, 13, 682, 1566, 337, 11403, 11, 48148, 11, 5027, 11, 370, 439, 264, 30853, 11], "temperature": 0.0, "avg_logprob": -0.20368368831681616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.001023853779770434}, {"id": 520, "seek": 406644, "start": 4088.68, "end": 4091.4, "text": " just historical stereotypes that you could think about.", "tokens": [445, 8584, 30853, 300, 291, 727, 519, 466, 13], "temperature": 0.0, "avg_logprob": -0.20368368831681616, "compression_ratio": 1.6865671641791045, "no_speech_prob": 0.001023853779770434}, {"id": 521, "seek": 409140, "start": 4091.4, "end": 4103.64, "text": " So what are possibilities to fix it? One way to fix it is actually simple. You could treat", "tokens": [407, 437, 366, 12178, 281, 3191, 309, 30, 1485, 636, 281, 3191, 309, 307, 767, 2199, 13, 509, 727, 2387], "temperature": 0.0, "avg_logprob": -0.19449566432407925, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.0015320578822866082}, {"id": 522, "seek": 409140, "start": 4103.64, "end": 4112.28, "text": " the target gender just as a target language is multilingual NMT. So I don't know, but I suspect you", "tokens": [264, 3779, 7898, 445, 382, 257, 3779, 2856, 307, 2120, 38219, 426, 44, 51, 13, 407, 286, 500, 380, 458, 11, 457, 286, 9091, 291], "temperature": 0.0, "avg_logprob": -0.19449566432407925, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.0015320578822866082}, {"id": 523, "seek": 409140, "start": 4112.28, "end": 4119.8, "text": " did look at this paper on multilingual neural machine translation. And basically you can add", "tokens": [630, 574, 412, 341, 3035, 322, 2120, 38219, 18161, 3479, 12853, 13, 400, 1936, 291, 393, 909], "temperature": 0.0, "avg_logprob": -0.19449566432407925, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.0015320578822866082}, {"id": 524, "seek": 411980, "start": 4119.8, "end": 4126.6, "text": " another token, for example, and you can controllably generate into female or male translation.", "tokens": [1071, 14862, 11, 337, 1365, 11, 293, 291, 393, 45159, 1188, 8460, 666, 6556, 420, 7133, 12853, 13], "temperature": 0.0, "avg_logprob": -0.13981647269670353, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.0011086114682257175}, {"id": 525, "seek": 411980, "start": 4126.6, "end": 4133.320000000001, "text": " So the fix is not difficult, but you need to be aware of potential dangers to be able to fix", "tokens": [407, 264, 3191, 307, 406, 2252, 11, 457, 291, 643, 281, 312, 3650, 295, 3995, 27701, 281, 312, 1075, 281, 3191], "temperature": 0.0, "avg_logprob": -0.13981647269670353, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.0011086114682257175}, {"id": 526, "seek": 411980, "start": 4133.320000000001, "end": 4139.56, "text": " model. And importantly, this is not about only about fixing the model itself, but also about fixing", "tokens": [2316, 13, 400, 8906, 11, 341, 307, 406, 466, 787, 466, 19442, 264, 2316, 2564, 11, 457, 611, 466, 19442], "temperature": 0.0, "avg_logprob": -0.13981647269670353, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.0011086114682257175}, {"id": 527, "seek": 411980, "start": 4139.56, "end": 4146.52, "text": " the user interface. So the way Google fix this interface is they provided different translations,", "tokens": [264, 4195, 9226, 13, 407, 264, 636, 3329, 3191, 341, 9226, 307, 436, 5649, 819, 37578, 11], "temperature": 0.0, "avg_logprob": -0.13981647269670353, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.0011086114682257175}, {"id": 528, "seek": 414652, "start": 4146.52, "end": 4149.56, "text": " so basically all possible translations for different jenders.", "tokens": [370, 1936, 439, 1944, 37578, 337, 819, 361, 16292, 13], "temperature": 0.0, "avg_logprob": -0.31246139385082106, "compression_ratio": 1.4147727272727273, "no_speech_prob": 0.0007350913947448134}, {"id": 529, "seek": 414652, "start": 4152.92, "end": 4160.84, "text": " So in the similar kind of harms were shown especially in dialogue systems. So occasionally such", "tokens": [407, 294, 264, 2531, 733, 295, 48505, 645, 4898, 2318, 294, 10221, 3652, 13, 407, 16895, 1270], "temperature": 0.0, "avg_logprob": -0.31246139385082106, "compression_ratio": 1.4147727272727273, "no_speech_prob": 0.0007350913947448134}, {"id": 530, "seek": 414652, "start": 4160.84, "end": 4168.76, "text": " models make big headings and news like Microsoft's Tire Chatbot that became very racist and", "tokens": [5245, 652, 955, 1378, 1109, 293, 2583, 411, 8116, 311, 314, 621, 27503, 18870, 300, 3062, 588, 16419, 293], "temperature": 0.0, "avg_logprob": -0.31246139385082106, "compression_ratio": 1.4147727272727273, "no_speech_prob": 0.0007350913947448134}, {"id": 531, "seek": 416876, "start": 4168.76, "end": 4177.400000000001, "text": " sexist overnight. And the GPT-3 based models that was offering a suicide advice. And about two", "tokens": [3260, 468, 13935, 13, 400, 264, 26039, 51, 12, 18, 2361, 5245, 300, 390, 8745, 257, 12308, 5192, 13, 400, 466, 732], "temperature": 0.0, "avg_logprob": -0.2061907824348001, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.0009058120776899159}, {"id": 532, "seek": 416876, "start": 4177.400000000001, "end": 4184.280000000001, "text": " weeks ago there was a Korean chatbot that became extremely homophobic very quickly and had to be", "tokens": [3259, 2057, 456, 390, 257, 6933, 5081, 18870, 300, 3062, 4664, 3655, 5317, 22234, 588, 2661, 293, 632, 281, 312], "temperature": 0.0, "avg_logprob": -0.2061907824348001, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.0009058120776899159}, {"id": 533, "seek": 416876, "start": 4184.280000000001, "end": 4190.52, "text": " removed. So these titles come up again, these headlines coming again and again and again.", "tokens": [7261, 13, 407, 613, 12992, 808, 493, 797, 11, 613, 23867, 1348, 797, 293, 797, 293, 797, 13], "temperature": 0.0, "avg_logprob": -0.2061907824348001, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.0009058120776899159}, {"id": 534, "seek": 419052, "start": 4190.52, "end": 4198.6, "text": " And I guess the point here is that what we do in LP today, I call it a reactive approach.", "tokens": [400, 286, 2041, 264, 935, 510, 307, 300, 437, 321, 360, 294, 38095, 965, 11, 286, 818, 309, 257, 28897, 3109, 13], "temperature": 0.0, "avg_logprob": -0.23863408372208877, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.000987385748885572}, {"id": 535, "seek": 419052, "start": 4199.320000000001, "end": 4210.120000000001, "text": " So we have a we expose specific problem, a problem in search, a problem in chatbots, racist chatbots,", "tokens": [407, 321, 362, 257, 321, 19219, 2685, 1154, 11, 257, 1154, 294, 3164, 11, 257, 1154, 294, 5081, 65, 1971, 11, 16419, 5081, 65, 1971, 11], "temperature": 0.0, "avg_logprob": -0.23863408372208877, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.000987385748885572}, {"id": 536, "seek": 419052, "start": 4210.120000000001, "end": 4216.76, "text": " or a problem in machine translation. And then it gives creates bad publicity and then we start", "tokens": [420, 257, 1154, 294, 3479, 12853, 13, 400, 550, 309, 2709, 7829, 1578, 37264, 293, 550, 321, 722], "temperature": 0.0, "avg_logprob": -0.23863408372208877, "compression_ratio": 1.5888888888888888, "no_speech_prob": 0.000987385748885572}, {"id": 537, "seek": 421676, "start": 4216.76, "end": 4223.72, "text": " debicing the models. But it's not necessarily that we need to develop the tools in this way,", "tokens": [3001, 5776, 264, 5245, 13, 583, 309, 311, 406, 4725, 300, 321, 643, 281, 1499, 264, 3873, 294, 341, 636, 11], "temperature": 0.0, "avg_logprob": -0.19233882788455847, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.0010046127717942}, {"id": 538, "seek": 421676, "start": 4223.72, "end": 4230.92, "text": " right? So I hope that kind of in the future we make a paradigm shift, talk the more proactive", "tokens": [558, 30, 407, 286, 1454, 300, 733, 295, 294, 264, 2027, 321, 652, 257, 24709, 5513, 11, 751, 264, 544, 28028], "temperature": 0.0, "avg_logprob": -0.19233882788455847, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.0010046127717942}, {"id": 539, "seek": 421676, "start": 4230.92, "end": 4242.360000000001, "text": " approach. And the specific, what would proactive approach require is, for example,", "tokens": [3109, 13, 400, 264, 2685, 11, 437, 576, 28028, 3109, 3651, 307, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.19233882788455847, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.0010046127717942}, {"id": 540, "seek": 424236, "start": 4242.36, "end": 4252.599999999999, "text": " building new data analytics. So basically, rather than exposing biases, going further up the pipeline", "tokens": [2390, 777, 1412, 15370, 13, 407, 1936, 11, 2831, 813, 33178, 32152, 11, 516, 3052, 493, 264, 15517], "temperature": 0.0, "avg_logprob": -0.1978642200601512, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0005186761263757944}, {"id": 541, "seek": 424236, "start": 4252.599999999999, "end": 4258.12, "text": " and starting actually with the data and building automatic moderators and data analytics that", "tokens": [293, 2891, 767, 365, 264, 1412, 293, 2390, 12509, 10494, 3391, 293, 1412, 15370, 300], "temperature": 0.0, "avg_logprob": -0.1978642200601512, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0005186761263757944}, {"id": 542, "seek": 424236, "start": 4258.12, "end": 4265.48, "text": " can identify problematic texts, dramatic images beyond the workly hate speech. And then incorporating", "tokens": [393, 5876, 19011, 15765, 11, 12023, 5267, 4399, 264, 589, 356, 4700, 6218, 13, 400, 550, 33613], "temperature": 0.0, "avg_logprob": -0.1978642200601512, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0005186761263757944}, {"id": 543, "seek": 426548, "start": 4265.48, "end": 4274.04, "text": " the right inductive biases into the models and understanding what, of understanding, moving from", "tokens": [264, 558, 31612, 488, 32152, 666, 264, 5245, 293, 3701, 437, 11, 295, 3701, 11, 2684, 490], "temperature": 0.0, "avg_logprob": -0.23686995596255897, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0011080179829150438}, {"id": 544, "seek": 426548, "start": 4274.919999999999, "end": 4281.0, "text": " the data center approaches to people center approaches, incorporating social cultural and", "tokens": [264, 1412, 3056, 11587, 281, 561, 3056, 11587, 11, 33613, 2093, 6988, 293], "temperature": 0.0, "avg_logprob": -0.23686995596255897, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0011080179829150438}, {"id": 545, "seek": 426548, "start": 4281.0, "end": 4286.599999999999, "text": " pragmatic knowledge. And in modeling, there are interesting research questions on how to", "tokens": [46904, 3601, 13, 400, 294, 15983, 11, 456, 366, 1880, 2132, 1651, 322, 577, 281], "temperature": 0.0, "avg_logprob": -0.23686995596255897, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0011080179829150438}, {"id": 546, "seek": 428660, "start": 4286.6, "end": 4295.400000000001, "text": " the most furious confounds, but predict only the target label, not necessarily the picking up on", "tokens": [264, 881, 33470, 1497, 4432, 11, 457, 6069, 787, 264, 3779, 7645, 11, 406, 4725, 264, 8867, 493, 322], "temperature": 0.0, "avg_logprob": -0.13233775627322314, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.0005595395341515541}, {"id": 547, "seek": 428660, "start": 4295.400000000001, "end": 4300.92, "text": " spurious correlations. And finally, on building more interpretable models. And importantly,", "tokens": [637, 24274, 13983, 763, 13, 400, 2721, 11, 322, 2390, 544, 7302, 712, 5245, 13, 400, 8906, 11], "temperature": 0.0, "avg_logprob": -0.13233775627322314, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.0005595395341515541}, {"id": 548, "seek": 428660, "start": 4300.92, "end": 4307.320000000001, "text": " these are not orthogonal research directions. So for example, to build good data analytics,", "tokens": [613, 366, 406, 41488, 2132, 11095, 13, 407, 337, 1365, 11, 281, 1322, 665, 1412, 15370, 11], "temperature": 0.0, "avg_logprob": -0.13233775627322314, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.0005595395341515541}, {"id": 549, "seek": 428660, "start": 4308.120000000001, "end": 4315.96, "text": " you necessarily need to maybe have an interpretable model and also be able to incorporate the right", "tokens": [291, 4725, 643, 281, 1310, 362, 364, 7302, 712, 2316, 293, 611, 312, 1075, 281, 16091, 264, 558], "temperature": 0.0, "avg_logprob": -0.13233775627322314, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.0005595395341515541}, {"id": 550, "seek": 431596, "start": 4315.96, "end": 4322.68, "text": " social cultural knowledge, because again, the microaggressions, the text is not necessarily in words.", "tokens": [2093, 6988, 3601, 11, 570, 797, 11, 264, 4532, 559, 3091, 626, 11, 264, 2487, 307, 406, 4725, 294, 2283, 13], "temperature": 0.0, "avg_logprob": -0.20151901245117188, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.001122556976042688}, {"id": 551, "seek": 431596, "start": 4324.52, "end": 4333.96, "text": " So what I was going to do, if I had time, is I was going to show to who case studies,", "tokens": [407, 437, 286, 390, 516, 281, 360, 11, 498, 286, 632, 565, 11, 307, 286, 390, 516, 281, 855, 281, 567, 1389, 5313, 11], "temperature": 0.0, "avg_logprob": -0.20151901245117188, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.001122556976042688}, {"id": 552, "seek": 431596, "start": 4333.96, "end": 4339.88, "text": " from research studies, from my group that specifically focus on these data analytics,", "tokens": [490, 2132, 5313, 11, 490, 452, 1594, 300, 4682, 1879, 322, 613, 1412, 15370, 11], "temperature": 0.0, "avg_logprob": -0.20151901245117188, "compression_ratio": 1.5511363636363635, "no_speech_prob": 0.001122556976042688}, {"id": 553, "seek": 433988, "start": 4339.88, "end": 4348.84, "text": " identifying unsupervised bias or an interpretable model for making hate speech classifiers more robust.", "tokens": [16696, 2693, 12879, 24420, 12577, 420, 364, 7302, 712, 2316, 337, 1455, 4700, 6218, 1508, 23463, 544, 13956, 13], "temperature": 0.0, "avg_logprob": -0.2151273161500365, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0008880289969965816}, {"id": 554, "seek": 433988, "start": 4348.84, "end": 4354.28, "text": " But I will skip it because we are out of time. Good show what you've stored a few minutes,", "tokens": [583, 286, 486, 10023, 309, 570, 321, 366, 484, 295, 565, 13, 2205, 855, 437, 291, 600, 12187, 257, 1326, 2077, 11], "temperature": 0.0, "avg_logprob": -0.2151273161500365, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0008880289969965816}, {"id": 555, "seek": 433988, "start": 4354.28, "end": 4361.64, "text": " you could show one quickly for five minutes. So basically, we are trying to build this green", "tokens": [291, 727, 855, 472, 2661, 337, 1732, 2077, 13, 407, 1936, 11, 321, 366, 1382, 281, 1322, 341, 3092], "temperature": 0.0, "avg_logprob": -0.2151273161500365, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0008880289969965816}, {"id": 556, "seek": 433988, "start": 4361.64, "end": 4368.6, "text": " boxes is what we have today, hate speech or sentiment analysis. But we are trying to build a new", "tokens": [9002, 307, 437, 321, 362, 965, 11, 4700, 6218, 420, 16149, 5215, 13, 583, 321, 366, 1382, 281, 1322, 257, 777], "temperature": 0.0, "avg_logprob": -0.2151273161500365, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0008880289969965816}, {"id": 557, "seek": 436860, "start": 4368.6, "end": 4375.88, "text": " kind of class of models specifically for social bias analysis. And in these models, we would want to", "tokens": [733, 295, 1508, 295, 5245, 4682, 337, 2093, 12577, 5215, 13, 400, 294, 613, 5245, 11, 321, 576, 528, 281], "temperature": 0.0, "avg_logprob": -0.1461074380313649, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.0010732446098700166}, {"id": 558, "seek": 436860, "start": 4375.88, "end": 4381.8, "text": " detect who are the people involved, so who the comment, for example, is directed to if it's", "tokens": [5531, 567, 366, 264, 561, 3288, 11, 370, 567, 264, 2871, 11, 337, 1365, 11, 307, 12898, 281, 498, 309, 311], "temperature": 0.0, "avg_logprob": -0.1461074380313649, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.0010732446098700166}, {"id": 559, "seek": 436860, "start": 4381.8, "end": 4389.64, "text": " a conversational domain. And also to understand what kinds of microaggressions these are. And also to", "tokens": [257, 2615, 1478, 9274, 13, 400, 611, 281, 1223, 437, 3685, 295, 4532, 559, 3091, 626, 613, 366, 13, 400, 611, 281], "temperature": 0.0, "avg_logprob": -0.1461074380313649, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.0010732446098700166}, {"id": 560, "seek": 436860, "start": 4389.64, "end": 4395.88, "text": " maybe generate explanations or interpretations through building more interpretable models.", "tokens": [1310, 8460, 28708, 420, 37547, 807, 2390, 544, 7302, 712, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1461074380313649, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.0010732446098700166}, {"id": 561, "seek": 439588, "start": 4395.88, "end": 4404.76, "text": " And these are the two papers that I was going to talk about. One is an unsupervised approach to", "tokens": [400, 613, 366, 264, 732, 10577, 300, 286, 390, 516, 281, 751, 466, 13, 1485, 307, 364, 2693, 12879, 24420, 3109, 281], "temperature": 0.0, "avg_logprob": -0.1864506224511375, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0010037817992269993}, {"id": 562, "seek": 439588, "start": 4405.56, "end": 4413.400000000001, "text": " detection gender bias. And one is on if we have just a few examples of microaggressions and the", "tokens": [17784, 7898, 12577, 13, 400, 472, 307, 322, 498, 321, 362, 445, 257, 1326, 5110, 295, 4532, 559, 3091, 626, 293, 264], "temperature": 0.0, "avg_logprob": -0.1864506224511375, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0010037817992269993}, {"id": 563, "seek": 439588, "start": 4414.2, "end": 4419.96, "text": " classifier of hate speech, these examples of microaggressions are adversarial examples to the", "tokens": [1508, 9902, 295, 4700, 6218, 11, 613, 5110, 295, 4532, 559, 3091, 626, 366, 17641, 44745, 5110, 281, 264], "temperature": 0.0, "avg_logprob": -0.1864506224511375, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0010037817992269993}, {"id": 564, "seek": 441996, "start": 4419.96, "end": 4426.6, "text": " classifier. The classifier is not able to deal with them. But what we can do, we can focus on", "tokens": [1508, 9902, 13, 440, 1508, 9902, 307, 406, 1075, 281, 2028, 365, 552, 13, 583, 437, 321, 393, 360, 11, 321, 393, 1879, 322], "temperature": 0.0, "avg_logprob": -0.11294372691664585, "compression_ratio": 1.8685446009389672, "no_speech_prob": 0.0014327858807519078}, {"id": 565, "seek": 441996, "start": 4426.6, "end": 4433.32, "text": " interpretability of the classifier and specifically making the classifier understanding for each", "tokens": [7302, 2310, 295, 264, 1508, 9902, 293, 4682, 1455, 264, 1508, 9902, 3701, 337, 1184], "temperature": 0.0, "avg_logprob": -0.11294372691664585, "compression_ratio": 1.8685446009389672, "no_speech_prob": 0.0014327858807519078}, {"id": 566, "seek": 441996, "start": 4433.32, "end": 4442.12, "text": " probing example, which examples in the training data influenced the classifier's decision. So changing", "tokens": [1239, 278, 1365, 11, 597, 5110, 294, 264, 3097, 1412, 15269, 264, 1508, 9902, 311, 3537, 13, 407, 4473], "temperature": 0.0, "avg_logprob": -0.11294372691664585, "compression_ratio": 1.8685446009389672, "no_speech_prob": 0.0014327858807519078}, {"id": 567, "seek": 441996, "start": 4442.12, "end": 4448.92, "text": " the approach to interpretability from interpreting specific salient words in the input of the classifier", "tokens": [264, 3109, 281, 7302, 2310, 490, 37395, 2685, 1845, 1196, 2283, 294, 264, 4846, 295, 264, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.11294372691664585, "compression_ratio": 1.8685446009389672, "no_speech_prob": 0.0014327858807519078}, {"id": 568, "seek": 444892, "start": 4448.92, "end": 4456.28, "text": " into looking at the training data, sorting the training data and identifying which examples were", "tokens": [666, 1237, 412, 264, 3097, 1412, 11, 32411, 264, 3097, 1412, 293, 16696, 597, 5110, 645], "temperature": 0.0, "avg_logprob": -0.173784535506676, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.0007998485816642642}, {"id": 569, "seek": 444892, "start": 4456.28, "end": 4461.72, "text": " most influential for classifier predictions. So using influence functions from example,", "tokens": [881, 22215, 337, 1508, 9902, 21264, 13, 407, 1228, 6503, 6828, 490, 1365, 11], "temperature": 0.0, "avg_logprob": -0.173784535506676, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.0007998485816642642}, {"id": 570, "seek": 444892, "start": 4463.4, "end": 4473.24, "text": " this is a paper that Percy published in 2017. And through this classifier we are able to surface", "tokens": [341, 307, 257, 3035, 300, 46216, 6572, 294, 6591, 13, 400, 807, 341, 1508, 9902, 321, 366, 1075, 281, 3753], "temperature": 0.0, "avg_logprob": -0.173784535506676, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.0007998485816642642}, {"id": 571, "seek": 447324, "start": 4473.24, "end": 4479.16, "text": " microaggressions despite that the classifier makes the wrong prediction. So this is just a high level,", "tokens": [4532, 559, 3091, 626, 7228, 300, 264, 1508, 9902, 1669, 264, 2085, 17630, 13, 407, 341, 307, 445, 257, 1090, 1496, 11], "temperature": 0.0, "avg_logprob": -0.20539007960139094, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.0008808873826637864}, {"id": 572, "seek": 447324, "start": 4479.96, "end": 4487.639999999999, "text": " very high level summary without talking about the actual studies. So I will skip, let me just skip", "tokens": [588, 1090, 1496, 12691, 1553, 1417, 466, 264, 3539, 5313, 13, 407, 286, 486, 10023, 11, 718, 385, 445, 10023], "temperature": 0.0, "avg_logprob": -0.20539007960139094, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.0008808873826637864}, {"id": 573, "seek": 447324, "start": 4490.84, "end": 4496.36, "text": " the actual papers. And the slides are there. I'm happy to discuss later. I just don't want to go", "tokens": [264, 3539, 10577, 13, 400, 264, 9788, 366, 456, 13, 286, 478, 2055, 281, 2248, 1780, 13, 286, 445, 500, 380, 528, 281, 352], "temperature": 0.0, "avg_logprob": -0.20539007960139094, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.0008808873826637864}, {"id": 574, "seek": 449636, "start": 4496.36, "end": 4505.08, "text": " over time. So to summarize, the field of computational ethics is super interesting and there are", "tokens": [670, 565, 13, 407, 281, 20858, 11, 264, 2519, 295, 28270, 19769, 307, 1687, 1880, 293, 456, 366], "temperature": 0.0, "avg_logprob": -0.13418315208121523, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.00082182209007442}, {"id": 575, "seek": 449636, "start": 4505.08, "end": 4511.48, "text": " interesting problems that are technically interesting, challenging. So you don't need to have", "tokens": [1880, 2740, 300, 366, 12120, 1880, 11, 7595, 13, 407, 291, 500, 380, 643, 281, 362], "temperature": 0.0, "avg_logprob": -0.13418315208121523, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.00082182209007442}, {"id": 576, "seek": 449636, "start": 4511.48, "end": 4516.599999999999, "text": " separate kind of important problems and the technical interesting problems. We can work on", "tokens": [4994, 733, 295, 1021, 2740, 293, 264, 6191, 1880, 2740, 13, 492, 393, 589, 322], "temperature": 0.0, "avg_logprob": -0.13418315208121523, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.00082182209007442}, {"id": 577, "seek": 449636, "start": 4516.599999999999, "end": 4521.799999999999, "text": " important problems, which are also technically interesting and focus on important things like", "tokens": [1021, 2740, 11, 597, 366, 611, 12120, 1880, 293, 1879, 322, 1021, 721, 411], "temperature": 0.0, "avg_logprob": -0.13418315208121523, "compression_ratio": 1.9035532994923858, "no_speech_prob": 0.00082182209007442}, {"id": 578, "seek": 452180, "start": 4521.8, "end": 4528.6, "text": " building better, deploring models. And these are interesting subfields. And if some of you are", "tokens": [2390, 1101, 11, 37546, 3662, 5245, 13, 400, 613, 366, 1880, 1422, 7610, 82, 13, 400, 498, 512, 295, 291, 366], "temperature": 0.0, "avg_logprob": -0.19458946104972594, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.0012717177160084248}, {"id": 579, "seek": 452180, "start": 4528.6, "end": 4537.8, "text": " interested in specific projects, so it's just in our course we just put together a presentation", "tokens": [3102, 294, 2685, 4455, 11, 370, 309, 311, 445, 294, 527, 1164, 321, 445, 829, 1214, 257, 5860], "temperature": 0.0, "avg_logprob": -0.19458946104972594, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.0012717177160084248}, {"id": 580, "seek": 452180, "start": 4537.8, "end": 4545.88, "text": " that just summarizes all kinds of possible projects. Thank you very much.", "tokens": [300, 445, 14611, 5660, 439, 3685, 295, 1944, 4455, 13, 1044, 291, 588, 709, 13], "temperature": 0.0, "avg_logprob": -0.19458946104972594, "compression_ratio": 1.4915254237288136, "no_speech_prob": 0.0012717177160084248}, {"id": 581, "seek": 454588, "start": 4545.88, "end": 4554.04, "text": " I wish I could see the audience. This is so weird. Thank you, Julia, for that great talk.", "tokens": [286, 3172, 286, 727, 536, 264, 4034, 13, 639, 307, 370, 3657, 13, 1044, 291, 11, 18551, 11, 337, 300, 869, 751, 13], "temperature": 0.0, "avg_logprob": -0.23153825002173856, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.000784743984695524}, {"id": 582, "seek": 454588, "start": 4555.32, "end": 4563.88, "text": " Yeah, so if people would like to ask some questions to Julia, if you raise your hand,", "tokens": [865, 11, 370, 498, 561, 576, 411, 281, 1029, 512, 1651, 281, 18551, 11, 498, 291, 5300, 428, 1011, 11], "temperature": 0.0, "avg_logprob": -0.23153825002173856, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.000784743984695524}, {"id": 583, "seek": 454588, "start": 4564.68, "end": 4570.2, "text": " we can promote you to be panelists. And I think then we can even have you turn on your cameras if", "tokens": [321, 393, 9773, 291, 281, 312, 20162, 13, 400, 286, 519, 550, 321, 393, 754, 362, 291, 1261, 322, 428, 8622, 498], "temperature": 0.0, "avg_logprob": -0.23153825002173856, "compression_ratio": 1.483695652173913, "no_speech_prob": 0.000784743984695524}, {"id": 584, "seek": 457020, "start": 4570.2, "end": 4578.84, "text": " you want to show you're a real human being. But you know, if we're waiting to see if there are", "tokens": [291, 528, 281, 855, 291, 434, 257, 957, 1952, 885, 13, 583, 291, 458, 11, 498, 321, 434, 3806, 281, 536, 498, 456, 366], "temperature": 0.0, "avg_logprob": -0.1505123261482485, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0002634337288327515}, {"id": 585, "seek": 457020, "start": 4579.4, "end": 4586.2, "text": " people who would like to do that, I mean, there is one question that's outstanding at the moment,", "tokens": [561, 567, 576, 411, 281, 360, 300, 11, 286, 914, 11, 456, 307, 472, 1168, 300, 311, 14485, 412, 264, 1623, 11], "temperature": 0.0, "avg_logprob": -0.1505123261482485, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0002634337288327515}, {"id": 586, "seek": 457020, "start": 4586.2, "end": 4592.2, "text": " which is, do these bots become racist sexists so quickly after exposure to the public,", "tokens": [597, 307, 11, 360, 613, 35410, 1813, 16419, 3260, 1751, 370, 2661, 934, 10420, 281, 264, 1908, 11], "temperature": 0.0, "avg_logprob": -0.1505123261482485, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0002634337288327515}, {"id": 587, "seek": 457020, "start": 4592.2, "end": 4597.16, "text": " due to the public intentionally trying to bias them, or is it that common talk among the public", "tokens": [3462, 281, 264, 1908, 22062, 1382, 281, 12577, 552, 11, 420, 307, 309, 300, 2689, 751, 3654, 264, 1908], "temperature": 0.0, "avg_logprob": -0.1505123261482485, "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0002634337288327515}, {"id": 588, "seek": 459716, "start": 4597.16, "end": 4604.76, "text": " is racist sexists enough to bias any model upon exposure? So I think this is both. But in the case,", "tokens": [307, 16419, 3260, 1751, 1547, 281, 12577, 604, 2316, 3564, 10420, 30, 407, 286, 519, 341, 307, 1293, 13, 583, 294, 264, 1389, 11], "temperature": 0.0, "avg_logprob": -0.2264961809725375, "compression_ratio": 1.4673366834170853, "no_speech_prob": 0.0007053581066429615}, {"id": 589, "seek": 459716, "start": 4604.76, "end": 4611.32, "text": " for example, of tie-bought, the way it was built is it's a continual learning system. So it collects", "tokens": [337, 1365, 11, 295, 7582, 12, 65, 930, 11, 264, 636, 309, 390, 3094, 307, 309, 311, 257, 1421, 901, 2539, 1185, 13, 407, 309, 39897], "temperature": 0.0, "avg_logprob": -0.2264961809725375, "compression_ratio": 1.4673366834170853, "no_speech_prob": 0.0007053581066429615}, {"id": 590, "seek": 459716, "start": 4612.36, "end": 4620.28, "text": " inputs from people and then uses them as training examples to generate forward answers. And", "tokens": [15743, 490, 561, 293, 550, 4960, 552, 382, 3097, 5110, 281, 8460, 2128, 6338, 13, 400], "temperature": 0.0, "avg_logprob": -0.2264961809725375, "compression_ratio": 1.4673366834170853, "no_speech_prob": 0.0007053581066429615}, {"id": 591, "seek": 462028, "start": 4620.28, "end": 4627.4, "text": " people, as usually people, pick up on such things very quickly. And then they intentionally became", "tokens": [561, 11, 382, 2673, 561, 11, 1888, 493, 322, 1270, 721, 588, 2661, 13, 400, 550, 436, 22062, 3062], "temperature": 0.0, "avg_logprob": -0.18797172717194058, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.0012067249044775963}, {"id": 592, "seek": 462028, "start": 4627.4, "end": 4633.639999999999, "text": " racist and the sexist again against the bot. And the bot very quickly learned to just", "tokens": [16419, 293, 264, 3260, 468, 797, 1970, 264, 10592, 13, 400, 264, 10592, 588, 2661, 3264, 281, 445], "temperature": 0.0, "avg_logprob": -0.18797172717194058, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.0012067249044775963}, {"id": 593, "seek": 462028, "start": 4633.639999999999, "end": 4641.48, "text": " meaning the people's behavior. So it was some malicious attempt to turn this bot into racist and sexist.", "tokens": [3620, 264, 561, 311, 5223, 13, 407, 309, 390, 512, 33496, 5217, 281, 1261, 341, 10592, 666, 16419, 293, 3260, 468, 13], "temperature": 0.0, "avg_logprob": -0.18797172717194058, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.0012067249044775963}, {"id": 594, "seek": 464148, "start": 4641.48, "end": 4653.879999999999, "text": " But this is how the model was designed to collect inputs from people, but not monitor the kind of", "tokens": [583, 341, 307, 577, 264, 2316, 390, 4761, 281, 2500, 15743, 490, 561, 11, 457, 406, 6002, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.13914385575514573, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.0010964898392558098}, {"id": 595, "seek": 464148, "start": 4653.879999999999, "end": 4659.639999999999, "text": " sentences that are used or not used in the training data. So this is again going back to the", "tokens": [16579, 300, 366, 1143, 420, 406, 1143, 294, 264, 3097, 1412, 13, 407, 341, 307, 797, 516, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.13914385575514573, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.0010964898392558098}, {"id": 596, "seek": 464148, "start": 4659.639999999999, "end": 4666.44, "text": " discussion of that we actually don't have good analytics. Many of these analytics are just", "tokens": [5017, 295, 300, 321, 767, 500, 380, 362, 665, 15370, 13, 5126, 295, 613, 15370, 366, 445], "temperature": 0.0, "avg_logprob": -0.13914385575514573, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.0010964898392558098}, {"id": 597, "seek": 466644, "start": 4666.44, "end": 4673.799999999999, "text": " at least so, whitelist, they are very, very primitive. It's not very easy to incorporate such", "tokens": [412, 1935, 370, 11, 315, 38906, 468, 11, 436, 366, 588, 11, 588, 28540, 13, 467, 311, 406, 588, 1858, 281, 16091, 1270], "temperature": 0.0, "avg_logprob": -0.35774963042315316, "compression_ratio": 1.5027624309392265, "no_speech_prob": 0.0012985133798792958}, {"id": 598, "seek": 466644, "start": 4673.799999999999, "end": 4682.919999999999, "text": " constraints into generation or the automatic filtering of data. There is another question.", "tokens": [18491, 666, 5125, 420, 264, 12509, 30822, 295, 1412, 13, 821, 307, 1071, 1168, 13], "temperature": 0.0, "avg_logprob": -0.35774963042315316, "compression_ratio": 1.5027624309392265, "no_speech_prob": 0.0012985133798792958}, {"id": 599, "seek": 466644, "start": 4686.12, "end": 4690.919999999999, "text": " Do you want to ask my question? Yes. So I guess you got both questions. Thank you and I", "tokens": [1144, 291, 528, 281, 1029, 452, 1168, 30, 1079, 13, 407, 286, 2041, 291, 658, 1293, 1651, 13, 1044, 291, 293, 286], "temperature": 0.0, "avg_logprob": -0.35774963042315316, "compression_ratio": 1.5027624309392265, "no_speech_prob": 0.0012985133798792958}, {"id": 600, "seek": 469092, "start": 4690.92, "end": 4698.12, "text": " end live people. So you get it. Let's ask the question. Yeah. I'm curious. I may be", "tokens": [917, 1621, 561, 13, 407, 291, 483, 309, 13, 961, 311, 1029, 264, 1168, 13, 865, 13, 286, 478, 6369, 13, 286, 815, 312], "temperature": 0.0, "avg_logprob": -0.3192635882984508, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0008900551474653184}, {"id": 601, "seek": 469092, "start": 4698.12, "end": 4704.36, "text": " going to a little bit more about how you measure your model's performance. Are there actually public", "tokens": [516, 281, 257, 707, 857, 544, 466, 577, 291, 3481, 428, 2316, 311, 3389, 13, 2014, 456, 767, 1908], "temperature": 0.0, "avg_logprob": -0.3192635882984508, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0008900551474653184}, {"id": 602, "seek": 469092, "start": 4704.36, "end": 4712.76, "text": " benchmarked data sets? Are any sort of well defined metrics that you can sort of objectively", "tokens": [18927, 292, 1412, 6352, 30, 2014, 604, 1333, 295, 731, 7642, 16367, 300, 291, 393, 1333, 295, 46067], "temperature": 0.0, "avg_logprob": -0.3192635882984508, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0008900551474653184}, {"id": 603, "seek": 469092, "start": 4712.76, "end": 4718.2, "text": " measure your model's improvements? Are you talking about specifically our papers that I skipped?", "tokens": [3481, 428, 2316, 311, 13797, 30, 2014, 291, 1417, 466, 4682, 527, 10577, 300, 286, 30193, 30], "temperature": 0.0, "avg_logprob": -0.3192635882984508, "compression_ratio": 1.5847457627118644, "no_speech_prob": 0.0008900551474653184}, {"id": 604, "seek": 471820, "start": 4718.2, "end": 4726.679999999999, "text": " I mean, I know it's a very new field and maybe it's harder to define real objective", "tokens": [286, 914, 11, 286, 458, 309, 311, 257, 588, 777, 2519, 293, 1310, 309, 311, 6081, 281, 6964, 957, 10024], "temperature": 0.0, "avg_logprob": -0.3343042373657227, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.00036571320379152894}, {"id": 605, "seek": 471820, "start": 4727.639999999999, "end": 4734.28, "text": " measure of bias. So how do you measure progress in general? Are you also just mentioning this?", "tokens": [3481, 295, 12577, 13, 407, 577, 360, 291, 3481, 4205, 294, 2674, 30, 2014, 291, 611, 445, 18315, 341, 30], "temperature": 0.0, "avg_logprob": -0.3343042373657227, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.00036571320379152894}, {"id": 606, "seek": 471820, "start": 4734.28, "end": 4741.32, "text": " This is a good question. It's very difficult. There is growing body of data sets. For example,", "tokens": [639, 307, 257, 665, 1168, 13, 467, 311, 588, 2252, 13, 821, 307, 4194, 1772, 295, 1412, 6352, 13, 1171, 1365, 11], "temperature": 0.0, "avg_logprob": -0.3343042373657227, "compression_ratio": 1.4293193717277486, "no_speech_prob": 0.00036571320379152894}, {"id": 607, "seek": 474132, "start": 4741.32, "end": 4748.84, "text": " in Yodav, in the Joyce group created the social bias inference corpus. I don't remember what", "tokens": [294, 398, 378, 706, 11, 294, 264, 40044, 1594, 2942, 264, 2093, 12577, 38253, 1181, 31624, 13, 286, 500, 380, 1604, 437], "temperature": 0.0, "avg_logprob": -0.30344651450573557, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.000793861283455044}, {"id": 608, "seek": 474132, "start": 4748.84, "end": 4758.36, "text": " I said as BIC. Overall, the problem of evaluation is actually very difficult. And there are some", "tokens": [286, 848, 382, 363, 2532, 13, 18420, 11, 264, 1154, 295, 13344, 307, 767, 588, 2252, 13, 400, 456, 366, 512], "temperature": 0.0, "avg_logprob": -0.30344651450573557, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.000793861283455044}, {"id": 609, "seek": 474132, "start": 4758.36, "end": 4765.16, "text": " problems in which there are existing evaluation data sets. If you think about hate speech, for example,", "tokens": [2740, 294, 597, 456, 366, 6741, 13344, 1412, 6352, 13, 759, 291, 519, 466, 4700, 6218, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.30344651450573557, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.000793861283455044}, {"id": 610, "seek": 476516, "start": 4765.16, "end": 4775.24, "text": " there are many data sets for training and evaluating performance of hate speech classifiers.", "tokens": [456, 366, 867, 1412, 6352, 337, 3097, 293, 27479, 3389, 295, 4700, 6218, 1508, 23463, 13], "temperature": 0.0, "avg_logprob": -0.1808755789230119, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0007001487538218498}, {"id": 611, "seek": 476516, "start": 4776.44, "end": 4783.88, "text": " But when we think about biases, there are much less. And the big problem here is not easy to", "tokens": [583, 562, 321, 519, 466, 32152, 11, 456, 366, 709, 1570, 13, 400, 264, 955, 1154, 510, 307, 406, 1858, 281], "temperature": 0.0, "avg_logprob": -0.1808755789230119, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0007001487538218498}, {"id": 612, "seek": 476516, "start": 4783.88, "end": 4791.48, "text": " collect such a data set. So if you think, let me actually show why it is difficult to collect a", "tokens": [2500, 1270, 257, 1412, 992, 13, 407, 498, 291, 519, 11, 718, 385, 767, 855, 983, 309, 307, 2252, 281, 2500, 257], "temperature": 0.0, "avg_logprob": -0.1808755789230119, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0007001487538218498}, {"id": 613, "seek": 479148, "start": 4791.48, "end": 4797.719999999999, "text": " data set of say of microaggressions. So an if solution would be to", "tokens": [1412, 992, 295, 584, 295, 4532, 559, 3091, 626, 13, 407, 364, 498, 3827, 576, 312, 281], "temperature": 0.0, "avg_logprob": -0.2725689437363174, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0009418375557288527}, {"id": 614, "seek": 479148, "start": 4802.12, "end": 4808.599999999999, "text": " so if you think about the standard way of data collection, so we would sample some data from the", "tokens": [370, 498, 291, 519, 466, 264, 3832, 636, 295, 1412, 5765, 11, 370, 321, 576, 6889, 512, 1412, 490, 264], "temperature": 0.0, "avg_logprob": -0.2725689437363174, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0009418375557288527}, {"id": 615, "seek": 479148, "start": 4808.599999999999, "end": 4815.639999999999, "text": " internet and we give it to mechanical token, annotators. And then they would analyze is it bias or", "tokens": [4705, 293, 321, 976, 309, 281, 12070, 14862, 11, 25339, 3391, 13, 400, 550, 436, 576, 12477, 307, 309, 12577, 420], "temperature": 0.0, "avg_logprob": -0.2725689437363174, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0009418375557288527}, {"id": 616, "seek": 479148, "start": 4815.639999999999, "end": 4819.959999999999, "text": " not? And we build a supervised specifier. So this is what we cannot do in the case of most subtle", "tokens": [406, 30, 400, 321, 1322, 257, 46533, 1608, 9902, 13, 407, 341, 307, 437, 321, 2644, 360, 294, 264, 1389, 295, 881, 13743], "temperature": 0.0, "avg_logprob": -0.2725689437363174, "compression_ratio": 1.6216216216216217, "no_speech_prob": 0.0009418375557288527}, {"id": 617, "seek": 481996, "start": 4819.96, "end": 4827.4, "text": " biases. First, because we don't have a strong lexical C to sample the right data. Because again,", "tokens": [32152, 13, 2386, 11, 570, 321, 500, 380, 362, 257, 2068, 476, 87, 804, 383, 281, 6889, 264, 558, 1412, 13, 1436, 797, 11], "temperature": 0.0, "avg_logprob": -0.2045961670253588, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.001622654963284731}, {"id": 618, "seek": 481996, "start": 4827.4, "end": 4832.28, "text": " these biases are not in words. What like you just sample from the whole reddit corpus, it's not", "tokens": [613, 32152, 366, 406, 294, 2283, 13, 708, 411, 291, 445, 6889, 490, 264, 1379, 2182, 17975, 1181, 31624, 11, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.2045961670253588, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.001622654963284731}, {"id": 619, "seek": 481996, "start": 4832.28, "end": 4837.4, "text": " clear how to annotate to make it feasible, not too expensive. But more importantly that", "tokens": [1850, 577, 281, 25339, 473, 281, 652, 309, 26648, 11, 406, 886, 5124, 13, 583, 544, 8906, 300], "temperature": 0.0, "avg_logprob": -0.2045961670253588, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.001622654963284731}, {"id": 620, "seek": 481996, "start": 4839.08, "end": 4843.8, "text": " every annotator when you incorporate their own biases, so you actually need very well trained", "tokens": [633, 25339, 1639, 562, 291, 16091, 641, 1065, 32152, 11, 370, 291, 767, 643, 588, 731, 8895], "temperature": 0.0, "avg_logprob": -0.2045961670253588, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.001622654963284731}, {"id": 621, "seek": 484380, "start": 4843.8, "end": 4850.2, "text": " annotators and multiple annotations per sample. So the question of how to create such a data set is", "tokens": [25339, 3391, 293, 3866, 25339, 763, 680, 6889, 13, 407, 264, 1168, 295, 577, 281, 1884, 1270, 257, 1412, 992, 307], "temperature": 0.0, "avg_logprob": -0.12571445883136906, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.001302206888794899}, {"id": 622, "seek": 484380, "start": 4850.2, "end": 4858.6, "text": " very, very difficult. In our study, we collected the so there is a corp website called microaggressions.com", "tokens": [588, 11, 588, 2252, 13, 682, 527, 2979, 11, 321, 11087, 264, 370, 456, 307, 257, 1181, 79, 3144, 1219, 4532, 559, 3091, 626, 13, 1112], "temperature": 0.0, "avg_logprob": -0.12571445883136906, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.001302206888794899}, {"id": 623, "seek": 484380, "start": 4859.4800000000005, "end": 4866.2, "text": " that has self-reported microaggressions. When people actually recall experiences of", "tokens": [300, 575, 2698, 12, 265, 2707, 292, 4532, 559, 3091, 626, 13, 1133, 561, 767, 9901, 5235, 295], "temperature": 0.0, "avg_logprob": -0.12571445883136906, "compression_ratio": 1.5561497326203209, "no_speech_prob": 0.001302206888794899}, {"id": 624, "seek": 486620, "start": 4866.2, "end": 4876.92, "text": " microaggressions against them and they quote them. And this is what we use to evaluate our data.", "tokens": [4532, 559, 3091, 626, 1970, 552, 293, 436, 6513, 552, 13, 400, 341, 307, 437, 321, 764, 281, 13059, 527, 1412, 13], "temperature": 0.0, "avg_logprob": -0.2387078977098652, "compression_ratio": 1.4113475177304964, "no_speech_prob": 0.0003455175319686532}, {"id": 625, "seek": 486620, "start": 4876.92, "end": 4883.48, "text": " But the data collection is as a big problem currently as just modeling.", "tokens": [583, 264, 1412, 5765, 307, 382, 257, 955, 1154, 4362, 382, 445, 15983, 13], "temperature": 0.0, "avg_logprob": -0.2387078977098652, "compression_ratio": 1.4113475177304964, "no_speech_prob": 0.0003455175319686532}, {"id": 626, "seek": 488348, "start": 4883.48, "end": 4892.44, "text": " Do you want to ask any questions?", "tokens": [1144, 291, 528, 281, 1029, 604, 1651, 30], "temperature": 0.0, "avg_logprob": -0.5687621977271103, "compression_ratio": 1.16, "no_speech_prob": 0.0005585351027548313}, {"id": 627, "seek": 488348, "start": 4899.879999999999, "end": 4901.08, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.5687621977271103, "compression_ratio": 1.16, "no_speech_prob": 0.0005585351027548313}, {"id": 628, "seek": 488348, "start": 4904.759999999999, "end": 4906.679999999999, "text": " I don't have any other questions to ask.", "tokens": [286, 500, 380, 362, 604, 661, 1651, 281, 1029, 13], "temperature": 0.0, "avg_logprob": -0.5687621977271103, "compression_ratio": 1.16, "no_speech_prob": 0.0005585351027548313}, {"id": 629, "seek": 490668, "start": 4906.68, "end": 4914.4400000000005, "text": " Oh, sorry. Okay, so I should go on.", "tokens": [876, 11, 2597, 13, 1033, 11, 370, 286, 820, 352, 322, 13], "temperature": 0.0, "avg_logprob": -0.3829659013187184, "compression_ratio": 1.3701657458563536, "no_speech_prob": 0.00016333961684722453}, {"id": 630, "seek": 490668, "start": 4915.88, "end": 4918.04, "text": " Will that pay for me? Yes.", "tokens": [3099, 300, 1689, 337, 385, 30, 1079, 13], "temperature": 0.0, "avg_logprob": -0.3829659013187184, "compression_ratio": 1.3701657458563536, "no_speech_prob": 0.00016333961684722453}, {"id": 631, "seek": 490668, "start": 4918.76, "end": 4925.16, "text": " Yeah, things for the green lecture. It's a very appropriate topic for the sketch lecture.", "tokens": [865, 11, 721, 337, 264, 3092, 7991, 13, 467, 311, 257, 588, 6854, 4829, 337, 264, 12325, 7991, 13], "temperature": 0.0, "avg_logprob": -0.3829659013187184, "compression_ratio": 1.3701657458563536, "no_speech_prob": 0.00016333961684722453}, {"id": 632, "seek": 490668, "start": 4927.4800000000005, "end": 4935.96, "text": " So I took the course CS 182 which introduced many notions of theirness through case studies and", "tokens": [407, 286, 1890, 264, 1164, 9460, 2443, 17, 597, 7268, 867, 35799, 295, 641, 1287, 807, 1389, 5313, 293], "temperature": 0.0, "avg_logprob": -0.3829659013187184, "compression_ratio": 1.3701657458563536, "no_speech_prob": 0.00016333961684722453}, {"id": 633, "seek": 493596, "start": 4935.96, "end": 4941.8, "text": " like assignments. I've been thinking a lot about how to use notions. And of course there was", "tokens": [411, 22546, 13, 286, 600, 668, 1953, 257, 688, 466, 577, 281, 764, 35799, 13, 400, 295, 1164, 456, 390], "temperature": 0.0, "avg_logprob": -0.22949213921269285, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.0002001997927436605}, {"id": 634, "seek": 493596, "start": 4941.8, "end": 4950.28, "text": " like mentioned for the research done by timeberg who showed three different notions of theirness", "tokens": [411, 2835, 337, 264, 2132, 1096, 538, 565, 6873, 567, 4712, 1045, 819, 35799, 295, 641, 1287], "temperature": 0.0, "avg_logprob": -0.22949213921269285, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.0002001997927436605}, {"id": 635, "seek": 493596, "start": 4950.28, "end": 4957.24, "text": " can be simultaneously satisfied. The calibration which is like the probability of outcome given", "tokens": [393, 312, 16561, 11239, 13, 440, 38732, 597, 307, 411, 264, 8482, 295, 9700, 2212], "temperature": 0.0, "avg_logprob": -0.22949213921269285, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.0002001997927436605}, {"id": 636, "seek": 493596, "start": 4957.24, "end": 4963.4, "text": " risk scores, the false positive rate, the false negative rate and not all be like", "tokens": [3148, 13444, 11, 264, 7908, 3353, 3314, 11, 264, 7908, 3671, 3314, 293, 406, 439, 312, 411], "temperature": 0.0, "avg_logprob": -0.22949213921269285, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.0002001997927436605}, {"id": 637, "seek": 496340, "start": 4963.4, "end": 4969.719999999999, "text": " completely independent across protective traits. So if like pass a certain point,", "tokens": [2584, 6695, 2108, 16314, 19526, 13, 407, 498, 411, 1320, 257, 1629, 935, 11], "temperature": 0.0, "avg_logprob": -0.1683098856608073, "compression_ratio": 1.5526315789473684, "no_speech_prob": 9.168936230707914e-05}, {"id": 638, "seek": 496340, "start": 4969.719999999999, "end": 4976.44, "text": " you know, these metrics just become direct trade-offs. Is it the case that fairness becomes", "tokens": [291, 458, 11, 613, 16367, 445, 1813, 2047, 4923, 12, 19231, 13, 1119, 309, 264, 1389, 300, 29765, 3643], "temperature": 0.0, "avg_logprob": -0.1683098856608073, "compression_ratio": 1.5526315789473684, "no_speech_prob": 9.168936230707914e-05}, {"id": 639, "seek": 496340, "start": 4976.44, "end": 4983.0, "text": " subjective after that? And I guess like more generally, you know, in ethics research has", "tokens": [25972, 934, 300, 30, 400, 286, 2041, 411, 544, 5101, 11, 291, 458, 11, 294, 19769, 2132, 575], "temperature": 0.0, "avg_logprob": -0.1683098856608073, "compression_ratio": 1.5526315789473684, "no_speech_prob": 9.168936230707914e-05}, {"id": 640, "seek": 496340, "start": 4983.0, "end": 4989.16, "text": " there been frameworks of creating sort of upper bounds or constraints among these different", "tokens": [456, 668, 29834, 295, 4084, 1333, 295, 6597, 29905, 420, 18491, 3654, 613, 819], "temperature": 0.0, "avg_logprob": -0.1683098856608073, "compression_ratio": 1.5526315789473684, "no_speech_prob": 9.168936230707914e-05}, {"id": 641, "seek": 498916, "start": 4989.16, "end": 4993.4, "text": " metrics. So we sort of measure how close we get to the ideal.", "tokens": [16367, 13, 407, 321, 1333, 295, 3481, 577, 1998, 321, 483, 281, 264, 7157, 13], "temperature": 0.0, "avg_logprob": -0.14552969932556153, "compression_ratio": 1.4971098265895955, "no_speech_prob": 9.178197069559246e-05}, {"id": 642, "seek": 498916, "start": 4995.4, "end": 5001.16, "text": " This is a very difficult question. So right, the fairness research, there actually proves that you", "tokens": [639, 307, 257, 588, 2252, 1168, 13, 407, 558, 11, 264, 29765, 2132, 11, 456, 767, 25019, 300, 291], "temperature": 0.0, "avg_logprob": -0.14552969932556153, "compression_ratio": 1.4971098265895955, "no_speech_prob": 9.178197069559246e-05}, {"id": 643, "seek": 498916, "start": 5001.16, "end": 5014.12, "text": " cannot satisfy both the measures of performance and inclusivity. And this is why they are measured", "tokens": [2644, 19319, 1293, 264, 8000, 295, 3389, 293, 17204, 4253, 13, 400, 341, 307, 983, 436, 366, 12690], "temperature": 0.0, "avg_logprob": -0.14552969932556153, "compression_ratio": 1.4971098265895955, "no_speech_prob": 9.178197069559246e-05}, {"id": 644, "seek": 501412, "start": 5014.12, "end": 5023.96, "text": " separately, false positives, false negatives. And the question is whether it because of this", "tokens": [14759, 11, 7908, 35127, 11, 7908, 40019, 13, 400, 264, 1168, 307, 1968, 309, 570, 295, 341], "temperature": 0.0, "avg_logprob": -0.16357436776161194, "compression_ratio": 1.6975308641975309, "no_speech_prob": 0.0005041630938649178}, {"id": 645, "seek": 501412, "start": 5023.96, "end": 5032.5199999999995, "text": " issue, whether it becomes subjective, it's even bigger, I guess. The question here is even bigger", "tokens": [2734, 11, 1968, 309, 3643, 25972, 11, 309, 311, 754, 3801, 11, 286, 2041, 13, 440, 1168, 510, 307, 754, 3801], "temperature": 0.0, "avg_logprob": -0.16357436776161194, "compression_ratio": 1.6975308641975309, "no_speech_prob": 0.0005041630938649178}, {"id": 646, "seek": 501412, "start": 5032.5199999999995, "end": 5042.5199999999995, "text": " because the question of inclusivity, so it competes with a question of monetization.", "tokens": [570, 264, 1168, 295, 17204, 4253, 11, 370, 309, 2850, 279, 365, 257, 1168, 295, 15556, 2144, 13], "temperature": 0.0, "avg_logprob": -0.16357436776161194, "compression_ratio": 1.6975308641975309, "no_speech_prob": 0.0005041630938649178}, {"id": 647, "seek": 504252, "start": 5042.52, "end": 5049.0, "text": " If you think who are the main owners of data and how they train algorithms, the goal is to", "tokens": [759, 291, 519, 567, 366, 264, 2135, 7710, 295, 1412, 293, 577, 436, 3847, 14642, 11, 264, 3387, 307, 281], "temperature": 0.0, "avg_logprob": -0.15037933228507874, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0010683726286515594}, {"id": 648, "seek": 504252, "start": 5049.0, "end": 5055.8, "text": " basically have better monetization, like who will see this advertisement. But there is a", "tokens": [1936, 362, 1101, 15556, 2144, 11, 411, 567, 486, 536, 341, 31370, 13, 583, 456, 307, 257], "temperature": 0.0, "avg_logprob": -0.15037933228507874, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0010683726286515594}, {"id": 649, "seek": 504252, "start": 5055.8, "end": 5065.0, "text": " competing objective of inclusivity, who will this advertisement reach out to all kinds of populations.", "tokens": [15439, 10024, 295, 17204, 4253, 11, 567, 486, 341, 31370, 2524, 484, 281, 439, 3685, 295, 12822, 13], "temperature": 0.0, "avg_logprob": -0.15037933228507874, "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0010683726286515594}, {"id": 650, "seek": 506500, "start": 5065.0, "end": 5073.4, "text": " And it's not only subjective, there is a kind of clear incentive, for example, in companies,", "tokens": [400, 309, 311, 406, 787, 25972, 11, 456, 307, 257, 733, 295, 1850, 22346, 11, 337, 1365, 11, 294, 3431, 11], "temperature": 0.0, "avg_logprob": -0.23578429944587476, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.0006983589846640825}, {"id": 651, "seek": 506500, "start": 5074.04, "end": 5078.68, "text": " to maximize monetization rather than inclusivity, right, because it's also internal.", "tokens": [281, 19874, 15556, 2144, 2831, 813, 17204, 4253, 11, 558, 11, 570, 309, 311, 611, 6920, 13], "temperature": 0.0, "avg_logprob": -0.23578429944587476, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.0006983589846640825}, {"id": 652, "seek": 506500, "start": 5080.28, "end": 5086.12, "text": " I don't have an easy answer to this. I agree, it can be subjective or can be", "tokens": [286, 500, 380, 362, 364, 1858, 1867, 281, 341, 13, 286, 3986, 11, 309, 393, 312, 25972, 420, 393, 312], "temperature": 0.0, "avg_logprob": -0.23578429944587476, "compression_ratio": 1.4767441860465116, "no_speech_prob": 0.0006983589846640825}, {"id": 653, "seek": 508612, "start": 5086.12, "end": 5095.64, "text": " more than subjective because these objectives are compete. So would you say it's sort of more", "tokens": [544, 813, 25972, 570, 613, 15961, 366, 11831, 13, 407, 576, 291, 584, 309, 311, 1333, 295, 544], "temperature": 0.0, "avg_logprob": -0.32035737718854634, "compression_ratio": 1.6632124352331605, "no_speech_prob": 0.0007033691508695483}, {"id": 654, "seek": 508612, "start": 5096.5199999999995, "end": 5102.28, "text": " the field of ethics research overall is more interdisciplinary and long these answers to", "tokens": [264, 2519, 295, 19769, 2132, 4787, 307, 544, 38280, 293, 938, 613, 6338, 281], "temperature": 0.0, "avg_logprob": -0.32035737718854634, "compression_ratio": 1.6632124352331605, "no_speech_prob": 0.0007033691508695483}, {"id": 655, "seek": 508612, "start": 5103.16, "end": 5107.16, "text": " these questions are more context dependent. It's very context dependent.", "tokens": [613, 1651, 366, 544, 4319, 12334, 13, 467, 311, 588, 4319, 12334, 13], "temperature": 0.0, "avg_logprob": -0.32035737718854634, "compression_ratio": 1.6632124352331605, "no_speech_prob": 0.0007033691508695483}, {"id": 656, "seek": 508612, "start": 5108.2, "end": 5111.64, "text": " Right, it is very context dependent. As I mentioned, for example,", "tokens": [1779, 11, 309, 307, 588, 4319, 12334, 13, 1018, 286, 2835, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.32035737718854634, "compression_ratio": 1.6632124352331605, "no_speech_prob": 0.0007033691508695483}, {"id": 657, "seek": 511164, "start": 5111.64, "end": 5122.68, "text": " the same application in different contexts can be used for good and for bad, right, and different", "tokens": [264, 912, 3861, 294, 819, 30628, 393, 312, 1143, 337, 665, 293, 337, 1578, 11, 558, 11, 293, 819], "temperature": 0.0, "avg_logprob": -0.2075408697128296, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.002370115602388978}, {"id": 658, "seek": 511164, "start": 5122.68, "end": 5128.92, "text": " thresholds on performance can be applied for different types of different settings.", "tokens": [14678, 82, 322, 3389, 393, 312, 6456, 337, 819, 3467, 295, 819, 6257, 13], "temperature": 0.0, "avg_logprob": -0.2075408697128296, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.002370115602388978}, {"id": 659, "seek": 511164, "start": 5130.200000000001, "end": 5136.12, "text": " And also, I really think, like I'm not qualified even to answer this question, right, we should ask", "tokens": [400, 611, 11, 286, 534, 519, 11, 411, 286, 478, 406, 15904, 754, 281, 1867, 341, 1168, 11, 558, 11, 321, 820, 1029], "temperature": 0.0, "avg_logprob": -0.2075408697128296, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.002370115602388978}, {"id": 660, "seek": 513612, "start": 5136.12, "end": 5146.599999999999, "text": " maybe philosopher or expert in policy, right, because eventually I'm I know how to build the", "tokens": [1310, 29805, 420, 5844, 294, 3897, 11, 558, 11, 570, 4728, 286, 478, 286, 458, 577, 281, 1322, 264], "temperature": 0.0, "avg_logprob": -0.24299160883976864, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0018605171935632825}, {"id": 661, "seek": 513612, "start": 5148.04, "end": 5155.8, "text": " tools and I'm trying to like I'm trying to make technologies kind of more ethical, but the", "tokens": [3873, 293, 286, 478, 1382, 281, 411, 286, 478, 1382, 281, 652, 7943, 733, 295, 544, 18890, 11, 457, 264], "temperature": 0.0, "avg_logprob": -0.24299160883976864, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0018605171935632825}, {"id": 662, "seek": 513612, "start": 5155.8, "end": 5161.64, "text": " question of how they are deployed and what are specific decisions it's it's very difficult to", "tokens": [1168, 295, 577, 436, 366, 17826, 293, 437, 366, 2685, 5327, 309, 311, 309, 311, 588, 2252, 281], "temperature": 0.0, "avg_logprob": -0.24299160883976864, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.0018605171935632825}, {"id": 663, "seek": 516164, "start": 5161.64, "end": 5168.52, "text": " control for and to kind of give definite answers about this. Well, here's a number of good", "tokens": [1969, 337, 293, 281, 733, 295, 976, 25131, 6338, 466, 341, 13, 1042, 11, 510, 311, 257, 1230, 295, 665], "temperature": 0.0, "avg_logprob": -0.23262092504608498, "compression_ratio": 1.5528455284552845, "no_speech_prob": 0.00025020004250109196}, {"id": 664, "seek": 516164, "start": 5168.52, "end": 5176.6, "text": " gold question for you from and you can't use that copy of our answer. So I said earlier you", "tokens": [3821, 1168, 337, 291, 490, 293, 291, 393, 380, 764, 300, 5055, 295, 527, 1867, 13, 407, 286, 848, 3071, 291], "temperature": 0.0, "avg_logprob": -0.23262092504608498, "compression_ratio": 1.5528455284552845, "no_speech_prob": 0.00025020004250109196}, {"id": 665, "seek": 516164, "start": 5176.6, "end": 5182.84, "text": " showed the example of AI GADA with the question of why would we want to study this? The author is", "tokens": [4712, 264, 1365, 295, 7318, 460, 45852, 365, 264, 1168, 295, 983, 576, 321, 528, 281, 2979, 341, 30, 440, 3793, 307], "temperature": 0.0, "avg_logprob": -0.23262092504608498, "compression_ratio": 1.5528455284552845, "no_speech_prob": 0.00025020004250109196}, {"id": 666, "seek": 516164, "start": 5182.84, "end": 5188.84, "text": " justified by claiming that given the widespread use of facial recognition, our findings have critical", "tokens": [27808, 538, 19232, 300, 2212, 264, 22679, 764, 295, 15642, 11150, 11, 527, 16483, 362, 4924], "temperature": 0.0, "avg_logprob": -0.23262092504608498, "compression_ratio": 1.5528455284552845, "no_speech_prob": 0.00025020004250109196}, {"id": 667, "seek": 518884, "start": 5188.84, "end": 5194.76, "text": " implications for the protection of civil liberties. Given that some unscrupulous governments may", "tokens": [16602, 337, 264, 6334, 295, 5605, 47241, 13, 18600, 300, 512, 2693, 66, 11976, 6893, 11280, 815], "temperature": 0.0, "avg_logprob": -0.10769516908669774, "compression_ratio": 1.5625, "no_speech_prob": 0.00041604411671869457}, {"id": 668, "seek": 518884, "start": 5194.76, "end": 5200.52, "text": " indeed implement such technology to oppress minorities based on such things as orientation,", "tokens": [6451, 4445, 1270, 2899, 281, 50240, 30373, 2361, 322, 1270, 721, 382, 14764, 11], "temperature": 0.0, "avg_logprob": -0.10769516908669774, "compression_ratio": 1.5625, "no_speech_prob": 0.00041604411671869457}, {"id": 669, "seek": 518884, "start": 5200.52, "end": 5206.04, "text": " do social scientists have an obligation to get ahead of this threat by understanding the", "tokens": [360, 2093, 7708, 362, 364, 20326, 281, 483, 2286, 295, 341, 4734, 538, 3701, 264], "temperature": 0.0, "avg_logprob": -0.10769516908669774, "compression_ratio": 1.5625, "no_speech_prob": 0.00041604411671869457}, {"id": 670, "seek": 518884, "start": 5206.04, "end": 5213.08, "text": " properties of such models? How do we weigh the ethical trade-offs? Oh gosh, now I need to respond", "tokens": [7221, 295, 1270, 5245, 30, 1012, 360, 321, 13843, 264, 18890, 4923, 12, 19231, 30, 876, 6502, 11, 586, 286, 643, 281, 4196], "temperature": 0.0, "avg_logprob": -0.10769516908669774, "compression_ratio": 1.5625, "no_speech_prob": 0.00041604411671869457}, {"id": 671, "seek": 521308, "start": 5213.08, "end": 5222.36, "text": " from the point of view of all social scientists. I don't want to answer philosophical questions,", "tokens": [490, 264, 935, 295, 1910, 295, 439, 2093, 7708, 13, 286, 500, 380, 528, 281, 1867, 25066, 1651, 11], "temperature": 0.0, "avg_logprob": -0.10233955830335617, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0002325275563634932}, {"id": 672, "seek": 521308, "start": 5222.36, "end": 5233.96, "text": " but I kind of I have the answer about maybe a simple answer to why I don't agree with the", "tokens": [457, 286, 733, 295, 286, 362, 264, 1867, 466, 1310, 257, 2199, 1867, 281, 983, 286, 500, 380, 3986, 365, 264], "temperature": 0.0, "avg_logprob": -0.10233955830335617, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0002325275563634932}, {"id": 673, "seek": 521308, "start": 5233.96, "end": 5239.64, "text": " claim of researchers that we need to expose this technology to publish actually this paper to", "tokens": [3932, 295, 10309, 300, 321, 643, 281, 19219, 341, 2899, 281, 11374, 767, 341, 3035, 281], "temperature": 0.0, "avg_logprob": -0.10233955830335617, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0002325275563634932}, {"id": 674, "seek": 523964, "start": 5239.64, "end": 5247.4800000000005, "text": " expose the dangers of this technology. One of them like the knife analogies that I gave is one", "tokens": [19219, 264, 27701, 295, 341, 2899, 13, 1485, 295, 552, 411, 264, 7976, 16660, 530, 300, 286, 2729, 307, 472], "temperature": 0.0, "avg_logprob": -0.16634687235657597, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.0012541132746264338}, {"id": 675, "seek": 523964, "start": 5247.4800000000005, "end": 5255.56, "text": " of the answers. So if you think about similar field, not a similar field, but a similar type of", "tokens": [295, 264, 6338, 13, 407, 498, 291, 519, 466, 2531, 2519, 11, 406, 257, 2531, 2519, 11, 457, 257, 2531, 2010, 295], "temperature": 0.0, "avg_logprob": -0.16634687235657597, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.0012541132746264338}, {"id": 676, "seek": 523964, "start": 5256.200000000001, "end": 5266.360000000001, "text": " interaction in security, right, in cyber security. So it's very common to kind of break the algorithm", "tokens": [9285, 294, 3825, 11, 558, 11, 294, 13411, 3825, 13, 407, 309, 311, 588, 2689, 281, 733, 295, 1821, 264, 9284], "temperature": 0.0, "avg_logprob": -0.16634687235657597, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.0012541132746264338}, {"id": 677, "seek": 526636, "start": 5266.36, "end": 5273.0, "text": " to show its vulnerabilities and then to iteratively fix it. So this is like the approach that researchers", "tokens": [281, 855, 1080, 37633, 293, 550, 281, 17138, 19020, 3191, 309, 13, 407, 341, 307, 411, 264, 3109, 300, 10309], "temperature": 0.0, "avg_logprob": -0.13726767026461087, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0006792383501306176}, {"id": 678, "seek": 526636, "start": 5273.0, "end": 5280.12, "text": " took. Let's show the vulnerabilities that we are able to build this technology to expose its threats.", "tokens": [1890, 13, 961, 311, 855, 264, 37633, 300, 321, 366, 1075, 281, 1322, 341, 2899, 281, 19219, 1080, 14909, 13], "temperature": 0.0, "avg_logprob": -0.13726767026461087, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0006792383501306176}, {"id": 679, "seek": 526636, "start": 5280.92, "end": 5291.4, "text": " But unlike with security field, here the kind of exposure of this technology, publication of", "tokens": [583, 8343, 365, 3825, 2519, 11, 510, 264, 733, 295, 10420, 295, 341, 2899, 11, 19953, 295], "temperature": 0.0, "avg_logprob": -0.13726767026461087, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0006792383501306176}, {"id": 680, "seek": 529140, "start": 5291.4, "end": 5300.2, "text": " this technology can have real implications of human lives. So again, the cost of misclassification.", "tokens": [341, 2899, 393, 362, 957, 16602, 295, 1952, 2909, 13, 407, 797, 11, 264, 2063, 295, 3346, 11665, 3774, 13], "temperature": 0.0, "avg_logprob": -0.21706148982048035, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.0006828812183812261}, {"id": 681, "seek": 529140, "start": 5303.24, "end": 5311.0, "text": " And if you think about other problems, like similar problems, like the problems of, I can give", "tokens": [400, 498, 291, 519, 466, 661, 2740, 11, 411, 2531, 2740, 11, 411, 264, 2740, 295, 11, 286, 393, 976], "temperature": 0.0, "avg_logprob": -0.21706148982048035, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.0006828812183812261}, {"id": 682, "seek": 529140, "start": 5311.0, "end": 5317.16, "text": " many other similar problems in which we can expose this technology which will harm people.", "tokens": [867, 661, 2531, 2740, 294, 597, 321, 393, 19219, 341, 2899, 597, 486, 6491, 561, 13], "temperature": 0.0, "avg_logprob": -0.21706148982048035, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.0006828812183812261}, {"id": 683, "seek": 531716, "start": 5317.16, "end": 5327.32, "text": " Like let's create deep fakes video, a poor video with professors. We can do it right to expose", "tokens": [1743, 718, 311, 1884, 2452, 283, 3419, 960, 11, 257, 4716, 960, 365, 15924, 13, 492, 393, 360, 309, 558, 281, 19219], "temperature": 0.0, "avg_logprob": -0.12864277703421456, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.001455524587072432}, {"id": 684, "seek": 531716, "start": 5327.32, "end": 5334.44, "text": " the danger of technology of deep fakes. But what kind of harm it will bring to specific people", "tokens": [264, 4330, 295, 2899, 295, 2452, 283, 3419, 13, 583, 437, 733, 295, 6491, 309, 486, 1565, 281, 2685, 561], "temperature": 0.0, "avg_logprob": -0.12864277703421456, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.001455524587072432}, {"id": 685, "seek": 531716, "start": 5334.44, "end": 5341.0, "text": " who were involved in this kind of exposure of the harm of this technology? So I have the answer", "tokens": [567, 645, 3288, 294, 341, 733, 295, 10420, 295, 264, 6491, 295, 341, 2899, 30, 407, 286, 362, 264, 1867], "temperature": 0.0, "avg_logprob": -0.12864277703421456, "compression_ratio": 1.5240641711229947, "no_speech_prob": 0.001455524587072432}, {"id": 686, "seek": 534100, "start": 5341.0, "end": 5348.28, "text": " of why it was wrong to publish this study in the first place. And why it's not productive,", "tokens": [295, 983, 309, 390, 2085, 281, 11374, 341, 2979, 294, 264, 700, 1081, 13, 400, 983, 309, 311, 406, 13304, 11], "temperature": 0.0, "avg_logprob": -0.2822940929515942, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.000553131802007556}, {"id": 687, "seek": 534100, "start": 5348.28, "end": 5354.2, "text": " not helpful, but it's very difficult to answer the question, what should social scientists do? I don't", "tokens": [406, 4961, 11, 457, 309, 311, 588, 2252, 281, 1867, 264, 1168, 11, 437, 820, 2093, 7708, 360, 30, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.2822940929515942, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.000553131802007556}, {"id": 688, "seek": 534100, "start": 5354.2, "end": 5364.36, "text": " know. Okay, well I can ask the question next. I was just disappointed. Oh wait, I think can", "tokens": [458, 13, 1033, 11, 731, 286, 393, 1029, 264, 1168, 958, 13, 286, 390, 445, 13856, 13, 876, 1699, 11, 286, 519, 393], "temperature": 0.0, "avg_logprob": -0.2822940929515942, "compression_ratio": 1.4615384615384615, "no_speech_prob": 0.000553131802007556}, {"id": 689, "seek": 536436, "start": 5364.36, "end": 5371.0, "text": " you hear me now? Yeah. Okay, thank you so much for the talk. That's really interesting. And", "tokens": [291, 1568, 385, 586, 30, 865, 13, 1033, 11, 1309, 291, 370, 709, 337, 264, 751, 13, 663, 311, 534, 1880, 13, 400], "temperature": 0.0, "avg_logprob": -0.18849597026392356, "compression_ratio": 1.4959349593495934, "no_speech_prob": 0.0003004439640790224}, {"id": 690, "seek": 536436, "start": 5371.639999999999, "end": 5376.759999999999, "text": " as we've just seen really challenging stuff. I guess my question is a little bit more practical.", "tokens": [382, 321, 600, 445, 1612, 534, 7595, 1507, 13, 286, 2041, 452, 1168, 307, 257, 707, 857, 544, 8496, 13], "temperature": 0.0, "avg_logprob": -0.18849597026392356, "compression_ratio": 1.4959349593495934, "no_speech_prob": 0.0003004439640790224}, {"id": 691, "seek": 536436, "start": 5376.759999999999, "end": 5384.599999999999, "text": " So maybe that's a reprieve for you. But I like unfortunately it seems like in a lot of", "tokens": [407, 1310, 300, 311, 257, 1085, 5469, 303, 337, 291, 13, 583, 286, 411, 7015, 309, 2544, 411, 294, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.18849597026392356, "compression_ratio": 1.4959349593495934, "no_speech_prob": 0.0003004439640790224}, {"id": 692, "seek": 536436, "start": 5385.639999999999, "end": 5391.24, "text": " NLP and AI more broadly. Some of this ethics and bias stuff is like kind of an afterthought.", "tokens": [426, 45196, 293, 7318, 544, 19511, 13, 2188, 295, 341, 19769, 293, 12577, 1507, 307, 411, 733, 295, 364, 934, 43135, 13], "temperature": 0.0, "avg_logprob": -0.18849597026392356, "compression_ratio": 1.4959349593495934, "no_speech_prob": 0.0003004439640790224}, {"id": 693, "seek": 539124, "start": 5391.24, "end": 5397.5599999999995, "text": " A lot of projects don't really necessarily take it into account from the outset and it's more", "tokens": [316, 688, 295, 4455, 500, 380, 534, 4725, 747, 309, 666, 2696, 490, 264, 44618, 293, 309, 311, 544], "temperature": 0.0, "avg_logprob": -0.12659351392225784, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.00015352402988355607}, {"id": 694, "seek": 539124, "start": 5397.5599999999995, "end": 5406.04, "text": " sort of incidental. So my question is like, you know, as we're working on NLP projects maybe even", "tokens": [1333, 295, 9348, 304, 13, 407, 452, 1168, 307, 411, 11, 291, 458, 11, 382, 321, 434, 1364, 322, 426, 45196, 4455, 1310, 754], "temperature": 0.0, "avg_logprob": -0.12659351392225784, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.00015352402988355607}, {"id": 695, "seek": 539124, "start": 5406.04, "end": 5411.639999999999, "text": " our final course project, what are some kind of concrete steps that we can take or like a", "tokens": [527, 2572, 1164, 1716, 11, 437, 366, 512, 733, 295, 9859, 4439, 300, 321, 393, 747, 420, 411, 257], "temperature": 0.0, "avg_logprob": -0.12659351392225784, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.00015352402988355607}, {"id": 696, "seek": 539124, "start": 5411.639999999999, "end": 5418.5199999999995, "text": " systematic approach that we can use to sort of incorporating some of this ethics knowledge in", "tokens": [27249, 3109, 300, 321, 393, 764, 281, 1333, 295, 33613, 512, 295, 341, 19769, 3601, 294], "temperature": 0.0, "avg_logprob": -0.12659351392225784, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.00015352402988355607}, {"id": 697, "seek": 541852, "start": 5418.52, "end": 5422.92, "text": " things that might not explicitly seem like they have a lot to do without the X.", "tokens": [721, 300, 1062, 406, 20803, 1643, 411, 436, 362, 257, 688, 281, 360, 1553, 264, 1783, 13], "temperature": 0.0, "avg_logprob": -0.1423004064986955, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0002434881462249905}, {"id": 698, "seek": 541852, "start": 5427.400000000001, "end": 5434.52, "text": " So it depends on the project, right? It's like I cannot answer generally if the first part of", "tokens": [407, 309, 5946, 322, 264, 1716, 11, 558, 30, 467, 311, 411, 286, 2644, 1867, 5101, 498, 264, 700, 644, 295], "temperature": 0.0, "avg_logprob": -0.1423004064986955, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0002434881462249905}, {"id": 699, "seek": 541852, "start": 5434.52, "end": 5441.4800000000005, "text": " the lecture was exactly about this. If I build my project, what kind of questions I can ask to", "tokens": [264, 7991, 390, 2293, 466, 341, 13, 759, 286, 1322, 452, 1716, 11, 437, 733, 295, 1651, 286, 393, 1029, 281], "temperature": 0.0, "avg_logprob": -0.1423004064986955, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0002434881462249905}, {"id": 700, "seek": 544148, "start": 5441.48, "end": 5451.799999999999, "text": " know if there are some pitfalls. If it's a different project, I think overall,", "tokens": [458, 498, 456, 366, 512, 10147, 18542, 13, 759, 309, 311, 257, 819, 1716, 11, 286, 519, 4787, 11], "temperature": 0.0, "avg_logprob": -0.17658969832629692, "compression_ratio": 1.3622047244094488, "no_speech_prob": 0.0005300281918607652}, {"id": 701, "seek": 544148, "start": 5455.32, "end": 5464.839999999999, "text": " these are important questions which are general for deep learning models, which could be later", "tokens": [613, 366, 1021, 1651, 597, 366, 2674, 337, 2452, 2539, 5245, 11, 597, 727, 312, 1780], "temperature": 0.0, "avg_logprob": -0.17658969832629692, "compression_ratio": 1.3622047244094488, "no_speech_prob": 0.0005300281918607652}, {"id": 702, "seek": 546484, "start": 5464.84, "end": 5472.76, "text": " used to create better technology. So how to incorporate understanding who are the people", "tokens": [1143, 281, 1884, 1101, 2899, 13, 407, 577, 281, 16091, 3701, 567, 366, 264, 561], "temperature": 0.0, "avg_logprob": -0.15678727424750893, "compression_ratio": 1.6091954022988506, "no_speech_prob": 0.0008422433165833354}, {"id": 703, "seek": 546484, "start": 5474.04, "end": 5480.2, "text": " who produce the language or who are the users incorporating the right inductive biases or", "tokens": [567, 5258, 264, 2856, 420, 567, 366, 264, 5022, 33613, 264, 558, 31612, 488, 32152, 420], "temperature": 0.0, "avg_logprob": -0.15678727424750893, "compression_ratio": 1.6091954022988506, "no_speech_prob": 0.0008422433165833354}, {"id": 704, "seek": 546484, "start": 5480.2, "end": 5486.6, "text": " a technology for demoting confounds. It doesn't have to be really like specifically on ethics related", "tokens": [257, 2899, 337, 1371, 17001, 1497, 4432, 13, 467, 1177, 380, 362, 281, 312, 534, 411, 4682, 322, 19769, 4077], "temperature": 0.0, "avg_logprob": -0.15678727424750893, "compression_ratio": 1.6091954022988506, "no_speech_prob": 0.0008422433165833354}, {"id": 705, "seek": 548660, "start": 5486.6, "end": 5495.88, "text": " problems or interpretability of deep learning. It doesn't have to be ethics and of project,", "tokens": [2740, 420, 7302, 2310, 295, 2452, 2539, 13, 467, 1177, 380, 362, 281, 312, 19769, 293, 295, 1716, 11], "temperature": 0.0, "avg_logprob": -0.16117572784423828, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00016701109416317195}, {"id": 706, "seek": 548660, "start": 5495.88, "end": 5501.160000000001, "text": " but the kind of technology, if you can develop such a technology, eventually it can be useful for", "tokens": [457, 264, 733, 295, 2899, 11, 498, 291, 393, 1499, 1270, 257, 2899, 11, 4728, 309, 393, 312, 4420, 337], "temperature": 0.0, "avg_logprob": -0.16117572784423828, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00016701109416317195}, {"id": 707, "seek": 548660, "start": 5501.160000000001, "end": 5509.160000000001, "text": " building such better like productively models that proactively prevent unintended harms.", "tokens": [2390, 1270, 1101, 411, 1674, 3413, 5245, 300, 447, 45679, 4871, 49902, 48505, 13], "temperature": 0.0, "avg_logprob": -0.16117572784423828, "compression_ratio": 1.5191256830601092, "no_speech_prob": 0.00016701109416317195}, {"id": 708, "seek": 550916, "start": 5509.16, "end": 5518.84, "text": " So I guess the strategy would be sort of to just lay out these general topics,", "tokens": [407, 286, 2041, 264, 5206, 576, 312, 1333, 295, 281, 445, 2360, 484, 613, 2674, 8378, 11], "temperature": 0.0, "avg_logprob": -0.1587749481201172, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0003495615965221077}, {"id": 709, "seek": 550916, "start": 5519.5599999999995, "end": 5525.16, "text": " pose these questions to yourself, maybe write them down or just think about them and then proceed", "tokens": [10774, 613, 1651, 281, 1803, 11, 1310, 2464, 552, 760, 420, 445, 519, 466, 552, 293, 550, 8991], "temperature": 0.0, "avg_logprob": -0.1587749481201172, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0003495615965221077}, {"id": 710, "seek": 550916, "start": 5525.16, "end": 5532.599999999999, "text": " as such. Yeah, so I think about it. I think about potential, like if this technology would be", "tokens": [382, 1270, 13, 865, 11, 370, 286, 519, 466, 309, 13, 286, 519, 466, 3995, 11, 411, 498, 341, 2899, 576, 312], "temperature": 0.0, "avg_logprob": -0.1587749481201172, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0003495615965221077}, {"id": 711, "seek": 553260, "start": 5532.6, "end": 5541.96, "text": " deployed, what could be corner cases? What is potential for dual use? If it works, how can it be", "tokens": [17826, 11, 437, 727, 312, 4538, 3331, 30, 708, 307, 3995, 337, 11848, 764, 30, 759, 309, 1985, 11, 577, 393, 309, 312], "temperature": 0.0, "avg_logprob": -0.1338211034799551, "compression_ratio": 1.515625, "no_speech_prob": 0.0003875178808812052}, {"id": 712, "seek": 553260, "start": 5541.96, "end": 5551.64, "text": " misused? And also when it doesn't work, what kind of errors can be harmful? So this is, if you go", "tokens": [3346, 4717, 30, 400, 611, 562, 309, 1177, 380, 589, 11, 437, 733, 295, 13603, 393, 312, 19727, 30, 407, 341, 307, 11, 498, 291, 352], "temperature": 0.0, "avg_logprob": -0.1338211034799551, "compression_ratio": 1.515625, "no_speech_prob": 0.0003875178808812052}, {"id": 713, "seek": 553260, "start": 5551.64, "end": 5557.96, "text": " back to in the slides to the beginning of the lecture, these questions, they could be applied to", "tokens": [646, 281, 294, 264, 9788, 281, 264, 2863, 295, 264, 7991, 11, 613, 1651, 11, 436, 727, 312, 6456, 281], "temperature": 0.0, "avg_logprob": -0.1338211034799551, "compression_ratio": 1.515625, "no_speech_prob": 0.0003875178808812052}, {"id": 714, "seek": 555796, "start": 5557.96, "end": 5565.8, "text": " many kinds of technology and they give a more clear kind of guidelines to what things,", "tokens": [867, 3685, 295, 2899, 293, 436, 976, 257, 544, 1850, 733, 295, 12470, 281, 437, 721, 11], "temperature": 0.0, "avg_logprob": -0.12635738618912234, "compression_ratio": 1.625, "no_speech_prob": 0.0006675728945992887}, {"id": 715, "seek": 555796, "start": 5565.8, "end": 5572.84, "text": " how bad outcomes could be prevented. I'm sure maybe I'm missing something totally, right? It's", "tokens": [577, 1578, 10070, 727, 312, 27314, 13, 286, 478, 988, 1310, 286, 478, 5361, 746, 3879, 11, 558, 30, 467, 311], "temperature": 0.0, "avg_logprob": -0.12635738618912234, "compression_ratio": 1.625, "no_speech_prob": 0.0006675728945992887}, {"id": 716, "seek": 555796, "start": 5573.56, "end": 5581.32, "text": " all of this content, much of this content is just I came up with it by reading a lot of different", "tokens": [439, 295, 341, 2701, 11, 709, 295, 341, 2701, 307, 445, 286, 1361, 493, 365, 309, 538, 3760, 257, 688, 295, 819], "temperature": 0.0, "avg_logprob": -0.12635738618912234, "compression_ratio": 1.625, "no_speech_prob": 0.0006675728945992887}, {"id": 717, "seek": 555796, "start": 5581.32, "end": 5587.4, "text": " papers, but there is no clear guidelines. It's such a new field. So maybe there are things that I", "tokens": [10577, 11, 457, 456, 307, 572, 1850, 12470, 13, 467, 311, 1270, 257, 777, 2519, 13, 407, 1310, 456, 366, 721, 300, 286], "temperature": 0.0, "avg_logprob": -0.12635738618912234, "compression_ratio": 1.625, "no_speech_prob": 0.0006675728945992887}, {"id": 718, "seek": 558740, "start": 5587.4, "end": 5597.0, "text": " am also missing. Here's another question from our models wrong for being biased. In the end,", "tokens": [669, 611, 5361, 13, 1692, 311, 1071, 1168, 490, 527, 5245, 2085, 337, 885, 28035, 13, 682, 264, 917, 11], "temperature": 0.0, "avg_logprob": -0.15284230368477958, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.0002541531575843692}, {"id": 719, "seek": 558740, "start": 5597.0, "end": 5602.839999999999, "text": " they just learn what they're designed to learn and isn't our intervention to correct this behavior", "tokens": [436, 445, 1466, 437, 436, 434, 4761, 281, 1466, 293, 1943, 380, 527, 13176, 281, 3006, 341, 5223], "temperature": 0.0, "avg_logprob": -0.15284230368477958, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.0002541531575843692}, {"id": 720, "seek": 558740, "start": 5602.839999999999, "end": 5613.4, "text": " actually cause a bias. This is a good question. So it is kind of it is a question that I also have", "tokens": [767, 3082, 257, 12577, 13, 639, 307, 257, 665, 1168, 13, 407, 309, 307, 733, 295, 309, 307, 257, 1168, 300, 286, 611, 362], "temperature": 0.0, "avg_logprob": -0.15284230368477958, "compression_ratio": 1.5425531914893618, "no_speech_prob": 0.0002541531575843692}, {"id": 721, "seek": 561340, "start": 5613.4, "end": 5620.839999999999, "text": " been thinking about, like a model is wrong for for example reflecting accurately the real world,", "tokens": [668, 1953, 466, 11, 411, 257, 2316, 307, 2085, 337, 337, 1365, 23543, 20095, 264, 957, 1002, 11], "temperature": 0.0, "avg_logprob": -0.1909983687930637, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0005011418252252042}, {"id": 722, "seek": 561340, "start": 5620.839999999999, "end": 5628.44, "text": " right? Do we need to de-bias actively models to make them fair when the world is not fair,", "tokens": [558, 30, 1144, 321, 643, 281, 368, 12, 65, 4609, 13022, 5245, 281, 652, 552, 3143, 562, 264, 1002, 307, 406, 3143, 11], "temperature": 0.0, "avg_logprob": -0.1909983687930637, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0005011418252252042}, {"id": 723, "seek": 561340, "start": 5628.44, "end": 5636.839999999999, "text": " when our data is not fair? So first of all, the way we train models today, they don't only", "tokens": [562, 527, 1412, 307, 406, 3143, 30, 407, 700, 295, 439, 11, 264, 636, 321, 3847, 5245, 965, 11, 436, 500, 380, 787], "temperature": 0.0, "avg_logprob": -0.1909983687930637, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0005011418252252042}, {"id": 724, "seek": 563684, "start": 5636.84, "end": 5646.92, "text": " perpetuate biases, they amplify biases. So this is a natural behavior of a machine learning model", "tokens": [16211, 10107, 32152, 11, 436, 41174, 32152, 13, 407, 341, 307, 257, 3303, 5223, 295, 257, 3479, 2539, 2316], "temperature": 0.0, "avg_logprob": -0.1173621118068695, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0010660530533641577}, {"id": 725, "seek": 563684, "start": 5648.4400000000005, "end": 5656.68, "text": " that basically when you have an input example for which the confidence is lower, it will default", "tokens": [300, 1936, 562, 291, 362, 364, 4846, 1365, 337, 597, 264, 6687, 307, 3126, 11, 309, 486, 7576], "temperature": 0.0, "avg_logprob": -0.1173621118068695, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0010660530533641577}, {"id": 726, "seek": 563684, "start": 5656.68, "end": 5665.88, "text": " to a majority class. This is why if your data contains biases, these biases will be amplified", "tokens": [281, 257, 6286, 1508, 13, 639, 307, 983, 498, 428, 1412, 8306, 32152, 11, 613, 32152, 486, 312, 49237], "temperature": 0.0, "avg_logprob": -0.1173621118068695, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0010660530533641577}, {"id": 727, "seek": 566588, "start": 5665.88, "end": 5674.68, "text": " in a machine learning model trained on this data. And this is clearly wrong. Whether it is wrong to", "tokens": [294, 257, 3479, 2539, 2316, 8895, 322, 341, 1412, 13, 400, 341, 307, 4448, 2085, 13, 8503, 309, 307, 2085, 281], "temperature": 0.0, "avg_logprob": -0.20596040998186385, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.0006409594207070768}, {"id": 728, "seek": 566588, "start": 5676.68, "end": 5683.24, "text": " to build models that do not reflect the actual true distribution in the data, it's a much more", "tokens": [281, 1322, 5245, 300, 360, 406, 5031, 264, 3539, 2074, 7316, 294, 264, 1412, 11, 309, 311, 257, 709, 544], "temperature": 0.0, "avg_logprob": -0.20596040998186385, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.0006409594207070768}, {"id": 729, "seek": 566588, "start": 5683.24, "end": 5694.36, "text": " difficult questions. But there are clear, there are clear kind of cases in which I would say it is", "tokens": [2252, 1651, 13, 583, 456, 366, 1850, 11, 456, 366, 1850, 733, 295, 3331, 294, 597, 286, 576, 584, 309, 307], "temperature": 0.0, "avg_logprob": -0.20596040998186385, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.0006409594207070768}, {"id": 730, "seek": 569436, "start": 5694.36, "end": 5706.2, "text": " wrong to build a model that in the search for CEO shows only male CEOs why why kind of", "tokens": [2085, 281, 1322, 257, 2316, 300, 294, 264, 3164, 337, 9282, 3110, 787, 7133, 40736, 983, 983, 733, 295], "temperature": 0.0, "avg_logprob": -0.22041668525108923, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0004885433590970933}, {"id": 731, "seek": 569436, "start": 5707.16, "end": 5714.599999999999, "text": " because it amplifies biases. But yeah, this is already a subjective kind of answer. It's just", "tokens": [570, 309, 9731, 11221, 32152, 13, 583, 1338, 11, 341, 307, 1217, 257, 25972, 733, 295, 1867, 13, 467, 311, 445], "temperature": 0.0, "avg_logprob": -0.22041668525108923, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0004885433590970933}, {"id": 732, "seek": 569436, "start": 5714.599999999999, "end": 5720.599999999999, "text": " my personal opinion because not much to do is research that we are doing, right?", "tokens": [452, 2973, 4800, 570, 406, 709, 281, 360, 307, 2132, 300, 321, 366, 884, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22041668525108923, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0004885433590970933}, {"id": 733, "seek": 572060, "start": 5720.6, "end": 5725.160000000001, "text": " So, would you like to ask your question?", "tokens": [407, 11, 576, 291, 411, 281, 1029, 428, 1168, 30], "temperature": 0.0, "avg_logprob": -0.42811195373535155, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0013726407196372747}, {"id": 734, "seek": 572060, "start": 5727.96, "end": 5733.08, "text": " Oh yeah, one said, okay, I was trying to start my video but it's saying I can't, so I guess I'm", "tokens": [876, 1338, 11, 472, 848, 11, 1392, 11, 286, 390, 1382, 281, 722, 452, 960, 457, 309, 311, 1566, 286, 393, 380, 11, 370, 286, 2041, 286, 478], "temperature": 0.0, "avg_logprob": -0.42811195373535155, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0013726407196372747}, {"id": 735, "seek": 572060, "start": 5733.08, "end": 5738.4400000000005, "text": " just going to ask. By the way, I don't see people anyway. Well, you'd like to start", "tokens": [445, 516, 281, 1029, 13, 3146, 264, 636, 11, 286, 500, 380, 536, 561, 4033, 13, 1042, 11, 291, 1116, 411, 281, 722], "temperature": 0.0, "avg_logprob": -0.42811195373535155, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0013726407196372747}, {"id": 736, "seek": 572060, "start": 5738.4400000000005, "end": 5743.56, "text": " with DDR, but yeah, anyway, let's just go. Yeah, I'll just ask. Oh yes, thank you so much for", "tokens": [365, 413, 9301, 11, 457, 1338, 11, 4033, 11, 718, 311, 445, 352, 13, 865, 11, 286, 603, 445, 1029, 13, 876, 2086, 11, 1309, 291, 370, 709, 337], "temperature": 0.0, "avg_logprob": -0.42811195373535155, "compression_ratio": 1.5242718446601942, "no_speech_prob": 0.0013726407196372747}, {"id": 737, "seek": 574356, "start": 5743.56, "end": 5750.04, "text": " talking. I have a question, microaggressions. So who is to decide what is what is contrary to", "tokens": [1417, 13, 286, 362, 257, 1168, 11, 4532, 559, 3091, 626, 13, 407, 567, 307, 281, 4536, 437, 307, 437, 307, 19506, 281], "temperature": 0.0, "avg_logprob": -0.2736483422395225, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0003564062062650919}, {"id": 738, "seek": 574356, "start": 5750.04, "end": 5755.4800000000005, "text": " microaggression? Is it the people who microaggressions are potentially targeted against? Is it", "tokens": [4532, 559, 13338, 30, 1119, 309, 264, 561, 567, 4532, 559, 3091, 626, 366, 7263, 15045, 1970, 30, 1119, 309], "temperature": 0.0, "avg_logprob": -0.2736483422395225, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0003564062062650919}, {"id": 739, "seek": 574356, "start": 5755.4800000000005, "end": 5761.320000000001, "text": " philosophers or social scientists or just people in education? And in case opinions differ,", "tokens": [36839, 420, 2093, 7708, 420, 445, 561, 294, 3309, 30, 400, 294, 1389, 11819, 743, 11], "temperature": 0.0, "avg_logprob": -0.2736483422395225, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0003564062062650919}, {"id": 740, "seek": 574356, "start": 5761.320000000001, "end": 5766.200000000001, "text": " you know, do we just listen to majority? It all seems culturally important to me, but very,", "tokens": [291, 458, 11, 360, 321, 445, 2140, 281, 6286, 30, 467, 439, 2544, 28879, 1021, 281, 385, 11, 457, 588, 11], "temperature": 0.0, "avg_logprob": -0.2736483422395225, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0003564062062650919}, {"id": 741, "seek": 574356, "start": 5766.200000000001, "end": 5770.84, "text": " very difficult to standardize and kind of reach consensus, especially because your time", "tokens": [588, 2252, 281, 3832, 1125, 293, 733, 295, 2524, 19115, 11, 2318, 570, 428, 565], "temperature": 0.0, "avg_logprob": -0.2736483422395225, "compression_ratio": 1.7164179104477613, "no_speech_prob": 0.0003564062062650919}, {"id": 742, "seek": 577084, "start": 5770.84, "end": 5776.360000000001, "text": " culture is perceived differently. Thank you very much for these questions. These are amazing", "tokens": [3713, 307, 19049, 7614, 13, 1044, 291, 588, 709, 337, 613, 1651, 13, 1981, 366, 2243], "temperature": 0.0, "avg_logprob": -0.17883149315329158, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.0007535377517342567}, {"id": 743, "seek": 577084, "start": 5776.360000000001, "end": 5786.68, "text": " questions. These are difficult questions. This is why we did not create our own corpus of microaggressions,", "tokens": [1651, 13, 1981, 366, 2252, 1651, 13, 639, 307, 983, 321, 630, 406, 1884, 527, 1065, 1181, 31624, 295, 4532, 559, 3091, 626, 11], "temperature": 0.0, "avg_logprob": -0.17883149315329158, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.0007535377517342567}, {"id": 744, "seek": 577084, "start": 5786.68, "end": 5791.56, "text": " right? Because it's cultural dependent. It's very, very personal subjective. This is why we", "tokens": [558, 30, 1436, 309, 311, 6988, 12334, 13, 467, 311, 588, 11, 588, 2973, 25972, 13, 639, 307, 983, 321], "temperature": 0.0, "avg_logprob": -0.17883149315329158, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.0007535377517342567}, {"id": 745, "seek": 579156, "start": 5791.56, "end": 5805.4800000000005, "text": " were focused on a corpus of perceived microaggressions, people that actually felt that the", "tokens": [645, 5178, 322, 257, 1181, 31624, 295, 19049, 4532, 559, 3091, 626, 11, 561, 300, 767, 2762, 300, 264], "temperature": 0.0, "avg_logprob": -0.277316411336263, "compression_ratio": 1.4462809917355373, "no_speech_prob": 0.0002687828673515469}, {"id": 746, "seek": 579156, "start": 5805.4800000000005, "end": 5813.0, "text": " interactions were negative, because they knew that these were microaggressions. It's", "tokens": [13280, 645, 3671, 11, 570, 436, 2586, 300, 613, 645, 4532, 559, 3091, 626, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.277316411336263, "compression_ratio": 1.4462809917355373, "no_speech_prob": 0.0002687828673515469}, {"id": 747, "seek": 581300, "start": 5813.0, "end": 5820.44, "text": " who is to decide whether something is microaggression?", "tokens": [567, 307, 281, 4536, 1968, 746, 307, 4532, 559, 13338, 30], "temperature": 0.0, "avg_logprob": -0.2295395851135254, "compression_ratio": 1.21875, "no_speech_prob": 0.0007659962284378707}, {"id": 748, "seek": 581300, "start": 5827.88, "end": 5837.64, "text": " Yeah, I don't know. This is very difficult. What I can think about like practical solutions about it,", "tokens": [865, 11, 286, 500, 380, 458, 13, 639, 307, 588, 2252, 13, 708, 286, 393, 519, 466, 411, 8496, 6547, 466, 309, 11], "temperature": 0.0, "avg_logprob": -0.2295395851135254, "compression_ratio": 1.21875, "no_speech_prob": 0.0007659962284378707}, {"id": 749, "seek": 583764, "start": 5837.64, "end": 5847.72, "text": " I would say, have a very well trained annotators who understand what microaggression is. So kind", "tokens": [286, 576, 584, 11, 362, 257, 588, 731, 8895, 25339, 3391, 567, 1223, 437, 4532, 559, 13338, 307, 13, 407, 733], "temperature": 0.0, "avg_logprob": -0.2871854863268264, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.0010167572181671858}, {"id": 750, "seek": 583764, "start": 5847.72, "end": 5857.240000000001, "text": " of we explain what is microaggression. They see many examples. They understand, for example,", "tokens": [295, 321, 2903, 437, 307, 4532, 559, 13338, 13, 814, 536, 867, 5110, 13, 814, 1223, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.2871854863268264, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.0010167572181671858}, {"id": 751, "seek": 585724, "start": 5857.24, "end": 5867.48, "text": " a sentence that targets a minority group and other things, and then have many annotators", "tokens": [257, 8174, 300, 12911, 257, 16166, 1594, 293, 661, 721, 11, 293, 550, 362, 867, 25339, 3391], "temperature": 0.0, "avg_logprob": -0.25745911231407753, "compression_ratio": 1.530054644808743, "no_speech_prob": 0.0005996379186399281}, {"id": 752, "seek": 585724, "start": 5867.48, "end": 5878.28, "text": " pair one sentence. So like in say the study, like this is about other social concepts that are", "tokens": [6119, 472, 8174, 13, 407, 411, 294, 584, 264, 2979, 11, 411, 341, 307, 466, 661, 2093, 10392, 300, 366], "temperature": 0.0, "avg_logprob": -0.25745911231407753, "compression_ratio": 1.530054644808743, "no_speech_prob": 0.0005996379186399281}, {"id": 753, "seek": 587828, "start": 5878.28, "end": 5887.24, "text": " abstract. For example, in Dandrovsky's study on respect in police interactions, respect is also a", "tokens": [12649, 13, 1171, 1365, 11, 294, 413, 474, 24088, 25810, 311, 2979, 322, 3104, 294, 3804, 13280, 11, 3104, 307, 611, 257], "temperature": 0.0, "avg_logprob": -0.22284962914206766, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00031657255021855235}, {"id": 754, "seek": 587828, "start": 5887.24, "end": 5893.4, "text": " subjective thing, right? So what they did, they took every utterance and they had multiple", "tokens": [25972, 551, 11, 558, 30, 407, 437, 436, 630, 11, 436, 1890, 633, 17567, 719, 293, 436, 632, 3866], "temperature": 0.0, "avg_logprob": -0.22284962914206766, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00031657255021855235}, {"id": 755, "seek": 587828, "start": 5893.4, "end": 5900.12, "text": " annotators, multiple trained annotators for each utterance and just increased the number of", "tokens": [25339, 3391, 11, 3866, 8895, 25339, 3391, 337, 1184, 17567, 719, 293, 445, 6505, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.22284962914206766, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.00031657255021855235}, {"id": 756, "seek": 590012, "start": 5900.12, "end": 5910.2, "text": " voters, whether an utterance is respectful or not. So like practically, I think this should be", "tokens": [14073, 11, 1968, 364, 17567, 719, 307, 26205, 420, 406, 13, 407, 411, 15667, 11, 286, 519, 341, 820, 312], "temperature": 0.0, "avg_logprob": -0.2704105092518365, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.0006900622975081205}, {"id": 757, "seek": 590012, "start": 5911.64, "end": 5917.64, "text": " the procedure for creating corpus of microaggressions. But more philosophical questions,", "tokens": [264, 10747, 337, 4084, 1181, 31624, 295, 4532, 559, 3091, 626, 13, 583, 544, 25066, 1651, 11], "temperature": 0.0, "avg_logprob": -0.2704105092518365, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.0006900622975081205}, {"id": 758, "seek": 590012, "start": 5917.64, "end": 5927.64, "text": " who is to decide, it's a more difficult one. So there's still more questions, but you're", "tokens": [567, 307, 281, 4536, 11, 309, 311, 257, 544, 2252, 472, 13, 407, 456, 311, 920, 544, 1651, 11, 457, 291, 434], "temperature": 0.0, "avg_logprob": -0.2704105092518365, "compression_ratio": 1.4702702702702704, "no_speech_prob": 0.0006900622975081205}, {"id": 759, "seek": 592764, "start": 5927.64, "end": 5931.96, "text": " allowed to say that you're worn out at any point you're liar. And if you're not the next question,", "tokens": [4350, 281, 584, 300, 291, 434, 15254, 484, 412, 604, 935, 291, 434, 375, 289, 13, 400, 498, 291, 434, 406, 264, 958, 1168, 11], "temperature": 0.0, "avg_logprob": -0.30974024191670035, "compression_ratio": 1.6225490196078431, "no_speech_prob": 0.0005397803033702075}, {"id": 760, "seek": 592764, "start": 5931.96, "end": 5937.320000000001, "text": " I feel bad about not being able to answer big questions about the society and", "tokens": [286, 841, 1578, 466, 406, 885, 1075, 281, 1867, 955, 1651, 466, 264, 4086, 293], "temperature": 0.0, "avg_logprob": -0.30974024191670035, "compression_ratio": 1.6225490196078431, "no_speech_prob": 0.0005397803033702075}, {"id": 761, "seek": 592764, "start": 5939.4800000000005, "end": 5942.200000000001, "text": " yeah, I'm happy to answer the next question from", "tokens": [1338, 11, 286, 478, 2055, 281, 1867, 264, 958, 1168, 490], "temperature": 0.0, "avg_logprob": -0.30974024191670035, "compression_ratio": 1.6225490196078431, "no_speech_prob": 0.0005397803033702075}, {"id": 762, "seek": 592764, "start": 5942.76, "end": 5948.76, "text": " here's, can you talk a bit more about the unsupervised approach to identifying implicit bias?", "tokens": [510, 311, 11, 393, 291, 751, 257, 857, 544, 466, 264, 2693, 12879, 24420, 3109, 281, 16696, 26947, 12577, 30], "temperature": 0.0, "avg_logprob": -0.30974024191670035, "compression_ratio": 1.6225490196078431, "no_speech_prob": 0.0005397803033702075}, {"id": 763, "seek": 594876, "start": 5948.76, "end": 5960.84, "text": " I can, it's, I just need to think how to talk about it in a few words.", "tokens": [286, 393, 11, 309, 311, 11, 286, 445, 643, 281, 519, 577, 281, 751, 466, 309, 294, 257, 1326, 2283, 13], "temperature": 0.0, "avg_logprob": -0.21754597508630089, "compression_ratio": 1.236842105263158, "no_speech_prob": 0.002854786580428481}, {"id": 764, "seek": 594876, "start": 5966.360000000001, "end": 5973.96, "text": " So intuitively, we cannot create a corpus of like which has utterance,", "tokens": [407, 46506, 11, 321, 2644, 1884, 257, 1181, 31624, 295, 411, 597, 575, 17567, 719, 11], "temperature": 0.0, "avg_logprob": -0.21754597508630089, "compression_ratio": 1.236842105263158, "no_speech_prob": 0.002854786580428481}, {"id": 765, "seek": 597396, "start": 5973.96, "end": 5983.96, "text": " which has an utterance and then a label is it sentence bias or not. So we create a causal framework", "tokens": [597, 575, 364, 17567, 719, 293, 550, 257, 7645, 307, 309, 8174, 12577, 420, 406, 13, 407, 321, 1884, 257, 38755, 8388], "temperature": 0.0, "avg_logprob": -0.22450297037760417, "compression_ratio": 1.6353591160220995, "no_speech_prob": 0.0012992970878258348}, {"id": 766, "seek": 597396, "start": 5984.76, "end": 5992.84, "text": " in which our target label is more objective, who is, who is the sentence directed to a man or", "tokens": [294, 597, 527, 3779, 7645, 307, 544, 10024, 11, 567, 307, 11, 567, 307, 264, 8174, 12898, 281, 257, 587, 420], "temperature": 0.0, "avg_logprob": -0.22450297037760417, "compression_ratio": 1.6353591160220995, "no_speech_prob": 0.0012992970878258348}, {"id": 767, "seek": 597396, "start": 5992.84, "end": 6003.0, "text": " to a woman. So our labels are general labels. And in a naive way, if given a sentence towards a person", "tokens": [281, 257, 3059, 13, 407, 527, 16949, 366, 2674, 16949, 13, 400, 294, 257, 29052, 636, 11, 498, 2212, 257, 8174, 3030, 257, 954], "temperature": 0.0, "avg_logprob": -0.22450297037760417, "compression_ratio": 1.6353591160220995, "no_speech_prob": 0.0012992970878258348}, {"id": 768, "seek": 600300, "start": 6003.0, "end": 6008.6, "text": " without looking at the actual person and their comment, just by this comment towards a person,", "tokens": [1553, 1237, 412, 264, 3539, 954, 293, 641, 2871, 11, 445, 538, 341, 2871, 3030, 257, 954, 11], "temperature": 0.0, "avg_logprob": -0.14497889643130096, "compression_ratio": 1.6420454545454546, "no_speech_prob": 0.0027306133415549994}, {"id": 769, "seek": 600300, "start": 6008.6, "end": 6018.28, "text": " we can predict if it's directed to a woman, we can say that kind of there is some bias in the", "tokens": [321, 393, 6069, 498, 309, 311, 12898, 281, 257, 3059, 11, 321, 393, 584, 300, 733, 295, 456, 307, 512, 12577, 294, 264], "temperature": 0.0, "avg_logprob": -0.14497889643130096, "compression_ratio": 1.6420454545454546, "no_speech_prob": 0.0027306133415549994}, {"id": 770, "seek": 600300, "start": 6018.28, "end": 6028.28, "text": " sentence, but it's an naive approach because there are other ways in which we can predict the target", "tokens": [8174, 11, 457, 309, 311, 364, 29052, 3109, 570, 456, 366, 661, 2098, 294, 597, 321, 393, 6069, 264, 3779], "temperature": 0.0, "avg_logprob": -0.14497889643130096, "compression_ratio": 1.6420454545454546, "no_speech_prob": 0.0027306133415549994}, {"id": 771, "seek": 602828, "start": 6028.28, "end": 6035.4, "text": " gender, but which will not be associated with bias. For example, it's the context of the", "tokens": [7898, 11, 457, 597, 486, 406, 312, 6615, 365, 12577, 13, 1171, 1365, 11, 309, 311, 264, 4319, 295, 264], "temperature": 0.0, "avg_logprob": -0.10608110293536119, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.0012070025550201535}, {"id": 772, "seek": 602828, "start": 6035.4, "end": 6041.08, "text": " current station, the traits of the person that we are writing to and so on. So the crux of our", "tokens": [2190, 5214, 11, 264, 19526, 295, 264, 954, 300, 321, 366, 3579, 281, 293, 370, 322, 13, 407, 264, 5140, 87, 295, 527], "temperature": 0.0, "avg_logprob": -0.10608110293536119, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.0012070025550201535}, {"id": 773, "seek": 602828, "start": 6041.08, "end": 6048.12, "text": " technology is that we predict gender, but the mode all kinds of confounds in the task of detecting", "tokens": [2899, 307, 300, 321, 6069, 7898, 11, 457, 264, 4391, 439, 3685, 295, 1497, 4432, 294, 264, 5633, 295, 40237], "temperature": 0.0, "avg_logprob": -0.10608110293536119, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.0012070025550201535}, {"id": 774, "seek": 604812, "start": 6048.12, "end": 6059.16, "text": " bias. So we demote the signals of the source sentence. We demote the latent traits of the target", "tokens": [12577, 13, 407, 321, 1371, 1370, 264, 12354, 295, 264, 4009, 8174, 13, 492, 1371, 1370, 264, 48994, 19526, 295, 264, 3779], "temperature": 0.0, "avg_logprob": -0.14172275646312818, "compression_ratio": 1.646067415730337, "no_speech_prob": 0.0005776382167823613}, {"id": 775, "seek": 604812, "start": 6059.16, "end": 6066.12, "text": " person. And so we make this task very difficult to detect who is a target, what is a target gender.", "tokens": [954, 13, 400, 370, 321, 652, 341, 5633, 588, 2252, 281, 5531, 567, 307, 257, 3779, 11, 437, 307, 257, 3779, 7898, 13], "temperature": 0.0, "avg_logprob": -0.14172275646312818, "compression_ratio": 1.646067415730337, "no_speech_prob": 0.0005776382167823613}, {"id": 776, "seek": 604812, "start": 6066.12, "end": 6073.32, "text": " And if after all these demotions of the confounds given utterance that is directed to a specific", "tokens": [400, 498, 934, 439, 613, 1371, 310, 626, 295, 264, 1497, 4432, 2212, 17567, 719, 300, 307, 12898, 281, 257, 2685], "temperature": 0.0, "avg_logprob": -0.14172275646312818, "compression_ratio": 1.646067415730337, "no_speech_prob": 0.0005776382167823613}, {"id": 777, "seek": 607332, "start": 6073.32, "end": 6080.12, "text": " person, we can still classify this utterance that is clearly directed to a woman, it is likely", "tokens": [954, 11, 321, 393, 920, 33872, 341, 17567, 719, 300, 307, 4448, 12898, 281, 257, 3059, 11, 309, 307, 3700], "temperature": 0.0, "avg_logprob": -0.15376673687945355, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.0011227356735616922}, {"id": 778, "seek": 607332, "start": 6080.12, "end": 6085.32, "text": " that this utterance contains bias. So what I was going to talk is all kinds of demotion approaches", "tokens": [300, 341, 17567, 719, 8306, 12577, 13, 407, 437, 286, 390, 516, 281, 751, 307, 439, 3685, 295, 1371, 19228, 11587], "temperature": 0.0, "avg_logprob": -0.15376673687945355, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.0011227356735616922}, {"id": 779, "seek": 607332, "start": 6085.32, "end": 6094.759999999999, "text": " that we develop, but once we demote this approach and we can still predict the utterance,", "tokens": [300, 321, 1499, 11, 457, 1564, 321, 1371, 1370, 341, 3109, 293, 321, 393, 920, 6069, 264, 17567, 719, 11], "temperature": 0.0, "avg_logprob": -0.15376673687945355, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.0011227356735616922}, {"id": 780, "seek": 607332, "start": 6095.719999999999, "end": 6102.5199999999995, "text": " what is a gender of the target and receive, we actually can surface some bias sentences. So these", "tokens": [437, 307, 257, 7898, 295, 264, 3779, 293, 4774, 11, 321, 767, 393, 3753, 512, 12577, 16579, 13, 407, 613], "temperature": 0.0, "avg_logprob": -0.15376673687945355, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.0011227356735616922}, {"id": 781, "seek": 610252, "start": 6102.52, "end": 6108.360000000001, "text": " are the main findings. For example, if we look at comments directed to politicians,", "tokens": [366, 264, 2135, 16483, 13, 1171, 1365, 11, 498, 321, 574, 412, 3053, 12898, 281, 14756, 11], "temperature": 0.0, "avg_logprob": -0.14938474893569947, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.001863303710706532}, {"id": 782, "seek": 610252, "start": 6109.160000000001, "end": 6116.68, "text": " after all these demotions and we see comments that kind of clearly predict the target gender,", "tokens": [934, 439, 613, 1371, 310, 626, 293, 321, 536, 3053, 300, 733, 295, 4448, 6069, 264, 3779, 7898, 11], "temperature": 0.0, "avg_logprob": -0.14938474893569947, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.001863303710706532}, {"id": 783, "seek": 610252, "start": 6116.68, "end": 6123.0, "text": " we can see that comments or politicians talk about their spouses, about their family love and also", "tokens": [321, 393, 536, 300, 3053, 420, 14756, 751, 466, 641, 49784, 11, 466, 641, 1605, 959, 293, 611], "temperature": 0.0, "avg_logprob": -0.14938474893569947, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.001863303710706532}, {"id": 784, "seek": 610252, "start": 6123.88, "end": 6130.4400000000005, "text": " about their competence, maybe question their confidence. And if we look at comments towards", "tokens": [466, 641, 39965, 11, 1310, 1168, 641, 6687, 13, 400, 498, 321, 574, 412, 3053, 3030], "temperature": 0.0, "avg_logprob": -0.14938474893569947, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.001863303710706532}, {"id": 785, "seek": 613044, "start": 6130.44, "end": 6136.12, "text": " public figures like actresses, we can see a lot of words that are just related to", "tokens": [1908, 9624, 411, 15410, 279, 11, 321, 393, 536, 257, 688, 295, 2283, 300, 366, 445, 4077, 281], "temperature": 0.0, "avg_logprob": -0.1703190803527832, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.0011969535844400525}, {"id": 786, "seek": 613044, "start": 6136.12, "end": 6141.16, "text": " objectification, sexualization, regardless of their source content. So they can talk about their", "tokens": [2657, 3774, 11, 6701, 2144, 11, 10060, 295, 641, 4009, 2701, 13, 407, 436, 393, 751, 466, 641], "temperature": 0.0, "avg_logprob": -0.1703190803527832, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.0011969535844400525}, {"id": 787, "seek": 613044, "start": 6141.16, "end": 6150.28, "text": " movie, but the comments will always be about that she is sexy. And this is what our model is", "tokens": [3169, 11, 457, 264, 3053, 486, 1009, 312, 466, 300, 750, 307, 13701, 13, 400, 341, 307, 437, 527, 2316, 307], "temperature": 0.0, "avg_logprob": -0.1703190803527832, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.0011969535844400525}, {"id": 788, "seek": 613044, "start": 6150.28, "end": 6155.96, "text": " able to surface, but it's again, it's just initial study in the local work.", "tokens": [1075, 281, 3753, 11, 457, 309, 311, 797, 11, 309, 311, 445, 5883, 2979, 294, 264, 2654, 589, 13], "temperature": 0.0, "avg_logprob": -0.1703190803527832, "compression_ratio": 1.591743119266055, "no_speech_prob": 0.0011969535844400525}, {"id": 789, "seek": 615596, "start": 6155.96, "end": 6164.04, "text": " Okay, so around here also, if microaggressions are pulled from a site where people can list what", "tokens": [1033, 11, 370, 926, 510, 611, 11, 498, 4532, 559, 3091, 626, 366, 7373, 490, 257, 3621, 689, 561, 393, 1329, 437], "temperature": 0.0, "avg_logprob": -0.2222755331742136, "compression_ratio": 1.3850931677018634, "no_speech_prob": 0.0014389685820788145}, {"id": 790, "seek": 615596, "start": 6164.04, "end": 6168.44, "text": " they have experienced, isn't that data very vulnerable to social engineering?", "tokens": [436, 362, 6751, 11, 1943, 380, 300, 1412, 588, 10955, 281, 2093, 7043, 30], "temperature": 0.0, "avg_logprob": -0.2222755331742136, "compression_ratio": 1.3850931677018634, "no_speech_prob": 0.0014389685820788145}, {"id": 791, "seek": 615596, "start": 6176.92, "end": 6181.64, "text": " Yes, this data is vulnerable. So in our case, we", "tokens": [1079, 11, 341, 1412, 307, 10955, 13, 407, 294, 527, 1389, 11, 321], "temperature": 0.0, "avg_logprob": -0.2222755331742136, "compression_ratio": 1.3850931677018634, "no_speech_prob": 0.0014389685820788145}, {"id": 792, "seek": 618164, "start": 6181.64, "end": 6191.64, "text": " anonymized this data. We extract only quotes from the data. We removed actual users who published", "tokens": [37293, 1602, 341, 1412, 13, 492, 8947, 787, 19963, 490, 264, 1412, 13, 492, 7261, 3539, 5022, 567, 6572], "temperature": 0.0, "avg_logprob": -0.23217425139054007, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.002017723163589835}, {"id": 793, "seek": 618164, "start": 6191.64, "end": 6201.400000000001, "text": " it. We remove all the text around these quotes. And this is a good question. Maybe we should also", "tokens": [309, 13, 492, 4159, 439, 264, 2487, 926, 613, 19963, 13, 400, 341, 307, 257, 665, 1168, 13, 2704, 321, 820, 611], "temperature": 0.0, "avg_logprob": -0.23217425139054007, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.002017723163589835}, {"id": 794, "seek": 618164, "start": 6201.400000000001, "end": 6209.400000000001, "text": " not make this data public, even, yeah. Hey, there are more questions. Thank you.", "tokens": [406, 652, 341, 1412, 1908, 11, 754, 11, 1338, 13, 1911, 11, 456, 366, 544, 1651, 13, 1044, 291, 13], "temperature": 0.0, "avg_logprob": -0.23217425139054007, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.002017723163589835}, {"id": 795, "seek": 620940, "start": 6209.4, "end": 6218.04, "text": " More questions. So by the way, like if Chris John and I'm saying Chris John because they said", "tokens": [5048, 1651, 13, 407, 538, 264, 636, 11, 411, 498, 6688, 2619, 293, 286, 478, 1566, 6688, 2619, 570, 436, 848], "temperature": 0.0, "avg_logprob": -0.18849580764770507, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.001778160803951323}, {"id": 796, "seek": 620940, "start": 6218.04, "end": 6225.0, "text": " the two faces, the faces that I see on my screen. Please let me know when we need to finish. I'm", "tokens": [264, 732, 8475, 11, 264, 8475, 300, 286, 536, 322, 452, 2568, 13, 2555, 718, 385, 458, 562, 321, 643, 281, 2413, 13, 286, 478], "temperature": 0.0, "avg_logprob": -0.18849580764770507, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.001778160803951323}, {"id": 797, "seek": 620940, "start": 6225.0, "end": 6229.24, "text": " happy to continue answering. We've often gone on for a few more minutes. I can ask a couple more", "tokens": [2055, 281, 2354, 13430, 13, 492, 600, 2049, 2780, 322, 337, 257, 1326, 544, 2077, 13, 286, 393, 1029, 257, 1916, 544], "temperature": 0.0, "avg_logprob": -0.18849580764770507, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.001778160803951323}, {"id": 798, "seek": 620940, "start": 6229.24, "end": 6238.36, "text": " questions. So here's one that's very prominent in AI right at the moment. Do you think of this", "tokens": [1651, 13, 407, 510, 311, 472, 300, 311, 588, 17034, 294, 7318, 558, 412, 264, 1623, 13, 1144, 291, 519, 295, 341], "temperature": 0.0, "avg_logprob": -0.18849580764770507, "compression_ratio": 1.5655737704918034, "no_speech_prob": 0.001778160803951323}, {"id": 799, "seek": 623836, "start": 6238.36, "end": 6244.36, "text": " fair for AI scientists and tech and academia who are definitely not representative of the general", "tokens": [3143, 337, 7318, 7708, 293, 7553, 293, 28937, 567, 366, 2138, 406, 12424, 295, 264, 2674], "temperature": 0.0, "avg_logprob": -0.1763166803302187, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.000297420920105651}, {"id": 800, "seek": 623836, "start": 6244.36, "end": 6251.88, "text": " population to decide what is biased and what is not? IE, the act of devising itself might be biased.", "tokens": [4415, 281, 4536, 437, 307, 28035, 293, 437, 307, 406, 30, 286, 36, 11, 264, 605, 295, 1905, 3436, 2564, 1062, 312, 28035, 13], "temperature": 0.0, "avg_logprob": -0.1763166803302187, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.000297420920105651}, {"id": 801, "seek": 623836, "start": 6253.4, "end": 6260.2, "text": " Yeah, this is one problem also that this is like more general problem. The researchers,", "tokens": [865, 11, 341, 307, 472, 1154, 611, 300, 341, 307, 411, 544, 2674, 1154, 13, 440, 10309, 11], "temperature": 0.0, "avg_logprob": -0.1763166803302187, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.000297420920105651}, {"id": 802, "seek": 626020, "start": 6260.2, "end": 6271.72, "text": " even those who work on the bias and can incorporate their own biases. We currently don't have any", "tokens": [754, 729, 567, 589, 322, 264, 12577, 293, 393, 16091, 641, 1065, 32152, 13, 492, 4362, 500, 380, 362, 604], "temperature": 0.0, "avg_logprob": -0.18578917101809853, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.001199479098431766}, {"id": 803, "seek": 626020, "start": 6271.72, "end": 6278.44, "text": " other alternative, right? We don't have a training how to do it. We don't have, I think it's a good", "tokens": [661, 8535, 11, 558, 30, 492, 500, 380, 362, 257, 3097, 577, 281, 360, 309, 13, 492, 500, 380, 362, 11, 286, 519, 309, 311, 257, 665], "temperature": 0.0, "avg_logprob": -0.18578917101809853, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.001199479098431766}, {"id": 804, "seek": 626020, "start": 6278.44, "end": 6285.24, "text": " thing to work on these topics to try to promote these topics as much as possible. We're in a", "tokens": [551, 281, 589, 322, 613, 8378, 281, 853, 281, 9773, 613, 8378, 382, 709, 382, 1944, 13, 492, 434, 294, 257], "temperature": 0.0, "avg_logprob": -0.18578917101809853, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.001199479098431766}, {"id": 805, "seek": 628524, "start": 6285.24, "end": 6291.96, "text": " awareness that we as researchers can incorporate our own biases. So this is what we also", "tokens": [8888, 300, 321, 382, 10309, 393, 16091, 527, 1065, 32152, 13, 407, 341, 307, 437, 321, 611], "temperature": 0.0, "avg_logprob": -0.177298500958611, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0016232512425631285}, {"id": 806, "seek": 628524, "start": 6293.16, "end": 6300.679999999999, "text": " write in ethical implications sections in the paper that we try to identify bias, we try to", "tokens": [2464, 294, 18890, 16602, 10863, 294, 264, 3035, 300, 321, 853, 281, 5876, 12577, 11, 321, 853, 281], "temperature": 0.0, "avg_logprob": -0.177298500958611, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0016232512425631285}, {"id": 807, "seek": 628524, "start": 6300.679999999999, "end": 6307.24, "text": " de-bias, but there are limitations to this study because we could incorporate our own biases", "tokens": [368, 12, 65, 4609, 11, 457, 456, 366, 15705, 281, 341, 2979, 570, 321, 727, 16091, 527, 1065, 32152], "temperature": 0.0, "avg_logprob": -0.177298500958611, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0016232512425631285}, {"id": 808, "seek": 628524, "start": 6307.24, "end": 6313.88, "text": " into our analysis, right? This is how we interpret this results. Maybe this is what we were looking", "tokens": [666, 527, 5215, 11, 558, 30, 639, 307, 577, 321, 7302, 341, 3542, 13, 2704, 341, 307, 437, 321, 645, 1237], "temperature": 0.0, "avg_logprob": -0.177298500958611, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0016232512425631285}, {"id": 809, "seek": 631388, "start": 6313.88, "end": 6325.24, "text": " for and this is a confirmation bias. Yeah. Okay, maybe should just have one more,", "tokens": [337, 293, 341, 307, 257, 21871, 12577, 13, 865, 13, 1033, 11, 1310, 820, 445, 362, 472, 544, 11], "temperature": 0.0, "avg_logprob": -0.28925663232803345, "compression_ratio": 1.5060240963855422, "no_speech_prob": 0.0003580524353310466}, {"id": 810, "seek": 631388, "start": 6326.12, "end": 6329.88, "text": " oh no, a new question just turned up. Maybe we'll have to be two questions more.", "tokens": [1954, 572, 11, 257, 777, 1168, 445, 3574, 493, 13, 2704, 321, 603, 362, 281, 312, 732, 1651, 544, 13], "temperature": 0.0, "avg_logprob": -0.28925663232803345, "compression_ratio": 1.5060240963855422, "no_speech_prob": 0.0003580524353310466}, {"id": 811, "seek": 631388, "start": 6334.92, "end": 6340.52, "text": " Now, maybe I should do that one immediately because it directly relates to that answer,", "tokens": [823, 11, 1310, 286, 820, 360, 300, 472, 4258, 570, 309, 3838, 16155, 281, 300, 1867, 11], "temperature": 0.0, "avg_logprob": -0.28925663232803345, "compression_ratio": 1.5060240963855422, "no_speech_prob": 0.0003580524353310466}, {"id": 812, "seek": 634052, "start": 6340.52, "end": 6347.72, "text": " which was from how are the perspectives of community stakeholders,", "tokens": [597, 390, 490, 577, 366, 264, 16766, 295, 1768, 17779, 11], "temperature": 0.0, "avg_logprob": -0.2061510615878635, "compression_ratio": 1.4930232558139536, "no_speech_prob": 0.0004439254989847541}, {"id": 813, "seek": 634052, "start": 6347.72, "end": 6351.96, "text": " are people from minoritized groups included when these systems are being built?", "tokens": [366, 561, 490, 6696, 270, 1602, 3935, 5556, 562, 613, 3652, 366, 885, 3094, 30], "temperature": 0.0, "avg_logprob": -0.2061510615878635, "compression_ratio": 1.4930232558139536, "no_speech_prob": 0.0004439254989847541}, {"id": 814, "seek": 634052, "start": 6353.72, "end": 6360.4400000000005, "text": " This is a wonderful question to, yeah. Currently not very good. Actually, we have,", "tokens": [639, 307, 257, 3715, 1168, 281, 11, 1338, 13, 19964, 406, 588, 665, 13, 5135, 11, 321, 362, 11], "temperature": 0.0, "avg_logprob": -0.2061510615878635, "compression_ratio": 1.4930232558139536, "no_speech_prob": 0.0004439254989847541}, {"id": 815, "seek": 634052, "start": 6360.4400000000005, "end": 6366.200000000001, "text": " currently have a paper also in submission about analysis of what kind of how race have been", "tokens": [4362, 362, 257, 3035, 611, 294, 23689, 466, 5215, 295, 437, 733, 295, 577, 4569, 362, 668], "temperature": 0.0, "avg_logprob": -0.2061510615878635, "compression_ratio": 1.4930232558139536, "no_speech_prob": 0.0004439254989847541}, {"id": 816, "seek": 636620, "start": 6366.2, "end": 6374.28, "text": " treated in LP systems, starting from data sets to models to potential users. And one of the things", "tokens": [8668, 294, 38095, 3652, 11, 2891, 490, 1412, 6352, 281, 5245, 281, 3995, 5022, 13, 400, 472, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.16533464193344116, "compression_ratio": 1.46875, "no_speech_prob": 0.0006283092661760747}, {"id": 817, "seek": 636620, "start": 6374.28, "end": 6383.4, "text": " that we found is that even people who work on identifying racism, they don't involve actually", "tokens": [300, 321, 1352, 307, 300, 754, 561, 567, 589, 322, 16696, 12664, 11, 436, 500, 380, 9494, 767], "temperature": 0.0, "avg_logprob": -0.16533464193344116, "compression_ratio": 1.46875, "no_speech_prob": 0.0006283092661760747}, {"id": 818, "seek": 636620, "start": 6384.04, "end": 6391.8, "text": " in group members. This is, yeah, you identified yet another problem in the community that", "tokens": [294, 1594, 2679, 13, 639, 307, 11, 1338, 11, 291, 9234, 1939, 1071, 1154, 294, 264, 1768, 300], "temperature": 0.0, "avg_logprob": -0.16533464193344116, "compression_ratio": 1.46875, "no_speech_prob": 0.0006283092661760747}, {"id": 819, "seek": 639180, "start": 6391.8, "end": 6399.72, "text": " perspectives of community are not often incorporated. In our acquisition paper, we try to advocate", "tokens": [16766, 295, 1768, 366, 406, 2049, 21654, 13, 682, 527, 21668, 3035, 11, 321, 853, 281, 14608], "temperature": 0.0, "avg_logprob": -0.19568336193378155, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0006735803908668458}, {"id": 820, "seek": 639180, "start": 6399.72, "end": 6407.72, "text": " for its importance, but like all these questions are very good, but like, there are maybe first,", "tokens": [337, 1080, 7379, 11, 457, 411, 439, 613, 1651, 366, 588, 665, 11, 457, 411, 11, 456, 366, 1310, 700, 11], "temperature": 0.0, "avg_logprob": -0.19568336193378155, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0006735803908668458}, {"id": 821, "seek": 639180, "start": 6407.72, "end": 6414.52, "text": " somebody like Chris who has a lot of influence could make changes in the community. It's very", "tokens": [2618, 411, 6688, 567, 575, 257, 688, 295, 6503, 727, 652, 2962, 294, 264, 1768, 13, 467, 311, 588], "temperature": 0.0, "avg_logprob": -0.19568336193378155, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0006735803908668458}, {"id": 822, "seek": 641452, "start": 6414.52, "end": 6423.240000000001, "text": " difficult to make such changes. Yes, I'm hopeful that there's actually starting to be a bit of change", "tokens": [2252, 281, 652, 1270, 2962, 13, 1079, 11, 286, 478, 20531, 300, 456, 311, 767, 2891, 281, 312, 257, 857, 295, 1319], "temperature": 0.0, "avg_logprob": -0.12927174903977084, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.000221455586142838}, {"id": 823, "seek": 641452, "start": 6423.240000000001, "end": 6431.56, "text": " right now. I mean, you know, like one can be pessimistic given the history and one can be", "tokens": [558, 586, 13, 286, 914, 11, 291, 458, 11, 411, 472, 393, 312, 37399, 3142, 2212, 264, 2503, 293, 472, 393, 312], "temperature": 0.0, "avg_logprob": -0.12927174903977084, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.000221455586142838}, {"id": 824, "seek": 641452, "start": 6432.120000000001, "end": 6440.76, "text": " pessimistic given the current statistics, but you know, I actually believe that, you know,", "tokens": [37399, 3142, 2212, 264, 2190, 12523, 11, 457, 291, 458, 11, 286, 767, 1697, 300, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.12927174903977084, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.000221455586142838}, {"id": 825, "seek": 644076, "start": 6440.76, "end": 6447.8, "text": " through recent events of like lives matter and other things that there's actually just more", "tokens": [807, 5162, 3931, 295, 411, 2909, 1871, 293, 661, 721, 300, 456, 311, 767, 445, 544], "temperature": 0.0, "avg_logprob": -0.13919503571557218, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.00033308970159851015}, {"id": 826, "seek": 644076, "start": 6448.6, "end": 6457.400000000001, "text": " genuine attempts to create change around, well, certainly the standard computer science department,", "tokens": [16699, 15257, 281, 1884, 1319, 926, 11, 731, 11, 3297, 264, 3832, 3820, 3497, 5882, 11], "temperature": 0.0, "avg_logprob": -0.13919503571557218, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.00033308970159851015}, {"id": 827, "seek": 644076, "start": 6457.400000000001, "end": 6464.84, "text": " but I think more generally around the field of AI than there's been at any time in the past 30", "tokens": [457, 286, 519, 544, 5101, 926, 264, 2519, 295, 7318, 813, 456, 311, 668, 412, 604, 565, 294, 264, 1791, 2217], "temperature": 0.0, "avg_logprob": -0.13919503571557218, "compression_ratio": 1.5212765957446808, "no_speech_prob": 0.00033308970159851015}, {"id": 828, "seek": 646484, "start": 6464.84, "end": 6470.92, "text": " years when I've been watching it. Right, even when I did my postdoc, it's 1014, 2016,", "tokens": [924, 562, 286, 600, 668, 1976, 309, 13, 1779, 11, 754, 562, 286, 630, 452, 2183, 39966, 11, 309, 311, 1266, 7271, 11, 6549, 11], "temperature": 0.0, "avg_logprob": -0.21439271120680975, "compression_ratio": 1.5020576131687242, "no_speech_prob": 0.0005160788423381746}, {"id": 829, "seek": 646484, "start": 6472.84, "end": 6478.360000000001, "text": " we started working on the problem of gender bias and it was total outlier. Like, I didn't know", "tokens": [321, 1409, 1364, 322, 264, 1154, 295, 7898, 12577, 293, 309, 390, 3217, 484, 2753, 13, 1743, 11, 286, 994, 380, 458], "temperature": 0.0, "avg_logprob": -0.21439271120680975, "compression_ratio": 1.5020576131687242, "no_speech_prob": 0.0005160788423381746}, {"id": 830, "seek": 646484, "start": 6478.360000000001, "end": 6484.4400000000005, "text": " if what I'm doing will be relevant to anyone. And then now look, we discussed this is a", "tokens": [498, 437, 286, 478, 884, 486, 312, 7340, 281, 2878, 13, 400, 550, 586, 574, 11, 321, 7152, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.21439271120680975, "compression_ratio": 1.5020576131687242, "no_speech_prob": 0.0005160788423381746}, {"id": 831, "seek": 646484, "start": 6485.24, "end": 6490.360000000001, "text": " kind of relevant question. This is already an amazing change in the community. And if there will", "tokens": [733, 295, 7340, 1168, 13, 639, 307, 1217, 364, 2243, 1319, 294, 264, 1768, 13, 400, 498, 456, 486], "temperature": 0.0, "avg_logprob": -0.21439271120680975, "compression_ratio": 1.5020576131687242, "no_speech_prob": 0.0005160788423381746}, {"id": 832, "seek": 649036, "start": 6490.36, "end": 6495.719999999999, "text": " be more focus also on the right hiring, which clearly now has more awareness than ever.", "tokens": [312, 544, 1879, 611, 322, 264, 558, 15335, 11, 597, 4448, 586, 575, 544, 8888, 813, 1562, 13], "temperature": 0.0, "avg_logprob": -0.20980236854082274, "compression_ratio": 1.5275229357798166, "no_speech_prob": 0.0005185528425499797}, {"id": 833, "seek": 649036, "start": 6497.4, "end": 6501.96, "text": " Right, I'm also more optimistic now than say three years ago.", "tokens": [1779, 11, 286, 478, 611, 544, 19397, 586, 813, 584, 1045, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.20980236854082274, "compression_ratio": 1.5275229357798166, "no_speech_prob": 0.0005185528425499797}, {"id": 834, "seek": 649036, "start": 6503.08, "end": 6508.5199999999995, "text": " Okay, well, maybe we'll give you some wonderful questions for which there are no good answers yet.", "tokens": [1033, 11, 731, 11, 1310, 321, 603, 976, 291, 512, 3715, 1651, 337, 597, 456, 366, 572, 665, 6338, 1939, 13], "temperature": 0.0, "avg_logprob": -0.20980236854082274, "compression_ratio": 1.5275229357798166, "no_speech_prob": 0.0005185528425499797}, {"id": 835, "seek": 649036, "start": 6510.12, "end": 6516.12, "text": " Maybe you can do this as the last question. And let's just say something that really", "tokens": [2704, 291, 393, 360, 341, 382, 264, 1036, 1168, 13, 400, 718, 311, 445, 584, 746, 300, 534], "temperature": 0.0, "avg_logprob": -0.20980236854082274, "compression_ratio": 1.5275229357798166, "no_speech_prob": 0.0005185528425499797}, {"id": 836, "seek": 651612, "start": 6516.12, "end": 6523.5599999999995, "text": " I could listen a lot more. What do you think the social and ethics space might look like say five to", "tokens": [286, 727, 2140, 257, 688, 544, 13, 708, 360, 291, 519, 264, 2093, 293, 19769, 1901, 1062, 574, 411, 584, 1732, 281], "temperature": 0.0, "avg_logprob": -0.12478457440386762, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.0009162237402051687}, {"id": 837, "seek": 651612, "start": 6523.5599999999995, "end": 6529.64, "text": " 10 years down the line? Do you think the industry might come down to a unified standard for ethics", "tokens": [1266, 924, 760, 264, 1622, 30, 1144, 291, 519, 264, 3518, 1062, 808, 760, 281, 257, 26787, 3832, 337, 19769], "temperature": 0.0, "avg_logprob": -0.12478457440386762, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.0009162237402051687}, {"id": 838, "seek": 651612, "start": 6529.64, "end": 6535.24, "text": " for AI systems? Given that a lot of the challenges come from the fact that social and ethics discussions", "tokens": [337, 7318, 3652, 30, 18600, 300, 257, 688, 295, 264, 4759, 808, 490, 264, 1186, 300, 2093, 293, 19769, 11088], "temperature": 0.0, "avg_logprob": -0.12478457440386762, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.0009162237402051687}, {"id": 839, "seek": 651612, "start": 6535.24, "end": 6544.5199999999995, "text": " are often subjective. Yeah, I'm also optimistic about the field of ethics in five years.", "tokens": [366, 2049, 25972, 13, 865, 11, 286, 478, 611, 19397, 466, 264, 2519, 295, 19769, 294, 1732, 924, 13], "temperature": 0.0, "avg_logprob": -0.12478457440386762, "compression_ratio": 1.6512605042016806, "no_speech_prob": 0.0009162237402051687}, {"id": 840, "seek": 654452, "start": 6544.52, "end": 6550.360000000001, "text": " These are difficult problems. The field of ethics, by the way, itself is 2,000 years old, right?", "tokens": [1981, 366, 2252, 2740, 13, 440, 2519, 295, 19769, 11, 538, 264, 636, 11, 2564, 307, 568, 11, 1360, 924, 1331, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2263328952174033, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0016520563513040543}, {"id": 841, "seek": 654452, "start": 6551.320000000001, "end": 6556.68, "text": " Aristotle already asked these questions. Now we're asking these questions about AI, but", "tokens": [42368, 1217, 2351, 613, 1651, 13, 823, 321, 434, 3365, 613, 1651, 466, 7318, 11, 457], "temperature": 0.0, "avg_logprob": -0.2263328952174033, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0016520563513040543}, {"id": 842, "seek": 654452, "start": 6559.96, "end": 6571.320000000001, "text": " giving the current awareness and the bad publicity that currently companies are the main", "tokens": [2902, 264, 2190, 8888, 293, 264, 1578, 37264, 300, 4362, 3431, 366, 264, 2135], "temperature": 0.0, "avg_logprob": -0.2263328952174033, "compression_ratio": 1.436842105263158, "no_speech_prob": 0.0016520563513040543}, {"id": 843, "seek": 657132, "start": 6571.32, "end": 6578.679999999999, "text": " players, right? It's more even than governments. And there is a big incentive with companies", "tokens": [4150, 11, 558, 30, 467, 311, 544, 754, 813, 11280, 13, 400, 456, 307, 257, 955, 22346, 365, 3431], "temperature": 0.0, "avg_logprob": -0.1972042360613423, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0020856179762631655}, {"id": 844, "seek": 657132, "start": 6578.679999999999, "end": 6588.599999999999, "text": " to fix things because of the bad publicity. And for example, today, I read an article about", "tokens": [281, 3191, 721, 570, 295, 264, 1578, 37264, 13, 400, 337, 1365, 11, 965, 11, 286, 1401, 364, 7222, 466], "temperature": 0.0, "avg_logprob": -0.1972042360613423, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0020856179762631655}, {"id": 845, "seek": 658860, "start": 6588.6, "end": 6600.92, "text": " that Google will stop advertising the truck, the profile users, like these plugins.", "tokens": [300, 3329, 486, 1590, 13097, 264, 5898, 11, 264, 7964, 5022, 11, 411, 613, 33759, 13], "temperature": 0.0, "avg_logprob": -0.14335127174854279, "compression_ratio": 1.502824858757062, "no_speech_prob": 0.0011504748836159706}, {"id": 846, "seek": 658860, "start": 6603.160000000001, "end": 6608.52, "text": " So overall, I do see a very positive trajectory. It's very difficult to predict what exactly", "tokens": [407, 4787, 11, 286, 360, 536, 257, 588, 3353, 21512, 13, 467, 311, 588, 2252, 281, 6069, 437, 2293], "temperature": 0.0, "avg_logprob": -0.14335127174854279, "compression_ratio": 1.502824858757062, "no_speech_prob": 0.0011504748836159706}, {"id": 847, "seek": 658860, "start": 6608.52, "end": 6613.56, "text": " will be like in five years. I don't think all the problems will be resolved, but overall,", "tokens": [486, 312, 411, 294, 1732, 924, 13, 286, 500, 380, 519, 439, 264, 2740, 486, 312, 20772, 11, 457, 4787, 11], "temperature": 0.0, "avg_logprob": -0.14335127174854279, "compression_ratio": 1.502824858757062, "no_speech_prob": 0.0011504748836159706}, {"id": 848, "seek": 661356, "start": 6613.56, "end": 6627.240000000001, "text": " I'm optimistic also about the new policies will be already not entirely in hands of decisions of", "tokens": [286, 478, 19397, 611, 466, 264, 777, 7657, 486, 312, 1217, 406, 7696, 294, 2377, 295, 5327, 295], "temperature": 0.0, "avg_logprob": -0.18392102311297162, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.0016226930310949683}, {"id": 849, "seek": 661356, "start": 6627.240000000001, "end": 6635.72, "text": " companies. And definitely about research, because I see how many students now are interested in", "tokens": [3431, 13, 400, 2138, 466, 2132, 11, 570, 286, 536, 577, 867, 1731, 586, 366, 3102, 294], "temperature": 0.0, "avg_logprob": -0.18392102311297162, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.0016226930310949683}, {"id": 850, "seek": 663572, "start": 6635.72, "end": 6644.04, "text": " these topics, which is totally amazing. Okay. And another comment that I want to make is actually", "tokens": [613, 8378, 11, 597, 307, 3879, 2243, 13, 1033, 13, 400, 1071, 2871, 300, 286, 528, 281, 652, 307, 767], "temperature": 0.0, "avg_logprob": -0.22263841068043427, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0016126177506521344}, {"id": 851, "seek": 663572, "start": 6644.04, "end": 6651.4800000000005, "text": " a no P is important in all this, which was much less say the field of fairness very much focused", "tokens": [257, 572, 430, 307, 1021, 294, 439, 341, 11, 597, 390, 709, 1570, 584, 264, 2519, 295, 29765, 588, 709, 5178], "temperature": 0.0, "avg_logprob": -0.22263841068043427, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0016126177506521344}, {"id": 852, "seek": 663572, "start": 6651.4800000000005, "end": 6656.12, "text": " on image recognition, but I think more and more is we will see more and more research on", "tokens": [322, 3256, 11150, 11, 457, 286, 519, 544, 293, 544, 307, 321, 486, 536, 544, 293, 544, 2132, 322], "temperature": 0.0, "avg_logprob": -0.22263841068043427, "compression_ratio": 1.5380434782608696, "no_speech_prob": 0.0016126177506521344}, {"id": 853, "seek": 665612, "start": 6656.12, "end": 6665.64, "text": " and there'll be in language, which is also exciting. Okay. Well, maybe we should call a", "tokens": [293, 456, 603, 312, 294, 2856, 11, 597, 307, 611, 4670, 13, 1033, 13, 1042, 11, 1310, 321, 820, 818, 257], "temperature": 0.0, "avg_logprob": -0.36570014953613283, "compression_ratio": 1.0481927710843373, "no_speech_prob": 0.0005346923717297614}, {"id": 854, "seek": 666564, "start": 6665.64, "end": 6686.6, "text": " quicks at that point. Thank you so much, Julia.", "tokens": [50364, 1702, 82, 412, 300, 935, 13, 1044, 291, 370, 709, 11, 18551, 13, 51412], "temperature": 0.0, "avg_logprob": -0.42156264185905457, "compression_ratio": 0.8867924528301887, "no_speech_prob": 0.00011271956464042887}], "language": "en"}