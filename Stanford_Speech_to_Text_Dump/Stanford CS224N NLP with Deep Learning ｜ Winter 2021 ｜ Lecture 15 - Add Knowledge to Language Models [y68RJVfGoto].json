{"text": " Welcome to CS224N Lecture 15. So I'm Megan and I'm one of the cities in this course and I'm also a PhD student working at Kirste. And today I'll be talking about integrating knowledge and language models. So some quick reminders, your project milestones we do today. So hopefully you turn those in already or we'll be turning them in in the next couple of days. And we'll try to get you feedback on those as fast as possible. So something to be aware of is a change of grading basis and of course withdrawal deadline is this Friday. So if you want to make any change of your grade, make sure to do that by then. And we'll be getting you the grades back on assignment five by then as well in case that's helpful in making your decision. And finally, your final projects are due in two weeks. So hopefully those are going smoothly. So topic of the day is integrating knowledge and language models. You've seen a bit about this idea in assignment five and also in call and raffles lecture last class. In assignment five, the task goes to train a model to predict the birthplace of a person given their name. And you sell it by pre training on a larger data set. You're actually able to do better on this task since you could encode some role knowledge into the language model. And then last lecture call and raffle presented how T5 could actually be fine tuned for a closed domain question answering task such that you can give T5 a natural language question and it'll return an answer. So they will be building on these threads and looking at techniques that researchers have recently been developing to increase the amount of knowledge in language models. So we're going to start with a quick recap of language models just to make sure we're all on the same page. Then we're going to talk about what types of knowledge language models can already encode and what they might struggle on. We'll also motivate why researchers are interested in increasing the amount of knowledge in language models and what this could enable for future AI systems if we have language models that can actually reliably recall knowledge. We'll talk about three broad classes of techniques that researchers have been using to add knowledge to language models. These include adding pre trained entity embeddings using external memory or key value store or even just modifying the training data. And for each of these techniques, we'll talk about at least one recent work that uses the technique. So hopefully it's clear to see how to actually employ and practice. And then finally, we'll wrap up by talking about how to evaluate the knowledge in language models and the challenges that come up in trying to do this. So let's dive right in. We're going to start by talking about standard language models. You learned about these at the beginning of the course. And the task is to predict the next word and sequence of text and to compute the probability of a sequence. So you may remember the example that students opened their blank. And we talked about, it could be mine's exams, bring those books here. And the task of standard language model is to predict the most likely next word in the sequence. A couple of lectures ago, John also introduced the notion of mass language models. And instead of predicting the next word and sequence of text, the task is to predict the mass token. And this is done using bi-jectional context. So you may remember the example I'm mass to the mask and the goal of the mass language model is to make the most likely token for each of the massed out words. So maybe I went to the store. So while there's some differences in these two types of language models, but they are predicting the next word, or whether you're predicting the massed out token, they're similar and that they can both be trained over large amounts of unlabeled text. And this is one of the reasons why they've been so wide they adopted. They don't require any human annotated data. So you've seen that language models can be used for a variety of tasks, from summarization, to dialogue, to fluency evaluation, tasks that involve either generating text or evaluating the probability of text. And more recently we've seen that language models can also be used to generate pre-chained representations of text that encodes some notion of language understanding, and has been shown to be widely useful for different downstream NLP tasks. And then finally, today we're going to touch on this idea that if language models are trained over massive amounts of text, can they even be used as a knowledge base? So we're going to start by looking at what types of factual knowledge a language model might already know. And these examples are taken from a paper by Petroni et al, in EML, P a couple years ago. And the goal is to test the factual or common sense knowledge in existing language models such as Bert large. So let's check out what Bert large predicts. iPod Touch is produced by Apple, London Jazz Festival is located in London, Danny Alve is placed with Santos, Carl III used to communicate in German, and Ravens can fly. So here we have the correct predictions in green and the incorrect predictions in red. And if you know anything about sports, you may know that Danny Alve is a soccer player, Santos is a soccer team. Here they were hoping that it would predict Barcelona, because at least at the time of this data set, apparently he played for Barcelona, and Carl III actually used to communicate in Swedish, not German. So it's good about these examples, it's a predictions are generally reasonable. If you didn't know the ground truth, they all make sense. When you want to predict a language, you do in fact predict a language. But of course they're not all factually correct. So why might this happen? Well, for one, the fact might not have been seen in training. And you can't expect the language model to do more than recall facts that it has seen in training. It can't make up facts about the world for instance. It's also possible the fact is just really rare. So maybe the language model has seen the fact during training, but it hasn't seen it enough times actually memorize the fact. And the last issue is a little more subtle, which a model might just be very sensitive to the phrasing of the fill in the blank statement. And so for example, you might have statements like X was created in blank that the model can't predict correctly, but if you change it to X was made in blank, suddenly it can predict it correctly. And we'll come back to this in how to actually evaluate the knowledge in these language models. So this inability to reliably recall knowledge is a key challenge facing language models today. It'll be the focus of this talk. Recent works have found that language models can recover some knowledge, including the work that Colin presented last class. They've had very encouraging results. But they're still a way to go as we saw with the fill in the blank statements and with these challenges that we just discussed above. So as a result, the past couple years have had a ton of rapid progress in this area of research in terms of trying to figure out how do you actually encode more knowledge in language models. So I also want to motivate why researchers are interested in building language models that can more reliably recall knowledge. And one of these reasons is that the pre-changed representations are used in a variety of downstream tasks. And some of these downstream tests are knowledge intensive. So for instance, you might have a downstream task to extract the relations between two entities in a sentence. And this is commonly known as relation extraction. And this is much easier if you have some knowledge of the entities, which could be potentially provided by this pre-trained language model representation. And we talk about evaluation. We'll talk about what types of tasks are most likely to benefit from these knowledge-rich pre-changed representations. And then as a stretch goal, some researchers are starting to propose the idea that can language models actually ultimately be used to replace traditional knowledge bases. So instead of creating a knowledge base for a fact, like you might right now with SQL, you'd create a language model with a natural language prompt. And of course, this does require the language model to have high quality under calling facts. So we might not be there yet, but it's an interesting direction for us to be moving towards. So I want to make it super clear what I mean by a knowledge base. Here we're just talking about a knowledge graph where the nodes in the graph would be entities. And the edges are going to be relations between the entities. So for example, here we have a subset of a knowledge graph for Franklin D. Roosevelt. And you see the information about his spouse, his place of birth, his date of birth, and so on. An important thing to note is this is a structured way of storing the knowledge, since it's just in a graph form. And you can actually describe these graphs with knowledge graph triples, which will be an important vocabulary word throughout this talk. So knowledge graph triple would be consisting of a subject entity, a relation, and then object entity. So for instance, here we might have Franklin D. Roosevelt, date of birth, January 30th, 1882. And that would form a knowledge graph triple. We'll also refer to this as a parent entity, a relation, and a tail entity. So wiki data is one very popular knowledge base you might come across if you're working this area. It's a free knowledge base that's actually populated by humans, so they're filling in these relations and entities. And it's also multilingual. So if you want information from this knowledge base, what you do is you'd write a SQL query. This is a simplified one. But the idea is you'd want to figure out the date of birth of Franklin Roosevelt, so you would write a query like follows. Now if instead you want to query a language model as a knowledge base, you'll have something like this diagram that you've actually probably seen in several lectures now. And the idea is you'll train a language model over this unstructured text. And then you'll use a language model to just answer these natural language query statements. So here, this is the work on T5 where they're training T5 over natural language or just unstructured text with the span corruption task. And then they're asking T5 when was Franklin D. Roosevelt born? And the idea is T5 will produce a textual answer. So you can see this contrast very much with the old approach of using a traditional knowledge base for the knowledge base is structured and you have the SQL statements to query it. So what are the advantages of using language models over traditional knowledge bases? And why might people think this could be a good idea? Well for one, the language models are pre-trained over large amounts of unstructured and unlabeled text, whereas traditional knowledge bases require manual annotation like with wiki data people actually are populating it, or complex NLP pipelines to extract from unstructured text into a structured form that forms a knowledge base. Language models can also support more flexible natural language queries. So if we take the example, what does the final F in the song UFOF stand for? A knowledge base probably won't have a field for final F, so it won't be able to answer your query. But there's a chance that a language model could actually learn and have a response for this natural language query. They also had a less extreme example in this paper by Petroni and others, where maybe your relation would be is works for in your knowledge base. And the new ask for is working for. And the knowledge base doesn't have an exact match on the field, and so it returns an empty response. And it's much, it's reasonable to believe that your language model could figure out that these relations are similar. So if I know the answer to one of them, I probably know the answer to the other. Of course, it's not all advantages. There's also many open challenges using language models as knowledge bases. So for one, it's harder to interpret. When a traditional knowledge base produces an answer, there's actually provenance information associated with why did it return that particular query. But with a language model, it's really not clear why it might produce a prediction. The knowledge is just encoded in the parameters of the model. It's also harder to trust. So you saw this in Simon 5, where the language model could produce realistic predictions, but they are incorrect. So it's not easy to know when the language model actually knows the fact versus it's using some like biases to make its prediction. And in the case of the traditional knowledge base, if it doesn't know a fact, it's just going to have an empty response. And then finally, language models are harder to modify. So in a knowledge base, if you want to update a fact, you just change the fact directly in the structured data. But a language model is not quite clear how you would do this. You could fine tune the model longer on the updated data, but how do you know if it still has some memorization of the old fact? So there are a lot of open challenges to this goal of actually using language models as traditional knowledge bases. But hopefully you see why some people think this could actually be a good idea. And why researchers are interested in training language models that can actually integrate more knowledge. So that brings us to section two of the talk. So I want to pause here just in case there's any questions. Okay. Okay. Awesome. So now we're going to be talking about what techniques researchers are using to actually add more knowledge to language models. So we're going to talk about three broad classes of techniques. This is by no means exhaustive, but hopefully it gives you a good overview so that if you want to dive deeper, you can. So we'll start by talking about adding pre-trained entity embeddings and for each section we'll kind of focus on the first work that you see in the bullets, but we'll also talk about briefly some of the variants. So you see how the works within each class can differ and what knobs you can turn. So for adding pre-trained embeddings, we first need to figure out what pre-trained embeddings would actually be the most useful to add knowledge to language models. And this can start with an observation that facts about the world are usually in terms of entities. So if we have a fact like Washington was the first president of the United States, we have the entities Washington United States. But pre-trained word embeddings don't have this notion of entities. So we'd have different word embeddings for USA, United States America, and America, even though these all refer to the same entity. And this makes it challenging for the language model to actually learn any representations over these entities, since they may be referred to many ways in the text. So what I've been said, we have a single embedding per entity and law for these as entity embeddings. So now you'd have a single entity embedding for USA, United States America, and America. And whenever you see a phrase in text referring to this entity, you would use the same entity embedding. And these entity embeddings can actually be pre-trained to encode this factual knowledge about the world. And this first class techniques we'll be looking at will be how do you actually best use these pre-trained entity embeddings in a language model. So I need to make up for a quick note that these entity embeddings are only useful to a language model. So if you can do another NLP task called entity linking well. So I'm going to take a quick aside and explain what is entity linking. So a definition of entity linking is the link mentions in text to entities in a knowledge base. I like to think about this in terms of how you use word embeddings. So if you want to use word embeddings and you have a sentence, you're going to first tokenize that sentence into words. And then for each word, you're going to look up their corresponding ID in some word embedding matrix. And now you have your word embedding. Well for entity embeddings, the dictionary look up isn't so easy. You might have sentences like Washington's the first present United States. Well Washington has two different candidates. Are we talking about George Washington or we're talking about Washington state? And these are different entities that have different entity embeddings. And the QIDs here would just be their identifiers and wiki data. And the United States just has a single entity. So a task of entity linking is to figure out correctly these ambiguous mentions what entity do they actually link to in a knowledge base. And there's many different ways you can do this entity linking. The one way you might be able to do this is to figure out that, oh, I see the context word of president. So Washington probably links to George Washington. Just some more definitions. We're going to refer to Washington as a mention in United States as a mention. And then the things that the mentioned could link to so the two options for Washington are going to be candidates. So this is a whole research area of its own. And I encourage you to check out the resources at the bottom if you're interested in learning more. But right now, the most important thing to understand is the entity linking is what is going to tell us which entity embeddings are actually relevant to the text and which ones do you want to use as you iterate through a sequence? There are a few questions around here. One of them is, so that's entity linking, but what about the relations? Yeah. So some of the works we'll talk about will only use the entity embeddings. So some of these have been pre-trained with relation information, but in the end, you only have an entity embedding. So relation extraction is yet another NLP task that you could also do. But yeah, here we're just talking about entity linking. But then if you have the knowledge graph you showed earlier, it had relations in it, right? Do you get any connection between that and the text? I mean, that's the goal of relation extraction, right? Just to figure out, like given the entities, what is relation between them, which would then form the full triple of tail entity and relation? Okay. Then I think people want to know more about how it's going to be used, but maybe you should go on and show some examples. Yeah, I will, for sure. Okay. Great. So entity embeddings, just to summarize, they're like word embeddings, but they're for entities and analogies. So you'll have some vector associated with George Washington, and it should be meaningful in embedding space such that maybe the George Washington vector is close to the vectors for other founding fathers. So we're going to briefly talk about some methods for training entity embeddings. There is knowledge graph embedding methods. You might have heard of the transient embedding method. So this starts from the idea of having these knowledge graph triples, and you want to learn pre-trained entity and pre-trained relation embeddings. And you want to be the case that the subject embedding and the relation embedding, the sum of those two, is close to the object embedding and vector space. So it's an algorithm to learn that constraint. There's also word entity co-occurrence methods. So these build off of word to vec, one of them is even called Wikipedia to vec, and the idea is given an entity, you want to figure out what words are most likely to co-occur around it. And then the last method, or one of the other methods that is common now is actually just using the transformer to learn representations of an entity by encoding the entity description. And so Blink from Facebook is an approach that does this. So the methods we'll talk about today are actually agnostic to how you train your pre-trained entity embedding. But I think it's important to know that there's actually a wide variety of methods to train these pre-trained entity embeddings. And it's actually not clear which method is best for using them downstream and language models. So one of the key challenges that using pre-trained entity embeddings and language models is figuring out how to incorporate them when they're from a different embedding space than the language model. And so what we'll do, or the approach we'll look at today, we'll learn a fusion layer to combine this context and entity information. So we have entity embeddings and we have the contextualized word embeddings from our language model. So if we take a sequence of text, and we imagine that J indicates the J element in a sequence, then the challenge here is you want to figure out how do we combine some word embedding WJ with some aligned entity embedding EK. So here an alignment could be like in the example where we had Washington was the first president, Washington would be your word embedding and George Washington would be the aligned entity embedding there. So you could imagine, in this case, let's say your WJ is Washington and your EK is your entity embedding for George Washington, and you want to align them together. So what you can do is learn a weight matrix WT for the text and WE for the entity to project these embeddings to the same dimension before you sum them and finally take an activation function over them. So the idea is that by having some fusion layer mechanism like this, you can actually use these entity embeddings and these contextual word embeddings that are in different embedding spaces and fuse them together to have this single hidden representation for the element in the sequence. So the project will talk about today, I'll have some mechanism either very similar to this or some variation of this to do this combination of a context and entity information. So the first approach we're going to talk about is called Ernie, enhanced language representation with informative entities. And so this just builds on what we've already talked about. It uses pre-trained entity embeddings and it also uses this notion of a fusion layer. So the first block in Ernie is a text encoder, which is a multi-layer bi-directional transformer encoder for the experiments they use BERT, but it doesn't have to be BERT. And this is followed by a knowledge encoder which has stacked blocks composed of two multi-headed attentions. One is over the entity embeddings and one is over your token or subword embeddings. And then the output of these contextualized entity and token embeddings from the multi-headed attentions are passed to a fusion layer, which looks very similar to what we just looked at. But now you also have new word and entity embeddings that you're producing as output of your fusion layer. So you see this WJ and this EK, which are produced as the next layer of word and entity embeddings. So the I here indicates that it's the I block in the knowledge encoder. So you'll actually have multiple stacks of these knowledge encoders and you'll be doing a fusion of the word entity embedding, producing new word and entity embeddings and then passing this to the next block of the knowledge encoder. So this is what the architecture diagram looks like. On the left side we have the T encoder or the text encoder followed by the K encoder or the knowledge encoder. And then on the right side we have a zoomed in version of the your knowledge encoder. So you see the multi-headed attentions over the tokens and orange and then over the entities and yellow and then you have this alignment between the word and entities with the dash lines. So they have this example as Bob Dylan wrote blowing in the wind in 1962. The entities here are Bob Dylan and blowing in the wind and they have a simple alignment rule where you want to align the entity to the first word in the entity phrase. So you want to align Bob Dylan to Bob. That's what the dash lines trying to indicate and you want to align blowing in the wind to blow. So here this already assumes the entity linking has been done and you know your entities in advance. So you can see that the entities are actually input into the model. So after you have your word and the alignment this goes through the information fusion layer and this light purple gray color and then finally it produces these new word and entity embeddings is output. And then remember that you have multiple blocks of these so as we pass into the next block of your knowledge encoder. So how do you actually train this? It's pretty similar to Bert. You have a mass language model loss and you have an ex sentence prediction loss. And they also introduce a knowledge pre-training task which they refer to as the DEA task. It's named after a denoising entity auto encoder from an ICML paper in 2008. And the idea is they're going to randomly mass these token entity alignments. So the idea that Bob goes to Bob Dylan, they're going to mask that out with some random percentage. And then they're going to predict the corresponding entity for a token out of the entities in the sequence. So this looks like as follows. The summation is over M entities in the sequence. So this would be over Bob Dylan and blowing in the wind in the previous example. And given a particular word, they want to figure out what entities that most likely to align to in that sequence. So does Bob align to Bob Dylan or does Bob align to blowing in the wind? And their motivation for doing this is that if you don't have this task, all you're ever going to be predicting is a token with the mass language model loss. And you really, to encode knowledge, should also probably be predicting over entities. So by adding this task, they have some kind of task that is actually predicting the entity. And they also suggest that this might better fuse the knowledge or the entity and the word representations than just using the fusion layer. And the final loss is then the summation of the mass language model loss, the next sentence prediction loss, and this DEA knowledge pre-training task loss. So they show that a Blatian experiment that it's actually very important to have this knowledge pre-training task. So this has Bert on the left most bar, Ernie as the second bar from the left. And so that's with all the features of Ernie. And then they try removing the pre-trained entity embeddings and removing this knowledge pre-training task. So you see that Bert performs a worst. This isn't very surprising, and that Ernie performs the best. But what's interesting is that if you remove the entity embeddings or you remove the pre-training task, they only do a little better than Bert. And so it's really necessary to actually use this pre-training task to get the most use to your pre-trained entity embeddings. So some strengths of this work were that they introduced some way to combine entity and context information through this fusion layer and this knowledge pre-training task. And then they also show improved performance on downstream tasks, which we'll come back to when we talk about evaluation. But of course, there's also some limitations. So it needs text data with the entities annotated as input. And this is even true for downstream tasks. So if you remember on the architecture diagram, we had the entity information actually input into the architecture. But it's not very realistic that you're necessarily going to have a good entity linker for any downstream tasks that you want to use Ernie on. And the next challenge is this requires more pre-training of your language model. So now you don't just need to pre-train Bert, but you also need to pre-train your knowledge and code around top. For the first challenge, we're going to actually talk about a work that presents a solution to address this. For the second challenge, I encourage you to check out the footnote on the bottom. This introduces a work that actually uses pre-trained entity embeddings, uses them in a language model, and doesn't require any more pre-training. So it's pretty cool. I guess that's all I have for Ernie, so I want to pause here for questions. Well, here's one that's up here. So on the fusion layer, it observed that passing the entity embedding into a fusion layer to provide with the word embedding is more powerful than just concatenating the entity embedding onto the end of the word embedding question mark. Yeah, so I guess people are still a little bit confused as to the motivation of that fusion layer. And so I guess here it's this, the simple strategy would be since you've got the entity linking, you could just concatenate entity embeddings onto the end of word embeddings and do regular Bert, but that worked just as well. I think the idea is it would not, because if you imagine that, let's say, your magnitude is very different, you need some way to, I guess, align the spaces so that anything meaningful in the entity embedding space is still meaningful in the word embedding space. So if you're close in the word embedding space, you also would be, you'd want to be close in the entity embedding space. So I guess that's one argument. Yeah. I mean, I think the question isn't, you know, it's a good question as people say, I mean, it's not completely obvious that it wouldn't work to do that. It seems like one of the potential problems is some words have entity links to them and some words don't. And so then you'd sort of have zero vectors for the ones that don't have anything linked in that way. Act a bit weirdly, but. Yeah. In this case, when they don't have entities linked, which is a great point. Yeah, the first equation just simplifies to the first term plus the bias. So like there's an obvious solution in that case when you're not concatenating that you just don't add on the term. Yeah, that could be one reason too. Okay. Are there any other questions? I think you can go on. Okay. Cool. Right. So now we're talking about nobert. And this is from the same folks that introduced the Elmo work. And the idea here is that they're going to pre-train and integrate into linker as an extension to bird. And so their loss function will now be the summation of the next sentence prediction, the mass language model loss and this entity linking loss. So instead of the knowledge pre-training DEA task from Ernie, we'll have an entity linking loss. And the idea of the entity linker is you'll now have just as normal sequence as input. And the integrated entity linker will figure out what are the entities in the sentence and or what are the mentions in the sentence or the candidates of those mentions and then what should be the scores those entities or the candidates given the context of the sentence. And so this is all done now as part of the model rather than requiring it as some external pipeline stage before you could even use Ernie for instance. So now for downstream tasks, you no longer need these entity annotations. Your integrated entity linker will figure out what the correct entity is and be able to use the correct entity embedding. So there's also this idea that learning is entity linking may actually better in code knowledge than this DEA pre-training task because they show that nobert actually outperforms Ernie on downstream tasks. So one reason this may occur is that if you think about the DEA task, it's actually a bit simpler than just entity linking. So you're trying to predict for instance what Bob linked to out of Bob Dylan and blowing in the wind. And it's much easier even as a human to see that Bob Dylan will more likely link to or Bob will more likely link to Bob Dylan than that Bob will link to blowing in the wind. And the entity linking task, you actually have a much harder set of candidates to predict over. You're not just looking at the ones in the sentence. So does Washington link to George Washington or Washington state actually requires you using more information about the entity. So given it's a harder task, it's not too surprising that it might perform better than just this easier knowledge pre-training task that Ernie introduced. So otherwise, nobert has a lot of similarities to Ernie. It uses a fusion layer that combines this context and entity information and it introduces some knowledge pre-training task. So I'd say a high level takeaways if you want to use pre-training entity embeddings in a language model, you'll probably at least want to consider both of these components in terms of actually going to integrate the pre-training entity embeddings and take the most advantage of a knowledge in them as possible. So that brings us to the next class of techniques, which is using an external memory. And here we'll mainly focus on this work called KGLN and then we'll also briefly talk about KNN LM. So the previous methods that you've talked about have relied on pre-trained entity embeddings to encode the factual knowledge from knowledge bases. And the one problem with this or one of the problems with this is if you want to, let's say, modify your knowledge base. You now need to retrain your entity embeddings and then retrain your language model on top of those entity embeddings. So this begs a question, are there more direct ways in pre-trained entity embeddings to provide the model factual knowledge? And so what we're going to talk about is how you can actually use an external memory or a key value store to give the model access to either knowledge graph triples or context information. And a key thing about this external memory is that it's independent of the learned model parameters. So this means you can actually support injecting and updating factual knowledge. You can do this directly to this symbolic external memory, while let's say changing the value for a particular key or maybe adding another key. And you don't have to retrain or retrain your entity embeddings when you make this change. And the approaches we'll talk about today can actually even have these updates to the external memory without more pre-training of the language model. So that's pretty neat. And then another benefit of using external memory over these pre-trained entity embedding approaches is they can also be more interpretable. So if you have a bug or not bug an air in your model where it's not predicting a correct fact, it's very challenging to figure out with pre-trained entity embeddings what the problem might be. Was it the original knowledge base? Was it the encoding in the entity embeddings? Is it how the language models using the entity embeddings? And here you have a little more information with an external memory. And that you can look in the external memory and see what's the fact in the external memory was not an external memory and so on. So it adds a little bit more interpretability than just using these pre-trained entity embeddings as an inject way to encode the knowledge base. So the first word we're going to talk about is called KGLM and unlike the other approaches we've talked about so far, this actually uses LSTMs and not transformers. So the key idea here is to condition the language model on a knowledge graph. So recall with the standard language model, we want to predict the next word given the previous words in the sequence. Well, now we also want to predict the next entity given the previous words in the sequence and given the previous entities in the sentence or the entities that are relevant to the sentence I should say. So KGLM will be building a local knowledge graph as it iterates over the sequence. And a local knowledge graph is just a subset of a full knowledge graph that only has the entities that are actually relevant to the sequence. So if we have this example here, a simplified example from the paper, that SuperMarioLand is a game developed by Blank. And SuperMarioLand here is an entity. You'd want a local knowledge graph as follows where you see that SuperMarioLand is in the local knowledge graph, but we also have the relations to SuperMarioLand to other entities that are copied from the full knowledge graph into this local knowledge graph. And you would build up this local knowledge graph as you iterate over the sentence. So whenever you see an entity, you would add it to the local knowledge graph as well as its relations to other entities. So obviously this is a much smaller example than what would really have all the relations to SuperMarioLand just for the purpose of the example. But hopefully it's clear that all of these are relevant to the sequence. Something important to note here is that this does assume that the entities are known during training so that you do have this entity annotated data for training. And therefore your local knowledge graph is always the ground truth local knowledge graph as you iterate over the sequence. So why might this be a good idea to do this? Well here the next word you want to predict is Nintendo. And you may notice that Nintendo is in your local knowledge graph. So sometimes this local knowledge graph can actually serve as a very strong signal for what you want to predict for your next word. Now you may be thinking well this wouldn't always be helpful and that's true as well. So if you look at just like the third word in the sequence and you want to predict that word, so is a game for instance. Well if this isn't in the local knowledge graph this wouldn't be necessarily that helpful. You would just do a standard language model prediction. Or if you're at the beginning of the sequence, your local knowledge graph is empty so of course you're not going to get any signal from it. So the first question they ask in KGLM is how can a language model know when to use a local knowledge graph and when it might actually be useful for predicting the next word. So we're going to keep the same example as a running example and we have our local knowledge graph here. We now have an LSTM that looks similar to the representations you've seen throughout this class. And normally you've seen the LSTM predicts the next word. Well now we're also going to use the LSTM to predict the next type of the word. So it's the next word going to be a related entity meaning it's in the local knowledge graph already. Is it going to be a new entity meaning it's not in the local knowledge graph or is it going to be not an entity in which case you just revert to a normal LSTM prediction. And they're going to use the LSTM hidden state to do this prediction of the type of the next word over this three way, three different classes that they might want to consider. So in the case of super Mario Land is a game developed mind Nintendo. We saw that this would be a related entity case because you saw that Nintendo was in the local knowledge graph. For the other cases, super Mario Land would be a new entity case since it's the local knowledge graph is empty at that point. And then any of the words between super Mario Land and Nintendo would be not an entity. Is there just a standard LSTM language model prediction that doesn't involve any entities. So now we need to talk about what the language model actually does in these three different scenarios to predict the next entity and the next word. So we're going to keep the example up at the top in case you want to further back to three different cases. And we're going to start with a related entity case. So here we assume that the next word or entity is actually in your local knowledge graph. And remember that we can describe an knowledge graph in terms of triples. So in terms of pairs of parent entities, relations and tail entities. And in the case of predicting the next word as Nintendo. There's only one possible parent entity in the local knowledge graph, which is super Mario Land. And the goal is you want to figure out what is the most relevant triple that will be useful in helping to predict the next word. So in this case, you could have the triple super Mario Land publisher Nintendo. You might have the triple super Mario Land genre platform game, which of these is actually helpful in predicting that Nintendo should be the next word. So here what you would want KGLN to do is predict that the top scoring parent entity is super Mario Land. And the top scoring relation is publisher. You can see there are actually contextual cues in the sentence that could help you figure out which triple you're talking about. And then given that your top scoring parent entity is super Mario Land and your top scoring relation is publisher, you can figure out that using knowledge graph triples, the tail entity has to be Nintendo. And therefore, this gives you a strong signal that the next word will be Nintendo. So the goal is you're going to find the top scoring parent entity and the top scoring relation using the nodes in your local knowledge graph. And you can do this by using the LSTM hidden state combined with pre-trained entity and relation embeddings. So I do admit I cheated here a little bit in that this does use pre-trained embeddings, but hopefully you'll see by the end of this discussion why I think it fits a bit better in this external memory use case as well. So what they're going to do is they're going to take a softmax using the LSTM hidden state and the entity embeddings for each of the potential parent entities and they'll take this top scoring one as a parent entity and they'll do the same thing for the relation embeddings. The next entity is then just this tail entity from the knowledge graph triple. So it's relatively trivial to figure out what the next entity should be once you've figured out the top scoring parent entity and your top scoring relation. And then finally to predict the next word, they take the vocabulary and they expand it to include different aliases that could refer to that entity. So what I mean by aliases here are phrases that could refer to the entity in text. So you might not just call it Nintendo, you might also say Nintendo Company or Copie and you want any of these to be possible words that you could predict as the next word. So the goal of this vocabulary expansion is to increase the probability that the next word you predict will actually be related to this next entity. So the new entity case is a bit simpler, this means that the entity that you're predicting is not in the local knowledge graph. So you're not getting any signal from this local knowledge graph that you've been building up. And all you want to do is find the top scoring entity in the full knowledge graph and you can do this using the LSTM hidden state and preaching and TMPeddings, similar to how we found the score for the top parent entity. Your next entity will just be the top scoring entity out of the full knowledge graph. And then your next word is once again, this vocabulary expanded to include aliases of that entity. The not in the entity case is the simplest. You just revert to normal LSTM. You don't have an X entity to predict. And your next word is just the most likely next token over your normal vocabulary. So here's a diagram from the paper that hopefully summarizes and makes even clearer what I just went over. So they have a longer example than the one we were looking at, but it's the same prediction as Nintendo's next word. And they have their predictions in red. So this is what they want KGLN to predict. The three different cases are in the horizontal. And we see that here, you're in the related entity case, since Nintendo is in your local knowledge graph. So they want KGLN to predict that Nintendo should be a related entity type of word, that Super Mario Land should be its parent entity, that publisher should be the relevant relation. And as a result, the next entity is Nintendo. And then they expand the vocabulary. You see that aliases of Nintendo at the bottom. And then finally, they actually predict Nintendo is the next word. And the other case is just summarized what we also already went over. So you find that KGLN actually outperforms GPT2 and AWD LSTM, which is a strong LSTM language model. On a fact completion task, similar to the fill in the blank examples that we looked at at the beginning of the talk, they also find qualitatively that compared to GPT2, KGLN tends to predict more specific tokens, since it can predict these tokens from just copying from the local knowledge graph. Whereas GPT2 will tend to predict more generic tokens. So if you want to predict the birthplace of someone, GPT2 is more likely to predict New York, for example, and KGLN might predict some obscure place. And then they have these really cool set of experiments where they show that KGLN actually supports modifying or updating facts. So they made a direct change in the knowledge graph, and then they saw what is the change in KGLN's predictions. So they have this example where the sequence was Barack Obama is born on blank. They had their knowledge graph triple as Barack Obama's original birth date, and then their most likely next tokens were as expected, August 4, 1961. And then they just changed their knowledge graph. So they changed the birth date of Obama. And they said, OK, he's now born 2013. And they looked to see what the next predictions were for KGLN, and it changed its predictions to match what was in the local knowledge graph. So this is something that's pretty cool, and that really only external memory approaches can do compared to these to the original pre-chain etian betting approach we talked about. And I think it's one of the reasons that KGLN at least my opinion fits better in these external memory use cases. Right, so the next slide is a different paper. So I guess I'll take questions on KGLN. Is there any? It's a pretty complex method, so feel free to have questions. Yeah, could you one more time explain what the definition of the local knowledge graph is in relationship to the global knowledge graph? Yep. So local knowledge graph is supposed to be a subset of the full knowledge graph, and it's only supposed to consist of entities that are actually have actually been seen in the sequence as well as their relevant entities. OK, so here you see that SuperMario land is in the local knowledge graph because SuperMario land is an entity that is seen in the sequence. And then you also want to copy over all the edges from SuperMario land that would be in the full knowledge graph. So this is just a subset of them for the purpose of the example, but you see that SuperMario land has an edge in Tendo to gain void platform gain. And so you would copy all edges that SuperMario land has to another node in the full knowledge graph. And they know in advance like they have the labels here for what the entities are during training. So that's how they can actually create this ground truth knowledge graph. And briefly, a student asked why we can't just use the whole knowledge graph and I gave an answer, but maybe you know better. Yeah, I think the idea is the signal will be much stronger if you just use local knowledge graph. So in the softmax for the related entity case, you would just be predicting over the potential parent entities in your local knowledge graph, which is a much smaller set than what's in your full knowledge graph. So I guess it's more likely that you're going to predict something that is correct in that case. Then when you have like 5 million or so entities in your full knowledge graph, it's also much cheaper to compute. In this case, there's only a single parent entity, but you could have multiple parent entities that you're trying to compute, which one's most likely over. Is that what you were also thinking? Yeah, I mainly just said efficiency. So the signal thing is cool too. Who's an exciting question, what about queries that require more than one step in the knowledge graph, such as the location of the publisher of Superrario Land? Yeah, that's a good question. So the idea is like can it support those types? Does it support multi-hop kind of building of the knowledge graph? Yeah, yeah, it's like KGLM perform in those cases. Yeah, I don't know. That's a very good question. They build up the knowledge graph, so that is just single hop as far as I know. But like if you saw the other entities, if you were to see the entities along the hops, it would have them in the local knowledge graph. Yeah, that's a good question. I don't know if they explored that. Great. Okay. Let's move along then. Okay. So the next piece of work we're going to talk about, you guys have actually briefly seen in the natural language generation lecture, but I'm going to go over it again quickly here. So unlike the other work, so you talked about that use knowledge graph, Chipples, this is actually going to take kind of a looser notion of knowledge in that the knowledge will just be encoded in the text and the training data set. So this is called K&N LM and the idea is that or it's building an idea that language models not only learn to predict the next word in text, but they also learn these representations of text. And the authors suggest that it might actually be easier to learn similarities between text sequences than it is to predict the next word in the text. So you have this example that Dickens is the author of blank and Dickens wrote blank. And they argue that it's easier to tell for human, but also for a model that these sequences are similar and they should probably have the same next word, even if you don't know what the next word is. So that's suggesting that it's easier to learn these similarities than it actually predict the next word. And they argue that this is even more true for long tail patterns, where it's very challenging for the model to predict that the next word is some rarely seen token or rare entity than it is to find another similar sequence that it's already seen and just copy the next word from that sequence. So what they propose to do is to our representations of text sequences in nearest neighbor data store. And then at inference, what you'll want to do is you find the K most similar sequences of text. You then retrieve their corresponding values, so you just peek at those sequences and see what were their next words. And then you combine the probability from this nearest neighbor data store with just a typical language model prediction. And so they call this an interpolation step in that they're reading how much to pay attention to the probability from this K and N approach and how much to pay attention to this language model approach. And the lambda here is just a hyperparameter they tune. So they have this diagram from their paper where they want to predict the next word in the sequence, Shakespeare's play blank. And so what they do is they have all the training context already encoded in their data store. So they have representations of all the training context. And then they compute representation of their text context. And they want to figure out which representations in the training context are most similar to this text text context representation. And so here in external memory view of things, the keys would be the representations of the training context and the values would be the next words. So they get the K nearest training representations. They then copy over their values. So that's what you see with this Macbeth Hamlet Macbeth example. They have a normalization step where they convert this to probability space. And then finally, they have an aggregation step. So if a word is seen as the next word and several of these K nearest neighbors, then they want to count more for that. So that's why they aggregate. So they see Macbeth twice. It means Macbeth is more likely. And then finally, they have this interpolation step where they try to balance between the classification probabilities from the language model and from the K and N approach. So some immediate observation you might have is this seems really expensive. They do propose ways to kind of try to minimize the expense of actually having to store all the training context in this data store because they actually store it for every single window of next word in the training context. And you can do quantization on some nearest neighbor approaches to try to make this less expensive. But I imagine this would still be pretty expensive for really large training data sets. They also have some cool experiments that show that this is very good for domain adaptation. So if you take your language model and you have a new domain that you want to apply your language model to, you could just create a nearest neighbor data store of your new domain. So you code all the representations that new domain you stick in a data store. And then you can just use your language model with these K and N probabilities as well, which is immediately on this new domain without actually having to further train your language model. So I thought that was a pretty cool use case of this external memory approach. So while it doesn't leverage knowledge bases directly, it does have this loose knowledge of or loose idea of encoding knowledge that is in a textual representation form into some external memory that the model can then take advantage of. That's all I have for this approach. Are there any questions on this approach? Well, so only one person is asking, how does the K and N make predictions for the next word? The K neighbors are for the context instead of the next word. Oh, okay. That was unclear. So the keys are the representations that context the values in your external memory are the next words. So when you figure out you figure out your nearest neighbors using your keys and then you copy over their values. So it does actually know what the next words are for each of those representations. Okay. So finally, we're going to talk about how you can just modify the training data to better and code knowledge and language models. So approaches you've talked about so far are actually incorporating knowledge explicitly by using the pre-trained embeddings or an external memory. We also want to talk about how can you just incorporate knowledge implicitly through the unstructured text. So what we're going to do is either mask or crop the data to introduce additional training tasks that require factual knowledge to figure out what data was masked, for instance. So some clear advantages. It doesn't have an additional memory or computation requirements. You don't have a data stored at deal with. You don't have extra knowledge and coder layers to train. All you do is modify the training data. And you don't have to modify your architecture either. So you can continue using your favorite bird model and just make these changes to the training data. So the first work we're going to look at is called WKLM, weekly supervised knowledge pre-training language model or pre-trained language model. And the key idea here is to train the model to distinguish between true and false knowledge. So they're going to corrupt the data by replacing mentions in the text with mentions that refer to different entities of the same type to create what they refer to as negative knowledge statements. And then the model will just predict has the entity been replaced or corrupted. This type constraint is necessary to make sure that or to encourage the model to actually use factual knowledge to figure out if this corruption is taking place. So you could imagine if you replace it with something that's not realistic at all, the model could just be basing its prediction based on is this sentence linguistically correct. So as an example, we have a true knowledge statement as JK rolling is the author of Harry Potter. And then we want to modify this to replace it with another author. So let's say we change this to JR Tolkien is the author of Harry Potter. So you can see that this requires some amount of knowledge background knowledge actually able to figure out which statements true and which statement is false. And the idea is that the model will be able to predict free to these mentions, whether it's a true or false mention. So this diagram here is from the paper and hopefully explains this a bit better. They have their original article on the left and then they have their replaced article with the corruptions on the right and the entities are in blue. So what they do is for a given entity, they first look up its type, they find other entities of that type. And they randomly sample the entity and get an alias of it to replace in the text. So they're going to replace Stanley, for instance, with Brian Johnson and Marvel Comics with DC Comics and their placements are in red on the right. And then the idea is that the model will be able to predict for each of these mentions was it replaced or not. So in the case of Brian Johnson, they have the red X for this is a false mention and in the case of the true mentions, they have the check mark. So this is a pretty simple approach, but they actually show that it can help the model increase the amount of knowledge that's encoded in parameters. So WKLN uses an entity or placement loss to train the model to distinguish between these true and false mentions. And this just looks like a binary classification loss where your true mentions are on the left and your false mentions are on the right. And you want to increase the probability that this P of E given C, so the probability of entity given the context, you want to increase that for the true mentions and decrease it for the false mentions. The total loss is then just a combination of the mass language model loss and this entity or placement loss. The mass language model loss is defined at the token level and the entity or placement loss is defined at the entity level, meaning it's not just over sub words, it's even potentially over words if you have multi word entities phrases, for instance. And this is an important theme that we really see occurring throughout these works that we'll look at in that modifying the data at the entity level seems to be an important component of actually increasing the amount of knowledge that a language model can encode. So you find that WKLN improves over BERT and GPT2, in fact completion tasks like the fill in the blank statements that we looked at at the beginning. They also find that it improves over the Ernie paper that we talked about on a downstream task. And they had a set of the Blatian experiments where they looked at, can you just remove this mass language model loss now? And if you just train BERT for longer, do you really need this entity or placement loss? So it's at the table here is looking at, the second row is looking at if we remove the mass language model loss, what happens. We see that it performs much worse without the mass language model loss. So you really need both losses. The intuition there was the mass language model loss helps to encode just general language understanding. And then training BERT for longer performs much worse than using this entity or placement loss. So this motivates even farther that you really do need. Or the entity or placement loss is actually really helping encode more knowledge in these language models. So in addition to corrupting the data, we're also going to look at, can we just mass the data differently? Can we be more clever about how we do the masking? And this is how thread and several recent works. So there's actually another paper called Ernie. So this is different than the one we talked about before. And this is enhanced representation through knowledge integration. And what they do is show improvements on downstream Chinese and LP tasks by doing phrase level and entity level masking. So instead of just masking out sub words, they're going to mask out phrases of multiple words and entities before phrase and entity which corresponds to some entity in the text that they might find like any art techniques, for example. And then the second work is actually something you heard about in the last lecture, which is the idea of using salient span masking to mask out salient spans. And a salient span is just a named entity or a date. So you can see this is pretty similar to what Ernie is doing. And they found that using salient span masking actually significantly helped T5 performance on these closed domain question answering tasks. So just to make sure we're all on the same page with the different masking techniques, this diagram from the Ernie paper is comparing to what Bert does versus what Ernie does. The top shows that Ernie massed out the sub word tokens or that Bert massed out the sub word tokens, whereas Ernie massed out phrases like a series of as well as entities like JK rolling. There's some interesting results on showing that salient span masking is helping encode more knowledge in these representations. So on the left, we're looking at the results of the original paper that proposed salient span masking. So this is the realm work. And the idea here was that they were training a knowledge retriever. So it's actually more of an external memory class of techniques, but they find that by using the salient span masking technique, they could actually train a much better knowledge retriever. So it's a good example of how these techniques are really complimentary. So while I presented three classes of techniques, you can definitely get benefits by doing multiple techniques together. And they found that doing salient span masking compared to using masking from Bert, which would be the random uniform masks or doing random masking of spans from a paper called Spanbert, it performs much better to do salient span masking. So you see a 38 exact match score versus like a 32 exact match score, for instance. And on the right, we have results from fine tuning T5 with either salient span masking or the span corruption task that you saw on assignment five. And you can see that on these different QA data sets, salient span masking is significantly better than just using the span corruption technique. So this really suggests that during the salient span masking and masking out the salient spans of these entities is in fact helping to encode more knowledge in these language models. So to recap, we talked about three different classes of techniques to add knowledge to language models. We talked about using pre trained entity embeddings. These weren't too difficult to apply to existing architectures and is a way to leverage this knowledge graph pre training. But it's rather inject way of incorporating knowledge and it could be hard to interpret. We also talked about approaches to add an external memory. This could support modifying the knowledge base. It was also easier to interpret. But they tended to be more complex in implementation like we saw KGLM and they also required more memory like we saw the K and N LM approach. And then finally we talked about modifying the training data. So this requires no model changes or additional computation. It also might be the easiest to theoretically analyze. So it's actually an active area research right now. But still open question if modifying the training data is always as effective as model changes and what the trade offs are in terms of how the data required versus doing one of these other knowledge enhancement approaches. So that leads us to section three. So I guess I'll pause again for questions. I think we may be good. Awesome. Okay. So section three is about how researchers are actually going about evaluating the knowledge and language models. And I guess how some of the techniques we actually just talked about stand up in this evaluation. So first we're going to talk about probes which don't require any fine tuning of the language model. And then we're going to talk about downstream tasks which look at how well do these pre-trained representations actually transfer their knowledge to other tasks. So one of the initial works in this area was called LAMA. And this really started a series of works to look into how much knowledge is already encoded in these language models. So their question was how much relational, common sense and factual knowledge is in off the shelf language models. So this is just taking pre-trained language models and evaluating the knowledge in them. And this is without any additional training or fine tuning. So they mainly constructed a set of what they're for to its closed statements. And these are just the fill in the blank statements that we actually drew from at the beginning of the talk. And we'll have some more examples here. And they mainly created these templates of closed statements using knowledge graph triples and question answering pairs from existing data sets. They wanted to compare pre-trained language models to supervised relation extraction and question answering systems to see how do these language models that were trained in unsupervised fashion compared to these baseline systems that are not only supervised but really targeted for this task of knowledge extraction. And their goal was to evaluate the knowledge in existing pre-trained language models. And a key point about this is like they're just using the language models as they are available to researchers. So this means there could be differences in the pre-trained corpora, for example. So when you look at the following table in your comparing language models, also keep in mind that these don't account for the differences in the pre-trained corpora. So a lot of these language models probably look familiar to you either from previous lectures or maybe your final projects. And what we see is that overall, birth base and birth large pre-trained models are performing much better than the previous language or the other language models here. As I forgot to mention what mean precision at one is. This is a pretty simple metric. The idea is if you look at the blank and you look at the top predictions for the top prediction for the blank, is it correct or not? That's what precision at one means, precision at 10 would be, let's look at the top 10 predictions, is it correct prediction in the top 10? So in addition to birth large and birth base performing well overall, we do see that in the TREX dataset, the relation extraction baseline is performing a bit better than birth. One thing they notice here that's pretty interesting is that this dataset has a lot of different types of relations. And relations can be classified in terms of are they a one-to-one relation, are they an end-to-one relation, are they an end-to-end relation? An example of a one-to-one relation would be your student ID relation, so you have a unique student ID. An example of an end-to-end relation would be the enrolled in relation, so there's lots of students enrolled in lots of classes, so this would be an end-to-end relation. And they find that birth really struggles on these end-to-end relations. So while it performs better than relation extraction baseline on some types of relations, overall it does pretty terribly on these end-to-end relations, so overall it does a bit worse than the baseline on this TREX dataset. They also compare to squad on Docker QA, and they find that it does a fair amount worse. They note that the language model is not fine tuned here, and also has no access to an information retrieval system. And then when they look at the precision at 10, they find that this gap between Docker QA's performance and birth actually closes quite a bit, which suggests that these language models do have some amount of knowledge encoded in them, and that they're even competitive with these knowledge extraction supervised baselines. So you can also try out examples on their GitHub repo for the Lama probe. We have an example that was from their repo that was the cat is on the mask. You can see what the top 10 predictions are to fill in the closed statement. Here they have the cat is on the phone. So this can be a fun way to just figure out what factual and common sense knowledge is in existing language models, and it's pretty easy to use with this interactive prompt. So some limitations on the Lama probe are that it can be hard to understand why the models perform well when they do. So for instance, birth might just be predicting those popular token, and this happens to be right. Maybe it's just memorizing co-occurrence patterns and doesn't really understand the knowledge statement and doesn't understand what the fact is. It might also just be identifying similarities between surface forms of the subject and object. So for instance, an example, Pope Comet VII has a position of blank. Even if you don't know anything about Pope Comet VII, you might be able to figure out that Pope is a likely next word for this triple or for this template. So the problem with this is if the model is just making these predictions based on these surface forms or co-occurrence patterns, it's difficult to know for actually evaluating the knowledge in the model. Maybe it's just making correct predictions for other reasons. And then more subtle issue that we've brought up is that language models might be just sensitive to the phrasing of the statement. So for each triple in their data set or for each relation in their data set, they just had one manually defined template. And qualitatively they found that if they just make small changes as template, it could actually change whether or not the model could recall the correct prediction or not. And so this means that the probe results are really a lower bound on the knowledge that's encoded in the language model. So if you change the phrasing, it's possible that the model might show that it actually does have the knowledge encoded in it. So the next lines of work we'll talk about are really building on these two limitations of this original Lama probe. So the first one is called Lama Un or Lama Unhelpful Names. And the key idea is to remove these examples from Lama that can be answered without the relational knowledge. So this is kind of addressing the first limitation on the last slide. So they observed that BERT relies on just surface forms entities, might not be using knowledge, make these predictions. This includes a string match situation that we talked about with the Pope. This also is dealing with the revealing person name issue that you saw in assignment five. So this is where the name could be an incorrect prior for the native language of someone, their place of birth, their nationality. They have this example from the table or from the paper, but they look at different people names or person's names, and then they look at BERT's prediction for their native language. And these are all French speaking actors. And BERT just predicts very biased and stereotypical languages for these particular names. So this can really work both ways. It can lead BERT to make incorrect predictions in some cases, but it could also work to let BERT make correct predictions, even if it has no factual knowledge of those people. So that's the issue they're trying to get at here is do we know that BERT actually knows this fact or is it just using some bias to make its prediction? So what they do is they introduce a couple heuristics to basically just filter out these examples from the LAMA probe that can either be solved by the string match setting or the serbiling person name setting. So they make a harder subset of the LAMA data set essentially. They find that when they test BERT on this harder subset that its performance drops about 8%. But when they test their knowledge enhanced model, which they call Ebert, the score only drops about 1%. So it's possible that as you make harder knowledge probes, we'll actually see even bigger differences in the performance of knowledge enhanced models to models without these knowledge enhancements. The next piece of work we'll talk about is actually getting at this issue of the phrasing of the prompt might actually trigger different responses from the language model. So the language model might know the fact, but it might fail on the task due to the phrasing. One reason this might happen is the pre-training is on different contexts and sentence structures in the query. So for example, you might have in your pre-training corpus, the birthplace of Barack Obama is Honolulu Hawaii. And this might be something you've seen with Kapedia for instance, that's a comment training data set. And then as a researcher, you write Barack Obama is born in blank. And you can see that these sentence structures are pretty different. So the model might have seen the first fact, but the sentence structure difference is actually enough to confuse it so it can't answer this query. So what they do is they generate a lot more of these prompts by mining templates from with Kapedia, one of their techniques actually uses dependency parsing, and also generating paraphrased prompts by taking inspiration from the machine translation literature and using back translation. So they generate a lot more prompts to try to query the language models and figure out do small variations in the prompt, trigger the correct prediction from the language model. They also experiment the sampling prompts. So if we give the model multiple prompts and then take some probability averaged over these different prompts, can we improve the performance on the model returning the correct prediction? So we give it a higher chance of seeing a context that it might have actually seen during pre-training. They find that the performance on Lama increases when they either use a top performing prompt or when they use this sampling approach. So it suggests that the original Lama really was a lower bound on the amount of knowledge encoded in these language models. And changing the phrasing can actually help the model recall the correct answer. This table is a bit frightening, but they find that small changes in the query can lead to really large gains on performance. So if you just have a query like x plays in y position, and then you change that to x plays at y position, this can actually lead to like a 23% accuracy gain on this particular relation in terms of the model actually being able to recall the correct answer. Or even just x was created in y to x is created in y 10% accuracy gain. So I think this motivates the need to not only develop better ways to query these models, but probably also build language models that are actually more robust to the query itself. So in addition to probes, another way to evaluate these language models is by looking at how well they transfer from the pre-trained representation to downstream tasks. And so the idea here is you're actually going to find two in the pre-trained representation on different downstream tasks, similar to how you evaluate Bert on glue tasks. So common tasks that are used for this are relation extraction, entity typing, and question answering. Relation extraction is where you want to predict the relation between two entities. So this is getting back at one of the questions earlier in this talk in terms of well, how do you get the relation that's the edges in these knowledge bases? So given two entities, you learn a model to predict what is a relation between them. Entity typing is a task of giving an entity what is the type of the entity. So here, Alice Rob the bank, you want to predict her as a criminal. And then you guys are very familiar with question answering. So the idea of these tasks is that they're knowledge intensive, so they're good candidates to see how well do these pre-trained representations actually transfer the knowledge to these downstream tasks. So we look at the performance on a relation extraction benchmark called tackred and all the models that we show here for one point stay the art on tackred. So this CGCN is a graph convolutional neural network over dependency trees. The Bert LSTM base is a, it's one of the first works that showed that you could actually get state of the art performance with Bert on relation extraction. And this is just putting LSTM layer over Bert's output. Ernie is the work that we talked about with the pre-trained entity embeddings. Actually the blanks we didn't get to today, but it's a really interesting work about learning meaningful relation representations. And it falls more into the training data modification approaches and that they are actually masking out entities again. And then no Bert is what we talked about. The W and W here means the action code two knowledge bases in no Bert. So they're encoding wordnet and they're also encoding Wikipedia. And the high level takeaway from this table is that you can see that the recent knowledge enhanced models have achieved state of the art over the original models that once performed very well on tackred. And we have about five F1 gains here. Another interesting takeaway from this table is there seems to be a trade-off in the size of a language model that's necessary to get a certain performance. So if you just consider the size of a language model, then no Bert forms the best. But if you don't consider that, then it's high with matching the blanks. So overall, this is pretty good evidence that these knowledge enhanced methods are in fact transferring to these knowledge intensive downstream tasks that can really take advantage of these pre-trained representations. We also have results on entity typing. So here we're comparing to slightly different set of models. Some of the base signs are LSTM models that were designed for entity typing. And we have Ernie and Nobert leading the, I guess, leaderboard here on the entity typing task of open entity. And we see gains of about 15 F1 points with Ernie and Nobert. So once again, we really do see that these knowledge-rich pre-trained representations are transferring and helping on these knowledge intensive downstream tasks. So just to recap, we talked about probes which evaluate the knowledge already present in models. These don't require any more training. But it can be challenging to construct benchmarks to actually make sure you're testing the knowledge in these language models. It can also be challenging to construct the queries used in the probe. We then talked about downstream tasks. These are a bit of an indirect way to evaluate knowledge in that they have this extra component of fine tuning. But it's a good way to evaluate how useful is this knowledge-rich pre-trained representation in actual applications. So I just touched on the exciting work in this area, but there's many other objections if you want to dive more into this. So there's retrieval augmented language models which learn knowledge retrievers to figure out what documents might be relevant for predicting the next word. There's work in modifying the knowledge in language models. So I talked about how this is one of the obstacles and challenges to using language models as knowledge bases. So there's been recent work in this area. We also saw how important the knowledge-pre-training task was. Well, there's many papers that are proposing different tasks to do the knowledge-pre-training. So it's still an open question in terms of what tasks are best to add to and code more knowledge. And there's also been work on more efficient knowledge systems. So add an search-mounted efficient QA challenge which aims at building the smallest QA system. And then finally, there's been work on building better knowledge benchmarks that build on the benchmarks that we saw today. So that's all I have for today and I hope your final projects are going well.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.92, "text": " Welcome to CS224N Lecture 15.", "tokens": [4027, 281, 9460, 7490, 19, 45, 37196, 540, 2119, 13], "temperature": 0.0, "avg_logprob": -0.2986312734669653, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.023645181208848953}, {"id": 1, "seek": 0, "start": 7.92, "end": 11.02, "text": " So I'm Megan and I'm one of the cities in this course and I'm also a PhD student", "tokens": [407, 286, 478, 21332, 293, 286, 478, 472, 295, 264, 6486, 294, 341, 1164, 293, 286, 478, 611, 257, 14476, 3107], "temperature": 0.0, "avg_logprob": -0.2986312734669653, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.023645181208848953}, {"id": 2, "seek": 0, "start": 11.02, "end": 12.72, "text": " working at Kirste.", "tokens": [1364, 412, 591, 653, 68, 13], "temperature": 0.0, "avg_logprob": -0.2986312734669653, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.023645181208848953}, {"id": 3, "seek": 0, "start": 12.72, "end": 18.92, "text": " And today I'll be talking about integrating knowledge and language models.", "tokens": [400, 965, 286, 603, 312, 1417, 466, 26889, 3601, 293, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2986312734669653, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.023645181208848953}, {"id": 4, "seek": 0, "start": 18.92, "end": 21.6, "text": " So some quick reminders, your project milestones we do today.", "tokens": [407, 512, 1702, 43458, 11, 428, 1716, 42038, 321, 360, 965, 13], "temperature": 0.0, "avg_logprob": -0.2986312734669653, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.023645181208848953}, {"id": 5, "seek": 0, "start": 21.6, "end": 24.36, "text": " So hopefully you turn those in already or we'll be turning them in in the next couple", "tokens": [407, 4696, 291, 1261, 729, 294, 1217, 420, 321, 603, 312, 6246, 552, 294, 294, 264, 958, 1916], "temperature": 0.0, "avg_logprob": -0.2986312734669653, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.023645181208848953}, {"id": 6, "seek": 0, "start": 24.36, "end": 25.52, "text": " of days.", "tokens": [295, 1708, 13], "temperature": 0.0, "avg_logprob": -0.2986312734669653, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.023645181208848953}, {"id": 7, "seek": 0, "start": 25.52, "end": 29.16, "text": " And we'll try to get you feedback on those as fast as possible.", "tokens": [400, 321, 603, 853, 281, 483, 291, 5824, 322, 729, 382, 2370, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.2986312734669653, "compression_ratio": 1.5740740740740742, "no_speech_prob": 0.023645181208848953}, {"id": 8, "seek": 2916, "start": 29.16, "end": 32.56, "text": " So something to be aware of is a change of grading basis and of course withdrawal deadline", "tokens": [407, 746, 281, 312, 3650, 295, 307, 257, 1319, 295, 35540, 5143, 293, 295, 1164, 30646, 20615], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 9, "seek": 2916, "start": 32.56, "end": 34.64, "text": " is this Friday.", "tokens": [307, 341, 6984, 13], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 10, "seek": 2916, "start": 34.64, "end": 37.92, "text": " So if you want to make any change of your grade, make sure to do that by then.", "tokens": [407, 498, 291, 528, 281, 652, 604, 1319, 295, 428, 7204, 11, 652, 988, 281, 360, 300, 538, 550, 13], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 11, "seek": 2916, "start": 37.92, "end": 41.120000000000005, "text": " And we'll be getting you the grades back on assignment five by then as well in case", "tokens": [400, 321, 603, 312, 1242, 291, 264, 18041, 646, 322, 15187, 1732, 538, 550, 382, 731, 294, 1389], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 12, "seek": 2916, "start": 41.120000000000005, "end": 44.32, "text": " that's helpful in making your decision.", "tokens": [300, 311, 4961, 294, 1455, 428, 3537, 13], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 13, "seek": 2916, "start": 44.32, "end": 46.6, "text": " And finally, your final projects are due in two weeks.", "tokens": [400, 2721, 11, 428, 2572, 4455, 366, 3462, 294, 732, 3259, 13], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 14, "seek": 2916, "start": 46.6, "end": 49.400000000000006, "text": " So hopefully those are going smoothly.", "tokens": [407, 4696, 729, 366, 516, 19565, 13], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 15, "seek": 2916, "start": 49.400000000000006, "end": 52.44, "text": " So topic of the day is integrating knowledge and language models.", "tokens": [407, 4829, 295, 264, 786, 307, 26889, 3601, 293, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 16, "seek": 2916, "start": 52.44, "end": 56.0, "text": " You've seen a bit about this idea in assignment five and also in call and raffles lecture", "tokens": [509, 600, 1612, 257, 857, 466, 341, 1558, 294, 15187, 1732, 293, 611, 294, 818, 293, 367, 40349, 7991], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 17, "seek": 2916, "start": 56.0, "end": 57.400000000000006, "text": " last class.", "tokens": [1036, 1508, 13], "temperature": 0.0, "avg_logprob": -0.1883974209637709, "compression_ratio": 1.756923076923077, "no_speech_prob": 7.0290217990987e-05}, {"id": 18, "seek": 5740, "start": 57.4, "end": 61.72, "text": " In assignment five, the task goes to train a model to predict the birthplace of a person", "tokens": [682, 15187, 1732, 11, 264, 5633, 1709, 281, 3847, 257, 2316, 281, 6069, 264, 3965, 6742, 295, 257, 954], "temperature": 0.0, "avg_logprob": -0.18773247213924632, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8923752577393316e-05}, {"id": 19, "seek": 5740, "start": 61.72, "end": 63.16, "text": " given their name.", "tokens": [2212, 641, 1315, 13], "temperature": 0.0, "avg_logprob": -0.18773247213924632, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8923752577393316e-05}, {"id": 20, "seek": 5740, "start": 63.16, "end": 65.67999999999999, "text": " And you sell it by pre training on a larger data set.", "tokens": [400, 291, 3607, 309, 538, 659, 3097, 322, 257, 4833, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.18773247213924632, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8923752577393316e-05}, {"id": 21, "seek": 5740, "start": 65.67999999999999, "end": 69.52, "text": " You're actually able to do better on this task since you could encode some role knowledge", "tokens": [509, 434, 767, 1075, 281, 360, 1101, 322, 341, 5633, 1670, 291, 727, 2058, 1429, 512, 3090, 3601], "temperature": 0.0, "avg_logprob": -0.18773247213924632, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8923752577393316e-05}, {"id": 22, "seek": 5740, "start": 69.52, "end": 71.64, "text": " into the language model.", "tokens": [666, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18773247213924632, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8923752577393316e-05}, {"id": 23, "seek": 5740, "start": 71.64, "end": 76.88, "text": " And then last lecture call and raffle presented how T5 could actually be fine tuned for a closed", "tokens": [400, 550, 1036, 7991, 818, 293, 367, 29264, 8212, 577, 314, 20, 727, 767, 312, 2489, 10870, 337, 257, 5395], "temperature": 0.0, "avg_logprob": -0.18773247213924632, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8923752577393316e-05}, {"id": 24, "seek": 5740, "start": 76.88, "end": 81.96000000000001, "text": " domain question answering task such that you can give T5 a natural language question and", "tokens": [9274, 1168, 13430, 5633, 1270, 300, 291, 393, 976, 314, 20, 257, 3303, 2856, 1168, 293], "temperature": 0.0, "avg_logprob": -0.18773247213924632, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8923752577393316e-05}, {"id": 25, "seek": 5740, "start": 81.96000000000001, "end": 84.03999999999999, "text": " it'll return an answer.", "tokens": [309, 603, 2736, 364, 1867, 13], "temperature": 0.0, "avg_logprob": -0.18773247213924632, "compression_ratio": 1.7017543859649122, "no_speech_prob": 1.8923752577393316e-05}, {"id": 26, "seek": 8404, "start": 84.04, "end": 87.2, "text": " So they will be building on these threads and looking at techniques that researchers have", "tokens": [407, 436, 486, 312, 2390, 322, 613, 19314, 293, 1237, 412, 7512, 300, 10309, 362], "temperature": 0.0, "avg_logprob": -0.15645675361156464, "compression_ratio": 1.9475409836065574, "no_speech_prob": 9.079705705516972e-06}, {"id": 27, "seek": 8404, "start": 87.2, "end": 92.72, "text": " recently been developing to increase the amount of knowledge in language models.", "tokens": [3938, 668, 6416, 281, 3488, 264, 2372, 295, 3601, 294, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15645675361156464, "compression_ratio": 1.9475409836065574, "no_speech_prob": 9.079705705516972e-06}, {"id": 28, "seek": 8404, "start": 92.72, "end": 95.28, "text": " So we're going to start with a quick recap of language models just to make sure we're", "tokens": [407, 321, 434, 516, 281, 722, 365, 257, 1702, 20928, 295, 2856, 5245, 445, 281, 652, 988, 321, 434], "temperature": 0.0, "avg_logprob": -0.15645675361156464, "compression_ratio": 1.9475409836065574, "no_speech_prob": 9.079705705516972e-06}, {"id": 29, "seek": 8404, "start": 95.28, "end": 97.04, "text": " all on the same page.", "tokens": [439, 322, 264, 912, 3028, 13], "temperature": 0.0, "avg_logprob": -0.15645675361156464, "compression_ratio": 1.9475409836065574, "no_speech_prob": 9.079705705516972e-06}, {"id": 30, "seek": 8404, "start": 97.04, "end": 100.24000000000001, "text": " Then we're going to talk about what types of knowledge language models can already encode", "tokens": [1396, 321, 434, 516, 281, 751, 466, 437, 3467, 295, 3601, 2856, 5245, 393, 1217, 2058, 1429], "temperature": 0.0, "avg_logprob": -0.15645675361156464, "compression_ratio": 1.9475409836065574, "no_speech_prob": 9.079705705516972e-06}, {"id": 31, "seek": 8404, "start": 100.24000000000001, "end": 101.72, "text": " and what they might struggle on.", "tokens": [293, 437, 436, 1062, 7799, 322, 13], "temperature": 0.0, "avg_logprob": -0.15645675361156464, "compression_ratio": 1.9475409836065574, "no_speech_prob": 9.079705705516972e-06}, {"id": 32, "seek": 8404, "start": 101.72, "end": 106.36000000000001, "text": " We'll also motivate why researchers are interested in increasing the amount of knowledge in language", "tokens": [492, 603, 611, 28497, 983, 10309, 366, 3102, 294, 5662, 264, 2372, 295, 3601, 294, 2856], "temperature": 0.0, "avg_logprob": -0.15645675361156464, "compression_ratio": 1.9475409836065574, "no_speech_prob": 9.079705705516972e-06}, {"id": 33, "seek": 8404, "start": 106.36000000000001, "end": 110.76, "text": " models and what this could enable for future AI systems if we have language models that can", "tokens": [5245, 293, 437, 341, 727, 9528, 337, 2027, 7318, 3652, 498, 321, 362, 2856, 5245, 300, 393], "temperature": 0.0, "avg_logprob": -0.15645675361156464, "compression_ratio": 1.9475409836065574, "no_speech_prob": 9.079705705516972e-06}, {"id": 34, "seek": 11076, "start": 110.76, "end": 114.76, "text": " actually reliably recall knowledge.", "tokens": [767, 49927, 9901, 3601, 13], "temperature": 0.0, "avg_logprob": -0.16514818785620516, "compression_ratio": 1.7973856209150327, "no_speech_prob": 2.0143821529927664e-05}, {"id": 35, "seek": 11076, "start": 114.76, "end": 118.36, "text": " We'll talk about three broad classes of techniques that researchers have been using to add knowledge", "tokens": [492, 603, 751, 466, 1045, 4152, 5359, 295, 7512, 300, 10309, 362, 668, 1228, 281, 909, 3601], "temperature": 0.0, "avg_logprob": -0.16514818785620516, "compression_ratio": 1.7973856209150327, "no_speech_prob": 2.0143821529927664e-05}, {"id": 36, "seek": 11076, "start": 118.36, "end": 119.76, "text": " to language models.", "tokens": [281, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16514818785620516, "compression_ratio": 1.7973856209150327, "no_speech_prob": 2.0143821529927664e-05}, {"id": 37, "seek": 11076, "start": 119.76, "end": 124.84, "text": " These include adding pre trained entity embeddings using external memory or key value store or", "tokens": [1981, 4090, 5127, 659, 8895, 13977, 12240, 29432, 1228, 8320, 4675, 420, 2141, 2158, 3531, 420], "temperature": 0.0, "avg_logprob": -0.16514818785620516, "compression_ratio": 1.7973856209150327, "no_speech_prob": 2.0143821529927664e-05}, {"id": 38, "seek": 11076, "start": 124.84, "end": 127.28, "text": " even just modifying the training data.", "tokens": [754, 445, 42626, 264, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16514818785620516, "compression_ratio": 1.7973856209150327, "no_speech_prob": 2.0143821529927664e-05}, {"id": 39, "seek": 11076, "start": 127.28, "end": 130.72, "text": " And for each of these techniques, we'll talk about at least one recent work that uses", "tokens": [400, 337, 1184, 295, 613, 7512, 11, 321, 603, 751, 466, 412, 1935, 472, 5162, 589, 300, 4960], "temperature": 0.0, "avg_logprob": -0.16514818785620516, "compression_ratio": 1.7973856209150327, "no_speech_prob": 2.0143821529927664e-05}, {"id": 40, "seek": 11076, "start": 130.72, "end": 131.72, "text": " the technique.", "tokens": [264, 6532, 13], "temperature": 0.0, "avg_logprob": -0.16514818785620516, "compression_ratio": 1.7973856209150327, "no_speech_prob": 2.0143821529927664e-05}, {"id": 41, "seek": 11076, "start": 131.72, "end": 134.96, "text": " So hopefully it's clear to see how to actually employ and practice.", "tokens": [407, 4696, 309, 311, 1850, 281, 536, 577, 281, 767, 3188, 293, 3124, 13], "temperature": 0.0, "avg_logprob": -0.16514818785620516, "compression_ratio": 1.7973856209150327, "no_speech_prob": 2.0143821529927664e-05}, {"id": 42, "seek": 11076, "start": 134.96, "end": 139.24, "text": " And then finally, we'll wrap up by talking about how to evaluate the knowledge in language", "tokens": [400, 550, 2721, 11, 321, 603, 7019, 493, 538, 1417, 466, 577, 281, 13059, 264, 3601, 294, 2856], "temperature": 0.0, "avg_logprob": -0.16514818785620516, "compression_ratio": 1.7973856209150327, "no_speech_prob": 2.0143821529927664e-05}, {"id": 43, "seek": 13924, "start": 139.24, "end": 144.96, "text": " models and the challenges that come up in trying to do this.", "tokens": [5245, 293, 264, 4759, 300, 808, 493, 294, 1382, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 44, "seek": 13924, "start": 144.96, "end": 145.96, "text": " So let's dive right in.", "tokens": [407, 718, 311, 9192, 558, 294, 13], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 45, "seek": 13924, "start": 145.96, "end": 148.72, "text": " We're going to start by talking about standard language models.", "tokens": [492, 434, 516, 281, 722, 538, 1417, 466, 3832, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 46, "seek": 13924, "start": 148.72, "end": 151.04000000000002, "text": " You learned about these at the beginning of the course.", "tokens": [509, 3264, 466, 613, 412, 264, 2863, 295, 264, 1164, 13], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 47, "seek": 13924, "start": 151.04000000000002, "end": 155.0, "text": " And the task is to predict the next word and sequence of text and to compute the probability", "tokens": [400, 264, 5633, 307, 281, 6069, 264, 958, 1349, 293, 8310, 295, 2487, 293, 281, 14722, 264, 8482], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 48, "seek": 13924, "start": 155.0, "end": 156.32000000000002, "text": " of a sequence.", "tokens": [295, 257, 8310, 13], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 49, "seek": 13924, "start": 156.32000000000002, "end": 159.20000000000002, "text": " So you may remember the example that students opened their blank.", "tokens": [407, 291, 815, 1604, 264, 1365, 300, 1731, 5625, 641, 8247, 13], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 50, "seek": 13924, "start": 159.20000000000002, "end": 163.88, "text": " And we talked about, it could be mine's exams, bring those books here.", "tokens": [400, 321, 2825, 466, 11, 309, 727, 312, 3892, 311, 20514, 11, 1565, 729, 3642, 510, 13], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 51, "seek": 13924, "start": 163.88, "end": 167.0, "text": " And the task of standard language model is to predict the most likely next word in the", "tokens": [400, 264, 5633, 295, 3832, 2856, 2316, 307, 281, 6069, 264, 881, 3700, 958, 1349, 294, 264], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 52, "seek": 13924, "start": 167.0, "end": 168.0, "text": " sequence.", "tokens": [8310, 13], "temperature": 0.0, "avg_logprob": -0.2141806991012008, "compression_ratio": 1.8571428571428572, "no_speech_prob": 6.301257235463709e-05}, {"id": 53, "seek": 16800, "start": 168.0, "end": 173.16, "text": " A couple of lectures ago, John also introduced the notion of mass language models.", "tokens": [316, 1916, 295, 16564, 2057, 11, 2619, 611, 7268, 264, 10710, 295, 2758, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.21366470817505845, "compression_ratio": 1.8064516129032258, "no_speech_prob": 4.9850521463667974e-05}, {"id": 54, "seek": 16800, "start": 173.16, "end": 176.4, "text": " And instead of predicting the next word and sequence of text, the task is to predict", "tokens": [400, 2602, 295, 32884, 264, 958, 1349, 293, 8310, 295, 2487, 11, 264, 5633, 307, 281, 6069], "temperature": 0.0, "avg_logprob": -0.21366470817505845, "compression_ratio": 1.8064516129032258, "no_speech_prob": 4.9850521463667974e-05}, {"id": 55, "seek": 16800, "start": 176.4, "end": 178.08, "text": " the mass token.", "tokens": [264, 2758, 14862, 13], "temperature": 0.0, "avg_logprob": -0.21366470817505845, "compression_ratio": 1.8064516129032258, "no_speech_prob": 4.9850521463667974e-05}, {"id": 56, "seek": 16800, "start": 178.08, "end": 181.12, "text": " And this is done using bi-jectional context.", "tokens": [400, 341, 307, 1096, 1228, 3228, 12, 1020, 1966, 4319, 13], "temperature": 0.0, "avg_logprob": -0.21366470817505845, "compression_ratio": 1.8064516129032258, "no_speech_prob": 4.9850521463667974e-05}, {"id": 57, "seek": 16800, "start": 181.12, "end": 184.84, "text": " So you may remember the example I'm mass to the mask and the goal of the mass language", "tokens": [407, 291, 815, 1604, 264, 1365, 286, 478, 2758, 281, 264, 6094, 293, 264, 3387, 295, 264, 2758, 2856], "temperature": 0.0, "avg_logprob": -0.21366470817505845, "compression_ratio": 1.8064516129032258, "no_speech_prob": 4.9850521463667974e-05}, {"id": 58, "seek": 16800, "start": 184.84, "end": 189.64, "text": " model is to make the most likely token for each of the massed out words.", "tokens": [2316, 307, 281, 652, 264, 881, 3700, 14862, 337, 1184, 295, 264, 2758, 292, 484, 2283, 13], "temperature": 0.0, "avg_logprob": -0.21366470817505845, "compression_ratio": 1.8064516129032258, "no_speech_prob": 4.9850521463667974e-05}, {"id": 59, "seek": 16800, "start": 189.64, "end": 191.96, "text": " So maybe I went to the store.", "tokens": [407, 1310, 286, 1437, 281, 264, 3531, 13], "temperature": 0.0, "avg_logprob": -0.21366470817505845, "compression_ratio": 1.8064516129032258, "no_speech_prob": 4.9850521463667974e-05}, {"id": 60, "seek": 16800, "start": 191.96, "end": 195.04, "text": " So while there's some differences in these two types of language models, but they are", "tokens": [407, 1339, 456, 311, 512, 7300, 294, 613, 732, 3467, 295, 2856, 5245, 11, 457, 436, 366], "temperature": 0.0, "avg_logprob": -0.21366470817505845, "compression_ratio": 1.8064516129032258, "no_speech_prob": 4.9850521463667974e-05}, {"id": 61, "seek": 19504, "start": 195.04, "end": 199.16, "text": " predicting the next word, or whether you're predicting the massed out token, they're similar", "tokens": [32884, 264, 958, 1349, 11, 420, 1968, 291, 434, 32884, 264, 2758, 292, 484, 14862, 11, 436, 434, 2531], "temperature": 0.0, "avg_logprob": -0.14772269685389633, "compression_ratio": 1.7003484320557491, "no_speech_prob": 2.3185171812656336e-05}, {"id": 62, "seek": 19504, "start": 199.16, "end": 203.56, "text": " and that they can both be trained over large amounts of unlabeled text.", "tokens": [293, 300, 436, 393, 1293, 312, 8895, 670, 2416, 11663, 295, 32118, 18657, 292, 2487, 13], "temperature": 0.0, "avg_logprob": -0.14772269685389633, "compression_ratio": 1.7003484320557491, "no_speech_prob": 2.3185171812656336e-05}, {"id": 63, "seek": 19504, "start": 203.56, "end": 206.2, "text": " And this is one of the reasons why they've been so wide they adopted.", "tokens": [400, 341, 307, 472, 295, 264, 4112, 983, 436, 600, 668, 370, 4874, 436, 12175, 13], "temperature": 0.0, "avg_logprob": -0.14772269685389633, "compression_ratio": 1.7003484320557491, "no_speech_prob": 2.3185171812656336e-05}, {"id": 64, "seek": 19504, "start": 206.2, "end": 210.56, "text": " They don't require any human annotated data.", "tokens": [814, 500, 380, 3651, 604, 1952, 25339, 770, 1412, 13], "temperature": 0.0, "avg_logprob": -0.14772269685389633, "compression_ratio": 1.7003484320557491, "no_speech_prob": 2.3185171812656336e-05}, {"id": 65, "seek": 19504, "start": 210.56, "end": 214.92, "text": " So you've seen that language models can be used for a variety of tasks, from summarization,", "tokens": [407, 291, 600, 1612, 300, 2856, 5245, 393, 312, 1143, 337, 257, 5673, 295, 9608, 11, 490, 14611, 2144, 11], "temperature": 0.0, "avg_logprob": -0.14772269685389633, "compression_ratio": 1.7003484320557491, "no_speech_prob": 2.3185171812656336e-05}, {"id": 66, "seek": 19504, "start": 214.92, "end": 220.2, "text": " to dialogue, to fluency evaluation, tasks that involve either generating text or evaluating", "tokens": [281, 10221, 11, 281, 5029, 3020, 13344, 11, 9608, 300, 9494, 2139, 17746, 2487, 420, 27479], "temperature": 0.0, "avg_logprob": -0.14772269685389633, "compression_ratio": 1.7003484320557491, "no_speech_prob": 2.3185171812656336e-05}, {"id": 67, "seek": 19504, "start": 220.2, "end": 223.28, "text": " the probability of text.", "tokens": [264, 8482, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.14772269685389633, "compression_ratio": 1.7003484320557491, "no_speech_prob": 2.3185171812656336e-05}, {"id": 68, "seek": 22328, "start": 223.28, "end": 226.8, "text": " And more recently we've seen that language models can also be used to generate pre-chained", "tokens": [400, 544, 3938, 321, 600, 1612, 300, 2856, 5245, 393, 611, 312, 1143, 281, 8460, 659, 12, 339, 3563], "temperature": 0.0, "avg_logprob": -0.14978228296552384, "compression_ratio": 1.7491166077738516, "no_speech_prob": 1.0129293514182791e-05}, {"id": 69, "seek": 22328, "start": 226.8, "end": 231.16, "text": " representations of text that encodes some notion of language understanding, and has", "tokens": [33358, 295, 2487, 300, 2058, 4789, 512, 10710, 295, 2856, 3701, 11, 293, 575], "temperature": 0.0, "avg_logprob": -0.14978228296552384, "compression_ratio": 1.7491166077738516, "no_speech_prob": 1.0129293514182791e-05}, {"id": 70, "seek": 22328, "start": 231.16, "end": 236.32, "text": " been shown to be widely useful for different downstream NLP tasks.", "tokens": [668, 4898, 281, 312, 13371, 4420, 337, 819, 30621, 426, 45196, 9608, 13], "temperature": 0.0, "avg_logprob": -0.14978228296552384, "compression_ratio": 1.7491166077738516, "no_speech_prob": 1.0129293514182791e-05}, {"id": 71, "seek": 22328, "start": 236.32, "end": 239.88, "text": " And then finally, today we're going to touch on this idea that if language models are", "tokens": [400, 550, 2721, 11, 965, 321, 434, 516, 281, 2557, 322, 341, 1558, 300, 498, 2856, 5245, 366], "temperature": 0.0, "avg_logprob": -0.14978228296552384, "compression_ratio": 1.7491166077738516, "no_speech_prob": 1.0129293514182791e-05}, {"id": 72, "seek": 22328, "start": 239.88, "end": 247.2, "text": " trained over massive amounts of text, can they even be used as a knowledge base?", "tokens": [8895, 670, 5994, 11663, 295, 2487, 11, 393, 436, 754, 312, 1143, 382, 257, 3601, 3096, 30], "temperature": 0.0, "avg_logprob": -0.14978228296552384, "compression_ratio": 1.7491166077738516, "no_speech_prob": 1.0129293514182791e-05}, {"id": 73, "seek": 22328, "start": 247.2, "end": 250.36, "text": " So we're going to start by looking at what types of factual knowledge a language model", "tokens": [407, 321, 434, 516, 281, 722, 538, 1237, 412, 437, 3467, 295, 48029, 3601, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.14978228296552384, "compression_ratio": 1.7491166077738516, "no_speech_prob": 1.0129293514182791e-05}, {"id": 74, "seek": 25036, "start": 250.36, "end": 255.28, "text": " might already know. And these examples are taken from a paper by Petroni et al, in", "tokens": [1062, 1217, 458, 13, 400, 613, 5110, 366, 2726, 490, 257, 3035, 538, 10472, 2044, 72, 1030, 419, 11, 294], "temperature": 0.0, "avg_logprob": -0.30409776118763704, "compression_ratio": 1.5228070175438597, "no_speech_prob": 2.709744694584515e-05}, {"id": 75, "seek": 25036, "start": 255.28, "end": 257.16, "text": " EML, P a couple years ago.", "tokens": [16237, 43, 11, 430, 257, 1916, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.30409776118763704, "compression_ratio": 1.5228070175438597, "no_speech_prob": 2.709744694584515e-05}, {"id": 76, "seek": 25036, "start": 257.16, "end": 262.04, "text": " And the goal is to test the factual or common sense knowledge in existing language models", "tokens": [400, 264, 3387, 307, 281, 1500, 264, 48029, 420, 2689, 2020, 3601, 294, 6741, 2856, 5245], "temperature": 0.0, "avg_logprob": -0.30409776118763704, "compression_ratio": 1.5228070175438597, "no_speech_prob": 2.709744694584515e-05}, {"id": 77, "seek": 25036, "start": 262.04, "end": 264.2, "text": " such as Bert large.", "tokens": [1270, 382, 29594, 2416, 13], "temperature": 0.0, "avg_logprob": -0.30409776118763704, "compression_ratio": 1.5228070175438597, "no_speech_prob": 2.709744694584515e-05}, {"id": 78, "seek": 25036, "start": 264.2, "end": 266.56, "text": " So let's check out what Bert large predicts.", "tokens": [407, 718, 311, 1520, 484, 437, 29594, 2416, 6069, 82, 13], "temperature": 0.0, "avg_logprob": -0.30409776118763704, "compression_ratio": 1.5228070175438597, "no_speech_prob": 2.709744694584515e-05}, {"id": 79, "seek": 25036, "start": 266.56, "end": 273.2, "text": " iPod Touch is produced by Apple, London Jazz Festival is located in London, Danny Alve", "tokens": [5180, 378, 20029, 307, 7126, 538, 6373, 11, 7042, 32213, 16512, 307, 6870, 294, 7042, 11, 16682, 967, 303], "temperature": 0.0, "avg_logprob": -0.30409776118763704, "compression_ratio": 1.5228070175438597, "no_speech_prob": 2.709744694584515e-05}, {"id": 80, "seek": 27320, "start": 273.2, "end": 280.64, "text": " is placed with Santos, Carl III used to communicate in German, and Ravens can fly.", "tokens": [307, 7074, 365, 36962, 11, 14256, 16317, 1143, 281, 7890, 294, 6521, 11, 293, 28956, 82, 393, 3603, 13], "temperature": 0.0, "avg_logprob": -0.18849134852743557, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00011056373477913439}, {"id": 81, "seek": 27320, "start": 280.64, "end": 284.08, "text": " So here we have the correct predictions in green and the incorrect predictions in red.", "tokens": [407, 510, 321, 362, 264, 3006, 21264, 294, 3092, 293, 264, 18424, 21264, 294, 2182, 13], "temperature": 0.0, "avg_logprob": -0.18849134852743557, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00011056373477913439}, {"id": 82, "seek": 27320, "start": 284.08, "end": 288.84, "text": " And if you know anything about sports, you may know that Danny Alve is a soccer player,", "tokens": [400, 498, 291, 458, 1340, 466, 6573, 11, 291, 815, 458, 300, 16682, 967, 303, 307, 257, 15469, 4256, 11], "temperature": 0.0, "avg_logprob": -0.18849134852743557, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00011056373477913439}, {"id": 83, "seek": 27320, "start": 288.84, "end": 290.59999999999997, "text": " Santos is a soccer team.", "tokens": [36962, 307, 257, 15469, 1469, 13], "temperature": 0.0, "avg_logprob": -0.18849134852743557, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00011056373477913439}, {"id": 84, "seek": 27320, "start": 290.59999999999997, "end": 293.84, "text": " Here they were hoping that it would predict Barcelona, because at least at the time of", "tokens": [1692, 436, 645, 7159, 300, 309, 576, 6069, 21247, 11, 570, 412, 1935, 412, 264, 565, 295], "temperature": 0.0, "avg_logprob": -0.18849134852743557, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00011056373477913439}, {"id": 85, "seek": 27320, "start": 293.84, "end": 298.0, "text": " this data set, apparently he played for Barcelona, and Carl III actually used to communicate", "tokens": [341, 1412, 992, 11, 7970, 415, 3737, 337, 21247, 11, 293, 14256, 16317, 767, 1143, 281, 7890], "temperature": 0.0, "avg_logprob": -0.18849134852743557, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00011056373477913439}, {"id": 86, "seek": 27320, "start": 298.0, "end": 301.03999999999996, "text": " in Swedish, not German.", "tokens": [294, 23523, 11, 406, 6521, 13], "temperature": 0.0, "avg_logprob": -0.18849134852743557, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00011056373477913439}, {"id": 87, "seek": 30104, "start": 301.04, "end": 305.16, "text": " So it's good about these examples, it's a predictions are generally reasonable.", "tokens": [407, 309, 311, 665, 466, 613, 5110, 11, 309, 311, 257, 21264, 366, 5101, 10585, 13], "temperature": 0.0, "avg_logprob": -0.2041799093930776, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.00013320449215825647}, {"id": 88, "seek": 30104, "start": 305.16, "end": 308.24, "text": " If you didn't know the ground truth, they all make sense.", "tokens": [759, 291, 994, 380, 458, 264, 2727, 3494, 11, 436, 439, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.2041799093930776, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.00013320449215825647}, {"id": 89, "seek": 30104, "start": 308.24, "end": 313.84000000000003, "text": " When you want to predict a language, you do in fact predict a language.", "tokens": [1133, 291, 528, 281, 6069, 257, 2856, 11, 291, 360, 294, 1186, 6069, 257, 2856, 13], "temperature": 0.0, "avg_logprob": -0.2041799093930776, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.00013320449215825647}, {"id": 90, "seek": 30104, "start": 313.84000000000003, "end": 317.48, "text": " But of course they're not all factually correct.", "tokens": [583, 295, 1164, 436, 434, 406, 439, 1186, 671, 3006, 13], "temperature": 0.0, "avg_logprob": -0.2041799093930776, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.00013320449215825647}, {"id": 91, "seek": 30104, "start": 317.48, "end": 319.24, "text": " So why might this happen?", "tokens": [407, 983, 1062, 341, 1051, 30], "temperature": 0.0, "avg_logprob": -0.2041799093930776, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.00013320449215825647}, {"id": 92, "seek": 30104, "start": 319.24, "end": 322.08000000000004, "text": " Well, for one, the fact might not have been seen in training.", "tokens": [1042, 11, 337, 472, 11, 264, 1186, 1062, 406, 362, 668, 1612, 294, 3097, 13], "temperature": 0.0, "avg_logprob": -0.2041799093930776, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.00013320449215825647}, {"id": 93, "seek": 30104, "start": 322.08000000000004, "end": 325.6, "text": " And you can't expect the language model to do more than recall facts that it has seen", "tokens": [400, 291, 393, 380, 2066, 264, 2856, 2316, 281, 360, 544, 813, 9901, 9130, 300, 309, 575, 1612], "temperature": 0.0, "avg_logprob": -0.2041799093930776, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.00013320449215825647}, {"id": 94, "seek": 30104, "start": 325.6, "end": 326.6, "text": " in training.", "tokens": [294, 3097, 13], "temperature": 0.0, "avg_logprob": -0.2041799093930776, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.00013320449215825647}, {"id": 95, "seek": 30104, "start": 326.6, "end": 329.6, "text": " It can't make up facts about the world for instance.", "tokens": [467, 393, 380, 652, 493, 9130, 466, 264, 1002, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.2041799093930776, "compression_ratio": 1.7473684210526317, "no_speech_prob": 0.00013320449215825647}, {"id": 96, "seek": 32960, "start": 329.6, "end": 331.92, "text": " It's also possible the fact is just really rare.", "tokens": [467, 311, 611, 1944, 264, 1186, 307, 445, 534, 5892, 13], "temperature": 0.0, "avg_logprob": -0.15027678958953372, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.00019402448378968984}, {"id": 97, "seek": 32960, "start": 331.92, "end": 335.32000000000005, "text": " So maybe the language model has seen the fact during training, but it hasn't seen it", "tokens": [407, 1310, 264, 2856, 2316, 575, 1612, 264, 1186, 1830, 3097, 11, 457, 309, 6132, 380, 1612, 309], "temperature": 0.0, "avg_logprob": -0.15027678958953372, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.00019402448378968984}, {"id": 98, "seek": 32960, "start": 335.32000000000005, "end": 338.96000000000004, "text": " enough times actually memorize the fact.", "tokens": [1547, 1413, 767, 27478, 264, 1186, 13], "temperature": 0.0, "avg_logprob": -0.15027678958953372, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.00019402448378968984}, {"id": 99, "seek": 32960, "start": 338.96000000000004, "end": 342.6, "text": " And the last issue is a little more subtle, which a model might just be very sensitive", "tokens": [400, 264, 1036, 2734, 307, 257, 707, 544, 13743, 11, 597, 257, 2316, 1062, 445, 312, 588, 9477], "temperature": 0.0, "avg_logprob": -0.15027678958953372, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.00019402448378968984}, {"id": 100, "seek": 32960, "start": 342.6, "end": 345.96000000000004, "text": " to the phrasing of the fill in the blank statement.", "tokens": [281, 264, 7636, 3349, 295, 264, 2836, 294, 264, 8247, 5629, 13], "temperature": 0.0, "avg_logprob": -0.15027678958953372, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.00019402448378968984}, {"id": 101, "seek": 32960, "start": 345.96000000000004, "end": 350.08000000000004, "text": " And so for example, you might have statements like X was created in blank that the model", "tokens": [400, 370, 337, 1365, 11, 291, 1062, 362, 12363, 411, 1783, 390, 2942, 294, 8247, 300, 264, 2316], "temperature": 0.0, "avg_logprob": -0.15027678958953372, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.00019402448378968984}, {"id": 102, "seek": 32960, "start": 350.08000000000004, "end": 354.64000000000004, "text": " can't predict correctly, but if you change it to X was made in blank, suddenly it can", "tokens": [393, 380, 6069, 8944, 11, 457, 498, 291, 1319, 309, 281, 1783, 390, 1027, 294, 8247, 11, 5800, 309, 393], "temperature": 0.0, "avg_logprob": -0.15027678958953372, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.00019402448378968984}, {"id": 103, "seek": 32960, "start": 354.64000000000004, "end": 355.64000000000004, "text": " predict it correctly.", "tokens": [6069, 309, 8944, 13], "temperature": 0.0, "avg_logprob": -0.15027678958953372, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.00019402448378968984}, {"id": 104, "seek": 35564, "start": 355.64, "end": 362.59999999999997, "text": " And we'll come back to this in how to actually evaluate the knowledge in these language models.", "tokens": [400, 321, 603, 808, 646, 281, 341, 294, 577, 281, 767, 13059, 264, 3601, 294, 613, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17564798990885416, "compression_ratio": 1.754208754208754, "no_speech_prob": 4.90811507916078e-05}, {"id": 105, "seek": 35564, "start": 362.59999999999997, "end": 367.12, "text": " So this inability to reliably recall knowledge is a key challenge facing language models", "tokens": [407, 341, 33162, 281, 49927, 9901, 3601, 307, 257, 2141, 3430, 7170, 2856, 5245], "temperature": 0.0, "avg_logprob": -0.17564798990885416, "compression_ratio": 1.754208754208754, "no_speech_prob": 4.90811507916078e-05}, {"id": 106, "seek": 35564, "start": 367.12, "end": 368.12, "text": " today.", "tokens": [965, 13], "temperature": 0.0, "avg_logprob": -0.17564798990885416, "compression_ratio": 1.754208754208754, "no_speech_prob": 4.90811507916078e-05}, {"id": 107, "seek": 35564, "start": 368.12, "end": 369.96, "text": " It'll be the focus of this talk.", "tokens": [467, 603, 312, 264, 1879, 295, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.17564798990885416, "compression_ratio": 1.754208754208754, "no_speech_prob": 4.90811507916078e-05}, {"id": 108, "seek": 35564, "start": 369.96, "end": 374.15999999999997, "text": " Recent works have found that language models can recover some knowledge, including the", "tokens": [17553, 1985, 362, 1352, 300, 2856, 5245, 393, 8114, 512, 3601, 11, 3009, 264], "temperature": 0.0, "avg_logprob": -0.17564798990885416, "compression_ratio": 1.754208754208754, "no_speech_prob": 4.90811507916078e-05}, {"id": 109, "seek": 35564, "start": 374.15999999999997, "end": 376.15999999999997, "text": " work that Colin presented last class.", "tokens": [589, 300, 29253, 8212, 1036, 1508, 13], "temperature": 0.0, "avg_logprob": -0.17564798990885416, "compression_ratio": 1.754208754208754, "no_speech_prob": 4.90811507916078e-05}, {"id": 110, "seek": 35564, "start": 376.15999999999997, "end": 378.56, "text": " They've had very encouraging results.", "tokens": [814, 600, 632, 588, 14580, 3542, 13], "temperature": 0.0, "avg_logprob": -0.17564798990885416, "compression_ratio": 1.754208754208754, "no_speech_prob": 4.90811507916078e-05}, {"id": 111, "seek": 35564, "start": 378.56, "end": 381.84, "text": " But they're still a way to go as we saw with the fill in the blank statements and with", "tokens": [583, 436, 434, 920, 257, 636, 281, 352, 382, 321, 1866, 365, 264, 2836, 294, 264, 8247, 12363, 293, 365], "temperature": 0.0, "avg_logprob": -0.17564798990885416, "compression_ratio": 1.754208754208754, "no_speech_prob": 4.90811507916078e-05}, {"id": 112, "seek": 35564, "start": 381.84, "end": 384.88, "text": " these challenges that we just discussed above.", "tokens": [613, 4759, 300, 321, 445, 7152, 3673, 13], "temperature": 0.0, "avg_logprob": -0.17564798990885416, "compression_ratio": 1.754208754208754, "no_speech_prob": 4.90811507916078e-05}, {"id": 113, "seek": 38488, "start": 384.88, "end": 388.88, "text": " So as a result, the past couple years have had a ton of rapid progress in this area of", "tokens": [407, 382, 257, 1874, 11, 264, 1791, 1916, 924, 362, 632, 257, 2952, 295, 7558, 4205, 294, 341, 1859, 295], "temperature": 0.0, "avg_logprob": -0.16601969744708087, "compression_ratio": 1.7830882352941178, "no_speech_prob": 1.0781824130390305e-05}, {"id": 114, "seek": 38488, "start": 388.88, "end": 393.32, "text": " research in terms of trying to figure out how do you actually encode more knowledge in", "tokens": [2132, 294, 2115, 295, 1382, 281, 2573, 484, 577, 360, 291, 767, 2058, 1429, 544, 3601, 294], "temperature": 0.0, "avg_logprob": -0.16601969744708087, "compression_ratio": 1.7830882352941178, "no_speech_prob": 1.0781824130390305e-05}, {"id": 115, "seek": 38488, "start": 393.32, "end": 397.71999999999997, "text": " language models.", "tokens": [2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16601969744708087, "compression_ratio": 1.7830882352941178, "no_speech_prob": 1.0781824130390305e-05}, {"id": 116, "seek": 38488, "start": 397.71999999999997, "end": 401.36, "text": " So I also want to motivate why researchers are interested in building language models", "tokens": [407, 286, 611, 528, 281, 28497, 983, 10309, 366, 3102, 294, 2390, 2856, 5245], "temperature": 0.0, "avg_logprob": -0.16601969744708087, "compression_ratio": 1.7830882352941178, "no_speech_prob": 1.0781824130390305e-05}, {"id": 117, "seek": 38488, "start": 401.36, "end": 405.04, "text": " that can more reliably recall knowledge.", "tokens": [300, 393, 544, 49927, 9901, 3601, 13], "temperature": 0.0, "avg_logprob": -0.16601969744708087, "compression_ratio": 1.7830882352941178, "no_speech_prob": 1.0781824130390305e-05}, {"id": 118, "seek": 38488, "start": 405.04, "end": 409.24, "text": " And one of these reasons is that the pre-changed representations are used in a variety of downstream", "tokens": [400, 472, 295, 613, 4112, 307, 300, 264, 659, 12, 339, 10296, 33358, 366, 1143, 294, 257, 5673, 295, 30621], "temperature": 0.0, "avg_logprob": -0.16601969744708087, "compression_ratio": 1.7830882352941178, "no_speech_prob": 1.0781824130390305e-05}, {"id": 119, "seek": 38488, "start": 409.24, "end": 410.24, "text": " tasks.", "tokens": [9608, 13], "temperature": 0.0, "avg_logprob": -0.16601969744708087, "compression_ratio": 1.7830882352941178, "no_speech_prob": 1.0781824130390305e-05}, {"id": 120, "seek": 38488, "start": 410.24, "end": 413.88, "text": " And some of these downstream tests are knowledge intensive.", "tokens": [400, 512, 295, 613, 30621, 6921, 366, 3601, 18957, 13], "temperature": 0.0, "avg_logprob": -0.16601969744708087, "compression_ratio": 1.7830882352941178, "no_speech_prob": 1.0781824130390305e-05}, {"id": 121, "seek": 41388, "start": 413.88, "end": 418.0, "text": " So for instance, you might have a downstream task to extract the relations between two", "tokens": [407, 337, 5197, 11, 291, 1062, 362, 257, 30621, 5633, 281, 8947, 264, 2299, 1296, 732], "temperature": 0.0, "avg_logprob": -0.16121839362884235, "compression_ratio": 1.7547169811320755, "no_speech_prob": 2.0458735889405943e-05}, {"id": 122, "seek": 41388, "start": 418.0, "end": 420.0, "text": " entities in a sentence.", "tokens": [16667, 294, 257, 8174, 13], "temperature": 0.0, "avg_logprob": -0.16121839362884235, "compression_ratio": 1.7547169811320755, "no_speech_prob": 2.0458735889405943e-05}, {"id": 123, "seek": 41388, "start": 420.0, "end": 422.44, "text": " And this is commonly known as relation extraction.", "tokens": [400, 341, 307, 12719, 2570, 382, 9721, 30197, 13], "temperature": 0.0, "avg_logprob": -0.16121839362884235, "compression_ratio": 1.7547169811320755, "no_speech_prob": 2.0458735889405943e-05}, {"id": 124, "seek": 41388, "start": 422.44, "end": 426.92, "text": " And this is much easier if you have some knowledge of the entities, which could be potentially", "tokens": [400, 341, 307, 709, 3571, 498, 291, 362, 512, 3601, 295, 264, 16667, 11, 597, 727, 312, 7263], "temperature": 0.0, "avg_logprob": -0.16121839362884235, "compression_ratio": 1.7547169811320755, "no_speech_prob": 2.0458735889405943e-05}, {"id": 125, "seek": 41388, "start": 426.92, "end": 431.88, "text": " provided by this pre-trained language model representation.", "tokens": [5649, 538, 341, 659, 12, 17227, 2001, 2856, 2316, 10290, 13], "temperature": 0.0, "avg_logprob": -0.16121839362884235, "compression_ratio": 1.7547169811320755, "no_speech_prob": 2.0458735889405943e-05}, {"id": 126, "seek": 41388, "start": 431.88, "end": 432.88, "text": " And we talk about evaluation.", "tokens": [400, 321, 751, 466, 13344, 13], "temperature": 0.0, "avg_logprob": -0.16121839362884235, "compression_ratio": 1.7547169811320755, "no_speech_prob": 2.0458735889405943e-05}, {"id": 127, "seek": 41388, "start": 432.88, "end": 437.15999999999997, "text": " We'll talk about what types of tasks are most likely to benefit from these knowledge-rich", "tokens": [492, 603, 751, 466, 437, 3467, 295, 9608, 366, 881, 3700, 281, 5121, 490, 613, 3601, 12, 10794], "temperature": 0.0, "avg_logprob": -0.16121839362884235, "compression_ratio": 1.7547169811320755, "no_speech_prob": 2.0458735889405943e-05}, {"id": 128, "seek": 41388, "start": 437.15999999999997, "end": 440.84, "text": " pre-changed representations.", "tokens": [659, 12, 339, 10296, 33358, 13], "temperature": 0.0, "avg_logprob": -0.16121839362884235, "compression_ratio": 1.7547169811320755, "no_speech_prob": 2.0458735889405943e-05}, {"id": 129, "seek": 44084, "start": 440.84, "end": 445.0, "text": " And then as a stretch goal, some researchers are starting to propose the idea that can", "tokens": [400, 550, 382, 257, 5985, 3387, 11, 512, 10309, 366, 2891, 281, 17421, 264, 1558, 300, 393], "temperature": 0.0, "avg_logprob": -0.17328723409901495, "compression_ratio": 1.7210884353741496, "no_speech_prob": 2.282456262037158e-05}, {"id": 130, "seek": 44084, "start": 445.0, "end": 450.56, "text": " language models actually ultimately be used to replace traditional knowledge bases.", "tokens": [2856, 5245, 767, 6284, 312, 1143, 281, 7406, 5164, 3601, 17949, 13], "temperature": 0.0, "avg_logprob": -0.17328723409901495, "compression_ratio": 1.7210884353741496, "no_speech_prob": 2.282456262037158e-05}, {"id": 131, "seek": 44084, "start": 450.56, "end": 454.15999999999997, "text": " So instead of creating a knowledge base for a fact, like you might right now with SQL,", "tokens": [407, 2602, 295, 4084, 257, 3601, 3096, 337, 257, 1186, 11, 411, 291, 1062, 558, 586, 365, 19200, 11], "temperature": 0.0, "avg_logprob": -0.17328723409901495, "compression_ratio": 1.7210884353741496, "no_speech_prob": 2.282456262037158e-05}, {"id": 132, "seek": 44084, "start": 454.15999999999997, "end": 457.32, "text": " you'd create a language model with a natural language prompt.", "tokens": [291, 1116, 1884, 257, 2856, 2316, 365, 257, 3303, 2856, 12391, 13], "temperature": 0.0, "avg_logprob": -0.17328723409901495, "compression_ratio": 1.7210884353741496, "no_speech_prob": 2.282456262037158e-05}, {"id": 133, "seek": 44084, "start": 457.32, "end": 461.88, "text": " And of course, this does require the language model to have high quality under calling", "tokens": [400, 295, 1164, 11, 341, 775, 3651, 264, 2856, 2316, 281, 362, 1090, 3125, 833, 5141], "temperature": 0.0, "avg_logprob": -0.17328723409901495, "compression_ratio": 1.7210884353741496, "no_speech_prob": 2.282456262037158e-05}, {"id": 134, "seek": 44084, "start": 461.88, "end": 462.88, "text": " facts.", "tokens": [9130, 13], "temperature": 0.0, "avg_logprob": -0.17328723409901495, "compression_ratio": 1.7210884353741496, "no_speech_prob": 2.282456262037158e-05}, {"id": 135, "seek": 44084, "start": 462.88, "end": 469.32, "text": " So we might not be there yet, but it's an interesting direction for us to be moving towards.", "tokens": [407, 321, 1062, 406, 312, 456, 1939, 11, 457, 309, 311, 364, 1880, 3513, 337, 505, 281, 312, 2684, 3030, 13], "temperature": 0.0, "avg_logprob": -0.17328723409901495, "compression_ratio": 1.7210884353741496, "no_speech_prob": 2.282456262037158e-05}, {"id": 136, "seek": 46932, "start": 469.32, "end": 472.32, "text": " So I want to make it super clear what I mean by a knowledge base.", "tokens": [407, 286, 528, 281, 652, 309, 1687, 1850, 437, 286, 914, 538, 257, 3601, 3096, 13], "temperature": 0.0, "avg_logprob": -0.19043533837617332, "compression_ratio": 1.7835051546391754, "no_speech_prob": 3.479649603832513e-05}, {"id": 137, "seek": 46932, "start": 472.32, "end": 475.96, "text": " Here we're just talking about a knowledge graph where the nodes in the graph would be", "tokens": [1692, 321, 434, 445, 1417, 466, 257, 3601, 4295, 689, 264, 13891, 294, 264, 4295, 576, 312], "temperature": 0.0, "avg_logprob": -0.19043533837617332, "compression_ratio": 1.7835051546391754, "no_speech_prob": 3.479649603832513e-05}, {"id": 138, "seek": 46932, "start": 475.96, "end": 477.56, "text": " entities.", "tokens": [16667, 13], "temperature": 0.0, "avg_logprob": -0.19043533837617332, "compression_ratio": 1.7835051546391754, "no_speech_prob": 3.479649603832513e-05}, {"id": 139, "seek": 46932, "start": 477.56, "end": 480.08, "text": " And the edges are going to be relations between the entities.", "tokens": [400, 264, 8819, 366, 516, 281, 312, 2299, 1296, 264, 16667, 13], "temperature": 0.0, "avg_logprob": -0.19043533837617332, "compression_ratio": 1.7835051546391754, "no_speech_prob": 3.479649603832513e-05}, {"id": 140, "seek": 46932, "start": 480.08, "end": 484.88, "text": " So for example, here we have a subset of a knowledge graph for Franklin D. Roosevelt.", "tokens": [407, 337, 1365, 11, 510, 321, 362, 257, 25993, 295, 257, 3601, 4295, 337, 22010, 413, 13, 28515, 13], "temperature": 0.0, "avg_logprob": -0.19043533837617332, "compression_ratio": 1.7835051546391754, "no_speech_prob": 3.479649603832513e-05}, {"id": 141, "seek": 46932, "start": 484.88, "end": 488.92, "text": " And you see the information about his spouse, his place of birth, his date of birth, and", "tokens": [400, 291, 536, 264, 1589, 466, 702, 23013, 11, 702, 1081, 295, 3965, 11, 702, 4002, 295, 3965, 11, 293], "temperature": 0.0, "avg_logprob": -0.19043533837617332, "compression_ratio": 1.7835051546391754, "no_speech_prob": 3.479649603832513e-05}, {"id": 142, "seek": 46932, "start": 488.92, "end": 489.92, "text": " so on.", "tokens": [370, 322, 13], "temperature": 0.0, "avg_logprob": -0.19043533837617332, "compression_ratio": 1.7835051546391754, "no_speech_prob": 3.479649603832513e-05}, {"id": 143, "seek": 46932, "start": 489.92, "end": 494.4, "text": " An important thing to note is this is a structured way of storing the knowledge, since it's", "tokens": [1107, 1021, 551, 281, 3637, 307, 341, 307, 257, 18519, 636, 295, 26085, 264, 3601, 11, 1670, 309, 311], "temperature": 0.0, "avg_logprob": -0.19043533837617332, "compression_ratio": 1.7835051546391754, "no_speech_prob": 3.479649603832513e-05}, {"id": 144, "seek": 46932, "start": 494.4, "end": 495.6, "text": " just in a graph form.", "tokens": [445, 294, 257, 4295, 1254, 13], "temperature": 0.0, "avg_logprob": -0.19043533837617332, "compression_ratio": 1.7835051546391754, "no_speech_prob": 3.479649603832513e-05}, {"id": 145, "seek": 49560, "start": 495.6, "end": 499.88, "text": " And you can actually describe these graphs with knowledge graph triples, which will be", "tokens": [400, 291, 393, 767, 6786, 613, 24877, 365, 3601, 4295, 1376, 2622, 11, 597, 486, 312], "temperature": 0.0, "avg_logprob": -0.19756978018242016, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.3551134290755726e-05}, {"id": 146, "seek": 49560, "start": 499.88, "end": 502.84000000000003, "text": " an important vocabulary word throughout this talk.", "tokens": [364, 1021, 19864, 1349, 3710, 341, 751, 13], "temperature": 0.0, "avg_logprob": -0.19756978018242016, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.3551134290755726e-05}, {"id": 147, "seek": 49560, "start": 502.84000000000003, "end": 509.0, "text": " So knowledge graph triple would be consisting of a subject entity, a relation, and then object", "tokens": [407, 3601, 4295, 15508, 576, 312, 33921, 295, 257, 3983, 13977, 11, 257, 9721, 11, 293, 550, 2657], "temperature": 0.0, "avg_logprob": -0.19756978018242016, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.3551134290755726e-05}, {"id": 148, "seek": 49560, "start": 509.0, "end": 510.16, "text": " entity.", "tokens": [13977, 13], "temperature": 0.0, "avg_logprob": -0.19756978018242016, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.3551134290755726e-05}, {"id": 149, "seek": 49560, "start": 510.16, "end": 514.6, "text": " So for instance, here we might have Franklin D. Roosevelt, date of birth, January 30th,", "tokens": [407, 337, 5197, 11, 510, 321, 1062, 362, 22010, 413, 13, 28515, 11, 4002, 295, 3965, 11, 7061, 2217, 392, 11], "temperature": 0.0, "avg_logprob": -0.19756978018242016, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.3551134290755726e-05}, {"id": 150, "seek": 49560, "start": 514.6, "end": 515.9200000000001, "text": " 1882.", "tokens": [2443, 32848, 13], "temperature": 0.0, "avg_logprob": -0.19756978018242016, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.3551134290755726e-05}, {"id": 151, "seek": 49560, "start": 515.9200000000001, "end": 517.32, "text": " And that would form a knowledge graph triple.", "tokens": [400, 300, 576, 1254, 257, 3601, 4295, 15508, 13], "temperature": 0.0, "avg_logprob": -0.19756978018242016, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.3551134290755726e-05}, {"id": 152, "seek": 49560, "start": 517.32, "end": 523.84, "text": " We'll also refer to this as a parent entity, a relation, and a tail entity.", "tokens": [492, 603, 611, 2864, 281, 341, 382, 257, 2596, 13977, 11, 257, 9721, 11, 293, 257, 6838, 13977, 13], "temperature": 0.0, "avg_logprob": -0.19756978018242016, "compression_ratio": 1.7014925373134329, "no_speech_prob": 2.3551134290755726e-05}, {"id": 153, "seek": 52384, "start": 523.84, "end": 526.84, "text": " So wiki data is one very popular knowledge base you might come across if you're working", "tokens": [407, 261, 9850, 1412, 307, 472, 588, 3743, 3601, 3096, 291, 1062, 808, 2108, 498, 291, 434, 1364], "temperature": 0.0, "avg_logprob": -0.1853440399169922, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.1478273336251732e-05}, {"id": 154, "seek": 52384, "start": 526.84, "end": 528.08, "text": " this area.", "tokens": [341, 1859, 13], "temperature": 0.0, "avg_logprob": -0.1853440399169922, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.1478273336251732e-05}, {"id": 155, "seek": 52384, "start": 528.08, "end": 532.2, "text": " It's a free knowledge base that's actually populated by humans, so they're filling in", "tokens": [467, 311, 257, 1737, 3601, 3096, 300, 311, 767, 32998, 538, 6255, 11, 370, 436, 434, 10623, 294], "temperature": 0.0, "avg_logprob": -0.1853440399169922, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.1478273336251732e-05}, {"id": 156, "seek": 52384, "start": 532.2, "end": 534.2800000000001, "text": " these relations and entities.", "tokens": [613, 2299, 293, 16667, 13], "temperature": 0.0, "avg_logprob": -0.1853440399169922, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.1478273336251732e-05}, {"id": 157, "seek": 52384, "start": 534.2800000000001, "end": 537.08, "text": " And it's also multilingual.", "tokens": [400, 309, 311, 611, 2120, 38219, 13], "temperature": 0.0, "avg_logprob": -0.1853440399169922, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.1478273336251732e-05}, {"id": 158, "seek": 52384, "start": 537.08, "end": 542.32, "text": " So if you want information from this knowledge base, what you do is you'd write a SQL query.", "tokens": [407, 498, 291, 528, 1589, 490, 341, 3601, 3096, 11, 437, 291, 360, 307, 291, 1116, 2464, 257, 19200, 14581, 13], "temperature": 0.0, "avg_logprob": -0.1853440399169922, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.1478273336251732e-05}, {"id": 159, "seek": 52384, "start": 542.32, "end": 543.6800000000001, "text": " This is a simplified one.", "tokens": [639, 307, 257, 26335, 472, 13], "temperature": 0.0, "avg_logprob": -0.1853440399169922, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.1478273336251732e-05}, {"id": 160, "seek": 52384, "start": 543.6800000000001, "end": 548.4000000000001, "text": " But the idea is you'd want to figure out the date of birth of Franklin Roosevelt, so", "tokens": [583, 264, 1558, 307, 291, 1116, 528, 281, 2573, 484, 264, 4002, 295, 3965, 295, 22010, 28515, 11, 370], "temperature": 0.0, "avg_logprob": -0.1853440399169922, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.1478273336251732e-05}, {"id": 161, "seek": 52384, "start": 548.4000000000001, "end": 552.2800000000001, "text": " you would write a query like follows.", "tokens": [291, 576, 2464, 257, 14581, 411, 10002, 13], "temperature": 0.0, "avg_logprob": -0.1853440399169922, "compression_ratio": 1.6923076923076923, "no_speech_prob": 1.1478273336251732e-05}, {"id": 162, "seek": 55228, "start": 552.28, "end": 556.24, "text": " Now if instead you want to query a language model as a knowledge base, you'll have something", "tokens": [823, 498, 2602, 291, 528, 281, 14581, 257, 2856, 2316, 382, 257, 3601, 3096, 11, 291, 603, 362, 746], "temperature": 0.0, "avg_logprob": -0.12358072454279119, "compression_ratio": 1.8269230769230769, "no_speech_prob": 3.882717646774836e-05}, {"id": 163, "seek": 55228, "start": 556.24, "end": 560.52, "text": " like this diagram that you've actually probably seen in several lectures now.", "tokens": [411, 341, 10686, 300, 291, 600, 767, 1391, 1612, 294, 2940, 16564, 586, 13], "temperature": 0.0, "avg_logprob": -0.12358072454279119, "compression_ratio": 1.8269230769230769, "no_speech_prob": 3.882717646774836e-05}, {"id": 164, "seek": 55228, "start": 560.52, "end": 565.04, "text": " And the idea is you'll train a language model over this unstructured text.", "tokens": [400, 264, 1558, 307, 291, 603, 3847, 257, 2856, 2316, 670, 341, 18799, 46847, 2487, 13], "temperature": 0.0, "avg_logprob": -0.12358072454279119, "compression_ratio": 1.8269230769230769, "no_speech_prob": 3.882717646774836e-05}, {"id": 165, "seek": 55228, "start": 565.04, "end": 570.0, "text": " And then you'll use a language model to just answer these natural language query statements.", "tokens": [400, 550, 291, 603, 764, 257, 2856, 2316, 281, 445, 1867, 613, 3303, 2856, 14581, 12363, 13], "temperature": 0.0, "avg_logprob": -0.12358072454279119, "compression_ratio": 1.8269230769230769, "no_speech_prob": 3.882717646774836e-05}, {"id": 166, "seek": 55228, "start": 570.0, "end": 576.04, "text": " So here, this is the work on T5 where they're training T5 over natural language or just", "tokens": [407, 510, 11, 341, 307, 264, 589, 322, 314, 20, 689, 436, 434, 3097, 314, 20, 670, 3303, 2856, 420, 445], "temperature": 0.0, "avg_logprob": -0.12358072454279119, "compression_ratio": 1.8269230769230769, "no_speech_prob": 3.882717646774836e-05}, {"id": 167, "seek": 55228, "start": 576.04, "end": 579.04, "text": " unstructured text with the span corruption task.", "tokens": [18799, 46847, 2487, 365, 264, 16174, 17959, 5633, 13], "temperature": 0.0, "avg_logprob": -0.12358072454279119, "compression_ratio": 1.8269230769230769, "no_speech_prob": 3.882717646774836e-05}, {"id": 168, "seek": 57904, "start": 579.04, "end": 582.88, "text": " And then they're asking T5 when was Franklin D. Roosevelt born?", "tokens": [400, 550, 436, 434, 3365, 314, 20, 562, 390, 22010, 413, 13, 28515, 4232, 30], "temperature": 0.0, "avg_logprob": -0.17490226503402467, "compression_ratio": 1.7707641196013288, "no_speech_prob": 5.014433554606512e-06}, {"id": 169, "seek": 57904, "start": 582.88, "end": 586.4399999999999, "text": " And the idea is T5 will produce a textual answer.", "tokens": [400, 264, 1558, 307, 314, 20, 486, 5258, 257, 2487, 901, 1867, 13], "temperature": 0.0, "avg_logprob": -0.17490226503402467, "compression_ratio": 1.7707641196013288, "no_speech_prob": 5.014433554606512e-06}, {"id": 170, "seek": 57904, "start": 586.4399999999999, "end": 590.12, "text": " So you can see this contrast very much with the old approach of using a traditional knowledge", "tokens": [407, 291, 393, 536, 341, 8712, 588, 709, 365, 264, 1331, 3109, 295, 1228, 257, 5164, 3601], "temperature": 0.0, "avg_logprob": -0.17490226503402467, "compression_ratio": 1.7707641196013288, "no_speech_prob": 5.014433554606512e-06}, {"id": 171, "seek": 57904, "start": 590.12, "end": 598.16, "text": " base for the knowledge base is structured and you have the SQL statements to query it.", "tokens": [3096, 337, 264, 3601, 3096, 307, 18519, 293, 291, 362, 264, 19200, 12363, 281, 14581, 309, 13], "temperature": 0.0, "avg_logprob": -0.17490226503402467, "compression_ratio": 1.7707641196013288, "no_speech_prob": 5.014433554606512e-06}, {"id": 172, "seek": 57904, "start": 598.16, "end": 601.4399999999999, "text": " So what are the advantages of using language models over traditional knowledge bases?", "tokens": [407, 437, 366, 264, 14906, 295, 1228, 2856, 5245, 670, 5164, 3601, 17949, 30], "temperature": 0.0, "avg_logprob": -0.17490226503402467, "compression_ratio": 1.7707641196013288, "no_speech_prob": 5.014433554606512e-06}, {"id": 173, "seek": 57904, "start": 601.4399999999999, "end": 604.7199999999999, "text": " And why might people think this could be a good idea?", "tokens": [400, 983, 1062, 561, 519, 341, 727, 312, 257, 665, 1558, 30], "temperature": 0.0, "avg_logprob": -0.17490226503402467, "compression_ratio": 1.7707641196013288, "no_speech_prob": 5.014433554606512e-06}, {"id": 174, "seek": 57904, "start": 604.7199999999999, "end": 608.8399999999999, "text": " Well for one, the language models are pre-trained over large amounts of unstructured and unlabeled", "tokens": [1042, 337, 472, 11, 264, 2856, 5245, 366, 659, 12, 17227, 2001, 670, 2416, 11663, 295, 18799, 46847, 293, 32118, 18657, 292], "temperature": 0.0, "avg_logprob": -0.17490226503402467, "compression_ratio": 1.7707641196013288, "no_speech_prob": 5.014433554606512e-06}, {"id": 175, "seek": 60884, "start": 608.84, "end": 614.08, "text": " text, whereas traditional knowledge bases require manual annotation like with wiki data", "tokens": [2487, 11, 9735, 5164, 3601, 17949, 3651, 9688, 48654, 411, 365, 261, 9850, 1412], "temperature": 0.0, "avg_logprob": -0.20619715522317325, "compression_ratio": 1.569672131147541, "no_speech_prob": 3.882816599798389e-05}, {"id": 176, "seek": 60884, "start": 614.08, "end": 619.24, "text": " people actually are populating it, or complex NLP pipelines to extract from unstructured", "tokens": [561, 767, 366, 1665, 12162, 309, 11, 420, 3997, 426, 45196, 40168, 281, 8947, 490, 18799, 46847], "temperature": 0.0, "avg_logprob": -0.20619715522317325, "compression_ratio": 1.569672131147541, "no_speech_prob": 3.882816599798389e-05}, {"id": 177, "seek": 60884, "start": 619.24, "end": 624.72, "text": " text into a structured form that forms a knowledge base.", "tokens": [2487, 666, 257, 18519, 1254, 300, 6422, 257, 3601, 3096, 13], "temperature": 0.0, "avg_logprob": -0.20619715522317325, "compression_ratio": 1.569672131147541, "no_speech_prob": 3.882816599798389e-05}, {"id": 178, "seek": 60884, "start": 624.72, "end": 628.76, "text": " Language models can also support more flexible natural language queries.", "tokens": [24445, 5245, 393, 611, 1406, 544, 11358, 3303, 2856, 24109, 13], "temperature": 0.0, "avg_logprob": -0.20619715522317325, "compression_ratio": 1.569672131147541, "no_speech_prob": 3.882816599798389e-05}, {"id": 179, "seek": 60884, "start": 628.76, "end": 634.2800000000001, "text": " So if we take the example, what does the final F in the song UFOF stand for?", "tokens": [407, 498, 321, 747, 264, 1365, 11, 437, 775, 264, 2572, 479, 294, 264, 2153, 28318, 37, 1463, 337, 30], "temperature": 0.0, "avg_logprob": -0.20619715522317325, "compression_ratio": 1.569672131147541, "no_speech_prob": 3.882816599798389e-05}, {"id": 180, "seek": 63428, "start": 634.28, "end": 639.24, "text": " A knowledge base probably won't have a field for final F, so it won't be able to answer your", "tokens": [316, 3601, 3096, 1391, 1582, 380, 362, 257, 2519, 337, 2572, 479, 11, 370, 309, 1582, 380, 312, 1075, 281, 1867, 428], "temperature": 0.0, "avg_logprob": -0.17112985253334045, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.0461164240259677e-05}, {"id": 181, "seek": 63428, "start": 639.24, "end": 640.24, "text": " query.", "tokens": [14581, 13], "temperature": 0.0, "avg_logprob": -0.17112985253334045, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.0461164240259677e-05}, {"id": 182, "seek": 63428, "start": 640.24, "end": 643.48, "text": " But there's a chance that a language model could actually learn and have a response for", "tokens": [583, 456, 311, 257, 2931, 300, 257, 2856, 2316, 727, 767, 1466, 293, 362, 257, 4134, 337], "temperature": 0.0, "avg_logprob": -0.17112985253334045, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.0461164240259677e-05}, {"id": 183, "seek": 63428, "start": 643.48, "end": 646.36, "text": " this natural language query.", "tokens": [341, 3303, 2856, 14581, 13], "temperature": 0.0, "avg_logprob": -0.17112985253334045, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.0461164240259677e-05}, {"id": 184, "seek": 63428, "start": 646.36, "end": 650.52, "text": " They also had a less extreme example in this paper by Petroni and others, where maybe", "tokens": [814, 611, 632, 257, 1570, 8084, 1365, 294, 341, 3035, 538, 10472, 2044, 72, 293, 2357, 11, 689, 1310], "temperature": 0.0, "avg_logprob": -0.17112985253334045, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.0461164240259677e-05}, {"id": 185, "seek": 63428, "start": 650.52, "end": 654.0799999999999, "text": " your relation would be is works for in your knowledge base.", "tokens": [428, 9721, 576, 312, 307, 1985, 337, 294, 428, 3601, 3096, 13], "temperature": 0.0, "avg_logprob": -0.17112985253334045, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.0461164240259677e-05}, {"id": 186, "seek": 63428, "start": 654.0799999999999, "end": 656.68, "text": " And the new ask for is working for.", "tokens": [400, 264, 777, 1029, 337, 307, 1364, 337, 13], "temperature": 0.0, "avg_logprob": -0.17112985253334045, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.0461164240259677e-05}, {"id": 187, "seek": 63428, "start": 656.68, "end": 660.24, "text": " And the knowledge base doesn't have an exact match on the field, and so it returns an", "tokens": [400, 264, 3601, 3096, 1177, 380, 362, 364, 1900, 2995, 322, 264, 2519, 11, 293, 370, 309, 11247, 364], "temperature": 0.0, "avg_logprob": -0.17112985253334045, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.0461164240259677e-05}, {"id": 188, "seek": 63428, "start": 660.24, "end": 661.24, "text": " empty response.", "tokens": [6707, 4134, 13], "temperature": 0.0, "avg_logprob": -0.17112985253334045, "compression_ratio": 1.7605633802816902, "no_speech_prob": 2.0461164240259677e-05}, {"id": 189, "seek": 66124, "start": 661.24, "end": 666.04, "text": " And it's much, it's reasonable to believe that your language model could figure out that", "tokens": [400, 309, 311, 709, 11, 309, 311, 10585, 281, 1697, 300, 428, 2856, 2316, 727, 2573, 484, 300], "temperature": 0.0, "avg_logprob": -0.1491487955642959, "compression_ratio": 1.7326388888888888, "no_speech_prob": 4.756950511364266e-05}, {"id": 190, "seek": 66124, "start": 666.04, "end": 667.72, "text": " these relations are similar.", "tokens": [613, 2299, 366, 2531, 13], "temperature": 0.0, "avg_logprob": -0.1491487955642959, "compression_ratio": 1.7326388888888888, "no_speech_prob": 4.756950511364266e-05}, {"id": 191, "seek": 66124, "start": 667.72, "end": 673.2, "text": " So if I know the answer to one of them, I probably know the answer to the other.", "tokens": [407, 498, 286, 458, 264, 1867, 281, 472, 295, 552, 11, 286, 1391, 458, 264, 1867, 281, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.1491487955642959, "compression_ratio": 1.7326388888888888, "no_speech_prob": 4.756950511364266e-05}, {"id": 192, "seek": 66124, "start": 673.2, "end": 675.16, "text": " Of course, it's not all advantages.", "tokens": [2720, 1164, 11, 309, 311, 406, 439, 14906, 13], "temperature": 0.0, "avg_logprob": -0.1491487955642959, "compression_ratio": 1.7326388888888888, "no_speech_prob": 4.756950511364266e-05}, {"id": 193, "seek": 66124, "start": 675.16, "end": 679.44, "text": " There's also many open challenges using language models as knowledge bases.", "tokens": [821, 311, 611, 867, 1269, 4759, 1228, 2856, 5245, 382, 3601, 17949, 13], "temperature": 0.0, "avg_logprob": -0.1491487955642959, "compression_ratio": 1.7326388888888888, "no_speech_prob": 4.756950511364266e-05}, {"id": 194, "seek": 66124, "start": 679.44, "end": 681.52, "text": " So for one, it's harder to interpret.", "tokens": [407, 337, 472, 11, 309, 311, 6081, 281, 7302, 13], "temperature": 0.0, "avg_logprob": -0.1491487955642959, "compression_ratio": 1.7326388888888888, "no_speech_prob": 4.756950511364266e-05}, {"id": 195, "seek": 66124, "start": 681.52, "end": 684.96, "text": " When a traditional knowledge base produces an answer, there's actually provenance information", "tokens": [1133, 257, 5164, 3601, 3096, 14725, 364, 1867, 11, 456, 311, 767, 12785, 719, 1589], "temperature": 0.0, "avg_logprob": -0.1491487955642959, "compression_ratio": 1.7326388888888888, "no_speech_prob": 4.756950511364266e-05}, {"id": 196, "seek": 66124, "start": 684.96, "end": 688.32, "text": " associated with why did it return that particular query.", "tokens": [6615, 365, 983, 630, 309, 2736, 300, 1729, 14581, 13], "temperature": 0.0, "avg_logprob": -0.1491487955642959, "compression_ratio": 1.7326388888888888, "no_speech_prob": 4.756950511364266e-05}, {"id": 197, "seek": 68832, "start": 688.32, "end": 694.32, "text": " But with a language model, it's really not clear why it might produce a prediction.", "tokens": [583, 365, 257, 2856, 2316, 11, 309, 311, 534, 406, 1850, 983, 309, 1062, 5258, 257, 17630, 13], "temperature": 0.0, "avg_logprob": -0.20167882101876394, "compression_ratio": 1.8071428571428572, "no_speech_prob": 4.1981678805314004e-05}, {"id": 198, "seek": 68832, "start": 694.32, "end": 698.36, "text": " The knowledge is just encoded in the parameters of the model.", "tokens": [440, 3601, 307, 445, 2058, 12340, 294, 264, 9834, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20167882101876394, "compression_ratio": 1.8071428571428572, "no_speech_prob": 4.1981678805314004e-05}, {"id": 199, "seek": 68832, "start": 698.36, "end": 699.96, "text": " It's also harder to trust.", "tokens": [467, 311, 611, 6081, 281, 3361, 13], "temperature": 0.0, "avg_logprob": -0.20167882101876394, "compression_ratio": 1.8071428571428572, "no_speech_prob": 4.1981678805314004e-05}, {"id": 200, "seek": 68832, "start": 699.96, "end": 705.08, "text": " So you saw this in Simon 5, where the language model could produce realistic predictions,", "tokens": [407, 291, 1866, 341, 294, 13193, 1025, 11, 689, 264, 2856, 2316, 727, 5258, 12465, 21264, 11], "temperature": 0.0, "avg_logprob": -0.20167882101876394, "compression_ratio": 1.8071428571428572, "no_speech_prob": 4.1981678805314004e-05}, {"id": 201, "seek": 68832, "start": 705.08, "end": 706.48, "text": " but they are incorrect.", "tokens": [457, 436, 366, 18424, 13], "temperature": 0.0, "avg_logprob": -0.20167882101876394, "compression_ratio": 1.8071428571428572, "no_speech_prob": 4.1981678805314004e-05}, {"id": 202, "seek": 68832, "start": 706.48, "end": 710.7600000000001, "text": " So it's not easy to know when the language model actually knows the fact versus it's using", "tokens": [407, 309, 311, 406, 1858, 281, 458, 562, 264, 2856, 2316, 767, 3255, 264, 1186, 5717, 309, 311, 1228], "temperature": 0.0, "avg_logprob": -0.20167882101876394, "compression_ratio": 1.8071428571428572, "no_speech_prob": 4.1981678805314004e-05}, {"id": 203, "seek": 68832, "start": 710.7600000000001, "end": 713.0, "text": " some like biases to make its prediction.", "tokens": [512, 411, 32152, 281, 652, 1080, 17630, 13], "temperature": 0.0, "avg_logprob": -0.20167882101876394, "compression_ratio": 1.8071428571428572, "no_speech_prob": 4.1981678805314004e-05}, {"id": 204, "seek": 68832, "start": 713.0, "end": 717.12, "text": " And in the case of the traditional knowledge base, if it doesn't know a fact, it's just", "tokens": [400, 294, 264, 1389, 295, 264, 5164, 3601, 3096, 11, 498, 309, 1177, 380, 458, 257, 1186, 11, 309, 311, 445], "temperature": 0.0, "avg_logprob": -0.20167882101876394, "compression_ratio": 1.8071428571428572, "no_speech_prob": 4.1981678805314004e-05}, {"id": 205, "seek": 71712, "start": 717.12, "end": 720.2, "text": " going to have an empty response.", "tokens": [516, 281, 362, 364, 6707, 4134, 13], "temperature": 0.0, "avg_logprob": -0.15564722173354206, "compression_ratio": 1.6470588235294117, "no_speech_prob": 2.392246460658498e-05}, {"id": 206, "seek": 71712, "start": 720.2, "end": 725.36, "text": " And then finally, language models are harder to modify.", "tokens": [400, 550, 2721, 11, 2856, 5245, 366, 6081, 281, 16927, 13], "temperature": 0.0, "avg_logprob": -0.15564722173354206, "compression_ratio": 1.6470588235294117, "no_speech_prob": 2.392246460658498e-05}, {"id": 207, "seek": 71712, "start": 725.36, "end": 729.2, "text": " So in a knowledge base, if you want to update a fact, you just change the fact directly", "tokens": [407, 294, 257, 3601, 3096, 11, 498, 291, 528, 281, 5623, 257, 1186, 11, 291, 445, 1319, 264, 1186, 3838], "temperature": 0.0, "avg_logprob": -0.15564722173354206, "compression_ratio": 1.6470588235294117, "no_speech_prob": 2.392246460658498e-05}, {"id": 208, "seek": 71712, "start": 729.2, "end": 731.96, "text": " in the structured data.", "tokens": [294, 264, 18519, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15564722173354206, "compression_ratio": 1.6470588235294117, "no_speech_prob": 2.392246460658498e-05}, {"id": 209, "seek": 71712, "start": 731.96, "end": 734.96, "text": " But a language model is not quite clear how you would do this.", "tokens": [583, 257, 2856, 2316, 307, 406, 1596, 1850, 577, 291, 576, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.15564722173354206, "compression_ratio": 1.6470588235294117, "no_speech_prob": 2.392246460658498e-05}, {"id": 210, "seek": 71712, "start": 734.96, "end": 739.44, "text": " You could fine tune the model longer on the updated data, but how do you know if it still", "tokens": [509, 727, 2489, 10864, 264, 2316, 2854, 322, 264, 10588, 1412, 11, 457, 577, 360, 291, 458, 498, 309, 920], "temperature": 0.0, "avg_logprob": -0.15564722173354206, "compression_ratio": 1.6470588235294117, "no_speech_prob": 2.392246460658498e-05}, {"id": 211, "seek": 71712, "start": 739.44, "end": 743.48, "text": " has some memorization of the old fact?", "tokens": [575, 512, 10560, 2144, 295, 264, 1331, 1186, 30], "temperature": 0.0, "avg_logprob": -0.15564722173354206, "compression_ratio": 1.6470588235294117, "no_speech_prob": 2.392246460658498e-05}, {"id": 212, "seek": 74348, "start": 743.48, "end": 747.52, "text": " So there are a lot of open challenges to this goal of actually using language models as", "tokens": [407, 456, 366, 257, 688, 295, 1269, 4759, 281, 341, 3387, 295, 767, 1228, 2856, 5245, 382], "temperature": 0.0, "avg_logprob": -0.21317755922358086, "compression_ratio": 1.7257383966244726, "no_speech_prob": 3.071251921937801e-05}, {"id": 213, "seek": 74348, "start": 747.52, "end": 749.4, "text": " traditional knowledge bases.", "tokens": [5164, 3601, 17949, 13], "temperature": 0.0, "avg_logprob": -0.21317755922358086, "compression_ratio": 1.7257383966244726, "no_speech_prob": 3.071251921937801e-05}, {"id": 214, "seek": 74348, "start": 749.4, "end": 753.16, "text": " But hopefully you see why some people think this could actually be a good idea.", "tokens": [583, 4696, 291, 536, 983, 512, 561, 519, 341, 727, 767, 312, 257, 665, 1558, 13], "temperature": 0.0, "avg_logprob": -0.21317755922358086, "compression_ratio": 1.7257383966244726, "no_speech_prob": 3.071251921937801e-05}, {"id": 215, "seek": 74348, "start": 753.16, "end": 757.9200000000001, "text": " And why researchers are interested in training language models that can actually integrate", "tokens": [400, 983, 10309, 366, 3102, 294, 3097, 2856, 5245, 300, 393, 767, 13365], "temperature": 0.0, "avg_logprob": -0.21317755922358086, "compression_ratio": 1.7257383966244726, "no_speech_prob": 3.071251921937801e-05}, {"id": 216, "seek": 74348, "start": 757.9200000000001, "end": 761.5600000000001, "text": " more knowledge.", "tokens": [544, 3601, 13], "temperature": 0.0, "avg_logprob": -0.21317755922358086, "compression_ratio": 1.7257383966244726, "no_speech_prob": 3.071251921937801e-05}, {"id": 217, "seek": 74348, "start": 761.5600000000001, "end": 763.72, "text": " So that brings us to section two of the talk.", "tokens": [407, 300, 5607, 505, 281, 3541, 732, 295, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.21317755922358086, "compression_ratio": 1.7257383966244726, "no_speech_prob": 3.071251921937801e-05}, {"id": 218, "seek": 74348, "start": 763.72, "end": 767.76, "text": " So I want to pause here just in case there's any questions.", "tokens": [407, 286, 528, 281, 10465, 510, 445, 294, 1389, 456, 311, 604, 1651, 13], "temperature": 0.0, "avg_logprob": -0.21317755922358086, "compression_ratio": 1.7257383966244726, "no_speech_prob": 3.071251921937801e-05}, {"id": 219, "seek": 76776, "start": 767.76, "end": 773.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.17075673016634854, "compression_ratio": 1.5586854460093897, "no_speech_prob": 5.6486635003238916e-05}, {"id": 220, "seek": 76776, "start": 773.72, "end": 774.72, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.17075673016634854, "compression_ratio": 1.5586854460093897, "no_speech_prob": 5.6486635003238916e-05}, {"id": 221, "seek": 76776, "start": 774.72, "end": 775.72, "text": " Awesome.", "tokens": [10391, 13], "temperature": 0.0, "avg_logprob": -0.17075673016634854, "compression_ratio": 1.5586854460093897, "no_speech_prob": 5.6486635003238916e-05}, {"id": 222, "seek": 76776, "start": 775.72, "end": 780.24, "text": " So now we're going to be talking about what techniques researchers are using to actually", "tokens": [407, 586, 321, 434, 516, 281, 312, 1417, 466, 437, 7512, 10309, 366, 1228, 281, 767], "temperature": 0.0, "avg_logprob": -0.17075673016634854, "compression_ratio": 1.5586854460093897, "no_speech_prob": 5.6486635003238916e-05}, {"id": 223, "seek": 76776, "start": 780.24, "end": 783.92, "text": " add more knowledge to language models.", "tokens": [909, 544, 3601, 281, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17075673016634854, "compression_ratio": 1.5586854460093897, "no_speech_prob": 5.6486635003238916e-05}, {"id": 224, "seek": 76776, "start": 783.92, "end": 786.6, "text": " So we're going to talk about three broad classes of techniques.", "tokens": [407, 321, 434, 516, 281, 751, 466, 1045, 4152, 5359, 295, 7512, 13], "temperature": 0.0, "avg_logprob": -0.17075673016634854, "compression_ratio": 1.5586854460093897, "no_speech_prob": 5.6486635003238916e-05}, {"id": 225, "seek": 76776, "start": 786.6, "end": 790.48, "text": " This is by no means exhaustive, but hopefully it gives you a good overview so that if you", "tokens": [639, 307, 538, 572, 1355, 14687, 488, 11, 457, 4696, 309, 2709, 291, 257, 665, 12492, 370, 300, 498, 291], "temperature": 0.0, "avg_logprob": -0.17075673016634854, "compression_ratio": 1.5586854460093897, "no_speech_prob": 5.6486635003238916e-05}, {"id": 226, "seek": 76776, "start": 790.48, "end": 793.68, "text": " want to dive deeper, you can.", "tokens": [528, 281, 9192, 7731, 11, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.17075673016634854, "compression_ratio": 1.5586854460093897, "no_speech_prob": 5.6486635003238916e-05}, {"id": 227, "seek": 79368, "start": 793.68, "end": 797.9599999999999, "text": " So we'll start by talking about adding pre-trained entity embeddings and for each section we'll", "tokens": [407, 321, 603, 722, 538, 1417, 466, 5127, 659, 12, 17227, 2001, 13977, 12240, 29432, 293, 337, 1184, 3541, 321, 603], "temperature": 0.0, "avg_logprob": -0.10531605455212127, "compression_ratio": 1.8862068965517242, "no_speech_prob": 1.4509534594253637e-05}, {"id": 228, "seek": 79368, "start": 797.9599999999999, "end": 802.7199999999999, "text": " kind of focus on the first work that you see in the bullets, but we'll also talk about", "tokens": [733, 295, 1879, 322, 264, 700, 589, 300, 291, 536, 294, 264, 20132, 11, 457, 321, 603, 611, 751, 466], "temperature": 0.0, "avg_logprob": -0.10531605455212127, "compression_ratio": 1.8862068965517242, "no_speech_prob": 1.4509534594253637e-05}, {"id": 229, "seek": 79368, "start": 802.7199999999999, "end": 803.92, "text": " briefly some of the variants.", "tokens": [10515, 512, 295, 264, 21669, 13], "temperature": 0.0, "avg_logprob": -0.10531605455212127, "compression_ratio": 1.8862068965517242, "no_speech_prob": 1.4509534594253637e-05}, {"id": 230, "seek": 79368, "start": 803.92, "end": 811.68, "text": " So you see how the works within each class can differ and what knobs you can turn.", "tokens": [407, 291, 536, 577, 264, 1985, 1951, 1184, 1508, 393, 743, 293, 437, 46999, 291, 393, 1261, 13], "temperature": 0.0, "avg_logprob": -0.10531605455212127, "compression_ratio": 1.8862068965517242, "no_speech_prob": 1.4509534594253637e-05}, {"id": 231, "seek": 79368, "start": 811.68, "end": 815.92, "text": " So for adding pre-trained embeddings, we first need to figure out what pre-trained embeddings", "tokens": [407, 337, 5127, 659, 12, 17227, 2001, 12240, 29432, 11, 321, 700, 643, 281, 2573, 484, 437, 659, 12, 17227, 2001, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.10531605455212127, "compression_ratio": 1.8862068965517242, "no_speech_prob": 1.4509534594253637e-05}, {"id": 232, "seek": 79368, "start": 815.92, "end": 819.8, "text": " would actually be the most useful to add knowledge to language models.", "tokens": [576, 767, 312, 264, 881, 4420, 281, 909, 3601, 281, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.10531605455212127, "compression_ratio": 1.8862068965517242, "no_speech_prob": 1.4509534594253637e-05}, {"id": 233, "seek": 79368, "start": 819.8, "end": 823.1999999999999, "text": " And this can start with an observation that facts about the world are usually in terms", "tokens": [400, 341, 393, 722, 365, 364, 14816, 300, 9130, 466, 264, 1002, 366, 2673, 294, 2115], "temperature": 0.0, "avg_logprob": -0.10531605455212127, "compression_ratio": 1.8862068965517242, "no_speech_prob": 1.4509534594253637e-05}, {"id": 234, "seek": 82320, "start": 823.2, "end": 825.32, "text": " of entities.", "tokens": [295, 16667, 13], "temperature": 0.0, "avg_logprob": -0.13323242524090936, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.753304761426989e-05}, {"id": 235, "seek": 82320, "start": 825.32, "end": 829.48, "text": " So if we have a fact like Washington was the first president of the United States, we have", "tokens": [407, 498, 321, 362, 257, 1186, 411, 6149, 390, 264, 700, 3868, 295, 264, 2824, 3040, 11, 321, 362], "temperature": 0.0, "avg_logprob": -0.13323242524090936, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.753304761426989e-05}, {"id": 236, "seek": 82320, "start": 829.48, "end": 833.12, "text": " the entities Washington United States.", "tokens": [264, 16667, 6149, 2824, 3040, 13], "temperature": 0.0, "avg_logprob": -0.13323242524090936, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.753304761426989e-05}, {"id": 237, "seek": 82320, "start": 833.12, "end": 837.12, "text": " But pre-trained word embeddings don't have this notion of entities.", "tokens": [583, 659, 12, 17227, 2001, 1349, 12240, 29432, 500, 380, 362, 341, 10710, 295, 16667, 13], "temperature": 0.0, "avg_logprob": -0.13323242524090936, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.753304761426989e-05}, {"id": 238, "seek": 82320, "start": 837.12, "end": 842.1600000000001, "text": " So we'd have different word embeddings for USA, United States America, and America, even", "tokens": [407, 321, 1116, 362, 819, 1349, 12240, 29432, 337, 10827, 11, 2824, 3040, 3374, 11, 293, 3374, 11, 754], "temperature": 0.0, "avg_logprob": -0.13323242524090936, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.753304761426989e-05}, {"id": 239, "seek": 82320, "start": 842.1600000000001, "end": 845.12, "text": " though these all refer to the same entity.", "tokens": [1673, 613, 439, 2864, 281, 264, 912, 13977, 13], "temperature": 0.0, "avg_logprob": -0.13323242524090936, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.753304761426989e-05}, {"id": 240, "seek": 82320, "start": 845.12, "end": 848.76, "text": " And this makes it challenging for the language model to actually learn any representations", "tokens": [400, 341, 1669, 309, 7595, 337, 264, 2856, 2316, 281, 767, 1466, 604, 33358], "temperature": 0.0, "avg_logprob": -0.13323242524090936, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.753304761426989e-05}, {"id": 241, "seek": 84876, "start": 848.76, "end": 855.68, "text": " over these entities, since they may be referred to many ways in the text.", "tokens": [670, 613, 16667, 11, 1670, 436, 815, 312, 10839, 281, 867, 2098, 294, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.17125539576753657, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.2028826859022956e-05}, {"id": 242, "seek": 84876, "start": 855.68, "end": 862.64, "text": " So what I've been said, we have a single embedding per entity and law for these as entity embeddings.", "tokens": [407, 437, 286, 600, 668, 848, 11, 321, 362, 257, 2167, 12240, 3584, 680, 13977, 293, 2101, 337, 613, 382, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.17125539576753657, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.2028826859022956e-05}, {"id": 243, "seek": 84876, "start": 862.64, "end": 867.84, "text": " So now you'd have a single entity embedding for USA, United States America, and America.", "tokens": [407, 586, 291, 1116, 362, 257, 2167, 13977, 12240, 3584, 337, 10827, 11, 2824, 3040, 3374, 11, 293, 3374, 13], "temperature": 0.0, "avg_logprob": -0.17125539576753657, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.2028826859022956e-05}, {"id": 244, "seek": 84876, "start": 867.84, "end": 873.48, "text": " And whenever you see a phrase in text referring to this entity, you would use the same entity", "tokens": [400, 5699, 291, 536, 257, 9535, 294, 2487, 13761, 281, 341, 13977, 11, 291, 576, 764, 264, 912, 13977], "temperature": 0.0, "avg_logprob": -0.17125539576753657, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.2028826859022956e-05}, {"id": 245, "seek": 84876, "start": 873.48, "end": 875.68, "text": " embedding.", "tokens": [12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.17125539576753657, "compression_ratio": 1.7826086956521738, "no_speech_prob": 1.2028826859022956e-05}, {"id": 246, "seek": 87568, "start": 875.68, "end": 880.12, "text": " And these entity embeddings can actually be pre-trained to encode this factual knowledge about the", "tokens": [400, 613, 13977, 12240, 29432, 393, 767, 312, 659, 12, 17227, 2001, 281, 2058, 1429, 341, 48029, 3601, 466, 264], "temperature": 0.0, "avg_logprob": -0.1547037555325416, "compression_ratio": 1.8697318007662835, "no_speech_prob": 2.6685544071369804e-05}, {"id": 247, "seek": 87568, "start": 880.12, "end": 881.12, "text": " world.", "tokens": [1002, 13], "temperature": 0.0, "avg_logprob": -0.1547037555325416, "compression_ratio": 1.8697318007662835, "no_speech_prob": 2.6685544071369804e-05}, {"id": 248, "seek": 87568, "start": 881.12, "end": 884.88, "text": " And this first class techniques we'll be looking at will be how do you actually best use", "tokens": [400, 341, 700, 1508, 7512, 321, 603, 312, 1237, 412, 486, 312, 577, 360, 291, 767, 1151, 764], "temperature": 0.0, "avg_logprob": -0.1547037555325416, "compression_ratio": 1.8697318007662835, "no_speech_prob": 2.6685544071369804e-05}, {"id": 249, "seek": 87568, "start": 884.88, "end": 890.4, "text": " these pre-trained entity embeddings in a language model.", "tokens": [613, 659, 12, 17227, 2001, 13977, 12240, 29432, 294, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1547037555325416, "compression_ratio": 1.8697318007662835, "no_speech_prob": 2.6685544071369804e-05}, {"id": 250, "seek": 87568, "start": 890.4, "end": 894.4399999999999, "text": " So I need to make up for a quick note that these entity embeddings are only useful to a language", "tokens": [407, 286, 643, 281, 652, 493, 337, 257, 1702, 3637, 300, 613, 13977, 12240, 29432, 366, 787, 4420, 281, 257, 2856], "temperature": 0.0, "avg_logprob": -0.1547037555325416, "compression_ratio": 1.8697318007662835, "no_speech_prob": 2.6685544071369804e-05}, {"id": 251, "seek": 87568, "start": 894.4399999999999, "end": 895.4399999999999, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.1547037555325416, "compression_ratio": 1.8697318007662835, "no_speech_prob": 2.6685544071369804e-05}, {"id": 252, "seek": 87568, "start": 895.4399999999999, "end": 900.5999999999999, "text": " So if you can do another NLP task called entity linking well.", "tokens": [407, 498, 291, 393, 360, 1071, 426, 45196, 5633, 1219, 13977, 25775, 731, 13], "temperature": 0.0, "avg_logprob": -0.1547037555325416, "compression_ratio": 1.8697318007662835, "no_speech_prob": 2.6685544071369804e-05}, {"id": 253, "seek": 87568, "start": 900.5999999999999, "end": 905.24, "text": " So I'm going to take a quick aside and explain what is entity linking.", "tokens": [407, 286, 478, 516, 281, 747, 257, 1702, 7359, 293, 2903, 437, 307, 13977, 25775, 13], "temperature": 0.0, "avg_logprob": -0.1547037555325416, "compression_ratio": 1.8697318007662835, "no_speech_prob": 2.6685544071369804e-05}, {"id": 254, "seek": 90524, "start": 905.24, "end": 909.48, "text": " So a definition of entity linking is the link mentions in text to entities in a knowledge", "tokens": [407, 257, 7123, 295, 13977, 25775, 307, 264, 2113, 23844, 294, 2487, 281, 16667, 294, 257, 3601], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 255, "seek": 90524, "start": 909.48, "end": 910.48, "text": " base.", "tokens": [3096, 13], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 256, "seek": 90524, "start": 910.48, "end": 914.24, "text": " I like to think about this in terms of how you use word embeddings.", "tokens": [286, 411, 281, 519, 466, 341, 294, 2115, 295, 577, 291, 764, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 257, "seek": 90524, "start": 914.24, "end": 917.16, "text": " So if you want to use word embeddings and you have a sentence, you're going to first", "tokens": [407, 498, 291, 528, 281, 764, 1349, 12240, 29432, 293, 291, 362, 257, 8174, 11, 291, 434, 516, 281, 700], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 258, "seek": 90524, "start": 917.16, "end": 919.4, "text": " tokenize that sentence into words.", "tokens": [14862, 1125, 300, 8174, 666, 2283, 13], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 259, "seek": 90524, "start": 919.4, "end": 922.92, "text": " And then for each word, you're going to look up their corresponding ID in some word embedding", "tokens": [400, 550, 337, 1184, 1349, 11, 291, 434, 516, 281, 574, 493, 641, 11760, 7348, 294, 512, 1349, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 260, "seek": 90524, "start": 922.92, "end": 923.92, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 261, "seek": 90524, "start": 923.92, "end": 926.32, "text": " And now you have your word embedding.", "tokens": [400, 586, 291, 362, 428, 1349, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 262, "seek": 90524, "start": 926.32, "end": 930.04, "text": " Well for entity embeddings, the dictionary look up isn't so easy.", "tokens": [1042, 337, 13977, 12240, 29432, 11, 264, 25890, 574, 493, 1943, 380, 370, 1858, 13], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 263, "seek": 90524, "start": 930.04, "end": 934.4, "text": " You might have sentences like Washington's the first present United States.", "tokens": [509, 1062, 362, 16579, 411, 6149, 311, 264, 700, 1974, 2824, 3040, 13], "temperature": 0.0, "avg_logprob": -0.15320601830115685, "compression_ratio": 1.8770764119601329, "no_speech_prob": 8.939057806855999e-06}, {"id": 264, "seek": 93440, "start": 934.4, "end": 936.6, "text": " Well Washington has two different candidates.", "tokens": [1042, 6149, 575, 732, 819, 11255, 13], "temperature": 0.0, "avg_logprob": -0.18182126884786493, "compression_ratio": 1.855072463768116, "no_speech_prob": 2.2124635506770574e-05}, {"id": 265, "seek": 93440, "start": 936.6, "end": 940.56, "text": " Are we talking about George Washington or we're talking about Washington state?", "tokens": [2014, 321, 1417, 466, 7136, 6149, 420, 321, 434, 1417, 466, 6149, 1785, 30], "temperature": 0.0, "avg_logprob": -0.18182126884786493, "compression_ratio": 1.855072463768116, "no_speech_prob": 2.2124635506770574e-05}, {"id": 266, "seek": 93440, "start": 940.56, "end": 944.24, "text": " And these are different entities that have different entity embeddings.", "tokens": [400, 613, 366, 819, 16667, 300, 362, 819, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.18182126884786493, "compression_ratio": 1.855072463768116, "no_speech_prob": 2.2124635506770574e-05}, {"id": 267, "seek": 93440, "start": 944.24, "end": 949.4399999999999, "text": " And the QIDs here would just be their identifiers and wiki data.", "tokens": [400, 264, 1249, 2777, 82, 510, 576, 445, 312, 641, 2473, 23463, 293, 261, 9850, 1412, 13], "temperature": 0.0, "avg_logprob": -0.18182126884786493, "compression_ratio": 1.855072463768116, "no_speech_prob": 2.2124635506770574e-05}, {"id": 268, "seek": 93440, "start": 949.4399999999999, "end": 952.56, "text": " And the United States just has a single entity.", "tokens": [400, 264, 2824, 3040, 445, 575, 257, 2167, 13977, 13], "temperature": 0.0, "avg_logprob": -0.18182126884786493, "compression_ratio": 1.855072463768116, "no_speech_prob": 2.2124635506770574e-05}, {"id": 269, "seek": 93440, "start": 952.56, "end": 956.88, "text": " So a task of entity linking is to figure out correctly these ambiguous mentions what", "tokens": [407, 257, 5633, 295, 13977, 25775, 307, 281, 2573, 484, 8944, 613, 39465, 23844, 437], "temperature": 0.0, "avg_logprob": -0.18182126884786493, "compression_ratio": 1.855072463768116, "no_speech_prob": 2.2124635506770574e-05}, {"id": 270, "seek": 93440, "start": 956.88, "end": 960.36, "text": " entity do they actually link to in a knowledge base.", "tokens": [13977, 360, 436, 767, 2113, 281, 294, 257, 3601, 3096, 13], "temperature": 0.0, "avg_logprob": -0.18182126884786493, "compression_ratio": 1.855072463768116, "no_speech_prob": 2.2124635506770574e-05}, {"id": 271, "seek": 93440, "start": 960.36, "end": 963.64, "text": " And there's many different ways you can do this entity linking.", "tokens": [400, 456, 311, 867, 819, 2098, 291, 393, 360, 341, 13977, 25775, 13], "temperature": 0.0, "avg_logprob": -0.18182126884786493, "compression_ratio": 1.855072463768116, "no_speech_prob": 2.2124635506770574e-05}, {"id": 272, "seek": 96364, "start": 963.64, "end": 966.68, "text": " The one way you might be able to do this is to figure out that, oh, I see the context", "tokens": [440, 472, 636, 291, 1062, 312, 1075, 281, 360, 341, 307, 281, 2573, 484, 300, 11, 1954, 11, 286, 536, 264, 4319], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 273, "seek": 96364, "start": 966.68, "end": 967.88, "text": " word of president.", "tokens": [1349, 295, 3868, 13], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 274, "seek": 96364, "start": 967.88, "end": 971.96, "text": " So Washington probably links to George Washington.", "tokens": [407, 6149, 1391, 6123, 281, 7136, 6149, 13], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 275, "seek": 96364, "start": 971.96, "end": 973.1999999999999, "text": " Just some more definitions.", "tokens": [1449, 512, 544, 21988, 13], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 276, "seek": 96364, "start": 973.1999999999999, "end": 976.92, "text": " We're going to refer to Washington as a mention in United States as a mention.", "tokens": [492, 434, 516, 281, 2864, 281, 6149, 382, 257, 2152, 294, 2824, 3040, 382, 257, 2152, 13], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 277, "seek": 96364, "start": 976.92, "end": 981.24, "text": " And then the things that the mentioned could link to so the two options for Washington", "tokens": [400, 550, 264, 721, 300, 264, 2835, 727, 2113, 281, 370, 264, 732, 3956, 337, 6149], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 278, "seek": 96364, "start": 981.24, "end": 984.16, "text": " are going to be candidates.", "tokens": [366, 516, 281, 312, 11255, 13], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 279, "seek": 96364, "start": 984.16, "end": 985.84, "text": " So this is a whole research area of its own.", "tokens": [407, 341, 307, 257, 1379, 2132, 1859, 295, 1080, 1065, 13], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 280, "seek": 96364, "start": 985.84, "end": 988.88, "text": " And I encourage you to check out the resources at the bottom if you're interested in learning", "tokens": [400, 286, 5373, 291, 281, 1520, 484, 264, 3593, 412, 264, 2767, 498, 291, 434, 3102, 294, 2539], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 281, "seek": 96364, "start": 988.88, "end": 989.88, "text": " more.", "tokens": [544, 13], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 282, "seek": 96364, "start": 989.88, "end": 993.48, "text": " But right now, the most important thing to understand is the entity linking is what is", "tokens": [583, 558, 586, 11, 264, 881, 1021, 551, 281, 1223, 307, 264, 13977, 25775, 307, 437, 307], "temperature": 0.0, "avg_logprob": -0.16194883982340494, "compression_ratio": 1.8125, "no_speech_prob": 1.3006652807234786e-05}, {"id": 283, "seek": 99348, "start": 993.48, "end": 996.96, "text": " going to tell us which entity embeddings are actually relevant to the text and which", "tokens": [516, 281, 980, 505, 597, 13977, 12240, 29432, 366, 767, 7340, 281, 264, 2487, 293, 597], "temperature": 0.0, "avg_logprob": -0.2119762875618191, "compression_ratio": 1.7183673469387755, "no_speech_prob": 2.9307457225513645e-05}, {"id": 284, "seek": 99348, "start": 996.96, "end": 1001.6, "text": " ones do you want to use as you iterate through a sequence?", "tokens": [2306, 360, 291, 528, 281, 764, 382, 291, 44497, 807, 257, 8310, 30], "temperature": 0.0, "avg_logprob": -0.2119762875618191, "compression_ratio": 1.7183673469387755, "no_speech_prob": 2.9307457225513645e-05}, {"id": 285, "seek": 99348, "start": 1001.6, "end": 1006.04, "text": " There are a few questions around here.", "tokens": [821, 366, 257, 1326, 1651, 926, 510, 13], "temperature": 0.0, "avg_logprob": -0.2119762875618191, "compression_ratio": 1.7183673469387755, "no_speech_prob": 2.9307457225513645e-05}, {"id": 286, "seek": 99348, "start": 1006.04, "end": 1011.52, "text": " One of them is, so that's entity linking, but what about the relations?", "tokens": [1485, 295, 552, 307, 11, 370, 300, 311, 13977, 25775, 11, 457, 437, 466, 264, 2299, 30], "temperature": 0.0, "avg_logprob": -0.2119762875618191, "compression_ratio": 1.7183673469387755, "no_speech_prob": 2.9307457225513645e-05}, {"id": 287, "seek": 99348, "start": 1011.52, "end": 1013.28, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2119762875618191, "compression_ratio": 1.7183673469387755, "no_speech_prob": 2.9307457225513645e-05}, {"id": 288, "seek": 99348, "start": 1013.28, "end": 1017.32, "text": " So some of the works we'll talk about will only use the entity embeddings.", "tokens": [407, 512, 295, 264, 1985, 321, 603, 751, 466, 486, 787, 764, 264, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.2119762875618191, "compression_ratio": 1.7183673469387755, "no_speech_prob": 2.9307457225513645e-05}, {"id": 289, "seek": 99348, "start": 1017.32, "end": 1021.04, "text": " So some of these have been pre-trained with relation information, but in the end, you", "tokens": [407, 512, 295, 613, 362, 668, 659, 12, 17227, 2001, 365, 9721, 1589, 11, 457, 294, 264, 917, 11, 291], "temperature": 0.0, "avg_logprob": -0.2119762875618191, "compression_ratio": 1.7183673469387755, "no_speech_prob": 2.9307457225513645e-05}, {"id": 290, "seek": 102104, "start": 1021.04, "end": 1024.0, "text": " only have an entity embedding.", "tokens": [787, 362, 364, 13977, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.26277802671704975, "compression_ratio": 1.6984732824427482, "no_speech_prob": 2.5865258066914976e-05}, {"id": 291, "seek": 102104, "start": 1024.0, "end": 1027.08, "text": " So relation extraction is yet another NLP task that you could also do.", "tokens": [407, 9721, 30197, 307, 1939, 1071, 426, 45196, 5633, 300, 291, 727, 611, 360, 13], "temperature": 0.0, "avg_logprob": -0.26277802671704975, "compression_ratio": 1.6984732824427482, "no_speech_prob": 2.5865258066914976e-05}, {"id": 292, "seek": 102104, "start": 1027.08, "end": 1029.36, "text": " But yeah, here we're just talking about entity linking.", "tokens": [583, 1338, 11, 510, 321, 434, 445, 1417, 466, 13977, 25775, 13], "temperature": 0.0, "avg_logprob": -0.26277802671704975, "compression_ratio": 1.6984732824427482, "no_speech_prob": 2.5865258066914976e-05}, {"id": 293, "seek": 102104, "start": 1029.36, "end": 1034.72, "text": " But then if you have the knowledge graph you showed earlier, it had relations in it, right?", "tokens": [583, 550, 498, 291, 362, 264, 3601, 4295, 291, 4712, 3071, 11, 309, 632, 2299, 294, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.26277802671704975, "compression_ratio": 1.6984732824427482, "no_speech_prob": 2.5865258066914976e-05}, {"id": 294, "seek": 102104, "start": 1034.72, "end": 1040.44, "text": " Do you get any connection between that and the text?", "tokens": [1144, 291, 483, 604, 4984, 1296, 300, 293, 264, 2487, 30], "temperature": 0.0, "avg_logprob": -0.26277802671704975, "compression_ratio": 1.6984732824427482, "no_speech_prob": 2.5865258066914976e-05}, {"id": 295, "seek": 102104, "start": 1040.44, "end": 1043.36, "text": " I mean, that's the goal of relation extraction, right?", "tokens": [286, 914, 11, 300, 311, 264, 3387, 295, 9721, 30197, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.26277802671704975, "compression_ratio": 1.6984732824427482, "no_speech_prob": 2.5865258066914976e-05}, {"id": 296, "seek": 102104, "start": 1043.36, "end": 1047.1599999999999, "text": " Just to figure out, like given the entities, what is relation between them, which would", "tokens": [1449, 281, 2573, 484, 11, 411, 2212, 264, 16667, 11, 437, 307, 9721, 1296, 552, 11, 597, 576], "temperature": 0.0, "avg_logprob": -0.26277802671704975, "compression_ratio": 1.6984732824427482, "no_speech_prob": 2.5865258066914976e-05}, {"id": 297, "seek": 104716, "start": 1047.16, "end": 1053.8400000000001, "text": " then form the full triple of tail entity and relation?", "tokens": [550, 1254, 264, 1577, 15508, 295, 6838, 13977, 293, 9721, 30], "temperature": 0.0, "avg_logprob": -0.3614931301194794, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.695792525424622e-05}, {"id": 298, "seek": 104716, "start": 1053.8400000000001, "end": 1054.8400000000001, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3614931301194794, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.695792525424622e-05}, {"id": 299, "seek": 104716, "start": 1054.8400000000001, "end": 1061.1200000000001, "text": " Then I think people want to know more about how it's going to be used, but maybe you should", "tokens": [1396, 286, 519, 561, 528, 281, 458, 544, 466, 577, 309, 311, 516, 281, 312, 1143, 11, 457, 1310, 291, 820], "temperature": 0.0, "avg_logprob": -0.3614931301194794, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.695792525424622e-05}, {"id": 300, "seek": 104716, "start": 1061.1200000000001, "end": 1063.1200000000001, "text": " go on and show some examples.", "tokens": [352, 322, 293, 855, 512, 5110, 13], "temperature": 0.0, "avg_logprob": -0.3614931301194794, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.695792525424622e-05}, {"id": 301, "seek": 104716, "start": 1063.1200000000001, "end": 1065.52, "text": " Yeah, I will, for sure.", "tokens": [865, 11, 286, 486, 11, 337, 988, 13], "temperature": 0.0, "avg_logprob": -0.3614931301194794, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.695792525424622e-05}, {"id": 302, "seek": 104716, "start": 1065.52, "end": 1066.76, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3614931301194794, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.695792525424622e-05}, {"id": 303, "seek": 104716, "start": 1066.76, "end": 1067.76, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.3614931301194794, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.695792525424622e-05}, {"id": 304, "seek": 104716, "start": 1067.76, "end": 1073.72, "text": " So entity embeddings, just to summarize, they're like word embeddings, but they're for entities", "tokens": [407, 13977, 12240, 29432, 11, 445, 281, 20858, 11, 436, 434, 411, 1349, 12240, 29432, 11, 457, 436, 434, 337, 16667], "temperature": 0.0, "avg_logprob": -0.3614931301194794, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.695792525424622e-05}, {"id": 305, "seek": 104716, "start": 1073.72, "end": 1074.72, "text": " and analogies.", "tokens": [293, 16660, 530, 13], "temperature": 0.0, "avg_logprob": -0.3614931301194794, "compression_ratio": 1.5348837209302326, "no_speech_prob": 1.695792525424622e-05}, {"id": 306, "seek": 107472, "start": 1074.72, "end": 1078.56, "text": " So you'll have some vector associated with George Washington, and it should be meaningful", "tokens": [407, 291, 603, 362, 512, 8062, 6615, 365, 7136, 6149, 11, 293, 309, 820, 312, 10995], "temperature": 0.0, "avg_logprob": -0.1494992044236925, "compression_ratio": 1.980891719745223, "no_speech_prob": 2.586587106634397e-05}, {"id": 307, "seek": 107472, "start": 1078.56, "end": 1082.88, "text": " in embedding space such that maybe the George Washington vector is close to the vectors", "tokens": [294, 12240, 3584, 1901, 1270, 300, 1310, 264, 7136, 6149, 8062, 307, 1998, 281, 264, 18875], "temperature": 0.0, "avg_logprob": -0.1494992044236925, "compression_ratio": 1.980891719745223, "no_speech_prob": 2.586587106634397e-05}, {"id": 308, "seek": 107472, "start": 1082.88, "end": 1085.76, "text": " for other founding fathers.", "tokens": [337, 661, 22223, 23450, 13], "temperature": 0.0, "avg_logprob": -0.1494992044236925, "compression_ratio": 1.980891719745223, "no_speech_prob": 2.586587106634397e-05}, {"id": 309, "seek": 107472, "start": 1085.76, "end": 1089.4, "text": " So we're going to briefly talk about some methods for training entity embeddings.", "tokens": [407, 321, 434, 516, 281, 10515, 751, 466, 512, 7150, 337, 3097, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.1494992044236925, "compression_ratio": 1.980891719745223, "no_speech_prob": 2.586587106634397e-05}, {"id": 310, "seek": 107472, "start": 1089.4, "end": 1091.16, "text": " There is knowledge graph embedding methods.", "tokens": [821, 307, 3601, 4295, 12240, 3584, 7150, 13], "temperature": 0.0, "avg_logprob": -0.1494992044236925, "compression_ratio": 1.980891719745223, "no_speech_prob": 2.586587106634397e-05}, {"id": 311, "seek": 107472, "start": 1091.16, "end": 1093.2, "text": " You might have heard of the transient embedding method.", "tokens": [509, 1062, 362, 2198, 295, 264, 41998, 12240, 3584, 3170, 13], "temperature": 0.0, "avg_logprob": -0.1494992044236925, "compression_ratio": 1.980891719745223, "no_speech_prob": 2.586587106634397e-05}, {"id": 312, "seek": 107472, "start": 1093.2, "end": 1097.32, "text": " So this starts from the idea of having these knowledge graph triples, and you want to learn", "tokens": [407, 341, 3719, 490, 264, 1558, 295, 1419, 613, 3601, 4295, 1376, 2622, 11, 293, 291, 528, 281, 1466], "temperature": 0.0, "avg_logprob": -0.1494992044236925, "compression_ratio": 1.980891719745223, "no_speech_prob": 2.586587106634397e-05}, {"id": 313, "seek": 107472, "start": 1097.32, "end": 1100.28, "text": " pre-trained entity and pre-trained relation embeddings.", "tokens": [659, 12, 17227, 2001, 13977, 293, 659, 12, 17227, 2001, 9721, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.1494992044236925, "compression_ratio": 1.980891719745223, "no_speech_prob": 2.586587106634397e-05}, {"id": 314, "seek": 107472, "start": 1100.28, "end": 1103.92, "text": " And you want to be the case that the subject embedding and the relation embedding, the", "tokens": [400, 291, 528, 281, 312, 264, 1389, 300, 264, 3983, 12240, 3584, 293, 264, 9721, 12240, 3584, 11, 264], "temperature": 0.0, "avg_logprob": -0.1494992044236925, "compression_ratio": 1.980891719745223, "no_speech_prob": 2.586587106634397e-05}, {"id": 315, "seek": 110392, "start": 1103.92, "end": 1108.1200000000001, "text": " sum of those two, is close to the object embedding and vector space.", "tokens": [2408, 295, 729, 732, 11, 307, 1998, 281, 264, 2657, 12240, 3584, 293, 8062, 1901, 13], "temperature": 0.0, "avg_logprob": -0.18011502453880587, "compression_ratio": 1.7540983606557377, "no_speech_prob": 4.757284477818757e-05}, {"id": 316, "seek": 110392, "start": 1108.1200000000001, "end": 1111.52, "text": " So it's an algorithm to learn that constraint.", "tokens": [407, 309, 311, 364, 9284, 281, 1466, 300, 25534, 13], "temperature": 0.0, "avg_logprob": -0.18011502453880587, "compression_ratio": 1.7540983606557377, "no_speech_prob": 4.757284477818757e-05}, {"id": 317, "seek": 110392, "start": 1111.52, "end": 1113.3200000000002, "text": " There's also word entity co-occurrence methods.", "tokens": [821, 311, 611, 1349, 13977, 598, 12, 905, 14112, 10760, 7150, 13], "temperature": 0.0, "avg_logprob": -0.18011502453880587, "compression_ratio": 1.7540983606557377, "no_speech_prob": 4.757284477818757e-05}, {"id": 318, "seek": 110392, "start": 1113.3200000000002, "end": 1117.52, "text": " So these build off of word to vec, one of them is even called Wikipedia to vec, and the", "tokens": [407, 613, 1322, 766, 295, 1349, 281, 1241, 66, 11, 472, 295, 552, 307, 754, 1219, 28999, 281, 1241, 66, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.18011502453880587, "compression_ratio": 1.7540983606557377, "no_speech_prob": 4.757284477818757e-05}, {"id": 319, "seek": 110392, "start": 1117.52, "end": 1122.16, "text": " idea is given an entity, you want to figure out what words are most likely to co-occur around", "tokens": [1558, 307, 2212, 364, 13977, 11, 291, 528, 281, 2573, 484, 437, 2283, 366, 881, 3700, 281, 598, 12, 905, 14112, 926], "temperature": 0.0, "avg_logprob": -0.18011502453880587, "compression_ratio": 1.7540983606557377, "no_speech_prob": 4.757284477818757e-05}, {"id": 320, "seek": 110392, "start": 1122.16, "end": 1124.3600000000001, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.18011502453880587, "compression_ratio": 1.7540983606557377, "no_speech_prob": 4.757284477818757e-05}, {"id": 321, "seek": 110392, "start": 1124.3600000000001, "end": 1128.76, "text": " And then the last method, or one of the other methods that is common now is actually just", "tokens": [400, 550, 264, 1036, 3170, 11, 420, 472, 295, 264, 661, 7150, 300, 307, 2689, 586, 307, 767, 445], "temperature": 0.0, "avg_logprob": -0.18011502453880587, "compression_ratio": 1.7540983606557377, "no_speech_prob": 4.757284477818757e-05}, {"id": 322, "seek": 110392, "start": 1128.76, "end": 1133.8400000000001, "text": " using the transformer to learn representations of an entity by encoding the entity description.", "tokens": [1228, 264, 31782, 281, 1466, 33358, 295, 364, 13977, 538, 43430, 264, 13977, 3855, 13], "temperature": 0.0, "avg_logprob": -0.18011502453880587, "compression_ratio": 1.7540983606557377, "no_speech_prob": 4.757284477818757e-05}, {"id": 323, "seek": 113384, "start": 1133.84, "end": 1138.0, "text": " And so Blink from Facebook is an approach that does this.", "tokens": [400, 370, 2177, 475, 490, 4384, 307, 364, 3109, 300, 775, 341, 13], "temperature": 0.0, "avg_logprob": -0.16226981297012202, "compression_ratio": 1.8615384615384616, "no_speech_prob": 5.561274156207219e-05}, {"id": 324, "seek": 113384, "start": 1138.0, "end": 1141.84, "text": " So the methods we'll talk about today are actually agnostic to how you train your pre-trained", "tokens": [407, 264, 7150, 321, 603, 751, 466, 965, 366, 767, 623, 77, 19634, 281, 577, 291, 3847, 428, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.16226981297012202, "compression_ratio": 1.8615384615384616, "no_speech_prob": 5.561274156207219e-05}, {"id": 325, "seek": 113384, "start": 1141.84, "end": 1142.84, "text": " entity embedding.", "tokens": [13977, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.16226981297012202, "compression_ratio": 1.8615384615384616, "no_speech_prob": 5.561274156207219e-05}, {"id": 326, "seek": 113384, "start": 1142.84, "end": 1146.76, "text": " But I think it's important to know that there's actually a wide variety of methods to train", "tokens": [583, 286, 519, 309, 311, 1021, 281, 458, 300, 456, 311, 767, 257, 4874, 5673, 295, 7150, 281, 3847], "temperature": 0.0, "avg_logprob": -0.16226981297012202, "compression_ratio": 1.8615384615384616, "no_speech_prob": 5.561274156207219e-05}, {"id": 327, "seek": 113384, "start": 1146.76, "end": 1148.72, "text": " these pre-trained entity embeddings.", "tokens": [613, 659, 12, 17227, 2001, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.16226981297012202, "compression_ratio": 1.8615384615384616, "no_speech_prob": 5.561274156207219e-05}, {"id": 328, "seek": 113384, "start": 1148.72, "end": 1153.08, "text": " And it's actually not clear which method is best for using them downstream and language", "tokens": [400, 309, 311, 767, 406, 1850, 597, 3170, 307, 1151, 337, 1228, 552, 30621, 293, 2856], "temperature": 0.0, "avg_logprob": -0.16226981297012202, "compression_ratio": 1.8615384615384616, "no_speech_prob": 5.561274156207219e-05}, {"id": 329, "seek": 113384, "start": 1153.08, "end": 1154.56, "text": " models.", "tokens": [5245, 13], "temperature": 0.0, "avg_logprob": -0.16226981297012202, "compression_ratio": 1.8615384615384616, "no_speech_prob": 5.561274156207219e-05}, {"id": 330, "seek": 113384, "start": 1154.56, "end": 1160.1599999999999, "text": " So one of the key challenges that using pre-trained entity embeddings and language models", "tokens": [407, 472, 295, 264, 2141, 4759, 300, 1228, 659, 12, 17227, 2001, 13977, 12240, 29432, 293, 2856, 5245], "temperature": 0.0, "avg_logprob": -0.16226981297012202, "compression_ratio": 1.8615384615384616, "no_speech_prob": 5.561274156207219e-05}, {"id": 331, "seek": 116016, "start": 1160.16, "end": 1164.0400000000002, "text": " is figuring out how to incorporate them when they're from a different embedding space than", "tokens": [307, 15213, 484, 577, 281, 16091, 552, 562, 436, 434, 490, 257, 819, 12240, 3584, 1901, 813], "temperature": 0.0, "avg_logprob": -0.1564847517383191, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.6962872905423865e-05}, {"id": 332, "seek": 116016, "start": 1164.0400000000002, "end": 1166.0800000000002, "text": " the language model.", "tokens": [264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1564847517383191, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.6962872905423865e-05}, {"id": 333, "seek": 116016, "start": 1166.0800000000002, "end": 1169.96, "text": " And so what we'll do, or the approach we'll look at today, we'll learn a fusion layer", "tokens": [400, 370, 437, 321, 603, 360, 11, 420, 264, 3109, 321, 603, 574, 412, 965, 11, 321, 603, 1466, 257, 23100, 4583], "temperature": 0.0, "avg_logprob": -0.1564847517383191, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.6962872905423865e-05}, {"id": 334, "seek": 116016, "start": 1169.96, "end": 1172.96, "text": " to combine this context and entity information.", "tokens": [281, 10432, 341, 4319, 293, 13977, 1589, 13], "temperature": 0.0, "avg_logprob": -0.1564847517383191, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.6962872905423865e-05}, {"id": 335, "seek": 116016, "start": 1172.96, "end": 1177.4, "text": " So we have entity embeddings and we have the contextualized word embeddings from our language", "tokens": [407, 321, 362, 13977, 12240, 29432, 293, 321, 362, 264, 35526, 1602, 1349, 12240, 29432, 490, 527, 2856], "temperature": 0.0, "avg_logprob": -0.1564847517383191, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.6962872905423865e-05}, {"id": 336, "seek": 116016, "start": 1177.4, "end": 1179.24, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.1564847517383191, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.6962872905423865e-05}, {"id": 337, "seek": 116016, "start": 1179.24, "end": 1185.1200000000001, "text": " So if we take a sequence of text, and we imagine that J indicates the J element in a sequence,", "tokens": [407, 498, 321, 747, 257, 8310, 295, 2487, 11, 293, 321, 3811, 300, 508, 16203, 264, 508, 4478, 294, 257, 8310, 11], "temperature": 0.0, "avg_logprob": -0.1564847517383191, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.6962872905423865e-05}, {"id": 338, "seek": 116016, "start": 1185.1200000000001, "end": 1188.64, "text": " then the challenge here is you want to figure out how do we combine some word embedding", "tokens": [550, 264, 3430, 510, 307, 291, 528, 281, 2573, 484, 577, 360, 321, 10432, 512, 1349, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.1564847517383191, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.6962872905423865e-05}, {"id": 339, "seek": 118864, "start": 1188.64, "end": 1192.6000000000001, "text": " WJ with some aligned entity embedding EK.", "tokens": [343, 41, 365, 512, 17962, 13977, 12240, 3584, 46078, 13], "temperature": 0.0, "avg_logprob": -0.16427478945352197, "compression_ratio": 1.9803921568627452, "no_speech_prob": 4.3314274080330506e-05}, {"id": 340, "seek": 118864, "start": 1192.6000000000001, "end": 1197.8000000000002, "text": " So here an alignment could be like in the example where we had Washington was the first president,", "tokens": [407, 510, 364, 18515, 727, 312, 411, 294, 264, 1365, 689, 321, 632, 6149, 390, 264, 700, 3868, 11], "temperature": 0.0, "avg_logprob": -0.16427478945352197, "compression_ratio": 1.9803921568627452, "no_speech_prob": 4.3314274080330506e-05}, {"id": 341, "seek": 118864, "start": 1197.8000000000002, "end": 1201.72, "text": " Washington would be your word embedding and George Washington would be the aligned entity", "tokens": [6149, 576, 312, 428, 1349, 12240, 3584, 293, 7136, 6149, 576, 312, 264, 17962, 13977], "temperature": 0.0, "avg_logprob": -0.16427478945352197, "compression_ratio": 1.9803921568627452, "no_speech_prob": 4.3314274080330506e-05}, {"id": 342, "seek": 118864, "start": 1201.72, "end": 1202.72, "text": " embedding there.", "tokens": [12240, 3584, 456, 13], "temperature": 0.0, "avg_logprob": -0.16427478945352197, "compression_ratio": 1.9803921568627452, "no_speech_prob": 4.3314274080330506e-05}, {"id": 343, "seek": 118864, "start": 1202.72, "end": 1208.1200000000001, "text": " So you could imagine, in this case, let's say your WJ is Washington and your EK is your", "tokens": [407, 291, 727, 3811, 11, 294, 341, 1389, 11, 718, 311, 584, 428, 343, 41, 307, 6149, 293, 428, 46078, 307, 428], "temperature": 0.0, "avg_logprob": -0.16427478945352197, "compression_ratio": 1.9803921568627452, "no_speech_prob": 4.3314274080330506e-05}, {"id": 344, "seek": 118864, "start": 1208.1200000000001, "end": 1211.76, "text": " entity embedding for George Washington, and you want to align them together.", "tokens": [13977, 12240, 3584, 337, 7136, 6149, 11, 293, 291, 528, 281, 7975, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.16427478945352197, "compression_ratio": 1.9803921568627452, "no_speech_prob": 4.3314274080330506e-05}, {"id": 345, "seek": 118864, "start": 1211.76, "end": 1218.0800000000002, "text": " So what you can do is learn a weight matrix WT for the text and WE for the entity to project", "tokens": [407, 437, 291, 393, 360, 307, 1466, 257, 3364, 8141, 343, 51, 337, 264, 2487, 293, 15813, 337, 264, 13977, 281, 1716], "temperature": 0.0, "avg_logprob": -0.16427478945352197, "compression_ratio": 1.9803921568627452, "no_speech_prob": 4.3314274080330506e-05}, {"id": 346, "seek": 121808, "start": 1218.08, "end": 1222.96, "text": " these embeddings to the same dimension before you sum them and finally take an activation", "tokens": [613, 12240, 29432, 281, 264, 912, 10139, 949, 291, 2408, 552, 293, 2721, 747, 364, 24433], "temperature": 0.0, "avg_logprob": -0.1080000455989394, "compression_ratio": 1.7566371681415929, "no_speech_prob": 3.590995765989646e-05}, {"id": 347, "seek": 121808, "start": 1222.96, "end": 1225.12, "text": " function over them.", "tokens": [2445, 670, 552, 13], "temperature": 0.0, "avg_logprob": -0.1080000455989394, "compression_ratio": 1.7566371681415929, "no_speech_prob": 3.590995765989646e-05}, {"id": 348, "seek": 121808, "start": 1225.12, "end": 1230.28, "text": " So the idea is that by having some fusion layer mechanism like this, you can actually", "tokens": [407, 264, 1558, 307, 300, 538, 1419, 512, 23100, 4583, 7513, 411, 341, 11, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.1080000455989394, "compression_ratio": 1.7566371681415929, "no_speech_prob": 3.590995765989646e-05}, {"id": 349, "seek": 121808, "start": 1230.28, "end": 1234.6399999999999, "text": " use these entity embeddings and these contextual word embeddings that are in different embedding", "tokens": [764, 613, 13977, 12240, 29432, 293, 613, 35526, 1349, 12240, 29432, 300, 366, 294, 819, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.1080000455989394, "compression_ratio": 1.7566371681415929, "no_speech_prob": 3.590995765989646e-05}, {"id": 350, "seek": 121808, "start": 1234.6399999999999, "end": 1240.76, "text": " spaces and fuse them together to have this single hidden representation for the element", "tokens": [7673, 293, 31328, 552, 1214, 281, 362, 341, 2167, 7633, 10290, 337, 264, 4478], "temperature": 0.0, "avg_logprob": -0.1080000455989394, "compression_ratio": 1.7566371681415929, "no_speech_prob": 3.590995765989646e-05}, {"id": 351, "seek": 121808, "start": 1240.76, "end": 1244.08, "text": " in the sequence.", "tokens": [294, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.1080000455989394, "compression_ratio": 1.7566371681415929, "no_speech_prob": 3.590995765989646e-05}, {"id": 352, "seek": 124408, "start": 1244.08, "end": 1248.36, "text": " So the project will talk about today, I'll have some mechanism either very similar to this", "tokens": [407, 264, 1716, 486, 751, 466, 965, 11, 286, 603, 362, 512, 7513, 2139, 588, 2531, 281, 341], "temperature": 0.0, "avg_logprob": -0.17833781936793652, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.972443876904435e-06}, {"id": 353, "seek": 124408, "start": 1248.36, "end": 1255.84, "text": " or some variation of this to do this combination of a context and entity information.", "tokens": [420, 512, 12990, 295, 341, 281, 360, 341, 6562, 295, 257, 4319, 293, 13977, 1589, 13], "temperature": 0.0, "avg_logprob": -0.17833781936793652, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.972443876904435e-06}, {"id": 354, "seek": 124408, "start": 1255.84, "end": 1259.84, "text": " So the first approach we're going to talk about is called Ernie, enhanced language representation", "tokens": [407, 264, 700, 3109, 321, 434, 516, 281, 751, 466, 307, 1219, 3300, 2766, 11, 21191, 2856, 10290], "temperature": 0.0, "avg_logprob": -0.17833781936793652, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.972443876904435e-06}, {"id": 355, "seek": 124408, "start": 1259.84, "end": 1261.6, "text": " with informative entities.", "tokens": [365, 27759, 16667, 13], "temperature": 0.0, "avg_logprob": -0.17833781936793652, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.972443876904435e-06}, {"id": 356, "seek": 124408, "start": 1261.6, "end": 1263.72, "text": " And so this just builds on what we've already talked about.", "tokens": [400, 370, 341, 445, 15182, 322, 437, 321, 600, 1217, 2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.17833781936793652, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.972443876904435e-06}, {"id": 357, "seek": 124408, "start": 1263.72, "end": 1269.56, "text": " It uses pre-trained entity embeddings and it also uses this notion of a fusion layer.", "tokens": [467, 4960, 659, 12, 17227, 2001, 13977, 12240, 29432, 293, 309, 611, 4960, 341, 10710, 295, 257, 23100, 4583, 13], "temperature": 0.0, "avg_logprob": -0.17833781936793652, "compression_ratio": 1.7061068702290076, "no_speech_prob": 9.972443876904435e-06}, {"id": 358, "seek": 126956, "start": 1269.56, "end": 1274.6799999999998, "text": " So the first block in Ernie is a text encoder, which is a multi-layer bi-directional transformer", "tokens": [407, 264, 700, 3461, 294, 3300, 2766, 307, 257, 2487, 2058, 19866, 11, 597, 307, 257, 4825, 12, 8376, 260, 3228, 12, 18267, 41048, 31782], "temperature": 0.0, "avg_logprob": -0.15235904111700543, "compression_ratio": 1.7063197026022305, "no_speech_prob": 1.0782880963233765e-05}, {"id": 359, "seek": 126956, "start": 1274.6799999999998, "end": 1280.44, "text": " encoder for the experiments they use BERT, but it doesn't have to be BERT.", "tokens": [2058, 19866, 337, 264, 12050, 436, 764, 363, 31479, 11, 457, 309, 1177, 380, 362, 281, 312, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.15235904111700543, "compression_ratio": 1.7063197026022305, "no_speech_prob": 1.0782880963233765e-05}, {"id": 360, "seek": 126956, "start": 1280.44, "end": 1285.04, "text": " And this is followed by a knowledge encoder which has stacked blocks composed of two multi-headed", "tokens": [400, 341, 307, 6263, 538, 257, 3601, 2058, 19866, 597, 575, 28867, 8474, 18204, 295, 732, 4825, 12, 28409], "temperature": 0.0, "avg_logprob": -0.15235904111700543, "compression_ratio": 1.7063197026022305, "no_speech_prob": 1.0782880963233765e-05}, {"id": 361, "seek": 126956, "start": 1285.04, "end": 1286.36, "text": " attentions.", "tokens": [30980, 626, 13], "temperature": 0.0, "avg_logprob": -0.15235904111700543, "compression_ratio": 1.7063197026022305, "no_speech_prob": 1.0782880963233765e-05}, {"id": 362, "seek": 126956, "start": 1286.36, "end": 1291.9199999999998, "text": " One is over the entity embeddings and one is over your token or subword embeddings.", "tokens": [1485, 307, 670, 264, 13977, 12240, 29432, 293, 472, 307, 670, 428, 14862, 420, 1422, 7462, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.15235904111700543, "compression_ratio": 1.7063197026022305, "no_speech_prob": 1.0782880963233765e-05}, {"id": 363, "seek": 126956, "start": 1291.9199999999998, "end": 1295.8, "text": " And then the output of these contextualized entity and token embeddings from the multi-headed", "tokens": [400, 550, 264, 5598, 295, 613, 35526, 1602, 13977, 293, 14862, 12240, 29432, 490, 264, 4825, 12, 28409], "temperature": 0.0, "avg_logprob": -0.15235904111700543, "compression_ratio": 1.7063197026022305, "no_speech_prob": 1.0782880963233765e-05}, {"id": 364, "seek": 129580, "start": 1295.8, "end": 1300.8, "text": " attentions are passed to a fusion layer, which looks very similar to what we just looked", "tokens": [30980, 626, 366, 4678, 281, 257, 23100, 4583, 11, 597, 1542, 588, 2531, 281, 437, 321, 445, 2956], "temperature": 0.0, "avg_logprob": -0.162870113666241, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.373450817889534e-05}, {"id": 365, "seek": 129580, "start": 1300.8, "end": 1302.12, "text": " at.", "tokens": [412, 13], "temperature": 0.0, "avg_logprob": -0.162870113666241, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.373450817889534e-05}, {"id": 366, "seek": 129580, "start": 1302.12, "end": 1307.36, "text": " But now you also have new word and entity embeddings that you're producing as output of your fusion", "tokens": [583, 586, 291, 611, 362, 777, 1349, 293, 13977, 12240, 29432, 300, 291, 434, 10501, 382, 5598, 295, 428, 23100], "temperature": 0.0, "avg_logprob": -0.162870113666241, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.373450817889534e-05}, {"id": 367, "seek": 129580, "start": 1307.36, "end": 1308.36, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.162870113666241, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.373450817889534e-05}, {"id": 368, "seek": 129580, "start": 1308.36, "end": 1315.8799999999999, "text": " So you see this WJ and this EK, which are produced as the next layer of word and entity embeddings.", "tokens": [407, 291, 536, 341, 343, 41, 293, 341, 46078, 11, 597, 366, 7126, 382, 264, 958, 4583, 295, 1349, 293, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.162870113666241, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.373450817889534e-05}, {"id": 369, "seek": 129580, "start": 1315.8799999999999, "end": 1320.48, "text": " So the I here indicates that it's the I block in the knowledge encoder.", "tokens": [407, 264, 286, 510, 16203, 300, 309, 311, 264, 286, 3461, 294, 264, 3601, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.162870113666241, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.373450817889534e-05}, {"id": 370, "seek": 129580, "start": 1320.48, "end": 1324.3999999999999, "text": " So you'll actually have multiple stacks of these knowledge encoders and you'll be doing", "tokens": [407, 291, 603, 767, 362, 3866, 30792, 295, 613, 3601, 2058, 378, 433, 293, 291, 603, 312, 884], "temperature": 0.0, "avg_logprob": -0.162870113666241, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.373450817889534e-05}, {"id": 371, "seek": 132440, "start": 1324.4, "end": 1329.24, "text": " a fusion of the word entity embedding, producing new word and entity embeddings and then passing", "tokens": [257, 23100, 295, 264, 1349, 13977, 12240, 3584, 11, 10501, 777, 1349, 293, 13977, 12240, 29432, 293, 550, 8437], "temperature": 0.0, "avg_logprob": -0.16946065621297868, "compression_ratio": 1.9757085020242915, "no_speech_prob": 5.014578164264094e-06}, {"id": 372, "seek": 132440, "start": 1329.24, "end": 1334.96, "text": " this to the next block of the knowledge encoder.", "tokens": [341, 281, 264, 958, 3461, 295, 264, 3601, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.16946065621297868, "compression_ratio": 1.9757085020242915, "no_speech_prob": 5.014578164264094e-06}, {"id": 373, "seek": 132440, "start": 1334.96, "end": 1337.2, "text": " So this is what the architecture diagram looks like.", "tokens": [407, 341, 307, 437, 264, 9482, 10686, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.16946065621297868, "compression_ratio": 1.9757085020242915, "no_speech_prob": 5.014578164264094e-06}, {"id": 374, "seek": 132440, "start": 1337.2, "end": 1342.3200000000002, "text": " On the left side we have the T encoder or the text encoder followed by the K encoder", "tokens": [1282, 264, 1411, 1252, 321, 362, 264, 314, 2058, 19866, 420, 264, 2487, 2058, 19866, 6263, 538, 264, 591, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.16946065621297868, "compression_ratio": 1.9757085020242915, "no_speech_prob": 5.014578164264094e-06}, {"id": 375, "seek": 132440, "start": 1342.3200000000002, "end": 1344.3200000000002, "text": " or the knowledge encoder.", "tokens": [420, 264, 3601, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.16946065621297868, "compression_ratio": 1.9757085020242915, "no_speech_prob": 5.014578164264094e-06}, {"id": 376, "seek": 132440, "start": 1344.3200000000002, "end": 1347.68, "text": " And then on the right side we have a zoomed in version of the your knowledge encoder.", "tokens": [400, 550, 322, 264, 558, 1252, 321, 362, 257, 8863, 292, 294, 3037, 295, 264, 428, 3601, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.16946065621297868, "compression_ratio": 1.9757085020242915, "no_speech_prob": 5.014578164264094e-06}, {"id": 377, "seek": 132440, "start": 1347.68, "end": 1351.44, "text": " So you see the multi-headed attentions over the tokens and orange and then over the entities", "tokens": [407, 291, 536, 264, 4825, 12, 28409, 30980, 626, 670, 264, 22667, 293, 7671, 293, 550, 670, 264, 16667], "temperature": 0.0, "avg_logprob": -0.16946065621297868, "compression_ratio": 1.9757085020242915, "no_speech_prob": 5.014578164264094e-06}, {"id": 378, "seek": 135144, "start": 1351.44, "end": 1356.16, "text": " and yellow and then you have this alignment between the word and entities with the dash", "tokens": [293, 5566, 293, 550, 291, 362, 341, 18515, 1296, 264, 1349, 293, 16667, 365, 264, 8240], "temperature": 0.0, "avg_logprob": -0.15783875620263255, "compression_ratio": 2.0655021834061134, "no_speech_prob": 4.8321795475203544e-05}, {"id": 379, "seek": 135144, "start": 1356.16, "end": 1357.56, "text": " lines.", "tokens": [3876, 13], "temperature": 0.0, "avg_logprob": -0.15783875620263255, "compression_ratio": 2.0655021834061134, "no_speech_prob": 4.8321795475203544e-05}, {"id": 380, "seek": 135144, "start": 1357.56, "end": 1362.3200000000002, "text": " So they have this example as Bob Dylan wrote blowing in the wind in 1962.", "tokens": [407, 436, 362, 341, 1365, 382, 6085, 28160, 4114, 15068, 294, 264, 2468, 294, 39498, 13], "temperature": 0.0, "avg_logprob": -0.15783875620263255, "compression_ratio": 2.0655021834061134, "no_speech_prob": 4.8321795475203544e-05}, {"id": 381, "seek": 135144, "start": 1362.3200000000002, "end": 1366.92, "text": " The entities here are Bob Dylan and blowing in the wind and they have a simple alignment", "tokens": [440, 16667, 510, 366, 6085, 28160, 293, 15068, 294, 264, 2468, 293, 436, 362, 257, 2199, 18515], "temperature": 0.0, "avg_logprob": -0.15783875620263255, "compression_ratio": 2.0655021834061134, "no_speech_prob": 4.8321795475203544e-05}, {"id": 382, "seek": 135144, "start": 1366.92, "end": 1371.44, "text": " rule where you want to align the entity to the first word in the entity phrase.", "tokens": [4978, 689, 291, 528, 281, 7975, 264, 13977, 281, 264, 700, 1349, 294, 264, 13977, 9535, 13], "temperature": 0.0, "avg_logprob": -0.15783875620263255, "compression_ratio": 2.0655021834061134, "no_speech_prob": 4.8321795475203544e-05}, {"id": 383, "seek": 135144, "start": 1371.44, "end": 1373.68, "text": " So you want to align Bob Dylan to Bob.", "tokens": [407, 291, 528, 281, 7975, 6085, 28160, 281, 6085, 13], "temperature": 0.0, "avg_logprob": -0.15783875620263255, "compression_ratio": 2.0655021834061134, "no_speech_prob": 4.8321795475203544e-05}, {"id": 384, "seek": 135144, "start": 1373.68, "end": 1377.6000000000001, "text": " That's what the dash lines trying to indicate and you want to align blowing in the wind", "tokens": [663, 311, 437, 264, 8240, 3876, 1382, 281, 13330, 293, 291, 528, 281, 7975, 15068, 294, 264, 2468], "temperature": 0.0, "avg_logprob": -0.15783875620263255, "compression_ratio": 2.0655021834061134, "no_speech_prob": 4.8321795475203544e-05}, {"id": 385, "seek": 135144, "start": 1377.6000000000001, "end": 1379.2, "text": " to blow.", "tokens": [281, 6327, 13], "temperature": 0.0, "avg_logprob": -0.15783875620263255, "compression_ratio": 2.0655021834061134, "no_speech_prob": 4.8321795475203544e-05}, {"id": 386, "seek": 137920, "start": 1379.2, "end": 1382.64, "text": " So here this already assumes the entity linking has been done and you know your entities", "tokens": [407, 510, 341, 1217, 37808, 264, 13977, 25775, 575, 668, 1096, 293, 291, 458, 428, 16667], "temperature": 0.0, "avg_logprob": -0.20848512649536133, "compression_ratio": 1.8044280442804428, "no_speech_prob": 1.0289162673871033e-05}, {"id": 387, "seek": 137920, "start": 1382.64, "end": 1383.64, "text": " in advance.", "tokens": [294, 7295, 13], "temperature": 0.0, "avg_logprob": -0.20848512649536133, "compression_ratio": 1.8044280442804428, "no_speech_prob": 1.0289162673871033e-05}, {"id": 388, "seek": 137920, "start": 1383.64, "end": 1388.32, "text": " So you can see that the entities are actually input into the model.", "tokens": [407, 291, 393, 536, 300, 264, 16667, 366, 767, 4846, 666, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20848512649536133, "compression_ratio": 1.8044280442804428, "no_speech_prob": 1.0289162673871033e-05}, {"id": 389, "seek": 137920, "start": 1388.32, "end": 1392.04, "text": " So after you have your word and the alignment this goes through the information fusion layer", "tokens": [407, 934, 291, 362, 428, 1349, 293, 264, 18515, 341, 1709, 807, 264, 1589, 23100, 4583], "temperature": 0.0, "avg_logprob": -0.20848512649536133, "compression_ratio": 1.8044280442804428, "no_speech_prob": 1.0289162673871033e-05}, {"id": 390, "seek": 137920, "start": 1392.04, "end": 1396.52, "text": " and this light purple gray color and then finally it produces these new word and entity", "tokens": [293, 341, 1442, 9656, 10855, 2017, 293, 550, 2721, 309, 14725, 613, 777, 1349, 293, 13977], "temperature": 0.0, "avg_logprob": -0.20848512649536133, "compression_ratio": 1.8044280442804428, "no_speech_prob": 1.0289162673871033e-05}, {"id": 391, "seek": 137920, "start": 1396.52, "end": 1398.8, "text": " embeddings is output.", "tokens": [12240, 29432, 307, 5598, 13], "temperature": 0.0, "avg_logprob": -0.20848512649536133, "compression_ratio": 1.8044280442804428, "no_speech_prob": 1.0289162673871033e-05}, {"id": 392, "seek": 137920, "start": 1398.8, "end": 1402.16, "text": " And then remember that you have multiple blocks of these so as we pass into the next", "tokens": [400, 550, 1604, 300, 291, 362, 3866, 8474, 295, 613, 370, 382, 321, 1320, 666, 264, 958], "temperature": 0.0, "avg_logprob": -0.20848512649536133, "compression_ratio": 1.8044280442804428, "no_speech_prob": 1.0289162673871033e-05}, {"id": 393, "seek": 137920, "start": 1402.16, "end": 1406.8, "text": " block of your knowledge encoder.", "tokens": [3461, 295, 428, 3601, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.20848512649536133, "compression_ratio": 1.8044280442804428, "no_speech_prob": 1.0289162673871033e-05}, {"id": 394, "seek": 140680, "start": 1406.8, "end": 1409.6, "text": " So how do you actually train this? It's pretty similar to Bert.", "tokens": [407, 577, 360, 291, 767, 3847, 341, 30, 467, 311, 1238, 2531, 281, 29594, 13], "temperature": 0.0, "avg_logprob": -0.182055049472385, "compression_ratio": 1.6610169491525424, "no_speech_prob": 1.9523900846252218e-05}, {"id": 395, "seek": 140680, "start": 1409.6, "end": 1413.96, "text": " You have a mass language model loss and you have an ex sentence prediction loss.", "tokens": [509, 362, 257, 2758, 2856, 2316, 4470, 293, 291, 362, 364, 454, 8174, 17630, 4470, 13], "temperature": 0.0, "avg_logprob": -0.182055049472385, "compression_ratio": 1.6610169491525424, "no_speech_prob": 1.9523900846252218e-05}, {"id": 396, "seek": 140680, "start": 1413.96, "end": 1419.24, "text": " And they also introduce a knowledge pre-training task which they refer to as the DEA task.", "tokens": [400, 436, 611, 5366, 257, 3601, 659, 12, 17227, 1760, 5633, 597, 436, 2864, 281, 382, 264, 10113, 32, 5633, 13], "temperature": 0.0, "avg_logprob": -0.182055049472385, "compression_ratio": 1.6610169491525424, "no_speech_prob": 1.9523900846252218e-05}, {"id": 397, "seek": 140680, "start": 1419.24, "end": 1425.8799999999999, "text": " It's named after a denoising entity auto encoder from an ICML paper in 2008.", "tokens": [467, 311, 4926, 934, 257, 1441, 78, 3436, 13977, 8399, 2058, 19866, 490, 364, 14360, 12683, 3035, 294, 10389, 13], "temperature": 0.0, "avg_logprob": -0.182055049472385, "compression_ratio": 1.6610169491525424, "no_speech_prob": 1.9523900846252218e-05}, {"id": 398, "seek": 140680, "start": 1425.8799999999999, "end": 1429.24, "text": " And the idea is they're going to randomly mass these token entity alignments.", "tokens": [400, 264, 1558, 307, 436, 434, 516, 281, 16979, 2758, 613, 14862, 13977, 7975, 1117, 13], "temperature": 0.0, "avg_logprob": -0.182055049472385, "compression_ratio": 1.6610169491525424, "no_speech_prob": 1.9523900846252218e-05}, {"id": 399, "seek": 140680, "start": 1429.24, "end": 1433.48, "text": " So the idea that Bob goes to Bob Dylan, they're going to mask that out with some random", "tokens": [407, 264, 1558, 300, 6085, 1709, 281, 6085, 28160, 11, 436, 434, 516, 281, 6094, 300, 484, 365, 512, 4974], "temperature": 0.0, "avg_logprob": -0.182055049472385, "compression_ratio": 1.6610169491525424, "no_speech_prob": 1.9523900846252218e-05}, {"id": 400, "seek": 140680, "start": 1433.48, "end": 1434.48, "text": " percentage.", "tokens": [9668, 13], "temperature": 0.0, "avg_logprob": -0.182055049472385, "compression_ratio": 1.6610169491525424, "no_speech_prob": 1.9523900846252218e-05}, {"id": 401, "seek": 143448, "start": 1434.48, "end": 1438.84, "text": " And then they're going to predict the corresponding entity for a token out of the entities in", "tokens": [400, 550, 436, 434, 516, 281, 6069, 264, 11760, 13977, 337, 257, 14862, 484, 295, 264, 16667, 294], "temperature": 0.0, "avg_logprob": -0.18436985522244884, "compression_ratio": 1.892116182572614, "no_speech_prob": 2.212462277384475e-05}, {"id": 402, "seek": 143448, "start": 1438.84, "end": 1440.84, "text": " the sequence.", "tokens": [264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.18436985522244884, "compression_ratio": 1.892116182572614, "no_speech_prob": 2.212462277384475e-05}, {"id": 403, "seek": 143448, "start": 1440.84, "end": 1442.48, "text": " So this looks like as follows.", "tokens": [407, 341, 1542, 411, 382, 10002, 13], "temperature": 0.0, "avg_logprob": -0.18436985522244884, "compression_ratio": 1.892116182572614, "no_speech_prob": 2.212462277384475e-05}, {"id": 404, "seek": 143448, "start": 1442.48, "end": 1445.16, "text": " The summation is over M entities in the sequence.", "tokens": [440, 28811, 307, 670, 376, 16667, 294, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.18436985522244884, "compression_ratio": 1.892116182572614, "no_speech_prob": 2.212462277384475e-05}, {"id": 405, "seek": 143448, "start": 1445.16, "end": 1449.8, "text": " So this would be over Bob Dylan and blowing in the wind in the previous example.", "tokens": [407, 341, 576, 312, 670, 6085, 28160, 293, 15068, 294, 264, 2468, 294, 264, 3894, 1365, 13], "temperature": 0.0, "avg_logprob": -0.18436985522244884, "compression_ratio": 1.892116182572614, "no_speech_prob": 2.212462277384475e-05}, {"id": 406, "seek": 143448, "start": 1449.8, "end": 1453.96, "text": " And given a particular word, they want to figure out what entities that most likely to", "tokens": [400, 2212, 257, 1729, 1349, 11, 436, 528, 281, 2573, 484, 437, 16667, 300, 881, 3700, 281], "temperature": 0.0, "avg_logprob": -0.18436985522244884, "compression_ratio": 1.892116182572614, "no_speech_prob": 2.212462277384475e-05}, {"id": 407, "seek": 143448, "start": 1453.96, "end": 1455.6, "text": " align to in that sequence.", "tokens": [7975, 281, 294, 300, 8310, 13], "temperature": 0.0, "avg_logprob": -0.18436985522244884, "compression_ratio": 1.892116182572614, "no_speech_prob": 2.212462277384475e-05}, {"id": 408, "seek": 143448, "start": 1455.6, "end": 1461.24, "text": " So does Bob align to Bob Dylan or does Bob align to blowing in the wind?", "tokens": [407, 775, 6085, 7975, 281, 6085, 28160, 420, 775, 6085, 7975, 281, 15068, 294, 264, 2468, 30], "temperature": 0.0, "avg_logprob": -0.18436985522244884, "compression_ratio": 1.892116182572614, "no_speech_prob": 2.212462277384475e-05}, {"id": 409, "seek": 146124, "start": 1461.24, "end": 1464.96, "text": " And their motivation for doing this is that if you don't have this task, all you're ever", "tokens": [400, 641, 12335, 337, 884, 341, 307, 300, 498, 291, 500, 380, 362, 341, 5633, 11, 439, 291, 434, 1562], "temperature": 0.0, "avg_logprob": -0.14113021990574828, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.5688370694988407e-05}, {"id": 410, "seek": 146124, "start": 1464.96, "end": 1468.96, "text": " going to be predicting is a token with the mass language model loss.", "tokens": [516, 281, 312, 32884, 307, 257, 14862, 365, 264, 2758, 2856, 2316, 4470, 13], "temperature": 0.0, "avg_logprob": -0.14113021990574828, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.5688370694988407e-05}, {"id": 411, "seek": 146124, "start": 1468.96, "end": 1473.16, "text": " And you really, to encode knowledge, should also probably be predicting over entities.", "tokens": [400, 291, 534, 11, 281, 2058, 1429, 3601, 11, 820, 611, 1391, 312, 32884, 670, 16667, 13], "temperature": 0.0, "avg_logprob": -0.14113021990574828, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.5688370694988407e-05}, {"id": 412, "seek": 146124, "start": 1473.16, "end": 1478.48, "text": " So by adding this task, they have some kind of task that is actually predicting the entity.", "tokens": [407, 538, 5127, 341, 5633, 11, 436, 362, 512, 733, 295, 5633, 300, 307, 767, 32884, 264, 13977, 13], "temperature": 0.0, "avg_logprob": -0.14113021990574828, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.5688370694988407e-05}, {"id": 413, "seek": 146124, "start": 1478.48, "end": 1483.28, "text": " And they also suggest that this might better fuse the knowledge or the entity and the word", "tokens": [400, 436, 611, 3402, 300, 341, 1062, 1101, 31328, 264, 3601, 420, 264, 13977, 293, 264, 1349], "temperature": 0.0, "avg_logprob": -0.14113021990574828, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.5688370694988407e-05}, {"id": 414, "seek": 146124, "start": 1483.28, "end": 1488.2, "text": " representations than just using the fusion layer.", "tokens": [33358, 813, 445, 1228, 264, 23100, 4583, 13], "temperature": 0.0, "avg_logprob": -0.14113021990574828, "compression_ratio": 1.7932330827067668, "no_speech_prob": 1.5688370694988407e-05}, {"id": 415, "seek": 148820, "start": 1488.2, "end": 1492.3600000000001, "text": " And the final loss is then the summation of the mass language model loss, the next sentence", "tokens": [400, 264, 2572, 4470, 307, 550, 264, 28811, 295, 264, 2758, 2856, 2316, 4470, 11, 264, 958, 8174], "temperature": 0.0, "avg_logprob": -0.20226171362491055, "compression_ratio": 1.916030534351145, "no_speech_prob": 1.9832246834994294e-05}, {"id": 416, "seek": 148820, "start": 1492.3600000000001, "end": 1499.96, "text": " prediction loss, and this DEA knowledge pre-training task loss.", "tokens": [17630, 4470, 11, 293, 341, 10113, 32, 3601, 659, 12, 17227, 1760, 5633, 4470, 13], "temperature": 0.0, "avg_logprob": -0.20226171362491055, "compression_ratio": 1.916030534351145, "no_speech_prob": 1.9832246834994294e-05}, {"id": 417, "seek": 148820, "start": 1499.96, "end": 1503.0800000000002, "text": " So they show that a Blatian experiment that it's actually very important to have this", "tokens": [407, 436, 855, 300, 257, 2177, 267, 952, 5120, 300, 309, 311, 767, 588, 1021, 281, 362, 341], "temperature": 0.0, "avg_logprob": -0.20226171362491055, "compression_ratio": 1.916030534351145, "no_speech_prob": 1.9832246834994294e-05}, {"id": 418, "seek": 148820, "start": 1503.0800000000002, "end": 1504.8400000000001, "text": " knowledge pre-training task.", "tokens": [3601, 659, 12, 17227, 1760, 5633, 13], "temperature": 0.0, "avg_logprob": -0.20226171362491055, "compression_ratio": 1.916030534351145, "no_speech_prob": 1.9832246834994294e-05}, {"id": 419, "seek": 148820, "start": 1504.8400000000001, "end": 1510.48, "text": " So this has Bert on the left most bar, Ernie as the second bar from the left.", "tokens": [407, 341, 575, 29594, 322, 264, 1411, 881, 2159, 11, 3300, 2766, 382, 264, 1150, 2159, 490, 264, 1411, 13], "temperature": 0.0, "avg_logprob": -0.20226171362491055, "compression_ratio": 1.916030534351145, "no_speech_prob": 1.9832246834994294e-05}, {"id": 420, "seek": 148820, "start": 1510.48, "end": 1512.48, "text": " And so that's with all the features of Ernie.", "tokens": [400, 370, 300, 311, 365, 439, 264, 4122, 295, 3300, 2766, 13], "temperature": 0.0, "avg_logprob": -0.20226171362491055, "compression_ratio": 1.916030534351145, "no_speech_prob": 1.9832246834994294e-05}, {"id": 421, "seek": 148820, "start": 1512.48, "end": 1515.96, "text": " And then they try removing the pre-trained entity embeddings and removing this knowledge", "tokens": [400, 550, 436, 853, 12720, 264, 659, 12, 17227, 2001, 13977, 12240, 29432, 293, 12720, 341, 3601], "temperature": 0.0, "avg_logprob": -0.20226171362491055, "compression_ratio": 1.916030534351145, "no_speech_prob": 1.9832246834994294e-05}, {"id": 422, "seek": 148820, "start": 1515.96, "end": 1517.2, "text": " pre-training task.", "tokens": [659, 12, 17227, 1760, 5633, 13], "temperature": 0.0, "avg_logprob": -0.20226171362491055, "compression_ratio": 1.916030534351145, "no_speech_prob": 1.9832246834994294e-05}, {"id": 423, "seek": 151720, "start": 1517.2, "end": 1519.8400000000001, "text": " So you see that Bert performs a worst.", "tokens": [407, 291, 536, 300, 29594, 26213, 257, 5855, 13], "temperature": 0.0, "avg_logprob": -0.14807728824452457, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.50696739385603e-05}, {"id": 424, "seek": 151720, "start": 1519.8400000000001, "end": 1522.4, "text": " This isn't very surprising, and that Ernie performs the best.", "tokens": [639, 1943, 380, 588, 8830, 11, 293, 300, 3300, 2766, 26213, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.14807728824452457, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.50696739385603e-05}, {"id": 425, "seek": 151720, "start": 1522.4, "end": 1526.2, "text": " But what's interesting is that if you remove the entity embeddings or you remove the", "tokens": [583, 437, 311, 1880, 307, 300, 498, 291, 4159, 264, 13977, 12240, 29432, 420, 291, 4159, 264], "temperature": 0.0, "avg_logprob": -0.14807728824452457, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.50696739385603e-05}, {"id": 426, "seek": 151720, "start": 1526.2, "end": 1530.92, "text": " pre-training task, they only do a little better than Bert.", "tokens": [659, 12, 17227, 1760, 5633, 11, 436, 787, 360, 257, 707, 1101, 813, 29594, 13], "temperature": 0.0, "avg_logprob": -0.14807728824452457, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.50696739385603e-05}, {"id": 427, "seek": 151720, "start": 1530.92, "end": 1535.72, "text": " And so it's really necessary to actually use this pre-training task to get the most", "tokens": [400, 370, 309, 311, 534, 4818, 281, 767, 764, 341, 659, 12, 17227, 1760, 5633, 281, 483, 264, 881], "temperature": 0.0, "avg_logprob": -0.14807728824452457, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.50696739385603e-05}, {"id": 428, "seek": 151720, "start": 1535.72, "end": 1541.04, "text": " use to your pre-trained entity embeddings.", "tokens": [764, 281, 428, 659, 12, 17227, 2001, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.14807728824452457, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.50696739385603e-05}, {"id": 429, "seek": 151720, "start": 1541.04, "end": 1544.32, "text": " So some strengths of this work were that they introduced some way to combine entity and", "tokens": [407, 512, 16986, 295, 341, 589, 645, 300, 436, 7268, 512, 636, 281, 10432, 13977, 293], "temperature": 0.0, "avg_logprob": -0.14807728824452457, "compression_ratio": 1.8214285714285714, "no_speech_prob": 2.50696739385603e-05}, {"id": 430, "seek": 154432, "start": 1544.32, "end": 1549.2, "text": " context information through this fusion layer and this knowledge pre-training task.", "tokens": [4319, 1589, 807, 341, 23100, 4583, 293, 341, 3601, 659, 12, 17227, 1760, 5633, 13], "temperature": 0.0, "avg_logprob": -0.1651308059692383, "compression_ratio": 1.76875, "no_speech_prob": 1.5936158888507634e-05}, {"id": 431, "seek": 154432, "start": 1549.2, "end": 1552.76, "text": " And then they also show improved performance on downstream tasks, which we'll come back", "tokens": [400, 550, 436, 611, 855, 9689, 3389, 322, 30621, 9608, 11, 597, 321, 603, 808, 646], "temperature": 0.0, "avg_logprob": -0.1651308059692383, "compression_ratio": 1.76875, "no_speech_prob": 1.5936158888507634e-05}, {"id": 432, "seek": 154432, "start": 1552.76, "end": 1555.6799999999998, "text": " to when we talk about evaluation.", "tokens": [281, 562, 321, 751, 466, 13344, 13], "temperature": 0.0, "avg_logprob": -0.1651308059692383, "compression_ratio": 1.76875, "no_speech_prob": 1.5936158888507634e-05}, {"id": 433, "seek": 154432, "start": 1555.6799999999998, "end": 1558.32, "text": " But of course, there's also some limitations.", "tokens": [583, 295, 1164, 11, 456, 311, 611, 512, 15705, 13], "temperature": 0.0, "avg_logprob": -0.1651308059692383, "compression_ratio": 1.76875, "no_speech_prob": 1.5936158888507634e-05}, {"id": 434, "seek": 154432, "start": 1558.32, "end": 1561.1599999999999, "text": " So it needs text data with the entities annotated as input.", "tokens": [407, 309, 2203, 2487, 1412, 365, 264, 16667, 25339, 770, 382, 4846, 13], "temperature": 0.0, "avg_logprob": -0.1651308059692383, "compression_ratio": 1.76875, "no_speech_prob": 1.5936158888507634e-05}, {"id": 435, "seek": 154432, "start": 1561.1599999999999, "end": 1563.6399999999999, "text": " And this is even true for downstream tasks.", "tokens": [400, 341, 307, 754, 2074, 337, 30621, 9608, 13], "temperature": 0.0, "avg_logprob": -0.1651308059692383, "compression_ratio": 1.76875, "no_speech_prob": 1.5936158888507634e-05}, {"id": 436, "seek": 154432, "start": 1563.6399999999999, "end": 1568.28, "text": " So if you remember on the architecture diagram, we had the entity information actually input", "tokens": [407, 498, 291, 1604, 322, 264, 9482, 10686, 11, 321, 632, 264, 13977, 1589, 767, 4846], "temperature": 0.0, "avg_logprob": -0.1651308059692383, "compression_ratio": 1.76875, "no_speech_prob": 1.5936158888507634e-05}, {"id": 437, "seek": 154432, "start": 1568.28, "end": 1570.24, "text": " into the architecture.", "tokens": [666, 264, 9482, 13], "temperature": 0.0, "avg_logprob": -0.1651308059692383, "compression_ratio": 1.76875, "no_speech_prob": 1.5936158888507634e-05}, {"id": 438, "seek": 154432, "start": 1570.24, "end": 1574.12, "text": " But it's not very realistic that you're necessarily going to have a good entity linker for any", "tokens": [583, 309, 311, 406, 588, 12465, 300, 291, 434, 4725, 516, 281, 362, 257, 665, 13977, 2113, 260, 337, 604], "temperature": 0.0, "avg_logprob": -0.1651308059692383, "compression_ratio": 1.76875, "no_speech_prob": 1.5936158888507634e-05}, {"id": 439, "seek": 157412, "start": 1574.12, "end": 1578.28, "text": " downstream tasks that you want to use Ernie on.", "tokens": [30621, 9608, 300, 291, 528, 281, 764, 3300, 2766, 322, 13], "temperature": 0.0, "avg_logprob": -0.14134596515175524, "compression_ratio": 1.8562300319488818, "no_speech_prob": 1.6699863408575766e-05}, {"id": 440, "seek": 157412, "start": 1578.28, "end": 1581.6399999999999, "text": " And the next challenge is this requires more pre-training of your language model.", "tokens": [400, 264, 958, 3430, 307, 341, 7029, 544, 659, 12, 17227, 1760, 295, 428, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14134596515175524, "compression_ratio": 1.8562300319488818, "no_speech_prob": 1.6699863408575766e-05}, {"id": 441, "seek": 157412, "start": 1581.6399999999999, "end": 1584.9199999999998, "text": " So now you don't just need to pre-train Bert, but you also need to pre-train your knowledge", "tokens": [407, 586, 291, 500, 380, 445, 643, 281, 659, 12, 83, 7146, 29594, 11, 457, 291, 611, 643, 281, 659, 12, 83, 7146, 428, 3601], "temperature": 0.0, "avg_logprob": -0.14134596515175524, "compression_ratio": 1.8562300319488818, "no_speech_prob": 1.6699863408575766e-05}, {"id": 442, "seek": 157412, "start": 1584.9199999999998, "end": 1587.28, "text": " and code around top.", "tokens": [293, 3089, 926, 1192, 13], "temperature": 0.0, "avg_logprob": -0.14134596515175524, "compression_ratio": 1.8562300319488818, "no_speech_prob": 1.6699863408575766e-05}, {"id": 443, "seek": 157412, "start": 1587.28, "end": 1590.2399999999998, "text": " For the first challenge, we're going to actually talk about a work that presents a solution", "tokens": [1171, 264, 700, 3430, 11, 321, 434, 516, 281, 767, 751, 466, 257, 589, 300, 13533, 257, 3827], "temperature": 0.0, "avg_logprob": -0.14134596515175524, "compression_ratio": 1.8562300319488818, "no_speech_prob": 1.6699863408575766e-05}, {"id": 444, "seek": 157412, "start": 1590.2399999999998, "end": 1591.2399999999998, "text": " to address this.", "tokens": [281, 2985, 341, 13], "temperature": 0.0, "avg_logprob": -0.14134596515175524, "compression_ratio": 1.8562300319488818, "no_speech_prob": 1.6699863408575766e-05}, {"id": 445, "seek": 157412, "start": 1591.2399999999998, "end": 1595.3999999999999, "text": " For the second challenge, I encourage you to check out the footnote on the bottom.", "tokens": [1171, 264, 1150, 3430, 11, 286, 5373, 291, 281, 1520, 484, 264, 2671, 22178, 322, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.14134596515175524, "compression_ratio": 1.8562300319488818, "no_speech_prob": 1.6699863408575766e-05}, {"id": 446, "seek": 157412, "start": 1595.3999999999999, "end": 1600.2399999999998, "text": " This introduces a work that actually uses pre-trained entity embeddings, uses them in a language", "tokens": [639, 31472, 257, 589, 300, 767, 4960, 659, 12, 17227, 2001, 13977, 12240, 29432, 11, 4960, 552, 294, 257, 2856], "temperature": 0.0, "avg_logprob": -0.14134596515175524, "compression_ratio": 1.8562300319488818, "no_speech_prob": 1.6699863408575766e-05}, {"id": 447, "seek": 157412, "start": 1600.2399999999998, "end": 1602.28, "text": " model, and doesn't require any more pre-training.", "tokens": [2316, 11, 293, 1177, 380, 3651, 604, 544, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.14134596515175524, "compression_ratio": 1.8562300319488818, "no_speech_prob": 1.6699863408575766e-05}, {"id": 448, "seek": 160228, "start": 1602.28, "end": 1605.8, "text": " So it's pretty cool.", "tokens": [407, 309, 311, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.2063132889416753, "compression_ratio": 1.7853658536585366, "no_speech_prob": 7.362988981185481e-05}, {"id": 449, "seek": 160228, "start": 1605.8, "end": 1610.3999999999999, "text": " I guess that's all I have for Ernie, so I want to pause here for questions.", "tokens": [286, 2041, 300, 311, 439, 286, 362, 337, 3300, 2766, 11, 370, 286, 528, 281, 10465, 510, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2063132889416753, "compression_ratio": 1.7853658536585366, "no_speech_prob": 7.362988981185481e-05}, {"id": 450, "seek": 160228, "start": 1610.3999999999999, "end": 1615.3999999999999, "text": " Well, here's one that's up here.", "tokens": [1042, 11, 510, 311, 472, 300, 311, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.2063132889416753, "compression_ratio": 1.7853658536585366, "no_speech_prob": 7.362988981185481e-05}, {"id": 451, "seek": 160228, "start": 1615.3999999999999, "end": 1621.52, "text": " So on the fusion layer, it observed that passing the entity embedding into a fusion layer", "tokens": [407, 322, 264, 23100, 4583, 11, 309, 13095, 300, 8437, 264, 13977, 12240, 3584, 666, 257, 23100, 4583], "temperature": 0.0, "avg_logprob": -0.2063132889416753, "compression_ratio": 1.7853658536585366, "no_speech_prob": 7.362988981185481e-05}, {"id": 452, "seek": 160228, "start": 1621.52, "end": 1625.68, "text": " to provide with the word embedding is more powerful than just concatenating the entity", "tokens": [281, 2893, 365, 264, 1349, 12240, 3584, 307, 544, 4005, 813, 445, 1588, 7186, 990, 264, 13977], "temperature": 0.0, "avg_logprob": -0.2063132889416753, "compression_ratio": 1.7853658536585366, "no_speech_prob": 7.362988981185481e-05}, {"id": 453, "seek": 160228, "start": 1625.68, "end": 1628.8, "text": " embedding onto the end of the word embedding question mark.", "tokens": [12240, 3584, 3911, 264, 917, 295, 264, 1349, 12240, 3584, 1168, 1491, 13], "temperature": 0.0, "avg_logprob": -0.2063132889416753, "compression_ratio": 1.7853658536585366, "no_speech_prob": 7.362988981185481e-05}, {"id": 454, "seek": 162880, "start": 1628.8, "end": 1634.6399999999999, "text": " Yeah, so I guess people are still a little bit confused as to the motivation of that fusion", "tokens": [865, 11, 370, 286, 2041, 561, 366, 920, 257, 707, 857, 9019, 382, 281, 264, 12335, 295, 300, 23100], "temperature": 0.0, "avg_logprob": -0.24962696939144494, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.00010378625302109867}, {"id": 455, "seek": 162880, "start": 1634.6399999999999, "end": 1635.6399999999999, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.24962696939144494, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.00010378625302109867}, {"id": 456, "seek": 162880, "start": 1635.6399999999999, "end": 1640.84, "text": " And so I guess here it's this, the simple strategy would be since you've got the entity", "tokens": [400, 370, 286, 2041, 510, 309, 311, 341, 11, 264, 2199, 5206, 576, 312, 1670, 291, 600, 658, 264, 13977], "temperature": 0.0, "avg_logprob": -0.24962696939144494, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.00010378625302109867}, {"id": 457, "seek": 162880, "start": 1640.84, "end": 1645.8799999999999, "text": " linking, you could just concatenate entity embeddings onto the end of word embeddings and", "tokens": [25775, 11, 291, 727, 445, 1588, 7186, 473, 13977, 12240, 29432, 3911, 264, 917, 295, 1349, 12240, 29432, 293], "temperature": 0.0, "avg_logprob": -0.24962696939144494, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.00010378625302109867}, {"id": 458, "seek": 162880, "start": 1645.8799999999999, "end": 1650.08, "text": " do regular Bert, but that worked just as well.", "tokens": [360, 3890, 29594, 11, 457, 300, 2732, 445, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.24962696939144494, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.00010378625302109867}, {"id": 459, "seek": 162880, "start": 1650.08, "end": 1657.68, "text": " I think the idea is it would not, because if you imagine that, let's say, your magnitude", "tokens": [286, 519, 264, 1558, 307, 309, 576, 406, 11, 570, 498, 291, 3811, 300, 11, 718, 311, 584, 11, 428, 15668], "temperature": 0.0, "avg_logprob": -0.24962696939144494, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.00010378625302109867}, {"id": 460, "seek": 165768, "start": 1657.68, "end": 1664.28, "text": " is very different, you need some way to, I guess, align the spaces so that anything meaningful", "tokens": [307, 588, 819, 11, 291, 643, 512, 636, 281, 11, 286, 2041, 11, 7975, 264, 7673, 370, 300, 1340, 10995], "temperature": 0.0, "avg_logprob": -0.19090342725444043, "compression_ratio": 1.9181818181818182, "no_speech_prob": 0.00013961779768578708}, {"id": 461, "seek": 165768, "start": 1664.28, "end": 1667.3200000000002, "text": " in the entity embedding space is still meaningful in the word embedding space.", "tokens": [294, 264, 13977, 12240, 3584, 1901, 307, 920, 10995, 294, 264, 1349, 12240, 3584, 1901, 13], "temperature": 0.0, "avg_logprob": -0.19090342725444043, "compression_ratio": 1.9181818181818182, "no_speech_prob": 0.00013961779768578708}, {"id": 462, "seek": 165768, "start": 1667.3200000000002, "end": 1670.8400000000001, "text": " So if you're close in the word embedding space, you also would be, you'd want to be close", "tokens": [407, 498, 291, 434, 1998, 294, 264, 1349, 12240, 3584, 1901, 11, 291, 611, 576, 312, 11, 291, 1116, 528, 281, 312, 1998], "temperature": 0.0, "avg_logprob": -0.19090342725444043, "compression_ratio": 1.9181818181818182, "no_speech_prob": 0.00013961779768578708}, {"id": 463, "seek": 165768, "start": 1670.8400000000001, "end": 1672.5600000000002, "text": " in the entity embedding space.", "tokens": [294, 264, 13977, 12240, 3584, 1901, 13], "temperature": 0.0, "avg_logprob": -0.19090342725444043, "compression_ratio": 1.9181818181818182, "no_speech_prob": 0.00013961779768578708}, {"id": 464, "seek": 165768, "start": 1672.5600000000002, "end": 1675.5600000000002, "text": " So I guess that's one argument.", "tokens": [407, 286, 2041, 300, 311, 472, 6770, 13], "temperature": 0.0, "avg_logprob": -0.19090342725444043, "compression_ratio": 1.9181818181818182, "no_speech_prob": 0.00013961779768578708}, {"id": 465, "seek": 165768, "start": 1675.5600000000002, "end": 1676.5600000000002, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.19090342725444043, "compression_ratio": 1.9181818181818182, "no_speech_prob": 0.00013961779768578708}, {"id": 466, "seek": 165768, "start": 1676.5600000000002, "end": 1685.96, "text": " I mean, I think the question isn't, you know, it's a good question as people say, I mean,", "tokens": [286, 914, 11, 286, 519, 264, 1168, 1943, 380, 11, 291, 458, 11, 309, 311, 257, 665, 1168, 382, 561, 584, 11, 286, 914, 11], "temperature": 0.0, "avg_logprob": -0.19090342725444043, "compression_ratio": 1.9181818181818182, "no_speech_prob": 0.00013961779768578708}, {"id": 467, "seek": 168596, "start": 1685.96, "end": 1690.1200000000001, "text": " it's not completely obvious that it wouldn't work to do that.", "tokens": [309, 311, 406, 2584, 6322, 300, 309, 2759, 380, 589, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.22359481224646935, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00011943004938075319}, {"id": 468, "seek": 168596, "start": 1690.1200000000001, "end": 1696.48, "text": " It seems like one of the potential problems is some words have entity links to them and", "tokens": [467, 2544, 411, 472, 295, 264, 3995, 2740, 307, 512, 2283, 362, 13977, 6123, 281, 552, 293], "temperature": 0.0, "avg_logprob": -0.22359481224646935, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00011943004938075319}, {"id": 469, "seek": 168596, "start": 1696.48, "end": 1698.08, "text": " some words don't.", "tokens": [512, 2283, 500, 380, 13], "temperature": 0.0, "avg_logprob": -0.22359481224646935, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00011943004938075319}, {"id": 470, "seek": 168596, "start": 1698.08, "end": 1702.8, "text": " And so then you'd sort of have zero vectors for the ones that don't have anything linked", "tokens": [400, 370, 550, 291, 1116, 1333, 295, 362, 4018, 18875, 337, 264, 2306, 300, 500, 380, 362, 1340, 9408], "temperature": 0.0, "avg_logprob": -0.22359481224646935, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00011943004938075319}, {"id": 471, "seek": 168596, "start": 1702.8, "end": 1703.8, "text": " in that way.", "tokens": [294, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.22359481224646935, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00011943004938075319}, {"id": 472, "seek": 168596, "start": 1703.8, "end": 1705.92, "text": " Act a bit weirdly, but.", "tokens": [3251, 257, 857, 48931, 11, 457, 13], "temperature": 0.0, "avg_logprob": -0.22359481224646935, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00011943004938075319}, {"id": 473, "seek": 168596, "start": 1705.92, "end": 1706.92, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22359481224646935, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00011943004938075319}, {"id": 474, "seek": 168596, "start": 1706.92, "end": 1712.72, "text": " In this case, when they don't have entities linked, which is a great point.", "tokens": [682, 341, 1389, 11, 562, 436, 500, 380, 362, 16667, 9408, 11, 597, 307, 257, 869, 935, 13], "temperature": 0.0, "avg_logprob": -0.22359481224646935, "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.00011943004938075319}, {"id": 475, "seek": 171272, "start": 1712.72, "end": 1717.64, "text": " Yeah, the first equation just simplifies to the first term plus the bias.", "tokens": [865, 11, 264, 700, 5367, 445, 6883, 11221, 281, 264, 700, 1433, 1804, 264, 12577, 13], "temperature": 0.0, "avg_logprob": -0.36063881380012236, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.5463248675805517e-05}, {"id": 476, "seek": 171272, "start": 1717.64, "end": 1720.92, "text": " So like there's an obvious solution in that case when you're not concatenating that you", "tokens": [407, 411, 456, 311, 364, 6322, 3827, 294, 300, 1389, 562, 291, 434, 406, 1588, 7186, 990, 300, 291], "temperature": 0.0, "avg_logprob": -0.36063881380012236, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.5463248675805517e-05}, {"id": 477, "seek": 171272, "start": 1720.92, "end": 1722.84, "text": " just don't add on the term.", "tokens": [445, 500, 380, 909, 322, 264, 1433, 13], "temperature": 0.0, "avg_logprob": -0.36063881380012236, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.5463248675805517e-05}, {"id": 478, "seek": 171272, "start": 1722.84, "end": 1728.04, "text": " Yeah, that could be one reason too.", "tokens": [865, 11, 300, 727, 312, 472, 1778, 886, 13], "temperature": 0.0, "avg_logprob": -0.36063881380012236, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.5463248675805517e-05}, {"id": 479, "seek": 171272, "start": 1728.04, "end": 1731.04, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.36063881380012236, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.5463248675805517e-05}, {"id": 480, "seek": 171272, "start": 1731.04, "end": 1735.64, "text": " Are there any other questions?", "tokens": [2014, 456, 604, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.36063881380012236, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.5463248675805517e-05}, {"id": 481, "seek": 171272, "start": 1735.64, "end": 1742.64, "text": " I think you can go on.", "tokens": [286, 519, 291, 393, 352, 322, 13], "temperature": 0.0, "avg_logprob": -0.36063881380012236, "compression_ratio": 1.5159574468085106, "no_speech_prob": 2.5463248675805517e-05}, {"id": 482, "seek": 174264, "start": 1742.64, "end": 1743.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.26500425338745115, "compression_ratio": 1.6222222222222222, "no_speech_prob": 5.920502735534683e-05}, {"id": 483, "seek": 174264, "start": 1743.64, "end": 1744.64, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.26500425338745115, "compression_ratio": 1.6222222222222222, "no_speech_prob": 5.920502735534683e-05}, {"id": 484, "seek": 174264, "start": 1744.64, "end": 1745.64, "text": " Right.", "tokens": [1779, 13], "temperature": 0.0, "avg_logprob": -0.26500425338745115, "compression_ratio": 1.6222222222222222, "no_speech_prob": 5.920502735534683e-05}, {"id": 485, "seek": 174264, "start": 1745.64, "end": 1751.72, "text": " So now we're talking about nobert.", "tokens": [407, 586, 321, 434, 1417, 466, 572, 4290, 13], "temperature": 0.0, "avg_logprob": -0.26500425338745115, "compression_ratio": 1.6222222222222222, "no_speech_prob": 5.920502735534683e-05}, {"id": 486, "seek": 174264, "start": 1751.72, "end": 1755.2800000000002, "text": " And this is from the same folks that introduced the Elmo work.", "tokens": [400, 341, 307, 490, 264, 912, 4024, 300, 7268, 264, 38722, 589, 13], "temperature": 0.0, "avg_logprob": -0.26500425338745115, "compression_ratio": 1.6222222222222222, "no_speech_prob": 5.920502735534683e-05}, {"id": 487, "seek": 174264, "start": 1755.2800000000002, "end": 1760.1200000000001, "text": " And the idea here is that they're going to pre-train and integrate into linker as an extension", "tokens": [400, 264, 1558, 510, 307, 300, 436, 434, 516, 281, 659, 12, 83, 7146, 293, 13365, 666, 2113, 260, 382, 364, 10320], "temperature": 0.0, "avg_logprob": -0.26500425338745115, "compression_ratio": 1.6222222222222222, "no_speech_prob": 5.920502735534683e-05}, {"id": 488, "seek": 174264, "start": 1760.1200000000001, "end": 1763.6000000000001, "text": " to bird.", "tokens": [281, 5255, 13], "temperature": 0.0, "avg_logprob": -0.26500425338745115, "compression_ratio": 1.6222222222222222, "no_speech_prob": 5.920502735534683e-05}, {"id": 489, "seek": 174264, "start": 1763.6000000000001, "end": 1768.16, "text": " And so their loss function will now be the summation of the next sentence prediction,", "tokens": [400, 370, 641, 4470, 2445, 486, 586, 312, 264, 28811, 295, 264, 958, 8174, 17630, 11], "temperature": 0.0, "avg_logprob": -0.26500425338745115, "compression_ratio": 1.6222222222222222, "no_speech_prob": 5.920502735534683e-05}, {"id": 490, "seek": 174264, "start": 1768.16, "end": 1770.48, "text": " the mass language model loss and this entity linking loss.", "tokens": [264, 2758, 2856, 2316, 4470, 293, 341, 13977, 25775, 4470, 13], "temperature": 0.0, "avg_logprob": -0.26500425338745115, "compression_ratio": 1.6222222222222222, "no_speech_prob": 5.920502735534683e-05}, {"id": 491, "seek": 177048, "start": 1770.48, "end": 1774.64, "text": " So instead of the knowledge pre-training DEA task from Ernie, we'll have an entity linking", "tokens": [407, 2602, 295, 264, 3601, 659, 12, 17227, 1760, 10113, 32, 5633, 490, 3300, 2766, 11, 321, 603, 362, 364, 13977, 25775], "temperature": 0.0, "avg_logprob": -0.22231177396552507, "compression_ratio": 1.967509025270758, "no_speech_prob": 4.331649688538164e-05}, {"id": 492, "seek": 177048, "start": 1774.64, "end": 1775.92, "text": " loss.", "tokens": [4470, 13], "temperature": 0.0, "avg_logprob": -0.22231177396552507, "compression_ratio": 1.967509025270758, "no_speech_prob": 4.331649688538164e-05}, {"id": 493, "seek": 177048, "start": 1775.92, "end": 1781.2, "text": " And the idea of the entity linker is you'll now have just as normal sequence as input.", "tokens": [400, 264, 1558, 295, 264, 13977, 2113, 260, 307, 291, 603, 586, 362, 445, 382, 2710, 8310, 382, 4846, 13], "temperature": 0.0, "avg_logprob": -0.22231177396552507, "compression_ratio": 1.967509025270758, "no_speech_prob": 4.331649688538164e-05}, {"id": 494, "seek": 177048, "start": 1781.2, "end": 1786.24, "text": " And the integrated entity linker will figure out what are the entities in the sentence and", "tokens": [400, 264, 10919, 13977, 2113, 260, 486, 2573, 484, 437, 366, 264, 16667, 294, 264, 8174, 293], "temperature": 0.0, "avg_logprob": -0.22231177396552507, "compression_ratio": 1.967509025270758, "no_speech_prob": 4.331649688538164e-05}, {"id": 495, "seek": 177048, "start": 1786.24, "end": 1790.0, "text": " or what are the mentions in the sentence or the candidates of those mentions and then", "tokens": [420, 437, 366, 264, 23844, 294, 264, 8174, 420, 264, 11255, 295, 729, 23844, 293, 550], "temperature": 0.0, "avg_logprob": -0.22231177396552507, "compression_ratio": 1.967509025270758, "no_speech_prob": 4.331649688538164e-05}, {"id": 496, "seek": 177048, "start": 1790.0, "end": 1795.4, "text": " what should be the scores those entities or the candidates given the context of the sentence.", "tokens": [437, 820, 312, 264, 13444, 729, 16667, 420, 264, 11255, 2212, 264, 4319, 295, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.22231177396552507, "compression_ratio": 1.967509025270758, "no_speech_prob": 4.331649688538164e-05}, {"id": 497, "seek": 177048, "start": 1795.4, "end": 1799.96, "text": " And so this is all done now as part of the model rather than requiring it as some external", "tokens": [400, 370, 341, 307, 439, 1096, 586, 382, 644, 295, 264, 2316, 2831, 813, 24165, 309, 382, 512, 8320], "temperature": 0.0, "avg_logprob": -0.22231177396552507, "compression_ratio": 1.967509025270758, "no_speech_prob": 4.331649688538164e-05}, {"id": 498, "seek": 179996, "start": 1799.96, "end": 1803.96, "text": " pipeline stage before you could even use Ernie for instance.", "tokens": [15517, 3233, 949, 291, 727, 754, 764, 3300, 2766, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.16494983300230556, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.282605055370368e-05}, {"id": 499, "seek": 179996, "start": 1803.96, "end": 1807.0, "text": " So now for downstream tasks, you no longer need these entity annotations.", "tokens": [407, 586, 337, 30621, 9608, 11, 291, 572, 2854, 643, 613, 13977, 25339, 763, 13], "temperature": 0.0, "avg_logprob": -0.16494983300230556, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.282605055370368e-05}, {"id": 500, "seek": 179996, "start": 1807.0, "end": 1811.16, "text": " Your integrated entity linker will figure out what the correct entity is and be able to", "tokens": [2260, 10919, 13977, 2113, 260, 486, 2573, 484, 437, 264, 3006, 13977, 307, 293, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.16494983300230556, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.282605055370368e-05}, {"id": 501, "seek": 179996, "start": 1811.16, "end": 1814.52, "text": " use the correct entity embedding.", "tokens": [764, 264, 3006, 13977, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.16494983300230556, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.282605055370368e-05}, {"id": 502, "seek": 179996, "start": 1814.52, "end": 1817.8, "text": " So there's also this idea that learning is entity linking may actually better in code", "tokens": [407, 456, 311, 611, 341, 1558, 300, 2539, 307, 13977, 25775, 815, 767, 1101, 294, 3089], "temperature": 0.0, "avg_logprob": -0.16494983300230556, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.282605055370368e-05}, {"id": 503, "seek": 179996, "start": 1817.8, "end": 1822.4, "text": " knowledge than this DEA pre-training task because they show that nobert actually outperforms", "tokens": [3601, 813, 341, 10113, 32, 659, 12, 17227, 1760, 5633, 570, 436, 855, 300, 572, 4290, 767, 484, 26765, 82], "temperature": 0.0, "avg_logprob": -0.16494983300230556, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.282605055370368e-05}, {"id": 504, "seek": 179996, "start": 1822.4, "end": 1825.2, "text": " Ernie on downstream tasks.", "tokens": [3300, 2766, 322, 30621, 9608, 13], "temperature": 0.0, "avg_logprob": -0.16494983300230556, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.282605055370368e-05}, {"id": 505, "seek": 179996, "start": 1825.2, "end": 1829.6000000000001, "text": " So one reason this may occur is that if you think about the DEA task, it's actually a bit", "tokens": [407, 472, 1778, 341, 815, 5160, 307, 300, 498, 291, 519, 466, 264, 10113, 32, 5633, 11, 309, 311, 767, 257, 857], "temperature": 0.0, "avg_logprob": -0.16494983300230556, "compression_ratio": 1.8157894736842106, "no_speech_prob": 2.282605055370368e-05}, {"id": 506, "seek": 182960, "start": 1829.6, "end": 1832.12, "text": " simpler than just entity linking.", "tokens": [18587, 813, 445, 13977, 25775, 13], "temperature": 0.0, "avg_logprob": -0.1844585360461519, "compression_ratio": 1.981818181818182, "no_speech_prob": 1.496995992056327e-05}, {"id": 507, "seek": 182960, "start": 1832.12, "end": 1836.7199999999998, "text": " So you're trying to predict for instance what Bob linked to out of Bob Dylan and blowing", "tokens": [407, 291, 434, 1382, 281, 6069, 337, 5197, 437, 6085, 9408, 281, 484, 295, 6085, 28160, 293, 15068], "temperature": 0.0, "avg_logprob": -0.1844585360461519, "compression_ratio": 1.981818181818182, "no_speech_prob": 1.496995992056327e-05}, {"id": 508, "seek": 182960, "start": 1836.7199999999998, "end": 1837.7199999999998, "text": " in the wind.", "tokens": [294, 264, 2468, 13], "temperature": 0.0, "avg_logprob": -0.1844585360461519, "compression_ratio": 1.981818181818182, "no_speech_prob": 1.496995992056327e-05}, {"id": 509, "seek": 182960, "start": 1837.7199999999998, "end": 1841.6799999999998, "text": " And it's much easier even as a human to see that Bob Dylan will more likely link to or", "tokens": [400, 309, 311, 709, 3571, 754, 382, 257, 1952, 281, 536, 300, 6085, 28160, 486, 544, 3700, 2113, 281, 420], "temperature": 0.0, "avg_logprob": -0.1844585360461519, "compression_ratio": 1.981818181818182, "no_speech_prob": 1.496995992056327e-05}, {"id": 510, "seek": 182960, "start": 1841.6799999999998, "end": 1846.52, "text": " Bob will more likely link to Bob Dylan than that Bob will link to blowing in the wind.", "tokens": [6085, 486, 544, 3700, 2113, 281, 6085, 28160, 813, 300, 6085, 486, 2113, 281, 15068, 294, 264, 2468, 13], "temperature": 0.0, "avg_logprob": -0.1844585360461519, "compression_ratio": 1.981818181818182, "no_speech_prob": 1.496995992056327e-05}, {"id": 511, "seek": 182960, "start": 1846.52, "end": 1849.8, "text": " And the entity linking task, you actually have a much harder set of candidates to predict", "tokens": [400, 264, 13977, 25775, 5633, 11, 291, 767, 362, 257, 709, 6081, 992, 295, 11255, 281, 6069], "temperature": 0.0, "avg_logprob": -0.1844585360461519, "compression_ratio": 1.981818181818182, "no_speech_prob": 1.496995992056327e-05}, {"id": 512, "seek": 182960, "start": 1849.8, "end": 1850.8, "text": " over.", "tokens": [670, 13], "temperature": 0.0, "avg_logprob": -0.1844585360461519, "compression_ratio": 1.981818181818182, "no_speech_prob": 1.496995992056327e-05}, {"id": 513, "seek": 182960, "start": 1850.8, "end": 1852.56, "text": " You're not just looking at the ones in the sentence.", "tokens": [509, 434, 406, 445, 1237, 412, 264, 2306, 294, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.1844585360461519, "compression_ratio": 1.981818181818182, "no_speech_prob": 1.496995992056327e-05}, {"id": 514, "seek": 182960, "start": 1852.56, "end": 1857.1599999999999, "text": " So does Washington link to George Washington or Washington state actually requires you", "tokens": [407, 775, 6149, 2113, 281, 7136, 6149, 420, 6149, 1785, 767, 7029, 291], "temperature": 0.0, "avg_logprob": -0.1844585360461519, "compression_ratio": 1.981818181818182, "no_speech_prob": 1.496995992056327e-05}, {"id": 515, "seek": 185716, "start": 1857.16, "end": 1859.92, "text": " using more information about the entity.", "tokens": [1228, 544, 1589, 466, 264, 13977, 13], "temperature": 0.0, "avg_logprob": -0.13515699270999793, "compression_ratio": 1.8, "no_speech_prob": 1.0782699973788112e-05}, {"id": 516, "seek": 185716, "start": 1859.92, "end": 1864.8000000000002, "text": " So given it's a harder task, it's not too surprising that it might perform better than", "tokens": [407, 2212, 309, 311, 257, 6081, 5633, 11, 309, 311, 406, 886, 8830, 300, 309, 1062, 2042, 1101, 813], "temperature": 0.0, "avg_logprob": -0.13515699270999793, "compression_ratio": 1.8, "no_speech_prob": 1.0782699973788112e-05}, {"id": 517, "seek": 185716, "start": 1864.8000000000002, "end": 1870.24, "text": " just this easier knowledge pre-training task that Ernie introduced.", "tokens": [445, 341, 3571, 3601, 659, 12, 17227, 1760, 5633, 300, 3300, 2766, 7268, 13], "temperature": 0.0, "avg_logprob": -0.13515699270999793, "compression_ratio": 1.8, "no_speech_prob": 1.0782699973788112e-05}, {"id": 518, "seek": 185716, "start": 1870.24, "end": 1872.8400000000001, "text": " So otherwise, nobert has a lot of similarities to Ernie.", "tokens": [407, 5911, 11, 572, 4290, 575, 257, 688, 295, 24197, 281, 3300, 2766, 13], "temperature": 0.0, "avg_logprob": -0.13515699270999793, "compression_ratio": 1.8, "no_speech_prob": 1.0782699973788112e-05}, {"id": 519, "seek": 185716, "start": 1872.8400000000001, "end": 1877.6000000000001, "text": " It uses a fusion layer that combines this context and entity information and it introduces", "tokens": [467, 4960, 257, 23100, 4583, 300, 29520, 341, 4319, 293, 13977, 1589, 293, 309, 31472], "temperature": 0.0, "avg_logprob": -0.13515699270999793, "compression_ratio": 1.8, "no_speech_prob": 1.0782699973788112e-05}, {"id": 520, "seek": 185716, "start": 1877.6000000000001, "end": 1879.8400000000001, "text": " some knowledge pre-training task.", "tokens": [512, 3601, 659, 12, 17227, 1760, 5633, 13], "temperature": 0.0, "avg_logprob": -0.13515699270999793, "compression_ratio": 1.8, "no_speech_prob": 1.0782699973788112e-05}, {"id": 521, "seek": 185716, "start": 1879.8400000000001, "end": 1882.64, "text": " So I'd say a high level takeaways if you want to use pre-training entity embeddings", "tokens": [407, 286, 1116, 584, 257, 1090, 1496, 45584, 498, 291, 528, 281, 764, 659, 12, 17227, 1760, 13977, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.13515699270999793, "compression_ratio": 1.8, "no_speech_prob": 1.0782699973788112e-05}, {"id": 522, "seek": 185716, "start": 1882.64, "end": 1886.64, "text": " in a language model, you'll probably at least want to consider both of these components", "tokens": [294, 257, 2856, 2316, 11, 291, 603, 1391, 412, 1935, 528, 281, 1949, 1293, 295, 613, 6677], "temperature": 0.0, "avg_logprob": -0.13515699270999793, "compression_ratio": 1.8, "no_speech_prob": 1.0782699973788112e-05}, {"id": 523, "seek": 188664, "start": 1886.64, "end": 1891.68, "text": " in terms of actually going to integrate the pre-training entity embeddings and take the", "tokens": [294, 2115, 295, 767, 516, 281, 13365, 264, 659, 12, 17227, 1760, 13977, 12240, 29432, 293, 747, 264], "temperature": 0.0, "avg_logprob": -0.16755173092796688, "compression_ratio": 1.6328125, "no_speech_prob": 4.610751784639433e-05}, {"id": 524, "seek": 188664, "start": 1891.68, "end": 1897.48, "text": " most advantage of a knowledge in them as possible.", "tokens": [881, 5002, 295, 257, 3601, 294, 552, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.16755173092796688, "compression_ratio": 1.6328125, "no_speech_prob": 4.610751784639433e-05}, {"id": 525, "seek": 188664, "start": 1897.48, "end": 1903.0800000000002, "text": " So that brings us to the next class of techniques, which is using an external memory.", "tokens": [407, 300, 5607, 505, 281, 264, 958, 1508, 295, 7512, 11, 597, 307, 1228, 364, 8320, 4675, 13], "temperature": 0.0, "avg_logprob": -0.16755173092796688, "compression_ratio": 1.6328125, "no_speech_prob": 4.610751784639433e-05}, {"id": 526, "seek": 188664, "start": 1903.0800000000002, "end": 1906.44, "text": " And here we'll mainly focus on this work called KGLN and then we'll also briefly talk", "tokens": [400, 510, 321, 603, 8704, 1879, 322, 341, 589, 1219, 591, 19440, 45, 293, 550, 321, 603, 611, 10515, 751], "temperature": 0.0, "avg_logprob": -0.16755173092796688, "compression_ratio": 1.6328125, "no_speech_prob": 4.610751784639433e-05}, {"id": 527, "seek": 188664, "start": 1906.44, "end": 1909.96, "text": " about KNN LM.", "tokens": [466, 591, 45, 45, 46529, 13], "temperature": 0.0, "avg_logprob": -0.16755173092796688, "compression_ratio": 1.6328125, "no_speech_prob": 4.610751784639433e-05}, {"id": 528, "seek": 188664, "start": 1909.96, "end": 1913.5200000000002, "text": " So the previous methods that you've talked about have relied on pre-trained entity embeddings", "tokens": [407, 264, 3894, 7150, 300, 291, 600, 2825, 466, 362, 35463, 322, 659, 12, 17227, 2001, 13977, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.16755173092796688, "compression_ratio": 1.6328125, "no_speech_prob": 4.610751784639433e-05}, {"id": 529, "seek": 191352, "start": 1913.52, "end": 1917.2, "text": " to encode the factual knowledge from knowledge bases.", "tokens": [281, 2058, 1429, 264, 48029, 3601, 490, 3601, 17949, 13], "temperature": 0.0, "avg_logprob": -0.14774230896957277, "compression_ratio": 1.8810408921933086, "no_speech_prob": 2.0783962099812925e-05}, {"id": 530, "seek": 191352, "start": 1917.2, "end": 1921.12, "text": " And the one problem with this or one of the problems with this is if you want to, let's", "tokens": [400, 264, 472, 1154, 365, 341, 420, 472, 295, 264, 2740, 365, 341, 307, 498, 291, 528, 281, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.14774230896957277, "compression_ratio": 1.8810408921933086, "no_speech_prob": 2.0783962099812925e-05}, {"id": 531, "seek": 191352, "start": 1921.12, "end": 1922.84, "text": " say, modify your knowledge base.", "tokens": [584, 11, 16927, 428, 3601, 3096, 13], "temperature": 0.0, "avg_logprob": -0.14774230896957277, "compression_ratio": 1.8810408921933086, "no_speech_prob": 2.0783962099812925e-05}, {"id": 532, "seek": 191352, "start": 1922.84, "end": 1926.44, "text": " You now need to retrain your entity embeddings and then retrain your language model on top", "tokens": [509, 586, 643, 281, 1533, 7146, 428, 13977, 12240, 29432, 293, 550, 1533, 7146, 428, 2856, 2316, 322, 1192], "temperature": 0.0, "avg_logprob": -0.14774230896957277, "compression_ratio": 1.8810408921933086, "no_speech_prob": 2.0783962099812925e-05}, {"id": 533, "seek": 191352, "start": 1926.44, "end": 1928.8799999999999, "text": " of those entity embeddings.", "tokens": [295, 729, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.14774230896957277, "compression_ratio": 1.8810408921933086, "no_speech_prob": 2.0783962099812925e-05}, {"id": 534, "seek": 191352, "start": 1928.8799999999999, "end": 1933.36, "text": " So this begs a question, are there more direct ways in pre-trained entity embeddings", "tokens": [407, 341, 4612, 82, 257, 1168, 11, 366, 456, 544, 2047, 2098, 294, 659, 12, 17227, 2001, 13977, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.14774230896957277, "compression_ratio": 1.8810408921933086, "no_speech_prob": 2.0783962099812925e-05}, {"id": 535, "seek": 191352, "start": 1933.36, "end": 1937.12, "text": " to provide the model factual knowledge?", "tokens": [281, 2893, 264, 2316, 48029, 3601, 30], "temperature": 0.0, "avg_logprob": -0.14774230896957277, "compression_ratio": 1.8810408921933086, "no_speech_prob": 2.0783962099812925e-05}, {"id": 536, "seek": 191352, "start": 1937.12, "end": 1940.28, "text": " And so what we're going to talk about is how you can actually use an external memory or", "tokens": [400, 370, 437, 321, 434, 516, 281, 751, 466, 307, 577, 291, 393, 767, 764, 364, 8320, 4675, 420], "temperature": 0.0, "avg_logprob": -0.14774230896957277, "compression_ratio": 1.8810408921933086, "no_speech_prob": 2.0783962099812925e-05}, {"id": 537, "seek": 194028, "start": 1940.28, "end": 1945.16, "text": " a key value store to give the model access to either knowledge graph triples or context", "tokens": [257, 2141, 2158, 3531, 281, 976, 264, 2316, 2105, 281, 2139, 3601, 4295, 1376, 2622, 420, 4319], "temperature": 0.0, "avg_logprob": -0.16037195971888354, "compression_ratio": 1.7431506849315068, "no_speech_prob": 5.421992227638839e-06}, {"id": 538, "seek": 194028, "start": 1945.16, "end": 1946.16, "text": " information.", "tokens": [1589, 13], "temperature": 0.0, "avg_logprob": -0.16037195971888354, "compression_ratio": 1.7431506849315068, "no_speech_prob": 5.421992227638839e-06}, {"id": 539, "seek": 194028, "start": 1946.16, "end": 1950.72, "text": " And a key thing about this external memory is that it's independent of the learned model", "tokens": [400, 257, 2141, 551, 466, 341, 8320, 4675, 307, 300, 309, 311, 6695, 295, 264, 3264, 2316], "temperature": 0.0, "avg_logprob": -0.16037195971888354, "compression_ratio": 1.7431506849315068, "no_speech_prob": 5.421992227638839e-06}, {"id": 540, "seek": 194028, "start": 1950.72, "end": 1953.08, "text": " parameters.", "tokens": [9834, 13], "temperature": 0.0, "avg_logprob": -0.16037195971888354, "compression_ratio": 1.7431506849315068, "no_speech_prob": 5.421992227638839e-06}, {"id": 541, "seek": 194028, "start": 1953.08, "end": 1957.08, "text": " So this means you can actually support injecting and updating factual knowledge.", "tokens": [407, 341, 1355, 291, 393, 767, 1406, 10711, 278, 293, 25113, 48029, 3601, 13], "temperature": 0.0, "avg_logprob": -0.16037195971888354, "compression_ratio": 1.7431506849315068, "no_speech_prob": 5.421992227638839e-06}, {"id": 542, "seek": 194028, "start": 1957.08, "end": 1960.72, "text": " You can do this directly to this symbolic external memory, while let's say changing the", "tokens": [509, 393, 360, 341, 3838, 281, 341, 25755, 8320, 4675, 11, 1339, 718, 311, 584, 4473, 264], "temperature": 0.0, "avg_logprob": -0.16037195971888354, "compression_ratio": 1.7431506849315068, "no_speech_prob": 5.421992227638839e-06}, {"id": 543, "seek": 194028, "start": 1960.72, "end": 1964.76, "text": " value for a particular key or maybe adding another key.", "tokens": [2158, 337, 257, 1729, 2141, 420, 1310, 5127, 1071, 2141, 13], "temperature": 0.0, "avg_logprob": -0.16037195971888354, "compression_ratio": 1.7431506849315068, "no_speech_prob": 5.421992227638839e-06}, {"id": 544, "seek": 194028, "start": 1964.76, "end": 1969.0, "text": " And you don't have to retrain or retrain your entity embeddings when you make this", "tokens": [400, 291, 500, 380, 362, 281, 1533, 7146, 420, 1533, 7146, 428, 13977, 12240, 29432, 562, 291, 652, 341], "temperature": 0.0, "avg_logprob": -0.16037195971888354, "compression_ratio": 1.7431506849315068, "no_speech_prob": 5.421992227638839e-06}, {"id": 545, "seek": 196900, "start": 1969.0, "end": 1970.88, "text": " change.", "tokens": [1319, 13], "temperature": 0.0, "avg_logprob": -0.16627929249747855, "compression_ratio": 1.8339483394833949, "no_speech_prob": 6.108150410000235e-05}, {"id": 546, "seek": 196900, "start": 1970.88, "end": 1974.24, "text": " And the approaches we'll talk about today can actually even have these updates to the", "tokens": [400, 264, 11587, 321, 603, 751, 466, 965, 393, 767, 754, 362, 613, 9205, 281, 264], "temperature": 0.0, "avg_logprob": -0.16627929249747855, "compression_ratio": 1.8339483394833949, "no_speech_prob": 6.108150410000235e-05}, {"id": 547, "seek": 196900, "start": 1974.24, "end": 1978.0, "text": " external memory without more pre-training of the language model.", "tokens": [8320, 4675, 1553, 544, 659, 12, 17227, 1760, 295, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16627929249747855, "compression_ratio": 1.8339483394833949, "no_speech_prob": 6.108150410000235e-05}, {"id": 548, "seek": 196900, "start": 1978.0, "end": 1981.04, "text": " So that's pretty neat.", "tokens": [407, 300, 311, 1238, 10654, 13], "temperature": 0.0, "avg_logprob": -0.16627929249747855, "compression_ratio": 1.8339483394833949, "no_speech_prob": 6.108150410000235e-05}, {"id": 549, "seek": 196900, "start": 1981.04, "end": 1984.72, "text": " And then another benefit of using external memory over these pre-trained entity embedding", "tokens": [400, 550, 1071, 5121, 295, 1228, 8320, 4675, 670, 613, 659, 12, 17227, 2001, 13977, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.16627929249747855, "compression_ratio": 1.8339483394833949, "no_speech_prob": 6.108150410000235e-05}, {"id": 550, "seek": 196900, "start": 1984.72, "end": 1987.96, "text": " approaches is they can also be more interpretable.", "tokens": [11587, 307, 436, 393, 611, 312, 544, 7302, 712, 13], "temperature": 0.0, "avg_logprob": -0.16627929249747855, "compression_ratio": 1.8339483394833949, "no_speech_prob": 6.108150410000235e-05}, {"id": 551, "seek": 196900, "start": 1987.96, "end": 1993.48, "text": " So if you have a bug or not bug an air in your model where it's not predicting a correct", "tokens": [407, 498, 291, 362, 257, 7426, 420, 406, 7426, 364, 1988, 294, 428, 2316, 689, 309, 311, 406, 32884, 257, 3006], "temperature": 0.0, "avg_logprob": -0.16627929249747855, "compression_ratio": 1.8339483394833949, "no_speech_prob": 6.108150410000235e-05}, {"id": 552, "seek": 196900, "start": 1993.48, "end": 1998.16, "text": " fact, it's very challenging to figure out with pre-trained entity embeddings what the", "tokens": [1186, 11, 309, 311, 588, 7595, 281, 2573, 484, 365, 659, 12, 17227, 2001, 13977, 12240, 29432, 437, 264], "temperature": 0.0, "avg_logprob": -0.16627929249747855, "compression_ratio": 1.8339483394833949, "no_speech_prob": 6.108150410000235e-05}, {"id": 553, "seek": 199816, "start": 1998.16, "end": 1999.48, "text": " problem might be.", "tokens": [1154, 1062, 312, 13], "temperature": 0.0, "avg_logprob": -0.1747004852294922, "compression_ratio": 1.992063492063492, "no_speech_prob": 2.282661444041878e-05}, {"id": 554, "seek": 199816, "start": 1999.48, "end": 2000.8000000000002, "text": " Was it the original knowledge base?", "tokens": [3027, 309, 264, 3380, 3601, 3096, 30], "temperature": 0.0, "avg_logprob": -0.1747004852294922, "compression_ratio": 1.992063492063492, "no_speech_prob": 2.282661444041878e-05}, {"id": 555, "seek": 199816, "start": 2000.8000000000002, "end": 2002.48, "text": " Was it the encoding in the entity embeddings?", "tokens": [3027, 309, 264, 43430, 294, 264, 13977, 12240, 29432, 30], "temperature": 0.0, "avg_logprob": -0.1747004852294922, "compression_ratio": 1.992063492063492, "no_speech_prob": 2.282661444041878e-05}, {"id": 556, "seek": 199816, "start": 2002.48, "end": 2005.4, "text": " Is it how the language models using the entity embeddings?", "tokens": [1119, 309, 577, 264, 2856, 5245, 1228, 264, 13977, 12240, 29432, 30], "temperature": 0.0, "avg_logprob": -0.1747004852294922, "compression_ratio": 1.992063492063492, "no_speech_prob": 2.282661444041878e-05}, {"id": 557, "seek": 199816, "start": 2005.4, "end": 2008.76, "text": " And here you have a little more information with an external memory.", "tokens": [400, 510, 291, 362, 257, 707, 544, 1589, 365, 364, 8320, 4675, 13], "temperature": 0.0, "avg_logprob": -0.1747004852294922, "compression_ratio": 1.992063492063492, "no_speech_prob": 2.282661444041878e-05}, {"id": 558, "seek": 199816, "start": 2008.76, "end": 2013.3600000000001, "text": " And that you can look in the external memory and see what's the fact in the external memory", "tokens": [400, 300, 291, 393, 574, 294, 264, 8320, 4675, 293, 536, 437, 311, 264, 1186, 294, 264, 8320, 4675], "temperature": 0.0, "avg_logprob": -0.1747004852294922, "compression_ratio": 1.992063492063492, "no_speech_prob": 2.282661444041878e-05}, {"id": 559, "seek": 199816, "start": 2013.3600000000001, "end": 2015.88, "text": " was not an external memory and so on.", "tokens": [390, 406, 364, 8320, 4675, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.1747004852294922, "compression_ratio": 1.992063492063492, "no_speech_prob": 2.282661444041878e-05}, {"id": 560, "seek": 199816, "start": 2015.88, "end": 2020.4, "text": " So it adds a little bit more interpretability than just using these pre-trained entity embeddings", "tokens": [407, 309, 10860, 257, 707, 857, 544, 7302, 2310, 813, 445, 1228, 613, 659, 12, 17227, 2001, 13977, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.1747004852294922, "compression_ratio": 1.992063492063492, "no_speech_prob": 2.282661444041878e-05}, {"id": 561, "seek": 199816, "start": 2020.4, "end": 2025.92, "text": " as an inject way to encode the knowledge base.", "tokens": [382, 364, 10711, 636, 281, 2058, 1429, 264, 3601, 3096, 13], "temperature": 0.0, "avg_logprob": -0.1747004852294922, "compression_ratio": 1.992063492063492, "no_speech_prob": 2.282661444041878e-05}, {"id": 562, "seek": 202592, "start": 2025.92, "end": 2029.8400000000001, "text": " So the first word we're going to talk about is called KGLM and unlike the other approaches", "tokens": [407, 264, 700, 1349, 321, 434, 516, 281, 751, 466, 307, 1219, 591, 19440, 44, 293, 8343, 264, 661, 11587], "temperature": 0.0, "avg_logprob": -0.1598714426023151, "compression_ratio": 1.8152610441767068, "no_speech_prob": 1.5935342162265442e-05}, {"id": 563, "seek": 202592, "start": 2029.8400000000001, "end": 2035.8400000000001, "text": " we've talked about so far, this actually uses LSTMs and not transformers.", "tokens": [321, 600, 2825, 466, 370, 1400, 11, 341, 767, 4960, 441, 6840, 26386, 293, 406, 4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.1598714426023151, "compression_ratio": 1.8152610441767068, "no_speech_prob": 1.5935342162265442e-05}, {"id": 564, "seek": 202592, "start": 2035.8400000000001, "end": 2040.92, "text": " So the key idea here is to condition the language model on a knowledge graph.", "tokens": [407, 264, 2141, 1558, 510, 307, 281, 4188, 264, 2856, 2316, 322, 257, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.1598714426023151, "compression_ratio": 1.8152610441767068, "no_speech_prob": 1.5935342162265442e-05}, {"id": 565, "seek": 202592, "start": 2040.92, "end": 2044.44, "text": " So recall with the standard language model, we want to predict the next word given the", "tokens": [407, 9901, 365, 264, 3832, 2856, 2316, 11, 321, 528, 281, 6069, 264, 958, 1349, 2212, 264], "temperature": 0.0, "avg_logprob": -0.1598714426023151, "compression_ratio": 1.8152610441767068, "no_speech_prob": 1.5935342162265442e-05}, {"id": 566, "seek": 202592, "start": 2044.44, "end": 2046.4, "text": " previous words in the sequence.", "tokens": [3894, 2283, 294, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.1598714426023151, "compression_ratio": 1.8152610441767068, "no_speech_prob": 1.5935342162265442e-05}, {"id": 567, "seek": 202592, "start": 2046.4, "end": 2051.4, "text": " Well, now we also want to predict the next entity given the previous words in the sequence", "tokens": [1042, 11, 586, 321, 611, 528, 281, 6069, 264, 958, 13977, 2212, 264, 3894, 2283, 294, 264, 8310], "temperature": 0.0, "avg_logprob": -0.1598714426023151, "compression_ratio": 1.8152610441767068, "no_speech_prob": 1.5935342162265442e-05}, {"id": 568, "seek": 205140, "start": 2051.4, "end": 2056.52, "text": " and given the previous entities in the sentence or the entities that are relevant to the sentence", "tokens": [293, 2212, 264, 3894, 16667, 294, 264, 8174, 420, 264, 16667, 300, 366, 7340, 281, 264, 8174], "temperature": 0.0, "avg_logprob": -0.1774327073778425, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.747692168573849e-06}, {"id": 569, "seek": 205140, "start": 2056.52, "end": 2059.44, "text": " I should say.", "tokens": [286, 820, 584, 13], "temperature": 0.0, "avg_logprob": -0.1774327073778425, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.747692168573849e-06}, {"id": 570, "seek": 205140, "start": 2059.44, "end": 2064.48, "text": " So KGLM will be building a local knowledge graph as it iterates over the sequence.", "tokens": [407, 591, 19440, 44, 486, 312, 2390, 257, 2654, 3601, 4295, 382, 309, 17138, 1024, 670, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.1774327073778425, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.747692168573849e-06}, {"id": 571, "seek": 205140, "start": 2064.48, "end": 2068.28, "text": " And a local knowledge graph is just a subset of a full knowledge graph that only has the", "tokens": [400, 257, 2654, 3601, 4295, 307, 445, 257, 25993, 295, 257, 1577, 3601, 4295, 300, 787, 575, 264], "temperature": 0.0, "avg_logprob": -0.1774327073778425, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.747692168573849e-06}, {"id": 572, "seek": 205140, "start": 2068.28, "end": 2072.2400000000002, "text": " entities that are actually relevant to the sequence.", "tokens": [16667, 300, 366, 767, 7340, 281, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.1774327073778425, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.747692168573849e-06}, {"id": 573, "seek": 205140, "start": 2072.2400000000002, "end": 2077.48, "text": " So if we have this example here, a simplified example from the paper, that SuperMarioLand", "tokens": [407, 498, 321, 362, 341, 1365, 510, 11, 257, 26335, 1365, 490, 264, 3035, 11, 300, 4548, 44, 4912, 43, 474], "temperature": 0.0, "avg_logprob": -0.1774327073778425, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.747692168573849e-06}, {"id": 574, "seek": 205140, "start": 2077.48, "end": 2079.76, "text": " is a game developed by Blank.", "tokens": [307, 257, 1216, 4743, 538, 2177, 657, 13], "temperature": 0.0, "avg_logprob": -0.1774327073778425, "compression_ratio": 1.8461538461538463, "no_speech_prob": 6.747692168573849e-06}, {"id": 575, "seek": 207976, "start": 2079.76, "end": 2083.1200000000003, "text": " And SuperMarioLand here is an entity.", "tokens": [400, 4548, 44, 4912, 43, 474, 510, 307, 364, 13977, 13], "temperature": 0.0, "avg_logprob": -0.1138001660831639, "compression_ratio": 2.1319148936170214, "no_speech_prob": 8.139390047290362e-06}, {"id": 576, "seek": 207976, "start": 2083.1200000000003, "end": 2087.0800000000004, "text": " You'd want a local knowledge graph as follows where you see that SuperMarioLand is in the", "tokens": [509, 1116, 528, 257, 2654, 3601, 4295, 382, 10002, 689, 291, 536, 300, 4548, 44, 4912, 43, 474, 307, 294, 264], "temperature": 0.0, "avg_logprob": -0.1138001660831639, "compression_ratio": 2.1319148936170214, "no_speech_prob": 8.139390047290362e-06}, {"id": 577, "seek": 207976, "start": 2087.0800000000004, "end": 2092.0800000000004, "text": " local knowledge graph, but we also have the relations to SuperMarioLand to other entities", "tokens": [2654, 3601, 4295, 11, 457, 321, 611, 362, 264, 2299, 281, 4548, 44, 4912, 43, 474, 281, 661, 16667], "temperature": 0.0, "avg_logprob": -0.1138001660831639, "compression_ratio": 2.1319148936170214, "no_speech_prob": 8.139390047290362e-06}, {"id": 578, "seek": 207976, "start": 2092.0800000000004, "end": 2096.44, "text": " that are copied from the full knowledge graph into this local knowledge graph.", "tokens": [300, 366, 25365, 490, 264, 1577, 3601, 4295, 666, 341, 2654, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.1138001660831639, "compression_ratio": 2.1319148936170214, "no_speech_prob": 8.139390047290362e-06}, {"id": 579, "seek": 207976, "start": 2096.44, "end": 2099.5600000000004, "text": " And you would build up this local knowledge graph as you iterate over the sentence.", "tokens": [400, 291, 576, 1322, 493, 341, 2654, 3601, 4295, 382, 291, 44497, 670, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.1138001660831639, "compression_ratio": 2.1319148936170214, "no_speech_prob": 8.139390047290362e-06}, {"id": 580, "seek": 207976, "start": 2099.5600000000004, "end": 2103.0400000000004, "text": " So whenever you see an entity, you would add it to the local knowledge graph as well as", "tokens": [407, 5699, 291, 536, 364, 13977, 11, 291, 576, 909, 309, 281, 264, 2654, 3601, 4295, 382, 731, 382], "temperature": 0.0, "avg_logprob": -0.1138001660831639, "compression_ratio": 2.1319148936170214, "no_speech_prob": 8.139390047290362e-06}, {"id": 581, "seek": 207976, "start": 2103.0400000000004, "end": 2106.48, "text": " its relations to other entities.", "tokens": [1080, 2299, 281, 661, 16667, 13], "temperature": 0.0, "avg_logprob": -0.1138001660831639, "compression_ratio": 2.1319148936170214, "no_speech_prob": 8.139390047290362e-06}, {"id": 582, "seek": 210648, "start": 2106.48, "end": 2111.12, "text": " So obviously this is a much smaller example than what would really have all the relations", "tokens": [407, 2745, 341, 307, 257, 709, 4356, 1365, 813, 437, 576, 534, 362, 439, 264, 2299], "temperature": 0.0, "avg_logprob": -0.15643666282532706, "compression_ratio": 1.805921052631579, "no_speech_prob": 7.527527486672625e-06}, {"id": 583, "seek": 210648, "start": 2111.12, "end": 2114.04, "text": " to SuperMarioLand just for the purpose of the example.", "tokens": [281, 4548, 44, 4912, 43, 474, 445, 337, 264, 4334, 295, 264, 1365, 13], "temperature": 0.0, "avg_logprob": -0.15643666282532706, "compression_ratio": 1.805921052631579, "no_speech_prob": 7.527527486672625e-06}, {"id": 584, "seek": 210648, "start": 2114.04, "end": 2120.0, "text": " But hopefully it's clear that all of these are relevant to the sequence.", "tokens": [583, 4696, 309, 311, 1850, 300, 439, 295, 613, 366, 7340, 281, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.15643666282532706, "compression_ratio": 1.805921052631579, "no_speech_prob": 7.527527486672625e-06}, {"id": 585, "seek": 210648, "start": 2120.0, "end": 2123.2400000000002, "text": " Something important to note here is that this does assume that the entities are known during", "tokens": [6595, 1021, 281, 3637, 510, 307, 300, 341, 775, 6552, 300, 264, 16667, 366, 2570, 1830], "temperature": 0.0, "avg_logprob": -0.15643666282532706, "compression_ratio": 1.805921052631579, "no_speech_prob": 7.527527486672625e-06}, {"id": 586, "seek": 210648, "start": 2123.2400000000002, "end": 2127.12, "text": " training so that you do have this entity annotated data for training.", "tokens": [3097, 370, 300, 291, 360, 362, 341, 13977, 25339, 770, 1412, 337, 3097, 13], "temperature": 0.0, "avg_logprob": -0.15643666282532706, "compression_ratio": 1.805921052631579, "no_speech_prob": 7.527527486672625e-06}, {"id": 587, "seek": 210648, "start": 2127.12, "end": 2130.64, "text": " And therefore your local knowledge graph is always the ground truth local knowledge graph", "tokens": [400, 4412, 428, 2654, 3601, 4295, 307, 1009, 264, 2727, 3494, 2654, 3601, 4295], "temperature": 0.0, "avg_logprob": -0.15643666282532706, "compression_ratio": 1.805921052631579, "no_speech_prob": 7.527527486672625e-06}, {"id": 588, "seek": 210648, "start": 2130.64, "end": 2133.84, "text": " as you iterate over the sequence.", "tokens": [382, 291, 44497, 670, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.15643666282532706, "compression_ratio": 1.805921052631579, "no_speech_prob": 7.527527486672625e-06}, {"id": 589, "seek": 210648, "start": 2133.84, "end": 2136.44, "text": " So why might this be a good idea to do this?", "tokens": [407, 983, 1062, 341, 312, 257, 665, 1558, 281, 360, 341, 30], "temperature": 0.0, "avg_logprob": -0.15643666282532706, "compression_ratio": 1.805921052631579, "no_speech_prob": 7.527527486672625e-06}, {"id": 590, "seek": 213644, "start": 2136.44, "end": 2139.64, "text": " Well here the next word you want to predict is Nintendo.", "tokens": [1042, 510, 264, 958, 1349, 291, 528, 281, 6069, 307, 11578, 13], "temperature": 0.0, "avg_logprob": -0.17996180374010473, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.952424099727068e-05}, {"id": 591, "seek": 213644, "start": 2139.64, "end": 2143.12, "text": " And you may notice that Nintendo is in your local knowledge graph.", "tokens": [400, 291, 815, 3449, 300, 11578, 307, 294, 428, 2654, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.17996180374010473, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.952424099727068e-05}, {"id": 592, "seek": 213644, "start": 2143.12, "end": 2147.0, "text": " So sometimes this local knowledge graph can actually serve as a very strong signal for", "tokens": [407, 2171, 341, 2654, 3601, 4295, 393, 767, 4596, 382, 257, 588, 2068, 6358, 337], "temperature": 0.0, "avg_logprob": -0.17996180374010473, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.952424099727068e-05}, {"id": 593, "seek": 213644, "start": 2147.0, "end": 2150.48, "text": " what you want to predict for your next word.", "tokens": [437, 291, 528, 281, 6069, 337, 428, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.17996180374010473, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.952424099727068e-05}, {"id": 594, "seek": 213644, "start": 2150.48, "end": 2155.64, "text": " Now you may be thinking well this wouldn't always be helpful and that's true as well.", "tokens": [823, 291, 815, 312, 1953, 731, 341, 2759, 380, 1009, 312, 4961, 293, 300, 311, 2074, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.17996180374010473, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.952424099727068e-05}, {"id": 595, "seek": 213644, "start": 2155.64, "end": 2158.4, "text": " So if you look at just like the third word in the sequence and you want to predict that", "tokens": [407, 498, 291, 574, 412, 445, 411, 264, 2636, 1349, 294, 264, 8310, 293, 291, 528, 281, 6069, 300], "temperature": 0.0, "avg_logprob": -0.17996180374010473, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.952424099727068e-05}, {"id": 596, "seek": 213644, "start": 2158.4, "end": 2162.36, "text": " word, so is a game for instance.", "tokens": [1349, 11, 370, 307, 257, 1216, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.17996180374010473, "compression_ratio": 1.8333333333333333, "no_speech_prob": 1.952424099727068e-05}, {"id": 597, "seek": 216236, "start": 2162.36, "end": 2166.84, "text": " Well if this isn't in the local knowledge graph this wouldn't be necessarily that helpful.", "tokens": [1042, 498, 341, 1943, 380, 294, 264, 2654, 3601, 4295, 341, 2759, 380, 312, 4725, 300, 4961, 13], "temperature": 0.0, "avg_logprob": -0.16090204065496272, "compression_ratio": 1.7265917602996255, "no_speech_prob": 1.2411040188453626e-05}, {"id": 598, "seek": 216236, "start": 2166.84, "end": 2170.32, "text": " You would just do a standard language model prediction.", "tokens": [509, 576, 445, 360, 257, 3832, 2856, 2316, 17630, 13], "temperature": 0.0, "avg_logprob": -0.16090204065496272, "compression_ratio": 1.7265917602996255, "no_speech_prob": 1.2411040188453626e-05}, {"id": 599, "seek": 216236, "start": 2170.32, "end": 2174.28, "text": " Or if you're at the beginning of the sequence, your local knowledge graph is empty so of course", "tokens": [1610, 498, 291, 434, 412, 264, 2863, 295, 264, 8310, 11, 428, 2654, 3601, 4295, 307, 6707, 370, 295, 1164], "temperature": 0.0, "avg_logprob": -0.16090204065496272, "compression_ratio": 1.7265917602996255, "no_speech_prob": 1.2411040188453626e-05}, {"id": 600, "seek": 216236, "start": 2174.28, "end": 2176.88, "text": " you're not going to get any signal from it.", "tokens": [291, 434, 406, 516, 281, 483, 604, 6358, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.16090204065496272, "compression_ratio": 1.7265917602996255, "no_speech_prob": 1.2411040188453626e-05}, {"id": 601, "seek": 216236, "start": 2176.88, "end": 2181.48, "text": " So the first question they ask in KGLM is how can a language model know when to use a", "tokens": [407, 264, 700, 1168, 436, 1029, 294, 591, 19440, 44, 307, 577, 393, 257, 2856, 2316, 458, 562, 281, 764, 257], "temperature": 0.0, "avg_logprob": -0.16090204065496272, "compression_ratio": 1.7265917602996255, "no_speech_prob": 1.2411040188453626e-05}, {"id": 602, "seek": 216236, "start": 2181.48, "end": 2189.44, "text": " local knowledge graph and when it might actually be useful for predicting the next word.", "tokens": [2654, 3601, 4295, 293, 562, 309, 1062, 767, 312, 4420, 337, 32884, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.16090204065496272, "compression_ratio": 1.7265917602996255, "no_speech_prob": 1.2411040188453626e-05}, {"id": 603, "seek": 218944, "start": 2189.44, "end": 2192.68, "text": " So we're going to keep the same example as a running example and we have our local knowledge", "tokens": [407, 321, 434, 516, 281, 1066, 264, 912, 1365, 382, 257, 2614, 1365, 293, 321, 362, 527, 2654, 3601], "temperature": 0.0, "avg_logprob": -0.13618739784186615, "compression_ratio": 2.065891472868217, "no_speech_prob": 1.300688018091023e-05}, {"id": 604, "seek": 218944, "start": 2192.68, "end": 2194.2000000000003, "text": " graph here.", "tokens": [4295, 510, 13], "temperature": 0.0, "avg_logprob": -0.13618739784186615, "compression_ratio": 2.065891472868217, "no_speech_prob": 1.300688018091023e-05}, {"id": 605, "seek": 218944, "start": 2194.2000000000003, "end": 2197.28, "text": " We now have an LSTM that looks similar to the representations you've seen throughout", "tokens": [492, 586, 362, 364, 441, 6840, 44, 300, 1542, 2531, 281, 264, 33358, 291, 600, 1612, 3710], "temperature": 0.0, "avg_logprob": -0.13618739784186615, "compression_ratio": 2.065891472868217, "no_speech_prob": 1.300688018091023e-05}, {"id": 606, "seek": 218944, "start": 2197.28, "end": 2198.6, "text": " this class.", "tokens": [341, 1508, 13], "temperature": 0.0, "avg_logprob": -0.13618739784186615, "compression_ratio": 2.065891472868217, "no_speech_prob": 1.300688018091023e-05}, {"id": 607, "seek": 218944, "start": 2198.6, "end": 2201.28, "text": " And normally you've seen the LSTM predicts the next word.", "tokens": [400, 5646, 291, 600, 1612, 264, 441, 6840, 44, 6069, 82, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.13618739784186615, "compression_ratio": 2.065891472868217, "no_speech_prob": 1.300688018091023e-05}, {"id": 608, "seek": 218944, "start": 2201.28, "end": 2206.92, "text": " Well now we're also going to use the LSTM to predict the next type of the word.", "tokens": [1042, 586, 321, 434, 611, 516, 281, 764, 264, 441, 6840, 44, 281, 6069, 264, 958, 2010, 295, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.13618739784186615, "compression_ratio": 2.065891472868217, "no_speech_prob": 1.300688018091023e-05}, {"id": 609, "seek": 218944, "start": 2206.92, "end": 2210.68, "text": " So it's the next word going to be a related entity meaning it's in the local knowledge", "tokens": [407, 309, 311, 264, 958, 1349, 516, 281, 312, 257, 4077, 13977, 3620, 309, 311, 294, 264, 2654, 3601], "temperature": 0.0, "avg_logprob": -0.13618739784186615, "compression_ratio": 2.065891472868217, "no_speech_prob": 1.300688018091023e-05}, {"id": 610, "seek": 218944, "start": 2210.68, "end": 2211.68, "text": " graph already.", "tokens": [4295, 1217, 13], "temperature": 0.0, "avg_logprob": -0.13618739784186615, "compression_ratio": 2.065891472868217, "no_speech_prob": 1.300688018091023e-05}, {"id": 611, "seek": 218944, "start": 2211.68, "end": 2216.32, "text": " Is it going to be a new entity meaning it's not in the local knowledge graph or is it going", "tokens": [1119, 309, 516, 281, 312, 257, 777, 13977, 3620, 309, 311, 406, 294, 264, 2654, 3601, 4295, 420, 307, 309, 516], "temperature": 0.0, "avg_logprob": -0.13618739784186615, "compression_ratio": 2.065891472868217, "no_speech_prob": 1.300688018091023e-05}, {"id": 612, "seek": 221632, "start": 2216.32, "end": 2222.0800000000004, "text": " to be not an entity in which case you just revert to a normal LSTM prediction.", "tokens": [281, 312, 406, 364, 13977, 294, 597, 1389, 291, 445, 319, 3281, 281, 257, 2710, 441, 6840, 44, 17630, 13], "temperature": 0.0, "avg_logprob": -0.16615558037391076, "compression_ratio": 1.803448275862069, "no_speech_prob": 1.922264345921576e-05}, {"id": 613, "seek": 221632, "start": 2222.0800000000004, "end": 2225.4, "text": " And they're going to use the LSTM hidden state to do this prediction of the type of the", "tokens": [400, 436, 434, 516, 281, 764, 264, 441, 6840, 44, 7633, 1785, 281, 360, 341, 17630, 295, 264, 2010, 295, 264], "temperature": 0.0, "avg_logprob": -0.16615558037391076, "compression_ratio": 1.803448275862069, "no_speech_prob": 1.922264345921576e-05}, {"id": 614, "seek": 221632, "start": 2225.4, "end": 2231.6400000000003, "text": " next word over this three way, three different classes that they might want to consider.", "tokens": [958, 1349, 670, 341, 1045, 636, 11, 1045, 819, 5359, 300, 436, 1062, 528, 281, 1949, 13], "temperature": 0.0, "avg_logprob": -0.16615558037391076, "compression_ratio": 1.803448275862069, "no_speech_prob": 1.922264345921576e-05}, {"id": 615, "seek": 221632, "start": 2231.6400000000003, "end": 2235.1600000000003, "text": " So in the case of super Mario Land is a game developed mind Nintendo.", "tokens": [407, 294, 264, 1389, 295, 1687, 9343, 6607, 307, 257, 1216, 4743, 1575, 11578, 13], "temperature": 0.0, "avg_logprob": -0.16615558037391076, "compression_ratio": 1.803448275862069, "no_speech_prob": 1.922264345921576e-05}, {"id": 616, "seek": 221632, "start": 2235.1600000000003, "end": 2239.36, "text": " We saw that this would be a related entity case because you saw that Nintendo was in", "tokens": [492, 1866, 300, 341, 576, 312, 257, 4077, 13977, 1389, 570, 291, 1866, 300, 11578, 390, 294], "temperature": 0.0, "avg_logprob": -0.16615558037391076, "compression_ratio": 1.803448275862069, "no_speech_prob": 1.922264345921576e-05}, {"id": 617, "seek": 221632, "start": 2239.36, "end": 2241.36, "text": " the local knowledge graph.", "tokens": [264, 2654, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.16615558037391076, "compression_ratio": 1.803448275862069, "no_speech_prob": 1.922264345921576e-05}, {"id": 618, "seek": 221632, "start": 2241.36, "end": 2245.8, "text": " For the other cases, super Mario Land would be a new entity case since it's the local", "tokens": [1171, 264, 661, 3331, 11, 1687, 9343, 6607, 576, 312, 257, 777, 13977, 1389, 1670, 309, 311, 264, 2654], "temperature": 0.0, "avg_logprob": -0.16615558037391076, "compression_ratio": 1.803448275862069, "no_speech_prob": 1.922264345921576e-05}, {"id": 619, "seek": 224580, "start": 2245.8, "end": 2247.96, "text": " knowledge graph is empty at that point.", "tokens": [3601, 4295, 307, 6707, 412, 300, 935, 13], "temperature": 0.0, "avg_logprob": -0.15517384151242813, "compression_ratio": 1.652014652014652, "no_speech_prob": 4.610631731338799e-05}, {"id": 620, "seek": 224580, "start": 2247.96, "end": 2253.1600000000003, "text": " And then any of the words between super Mario Land and Nintendo would be not an entity.", "tokens": [400, 550, 604, 295, 264, 2283, 1296, 1687, 9343, 6607, 293, 11578, 576, 312, 406, 364, 13977, 13], "temperature": 0.0, "avg_logprob": -0.15517384151242813, "compression_ratio": 1.652014652014652, "no_speech_prob": 4.610631731338799e-05}, {"id": 621, "seek": 224580, "start": 2253.1600000000003, "end": 2260.36, "text": " Is there just a standard LSTM language model prediction that doesn't involve any entities.", "tokens": [1119, 456, 445, 257, 3832, 441, 6840, 44, 2856, 2316, 17630, 300, 1177, 380, 9494, 604, 16667, 13], "temperature": 0.0, "avg_logprob": -0.15517384151242813, "compression_ratio": 1.652014652014652, "no_speech_prob": 4.610631731338799e-05}, {"id": 622, "seek": 224580, "start": 2260.36, "end": 2263.84, "text": " So now we need to talk about what the language model actually does in these three different", "tokens": [407, 586, 321, 643, 281, 751, 466, 437, 264, 2856, 2316, 767, 775, 294, 613, 1045, 819], "temperature": 0.0, "avg_logprob": -0.15517384151242813, "compression_ratio": 1.652014652014652, "no_speech_prob": 4.610631731338799e-05}, {"id": 623, "seek": 224580, "start": 2263.84, "end": 2271.2000000000003, "text": " scenarios to predict the next entity and the next word.", "tokens": [15077, 281, 6069, 264, 958, 13977, 293, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.15517384151242813, "compression_ratio": 1.652014652014652, "no_speech_prob": 4.610631731338799e-05}, {"id": 624, "seek": 224580, "start": 2271.2000000000003, "end": 2273.6000000000004, "text": " So we're going to keep the example up at the top in case you want to further back to", "tokens": [407, 321, 434, 516, 281, 1066, 264, 1365, 493, 412, 264, 1192, 294, 1389, 291, 528, 281, 3052, 646, 281], "temperature": 0.0, "avg_logprob": -0.15517384151242813, "compression_ratio": 1.652014652014652, "no_speech_prob": 4.610631731338799e-05}, {"id": 625, "seek": 227360, "start": 2273.6, "end": 2275.7999999999997, "text": " three different cases.", "tokens": [1045, 819, 3331, 13], "temperature": 0.0, "avg_logprob": -0.2050285509654454, "compression_ratio": 1.8228346456692914, "no_speech_prob": 4.860322405875195e-06}, {"id": 626, "seek": 227360, "start": 2275.7999999999997, "end": 2279.2, "text": " And we're going to start with a related entity case.", "tokens": [400, 321, 434, 516, 281, 722, 365, 257, 4077, 13977, 1389, 13], "temperature": 0.0, "avg_logprob": -0.2050285509654454, "compression_ratio": 1.8228346456692914, "no_speech_prob": 4.860322405875195e-06}, {"id": 627, "seek": 227360, "start": 2279.2, "end": 2284.08, "text": " So here we assume that the next word or entity is actually in your local knowledge graph.", "tokens": [407, 510, 321, 6552, 300, 264, 958, 1349, 420, 13977, 307, 767, 294, 428, 2654, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.2050285509654454, "compression_ratio": 1.8228346456692914, "no_speech_prob": 4.860322405875195e-06}, {"id": 628, "seek": 227360, "start": 2284.08, "end": 2287.44, "text": " And remember that we can describe an knowledge graph in terms of triples.", "tokens": [400, 1604, 300, 321, 393, 6786, 364, 3601, 4295, 294, 2115, 295, 1376, 2622, 13], "temperature": 0.0, "avg_logprob": -0.2050285509654454, "compression_ratio": 1.8228346456692914, "no_speech_prob": 4.860322405875195e-06}, {"id": 629, "seek": 227360, "start": 2287.44, "end": 2292.56, "text": " So in terms of pairs of parent entities, relations and tail entities.", "tokens": [407, 294, 2115, 295, 15494, 295, 2596, 16667, 11, 2299, 293, 6838, 16667, 13], "temperature": 0.0, "avg_logprob": -0.2050285509654454, "compression_ratio": 1.8228346456692914, "no_speech_prob": 4.860322405875195e-06}, {"id": 630, "seek": 227360, "start": 2292.56, "end": 2295.08, "text": " And in the case of predicting the next word as Nintendo.", "tokens": [400, 294, 264, 1389, 295, 32884, 264, 958, 1349, 382, 11578, 13], "temperature": 0.0, "avg_logprob": -0.2050285509654454, "compression_ratio": 1.8228346456692914, "no_speech_prob": 4.860322405875195e-06}, {"id": 631, "seek": 227360, "start": 2295.08, "end": 2299.88, "text": " There's only one possible parent entity in the local knowledge graph, which is super Mario", "tokens": [821, 311, 787, 472, 1944, 2596, 13977, 294, 264, 2654, 3601, 4295, 11, 597, 307, 1687, 9343], "temperature": 0.0, "avg_logprob": -0.2050285509654454, "compression_ratio": 1.8228346456692914, "no_speech_prob": 4.860322405875195e-06}, {"id": 632, "seek": 227360, "start": 2299.88, "end": 2301.36, "text": " Land.", "tokens": [6607, 13], "temperature": 0.0, "avg_logprob": -0.2050285509654454, "compression_ratio": 1.8228346456692914, "no_speech_prob": 4.860322405875195e-06}, {"id": 633, "seek": 230136, "start": 2301.36, "end": 2305.6400000000003, "text": " And the goal is you want to figure out what is the most relevant triple that will be useful", "tokens": [400, 264, 3387, 307, 291, 528, 281, 2573, 484, 437, 307, 264, 881, 7340, 15508, 300, 486, 312, 4420], "temperature": 0.0, "avg_logprob": -0.1482283694684998, "compression_ratio": 1.8921933085501859, "no_speech_prob": 9.515926649328321e-06}, {"id": 634, "seek": 230136, "start": 2305.6400000000003, "end": 2308.28, "text": " in helping to predict the next word.", "tokens": [294, 4315, 281, 6069, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1482283694684998, "compression_ratio": 1.8921933085501859, "no_speech_prob": 9.515926649328321e-06}, {"id": 635, "seek": 230136, "start": 2308.28, "end": 2312.44, "text": " So in this case, you could have the triple super Mario Land publisher Nintendo.", "tokens": [407, 294, 341, 1389, 11, 291, 727, 362, 264, 15508, 1687, 9343, 6607, 25088, 11578, 13], "temperature": 0.0, "avg_logprob": -0.1482283694684998, "compression_ratio": 1.8921933085501859, "no_speech_prob": 9.515926649328321e-06}, {"id": 636, "seek": 230136, "start": 2312.44, "end": 2316.36, "text": " You might have the triple super Mario Land genre platform game, which of these is actually", "tokens": [509, 1062, 362, 264, 15508, 1687, 9343, 6607, 11022, 3663, 1216, 11, 597, 295, 613, 307, 767], "temperature": 0.0, "avg_logprob": -0.1482283694684998, "compression_ratio": 1.8921933085501859, "no_speech_prob": 9.515926649328321e-06}, {"id": 637, "seek": 230136, "start": 2316.36, "end": 2320.84, "text": " helpful in predicting that Nintendo should be the next word.", "tokens": [4961, 294, 32884, 300, 11578, 820, 312, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1482283694684998, "compression_ratio": 1.8921933085501859, "no_speech_prob": 9.515926649328321e-06}, {"id": 638, "seek": 230136, "start": 2320.84, "end": 2325.84, "text": " So here what you would want KGLN to do is predict that the top scoring parent entity is super", "tokens": [407, 510, 437, 291, 576, 528, 591, 19440, 45, 281, 360, 307, 6069, 300, 264, 1192, 22358, 2596, 13977, 307, 1687], "temperature": 0.0, "avg_logprob": -0.1482283694684998, "compression_ratio": 1.8921933085501859, "no_speech_prob": 9.515926649328321e-06}, {"id": 639, "seek": 230136, "start": 2325.84, "end": 2326.84, "text": " Mario Land.", "tokens": [9343, 6607, 13], "temperature": 0.0, "avg_logprob": -0.1482283694684998, "compression_ratio": 1.8921933085501859, "no_speech_prob": 9.515926649328321e-06}, {"id": 640, "seek": 230136, "start": 2326.84, "end": 2329.7200000000003, "text": " And the top scoring relation is publisher.", "tokens": [400, 264, 1192, 22358, 9721, 307, 25088, 13], "temperature": 0.0, "avg_logprob": -0.1482283694684998, "compression_ratio": 1.8921933085501859, "no_speech_prob": 9.515926649328321e-06}, {"id": 641, "seek": 232972, "start": 2329.72, "end": 2333.0, "text": " You can see there are actually contextual cues in the sentence that could help you figure", "tokens": [509, 393, 536, 456, 366, 767, 35526, 32192, 294, 264, 8174, 300, 727, 854, 291, 2573], "temperature": 0.0, "avg_logprob": -0.12395774084946205, "compression_ratio": 1.9393939393939394, "no_speech_prob": 3.905417543137446e-06}, {"id": 642, "seek": 232972, "start": 2333.0, "end": 2336.68, "text": " out which triple you're talking about.", "tokens": [484, 597, 15508, 291, 434, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.12395774084946205, "compression_ratio": 1.9393939393939394, "no_speech_prob": 3.905417543137446e-06}, {"id": 643, "seek": 232972, "start": 2336.68, "end": 2340.48, "text": " And then given that your top scoring parent entity is super Mario Land and your top scoring", "tokens": [400, 550, 2212, 300, 428, 1192, 22358, 2596, 13977, 307, 1687, 9343, 6607, 293, 428, 1192, 22358], "temperature": 0.0, "avg_logprob": -0.12395774084946205, "compression_ratio": 1.9393939393939394, "no_speech_prob": 3.905417543137446e-06}, {"id": 644, "seek": 232972, "start": 2340.48, "end": 2345.48, "text": " relation is publisher, you can figure out that using knowledge graph triples, the tail", "tokens": [9721, 307, 25088, 11, 291, 393, 2573, 484, 300, 1228, 3601, 4295, 1376, 2622, 11, 264, 6838], "temperature": 0.0, "avg_logprob": -0.12395774084946205, "compression_ratio": 1.9393939393939394, "no_speech_prob": 3.905417543137446e-06}, {"id": 645, "seek": 232972, "start": 2345.48, "end": 2347.68, "text": " entity has to be Nintendo.", "tokens": [13977, 575, 281, 312, 11578, 13], "temperature": 0.0, "avg_logprob": -0.12395774084946205, "compression_ratio": 1.9393939393939394, "no_speech_prob": 3.905417543137446e-06}, {"id": 646, "seek": 232972, "start": 2347.68, "end": 2355.24, "text": " And therefore, this gives you a strong signal that the next word will be Nintendo.", "tokens": [400, 4412, 11, 341, 2709, 291, 257, 2068, 6358, 300, 264, 958, 1349, 486, 312, 11578, 13], "temperature": 0.0, "avg_logprob": -0.12395774084946205, "compression_ratio": 1.9393939393939394, "no_speech_prob": 3.905417543137446e-06}, {"id": 647, "seek": 232972, "start": 2355.24, "end": 2359.0, "text": " So the goal is you're going to find the top scoring parent entity and the top scoring relation", "tokens": [407, 264, 3387, 307, 291, 434, 516, 281, 915, 264, 1192, 22358, 2596, 13977, 293, 264, 1192, 22358, 9721], "temperature": 0.0, "avg_logprob": -0.12395774084946205, "compression_ratio": 1.9393939393939394, "no_speech_prob": 3.905417543137446e-06}, {"id": 648, "seek": 235900, "start": 2359.0, "end": 2361.32, "text": " using the nodes in your local knowledge graph.", "tokens": [1228, 264, 13891, 294, 428, 2654, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.16325308571399097, "compression_ratio": 1.8283828382838283, "no_speech_prob": 1.9524459275999106e-05}, {"id": 649, "seek": 235900, "start": 2361.32, "end": 2365.04, "text": " And you can do this by using the LSTM hidden state combined with pre-trained entity and", "tokens": [400, 291, 393, 360, 341, 538, 1228, 264, 441, 6840, 44, 7633, 1785, 9354, 365, 659, 12, 17227, 2001, 13977, 293], "temperature": 0.0, "avg_logprob": -0.16325308571399097, "compression_ratio": 1.8283828382838283, "no_speech_prob": 1.9524459275999106e-05}, {"id": 650, "seek": 235900, "start": 2365.04, "end": 2367.04, "text": " relation embeddings.", "tokens": [9721, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.16325308571399097, "compression_ratio": 1.8283828382838283, "no_speech_prob": 1.9524459275999106e-05}, {"id": 651, "seek": 235900, "start": 2367.04, "end": 2371.16, "text": " So I do admit I cheated here a little bit in that this does use pre-trained embeddings,", "tokens": [407, 286, 360, 9796, 286, 28079, 510, 257, 707, 857, 294, 300, 341, 775, 764, 659, 12, 17227, 2001, 12240, 29432, 11], "temperature": 0.0, "avg_logprob": -0.16325308571399097, "compression_ratio": 1.8283828382838283, "no_speech_prob": 1.9524459275999106e-05}, {"id": 652, "seek": 235900, "start": 2371.16, "end": 2374.48, "text": " but hopefully you'll see by the end of this discussion why I think it fits a bit better", "tokens": [457, 4696, 291, 603, 536, 538, 264, 917, 295, 341, 5017, 983, 286, 519, 309, 9001, 257, 857, 1101], "temperature": 0.0, "avg_logprob": -0.16325308571399097, "compression_ratio": 1.8283828382838283, "no_speech_prob": 1.9524459275999106e-05}, {"id": 653, "seek": 235900, "start": 2374.48, "end": 2379.04, "text": " in this external memory use case as well.", "tokens": [294, 341, 8320, 4675, 764, 1389, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16325308571399097, "compression_ratio": 1.8283828382838283, "no_speech_prob": 1.9524459275999106e-05}, {"id": 654, "seek": 235900, "start": 2379.04, "end": 2382.08, "text": " So what they're going to do is they're going to take a softmax using the LSTM hidden state", "tokens": [407, 437, 436, 434, 516, 281, 360, 307, 436, 434, 516, 281, 747, 257, 2787, 41167, 1228, 264, 441, 6840, 44, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.16325308571399097, "compression_ratio": 1.8283828382838283, "no_speech_prob": 1.9524459275999106e-05}, {"id": 655, "seek": 235900, "start": 2382.08, "end": 2386.08, "text": " and the entity embeddings for each of the potential parent entities and they'll take this", "tokens": [293, 264, 13977, 12240, 29432, 337, 1184, 295, 264, 3995, 2596, 16667, 293, 436, 603, 747, 341], "temperature": 0.0, "avg_logprob": -0.16325308571399097, "compression_ratio": 1.8283828382838283, "no_speech_prob": 1.9524459275999106e-05}, {"id": 656, "seek": 238608, "start": 2386.08, "end": 2392.24, "text": " top scoring one as a parent entity and they'll do the same thing for the relation embeddings.", "tokens": [1192, 22358, 472, 382, 257, 2596, 13977, 293, 436, 603, 360, 264, 912, 551, 337, 264, 9721, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.12974455621507433, "compression_ratio": 1.8527131782945736, "no_speech_prob": 1.3211397345003206e-05}, {"id": 657, "seek": 238608, "start": 2392.24, "end": 2396.24, "text": " The next entity is then just this tail entity from the knowledge graph triple.", "tokens": [440, 958, 13977, 307, 550, 445, 341, 6838, 13977, 490, 264, 3601, 4295, 15508, 13], "temperature": 0.0, "avg_logprob": -0.12974455621507433, "compression_ratio": 1.8527131782945736, "no_speech_prob": 1.3211397345003206e-05}, {"id": 658, "seek": 238608, "start": 2396.24, "end": 2400.2, "text": " So it's relatively trivial to figure out what the next entity should be once you've figured", "tokens": [407, 309, 311, 7226, 26703, 281, 2573, 484, 437, 264, 958, 13977, 820, 312, 1564, 291, 600, 8932], "temperature": 0.0, "avg_logprob": -0.12974455621507433, "compression_ratio": 1.8527131782945736, "no_speech_prob": 1.3211397345003206e-05}, {"id": 659, "seek": 238608, "start": 2400.2, "end": 2404.92, "text": " out the top scoring parent entity and your top scoring relation.", "tokens": [484, 264, 1192, 22358, 2596, 13977, 293, 428, 1192, 22358, 9721, 13], "temperature": 0.0, "avg_logprob": -0.12974455621507433, "compression_ratio": 1.8527131782945736, "no_speech_prob": 1.3211397345003206e-05}, {"id": 660, "seek": 238608, "start": 2404.92, "end": 2409.7999999999997, "text": " And then finally to predict the next word, they take the vocabulary and they expand it", "tokens": [400, 550, 2721, 281, 6069, 264, 958, 1349, 11, 436, 747, 264, 19864, 293, 436, 5268, 309], "temperature": 0.0, "avg_logprob": -0.12974455621507433, "compression_ratio": 1.8527131782945736, "no_speech_prob": 1.3211397345003206e-05}, {"id": 661, "seek": 238608, "start": 2409.7999999999997, "end": 2414.04, "text": " to include different aliases that could refer to that entity.", "tokens": [281, 4090, 819, 10198, 1957, 300, 727, 2864, 281, 300, 13977, 13], "temperature": 0.0, "avg_logprob": -0.12974455621507433, "compression_ratio": 1.8527131782945736, "no_speech_prob": 1.3211397345003206e-05}, {"id": 662, "seek": 241404, "start": 2414.04, "end": 2418.96, "text": " So what I mean by aliases here are phrases that could refer to the entity in text.", "tokens": [407, 437, 286, 914, 538, 10198, 1957, 510, 366, 20312, 300, 727, 2864, 281, 264, 13977, 294, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1381898980391653, "compression_ratio": 1.7046413502109705, "no_speech_prob": 1.6441581465187483e-05}, {"id": 663, "seek": 241404, "start": 2418.96, "end": 2423.56, "text": " So you might not just call it Nintendo, you might also say Nintendo Company or Copie", "tokens": [407, 291, 1062, 406, 445, 818, 309, 11578, 11, 291, 1062, 611, 584, 11578, 13918, 420, 11579, 414], "temperature": 0.0, "avg_logprob": -0.1381898980391653, "compression_ratio": 1.7046413502109705, "no_speech_prob": 1.6441581465187483e-05}, {"id": 664, "seek": 241404, "start": 2423.56, "end": 2428.92, "text": " and you want any of these to be possible words that you could predict as the next word.", "tokens": [293, 291, 528, 604, 295, 613, 281, 312, 1944, 2283, 300, 291, 727, 6069, 382, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1381898980391653, "compression_ratio": 1.7046413502109705, "no_speech_prob": 1.6441581465187483e-05}, {"id": 665, "seek": 241404, "start": 2428.92, "end": 2433.48, "text": " So the goal of this vocabulary expansion is to increase the probability that the next", "tokens": [407, 264, 3387, 295, 341, 19864, 11260, 307, 281, 3488, 264, 8482, 300, 264, 958], "temperature": 0.0, "avg_logprob": -0.1381898980391653, "compression_ratio": 1.7046413502109705, "no_speech_prob": 1.6441581465187483e-05}, {"id": 666, "seek": 241404, "start": 2433.48, "end": 2440.2799999999997, "text": " word you predict will actually be related to this next entity.", "tokens": [1349, 291, 6069, 486, 767, 312, 4077, 281, 341, 958, 13977, 13], "temperature": 0.0, "avg_logprob": -0.1381898980391653, "compression_ratio": 1.7046413502109705, "no_speech_prob": 1.6441581465187483e-05}, {"id": 667, "seek": 244028, "start": 2440.28, "end": 2444.1600000000003, "text": " So the new entity case is a bit simpler, this means that the entity that you're predicting", "tokens": [407, 264, 777, 13977, 1389, 307, 257, 857, 18587, 11, 341, 1355, 300, 264, 13977, 300, 291, 434, 32884], "temperature": 0.0, "avg_logprob": -0.1997735554935368, "compression_ratio": 1.9448529411764706, "no_speech_prob": 1.98323014046764e-05}, {"id": 668, "seek": 244028, "start": 2444.1600000000003, "end": 2445.5600000000004, "text": " is not in the local knowledge graph.", "tokens": [307, 406, 294, 264, 2654, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.1997735554935368, "compression_ratio": 1.9448529411764706, "no_speech_prob": 1.98323014046764e-05}, {"id": 669, "seek": 244028, "start": 2445.5600000000004, "end": 2448.6000000000004, "text": " So you're not getting any signal from this local knowledge graph that you've been building", "tokens": [407, 291, 434, 406, 1242, 604, 6358, 490, 341, 2654, 3601, 4295, 300, 291, 600, 668, 2390], "temperature": 0.0, "avg_logprob": -0.1997735554935368, "compression_ratio": 1.9448529411764706, "no_speech_prob": 1.98323014046764e-05}, {"id": 670, "seek": 244028, "start": 2448.6000000000004, "end": 2449.6000000000004, "text": " up.", "tokens": [493, 13], "temperature": 0.0, "avg_logprob": -0.1997735554935368, "compression_ratio": 1.9448529411764706, "no_speech_prob": 1.98323014046764e-05}, {"id": 671, "seek": 244028, "start": 2449.6000000000004, "end": 2454.1600000000003, "text": " And all you want to do is find the top scoring entity in the full knowledge graph and", "tokens": [400, 439, 291, 528, 281, 360, 307, 915, 264, 1192, 22358, 13977, 294, 264, 1577, 3601, 4295, 293], "temperature": 0.0, "avg_logprob": -0.1997735554935368, "compression_ratio": 1.9448529411764706, "no_speech_prob": 1.98323014046764e-05}, {"id": 672, "seek": 244028, "start": 2454.1600000000003, "end": 2458.1200000000003, "text": " you can do this using the LSTM hidden state and preaching and TMPeddings, similar to how", "tokens": [291, 393, 360, 341, 1228, 264, 441, 6840, 44, 7633, 1785, 293, 25381, 293, 314, 12224, 292, 29432, 11, 2531, 281, 577], "temperature": 0.0, "avg_logprob": -0.1997735554935368, "compression_ratio": 1.9448529411764706, "no_speech_prob": 1.98323014046764e-05}, {"id": 673, "seek": 244028, "start": 2458.1200000000003, "end": 2462.0800000000004, "text": " we found the score for the top parent entity.", "tokens": [321, 1352, 264, 6175, 337, 264, 1192, 2596, 13977, 13], "temperature": 0.0, "avg_logprob": -0.1997735554935368, "compression_ratio": 1.9448529411764706, "no_speech_prob": 1.98323014046764e-05}, {"id": 674, "seek": 244028, "start": 2462.0800000000004, "end": 2466.36, "text": " Your next entity will just be the top scoring entity out of the full knowledge graph.", "tokens": [2260, 958, 13977, 486, 445, 312, 264, 1192, 22358, 13977, 484, 295, 264, 1577, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.1997735554935368, "compression_ratio": 1.9448529411764706, "no_speech_prob": 1.98323014046764e-05}, {"id": 675, "seek": 246636, "start": 2466.36, "end": 2470.48, "text": " And then your next word is once again, this vocabulary expanded to include aliases of", "tokens": [400, 550, 428, 958, 1349, 307, 1564, 797, 11, 341, 19864, 14342, 281, 4090, 10198, 1957, 295], "temperature": 0.0, "avg_logprob": -0.19837699746185877, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.9478853775799507e-06}, {"id": 676, "seek": 246636, "start": 2470.48, "end": 2473.28, "text": " that entity.", "tokens": [300, 13977, 13], "temperature": 0.0, "avg_logprob": -0.19837699746185877, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.9478853775799507e-06}, {"id": 677, "seek": 246636, "start": 2473.28, "end": 2475.88, "text": " The not in the entity case is the simplest.", "tokens": [440, 406, 294, 264, 13977, 1389, 307, 264, 22811, 13], "temperature": 0.0, "avg_logprob": -0.19837699746185877, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.9478853775799507e-06}, {"id": 678, "seek": 246636, "start": 2475.88, "end": 2477.96, "text": " You just revert to normal LSTM.", "tokens": [509, 445, 319, 3281, 281, 2710, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.19837699746185877, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.9478853775799507e-06}, {"id": 679, "seek": 246636, "start": 2477.96, "end": 2480.1600000000003, "text": " You don't have an X entity to predict.", "tokens": [509, 500, 380, 362, 364, 1783, 13977, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.19837699746185877, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.9478853775799507e-06}, {"id": 680, "seek": 246636, "start": 2480.1600000000003, "end": 2487.1200000000003, "text": " And your next word is just the most likely next token over your normal vocabulary.", "tokens": [400, 428, 958, 1349, 307, 445, 264, 881, 3700, 958, 14862, 670, 428, 2710, 19864, 13], "temperature": 0.0, "avg_logprob": -0.19837699746185877, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.9478853775799507e-06}, {"id": 681, "seek": 246636, "start": 2487.1200000000003, "end": 2491.96, "text": " So here's a diagram from the paper that hopefully summarizes and makes even clearer what I just", "tokens": [407, 510, 311, 257, 10686, 490, 264, 3035, 300, 4696, 14611, 5660, 293, 1669, 754, 26131, 437, 286, 445], "temperature": 0.0, "avg_logprob": -0.19837699746185877, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.9478853775799507e-06}, {"id": 682, "seek": 246636, "start": 2491.96, "end": 2493.6, "text": " went over.", "tokens": [1437, 670, 13], "temperature": 0.0, "avg_logprob": -0.19837699746185877, "compression_ratio": 1.6184738955823292, "no_speech_prob": 2.9478853775799507e-06}, {"id": 683, "seek": 249360, "start": 2493.6, "end": 2497.2, "text": " So they have a longer example than the one we were looking at, but it's the same prediction", "tokens": [407, 436, 362, 257, 2854, 1365, 813, 264, 472, 321, 645, 1237, 412, 11, 457, 309, 311, 264, 912, 17630], "temperature": 0.0, "avg_logprob": -0.20789696022316262, "compression_ratio": 1.8517241379310345, "no_speech_prob": 1.952340426214505e-05}, {"id": 684, "seek": 249360, "start": 2497.2, "end": 2499.2, "text": " as Nintendo's next word.", "tokens": [382, 11578, 311, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.20789696022316262, "compression_ratio": 1.8517241379310345, "no_speech_prob": 1.952340426214505e-05}, {"id": 685, "seek": 249360, "start": 2499.2, "end": 2500.72, "text": " And they have their predictions in red.", "tokens": [400, 436, 362, 641, 21264, 294, 2182, 13], "temperature": 0.0, "avg_logprob": -0.20789696022316262, "compression_ratio": 1.8517241379310345, "no_speech_prob": 1.952340426214505e-05}, {"id": 686, "seek": 249360, "start": 2500.72, "end": 2503.2, "text": " So this is what they want KGLN to predict.", "tokens": [407, 341, 307, 437, 436, 528, 591, 19440, 45, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.20789696022316262, "compression_ratio": 1.8517241379310345, "no_speech_prob": 1.952340426214505e-05}, {"id": 687, "seek": 249360, "start": 2503.2, "end": 2505.7599999999998, "text": " The three different cases are in the horizontal.", "tokens": [440, 1045, 819, 3331, 366, 294, 264, 12750, 13], "temperature": 0.0, "avg_logprob": -0.20789696022316262, "compression_ratio": 1.8517241379310345, "no_speech_prob": 1.952340426214505e-05}, {"id": 688, "seek": 249360, "start": 2505.7599999999998, "end": 2510.6, "text": " And we see that here, you're in the related entity case, since Nintendo is in your local", "tokens": [400, 321, 536, 300, 510, 11, 291, 434, 294, 264, 4077, 13977, 1389, 11, 1670, 11578, 307, 294, 428, 2654], "temperature": 0.0, "avg_logprob": -0.20789696022316262, "compression_ratio": 1.8517241379310345, "no_speech_prob": 1.952340426214505e-05}, {"id": 689, "seek": 249360, "start": 2510.6, "end": 2512.52, "text": " knowledge graph.", "tokens": [3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.20789696022316262, "compression_ratio": 1.8517241379310345, "no_speech_prob": 1.952340426214505e-05}, {"id": 690, "seek": 249360, "start": 2512.52, "end": 2517.68, "text": " So they want KGLN to predict that Nintendo should be a related entity type of word, that", "tokens": [407, 436, 528, 591, 19440, 45, 281, 6069, 300, 11578, 820, 312, 257, 4077, 13977, 2010, 295, 1349, 11, 300], "temperature": 0.0, "avg_logprob": -0.20789696022316262, "compression_ratio": 1.8517241379310345, "no_speech_prob": 1.952340426214505e-05}, {"id": 691, "seek": 249360, "start": 2517.68, "end": 2523.3199999999997, "text": " Super Mario Land should be its parent entity, that publisher should be the relevant relation.", "tokens": [4548, 9343, 6607, 820, 312, 1080, 2596, 13977, 11, 300, 25088, 820, 312, 264, 7340, 9721, 13], "temperature": 0.0, "avg_logprob": -0.20789696022316262, "compression_ratio": 1.8517241379310345, "no_speech_prob": 1.952340426214505e-05}, {"id": 692, "seek": 252332, "start": 2523.32, "end": 2526.28, "text": " And as a result, the next entity is Nintendo.", "tokens": [400, 382, 257, 1874, 11, 264, 958, 13977, 307, 11578, 13], "temperature": 0.0, "avg_logprob": -0.18919718650079542, "compression_ratio": 1.6134751773049645, "no_speech_prob": 1.3418566595646553e-05}, {"id": 693, "seek": 252332, "start": 2526.28, "end": 2528.0, "text": " And then they expand the vocabulary.", "tokens": [400, 550, 436, 5268, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.18919718650079542, "compression_ratio": 1.6134751773049645, "no_speech_prob": 1.3418566595646553e-05}, {"id": 694, "seek": 252332, "start": 2528.0, "end": 2531.2400000000002, "text": " You see that aliases of Nintendo at the bottom.", "tokens": [509, 536, 300, 10198, 1957, 295, 11578, 412, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.18919718650079542, "compression_ratio": 1.6134751773049645, "no_speech_prob": 1.3418566595646553e-05}, {"id": 695, "seek": 252332, "start": 2531.2400000000002, "end": 2534.8, "text": " And then finally, they actually predict Nintendo is the next word.", "tokens": [400, 550, 2721, 11, 436, 767, 6069, 11578, 307, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.18919718650079542, "compression_ratio": 1.6134751773049645, "no_speech_prob": 1.3418566595646553e-05}, {"id": 696, "seek": 252332, "start": 2534.8, "end": 2540.28, "text": " And the other case is just summarized what we also already went over.", "tokens": [400, 264, 661, 1389, 307, 445, 14611, 1602, 437, 321, 611, 1217, 1437, 670, 13], "temperature": 0.0, "avg_logprob": -0.18919718650079542, "compression_ratio": 1.6134751773049645, "no_speech_prob": 1.3418566595646553e-05}, {"id": 697, "seek": 252332, "start": 2540.28, "end": 2547.1200000000003, "text": " So you find that KGLN actually outperforms GPT2 and AWD LSTM, which is a strong LSTM language", "tokens": [407, 291, 915, 300, 591, 19440, 45, 767, 484, 26765, 82, 26039, 51, 17, 293, 25815, 35, 441, 6840, 44, 11, 597, 307, 257, 2068, 441, 6840, 44, 2856], "temperature": 0.0, "avg_logprob": -0.18919718650079542, "compression_ratio": 1.6134751773049645, "no_speech_prob": 1.3418566595646553e-05}, {"id": 698, "seek": 252332, "start": 2547.1200000000003, "end": 2548.1200000000003, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.18919718650079542, "compression_ratio": 1.6134751773049645, "no_speech_prob": 1.3418566595646553e-05}, {"id": 699, "seek": 252332, "start": 2548.1200000000003, "end": 2551.7200000000003, "text": " On a fact completion task, similar to the fill in the blank examples that we looked at", "tokens": [1282, 257, 1186, 19372, 5633, 11, 2531, 281, 264, 2836, 294, 264, 8247, 5110, 300, 321, 2956, 412], "temperature": 0.0, "avg_logprob": -0.18919718650079542, "compression_ratio": 1.6134751773049645, "no_speech_prob": 1.3418566595646553e-05}, {"id": 700, "seek": 255172, "start": 2551.72, "end": 2558.08, "text": " at the beginning of the talk, they also find qualitatively that compared to GPT2, KGLN", "tokens": [412, 264, 2863, 295, 264, 751, 11, 436, 611, 915, 31312, 356, 300, 5347, 281, 26039, 51, 17, 11, 591, 19440, 45], "temperature": 0.0, "avg_logprob": -0.13224106743222191, "compression_ratio": 1.7003367003367003, "no_speech_prob": 2.2825181076768786e-05}, {"id": 701, "seek": 255172, "start": 2558.08, "end": 2563.16, "text": " tends to predict more specific tokens, since it can predict these tokens from just copying", "tokens": [12258, 281, 6069, 544, 2685, 22667, 11, 1670, 309, 393, 6069, 613, 22667, 490, 445, 27976], "temperature": 0.0, "avg_logprob": -0.13224106743222191, "compression_ratio": 1.7003367003367003, "no_speech_prob": 2.2825181076768786e-05}, {"id": 702, "seek": 255172, "start": 2563.16, "end": 2564.52, "text": " from the local knowledge graph.", "tokens": [490, 264, 2654, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.13224106743222191, "compression_ratio": 1.7003367003367003, "no_speech_prob": 2.2825181076768786e-05}, {"id": 703, "seek": 255172, "start": 2564.52, "end": 2567.9599999999996, "text": " Whereas GPT2 will tend to predict more generic tokens.", "tokens": [13813, 26039, 51, 17, 486, 3928, 281, 6069, 544, 19577, 22667, 13], "temperature": 0.0, "avg_logprob": -0.13224106743222191, "compression_ratio": 1.7003367003367003, "no_speech_prob": 2.2825181076768786e-05}, {"id": 704, "seek": 255172, "start": 2567.9599999999996, "end": 2571.3199999999997, "text": " So if you want to predict the birthplace of someone, GPT2 is more likely to predict", "tokens": [407, 498, 291, 528, 281, 6069, 264, 3965, 6742, 295, 1580, 11, 26039, 51, 17, 307, 544, 3700, 281, 6069], "temperature": 0.0, "avg_logprob": -0.13224106743222191, "compression_ratio": 1.7003367003367003, "no_speech_prob": 2.2825181076768786e-05}, {"id": 705, "seek": 255172, "start": 2571.3199999999997, "end": 2577.2, "text": " New York, for example, and KGLN might predict some obscure place.", "tokens": [1873, 3609, 11, 337, 1365, 11, 293, 591, 19440, 45, 1062, 6069, 512, 34443, 1081, 13], "temperature": 0.0, "avg_logprob": -0.13224106743222191, "compression_ratio": 1.7003367003367003, "no_speech_prob": 2.2825181076768786e-05}, {"id": 706, "seek": 255172, "start": 2577.2, "end": 2580.56, "text": " And then they have these really cool set of experiments where they show that KGLN actually", "tokens": [400, 550, 436, 362, 613, 534, 1627, 992, 295, 12050, 689, 436, 855, 300, 591, 19440, 45, 767], "temperature": 0.0, "avg_logprob": -0.13224106743222191, "compression_ratio": 1.7003367003367003, "no_speech_prob": 2.2825181076768786e-05}, {"id": 707, "seek": 258056, "start": 2580.56, "end": 2583.84, "text": " supports modifying or updating facts.", "tokens": [9346, 42626, 420, 25113, 9130, 13], "temperature": 0.0, "avg_logprob": -0.19914869676556504, "compression_ratio": 1.797709923664122, "no_speech_prob": 3.7047448131488636e-05}, {"id": 708, "seek": 258056, "start": 2583.84, "end": 2587.36, "text": " So they made a direct change in the knowledge graph, and then they saw what is the change", "tokens": [407, 436, 1027, 257, 2047, 1319, 294, 264, 3601, 4295, 11, 293, 550, 436, 1866, 437, 307, 264, 1319], "temperature": 0.0, "avg_logprob": -0.19914869676556504, "compression_ratio": 1.797709923664122, "no_speech_prob": 3.7047448131488636e-05}, {"id": 709, "seek": 258056, "start": 2587.36, "end": 2590.24, "text": " in KGLN's predictions.", "tokens": [294, 591, 19440, 45, 311, 21264, 13], "temperature": 0.0, "avg_logprob": -0.19914869676556504, "compression_ratio": 1.797709923664122, "no_speech_prob": 3.7047448131488636e-05}, {"id": 710, "seek": 258056, "start": 2590.24, "end": 2595.72, "text": " So they have this example where the sequence was Barack Obama is born on blank.", "tokens": [407, 436, 362, 341, 1365, 689, 264, 8310, 390, 31705, 9560, 307, 4232, 322, 8247, 13], "temperature": 0.0, "avg_logprob": -0.19914869676556504, "compression_ratio": 1.797709923664122, "no_speech_prob": 3.7047448131488636e-05}, {"id": 711, "seek": 258056, "start": 2595.72, "end": 2599.32, "text": " They had their knowledge graph triple as Barack Obama's original birth date, and then", "tokens": [814, 632, 641, 3601, 4295, 15508, 382, 31705, 9560, 311, 3380, 3965, 4002, 11, 293, 550], "temperature": 0.0, "avg_logprob": -0.19914869676556504, "compression_ratio": 1.797709923664122, "no_speech_prob": 3.7047448131488636e-05}, {"id": 712, "seek": 258056, "start": 2599.32, "end": 2604.12, "text": " their most likely next tokens were as expected, August 4, 1961.", "tokens": [641, 881, 3700, 958, 22667, 645, 382, 5176, 11, 6897, 1017, 11, 41720, 13], "temperature": 0.0, "avg_logprob": -0.19914869676556504, "compression_ratio": 1.797709923664122, "no_speech_prob": 3.7047448131488636e-05}, {"id": 713, "seek": 258056, "start": 2604.12, "end": 2606.12, "text": " And then they just changed their knowledge graph.", "tokens": [400, 550, 436, 445, 3105, 641, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.19914869676556504, "compression_ratio": 1.797709923664122, "no_speech_prob": 3.7047448131488636e-05}, {"id": 714, "seek": 258056, "start": 2606.12, "end": 2608.04, "text": " So they changed the birth date of Obama.", "tokens": [407, 436, 3105, 264, 3965, 4002, 295, 9560, 13], "temperature": 0.0, "avg_logprob": -0.19914869676556504, "compression_ratio": 1.797709923664122, "no_speech_prob": 3.7047448131488636e-05}, {"id": 715, "seek": 260804, "start": 2608.04, "end": 2610.8, "text": " And they said, OK, he's now born 2013.", "tokens": [400, 436, 848, 11, 2264, 11, 415, 311, 586, 4232, 9012, 13], "temperature": 0.0, "avg_logprob": -0.2303269283830627, "compression_ratio": 1.6608391608391608, "no_speech_prob": 3.1690742616774514e-05}, {"id": 716, "seek": 260804, "start": 2610.8, "end": 2615.56, "text": " And they looked to see what the next predictions were for KGLN, and it changed its predictions", "tokens": [400, 436, 2956, 281, 536, 437, 264, 958, 21264, 645, 337, 591, 19440, 45, 11, 293, 309, 3105, 1080, 21264], "temperature": 0.0, "avg_logprob": -0.2303269283830627, "compression_ratio": 1.6608391608391608, "no_speech_prob": 3.1690742616774514e-05}, {"id": 717, "seek": 260804, "start": 2615.56, "end": 2618.56, "text": " to match what was in the local knowledge graph.", "tokens": [281, 2995, 437, 390, 294, 264, 2654, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.2303269283830627, "compression_ratio": 1.6608391608391608, "no_speech_prob": 3.1690742616774514e-05}, {"id": 718, "seek": 260804, "start": 2618.56, "end": 2622.88, "text": " So this is something that's pretty cool, and that really only external memory approaches", "tokens": [407, 341, 307, 746, 300, 311, 1238, 1627, 11, 293, 300, 534, 787, 8320, 4675, 11587], "temperature": 0.0, "avg_logprob": -0.2303269283830627, "compression_ratio": 1.6608391608391608, "no_speech_prob": 3.1690742616774514e-05}, {"id": 719, "seek": 260804, "start": 2622.88, "end": 2626.52, "text": " can do compared to these to the original pre-chain etian betting approach we talked", "tokens": [393, 360, 5347, 281, 613, 281, 264, 3380, 659, 12, 11509, 1030, 952, 34246, 3109, 321, 2825], "temperature": 0.0, "avg_logprob": -0.2303269283830627, "compression_ratio": 1.6608391608391608, "no_speech_prob": 3.1690742616774514e-05}, {"id": 720, "seek": 260804, "start": 2626.52, "end": 2627.52, "text": " about.", "tokens": [466, 13], "temperature": 0.0, "avg_logprob": -0.2303269283830627, "compression_ratio": 1.6608391608391608, "no_speech_prob": 3.1690742616774514e-05}, {"id": 721, "seek": 260804, "start": 2627.52, "end": 2631.36, "text": " And I think it's one of the reasons that KGLN at least my opinion fits better in these", "tokens": [400, 286, 519, 309, 311, 472, 295, 264, 4112, 300, 591, 19440, 45, 412, 1935, 452, 4800, 9001, 1101, 294, 613], "temperature": 0.0, "avg_logprob": -0.2303269283830627, "compression_ratio": 1.6608391608391608, "no_speech_prob": 3.1690742616774514e-05}, {"id": 722, "seek": 260804, "start": 2631.36, "end": 2633.7599999999998, "text": " external memory use cases.", "tokens": [8320, 4675, 764, 3331, 13], "temperature": 0.0, "avg_logprob": -0.2303269283830627, "compression_ratio": 1.6608391608391608, "no_speech_prob": 3.1690742616774514e-05}, {"id": 723, "seek": 263376, "start": 2633.76, "end": 2638.88, "text": " Right, so the next slide is a different paper.", "tokens": [1779, 11, 370, 264, 958, 4137, 307, 257, 819, 3035, 13], "temperature": 0.0, "avg_logprob": -0.31889007205054876, "compression_ratio": 1.4975369458128078, "no_speech_prob": 4.264281233190559e-05}, {"id": 724, "seek": 263376, "start": 2638.88, "end": 2641.36, "text": " So I guess I'll take questions on KGLN.", "tokens": [407, 286, 2041, 286, 603, 747, 1651, 322, 591, 19440, 45, 13], "temperature": 0.0, "avg_logprob": -0.31889007205054876, "compression_ratio": 1.4975369458128078, "no_speech_prob": 4.264281233190559e-05}, {"id": 725, "seek": 263376, "start": 2641.36, "end": 2644.36, "text": " Is there any?", "tokens": [1119, 456, 604, 30], "temperature": 0.0, "avg_logprob": -0.31889007205054876, "compression_ratio": 1.4975369458128078, "no_speech_prob": 4.264281233190559e-05}, {"id": 726, "seek": 263376, "start": 2644.36, "end": 2650.36, "text": " It's a pretty complex method, so feel free to have questions.", "tokens": [467, 311, 257, 1238, 3997, 3170, 11, 370, 841, 1737, 281, 362, 1651, 13], "temperature": 0.0, "avg_logprob": -0.31889007205054876, "compression_ratio": 1.4975369458128078, "no_speech_prob": 4.264281233190559e-05}, {"id": 727, "seek": 263376, "start": 2650.36, "end": 2655.44, "text": " Yeah, could you one more time explain what the definition of the local knowledge graph", "tokens": [865, 11, 727, 291, 472, 544, 565, 2903, 437, 264, 7123, 295, 264, 2654, 3601, 4295], "temperature": 0.0, "avg_logprob": -0.31889007205054876, "compression_ratio": 1.4975369458128078, "no_speech_prob": 4.264281233190559e-05}, {"id": 728, "seek": 263376, "start": 2655.44, "end": 2658.2000000000003, "text": " is in relationship to the global knowledge graph?", "tokens": [307, 294, 2480, 281, 264, 4338, 3601, 4295, 30], "temperature": 0.0, "avg_logprob": -0.31889007205054876, "compression_ratio": 1.4975369458128078, "no_speech_prob": 4.264281233190559e-05}, {"id": 729, "seek": 263376, "start": 2658.2000000000003, "end": 2660.8, "text": " Yep.", "tokens": [7010, 13], "temperature": 0.0, "avg_logprob": -0.31889007205054876, "compression_ratio": 1.4975369458128078, "no_speech_prob": 4.264281233190559e-05}, {"id": 730, "seek": 266080, "start": 2660.8, "end": 2664.8, "text": " So local knowledge graph is supposed to be a subset of the full knowledge graph, and", "tokens": [407, 2654, 3601, 4295, 307, 3442, 281, 312, 257, 25993, 295, 264, 1577, 3601, 4295, 11, 293], "temperature": 0.0, "avg_logprob": -0.2352881542471952, "compression_ratio": 1.894736842105263, "no_speech_prob": 2.6272920877090655e-05}, {"id": 731, "seek": 266080, "start": 2664.8, "end": 2668.6000000000004, "text": " it's only supposed to consist of entities that are actually have actually been seen in", "tokens": [309, 311, 787, 3442, 281, 4603, 295, 16667, 300, 366, 767, 362, 767, 668, 1612, 294], "temperature": 0.0, "avg_logprob": -0.2352881542471952, "compression_ratio": 1.894736842105263, "no_speech_prob": 2.6272920877090655e-05}, {"id": 732, "seek": 266080, "start": 2668.6000000000004, "end": 2675.6400000000003, "text": " the sequence as well as their relevant entities.", "tokens": [264, 8310, 382, 731, 382, 641, 7340, 16667, 13], "temperature": 0.0, "avg_logprob": -0.2352881542471952, "compression_ratio": 1.894736842105263, "no_speech_prob": 2.6272920877090655e-05}, {"id": 733, "seek": 266080, "start": 2675.6400000000003, "end": 2683.4, "text": " OK, so here you see that SuperMario land is in the local knowledge graph because SuperMario", "tokens": [2264, 11, 370, 510, 291, 536, 300, 4548, 44, 4912, 2117, 307, 294, 264, 2654, 3601, 4295, 570, 4548, 44, 4912], "temperature": 0.0, "avg_logprob": -0.2352881542471952, "compression_ratio": 1.894736842105263, "no_speech_prob": 2.6272920877090655e-05}, {"id": 734, "seek": 266080, "start": 2683.4, "end": 2685.92, "text": " land is an entity that is seen in the sequence.", "tokens": [2117, 307, 364, 13977, 300, 307, 1612, 294, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.2352881542471952, "compression_ratio": 1.894736842105263, "no_speech_prob": 2.6272920877090655e-05}, {"id": 735, "seek": 268592, "start": 2685.92, "end": 2690.88, "text": " And then you also want to copy over all the edges from SuperMario land that would be in", "tokens": [400, 550, 291, 611, 528, 281, 5055, 670, 439, 264, 8819, 490, 4548, 44, 4912, 2117, 300, 576, 312, 294], "temperature": 0.0, "avg_logprob": -0.18166272198712385, "compression_ratio": 1.9244604316546763, "no_speech_prob": 1.4509615539282095e-05}, {"id": 736, "seek": 268592, "start": 2690.88, "end": 2692.4, "text": " the full knowledge graph.", "tokens": [264, 1577, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.18166272198712385, "compression_ratio": 1.9244604316546763, "no_speech_prob": 1.4509615539282095e-05}, {"id": 737, "seek": 268592, "start": 2692.4, "end": 2695.96, "text": " So this is just a subset of them for the purpose of the example, but you see that SuperMario", "tokens": [407, 341, 307, 445, 257, 25993, 295, 552, 337, 264, 4334, 295, 264, 1365, 11, 457, 291, 536, 300, 4548, 44, 4912], "temperature": 0.0, "avg_logprob": -0.18166272198712385, "compression_ratio": 1.9244604316546763, "no_speech_prob": 1.4509615539282095e-05}, {"id": 738, "seek": 268592, "start": 2695.96, "end": 2699.44, "text": " land has an edge in Tendo to gain void platform gain.", "tokens": [2117, 575, 364, 4691, 294, 314, 3999, 281, 6052, 22009, 3663, 6052, 13], "temperature": 0.0, "avg_logprob": -0.18166272198712385, "compression_ratio": 1.9244604316546763, "no_speech_prob": 1.4509615539282095e-05}, {"id": 739, "seek": 268592, "start": 2699.44, "end": 2703.44, "text": " And so you would copy all edges that SuperMario land has to another node in the full knowledge", "tokens": [400, 370, 291, 576, 5055, 439, 8819, 300, 4548, 44, 4912, 2117, 575, 281, 1071, 9984, 294, 264, 1577, 3601], "temperature": 0.0, "avg_logprob": -0.18166272198712385, "compression_ratio": 1.9244604316546763, "no_speech_prob": 1.4509615539282095e-05}, {"id": 740, "seek": 268592, "start": 2703.44, "end": 2704.44, "text": " graph.", "tokens": [4295, 13], "temperature": 0.0, "avg_logprob": -0.18166272198712385, "compression_ratio": 1.9244604316546763, "no_speech_prob": 1.4509615539282095e-05}, {"id": 741, "seek": 268592, "start": 2704.44, "end": 2709.08, "text": " And they know in advance like they have the labels here for what the entities are during", "tokens": [400, 436, 458, 294, 7295, 411, 436, 362, 264, 16949, 510, 337, 437, 264, 16667, 366, 1830], "temperature": 0.0, "avg_logprob": -0.18166272198712385, "compression_ratio": 1.9244604316546763, "no_speech_prob": 1.4509615539282095e-05}, {"id": 742, "seek": 268592, "start": 2709.08, "end": 2710.08, "text": " training.", "tokens": [3097, 13], "temperature": 0.0, "avg_logprob": -0.18166272198712385, "compression_ratio": 1.9244604316546763, "no_speech_prob": 1.4509615539282095e-05}, {"id": 743, "seek": 268592, "start": 2710.08, "end": 2714.44, "text": " So that's how they can actually create this ground truth knowledge graph.", "tokens": [407, 300, 311, 577, 436, 393, 767, 1884, 341, 2727, 3494, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.18166272198712385, "compression_ratio": 1.9244604316546763, "no_speech_prob": 1.4509615539282095e-05}, {"id": 744, "seek": 271444, "start": 2714.44, "end": 2720.0, "text": " And briefly, a student asked why we can't just use the whole knowledge graph and I gave", "tokens": [400, 10515, 11, 257, 3107, 2351, 983, 321, 393, 380, 445, 764, 264, 1379, 3601, 4295, 293, 286, 2729], "temperature": 0.0, "avg_logprob": -0.1780266984600887, "compression_ratio": 1.74, "no_speech_prob": 2.7106480047223158e-05}, {"id": 745, "seek": 271444, "start": 2720.0, "end": 2722.6, "text": " an answer, but maybe you know better.", "tokens": [364, 1867, 11, 457, 1310, 291, 458, 1101, 13], "temperature": 0.0, "avg_logprob": -0.1780266984600887, "compression_ratio": 1.74, "no_speech_prob": 2.7106480047223158e-05}, {"id": 746, "seek": 271444, "start": 2722.6, "end": 2727.48, "text": " Yeah, I think the idea is the signal will be much stronger if you just use local knowledge", "tokens": [865, 11, 286, 519, 264, 1558, 307, 264, 6358, 486, 312, 709, 7249, 498, 291, 445, 764, 2654, 3601], "temperature": 0.0, "avg_logprob": -0.1780266984600887, "compression_ratio": 1.74, "no_speech_prob": 2.7106480047223158e-05}, {"id": 747, "seek": 271444, "start": 2727.48, "end": 2728.48, "text": " graph.", "tokens": [4295, 13], "temperature": 0.0, "avg_logprob": -0.1780266984600887, "compression_ratio": 1.74, "no_speech_prob": 2.7106480047223158e-05}, {"id": 748, "seek": 271444, "start": 2728.48, "end": 2736.12, "text": " So in the softmax for the related entity case, you would just be predicting over the potential", "tokens": [407, 294, 264, 2787, 41167, 337, 264, 4077, 13977, 1389, 11, 291, 576, 445, 312, 32884, 670, 264, 3995], "temperature": 0.0, "avg_logprob": -0.1780266984600887, "compression_ratio": 1.74, "no_speech_prob": 2.7106480047223158e-05}, {"id": 749, "seek": 271444, "start": 2736.12, "end": 2739.0, "text": " parent entities in your local knowledge graph, which is a much smaller set than what's in", "tokens": [2596, 16667, 294, 428, 2654, 3601, 4295, 11, 597, 307, 257, 709, 4356, 992, 813, 437, 311, 294], "temperature": 0.0, "avg_logprob": -0.1780266984600887, "compression_ratio": 1.74, "no_speech_prob": 2.7106480047223158e-05}, {"id": 750, "seek": 271444, "start": 2739.0, "end": 2741.48, "text": " your full knowledge graph.", "tokens": [428, 1577, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.1780266984600887, "compression_ratio": 1.74, "no_speech_prob": 2.7106480047223158e-05}, {"id": 751, "seek": 274148, "start": 2741.48, "end": 2744.84, "text": " So I guess it's more likely that you're going to predict something that is correct in", "tokens": [407, 286, 2041, 309, 311, 544, 3700, 300, 291, 434, 516, 281, 6069, 746, 300, 307, 3006, 294], "temperature": 0.0, "avg_logprob": -0.21276702255499166, "compression_ratio": 1.7014388489208634, "no_speech_prob": 2.9309612727956846e-05}, {"id": 752, "seek": 274148, "start": 2744.84, "end": 2745.84, "text": " that case.", "tokens": [300, 1389, 13], "temperature": 0.0, "avg_logprob": -0.21276702255499166, "compression_ratio": 1.7014388489208634, "no_speech_prob": 2.9309612727956846e-05}, {"id": 753, "seek": 274148, "start": 2745.84, "end": 2749.32, "text": " Then when you have like 5 million or so entities in your full knowledge graph, it's also", "tokens": [1396, 562, 291, 362, 411, 1025, 2459, 420, 370, 16667, 294, 428, 1577, 3601, 4295, 11, 309, 311, 611], "temperature": 0.0, "avg_logprob": -0.21276702255499166, "compression_ratio": 1.7014388489208634, "no_speech_prob": 2.9309612727956846e-05}, {"id": 754, "seek": 274148, "start": 2749.32, "end": 2751.64, "text": " much cheaper to compute.", "tokens": [709, 12284, 281, 14722, 13], "temperature": 0.0, "avg_logprob": -0.21276702255499166, "compression_ratio": 1.7014388489208634, "no_speech_prob": 2.9309612727956846e-05}, {"id": 755, "seek": 274148, "start": 2751.64, "end": 2754.76, "text": " In this case, there's only a single parent entity, but you could have multiple parent entities", "tokens": [682, 341, 1389, 11, 456, 311, 787, 257, 2167, 2596, 13977, 11, 457, 291, 727, 362, 3866, 2596, 16667], "temperature": 0.0, "avg_logprob": -0.21276702255499166, "compression_ratio": 1.7014388489208634, "no_speech_prob": 2.9309612727956846e-05}, {"id": 756, "seek": 274148, "start": 2754.76, "end": 2757.6, "text": " that you're trying to compute, which one's most likely over.", "tokens": [300, 291, 434, 1382, 281, 14722, 11, 597, 472, 311, 881, 3700, 670, 13], "temperature": 0.0, "avg_logprob": -0.21276702255499166, "compression_ratio": 1.7014388489208634, "no_speech_prob": 2.9309612727956846e-05}, {"id": 757, "seek": 274148, "start": 2757.6, "end": 2760.28, "text": " Is that what you were also thinking?", "tokens": [1119, 300, 437, 291, 645, 611, 1953, 30], "temperature": 0.0, "avg_logprob": -0.21276702255499166, "compression_ratio": 1.7014388489208634, "no_speech_prob": 2.9309612727956846e-05}, {"id": 758, "seek": 274148, "start": 2760.28, "end": 2765.36, "text": " Yeah, I mainly just said efficiency.", "tokens": [865, 11, 286, 8704, 445, 848, 10493, 13], "temperature": 0.0, "avg_logprob": -0.21276702255499166, "compression_ratio": 1.7014388489208634, "no_speech_prob": 2.9309612727956846e-05}, {"id": 759, "seek": 274148, "start": 2765.36, "end": 2767.28, "text": " So the signal thing is cool too.", "tokens": [407, 264, 6358, 551, 307, 1627, 886, 13], "temperature": 0.0, "avg_logprob": -0.21276702255499166, "compression_ratio": 1.7014388489208634, "no_speech_prob": 2.9309612727956846e-05}, {"id": 760, "seek": 276728, "start": 2767.28, "end": 2774.2000000000003, "text": " Who's an exciting question, what about queries that require more than one step in the", "tokens": [2102, 311, 364, 4670, 1168, 11, 437, 466, 24109, 300, 3651, 544, 813, 472, 1823, 294, 264], "temperature": 0.0, "avg_logprob": -0.34596200240285774, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.882757300743833e-05}, {"id": 761, "seek": 276728, "start": 2774.2000000000003, "end": 2781.1200000000003, "text": " knowledge graph, such as the location of the publisher of Superrario Land?", "tokens": [3601, 4295, 11, 1270, 382, 264, 4914, 295, 264, 25088, 295, 4548, 81, 4912, 6607, 30], "temperature": 0.0, "avg_logprob": -0.34596200240285774, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.882757300743833e-05}, {"id": 762, "seek": 276728, "start": 2781.1200000000003, "end": 2785.5600000000004, "text": " Yeah, that's a good question.", "tokens": [865, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.34596200240285774, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.882757300743833e-05}, {"id": 763, "seek": 276728, "start": 2785.5600000000004, "end": 2787.84, "text": " So the idea is like can it support those types?", "tokens": [407, 264, 1558, 307, 411, 393, 309, 1406, 729, 3467, 30], "temperature": 0.0, "avg_logprob": -0.34596200240285774, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.882757300743833e-05}, {"id": 764, "seek": 276728, "start": 2787.84, "end": 2792.4, "text": " Does it support multi-hop kind of building of the knowledge graph?", "tokens": [4402, 309, 1406, 4825, 12, 9050, 733, 295, 2390, 295, 264, 3601, 4295, 30], "temperature": 0.0, "avg_logprob": -0.34596200240285774, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.882757300743833e-05}, {"id": 765, "seek": 276728, "start": 2792.4, "end": 2796.2400000000002, "text": " Yeah, yeah, it's like KGLM perform in those cases.", "tokens": [865, 11, 1338, 11, 309, 311, 411, 591, 19440, 44, 2042, 294, 729, 3331, 13], "temperature": 0.0, "avg_logprob": -0.34596200240285774, "compression_ratio": 1.575221238938053, "no_speech_prob": 3.882757300743833e-05}, {"id": 766, "seek": 279624, "start": 2796.24, "end": 2797.8799999999997, "text": " Yeah, I don't know.", "tokens": [865, 11, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 767, "seek": 279624, "start": 2797.8799999999997, "end": 2799.2, "text": " That's a very good question.", "tokens": [663, 311, 257, 588, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 768, "seek": 279624, "start": 2799.2, "end": 2803.12, "text": " They build up the knowledge graph, so that is just single hop as far as I know.", "tokens": [814, 1322, 493, 264, 3601, 4295, 11, 370, 300, 307, 445, 2167, 3818, 382, 1400, 382, 286, 458, 13], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 769, "seek": 279624, "start": 2803.12, "end": 2807.64, "text": " But like if you saw the other entities, if you were to see the entities along the hops,", "tokens": [583, 411, 498, 291, 1866, 264, 661, 16667, 11, 498, 291, 645, 281, 536, 264, 16667, 2051, 264, 47579, 11], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 770, "seek": 279624, "start": 2807.64, "end": 2809.8799999999997, "text": " it would have them in the local knowledge graph.", "tokens": [309, 576, 362, 552, 294, 264, 2654, 3601, 4295, 13], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 771, "seek": 279624, "start": 2809.8799999999997, "end": 2811.4399999999996, "text": " Yeah, that's a good question.", "tokens": [865, 11, 300, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 772, "seek": 279624, "start": 2811.4399999999996, "end": 2814.4399999999996, "text": " I don't know if they explored that.", "tokens": [286, 500, 380, 458, 498, 436, 24016, 300, 13], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 773, "seek": 279624, "start": 2814.4399999999996, "end": 2815.4399999999996, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 774, "seek": 279624, "start": 2815.4399999999996, "end": 2816.4399999999996, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 775, "seek": 279624, "start": 2816.4399999999996, "end": 2825.04, "text": " Let's move along then.", "tokens": [961, 311, 1286, 2051, 550, 13], "temperature": 0.0, "avg_logprob": -0.30115580558776855, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.068731141160242e-05}, {"id": 776, "seek": 282504, "start": 2825.04, "end": 2826.04, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.24211576552618116, "compression_ratio": 1.6653386454183268, "no_speech_prob": 9.459775174036622e-05}, {"id": 777, "seek": 282504, "start": 2826.04, "end": 2834.08, "text": " So the next piece of work we're going to talk about, you guys have actually briefly seen", "tokens": [407, 264, 958, 2522, 295, 589, 321, 434, 516, 281, 751, 466, 11, 291, 1074, 362, 767, 10515, 1612], "temperature": 0.0, "avg_logprob": -0.24211576552618116, "compression_ratio": 1.6653386454183268, "no_speech_prob": 9.459775174036622e-05}, {"id": 778, "seek": 282504, "start": 2834.08, "end": 2840.12, "text": " in the natural language generation lecture, but I'm going to go over it again quickly here.", "tokens": [294, 264, 3303, 2856, 5125, 7991, 11, 457, 286, 478, 516, 281, 352, 670, 309, 797, 2661, 510, 13], "temperature": 0.0, "avg_logprob": -0.24211576552618116, "compression_ratio": 1.6653386454183268, "no_speech_prob": 9.459775174036622e-05}, {"id": 779, "seek": 282504, "start": 2840.12, "end": 2843.44, "text": " So unlike the other work, so you talked about that use knowledge graph, Chipples, this", "tokens": [407, 8343, 264, 661, 589, 11, 370, 291, 2825, 466, 300, 764, 3601, 4295, 11, 761, 2488, 904, 11, 341], "temperature": 0.0, "avg_logprob": -0.24211576552618116, "compression_ratio": 1.6653386454183268, "no_speech_prob": 9.459775174036622e-05}, {"id": 780, "seek": 282504, "start": 2843.44, "end": 2847.52, "text": " is actually going to take kind of a looser notion of knowledge in that the knowledge will", "tokens": [307, 767, 516, 281, 747, 733, 295, 257, 450, 22150, 10710, 295, 3601, 294, 300, 264, 3601, 486], "temperature": 0.0, "avg_logprob": -0.24211576552618116, "compression_ratio": 1.6653386454183268, "no_speech_prob": 9.459775174036622e-05}, {"id": 781, "seek": 282504, "start": 2847.52, "end": 2850.72, "text": " just be encoded in the text and the training data set.", "tokens": [445, 312, 2058, 12340, 294, 264, 2487, 293, 264, 3097, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.24211576552618116, "compression_ratio": 1.6653386454183268, "no_speech_prob": 9.459775174036622e-05}, {"id": 782, "seek": 285072, "start": 2850.72, "end": 2856.6, "text": " So this is called K&N LM and the idea is that or it's building an idea that language models", "tokens": [407, 341, 307, 1219, 591, 5, 45, 46529, 293, 264, 1558, 307, 300, 420, 309, 311, 2390, 364, 1558, 300, 2856, 5245], "temperature": 0.0, "avg_logprob": -0.17913514709472655, "compression_ratio": 1.8673835125448028, "no_speech_prob": 1.3006125300307758e-05}, {"id": 783, "seek": 285072, "start": 2856.6, "end": 2860.72, "text": " not only learn to predict the next word in text, but they also learn these representations", "tokens": [406, 787, 1466, 281, 6069, 264, 958, 1349, 294, 2487, 11, 457, 436, 611, 1466, 613, 33358], "temperature": 0.0, "avg_logprob": -0.17913514709472655, "compression_ratio": 1.8673835125448028, "no_speech_prob": 1.3006125300307758e-05}, {"id": 784, "seek": 285072, "start": 2860.72, "end": 2861.72, "text": " of text.", "tokens": [295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.17913514709472655, "compression_ratio": 1.8673835125448028, "no_speech_prob": 1.3006125300307758e-05}, {"id": 785, "seek": 285072, "start": 2861.72, "end": 2866.1, "text": " And the authors suggest that it might actually be easier to learn similarities between", "tokens": [400, 264, 16552, 3402, 300, 309, 1062, 767, 312, 3571, 281, 1466, 24197, 1296], "temperature": 0.0, "avg_logprob": -0.17913514709472655, "compression_ratio": 1.8673835125448028, "no_speech_prob": 1.3006125300307758e-05}, {"id": 786, "seek": 285072, "start": 2866.1, "end": 2870.8399999999997, "text": " text sequences than it is to predict the next word in the text.", "tokens": [2487, 22978, 813, 309, 307, 281, 6069, 264, 958, 1349, 294, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.17913514709472655, "compression_ratio": 1.8673835125448028, "no_speech_prob": 1.3006125300307758e-05}, {"id": 787, "seek": 285072, "start": 2870.8399999999997, "end": 2875.3199999999997, "text": " So you have this example that Dickens is the author of blank and Dickens wrote blank.", "tokens": [407, 291, 362, 341, 1365, 300, 18754, 694, 307, 264, 3793, 295, 8247, 293, 18754, 694, 4114, 8247, 13], "temperature": 0.0, "avg_logprob": -0.17913514709472655, "compression_ratio": 1.8673835125448028, "no_speech_prob": 1.3006125300307758e-05}, {"id": 788, "seek": 285072, "start": 2875.3199999999997, "end": 2880.08, "text": " And they argue that it's easier to tell for human, but also for a model that these sequences", "tokens": [400, 436, 9695, 300, 309, 311, 3571, 281, 980, 337, 1952, 11, 457, 611, 337, 257, 2316, 300, 613, 22978], "temperature": 0.0, "avg_logprob": -0.17913514709472655, "compression_ratio": 1.8673835125448028, "no_speech_prob": 1.3006125300307758e-05}, {"id": 789, "seek": 288008, "start": 2880.08, "end": 2884.04, "text": " are similar and they should probably have the same next word, even if you don't know", "tokens": [366, 2531, 293, 436, 820, 1391, 362, 264, 912, 958, 1349, 11, 754, 498, 291, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.15163618671007392, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.5688150597270578e-05}, {"id": 790, "seek": 288008, "start": 2884.04, "end": 2886.36, "text": " what the next word is.", "tokens": [437, 264, 958, 1349, 307, 13], "temperature": 0.0, "avg_logprob": -0.15163618671007392, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.5688150597270578e-05}, {"id": 791, "seek": 288008, "start": 2886.36, "end": 2890.4, "text": " So that's suggesting that it's easier to learn these similarities than it actually predict", "tokens": [407, 300, 311, 18094, 300, 309, 311, 3571, 281, 1466, 613, 24197, 813, 309, 767, 6069], "temperature": 0.0, "avg_logprob": -0.15163618671007392, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.5688150597270578e-05}, {"id": 792, "seek": 288008, "start": 2890.4, "end": 2892.64, "text": " the next word.", "tokens": [264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.15163618671007392, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.5688150597270578e-05}, {"id": 793, "seek": 288008, "start": 2892.64, "end": 2895.92, "text": " And they argue that this is even more true for long tail patterns, where it's very challenging", "tokens": [400, 436, 9695, 300, 341, 307, 754, 544, 2074, 337, 938, 6838, 8294, 11, 689, 309, 311, 588, 7595], "temperature": 0.0, "avg_logprob": -0.15163618671007392, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.5688150597270578e-05}, {"id": 794, "seek": 288008, "start": 2895.92, "end": 2901.08, "text": " for the model to predict that the next word is some rarely seen token or rare entity than", "tokens": [337, 264, 2316, 281, 6069, 300, 264, 958, 1349, 307, 512, 13752, 1612, 14862, 420, 5892, 13977, 813], "temperature": 0.0, "avg_logprob": -0.15163618671007392, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.5688150597270578e-05}, {"id": 795, "seek": 288008, "start": 2901.08, "end": 2905.88, "text": " it is to find another similar sequence that it's already seen and just copy the next word", "tokens": [309, 307, 281, 915, 1071, 2531, 8310, 300, 309, 311, 1217, 1612, 293, 445, 5055, 264, 958, 1349], "temperature": 0.0, "avg_logprob": -0.15163618671007392, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.5688150597270578e-05}, {"id": 796, "seek": 288008, "start": 2905.88, "end": 2908.64, "text": " from that sequence.", "tokens": [490, 300, 8310, 13], "temperature": 0.0, "avg_logprob": -0.15163618671007392, "compression_ratio": 1.8676470588235294, "no_speech_prob": 1.5688150597270578e-05}, {"id": 797, "seek": 290864, "start": 2908.64, "end": 2912.8399999999997, "text": " So what they propose to do is to our representations of text sequences in nearest neighbor data", "tokens": [407, 437, 436, 17421, 281, 360, 307, 281, 527, 33358, 295, 2487, 22978, 294, 23831, 5987, 1412], "temperature": 0.0, "avg_logprob": -0.21769081178258676, "compression_ratio": 1.821917808219178, "no_speech_prob": 1.722987144603394e-05}, {"id": 798, "seek": 290864, "start": 2912.8399999999997, "end": 2914.04, "text": " store.", "tokens": [3531, 13], "temperature": 0.0, "avg_logprob": -0.21769081178258676, "compression_ratio": 1.821917808219178, "no_speech_prob": 1.722987144603394e-05}, {"id": 799, "seek": 290864, "start": 2914.04, "end": 2917.7999999999997, "text": " And then at inference, what you'll want to do is you find the K most similar sequences", "tokens": [400, 550, 412, 38253, 11, 437, 291, 603, 528, 281, 360, 307, 291, 915, 264, 591, 881, 2531, 22978], "temperature": 0.0, "avg_logprob": -0.21769081178258676, "compression_ratio": 1.821917808219178, "no_speech_prob": 1.722987144603394e-05}, {"id": 800, "seek": 290864, "start": 2917.7999999999997, "end": 2918.7999999999997, "text": " of text.", "tokens": [295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.21769081178258676, "compression_ratio": 1.821917808219178, "no_speech_prob": 1.722987144603394e-05}, {"id": 801, "seek": 290864, "start": 2918.7999999999997, "end": 2922.3599999999997, "text": " You then retrieve their corresponding values, so you just peek at those sequences and", "tokens": [509, 550, 30254, 641, 11760, 4190, 11, 370, 291, 445, 19604, 412, 729, 22978, 293], "temperature": 0.0, "avg_logprob": -0.21769081178258676, "compression_ratio": 1.821917808219178, "no_speech_prob": 1.722987144603394e-05}, {"id": 802, "seek": 290864, "start": 2922.3599999999997, "end": 2925.2, "text": " see what were their next words.", "tokens": [536, 437, 645, 641, 958, 2283, 13], "temperature": 0.0, "avg_logprob": -0.21769081178258676, "compression_ratio": 1.821917808219178, "no_speech_prob": 1.722987144603394e-05}, {"id": 803, "seek": 290864, "start": 2925.2, "end": 2930.3199999999997, "text": " And then you combine the probability from this nearest neighbor data store with just a typical", "tokens": [400, 550, 291, 10432, 264, 8482, 490, 341, 23831, 5987, 1412, 3531, 365, 445, 257, 7476], "temperature": 0.0, "avg_logprob": -0.21769081178258676, "compression_ratio": 1.821917808219178, "no_speech_prob": 1.722987144603394e-05}, {"id": 804, "seek": 290864, "start": 2930.3199999999997, "end": 2932.08, "text": " language model prediction.", "tokens": [2856, 2316, 17630, 13], "temperature": 0.0, "avg_logprob": -0.21769081178258676, "compression_ratio": 1.821917808219178, "no_speech_prob": 1.722987144603394e-05}, {"id": 805, "seek": 290864, "start": 2932.08, "end": 2936.16, "text": " And so they call this an interpolation step in that they're reading how much to pay attention", "tokens": [400, 370, 436, 818, 341, 364, 44902, 399, 1823, 294, 300, 436, 434, 3760, 577, 709, 281, 1689, 3202], "temperature": 0.0, "avg_logprob": -0.21769081178258676, "compression_ratio": 1.821917808219178, "no_speech_prob": 1.722987144603394e-05}, {"id": 806, "seek": 293616, "start": 2936.16, "end": 2940.3599999999997, "text": " to the probability from this K and N approach and how much to pay attention to this language", "tokens": [281, 264, 8482, 490, 341, 591, 293, 426, 3109, 293, 577, 709, 281, 1689, 3202, 281, 341, 2856], "temperature": 0.0, "avg_logprob": -0.2287195046742757, "compression_ratio": 1.8876404494382022, "no_speech_prob": 1.1124954653496388e-05}, {"id": 807, "seek": 293616, "start": 2940.3599999999997, "end": 2942.56, "text": " model approach.", "tokens": [2316, 3109, 13], "temperature": 0.0, "avg_logprob": -0.2287195046742757, "compression_ratio": 1.8876404494382022, "no_speech_prob": 1.1124954653496388e-05}, {"id": 808, "seek": 293616, "start": 2942.56, "end": 2948.0, "text": " And the lambda here is just a hyperparameter they tune.", "tokens": [400, 264, 13607, 510, 307, 445, 257, 9848, 2181, 335, 2398, 436, 10864, 13], "temperature": 0.0, "avg_logprob": -0.2287195046742757, "compression_ratio": 1.8876404494382022, "no_speech_prob": 1.1124954653496388e-05}, {"id": 809, "seek": 293616, "start": 2948.0, "end": 2951.2799999999997, "text": " So they have this diagram from their paper where they want to predict the next word in", "tokens": [407, 436, 362, 341, 10686, 490, 641, 3035, 689, 436, 528, 281, 6069, 264, 958, 1349, 294], "temperature": 0.0, "avg_logprob": -0.2287195046742757, "compression_ratio": 1.8876404494382022, "no_speech_prob": 1.1124954653496388e-05}, {"id": 810, "seek": 293616, "start": 2951.2799999999997, "end": 2953.56, "text": " the sequence, Shakespeare's play blank.", "tokens": [264, 8310, 11, 22825, 311, 862, 8247, 13], "temperature": 0.0, "avg_logprob": -0.2287195046742757, "compression_ratio": 1.8876404494382022, "no_speech_prob": 1.1124954653496388e-05}, {"id": 811, "seek": 293616, "start": 2953.56, "end": 2957.92, "text": " And so what they do is they have all the training context already encoded in their data", "tokens": [400, 370, 437, 436, 360, 307, 436, 362, 439, 264, 3097, 4319, 1217, 2058, 12340, 294, 641, 1412], "temperature": 0.0, "avg_logprob": -0.2287195046742757, "compression_ratio": 1.8876404494382022, "no_speech_prob": 1.1124954653496388e-05}, {"id": 812, "seek": 293616, "start": 2957.92, "end": 2958.92, "text": " store.", "tokens": [3531, 13], "temperature": 0.0, "avg_logprob": -0.2287195046742757, "compression_ratio": 1.8876404494382022, "no_speech_prob": 1.1124954653496388e-05}, {"id": 813, "seek": 293616, "start": 2958.92, "end": 2961.7999999999997, "text": " So they have representations of all the training context.", "tokens": [407, 436, 362, 33358, 295, 439, 264, 3097, 4319, 13], "temperature": 0.0, "avg_logprob": -0.2287195046742757, "compression_ratio": 1.8876404494382022, "no_speech_prob": 1.1124954653496388e-05}, {"id": 814, "seek": 293616, "start": 2961.7999999999997, "end": 2964.68, "text": " And then they compute representation of their text context.", "tokens": [400, 550, 436, 14722, 10290, 295, 641, 2487, 4319, 13], "temperature": 0.0, "avg_logprob": -0.2287195046742757, "compression_ratio": 1.8876404494382022, "no_speech_prob": 1.1124954653496388e-05}, {"id": 815, "seek": 296468, "start": 2964.68, "end": 2968.6, "text": " And they want to figure out which representations in the training context are most similar", "tokens": [400, 436, 528, 281, 2573, 484, 597, 33358, 294, 264, 3097, 4319, 366, 881, 2531], "temperature": 0.0, "avg_logprob": -0.19858719098685992, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.4506767001876142e-05}, {"id": 816, "seek": 296468, "start": 2968.6, "end": 2972.8399999999997, "text": " to this text text context representation.", "tokens": [281, 341, 2487, 2487, 4319, 10290, 13], "temperature": 0.0, "avg_logprob": -0.19858719098685992, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.4506767001876142e-05}, {"id": 817, "seek": 296468, "start": 2972.8399999999997, "end": 2977.7999999999997, "text": " And so here in external memory view of things, the keys would be the representations of", "tokens": [400, 370, 510, 294, 8320, 4675, 1910, 295, 721, 11, 264, 9317, 576, 312, 264, 33358, 295], "temperature": 0.0, "avg_logprob": -0.19858719098685992, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.4506767001876142e-05}, {"id": 818, "seek": 296468, "start": 2977.7999999999997, "end": 2982.7999999999997, "text": " the training context and the values would be the next words.", "tokens": [264, 3097, 4319, 293, 264, 4190, 576, 312, 264, 958, 2283, 13], "temperature": 0.0, "avg_logprob": -0.19858719098685992, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.4506767001876142e-05}, {"id": 819, "seek": 296468, "start": 2982.7999999999997, "end": 2986.3999999999996, "text": " So they get the K nearest training representations.", "tokens": [407, 436, 483, 264, 591, 23831, 3097, 33358, 13], "temperature": 0.0, "avg_logprob": -0.19858719098685992, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.4506767001876142e-05}, {"id": 820, "seek": 296468, "start": 2986.3999999999996, "end": 2988.0, "text": " They then copy over their values.", "tokens": [814, 550, 5055, 670, 641, 4190, 13], "temperature": 0.0, "avg_logprob": -0.19858719098685992, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.4506767001876142e-05}, {"id": 821, "seek": 296468, "start": 2988.0, "end": 2991.56, "text": " So that's what you see with this Macbeth Hamlet Macbeth example.", "tokens": [407, 300, 311, 437, 291, 536, 365, 341, 5707, 65, 3293, 8234, 2631, 5707, 65, 3293, 1365, 13], "temperature": 0.0, "avg_logprob": -0.19858719098685992, "compression_ratio": 1.8461538461538463, "no_speech_prob": 1.4506767001876142e-05}, {"id": 822, "seek": 299156, "start": 2991.56, "end": 2995.72, "text": " They have a normalization step where they convert this to probability space.", "tokens": [814, 362, 257, 2710, 2144, 1823, 689, 436, 7620, 341, 281, 8482, 1901, 13], "temperature": 0.0, "avg_logprob": -0.1392004508671798, "compression_ratio": 1.8745387453874538, "no_speech_prob": 2.318189581274055e-05}, {"id": 823, "seek": 299156, "start": 2995.72, "end": 2998.16, "text": " And then finally, they have an aggregation step.", "tokens": [400, 550, 2721, 11, 436, 362, 364, 16743, 399, 1823, 13], "temperature": 0.0, "avg_logprob": -0.1392004508671798, "compression_ratio": 1.8745387453874538, "no_speech_prob": 2.318189581274055e-05}, {"id": 824, "seek": 299156, "start": 2998.16, "end": 3003.32, "text": " So if a word is seen as the next word and several of these K nearest neighbors, then they", "tokens": [407, 498, 257, 1349, 307, 1612, 382, 264, 958, 1349, 293, 2940, 295, 613, 591, 23831, 12512, 11, 550, 436], "temperature": 0.0, "avg_logprob": -0.1392004508671798, "compression_ratio": 1.8745387453874538, "no_speech_prob": 2.318189581274055e-05}, {"id": 825, "seek": 299156, "start": 3003.32, "end": 3004.64, "text": " want to count more for that.", "tokens": [528, 281, 1207, 544, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.1392004508671798, "compression_ratio": 1.8745387453874538, "no_speech_prob": 2.318189581274055e-05}, {"id": 826, "seek": 299156, "start": 3004.64, "end": 3005.64, "text": " So that's why they aggregate.", "tokens": [407, 300, 311, 983, 436, 26118, 13], "temperature": 0.0, "avg_logprob": -0.1392004508671798, "compression_ratio": 1.8745387453874538, "no_speech_prob": 2.318189581274055e-05}, {"id": 827, "seek": 299156, "start": 3005.64, "end": 3006.64, "text": " So they see Macbeth twice.", "tokens": [407, 436, 536, 5707, 65, 3293, 6091, 13], "temperature": 0.0, "avg_logprob": -0.1392004508671798, "compression_ratio": 1.8745387453874538, "no_speech_prob": 2.318189581274055e-05}, {"id": 828, "seek": 299156, "start": 3006.64, "end": 3010.2799999999997, "text": " It means Macbeth is more likely.", "tokens": [467, 1355, 5707, 65, 3293, 307, 544, 3700, 13], "temperature": 0.0, "avg_logprob": -0.1392004508671798, "compression_ratio": 1.8745387453874538, "no_speech_prob": 2.318189581274055e-05}, {"id": 829, "seek": 299156, "start": 3010.2799999999997, "end": 3015.04, "text": " And then finally, they have this interpolation step where they try to balance between the classification", "tokens": [400, 550, 2721, 11, 436, 362, 341, 44902, 399, 1823, 689, 436, 853, 281, 4772, 1296, 264, 21538], "temperature": 0.0, "avg_logprob": -0.1392004508671798, "compression_ratio": 1.8745387453874538, "no_speech_prob": 2.318189581274055e-05}, {"id": 830, "seek": 299156, "start": 3015.04, "end": 3020.96, "text": " probabilities from the language model and from the K and N approach.", "tokens": [33783, 490, 264, 2856, 2316, 293, 490, 264, 591, 293, 426, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1392004508671798, "compression_ratio": 1.8745387453874538, "no_speech_prob": 2.318189581274055e-05}, {"id": 831, "seek": 302096, "start": 3020.96, "end": 3025.6, "text": " So some immediate observation you might have is this seems really expensive.", "tokens": [407, 512, 11629, 14816, 291, 1062, 362, 307, 341, 2544, 534, 5124, 13], "temperature": 0.0, "avg_logprob": -0.17980267866602484, "compression_ratio": 1.8044280442804428, "no_speech_prob": 3.647378252935596e-05}, {"id": 832, "seek": 302096, "start": 3025.6, "end": 3030.76, "text": " They do propose ways to kind of try to minimize the expense of actually having to store all", "tokens": [814, 360, 17421, 2098, 281, 733, 295, 853, 281, 17522, 264, 18406, 295, 767, 1419, 281, 3531, 439], "temperature": 0.0, "avg_logprob": -0.17980267866602484, "compression_ratio": 1.8044280442804428, "no_speech_prob": 3.647378252935596e-05}, {"id": 833, "seek": 302096, "start": 3030.76, "end": 3035.48, "text": " the training context in this data store because they actually store it for every single window", "tokens": [264, 3097, 4319, 294, 341, 1412, 3531, 570, 436, 767, 3531, 309, 337, 633, 2167, 4910], "temperature": 0.0, "avg_logprob": -0.17980267866602484, "compression_ratio": 1.8044280442804428, "no_speech_prob": 3.647378252935596e-05}, {"id": 834, "seek": 302096, "start": 3035.48, "end": 3038.56, "text": " of next word in the training context.", "tokens": [295, 958, 1349, 294, 264, 3097, 4319, 13], "temperature": 0.0, "avg_logprob": -0.17980267866602484, "compression_ratio": 1.8044280442804428, "no_speech_prob": 3.647378252935596e-05}, {"id": 835, "seek": 302096, "start": 3038.56, "end": 3042.44, "text": " And you can do quantization on some nearest neighbor approaches to try to make this less", "tokens": [400, 291, 393, 360, 4426, 2144, 322, 512, 23831, 5987, 11587, 281, 853, 281, 652, 341, 1570], "temperature": 0.0, "avg_logprob": -0.17980267866602484, "compression_ratio": 1.8044280442804428, "no_speech_prob": 3.647378252935596e-05}, {"id": 836, "seek": 302096, "start": 3042.44, "end": 3044.0, "text": " expensive.", "tokens": [5124, 13], "temperature": 0.0, "avg_logprob": -0.17980267866602484, "compression_ratio": 1.8044280442804428, "no_speech_prob": 3.647378252935596e-05}, {"id": 837, "seek": 302096, "start": 3044.0, "end": 3047.84, "text": " But I imagine this would still be pretty expensive for really large training data sets.", "tokens": [583, 286, 3811, 341, 576, 920, 312, 1238, 5124, 337, 534, 2416, 3097, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.17980267866602484, "compression_ratio": 1.8044280442804428, "no_speech_prob": 3.647378252935596e-05}, {"id": 838, "seek": 304784, "start": 3047.84, "end": 3053.04, "text": " They also have some cool experiments that show that this is very good for domain adaptation.", "tokens": [814, 611, 362, 512, 1627, 12050, 300, 855, 300, 341, 307, 588, 665, 337, 9274, 21549, 13], "temperature": 0.0, "avg_logprob": -0.15193800926208495, "compression_ratio": 1.8215767634854771, "no_speech_prob": 3.119942994089797e-05}, {"id": 839, "seek": 304784, "start": 3053.04, "end": 3056.6400000000003, "text": " So if you take your language model and you have a new domain that you want to apply your", "tokens": [407, 498, 291, 747, 428, 2856, 2316, 293, 291, 362, 257, 777, 9274, 300, 291, 528, 281, 3079, 428], "temperature": 0.0, "avg_logprob": -0.15193800926208495, "compression_ratio": 1.8215767634854771, "no_speech_prob": 3.119942994089797e-05}, {"id": 840, "seek": 304784, "start": 3056.6400000000003, "end": 3062.4, "text": " language model to, you could just create a nearest neighbor data store of your new domain.", "tokens": [2856, 2316, 281, 11, 291, 727, 445, 1884, 257, 23831, 5987, 1412, 3531, 295, 428, 777, 9274, 13], "temperature": 0.0, "avg_logprob": -0.15193800926208495, "compression_ratio": 1.8215767634854771, "no_speech_prob": 3.119942994089797e-05}, {"id": 841, "seek": 304784, "start": 3062.4, "end": 3067.36, "text": " So you code all the representations that new domain you stick in a data store.", "tokens": [407, 291, 3089, 439, 264, 33358, 300, 777, 9274, 291, 2897, 294, 257, 1412, 3531, 13], "temperature": 0.0, "avg_logprob": -0.15193800926208495, "compression_ratio": 1.8215767634854771, "no_speech_prob": 3.119942994089797e-05}, {"id": 842, "seek": 304784, "start": 3067.36, "end": 3072.76, "text": " And then you can just use your language model with these K and N probabilities as well,", "tokens": [400, 550, 291, 393, 445, 764, 428, 2856, 2316, 365, 613, 591, 293, 426, 33783, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.15193800926208495, "compression_ratio": 1.8215767634854771, "no_speech_prob": 3.119942994089797e-05}, {"id": 843, "seek": 307276, "start": 3072.76, "end": 3078.0, "text": " which is immediately on this new domain without actually having to further train your language", "tokens": [597, 307, 4258, 322, 341, 777, 9274, 1553, 767, 1419, 281, 3052, 3847, 428, 2856], "temperature": 0.0, "avg_logprob": -0.19358515271953508, "compression_ratio": 1.7300380228136882, "no_speech_prob": 2.2124288079794496e-05}, {"id": 844, "seek": 307276, "start": 3078.0, "end": 3079.0, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.19358515271953508, "compression_ratio": 1.7300380228136882, "no_speech_prob": 2.2124288079794496e-05}, {"id": 845, "seek": 307276, "start": 3079.0, "end": 3083.5200000000004, "text": " So I thought that was a pretty cool use case of this external memory approach.", "tokens": [407, 286, 1194, 300, 390, 257, 1238, 1627, 764, 1389, 295, 341, 8320, 4675, 3109, 13], "temperature": 0.0, "avg_logprob": -0.19358515271953508, "compression_ratio": 1.7300380228136882, "no_speech_prob": 2.2124288079794496e-05}, {"id": 846, "seek": 307276, "start": 3083.5200000000004, "end": 3087.48, "text": " So while it doesn't leverage knowledge bases directly, it does have this loose knowledge", "tokens": [407, 1339, 309, 1177, 380, 13982, 3601, 17949, 3838, 11, 309, 775, 362, 341, 9612, 3601], "temperature": 0.0, "avg_logprob": -0.19358515271953508, "compression_ratio": 1.7300380228136882, "no_speech_prob": 2.2124288079794496e-05}, {"id": 847, "seek": 307276, "start": 3087.48, "end": 3092.92, "text": " of or loose idea of encoding knowledge that is in a textual representation form into", "tokens": [295, 420, 9612, 1558, 295, 43430, 3601, 300, 307, 294, 257, 2487, 901, 10290, 1254, 666], "temperature": 0.0, "avg_logprob": -0.19358515271953508, "compression_ratio": 1.7300380228136882, "no_speech_prob": 2.2124288079794496e-05}, {"id": 848, "seek": 307276, "start": 3092.92, "end": 3097.1200000000003, "text": " some external memory that the model can then take advantage of.", "tokens": [512, 8320, 4675, 300, 264, 2316, 393, 550, 747, 5002, 295, 13], "temperature": 0.0, "avg_logprob": -0.19358515271953508, "compression_ratio": 1.7300380228136882, "no_speech_prob": 2.2124288079794496e-05}, {"id": 849, "seek": 307276, "start": 3097.1200000000003, "end": 3101.84, "text": " That's all I have for this approach.", "tokens": [663, 311, 439, 286, 362, 337, 341, 3109, 13], "temperature": 0.0, "avg_logprob": -0.19358515271953508, "compression_ratio": 1.7300380228136882, "no_speech_prob": 2.2124288079794496e-05}, {"id": 850, "seek": 310184, "start": 3101.84, "end": 3103.84, "text": " Are there any questions on this approach?", "tokens": [2014, 456, 604, 1651, 322, 341, 3109, 30], "temperature": 0.0, "avg_logprob": -0.26011497324163263, "compression_ratio": 1.6, "no_speech_prob": 4.262315633241087e-05}, {"id": 851, "seek": 310184, "start": 3103.84, "end": 3115.32, "text": " Well, so only one person is asking, how does the K and N make predictions for the next", "tokens": [1042, 11, 370, 787, 472, 954, 307, 3365, 11, 577, 775, 264, 591, 293, 426, 652, 21264, 337, 264, 958], "temperature": 0.0, "avg_logprob": -0.26011497324163263, "compression_ratio": 1.6, "no_speech_prob": 4.262315633241087e-05}, {"id": 852, "seek": 310184, "start": 3115.32, "end": 3116.32, "text": " word?", "tokens": [1349, 30], "temperature": 0.0, "avg_logprob": -0.26011497324163263, "compression_ratio": 1.6, "no_speech_prob": 4.262315633241087e-05}, {"id": 853, "seek": 310184, "start": 3116.32, "end": 3120.2000000000003, "text": " The K neighbors are for the context instead of the next word.", "tokens": [440, 591, 12512, 366, 337, 264, 4319, 2602, 295, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.26011497324163263, "compression_ratio": 1.6, "no_speech_prob": 4.262315633241087e-05}, {"id": 854, "seek": 310184, "start": 3120.2000000000003, "end": 3121.2000000000003, "text": " Oh, okay.", "tokens": [876, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.26011497324163263, "compression_ratio": 1.6, "no_speech_prob": 4.262315633241087e-05}, {"id": 855, "seek": 310184, "start": 3121.2000000000003, "end": 3122.52, "text": " That was unclear.", "tokens": [663, 390, 25636, 13], "temperature": 0.0, "avg_logprob": -0.26011497324163263, "compression_ratio": 1.6, "no_speech_prob": 4.262315633241087e-05}, {"id": 856, "seek": 310184, "start": 3122.52, "end": 3127.76, "text": " So the keys are the representations that context the values in your external memory are", "tokens": [407, 264, 9317, 366, 264, 33358, 300, 4319, 264, 4190, 294, 428, 8320, 4675, 366], "temperature": 0.0, "avg_logprob": -0.26011497324163263, "compression_ratio": 1.6, "no_speech_prob": 4.262315633241087e-05}, {"id": 857, "seek": 310184, "start": 3127.76, "end": 3129.04, "text": " the next words.", "tokens": [264, 958, 2283, 13], "temperature": 0.0, "avg_logprob": -0.26011497324163263, "compression_ratio": 1.6, "no_speech_prob": 4.262315633241087e-05}, {"id": 858, "seek": 312904, "start": 3129.04, "end": 3132.88, "text": " So when you figure out you figure out your nearest neighbors using your keys and then you", "tokens": [407, 562, 291, 2573, 484, 291, 2573, 484, 428, 23831, 12512, 1228, 428, 9317, 293, 550, 291], "temperature": 0.0, "avg_logprob": -0.19670865912186472, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.0288373232469894e-05}, {"id": 859, "seek": 312904, "start": 3132.88, "end": 3134.44, "text": " copy over their values.", "tokens": [5055, 670, 641, 4190, 13], "temperature": 0.0, "avg_logprob": -0.19670865912186472, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.0288373232469894e-05}, {"id": 860, "seek": 312904, "start": 3134.44, "end": 3139.08, "text": " So it does actually know what the next words are for each of those representations.", "tokens": [407, 309, 775, 767, 458, 437, 264, 958, 2283, 366, 337, 1184, 295, 729, 33358, 13], "temperature": 0.0, "avg_logprob": -0.19670865912186472, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.0288373232469894e-05}, {"id": 861, "seek": 312904, "start": 3139.08, "end": 3143.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19670865912186472, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.0288373232469894e-05}, {"id": 862, "seek": 312904, "start": 3143.08, "end": 3149.16, "text": " So finally, we're going to talk about how you can just modify the training data to better", "tokens": [407, 2721, 11, 321, 434, 516, 281, 751, 466, 577, 291, 393, 445, 16927, 264, 3097, 1412, 281, 1101], "temperature": 0.0, "avg_logprob": -0.19670865912186472, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.0288373232469894e-05}, {"id": 863, "seek": 312904, "start": 3149.16, "end": 3152.2799999999997, "text": " and code knowledge and language models.", "tokens": [293, 3089, 3601, 293, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.19670865912186472, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.0288373232469894e-05}, {"id": 864, "seek": 312904, "start": 3152.2799999999997, "end": 3156.96, "text": " So approaches you've talked about so far are actually incorporating knowledge explicitly", "tokens": [407, 11587, 291, 600, 2825, 466, 370, 1400, 366, 767, 33613, 3601, 20803], "temperature": 0.0, "avg_logprob": -0.19670865912186472, "compression_ratio": 1.7016129032258065, "no_speech_prob": 1.0288373232469894e-05}, {"id": 865, "seek": 315696, "start": 3156.96, "end": 3160.8, "text": " by using the pre-trained embeddings or an external memory.", "tokens": [538, 1228, 264, 659, 12, 17227, 2001, 12240, 29432, 420, 364, 8320, 4675, 13], "temperature": 0.0, "avg_logprob": -0.21487065543115666, "compression_ratio": 1.6807017543859648, "no_speech_prob": 8.529991646355484e-06}, {"id": 866, "seek": 315696, "start": 3160.8, "end": 3164.84, "text": " We also want to talk about how can you just incorporate knowledge implicitly through the", "tokens": [492, 611, 528, 281, 751, 466, 577, 393, 291, 445, 16091, 3601, 26947, 356, 807, 264], "temperature": 0.0, "avg_logprob": -0.21487065543115666, "compression_ratio": 1.6807017543859648, "no_speech_prob": 8.529991646355484e-06}, {"id": 867, "seek": 315696, "start": 3164.84, "end": 3168.28, "text": " unstructured text.", "tokens": [18799, 46847, 2487, 13], "temperature": 0.0, "avg_logprob": -0.21487065543115666, "compression_ratio": 1.6807017543859648, "no_speech_prob": 8.529991646355484e-06}, {"id": 868, "seek": 315696, "start": 3168.28, "end": 3171.96, "text": " So what we're going to do is either mask or crop the data to introduce additional training", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 2139, 6094, 420, 9086, 264, 1412, 281, 5366, 4497, 3097], "temperature": 0.0, "avg_logprob": -0.21487065543115666, "compression_ratio": 1.6807017543859648, "no_speech_prob": 8.529991646355484e-06}, {"id": 869, "seek": 315696, "start": 3171.96, "end": 3178.08, "text": " tasks that require factual knowledge to figure out what data was masked, for instance.", "tokens": [9608, 300, 3651, 48029, 3601, 281, 2573, 484, 437, 1412, 390, 45249, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.21487065543115666, "compression_ratio": 1.6807017543859648, "no_speech_prob": 8.529991646355484e-06}, {"id": 870, "seek": 315696, "start": 3178.08, "end": 3179.76, "text": " So some clear advantages.", "tokens": [407, 512, 1850, 14906, 13], "temperature": 0.0, "avg_logprob": -0.21487065543115666, "compression_ratio": 1.6807017543859648, "no_speech_prob": 8.529991646355484e-06}, {"id": 871, "seek": 315696, "start": 3179.76, "end": 3182.52, "text": " It doesn't have an additional memory or computation requirements.", "tokens": [467, 1177, 380, 362, 364, 4497, 4675, 420, 24903, 7728, 13], "temperature": 0.0, "avg_logprob": -0.21487065543115666, "compression_ratio": 1.6807017543859648, "no_speech_prob": 8.529991646355484e-06}, {"id": 872, "seek": 315696, "start": 3182.52, "end": 3184.36, "text": " You don't have a data stored at deal with.", "tokens": [509, 500, 380, 362, 257, 1412, 12187, 412, 2028, 365, 13], "temperature": 0.0, "avg_logprob": -0.21487065543115666, "compression_ratio": 1.6807017543859648, "no_speech_prob": 8.529991646355484e-06}, {"id": 873, "seek": 318436, "start": 3184.36, "end": 3187.0, "text": " You don't have extra knowledge and coder layers to train.", "tokens": [509, 500, 380, 362, 2857, 3601, 293, 17656, 260, 7914, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.19715879784255733, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.0145245040766895e-06}, {"id": 874, "seek": 318436, "start": 3187.0, "end": 3189.08, "text": " All you do is modify the training data.", "tokens": [1057, 291, 360, 307, 16927, 264, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19715879784255733, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.0145245040766895e-06}, {"id": 875, "seek": 318436, "start": 3189.08, "end": 3191.88, "text": " And you don't have to modify your architecture either.", "tokens": [400, 291, 500, 380, 362, 281, 16927, 428, 9482, 2139, 13], "temperature": 0.0, "avg_logprob": -0.19715879784255733, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.0145245040766895e-06}, {"id": 876, "seek": 318436, "start": 3191.88, "end": 3195.6800000000003, "text": " So you can continue using your favorite bird model and just make these changes to the", "tokens": [407, 291, 393, 2354, 1228, 428, 2954, 5255, 2316, 293, 445, 652, 613, 2962, 281, 264], "temperature": 0.0, "avg_logprob": -0.19715879784255733, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.0145245040766895e-06}, {"id": 877, "seek": 318436, "start": 3195.6800000000003, "end": 3198.56, "text": " training data.", "tokens": [3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19715879784255733, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.0145245040766895e-06}, {"id": 878, "seek": 318436, "start": 3198.56, "end": 3202.44, "text": " So the first work we're going to look at is called WKLM, weekly supervised knowledge", "tokens": [407, 264, 700, 589, 321, 434, 516, 281, 574, 412, 307, 1219, 343, 42, 43, 44, 11, 12460, 46533, 3601], "temperature": 0.0, "avg_logprob": -0.19715879784255733, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.0145245040766895e-06}, {"id": 879, "seek": 318436, "start": 3202.44, "end": 3205.6, "text": " pre-training language model or pre-trained language model.", "tokens": [659, 12, 17227, 1760, 2856, 2316, 420, 659, 12, 17227, 2001, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19715879784255733, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.0145245040766895e-06}, {"id": 880, "seek": 318436, "start": 3205.6, "end": 3211.28, "text": " And the key idea here is to train the model to distinguish between true and false knowledge.", "tokens": [400, 264, 2141, 1558, 510, 307, 281, 3847, 264, 2316, 281, 20206, 1296, 2074, 293, 7908, 3601, 13], "temperature": 0.0, "avg_logprob": -0.19715879784255733, "compression_ratio": 1.8148148148148149, "no_speech_prob": 5.0145245040766895e-06}, {"id": 881, "seek": 321128, "start": 3211.28, "end": 3214.92, "text": " So they're going to corrupt the data by replacing mentions in the text with mentions that", "tokens": [407, 436, 434, 516, 281, 17366, 264, 1412, 538, 19139, 23844, 294, 264, 2487, 365, 23844, 300], "temperature": 0.0, "avg_logprob": -0.1440407769721851, "compression_ratio": 1.8013698630136987, "no_speech_prob": 2.3922117179608904e-05}, {"id": 882, "seek": 321128, "start": 3214.92, "end": 3219.4, "text": " refer to different entities of the same type to create what they refer to as negative", "tokens": [2864, 281, 819, 16667, 295, 264, 912, 2010, 281, 1884, 437, 436, 2864, 281, 382, 3671], "temperature": 0.0, "avg_logprob": -0.1440407769721851, "compression_ratio": 1.8013698630136987, "no_speech_prob": 2.3922117179608904e-05}, {"id": 883, "seek": 321128, "start": 3219.4, "end": 3221.96, "text": " knowledge statements.", "tokens": [3601, 12363, 13], "temperature": 0.0, "avg_logprob": -0.1440407769721851, "compression_ratio": 1.8013698630136987, "no_speech_prob": 2.3922117179608904e-05}, {"id": 884, "seek": 321128, "start": 3221.96, "end": 3227.7200000000003, "text": " And then the model will just predict has the entity been replaced or corrupted.", "tokens": [400, 550, 264, 2316, 486, 445, 6069, 575, 264, 13977, 668, 10772, 420, 39480, 13], "temperature": 0.0, "avg_logprob": -0.1440407769721851, "compression_ratio": 1.8013698630136987, "no_speech_prob": 2.3922117179608904e-05}, {"id": 885, "seek": 321128, "start": 3227.7200000000003, "end": 3232.1600000000003, "text": " This type constraint is necessary to make sure that or to encourage the model to actually", "tokens": [639, 2010, 25534, 307, 4818, 281, 652, 988, 300, 420, 281, 5373, 264, 2316, 281, 767], "temperature": 0.0, "avg_logprob": -0.1440407769721851, "compression_ratio": 1.8013698630136987, "no_speech_prob": 2.3922117179608904e-05}, {"id": 886, "seek": 321128, "start": 3232.1600000000003, "end": 3235.52, "text": " use factual knowledge to figure out if this corruption is taking place.", "tokens": [764, 48029, 3601, 281, 2573, 484, 498, 341, 17959, 307, 1940, 1081, 13], "temperature": 0.0, "avg_logprob": -0.1440407769721851, "compression_ratio": 1.8013698630136987, "no_speech_prob": 2.3922117179608904e-05}, {"id": 887, "seek": 321128, "start": 3235.52, "end": 3238.6000000000004, "text": " So you could imagine if you replace it with something that's not realistic at all, the", "tokens": [407, 291, 727, 3811, 498, 291, 7406, 309, 365, 746, 300, 311, 406, 12465, 412, 439, 11, 264], "temperature": 0.0, "avg_logprob": -0.1440407769721851, "compression_ratio": 1.8013698630136987, "no_speech_prob": 2.3922117179608904e-05}, {"id": 888, "seek": 323860, "start": 3238.6, "end": 3244.68, "text": " model could just be basing its prediction based on is this sentence linguistically correct.", "tokens": [2316, 727, 445, 312, 987, 278, 1080, 17630, 2361, 322, 307, 341, 8174, 21766, 20458, 3006, 13], "temperature": 0.0, "avg_logprob": -0.1984694174357823, "compression_ratio": 1.7526881720430108, "no_speech_prob": 1.5688887287979014e-05}, {"id": 889, "seek": 323860, "start": 3244.68, "end": 3249.64, "text": " So as an example, we have a true knowledge statement as JK rolling is the author of", "tokens": [407, 382, 364, 1365, 11, 321, 362, 257, 2074, 3601, 5629, 382, 35973, 9439, 307, 264, 3793, 295], "temperature": 0.0, "avg_logprob": -0.1984694174357823, "compression_ratio": 1.7526881720430108, "no_speech_prob": 1.5688887287979014e-05}, {"id": 890, "seek": 323860, "start": 3249.64, "end": 3251.96, "text": " Harry Potter.", "tokens": [9378, 18115, 13], "temperature": 0.0, "avg_logprob": -0.1984694174357823, "compression_ratio": 1.7526881720430108, "no_speech_prob": 1.5688887287979014e-05}, {"id": 891, "seek": 323860, "start": 3251.96, "end": 3255.0, "text": " And then we want to modify this to replace it with another author.", "tokens": [400, 550, 321, 528, 281, 16927, 341, 281, 7406, 309, 365, 1071, 3793, 13], "temperature": 0.0, "avg_logprob": -0.1984694174357823, "compression_ratio": 1.7526881720430108, "no_speech_prob": 1.5688887287979014e-05}, {"id": 892, "seek": 323860, "start": 3255.0, "end": 3259.7999999999997, "text": " So let's say we change this to JR Tolkien is the author of Harry Potter.", "tokens": [407, 718, 311, 584, 321, 1319, 341, 281, 32849, 48824, 307, 264, 3793, 295, 9378, 18115, 13], "temperature": 0.0, "avg_logprob": -0.1984694174357823, "compression_ratio": 1.7526881720430108, "no_speech_prob": 1.5688887287979014e-05}, {"id": 893, "seek": 323860, "start": 3259.7999999999997, "end": 3264.08, "text": " So you can see that this requires some amount of knowledge background knowledge actually", "tokens": [407, 291, 393, 536, 300, 341, 7029, 512, 2372, 295, 3601, 3678, 3601, 767], "temperature": 0.0, "avg_logprob": -0.1984694174357823, "compression_ratio": 1.7526881720430108, "no_speech_prob": 1.5688887287979014e-05}, {"id": 894, "seek": 323860, "start": 3264.08, "end": 3267.36, "text": " able to figure out which statements true and which statement is false.", "tokens": [1075, 281, 2573, 484, 597, 12363, 2074, 293, 597, 5629, 307, 7908, 13], "temperature": 0.0, "avg_logprob": -0.1984694174357823, "compression_ratio": 1.7526881720430108, "no_speech_prob": 1.5688887287979014e-05}, {"id": 895, "seek": 326736, "start": 3267.36, "end": 3271.6, "text": " And the idea is that the model will be able to predict free to these mentions, whether", "tokens": [400, 264, 1558, 307, 300, 264, 2316, 486, 312, 1075, 281, 6069, 1737, 281, 613, 23844, 11, 1968], "temperature": 0.0, "avg_logprob": -0.16147007261003768, "compression_ratio": 1.7913385826771653, "no_speech_prob": 1.7230377125088125e-05}, {"id": 896, "seek": 326736, "start": 3271.6, "end": 3276.88, "text": " it's a true or false mention.", "tokens": [309, 311, 257, 2074, 420, 7908, 2152, 13], "temperature": 0.0, "avg_logprob": -0.16147007261003768, "compression_ratio": 1.7913385826771653, "no_speech_prob": 1.7230377125088125e-05}, {"id": 897, "seek": 326736, "start": 3276.88, "end": 3280.36, "text": " So this diagram here is from the paper and hopefully explains this a bit better.", "tokens": [407, 341, 10686, 510, 307, 490, 264, 3035, 293, 4696, 13948, 341, 257, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.16147007261003768, "compression_ratio": 1.7913385826771653, "no_speech_prob": 1.7230377125088125e-05}, {"id": 898, "seek": 326736, "start": 3280.36, "end": 3283.56, "text": " They have their original article on the left and then they have their replaced article", "tokens": [814, 362, 641, 3380, 7222, 322, 264, 1411, 293, 550, 436, 362, 641, 10772, 7222], "temperature": 0.0, "avg_logprob": -0.16147007261003768, "compression_ratio": 1.7913385826771653, "no_speech_prob": 1.7230377125088125e-05}, {"id": 899, "seek": 326736, "start": 3283.56, "end": 3287.52, "text": " with the corruptions on the right and the entities are in blue.", "tokens": [365, 264, 17366, 626, 322, 264, 558, 293, 264, 16667, 366, 294, 3344, 13], "temperature": 0.0, "avg_logprob": -0.16147007261003768, "compression_ratio": 1.7913385826771653, "no_speech_prob": 1.7230377125088125e-05}, {"id": 900, "seek": 326736, "start": 3287.52, "end": 3292.1600000000003, "text": " So what they do is for a given entity, they first look up its type, they find other entities", "tokens": [407, 437, 436, 360, 307, 337, 257, 2212, 13977, 11, 436, 700, 574, 493, 1080, 2010, 11, 436, 915, 661, 16667], "temperature": 0.0, "avg_logprob": -0.16147007261003768, "compression_ratio": 1.7913385826771653, "no_speech_prob": 1.7230377125088125e-05}, {"id": 901, "seek": 326736, "start": 3292.1600000000003, "end": 3293.84, "text": " of that type.", "tokens": [295, 300, 2010, 13], "temperature": 0.0, "avg_logprob": -0.16147007261003768, "compression_ratio": 1.7913385826771653, "no_speech_prob": 1.7230377125088125e-05}, {"id": 902, "seek": 329384, "start": 3293.84, "end": 3299.4, "text": " And they randomly sample the entity and get an alias of it to replace in the text.", "tokens": [400, 436, 16979, 6889, 264, 13977, 293, 483, 364, 419, 4609, 295, 309, 281, 7406, 294, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.17856491765668314, "compression_ratio": 1.8377358490566038, "no_speech_prob": 8.938790415413678e-06}, {"id": 903, "seek": 329384, "start": 3299.4, "end": 3303.48, "text": " So they're going to replace Stanley, for instance, with Brian Johnson and Marvel Comics", "tokens": [407, 436, 434, 516, 281, 7406, 28329, 11, 337, 5197, 11, 365, 10765, 9779, 293, 13837, 43533], "temperature": 0.0, "avg_logprob": -0.17856491765668314, "compression_ratio": 1.8377358490566038, "no_speech_prob": 8.938790415413678e-06}, {"id": 904, "seek": 329384, "start": 3303.48, "end": 3308.32, "text": " with DC Comics and their placements are in red on the right.", "tokens": [365, 9114, 43533, 293, 641, 20831, 6400, 366, 294, 2182, 322, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.17856491765668314, "compression_ratio": 1.8377358490566038, "no_speech_prob": 8.938790415413678e-06}, {"id": 905, "seek": 329384, "start": 3308.32, "end": 3312.48, "text": " And then the idea is that the model will be able to predict for each of these mentions", "tokens": [400, 550, 264, 1558, 307, 300, 264, 2316, 486, 312, 1075, 281, 6069, 337, 1184, 295, 613, 23844], "temperature": 0.0, "avg_logprob": -0.17856491765668314, "compression_ratio": 1.8377358490566038, "no_speech_prob": 8.938790415413678e-06}, {"id": 906, "seek": 329384, "start": 3312.48, "end": 3314.04, "text": " was it replaced or not.", "tokens": [390, 309, 10772, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.17856491765668314, "compression_ratio": 1.8377358490566038, "no_speech_prob": 8.938790415413678e-06}, {"id": 907, "seek": 329384, "start": 3314.04, "end": 3319.04, "text": " So in the case of Brian Johnson, they have the red X for this is a false mention and in", "tokens": [407, 294, 264, 1389, 295, 10765, 9779, 11, 436, 362, 264, 2182, 1783, 337, 341, 307, 257, 7908, 2152, 293, 294], "temperature": 0.0, "avg_logprob": -0.17856491765668314, "compression_ratio": 1.8377358490566038, "no_speech_prob": 8.938790415413678e-06}, {"id": 908, "seek": 329384, "start": 3319.04, "end": 3322.4, "text": " the case of the true mentions, they have the check mark.", "tokens": [264, 1389, 295, 264, 2074, 23844, 11, 436, 362, 264, 1520, 1491, 13], "temperature": 0.0, "avg_logprob": -0.17856491765668314, "compression_ratio": 1.8377358490566038, "no_speech_prob": 8.938790415413678e-06}, {"id": 909, "seek": 332240, "start": 3322.4, "end": 3327.12, "text": " So this is a pretty simple approach, but they actually show that it can help the model", "tokens": [407, 341, 307, 257, 1238, 2199, 3109, 11, 457, 436, 767, 855, 300, 309, 393, 854, 264, 2316], "temperature": 0.0, "avg_logprob": -0.18990825084929772, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.905344783561304e-06}, {"id": 910, "seek": 332240, "start": 3327.12, "end": 3332.12, "text": " increase the amount of knowledge that's encoded in parameters.", "tokens": [3488, 264, 2372, 295, 3601, 300, 311, 2058, 12340, 294, 9834, 13], "temperature": 0.0, "avg_logprob": -0.18990825084929772, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.905344783561304e-06}, {"id": 911, "seek": 332240, "start": 3332.12, "end": 3340.92, "text": " So WKLN uses an entity or placement loss to train the model to distinguish between", "tokens": [407, 343, 42, 43, 45, 4960, 364, 13977, 420, 17257, 4470, 281, 3847, 264, 2316, 281, 20206, 1296], "temperature": 0.0, "avg_logprob": -0.18990825084929772, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.905344783561304e-06}, {"id": 912, "seek": 332240, "start": 3340.92, "end": 3342.64, "text": " these true and false mentions.", "tokens": [613, 2074, 293, 7908, 23844, 13], "temperature": 0.0, "avg_logprob": -0.18990825084929772, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.905344783561304e-06}, {"id": 913, "seek": 332240, "start": 3342.64, "end": 3346.48, "text": " And this just looks like a binary classification loss where your true mentions are on the", "tokens": [400, 341, 445, 1542, 411, 257, 17434, 21538, 4470, 689, 428, 2074, 23844, 366, 322, 264], "temperature": 0.0, "avg_logprob": -0.18990825084929772, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.905344783561304e-06}, {"id": 914, "seek": 332240, "start": 3346.48, "end": 3349.52, "text": " left and your false mentions are on the right.", "tokens": [1411, 293, 428, 7908, 23844, 366, 322, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.18990825084929772, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.905344783561304e-06}, {"id": 915, "seek": 334952, "start": 3349.52, "end": 3354.56, "text": " And you want to increase the probability that this P of E given C, so the probability", "tokens": [400, 291, 528, 281, 3488, 264, 8482, 300, 341, 430, 295, 462, 2212, 383, 11, 370, 264, 8482], "temperature": 0.0, "avg_logprob": -0.16408847311268682, "compression_ratio": 2.0798319327731094, "no_speech_prob": 6.85407667333493e-06}, {"id": 916, "seek": 334952, "start": 3354.56, "end": 3358.48, "text": " of entity given the context, you want to increase that for the true mentions and decrease", "tokens": [295, 13977, 2212, 264, 4319, 11, 291, 528, 281, 3488, 300, 337, 264, 2074, 23844, 293, 11514], "temperature": 0.0, "avg_logprob": -0.16408847311268682, "compression_ratio": 2.0798319327731094, "no_speech_prob": 6.85407667333493e-06}, {"id": 917, "seek": 334952, "start": 3358.48, "end": 3361.52, "text": " it for the false mentions.", "tokens": [309, 337, 264, 7908, 23844, 13], "temperature": 0.0, "avg_logprob": -0.16408847311268682, "compression_ratio": 2.0798319327731094, "no_speech_prob": 6.85407667333493e-06}, {"id": 918, "seek": 334952, "start": 3361.52, "end": 3365.4, "text": " The total loss is then just a combination of the mass language model loss and this entity", "tokens": [440, 3217, 4470, 307, 550, 445, 257, 6562, 295, 264, 2758, 2856, 2316, 4470, 293, 341, 13977], "temperature": 0.0, "avg_logprob": -0.16408847311268682, "compression_ratio": 2.0798319327731094, "no_speech_prob": 6.85407667333493e-06}, {"id": 919, "seek": 334952, "start": 3365.4, "end": 3368.16, "text": " or placement loss.", "tokens": [420, 17257, 4470, 13], "temperature": 0.0, "avg_logprob": -0.16408847311268682, "compression_ratio": 2.0798319327731094, "no_speech_prob": 6.85407667333493e-06}, {"id": 920, "seek": 334952, "start": 3368.16, "end": 3374.04, "text": " The mass language model loss is defined at the token level and the entity or placement", "tokens": [440, 2758, 2856, 2316, 4470, 307, 7642, 412, 264, 14862, 1496, 293, 264, 13977, 420, 17257], "temperature": 0.0, "avg_logprob": -0.16408847311268682, "compression_ratio": 2.0798319327731094, "no_speech_prob": 6.85407667333493e-06}, {"id": 921, "seek": 334952, "start": 3374.04, "end": 3379.44, "text": " loss is defined at the entity level, meaning it's not just over sub words, it's even potentially", "tokens": [4470, 307, 7642, 412, 264, 13977, 1496, 11, 3620, 309, 311, 406, 445, 670, 1422, 2283, 11, 309, 311, 754, 7263], "temperature": 0.0, "avg_logprob": -0.16408847311268682, "compression_ratio": 2.0798319327731094, "no_speech_prob": 6.85407667333493e-06}, {"id": 922, "seek": 337944, "start": 3379.44, "end": 3385.2000000000003, "text": " over words if you have multi word entities phrases, for instance.", "tokens": [670, 2283, 498, 291, 362, 4825, 1349, 16667, 20312, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.17719766282543695, "compression_ratio": 1.5900383141762453, "no_speech_prob": 5.649113154504448e-05}, {"id": 923, "seek": 337944, "start": 3385.2000000000003, "end": 3390.2000000000003, "text": " And this is an important theme that we really see occurring throughout these works that", "tokens": [400, 341, 307, 364, 1021, 6314, 300, 321, 534, 536, 18386, 3710, 613, 1985, 300], "temperature": 0.0, "avg_logprob": -0.17719766282543695, "compression_ratio": 1.5900383141762453, "no_speech_prob": 5.649113154504448e-05}, {"id": 924, "seek": 337944, "start": 3390.2000000000003, "end": 3395.0, "text": " we'll look at in that modifying the data at the entity level seems to be an important", "tokens": [321, 603, 574, 412, 294, 300, 42626, 264, 1412, 412, 264, 13977, 1496, 2544, 281, 312, 364, 1021], "temperature": 0.0, "avg_logprob": -0.17719766282543695, "compression_ratio": 1.5900383141762453, "no_speech_prob": 5.649113154504448e-05}, {"id": 925, "seek": 337944, "start": 3395.0, "end": 3399.92, "text": " component of actually increasing the amount of knowledge that a language model can encode.", "tokens": [6542, 295, 767, 5662, 264, 2372, 295, 3601, 300, 257, 2856, 2316, 393, 2058, 1429, 13], "temperature": 0.0, "avg_logprob": -0.17719766282543695, "compression_ratio": 1.5900383141762453, "no_speech_prob": 5.649113154504448e-05}, {"id": 926, "seek": 337944, "start": 3399.92, "end": 3407.7200000000003, "text": " So you find that WKLN improves over BERT and GPT2, in fact completion tasks like the", "tokens": [407, 291, 915, 300, 343, 42, 43, 45, 24771, 670, 363, 31479, 293, 26039, 51, 17, 11, 294, 1186, 19372, 9608, 411, 264], "temperature": 0.0, "avg_logprob": -0.17719766282543695, "compression_ratio": 1.5900383141762453, "no_speech_prob": 5.649113154504448e-05}, {"id": 927, "seek": 340772, "start": 3407.72, "end": 3410.8399999999997, "text": " fill in the blank statements that we looked at at the beginning.", "tokens": [2836, 294, 264, 8247, 12363, 300, 321, 2956, 412, 412, 264, 2863, 13], "temperature": 0.0, "avg_logprob": -0.22994811051375383, "compression_ratio": 1.8511326860841424, "no_speech_prob": 3.4804957977030426e-05}, {"id": 928, "seek": 340772, "start": 3410.8399999999997, "end": 3414.9199999999996, "text": " They also find that it improves over the Ernie paper that we talked about on a downstream", "tokens": [814, 611, 915, 300, 309, 24771, 670, 264, 3300, 2766, 3035, 300, 321, 2825, 466, 322, 257, 30621], "temperature": 0.0, "avg_logprob": -0.22994811051375383, "compression_ratio": 1.8511326860841424, "no_speech_prob": 3.4804957977030426e-05}, {"id": 929, "seek": 340772, "start": 3414.9199999999996, "end": 3415.9199999999996, "text": " task.", "tokens": [5633, 13], "temperature": 0.0, "avg_logprob": -0.22994811051375383, "compression_ratio": 1.8511326860841424, "no_speech_prob": 3.4804957977030426e-05}, {"id": 930, "seek": 340772, "start": 3415.9199999999996, "end": 3419.7999999999997, "text": " And they had a set of the Blatian experiments where they looked at, can you just remove", "tokens": [400, 436, 632, 257, 992, 295, 264, 2177, 267, 952, 12050, 689, 436, 2956, 412, 11, 393, 291, 445, 4159], "temperature": 0.0, "avg_logprob": -0.22994811051375383, "compression_ratio": 1.8511326860841424, "no_speech_prob": 3.4804957977030426e-05}, {"id": 931, "seek": 340772, "start": 3419.7999999999997, "end": 3422.9199999999996, "text": " this mass language model loss now?", "tokens": [341, 2758, 2856, 2316, 4470, 586, 30], "temperature": 0.0, "avg_logprob": -0.22994811051375383, "compression_ratio": 1.8511326860841424, "no_speech_prob": 3.4804957977030426e-05}, {"id": 932, "seek": 340772, "start": 3422.9199999999996, "end": 3428.0, "text": " And if you just train BERT for longer, do you really need this entity or placement loss?", "tokens": [400, 498, 291, 445, 3847, 363, 31479, 337, 2854, 11, 360, 291, 534, 643, 341, 13977, 420, 17257, 4470, 30], "temperature": 0.0, "avg_logprob": -0.22994811051375383, "compression_ratio": 1.8511326860841424, "no_speech_prob": 3.4804957977030426e-05}, {"id": 933, "seek": 340772, "start": 3428.0, "end": 3431.64, "text": " So it's at the table here is looking at, the second row is looking at if we remove the", "tokens": [407, 309, 311, 412, 264, 3199, 510, 307, 1237, 412, 11, 264, 1150, 5386, 307, 1237, 412, 498, 321, 4159, 264], "temperature": 0.0, "avg_logprob": -0.22994811051375383, "compression_ratio": 1.8511326860841424, "no_speech_prob": 3.4804957977030426e-05}, {"id": 934, "seek": 340772, "start": 3431.64, "end": 3434.24, "text": " mass language model loss, what happens.", "tokens": [2758, 2856, 2316, 4470, 11, 437, 2314, 13], "temperature": 0.0, "avg_logprob": -0.22994811051375383, "compression_ratio": 1.8511326860841424, "no_speech_prob": 3.4804957977030426e-05}, {"id": 935, "seek": 340772, "start": 3434.24, "end": 3437.24, "text": " We see that it performs much worse without the mass language model loss.", "tokens": [492, 536, 300, 309, 26213, 709, 5324, 1553, 264, 2758, 2856, 2316, 4470, 13], "temperature": 0.0, "avg_logprob": -0.22994811051375383, "compression_ratio": 1.8511326860841424, "no_speech_prob": 3.4804957977030426e-05}, {"id": 936, "seek": 343724, "start": 3437.24, "end": 3438.8799999999997, "text": " So you really need both losses.", "tokens": [407, 291, 534, 643, 1293, 15352, 13], "temperature": 0.0, "avg_logprob": -0.2169022978397838, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.7776366803445853e-05}, {"id": 937, "seek": 343724, "start": 3438.8799999999997, "end": 3444.64, "text": " The intuition there was the mass language model loss helps to encode just general language", "tokens": [440, 24002, 456, 390, 264, 2758, 2856, 2316, 4470, 3665, 281, 2058, 1429, 445, 2674, 2856], "temperature": 0.0, "avg_logprob": -0.2169022978397838, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.7776366803445853e-05}, {"id": 938, "seek": 343724, "start": 3444.64, "end": 3446.9199999999996, "text": " understanding.", "tokens": [3701, 13], "temperature": 0.0, "avg_logprob": -0.2169022978397838, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.7776366803445853e-05}, {"id": 939, "seek": 343724, "start": 3446.9199999999996, "end": 3451.0, "text": " And then training BERT for longer performs much worse than using this entity or placement", "tokens": [400, 550, 3097, 363, 31479, 337, 2854, 26213, 709, 5324, 813, 1228, 341, 13977, 420, 17257], "temperature": 0.0, "avg_logprob": -0.2169022978397838, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.7776366803445853e-05}, {"id": 940, "seek": 343724, "start": 3451.0, "end": 3452.0, "text": " loss.", "tokens": [4470, 13], "temperature": 0.0, "avg_logprob": -0.2169022978397838, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.7776366803445853e-05}, {"id": 941, "seek": 343724, "start": 3452.0, "end": 3455.16, "text": " So this motivates even farther that you really do need.", "tokens": [407, 341, 42569, 754, 20344, 300, 291, 534, 360, 643, 13], "temperature": 0.0, "avg_logprob": -0.2169022978397838, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.7776366803445853e-05}, {"id": 942, "seek": 343724, "start": 3455.16, "end": 3459.24, "text": " Or the entity or placement loss is actually really helping encode more knowledge in these", "tokens": [1610, 264, 13977, 420, 17257, 4470, 307, 767, 534, 4315, 2058, 1429, 544, 3601, 294, 613], "temperature": 0.0, "avg_logprob": -0.2169022978397838, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.7776366803445853e-05}, {"id": 943, "seek": 343724, "start": 3459.24, "end": 3463.4399999999996, "text": " language models.", "tokens": [2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2169022978397838, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.7776366803445853e-05}, {"id": 944, "seek": 343724, "start": 3463.4399999999996, "end": 3466.9199999999996, "text": " So in addition to corrupting the data, we're also going to look at, can we just mass", "tokens": [407, 294, 4500, 281, 17366, 278, 264, 1412, 11, 321, 434, 611, 516, 281, 574, 412, 11, 393, 321, 445, 2758], "temperature": 0.0, "avg_logprob": -0.2169022978397838, "compression_ratio": 1.7619047619047619, "no_speech_prob": 1.7776366803445853e-05}, {"id": 945, "seek": 346692, "start": 3466.92, "end": 3467.92, "text": " the data differently?", "tokens": [264, 1412, 7614, 30], "temperature": 0.0, "avg_logprob": -0.17291749533960374, "compression_ratio": 1.7195945945945945, "no_speech_prob": 9.016485273605213e-05}, {"id": 946, "seek": 346692, "start": 3467.92, "end": 3470.8, "text": " Can we be more clever about how we do the masking?", "tokens": [1664, 321, 312, 544, 13494, 466, 577, 321, 360, 264, 31226, 30], "temperature": 0.0, "avg_logprob": -0.17291749533960374, "compression_ratio": 1.7195945945945945, "no_speech_prob": 9.016485273605213e-05}, {"id": 947, "seek": 346692, "start": 3470.8, "end": 3473.52, "text": " And this is how thread and several recent works.", "tokens": [400, 341, 307, 577, 7207, 293, 2940, 5162, 1985, 13], "temperature": 0.0, "avg_logprob": -0.17291749533960374, "compression_ratio": 1.7195945945945945, "no_speech_prob": 9.016485273605213e-05}, {"id": 948, "seek": 346692, "start": 3473.52, "end": 3475.6800000000003, "text": " So there's actually another paper called Ernie.", "tokens": [407, 456, 311, 767, 1071, 3035, 1219, 3300, 2766, 13], "temperature": 0.0, "avg_logprob": -0.17291749533960374, "compression_ratio": 1.7195945945945945, "no_speech_prob": 9.016485273605213e-05}, {"id": 949, "seek": 346692, "start": 3475.6800000000003, "end": 3477.92, "text": " So this is different than the one we talked about before.", "tokens": [407, 341, 307, 819, 813, 264, 472, 321, 2825, 466, 949, 13], "temperature": 0.0, "avg_logprob": -0.17291749533960374, "compression_ratio": 1.7195945945945945, "no_speech_prob": 9.016485273605213e-05}, {"id": 950, "seek": 346692, "start": 3477.92, "end": 3481.44, "text": " And this is enhanced representation through knowledge integration.", "tokens": [400, 341, 307, 21191, 10290, 807, 3601, 10980, 13], "temperature": 0.0, "avg_logprob": -0.17291749533960374, "compression_ratio": 1.7195945945945945, "no_speech_prob": 9.016485273605213e-05}, {"id": 951, "seek": 346692, "start": 3481.44, "end": 3486.56, "text": " And what they do is show improvements on downstream Chinese and LP tasks by doing phrase level", "tokens": [400, 437, 436, 360, 307, 855, 13797, 322, 30621, 4649, 293, 38095, 9608, 538, 884, 9535, 1496], "temperature": 0.0, "avg_logprob": -0.17291749533960374, "compression_ratio": 1.7195945945945945, "no_speech_prob": 9.016485273605213e-05}, {"id": 952, "seek": 346692, "start": 3486.56, "end": 3488.56, "text": " and entity level masking.", "tokens": [293, 13977, 1496, 31226, 13], "temperature": 0.0, "avg_logprob": -0.17291749533960374, "compression_ratio": 1.7195945945945945, "no_speech_prob": 9.016485273605213e-05}, {"id": 953, "seek": 346692, "start": 3488.56, "end": 3493.52, "text": " So instead of just masking out sub words, they're going to mask out phrases of multiple words", "tokens": [407, 2602, 295, 445, 31226, 484, 1422, 2283, 11, 436, 434, 516, 281, 6094, 484, 20312, 295, 3866, 2283], "temperature": 0.0, "avg_logprob": -0.17291749533960374, "compression_ratio": 1.7195945945945945, "no_speech_prob": 9.016485273605213e-05}, {"id": 954, "seek": 349352, "start": 3493.52, "end": 3498.72, "text": " and entities before phrase and entity which corresponds to some entity in the text that", "tokens": [293, 16667, 949, 9535, 293, 13977, 597, 23249, 281, 512, 13977, 294, 264, 2487, 300], "temperature": 0.0, "avg_logprob": -0.16785830008883437, "compression_ratio": 1.8237410071942446, "no_speech_prob": 6.642951120738871e-06}, {"id": 955, "seek": 349352, "start": 3498.72, "end": 3503.72, "text": " they might find like any art techniques, for example.", "tokens": [436, 1062, 915, 411, 604, 1523, 7512, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.16785830008883437, "compression_ratio": 1.8237410071942446, "no_speech_prob": 6.642951120738871e-06}, {"id": 956, "seek": 349352, "start": 3503.72, "end": 3507.48, "text": " And then the second work is actually something you heard about in the last lecture, which", "tokens": [400, 550, 264, 1150, 589, 307, 767, 746, 291, 2198, 466, 294, 264, 1036, 7991, 11, 597], "temperature": 0.0, "avg_logprob": -0.16785830008883437, "compression_ratio": 1.8237410071942446, "no_speech_prob": 6.642951120738871e-06}, {"id": 957, "seek": 349352, "start": 3507.48, "end": 3512.44, "text": " is the idea of using salient span masking to mask out salient spans.", "tokens": [307, 264, 1558, 295, 1228, 1845, 1196, 16174, 31226, 281, 6094, 484, 1845, 1196, 44086, 13], "temperature": 0.0, "avg_logprob": -0.16785830008883437, "compression_ratio": 1.8237410071942446, "no_speech_prob": 6.642951120738871e-06}, {"id": 958, "seek": 349352, "start": 3512.44, "end": 3514.92, "text": " And a salient span is just a named entity or a date.", "tokens": [400, 257, 1845, 1196, 16174, 307, 445, 257, 4926, 13977, 420, 257, 4002, 13], "temperature": 0.0, "avg_logprob": -0.16785830008883437, "compression_ratio": 1.8237410071942446, "no_speech_prob": 6.642951120738871e-06}, {"id": 959, "seek": 349352, "start": 3514.92, "end": 3518.28, "text": " So you can see this is pretty similar to what Ernie is doing.", "tokens": [407, 291, 393, 536, 341, 307, 1238, 2531, 281, 437, 3300, 2766, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.16785830008883437, "compression_ratio": 1.8237410071942446, "no_speech_prob": 6.642951120738871e-06}, {"id": 960, "seek": 349352, "start": 3518.28, "end": 3523.2, "text": " And they found that using salient span masking actually significantly helped T5 performance", "tokens": [400, 436, 1352, 300, 1228, 1845, 1196, 16174, 31226, 767, 10591, 4254, 314, 20, 3389], "temperature": 0.0, "avg_logprob": -0.16785830008883437, "compression_ratio": 1.8237410071942446, "no_speech_prob": 6.642951120738871e-06}, {"id": 961, "seek": 352320, "start": 3523.2, "end": 3528.3999999999996, "text": " on these closed domain question answering tasks.", "tokens": [322, 613, 5395, 9274, 1168, 13430, 9608, 13], "temperature": 0.0, "avg_logprob": -0.19958864473829083, "compression_ratio": 1.7531914893617022, "no_speech_prob": 1.3005567780055571e-05}, {"id": 962, "seek": 352320, "start": 3528.3999999999996, "end": 3532.04, "text": " So just to make sure we're all on the same page with the different masking techniques, this", "tokens": [407, 445, 281, 652, 988, 321, 434, 439, 322, 264, 912, 3028, 365, 264, 819, 31226, 7512, 11, 341], "temperature": 0.0, "avg_logprob": -0.19958864473829083, "compression_ratio": 1.7531914893617022, "no_speech_prob": 1.3005567780055571e-05}, {"id": 963, "seek": 352320, "start": 3532.04, "end": 3536.6, "text": " diagram from the Ernie paper is comparing to what Bert does versus what Ernie does.", "tokens": [10686, 490, 264, 3300, 2766, 3035, 307, 15763, 281, 437, 29594, 775, 5717, 437, 3300, 2766, 775, 13], "temperature": 0.0, "avg_logprob": -0.19958864473829083, "compression_ratio": 1.7531914893617022, "no_speech_prob": 1.3005567780055571e-05}, {"id": 964, "seek": 352320, "start": 3536.6, "end": 3540.9199999999996, "text": " The top shows that Ernie massed out the sub word tokens or that Bert massed out the sub", "tokens": [440, 1192, 3110, 300, 3300, 2766, 2758, 292, 484, 264, 1422, 1349, 22667, 420, 300, 29594, 2758, 292, 484, 264, 1422], "temperature": 0.0, "avg_logprob": -0.19958864473829083, "compression_ratio": 1.7531914893617022, "no_speech_prob": 1.3005567780055571e-05}, {"id": 965, "seek": 352320, "start": 3540.9199999999996, "end": 3546.52, "text": " word tokens, whereas Ernie massed out phrases like a series of as well as entities like JK", "tokens": [1349, 22667, 11, 9735, 3300, 2766, 2758, 292, 484, 20312, 411, 257, 2638, 295, 382, 731, 382, 16667, 411, 35973], "temperature": 0.0, "avg_logprob": -0.19958864473829083, "compression_ratio": 1.7531914893617022, "no_speech_prob": 1.3005567780055571e-05}, {"id": 966, "seek": 352320, "start": 3546.52, "end": 3548.12, "text": " rolling.", "tokens": [9439, 13], "temperature": 0.0, "avg_logprob": -0.19958864473829083, "compression_ratio": 1.7531914893617022, "no_speech_prob": 1.3005567780055571e-05}, {"id": 967, "seek": 354812, "start": 3548.12, "end": 3555.7999999999997, "text": " There's some interesting results on showing that salient span masking is helping encode", "tokens": [821, 311, 512, 1880, 3542, 322, 4099, 300, 1845, 1196, 16174, 31226, 307, 4315, 2058, 1429], "temperature": 0.0, "avg_logprob": -0.15598608016967774, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.813601066824049e-05}, {"id": 968, "seek": 354812, "start": 3555.7999999999997, "end": 3558.72, "text": " more knowledge in these representations.", "tokens": [544, 3601, 294, 613, 33358, 13], "temperature": 0.0, "avg_logprob": -0.15598608016967774, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.813601066824049e-05}, {"id": 969, "seek": 354812, "start": 3558.72, "end": 3562.52, "text": " So on the left, we're looking at the results of the original paper that proposed salient", "tokens": [407, 322, 264, 1411, 11, 321, 434, 1237, 412, 264, 3542, 295, 264, 3380, 3035, 300, 10348, 1845, 1196], "temperature": 0.0, "avg_logprob": -0.15598608016967774, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.813601066824049e-05}, {"id": 970, "seek": 354812, "start": 3562.52, "end": 3563.8399999999997, "text": " span masking.", "tokens": [16174, 31226, 13], "temperature": 0.0, "avg_logprob": -0.15598608016967774, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.813601066824049e-05}, {"id": 971, "seek": 354812, "start": 3563.8399999999997, "end": 3567.3199999999997, "text": " So this is the realm work.", "tokens": [407, 341, 307, 264, 15355, 589, 13], "temperature": 0.0, "avg_logprob": -0.15598608016967774, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.813601066824049e-05}, {"id": 972, "seek": 354812, "start": 3567.3199999999997, "end": 3570.7599999999998, "text": " And the idea here was that they were training a knowledge retriever.", "tokens": [400, 264, 1558, 510, 390, 300, 436, 645, 3097, 257, 3601, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.15598608016967774, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.813601066824049e-05}, {"id": 973, "seek": 354812, "start": 3570.7599999999998, "end": 3575.3199999999997, "text": " So it's actually more of an external memory class of techniques, but they find that by using", "tokens": [407, 309, 311, 767, 544, 295, 364, 8320, 4675, 1508, 295, 7512, 11, 457, 436, 915, 300, 538, 1228], "temperature": 0.0, "avg_logprob": -0.15598608016967774, "compression_ratio": 1.7142857142857142, "no_speech_prob": 6.813601066824049e-05}, {"id": 974, "seek": 357532, "start": 3575.32, "end": 3581.0800000000004, "text": " the salient span masking technique, they could actually train a much better knowledge retriever.", "tokens": [264, 1845, 1196, 16174, 31226, 6532, 11, 436, 727, 767, 3847, 257, 709, 1101, 3601, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.11876158280806108, "compression_ratio": 1.768939393939394, "no_speech_prob": 1.1299885954940692e-05}, {"id": 975, "seek": 357532, "start": 3581.0800000000004, "end": 3585.88, "text": " So it's a good example of how these techniques are really complimentary.", "tokens": [407, 309, 311, 257, 665, 1365, 295, 577, 613, 7512, 366, 534, 47162, 13], "temperature": 0.0, "avg_logprob": -0.11876158280806108, "compression_ratio": 1.768939393939394, "no_speech_prob": 1.1299885954940692e-05}, {"id": 976, "seek": 357532, "start": 3585.88, "end": 3589.96, "text": " So while I presented three classes of techniques, you can definitely get benefits by doing multiple", "tokens": [407, 1339, 286, 8212, 1045, 5359, 295, 7512, 11, 291, 393, 2138, 483, 5311, 538, 884, 3866], "temperature": 0.0, "avg_logprob": -0.11876158280806108, "compression_ratio": 1.768939393939394, "no_speech_prob": 1.1299885954940692e-05}, {"id": 977, "seek": 357532, "start": 3589.96, "end": 3592.28, "text": " techniques together.", "tokens": [7512, 1214, 13], "temperature": 0.0, "avg_logprob": -0.11876158280806108, "compression_ratio": 1.768939393939394, "no_speech_prob": 1.1299885954940692e-05}, {"id": 978, "seek": 357532, "start": 3592.28, "end": 3596.36, "text": " And they found that doing salient span masking compared to using masking from Bert, which", "tokens": [400, 436, 1352, 300, 884, 1845, 1196, 16174, 31226, 5347, 281, 1228, 31226, 490, 29594, 11, 597], "temperature": 0.0, "avg_logprob": -0.11876158280806108, "compression_ratio": 1.768939393939394, "no_speech_prob": 1.1299885954940692e-05}, {"id": 979, "seek": 357532, "start": 3596.36, "end": 3601.76, "text": " would be the random uniform masks or doing random masking of spans from a paper called", "tokens": [576, 312, 264, 4974, 9452, 11830, 420, 884, 4974, 31226, 295, 44086, 490, 257, 3035, 1219], "temperature": 0.0, "avg_logprob": -0.11876158280806108, "compression_ratio": 1.768939393939394, "no_speech_prob": 1.1299885954940692e-05}, {"id": 980, "seek": 360176, "start": 3601.76, "end": 3606.6400000000003, "text": " Spanbert, it performs much better to do salient span masking.", "tokens": [1738, 282, 4290, 11, 309, 26213, 709, 1101, 281, 360, 1845, 1196, 16174, 31226, 13], "temperature": 0.0, "avg_logprob": -0.18492319186528525, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.247281008749269e-05}, {"id": 981, "seek": 360176, "start": 3606.6400000000003, "end": 3613.76, "text": " So you see a 38 exact match score versus like a 32 exact match score, for instance.", "tokens": [407, 291, 536, 257, 12843, 1900, 2995, 6175, 5717, 411, 257, 8858, 1900, 2995, 6175, 11, 337, 5197, 13], "temperature": 0.0, "avg_logprob": -0.18492319186528525, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.247281008749269e-05}, {"id": 982, "seek": 360176, "start": 3613.76, "end": 3620.1600000000003, "text": " And on the right, we have results from fine tuning T5 with either salient span masking or", "tokens": [400, 322, 264, 558, 11, 321, 362, 3542, 490, 2489, 15164, 314, 20, 365, 2139, 1845, 1196, 16174, 31226, 420], "temperature": 0.0, "avg_logprob": -0.18492319186528525, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.247281008749269e-05}, {"id": 983, "seek": 360176, "start": 3620.1600000000003, "end": 3623.0800000000004, "text": " the span corruption task that you saw on assignment five.", "tokens": [264, 16174, 17959, 5633, 300, 291, 1866, 322, 15187, 1732, 13], "temperature": 0.0, "avg_logprob": -0.18492319186528525, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.247281008749269e-05}, {"id": 984, "seek": 360176, "start": 3623.0800000000004, "end": 3627.28, "text": " And you can see that on these different QA data sets, salient span masking is significantly", "tokens": [400, 291, 393, 536, 300, 322, 613, 819, 1249, 32, 1412, 6352, 11, 1845, 1196, 16174, 31226, 307, 10591], "temperature": 0.0, "avg_logprob": -0.18492319186528525, "compression_ratio": 1.711111111111111, "no_speech_prob": 2.247281008749269e-05}, {"id": 985, "seek": 362728, "start": 3627.28, "end": 3631.92, "text": " better than just using the span corruption technique.", "tokens": [1101, 813, 445, 1228, 264, 16174, 17959, 6532, 13], "temperature": 0.0, "avg_logprob": -0.1383444358562601, "compression_ratio": 1.8450704225352113, "no_speech_prob": 1.1658718904072884e-05}, {"id": 986, "seek": 362728, "start": 3631.92, "end": 3636.8, "text": " So this really suggests that during the salient span masking and masking out the salient", "tokens": [407, 341, 534, 13409, 300, 1830, 264, 1845, 1196, 16174, 31226, 293, 31226, 484, 264, 1845, 1196], "temperature": 0.0, "avg_logprob": -0.1383444358562601, "compression_ratio": 1.8450704225352113, "no_speech_prob": 1.1658718904072884e-05}, {"id": 987, "seek": 362728, "start": 3636.8, "end": 3646.52, "text": " spans of these entities is in fact helping to encode more knowledge in these language models.", "tokens": [44086, 295, 613, 16667, 307, 294, 1186, 4315, 281, 2058, 1429, 544, 3601, 294, 613, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1383444358562601, "compression_ratio": 1.8450704225352113, "no_speech_prob": 1.1658718904072884e-05}, {"id": 988, "seek": 362728, "start": 3646.52, "end": 3649.76, "text": " So to recap, we talked about three different classes of techniques to add knowledge to", "tokens": [407, 281, 20928, 11, 321, 2825, 466, 1045, 819, 5359, 295, 7512, 281, 909, 3601, 281], "temperature": 0.0, "avg_logprob": -0.1383444358562601, "compression_ratio": 1.8450704225352113, "no_speech_prob": 1.1658718904072884e-05}, {"id": 989, "seek": 362728, "start": 3649.76, "end": 3651.96, "text": " language models.", "tokens": [2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1383444358562601, "compression_ratio": 1.8450704225352113, "no_speech_prob": 1.1658718904072884e-05}, {"id": 990, "seek": 362728, "start": 3651.96, "end": 3654.36, "text": " We talked about using pre trained entity embeddings.", "tokens": [492, 2825, 466, 1228, 659, 8895, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.1383444358562601, "compression_ratio": 1.8450704225352113, "no_speech_prob": 1.1658718904072884e-05}, {"id": 991, "seek": 365436, "start": 3654.36, "end": 3658.7200000000003, "text": " These weren't too difficult to apply to existing architectures and is a way to leverage this", "tokens": [1981, 4999, 380, 886, 2252, 281, 3079, 281, 6741, 6331, 1303, 293, 307, 257, 636, 281, 13982, 341], "temperature": 0.0, "avg_logprob": -0.2170111338297526, "compression_ratio": 1.6483516483516483, "no_speech_prob": 2.5464278223807923e-05}, {"id": 992, "seek": 365436, "start": 3658.7200000000003, "end": 3661.08, "text": " knowledge graph pre training.", "tokens": [3601, 4295, 659, 3097, 13], "temperature": 0.0, "avg_logprob": -0.2170111338297526, "compression_ratio": 1.6483516483516483, "no_speech_prob": 2.5464278223807923e-05}, {"id": 993, "seek": 365436, "start": 3661.08, "end": 3666.36, "text": " But it's rather inject way of incorporating knowledge and it could be hard to interpret.", "tokens": [583, 309, 311, 2831, 10711, 636, 295, 33613, 3601, 293, 309, 727, 312, 1152, 281, 7302, 13], "temperature": 0.0, "avg_logprob": -0.2170111338297526, "compression_ratio": 1.6483516483516483, "no_speech_prob": 2.5464278223807923e-05}, {"id": 994, "seek": 365436, "start": 3666.36, "end": 3670.1200000000003, "text": " We also talked about approaches to add an external memory.", "tokens": [492, 611, 2825, 466, 11587, 281, 909, 364, 8320, 4675, 13], "temperature": 0.0, "avg_logprob": -0.2170111338297526, "compression_ratio": 1.6483516483516483, "no_speech_prob": 2.5464278223807923e-05}, {"id": 995, "seek": 365436, "start": 3670.1200000000003, "end": 3672.96, "text": " This could support modifying the knowledge base.", "tokens": [639, 727, 1406, 42626, 264, 3601, 3096, 13], "temperature": 0.0, "avg_logprob": -0.2170111338297526, "compression_ratio": 1.6483516483516483, "no_speech_prob": 2.5464278223807923e-05}, {"id": 996, "seek": 365436, "start": 3672.96, "end": 3675.52, "text": " It was also easier to interpret.", "tokens": [467, 390, 611, 3571, 281, 7302, 13], "temperature": 0.0, "avg_logprob": -0.2170111338297526, "compression_ratio": 1.6483516483516483, "no_speech_prob": 2.5464278223807923e-05}, {"id": 997, "seek": 365436, "start": 3675.52, "end": 3680.4, "text": " But they tended to be more complex in implementation like we saw KGLM and they also required more", "tokens": [583, 436, 34732, 281, 312, 544, 3997, 294, 11420, 411, 321, 1866, 591, 19440, 44, 293, 436, 611, 4739, 544], "temperature": 0.0, "avg_logprob": -0.2170111338297526, "compression_ratio": 1.6483516483516483, "no_speech_prob": 2.5464278223807923e-05}, {"id": 998, "seek": 368040, "start": 3680.4, "end": 3684.7200000000003, "text": " memory like we saw the K and N LM approach.", "tokens": [4675, 411, 321, 1866, 264, 591, 293, 426, 441, 44, 3109, 13], "temperature": 0.0, "avg_logprob": -0.23060318139883187, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.24102571135154e-05}, {"id": 999, "seek": 368040, "start": 3684.7200000000003, "end": 3688.04, "text": " And then finally we talked about modifying the training data.", "tokens": [400, 550, 2721, 321, 2825, 466, 42626, 264, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.23060318139883187, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.24102571135154e-05}, {"id": 1000, "seek": 368040, "start": 3688.04, "end": 3691.48, "text": " So this requires no model changes or additional computation.", "tokens": [407, 341, 7029, 572, 2316, 2962, 420, 4497, 24903, 13], "temperature": 0.0, "avg_logprob": -0.23060318139883187, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.24102571135154e-05}, {"id": 1001, "seek": 368040, "start": 3691.48, "end": 3694.08, "text": " It also might be the easiest to theoretically analyze.", "tokens": [467, 611, 1062, 312, 264, 12889, 281, 29400, 12477, 13], "temperature": 0.0, "avg_logprob": -0.23060318139883187, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.24102571135154e-05}, {"id": 1002, "seek": 368040, "start": 3694.08, "end": 3697.6800000000003, "text": " So it's actually an active area research right now.", "tokens": [407, 309, 311, 767, 364, 4967, 1859, 2132, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.23060318139883187, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.24102571135154e-05}, {"id": 1003, "seek": 368040, "start": 3697.6800000000003, "end": 3702.96, "text": " But still open question if modifying the training data is always as effective as model changes", "tokens": [583, 920, 1269, 1168, 498, 42626, 264, 3097, 1412, 307, 1009, 382, 4942, 382, 2316, 2962], "temperature": 0.0, "avg_logprob": -0.23060318139883187, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.24102571135154e-05}, {"id": 1004, "seek": 368040, "start": 3702.96, "end": 3706.88, "text": " and what the trade offs are in terms of how the data required versus doing one of these", "tokens": [293, 437, 264, 4923, 39457, 366, 294, 2115, 295, 577, 264, 1412, 4739, 5717, 884, 472, 295, 613], "temperature": 0.0, "avg_logprob": -0.23060318139883187, "compression_ratio": 1.7272727272727273, "no_speech_prob": 1.24102571135154e-05}, {"id": 1005, "seek": 370688, "start": 3706.88, "end": 3712.76, "text": " other knowledge enhancement approaches.", "tokens": [661, 3601, 40776, 11587, 13], "temperature": 0.0, "avg_logprob": -0.2682846886771066, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.20338978478685e-05}, {"id": 1006, "seek": 370688, "start": 3712.76, "end": 3715.2000000000003, "text": " So that leads us to section three.", "tokens": [407, 300, 6689, 505, 281, 3541, 1045, 13], "temperature": 0.0, "avg_logprob": -0.2682846886771066, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.20338978478685e-05}, {"id": 1007, "seek": 370688, "start": 3715.2000000000003, "end": 3719.7200000000003, "text": " So I guess I'll pause again for questions.", "tokens": [407, 286, 2041, 286, 603, 10465, 797, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.2682846886771066, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.20338978478685e-05}, {"id": 1008, "seek": 370688, "start": 3719.7200000000003, "end": 3725.1600000000003, "text": " I think we may be good.", "tokens": [286, 519, 321, 815, 312, 665, 13], "temperature": 0.0, "avg_logprob": -0.2682846886771066, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.20338978478685e-05}, {"id": 1009, "seek": 370688, "start": 3725.1600000000003, "end": 3726.1600000000003, "text": " Awesome.", "tokens": [10391, 13], "temperature": 0.0, "avg_logprob": -0.2682846886771066, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.20338978478685e-05}, {"id": 1010, "seek": 370688, "start": 3726.1600000000003, "end": 3727.1600000000003, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2682846886771066, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.20338978478685e-05}, {"id": 1011, "seek": 370688, "start": 3727.1600000000003, "end": 3730.96, "text": " So section three is about how researchers are actually going about evaluating the knowledge", "tokens": [407, 3541, 1045, 307, 466, 577, 10309, 366, 767, 516, 466, 27479, 264, 3601], "temperature": 0.0, "avg_logprob": -0.2682846886771066, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.20338978478685e-05}, {"id": 1012, "seek": 370688, "start": 3730.96, "end": 3732.6800000000003, "text": " and language models.", "tokens": [293, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2682846886771066, "compression_ratio": 1.4944444444444445, "no_speech_prob": 6.20338978478685e-05}, {"id": 1013, "seek": 373268, "start": 3732.68, "end": 3737.96, "text": " And I guess how some of the techniques we actually just talked about stand up in this evaluation.", "tokens": [400, 286, 2041, 577, 512, 295, 264, 7512, 321, 767, 445, 2825, 466, 1463, 493, 294, 341, 13344, 13], "temperature": 0.0, "avg_logprob": -0.1483759268736228, "compression_ratio": 1.7964285714285715, "no_speech_prob": 5.390603473642841e-05}, {"id": 1014, "seek": 373268, "start": 3737.96, "end": 3742.56, "text": " So first we're going to talk about probes which don't require any fine tuning of the language", "tokens": [407, 700, 321, 434, 516, 281, 751, 466, 1239, 279, 597, 500, 380, 3651, 604, 2489, 15164, 295, 264, 2856], "temperature": 0.0, "avg_logprob": -0.1483759268736228, "compression_ratio": 1.7964285714285715, "no_speech_prob": 5.390603473642841e-05}, {"id": 1015, "seek": 373268, "start": 3742.56, "end": 3743.56, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.1483759268736228, "compression_ratio": 1.7964285714285715, "no_speech_prob": 5.390603473642841e-05}, {"id": 1016, "seek": 373268, "start": 3743.56, "end": 3747.3199999999997, "text": " And then we're going to talk about downstream tasks which look at how well do these pre-trained", "tokens": [400, 550, 321, 434, 516, 281, 751, 466, 30621, 9608, 597, 574, 412, 577, 731, 360, 613, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.1483759268736228, "compression_ratio": 1.7964285714285715, "no_speech_prob": 5.390603473642841e-05}, {"id": 1017, "seek": 373268, "start": 3747.3199999999997, "end": 3752.7999999999997, "text": " representations actually transfer their knowledge to other tasks.", "tokens": [33358, 767, 5003, 641, 3601, 281, 661, 9608, 13], "temperature": 0.0, "avg_logprob": -0.1483759268736228, "compression_ratio": 1.7964285714285715, "no_speech_prob": 5.390603473642841e-05}, {"id": 1018, "seek": 373268, "start": 3752.7999999999997, "end": 3755.7999999999997, "text": " So one of the initial works in this area was called LAMA.", "tokens": [407, 472, 295, 264, 5883, 1985, 294, 341, 1859, 390, 1219, 441, 38136, 13], "temperature": 0.0, "avg_logprob": -0.1483759268736228, "compression_ratio": 1.7964285714285715, "no_speech_prob": 5.390603473642841e-05}, {"id": 1019, "seek": 373268, "start": 3755.7999999999997, "end": 3760.68, "text": " And this really started a series of works to look into how much knowledge is already", "tokens": [400, 341, 534, 1409, 257, 2638, 295, 1985, 281, 574, 666, 577, 709, 3601, 307, 1217], "temperature": 0.0, "avg_logprob": -0.1483759268736228, "compression_ratio": 1.7964285714285715, "no_speech_prob": 5.390603473642841e-05}, {"id": 1020, "seek": 376068, "start": 3760.68, "end": 3763.68, "text": " encoded in these language models.", "tokens": [2058, 12340, 294, 613, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15682502341481436, "compression_ratio": 1.8371212121212122, "no_speech_prob": 1.12999932753155e-05}, {"id": 1021, "seek": 376068, "start": 3763.68, "end": 3768.0, "text": " So their question was how much relational, common sense and factual knowledge is in off", "tokens": [407, 641, 1168, 390, 577, 709, 38444, 11, 2689, 2020, 293, 48029, 3601, 307, 294, 766], "temperature": 0.0, "avg_logprob": -0.15682502341481436, "compression_ratio": 1.8371212121212122, "no_speech_prob": 1.12999932753155e-05}, {"id": 1022, "seek": 376068, "start": 3768.0, "end": 3769.3199999999997, "text": " the shelf language models.", "tokens": [264, 15222, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15682502341481436, "compression_ratio": 1.8371212121212122, "no_speech_prob": 1.12999932753155e-05}, {"id": 1023, "seek": 376068, "start": 3769.3199999999997, "end": 3774.3599999999997, "text": " So this is just taking pre-trained language models and evaluating the knowledge in them.", "tokens": [407, 341, 307, 445, 1940, 659, 12, 17227, 2001, 2856, 5245, 293, 27479, 264, 3601, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.15682502341481436, "compression_ratio": 1.8371212121212122, "no_speech_prob": 1.12999932753155e-05}, {"id": 1024, "seek": 376068, "start": 3774.3599999999997, "end": 3777.72, "text": " And this is without any additional training or fine tuning.", "tokens": [400, 341, 307, 1553, 604, 4497, 3097, 420, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.15682502341481436, "compression_ratio": 1.8371212121212122, "no_speech_prob": 1.12999932753155e-05}, {"id": 1025, "seek": 376068, "start": 3777.72, "end": 3781.08, "text": " So they mainly constructed a set of what they're for to its closed statements.", "tokens": [407, 436, 8704, 17083, 257, 992, 295, 437, 436, 434, 337, 281, 1080, 5395, 12363, 13], "temperature": 0.0, "avg_logprob": -0.15682502341481436, "compression_ratio": 1.8371212121212122, "no_speech_prob": 1.12999932753155e-05}, {"id": 1026, "seek": 376068, "start": 3781.08, "end": 3784.7599999999998, "text": " And these are just the fill in the blank statements that we actually drew from at the beginning", "tokens": [400, 613, 366, 445, 264, 2836, 294, 264, 8247, 12363, 300, 321, 767, 12804, 490, 412, 264, 2863], "temperature": 0.0, "avg_logprob": -0.15682502341481436, "compression_ratio": 1.8371212121212122, "no_speech_prob": 1.12999932753155e-05}, {"id": 1027, "seek": 376068, "start": 3784.7599999999998, "end": 3785.7599999999998, "text": " of the talk.", "tokens": [295, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.15682502341481436, "compression_ratio": 1.8371212121212122, "no_speech_prob": 1.12999932753155e-05}, {"id": 1028, "seek": 378576, "start": 3785.76, "end": 3790.88, "text": " And we'll have some more examples here.", "tokens": [400, 321, 603, 362, 512, 544, 5110, 510, 13], "temperature": 0.0, "avg_logprob": -0.2073214884554402, "compression_ratio": 1.8189300411522633, "no_speech_prob": 1.5445781173184514e-05}, {"id": 1029, "seek": 378576, "start": 3790.88, "end": 3794.7200000000003, "text": " And they mainly created these templates of closed statements using knowledge graph triples", "tokens": [400, 436, 8704, 2942, 613, 21165, 295, 5395, 12363, 1228, 3601, 4295, 1376, 2622], "temperature": 0.0, "avg_logprob": -0.2073214884554402, "compression_ratio": 1.8189300411522633, "no_speech_prob": 1.5445781173184514e-05}, {"id": 1030, "seek": 378576, "start": 3794.7200000000003, "end": 3799.2400000000002, "text": " and question answering pairs from existing data sets.", "tokens": [293, 1168, 13430, 15494, 490, 6741, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.2073214884554402, "compression_ratio": 1.8189300411522633, "no_speech_prob": 1.5445781173184514e-05}, {"id": 1031, "seek": 378576, "start": 3799.2400000000002, "end": 3803.5600000000004, "text": " They wanted to compare pre-trained language models to supervised relation extraction", "tokens": [814, 1415, 281, 6794, 659, 12, 17227, 2001, 2856, 5245, 281, 46533, 9721, 30197], "temperature": 0.0, "avg_logprob": -0.2073214884554402, "compression_ratio": 1.8189300411522633, "no_speech_prob": 1.5445781173184514e-05}, {"id": 1032, "seek": 378576, "start": 3803.5600000000004, "end": 3807.8, "text": " and question answering systems to see how do these language models that were trained", "tokens": [293, 1168, 13430, 3652, 281, 536, 577, 360, 613, 2856, 5245, 300, 645, 8895], "temperature": 0.0, "avg_logprob": -0.2073214884554402, "compression_ratio": 1.8189300411522633, "no_speech_prob": 1.5445781173184514e-05}, {"id": 1033, "seek": 378576, "start": 3807.8, "end": 3813.1200000000003, "text": " in unsupervised fashion compared to these baseline systems that are not only supervised", "tokens": [294, 2693, 12879, 24420, 6700, 5347, 281, 613, 20518, 3652, 300, 366, 406, 787, 46533], "temperature": 0.0, "avg_logprob": -0.2073214884554402, "compression_ratio": 1.8189300411522633, "no_speech_prob": 1.5445781173184514e-05}, {"id": 1034, "seek": 381312, "start": 3813.12, "end": 3817.6, "text": " but really targeted for this task of knowledge extraction.", "tokens": [457, 534, 15045, 337, 341, 5633, 295, 3601, 30197, 13], "temperature": 0.0, "avg_logprob": -0.11877107219535764, "compression_ratio": 1.8805970149253732, "no_speech_prob": 2.318475344509352e-05}, {"id": 1035, "seek": 381312, "start": 3817.6, "end": 3821.8399999999997, "text": " And their goal was to evaluate the knowledge in existing pre-trained language models.", "tokens": [400, 641, 3387, 390, 281, 13059, 264, 3601, 294, 6741, 659, 12, 17227, 2001, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11877107219535764, "compression_ratio": 1.8805970149253732, "no_speech_prob": 2.318475344509352e-05}, {"id": 1036, "seek": 381312, "start": 3821.8399999999997, "end": 3826.3599999999997, "text": " And a key point about this is like they're just using the language models as they are available", "tokens": [400, 257, 2141, 935, 466, 341, 307, 411, 436, 434, 445, 1228, 264, 2856, 5245, 382, 436, 366, 2435], "temperature": 0.0, "avg_logprob": -0.11877107219535764, "compression_ratio": 1.8805970149253732, "no_speech_prob": 2.318475344509352e-05}, {"id": 1037, "seek": 381312, "start": 3826.3599999999997, "end": 3827.6, "text": " to researchers.", "tokens": [281, 10309, 13], "temperature": 0.0, "avg_logprob": -0.11877107219535764, "compression_ratio": 1.8805970149253732, "no_speech_prob": 2.318475344509352e-05}, {"id": 1038, "seek": 381312, "start": 3827.6, "end": 3831.52, "text": " So this means there could be differences in the pre-trained corpora, for example.", "tokens": [407, 341, 1355, 456, 727, 312, 7300, 294, 264, 659, 12, 17227, 2001, 6804, 64, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.11877107219535764, "compression_ratio": 1.8805970149253732, "no_speech_prob": 2.318475344509352e-05}, {"id": 1039, "seek": 381312, "start": 3831.52, "end": 3834.6, "text": " So when you look at the following table in your comparing language models, also keep in", "tokens": [407, 562, 291, 574, 412, 264, 3480, 3199, 294, 428, 15763, 2856, 5245, 11, 611, 1066, 294], "temperature": 0.0, "avg_logprob": -0.11877107219535764, "compression_ratio": 1.8805970149253732, "no_speech_prob": 2.318475344509352e-05}, {"id": 1040, "seek": 381312, "start": 3834.6, "end": 3840.8399999999997, "text": " mind that these don't account for the differences in the pre-trained corpora.", "tokens": [1575, 300, 613, 500, 380, 2696, 337, 264, 7300, 294, 264, 659, 12, 17227, 2001, 6804, 64, 13], "temperature": 0.0, "avg_logprob": -0.11877107219535764, "compression_ratio": 1.8805970149253732, "no_speech_prob": 2.318475344509352e-05}, {"id": 1041, "seek": 384084, "start": 3840.84, "end": 3845.1600000000003, "text": " So a lot of these language models probably look familiar to you either from previous lectures", "tokens": [407, 257, 688, 295, 613, 2856, 5245, 1391, 574, 4963, 281, 291, 2139, 490, 3894, 16564], "temperature": 0.0, "avg_logprob": -0.18826814557685226, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3732889278326184e-05}, {"id": 1042, "seek": 384084, "start": 3845.1600000000003, "end": 3847.48, "text": " or maybe your final projects.", "tokens": [420, 1310, 428, 2572, 4455, 13], "temperature": 0.0, "avg_logprob": -0.18826814557685226, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3732889278326184e-05}, {"id": 1043, "seek": 384084, "start": 3847.48, "end": 3852.6000000000004, "text": " And what we see is that overall, birth base and birth large pre-trained models are performing", "tokens": [400, 437, 321, 536, 307, 300, 4787, 11, 3965, 3096, 293, 3965, 2416, 659, 12, 17227, 2001, 5245, 366, 10205], "temperature": 0.0, "avg_logprob": -0.18826814557685226, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3732889278326184e-05}, {"id": 1044, "seek": 384084, "start": 3852.6000000000004, "end": 3856.92, "text": " much better than the previous language or the other language models here.", "tokens": [709, 1101, 813, 264, 3894, 2856, 420, 264, 661, 2856, 5245, 510, 13], "temperature": 0.0, "avg_logprob": -0.18826814557685226, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3732889278326184e-05}, {"id": 1045, "seek": 384084, "start": 3856.92, "end": 3860.44, "text": " As I forgot to mention what mean precision at one is.", "tokens": [1018, 286, 5298, 281, 2152, 437, 914, 18356, 412, 472, 307, 13], "temperature": 0.0, "avg_logprob": -0.18826814557685226, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3732889278326184e-05}, {"id": 1046, "seek": 384084, "start": 3860.44, "end": 3861.92, "text": " This is a pretty simple metric.", "tokens": [639, 307, 257, 1238, 2199, 20678, 13], "temperature": 0.0, "avg_logprob": -0.18826814557685226, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3732889278326184e-05}, {"id": 1047, "seek": 384084, "start": 3861.92, "end": 3866.08, "text": " The idea is if you look at the blank and you look at the top predictions for the top", "tokens": [440, 1558, 307, 498, 291, 574, 412, 264, 8247, 293, 291, 574, 412, 264, 1192, 21264, 337, 264, 1192], "temperature": 0.0, "avg_logprob": -0.18826814557685226, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3732889278326184e-05}, {"id": 1048, "seek": 384084, "start": 3866.08, "end": 3868.36, "text": " prediction for the blank, is it correct or not?", "tokens": [17630, 337, 264, 8247, 11, 307, 309, 3006, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.18826814557685226, "compression_ratio": 1.8214285714285714, "no_speech_prob": 3.3732889278326184e-05}, {"id": 1049, "seek": 386836, "start": 3868.36, "end": 3872.48, "text": " That's what precision at one means, precision at 10 would be, let's look at the top 10", "tokens": [663, 311, 437, 18356, 412, 472, 1355, 11, 18356, 412, 1266, 576, 312, 11, 718, 311, 574, 412, 264, 1192, 1266], "temperature": 0.0, "avg_logprob": -0.2182762397910064, "compression_ratio": 1.7429718875502007, "no_speech_prob": 5.014525868318742e-06}, {"id": 1050, "seek": 386836, "start": 3872.48, "end": 3877.6, "text": " predictions, is it correct prediction in the top 10?", "tokens": [21264, 11, 307, 309, 3006, 17630, 294, 264, 1192, 1266, 30], "temperature": 0.0, "avg_logprob": -0.2182762397910064, "compression_ratio": 1.7429718875502007, "no_speech_prob": 5.014525868318742e-06}, {"id": 1051, "seek": 386836, "start": 3877.6, "end": 3882.76, "text": " So in addition to birth large and birth base performing well overall, we do see that", "tokens": [407, 294, 4500, 281, 3965, 2416, 293, 3965, 3096, 10205, 731, 4787, 11, 321, 360, 536, 300], "temperature": 0.0, "avg_logprob": -0.2182762397910064, "compression_ratio": 1.7429718875502007, "no_speech_prob": 5.014525868318742e-06}, {"id": 1052, "seek": 386836, "start": 3882.76, "end": 3886.6400000000003, "text": " in the TREX dataset, the relation extraction baseline is performing a bit better than", "tokens": [294, 264, 314, 3850, 55, 28872, 11, 264, 9721, 30197, 20518, 307, 10205, 257, 857, 1101, 813], "temperature": 0.0, "avg_logprob": -0.2182762397910064, "compression_ratio": 1.7429718875502007, "no_speech_prob": 5.014525868318742e-06}, {"id": 1053, "seek": 386836, "start": 3886.6400000000003, "end": 3888.84, "text": " birth.", "tokens": [3965, 13], "temperature": 0.0, "avg_logprob": -0.2182762397910064, "compression_ratio": 1.7429718875502007, "no_speech_prob": 5.014525868318742e-06}, {"id": 1054, "seek": 386836, "start": 3888.84, "end": 3892.96, "text": " One thing they notice here that's pretty interesting is that this dataset has a lot of different", "tokens": [1485, 551, 436, 3449, 510, 300, 311, 1238, 1880, 307, 300, 341, 28872, 575, 257, 688, 295, 819], "temperature": 0.0, "avg_logprob": -0.2182762397910064, "compression_ratio": 1.7429718875502007, "no_speech_prob": 5.014525868318742e-06}, {"id": 1055, "seek": 386836, "start": 3892.96, "end": 3893.96, "text": " types of relations.", "tokens": [3467, 295, 2299, 13], "temperature": 0.0, "avg_logprob": -0.2182762397910064, "compression_ratio": 1.7429718875502007, "no_speech_prob": 5.014525868318742e-06}, {"id": 1056, "seek": 389396, "start": 3893.96, "end": 3898.48, "text": " And relations can be classified in terms of are they a one-to-one relation, are they", "tokens": [400, 2299, 393, 312, 20627, 294, 2115, 295, 366, 436, 257, 472, 12, 1353, 12, 546, 9721, 11, 366, 436], "temperature": 0.0, "avg_logprob": -0.17366299909703872, "compression_ratio": 2.357487922705314, "no_speech_prob": 4.610540418070741e-05}, {"id": 1057, "seek": 389396, "start": 3898.48, "end": 3902.08, "text": " an end-to-one relation, are they an end-to-end relation?", "tokens": [364, 917, 12, 1353, 12, 546, 9721, 11, 366, 436, 364, 917, 12, 1353, 12, 521, 9721, 30], "temperature": 0.0, "avg_logprob": -0.17366299909703872, "compression_ratio": 2.357487922705314, "no_speech_prob": 4.610540418070741e-05}, {"id": 1058, "seek": 389396, "start": 3902.08, "end": 3907.08, "text": " An example of a one-to-one relation would be your student ID relation, so you have a unique", "tokens": [1107, 1365, 295, 257, 472, 12, 1353, 12, 546, 9721, 576, 312, 428, 3107, 7348, 9721, 11, 370, 291, 362, 257, 3845], "temperature": 0.0, "avg_logprob": -0.17366299909703872, "compression_ratio": 2.357487922705314, "no_speech_prob": 4.610540418070741e-05}, {"id": 1059, "seek": 389396, "start": 3907.08, "end": 3908.08, "text": " student ID.", "tokens": [3107, 7348, 13], "temperature": 0.0, "avg_logprob": -0.17366299909703872, "compression_ratio": 2.357487922705314, "no_speech_prob": 4.610540418070741e-05}, {"id": 1060, "seek": 389396, "start": 3908.08, "end": 3913.56, "text": " An example of an end-to-end relation would be the enrolled in relation, so there's lots", "tokens": [1107, 1365, 295, 364, 917, 12, 1353, 12, 521, 9721, 576, 312, 264, 25896, 294, 9721, 11, 370, 456, 311, 3195], "temperature": 0.0, "avg_logprob": -0.17366299909703872, "compression_ratio": 2.357487922705314, "no_speech_prob": 4.610540418070741e-05}, {"id": 1061, "seek": 389396, "start": 3913.56, "end": 3917.7200000000003, "text": " of students enrolled in lots of classes, so this would be an end-to-end relation.", "tokens": [295, 1731, 25896, 294, 3195, 295, 5359, 11, 370, 341, 576, 312, 364, 917, 12, 1353, 12, 521, 9721, 13], "temperature": 0.0, "avg_logprob": -0.17366299909703872, "compression_ratio": 2.357487922705314, "no_speech_prob": 4.610540418070741e-05}, {"id": 1062, "seek": 389396, "start": 3917.7200000000003, "end": 3921.92, "text": " And they find that birth really struggles on these end-to-end relations.", "tokens": [400, 436, 915, 300, 3965, 534, 17592, 322, 613, 917, 12, 1353, 12, 521, 2299, 13], "temperature": 0.0, "avg_logprob": -0.17366299909703872, "compression_ratio": 2.357487922705314, "no_speech_prob": 4.610540418070741e-05}, {"id": 1063, "seek": 392192, "start": 3921.92, "end": 3926.4, "text": " So while it performs better than relation extraction baseline on some types of relations,", "tokens": [407, 1339, 309, 26213, 1101, 813, 9721, 30197, 20518, 322, 512, 3467, 295, 2299, 11], "temperature": 0.0, "avg_logprob": -0.14615525817871095, "compression_ratio": 1.8041958041958042, "no_speech_prob": 2.078246689052321e-05}, {"id": 1064, "seek": 392192, "start": 3926.4, "end": 3930.08, "text": " overall it does pretty terribly on these end-to-end relations, so overall it does a bit worse", "tokens": [4787, 309, 775, 1238, 22903, 322, 613, 917, 12, 1353, 12, 521, 2299, 11, 370, 4787, 309, 775, 257, 857, 5324], "temperature": 0.0, "avg_logprob": -0.14615525817871095, "compression_ratio": 1.8041958041958042, "no_speech_prob": 2.078246689052321e-05}, {"id": 1065, "seek": 392192, "start": 3930.08, "end": 3933.8, "text": " than the baseline on this TREX dataset.", "tokens": [813, 264, 20518, 322, 341, 314, 3850, 55, 28872, 13], "temperature": 0.0, "avg_logprob": -0.14615525817871095, "compression_ratio": 1.8041958041958042, "no_speech_prob": 2.078246689052321e-05}, {"id": 1066, "seek": 392192, "start": 3933.8, "end": 3939.76, "text": " They also compare to squad on Docker QA, and they find that it does a fair amount worse.", "tokens": [814, 611, 6794, 281, 15310, 322, 33772, 1249, 32, 11, 293, 436, 915, 300, 309, 775, 257, 3143, 2372, 5324, 13], "temperature": 0.0, "avg_logprob": -0.14615525817871095, "compression_ratio": 1.8041958041958042, "no_speech_prob": 2.078246689052321e-05}, {"id": 1067, "seek": 392192, "start": 3939.76, "end": 3943.44, "text": " They note that the language model is not fine tuned here, and also has no access to", "tokens": [814, 3637, 300, 264, 2856, 2316, 307, 406, 2489, 10870, 510, 11, 293, 611, 575, 572, 2105, 281], "temperature": 0.0, "avg_logprob": -0.14615525817871095, "compression_ratio": 1.8041958041958042, "no_speech_prob": 2.078246689052321e-05}, {"id": 1068, "seek": 392192, "start": 3943.44, "end": 3945.76, "text": " an information retrieval system.", "tokens": [364, 1589, 19817, 3337, 1185, 13], "temperature": 0.0, "avg_logprob": -0.14615525817871095, "compression_ratio": 1.8041958041958042, "no_speech_prob": 2.078246689052321e-05}, {"id": 1069, "seek": 392192, "start": 3945.76, "end": 3949.2000000000003, "text": " And then when they look at the precision at 10, they find that this gap between Docker", "tokens": [400, 550, 562, 436, 574, 412, 264, 18356, 412, 1266, 11, 436, 915, 300, 341, 7417, 1296, 33772], "temperature": 0.0, "avg_logprob": -0.14615525817871095, "compression_ratio": 1.8041958041958042, "no_speech_prob": 2.078246689052321e-05}, {"id": 1070, "seek": 394920, "start": 3949.2, "end": 3954.4399999999996, "text": " QA's performance and birth actually closes quite a bit, which suggests that these language", "tokens": [1249, 32, 311, 3389, 293, 3965, 767, 24157, 1596, 257, 857, 11, 597, 13409, 300, 613, 2856], "temperature": 0.0, "avg_logprob": -0.16056156158447266, "compression_ratio": 1.5884773662551441, "no_speech_prob": 5.648491787724197e-05}, {"id": 1071, "seek": 394920, "start": 3954.4399999999996, "end": 3959.7599999999998, "text": " models do have some amount of knowledge encoded in them, and that they're even competitive", "tokens": [5245, 360, 362, 512, 2372, 295, 3601, 2058, 12340, 294, 552, 11, 293, 300, 436, 434, 754, 10043], "temperature": 0.0, "avg_logprob": -0.16056156158447266, "compression_ratio": 1.5884773662551441, "no_speech_prob": 5.648491787724197e-05}, {"id": 1072, "seek": 394920, "start": 3959.7599999999998, "end": 3966.56, "text": " with these knowledge extraction supervised baselines.", "tokens": [365, 613, 3601, 30197, 46533, 987, 9173, 13], "temperature": 0.0, "avg_logprob": -0.16056156158447266, "compression_ratio": 1.5884773662551441, "no_speech_prob": 5.648491787724197e-05}, {"id": 1073, "seek": 394920, "start": 3966.56, "end": 3970.68, "text": " So you can also try out examples on their GitHub repo for the Lama probe.", "tokens": [407, 291, 393, 611, 853, 484, 5110, 322, 641, 23331, 49040, 337, 264, 441, 2404, 22715, 13], "temperature": 0.0, "avg_logprob": -0.16056156158447266, "compression_ratio": 1.5884773662551441, "no_speech_prob": 5.648491787724197e-05}, {"id": 1074, "seek": 394920, "start": 3970.68, "end": 3975.04, "text": " We have an example that was from their repo that was the cat is on the mask.", "tokens": [492, 362, 364, 1365, 300, 390, 490, 641, 49040, 300, 390, 264, 3857, 307, 322, 264, 6094, 13], "temperature": 0.0, "avg_logprob": -0.16056156158447266, "compression_ratio": 1.5884773662551441, "no_speech_prob": 5.648491787724197e-05}, {"id": 1075, "seek": 397504, "start": 3975.04, "end": 3979.56, "text": " You can see what the top 10 predictions are to fill in the closed statement.", "tokens": [509, 393, 536, 437, 264, 1192, 1266, 21264, 366, 281, 2836, 294, 264, 5395, 5629, 13], "temperature": 0.0, "avg_logprob": -0.16393296057436646, "compression_ratio": 1.6962457337883958, "no_speech_prob": 2.2823633116786368e-05}, {"id": 1076, "seek": 397504, "start": 3979.56, "end": 3982.52, "text": " Here they have the cat is on the phone.", "tokens": [1692, 436, 362, 264, 3857, 307, 322, 264, 2593, 13], "temperature": 0.0, "avg_logprob": -0.16393296057436646, "compression_ratio": 1.6962457337883958, "no_speech_prob": 2.2823633116786368e-05}, {"id": 1077, "seek": 397504, "start": 3982.52, "end": 3986.84, "text": " So this can be a fun way to just figure out what factual and common sense knowledge is", "tokens": [407, 341, 393, 312, 257, 1019, 636, 281, 445, 2573, 484, 437, 48029, 293, 2689, 2020, 3601, 307], "temperature": 0.0, "avg_logprob": -0.16393296057436646, "compression_ratio": 1.6962457337883958, "no_speech_prob": 2.2823633116786368e-05}, {"id": 1078, "seek": 397504, "start": 3986.84, "end": 3993.6, "text": " in existing language models, and it's pretty easy to use with this interactive prompt.", "tokens": [294, 6741, 2856, 5245, 11, 293, 309, 311, 1238, 1858, 281, 764, 365, 341, 15141, 12391, 13], "temperature": 0.0, "avg_logprob": -0.16393296057436646, "compression_ratio": 1.6962457337883958, "no_speech_prob": 2.2823633116786368e-05}, {"id": 1079, "seek": 397504, "start": 3993.6, "end": 3997.6, "text": " So some limitations on the Lama probe are that it can be hard to understand why the models", "tokens": [407, 512, 15705, 322, 264, 441, 2404, 22715, 366, 300, 309, 393, 312, 1152, 281, 1223, 983, 264, 5245], "temperature": 0.0, "avg_logprob": -0.16393296057436646, "compression_ratio": 1.6962457337883958, "no_speech_prob": 2.2823633116786368e-05}, {"id": 1080, "seek": 397504, "start": 3997.6, "end": 4000.48, "text": " perform well when they do.", "tokens": [2042, 731, 562, 436, 360, 13], "temperature": 0.0, "avg_logprob": -0.16393296057436646, "compression_ratio": 1.6962457337883958, "no_speech_prob": 2.2823633116786368e-05}, {"id": 1081, "seek": 397504, "start": 4000.48, "end": 4004.24, "text": " So for instance, birth might just be predicting those popular token, and this happens to", "tokens": [407, 337, 5197, 11, 3965, 1062, 445, 312, 32884, 729, 3743, 14862, 11, 293, 341, 2314, 281], "temperature": 0.0, "avg_logprob": -0.16393296057436646, "compression_ratio": 1.6962457337883958, "no_speech_prob": 2.2823633116786368e-05}, {"id": 1082, "seek": 400424, "start": 4004.24, "end": 4005.24, "text": " be right.", "tokens": [312, 558, 13], "temperature": 0.0, "avg_logprob": -0.17939265087397413, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.70488669432234e-05}, {"id": 1083, "seek": 400424, "start": 4005.24, "end": 4010.0, "text": " Maybe it's just memorizing co-occurrence patterns and doesn't really understand the knowledge", "tokens": [2704, 309, 311, 445, 10560, 3319, 598, 12, 905, 14112, 10760, 8294, 293, 1177, 380, 534, 1223, 264, 3601], "temperature": 0.0, "avg_logprob": -0.17939265087397413, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.70488669432234e-05}, {"id": 1084, "seek": 400424, "start": 4010.0, "end": 4014.68, "text": " statement and doesn't understand what the fact is.", "tokens": [5629, 293, 1177, 380, 1223, 437, 264, 1186, 307, 13], "temperature": 0.0, "avg_logprob": -0.17939265087397413, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.70488669432234e-05}, {"id": 1085, "seek": 400424, "start": 4014.68, "end": 4019.52, "text": " It might also just be identifying similarities between surface forms of the subject and object.", "tokens": [467, 1062, 611, 445, 312, 16696, 24197, 1296, 3753, 6422, 295, 264, 3983, 293, 2657, 13], "temperature": 0.0, "avg_logprob": -0.17939265087397413, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.70488669432234e-05}, {"id": 1086, "seek": 400424, "start": 4019.52, "end": 4023.52, "text": " So for instance, an example, Pope Comet VII has a position of blank.", "tokens": [407, 337, 5197, 11, 364, 1365, 11, 19291, 383, 649, 48087, 575, 257, 2535, 295, 8247, 13], "temperature": 0.0, "avg_logprob": -0.17939265087397413, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.70488669432234e-05}, {"id": 1087, "seek": 400424, "start": 4023.52, "end": 4028.0, "text": " Even if you don't know anything about Pope Comet VII, you might be able to figure out", "tokens": [2754, 498, 291, 500, 380, 458, 1340, 466, 19291, 383, 649, 48087, 11, 291, 1062, 312, 1075, 281, 2573, 484], "temperature": 0.0, "avg_logprob": -0.17939265087397413, "compression_ratio": 1.6071428571428572, "no_speech_prob": 3.70488669432234e-05}, {"id": 1088, "seek": 402800, "start": 4028.0, "end": 4035.2, "text": " that Pope is a likely next word for this triple or for this template.", "tokens": [300, 19291, 307, 257, 3700, 958, 1349, 337, 341, 15508, 420, 337, 341, 12379, 13], "temperature": 0.0, "avg_logprob": -0.13321841528656286, "compression_ratio": 1.7106227106227105, "no_speech_prob": 1.0288656085322145e-05}, {"id": 1089, "seek": 402800, "start": 4035.2, "end": 4038.84, "text": " So the problem with this is if the model is just making these predictions based on these", "tokens": [407, 264, 1154, 365, 341, 307, 498, 264, 2316, 307, 445, 1455, 613, 21264, 2361, 322, 613], "temperature": 0.0, "avg_logprob": -0.13321841528656286, "compression_ratio": 1.7106227106227105, "no_speech_prob": 1.0288656085322145e-05}, {"id": 1090, "seek": 402800, "start": 4038.84, "end": 4043.92, "text": " surface forms or co-occurrence patterns, it's difficult to know for actually evaluating", "tokens": [3753, 6422, 420, 598, 12, 905, 14112, 10760, 8294, 11, 309, 311, 2252, 281, 458, 337, 767, 27479], "temperature": 0.0, "avg_logprob": -0.13321841528656286, "compression_ratio": 1.7106227106227105, "no_speech_prob": 1.0288656085322145e-05}, {"id": 1091, "seek": 402800, "start": 4043.92, "end": 4045.24, "text": " the knowledge in the model.", "tokens": [264, 3601, 294, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13321841528656286, "compression_ratio": 1.7106227106227105, "no_speech_prob": 1.0288656085322145e-05}, {"id": 1092, "seek": 402800, "start": 4045.24, "end": 4049.84, "text": " Maybe it's just making correct predictions for other reasons.", "tokens": [2704, 309, 311, 445, 1455, 3006, 21264, 337, 661, 4112, 13], "temperature": 0.0, "avg_logprob": -0.13321841528656286, "compression_ratio": 1.7106227106227105, "no_speech_prob": 1.0288656085322145e-05}, {"id": 1093, "seek": 402800, "start": 4049.84, "end": 4053.04, "text": " And then more subtle issue that we've brought up is that language models might be just", "tokens": [400, 550, 544, 13743, 2734, 300, 321, 600, 3038, 493, 307, 300, 2856, 5245, 1062, 312, 445], "temperature": 0.0, "avg_logprob": -0.13321841528656286, "compression_ratio": 1.7106227106227105, "no_speech_prob": 1.0288656085322145e-05}, {"id": 1094, "seek": 402800, "start": 4053.04, "end": 4055.48, "text": " sensitive to the phrasing of the statement.", "tokens": [9477, 281, 264, 7636, 3349, 295, 264, 5629, 13], "temperature": 0.0, "avg_logprob": -0.13321841528656286, "compression_ratio": 1.7106227106227105, "no_speech_prob": 1.0288656085322145e-05}, {"id": 1095, "seek": 405548, "start": 4055.48, "end": 4060.04, "text": " So for each triple in their data set or for each relation in their data set, they just", "tokens": [407, 337, 1184, 15508, 294, 641, 1412, 992, 420, 337, 1184, 9721, 294, 641, 1412, 992, 11, 436, 445], "temperature": 0.0, "avg_logprob": -0.17624886091365372, "compression_ratio": 1.9017543859649122, "no_speech_prob": 1.0129281690751668e-05}, {"id": 1096, "seek": 405548, "start": 4060.04, "end": 4062.32, "text": " had one manually defined template.", "tokens": [632, 472, 16945, 7642, 12379, 13], "temperature": 0.0, "avg_logprob": -0.17624886091365372, "compression_ratio": 1.9017543859649122, "no_speech_prob": 1.0129281690751668e-05}, {"id": 1097, "seek": 405548, "start": 4062.32, "end": 4065.88, "text": " And qualitatively they found that if they just make small changes as template, it could", "tokens": [400, 31312, 356, 436, 1352, 300, 498, 436, 445, 652, 1359, 2962, 382, 12379, 11, 309, 727], "temperature": 0.0, "avg_logprob": -0.17624886091365372, "compression_ratio": 1.9017543859649122, "no_speech_prob": 1.0129281690751668e-05}, {"id": 1098, "seek": 405548, "start": 4065.88, "end": 4071.48, "text": " actually change whether or not the model could recall the correct prediction or not.", "tokens": [767, 1319, 1968, 420, 406, 264, 2316, 727, 9901, 264, 3006, 17630, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.17624886091365372, "compression_ratio": 1.9017543859649122, "no_speech_prob": 1.0129281690751668e-05}, {"id": 1099, "seek": 405548, "start": 4071.48, "end": 4075.04, "text": " And so this means that the probe results are really a lower bound on the knowledge that's", "tokens": [400, 370, 341, 1355, 300, 264, 447, 650, 3542, 366, 534, 257, 3126, 5472, 322, 264, 3601, 300, 311], "temperature": 0.0, "avg_logprob": -0.17624886091365372, "compression_ratio": 1.9017543859649122, "no_speech_prob": 1.0129281690751668e-05}, {"id": 1100, "seek": 405548, "start": 4075.04, "end": 4078.04, "text": " encoded in the language model.", "tokens": [2058, 12340, 294, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17624886091365372, "compression_ratio": 1.9017543859649122, "no_speech_prob": 1.0129281690751668e-05}, {"id": 1101, "seek": 405548, "start": 4078.04, "end": 4081.52, "text": " So if you change the phrasing, it's possible that the model might show that it actually", "tokens": [407, 498, 291, 1319, 264, 7636, 3349, 11, 309, 311, 1944, 300, 264, 2316, 1062, 855, 300, 309, 767], "temperature": 0.0, "avg_logprob": -0.17624886091365372, "compression_ratio": 1.9017543859649122, "no_speech_prob": 1.0129281690751668e-05}, {"id": 1102, "seek": 405548, "start": 4081.52, "end": 4084.6, "text": " does have the knowledge encoded in it.", "tokens": [775, 362, 264, 3601, 2058, 12340, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.17624886091365372, "compression_ratio": 1.9017543859649122, "no_speech_prob": 1.0129281690751668e-05}, {"id": 1103, "seek": 408460, "start": 4084.6, "end": 4088.88, "text": " So the next lines of work we'll talk about are really building on these two limitations", "tokens": [407, 264, 958, 3876, 295, 589, 321, 603, 751, 466, 366, 534, 2390, 322, 613, 732, 15705], "temperature": 0.0, "avg_logprob": -0.17795577291714942, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.936687219014857e-06}, {"id": 1104, "seek": 408460, "start": 4088.88, "end": 4092.6, "text": " of this original Lama probe.", "tokens": [295, 341, 3380, 441, 2404, 22715, 13], "temperature": 0.0, "avg_logprob": -0.17795577291714942, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.936687219014857e-06}, {"id": 1105, "seek": 408460, "start": 4092.6, "end": 4096.32, "text": " So the first one is called Lama Un or Lama Unhelpful Names.", "tokens": [407, 264, 700, 472, 307, 1219, 441, 2404, 1156, 420, 441, 2404, 1156, 37451, 906, 426, 1632, 13], "temperature": 0.0, "avg_logprob": -0.17795577291714942, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.936687219014857e-06}, {"id": 1106, "seek": 408460, "start": 4096.32, "end": 4100.24, "text": " And the key idea is to remove these examples from Lama that can be answered without the", "tokens": [400, 264, 2141, 1558, 307, 281, 4159, 613, 5110, 490, 441, 2404, 300, 393, 312, 10103, 1553, 264], "temperature": 0.0, "avg_logprob": -0.17795577291714942, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.936687219014857e-06}, {"id": 1107, "seek": 408460, "start": 4100.24, "end": 4101.5599999999995, "text": " relational knowledge.", "tokens": [38444, 3601, 13], "temperature": 0.0, "avg_logprob": -0.17795577291714942, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.936687219014857e-06}, {"id": 1108, "seek": 408460, "start": 4101.5599999999995, "end": 4105.68, "text": " So this is kind of addressing the first limitation on the last slide.", "tokens": [407, 341, 307, 733, 295, 14329, 264, 700, 27432, 322, 264, 1036, 4137, 13], "temperature": 0.0, "avg_logprob": -0.17795577291714942, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.936687219014857e-06}, {"id": 1109, "seek": 408460, "start": 4105.68, "end": 4110.0, "text": " So they observed that BERT relies on just surface forms entities, might not be using knowledge,", "tokens": [407, 436, 13095, 300, 363, 31479, 30910, 322, 445, 3753, 6422, 16667, 11, 1062, 406, 312, 1228, 3601, 11], "temperature": 0.0, "avg_logprob": -0.17795577291714942, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.936687219014857e-06}, {"id": 1110, "seek": 408460, "start": 4110.0, "end": 4111.48, "text": " make these predictions.", "tokens": [652, 613, 21264, 13], "temperature": 0.0, "avg_logprob": -0.17795577291714942, "compression_ratio": 1.676056338028169, "no_speech_prob": 4.936687219014857e-06}, {"id": 1111, "seek": 411148, "start": 4111.48, "end": 4115.599999999999, "text": " This includes a string match situation that we talked about with the Pope.", "tokens": [639, 5974, 257, 6798, 2995, 2590, 300, 321, 2825, 466, 365, 264, 19291, 13], "temperature": 0.0, "avg_logprob": -0.18097050984700522, "compression_ratio": 1.7251655629139073, "no_speech_prob": 5.4692205594619736e-05}, {"id": 1112, "seek": 411148, "start": 4115.599999999999, "end": 4120.879999999999, "text": " This also is dealing with the revealing person name issue that you saw in assignment five.", "tokens": [639, 611, 307, 6260, 365, 264, 23983, 954, 1315, 2734, 300, 291, 1866, 294, 15187, 1732, 13], "temperature": 0.0, "avg_logprob": -0.18097050984700522, "compression_ratio": 1.7251655629139073, "no_speech_prob": 5.4692205594619736e-05}, {"id": 1113, "seek": 411148, "start": 4120.879999999999, "end": 4125.0, "text": " So this is where the name could be an incorrect prior for the native language of someone,", "tokens": [407, 341, 307, 689, 264, 1315, 727, 312, 364, 18424, 4059, 337, 264, 8470, 2856, 295, 1580, 11], "temperature": 0.0, "avg_logprob": -0.18097050984700522, "compression_ratio": 1.7251655629139073, "no_speech_prob": 5.4692205594619736e-05}, {"id": 1114, "seek": 411148, "start": 4125.0, "end": 4127.919999999999, "text": " their place of birth, their nationality.", "tokens": [641, 1081, 295, 3965, 11, 641, 4048, 507, 13], "temperature": 0.0, "avg_logprob": -0.18097050984700522, "compression_ratio": 1.7251655629139073, "no_speech_prob": 5.4692205594619736e-05}, {"id": 1115, "seek": 411148, "start": 4127.919999999999, "end": 4131.959999999999, "text": " They have this example from the table or from the paper, but they look at different people", "tokens": [814, 362, 341, 1365, 490, 264, 3199, 420, 490, 264, 3035, 11, 457, 436, 574, 412, 819, 561], "temperature": 0.0, "avg_logprob": -0.18097050984700522, "compression_ratio": 1.7251655629139073, "no_speech_prob": 5.4692205594619736e-05}, {"id": 1116, "seek": 411148, "start": 4131.959999999999, "end": 4136.48, "text": " names or person's names, and then they look at BERT's prediction for their native language.", "tokens": [5288, 420, 954, 311, 5288, 11, 293, 550, 436, 574, 412, 363, 31479, 311, 17630, 337, 641, 8470, 2856, 13], "temperature": 0.0, "avg_logprob": -0.18097050984700522, "compression_ratio": 1.7251655629139073, "no_speech_prob": 5.4692205594619736e-05}, {"id": 1117, "seek": 411148, "start": 4136.48, "end": 4138.719999999999, "text": " And these are all French speaking actors.", "tokens": [400, 613, 366, 439, 5522, 4124, 10037, 13], "temperature": 0.0, "avg_logprob": -0.18097050984700522, "compression_ratio": 1.7251655629139073, "no_speech_prob": 5.4692205594619736e-05}, {"id": 1118, "seek": 413872, "start": 4138.72, "end": 4144.84, "text": " And BERT just predicts very biased and stereotypical languages for these particular names.", "tokens": [400, 363, 31479, 445, 6069, 82, 588, 28035, 293, 41182, 34061, 8650, 337, 613, 1729, 5288, 13], "temperature": 0.0, "avg_logprob": -0.15939290286930463, "compression_ratio": 1.7516129032258065, "no_speech_prob": 1.6960404536803253e-05}, {"id": 1119, "seek": 413872, "start": 4144.84, "end": 4146.4400000000005, "text": " So this can really work both ways.", "tokens": [407, 341, 393, 534, 589, 1293, 2098, 13], "temperature": 0.0, "avg_logprob": -0.15939290286930463, "compression_ratio": 1.7516129032258065, "no_speech_prob": 1.6960404536803253e-05}, {"id": 1120, "seek": 413872, "start": 4146.4400000000005, "end": 4152.280000000001, "text": " It can lead BERT to make incorrect predictions in some cases, but it could also work to", "tokens": [467, 393, 1477, 363, 31479, 281, 652, 18424, 21264, 294, 512, 3331, 11, 457, 309, 727, 611, 589, 281], "temperature": 0.0, "avg_logprob": -0.15939290286930463, "compression_ratio": 1.7516129032258065, "no_speech_prob": 1.6960404536803253e-05}, {"id": 1121, "seek": 413872, "start": 4152.280000000001, "end": 4156.4400000000005, "text": " let BERT make correct predictions, even if it has no factual knowledge of those people.", "tokens": [718, 363, 31479, 652, 3006, 21264, 11, 754, 498, 309, 575, 572, 48029, 3601, 295, 729, 561, 13], "temperature": 0.0, "avg_logprob": -0.15939290286930463, "compression_ratio": 1.7516129032258065, "no_speech_prob": 1.6960404536803253e-05}, {"id": 1122, "seek": 413872, "start": 4156.4400000000005, "end": 4159.8, "text": " So that's the issue they're trying to get at here is do we know that BERT actually", "tokens": [407, 300, 311, 264, 2734, 436, 434, 1382, 281, 483, 412, 510, 307, 360, 321, 458, 300, 363, 31479, 767], "temperature": 0.0, "avg_logprob": -0.15939290286930463, "compression_ratio": 1.7516129032258065, "no_speech_prob": 1.6960404536803253e-05}, {"id": 1123, "seek": 413872, "start": 4159.8, "end": 4164.64, "text": " knows this fact or is it just using some bias to make its prediction?", "tokens": [3255, 341, 1186, 420, 307, 309, 445, 1228, 512, 12577, 281, 652, 1080, 17630, 30], "temperature": 0.0, "avg_logprob": -0.15939290286930463, "compression_ratio": 1.7516129032258065, "no_speech_prob": 1.6960404536803253e-05}, {"id": 1124, "seek": 413872, "start": 4164.64, "end": 4167.8, "text": " So what they do is they introduce a couple heuristics to basically just filter out these", "tokens": [407, 437, 436, 360, 307, 436, 5366, 257, 1916, 415, 374, 6006, 281, 1936, 445, 6608, 484, 613], "temperature": 0.0, "avg_logprob": -0.15939290286930463, "compression_ratio": 1.7516129032258065, "no_speech_prob": 1.6960404536803253e-05}, {"id": 1125, "seek": 416780, "start": 4167.8, "end": 4172.76, "text": " examples from the LAMA probe that can either be solved by the string match setting or", "tokens": [5110, 490, 264, 441, 2865, 32, 22715, 300, 393, 2139, 312, 13041, 538, 264, 6798, 2995, 3287, 420], "temperature": 0.0, "avg_logprob": -0.23925310770670574, "compression_ratio": 1.7163120567375887, "no_speech_prob": 4.0688282751943916e-05}, {"id": 1126, "seek": 416780, "start": 4172.76, "end": 4175.320000000001, "text": " the serbiling person name setting.", "tokens": [264, 816, 65, 4883, 954, 1315, 3287, 13], "temperature": 0.0, "avg_logprob": -0.23925310770670574, "compression_ratio": 1.7163120567375887, "no_speech_prob": 4.0688282751943916e-05}, {"id": 1127, "seek": 416780, "start": 4175.320000000001, "end": 4179.64, "text": " So they make a harder subset of the LAMA data set essentially.", "tokens": [407, 436, 652, 257, 6081, 25993, 295, 264, 441, 2865, 32, 1412, 992, 4476, 13], "temperature": 0.0, "avg_logprob": -0.23925310770670574, "compression_ratio": 1.7163120567375887, "no_speech_prob": 4.0688282751943916e-05}, {"id": 1128, "seek": 416780, "start": 4179.64, "end": 4183.52, "text": " They find that when they test BERT on this harder subset that its performance drops about", "tokens": [814, 915, 300, 562, 436, 1500, 363, 31479, 322, 341, 6081, 25993, 300, 1080, 3389, 11438, 466], "temperature": 0.0, "avg_logprob": -0.23925310770670574, "compression_ratio": 1.7163120567375887, "no_speech_prob": 4.0688282751943916e-05}, {"id": 1129, "seek": 416780, "start": 4183.52, "end": 4184.52, "text": " 8%.", "tokens": [1649, 6856], "temperature": 0.0, "avg_logprob": -0.23925310770670574, "compression_ratio": 1.7163120567375887, "no_speech_prob": 4.0688282751943916e-05}, {"id": 1130, "seek": 416780, "start": 4184.52, "end": 4188.4400000000005, "text": " But when they test their knowledge enhanced model, which they call Ebert, the score only", "tokens": [583, 562, 436, 1500, 641, 3601, 21191, 2316, 11, 597, 436, 818, 462, 4290, 11, 264, 6175, 787], "temperature": 0.0, "avg_logprob": -0.23925310770670574, "compression_ratio": 1.7163120567375887, "no_speech_prob": 4.0688282751943916e-05}, {"id": 1131, "seek": 416780, "start": 4188.4400000000005, "end": 4189.4400000000005, "text": " drops about 1%.", "tokens": [11438, 466, 502, 6856], "temperature": 0.0, "avg_logprob": -0.23925310770670574, "compression_ratio": 1.7163120567375887, "no_speech_prob": 4.0688282751943916e-05}, {"id": 1132, "seek": 416780, "start": 4189.4400000000005, "end": 4194.84, "text": " So it's possible that as you make harder knowledge probes, we'll actually see even bigger differences", "tokens": [407, 309, 311, 1944, 300, 382, 291, 652, 6081, 3601, 1239, 279, 11, 321, 603, 767, 536, 754, 3801, 7300], "temperature": 0.0, "avg_logprob": -0.23925310770670574, "compression_ratio": 1.7163120567375887, "no_speech_prob": 4.0688282751943916e-05}, {"id": 1133, "seek": 419484, "start": 4194.84, "end": 4202.92, "text": " in the performance of knowledge enhanced models to models without these knowledge enhancements.", "tokens": [294, 264, 3389, 295, 3601, 21191, 5245, 281, 5245, 1553, 613, 3601, 11985, 1117, 13], "temperature": 0.0, "avg_logprob": -0.12602528163364957, "compression_ratio": 1.8404669260700388, "no_speech_prob": 2.4681021386641078e-05}, {"id": 1134, "seek": 419484, "start": 4202.92, "end": 4209.0, "text": " The next piece of work we'll talk about is actually getting at this issue of the phrasing", "tokens": [440, 958, 2522, 295, 589, 321, 603, 751, 466, 307, 767, 1242, 412, 341, 2734, 295, 264, 7636, 3349], "temperature": 0.0, "avg_logprob": -0.12602528163364957, "compression_ratio": 1.8404669260700388, "no_speech_prob": 2.4681021386641078e-05}, {"id": 1135, "seek": 419484, "start": 4209.0, "end": 4214.08, "text": " of the prompt might actually trigger different responses from the language model.", "tokens": [295, 264, 12391, 1062, 767, 7875, 819, 13019, 490, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12602528163364957, "compression_ratio": 1.8404669260700388, "no_speech_prob": 2.4681021386641078e-05}, {"id": 1136, "seek": 419484, "start": 4214.08, "end": 4219.4800000000005, "text": " So the language model might know the fact, but it might fail on the task due to the phrasing.", "tokens": [407, 264, 2856, 2316, 1062, 458, 264, 1186, 11, 457, 309, 1062, 3061, 322, 264, 5633, 3462, 281, 264, 7636, 3349, 13], "temperature": 0.0, "avg_logprob": -0.12602528163364957, "compression_ratio": 1.8404669260700388, "no_speech_prob": 2.4681021386641078e-05}, {"id": 1137, "seek": 419484, "start": 4219.4800000000005, "end": 4223.28, "text": " One reason this might happen is the pre-training is on different contexts and sentence structures", "tokens": [1485, 1778, 341, 1062, 1051, 307, 264, 659, 12, 17227, 1760, 307, 322, 819, 30628, 293, 8174, 9227], "temperature": 0.0, "avg_logprob": -0.12602528163364957, "compression_ratio": 1.8404669260700388, "no_speech_prob": 2.4681021386641078e-05}, {"id": 1138, "seek": 419484, "start": 4223.28, "end": 4224.68, "text": " in the query.", "tokens": [294, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.12602528163364957, "compression_ratio": 1.8404669260700388, "no_speech_prob": 2.4681021386641078e-05}, {"id": 1139, "seek": 422468, "start": 4224.68, "end": 4229.0, "text": " So for example, you might have in your pre-training corpus, the birthplace of Barack Obama is", "tokens": [407, 337, 1365, 11, 291, 1062, 362, 294, 428, 659, 12, 17227, 1760, 1181, 31624, 11, 264, 3965, 6742, 295, 31705, 9560, 307], "temperature": 0.0, "avg_logprob": -0.21575990423455937, "compression_ratio": 1.758308157099698, "no_speech_prob": 2.5461014956817962e-05}, {"id": 1140, "seek": 422468, "start": 4229.0, "end": 4230.400000000001, "text": " Honolulu Hawaii.", "tokens": [6625, 401, 12845, 17930, 13], "temperature": 0.0, "avg_logprob": -0.21575990423455937, "compression_ratio": 1.758308157099698, "no_speech_prob": 2.5461014956817962e-05}, {"id": 1141, "seek": 422468, "start": 4230.400000000001, "end": 4233.200000000001, "text": " And this might be something you've seen with Kapedia for instance, that's a comment training", "tokens": [400, 341, 1062, 312, 746, 291, 600, 1612, 365, 21216, 14212, 337, 5197, 11, 300, 311, 257, 2871, 3097], "temperature": 0.0, "avg_logprob": -0.21575990423455937, "compression_ratio": 1.758308157099698, "no_speech_prob": 2.5461014956817962e-05}, {"id": 1142, "seek": 422468, "start": 4233.200000000001, "end": 4234.200000000001, "text": " data set.", "tokens": [1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.21575990423455937, "compression_ratio": 1.758308157099698, "no_speech_prob": 2.5461014956817962e-05}, {"id": 1143, "seek": 422468, "start": 4234.200000000001, "end": 4238.360000000001, "text": " And then as a researcher, you write Barack Obama is born in blank.", "tokens": [400, 550, 382, 257, 21751, 11, 291, 2464, 31705, 9560, 307, 4232, 294, 8247, 13], "temperature": 0.0, "avg_logprob": -0.21575990423455937, "compression_ratio": 1.758308157099698, "no_speech_prob": 2.5461014956817962e-05}, {"id": 1144, "seek": 422468, "start": 4238.360000000001, "end": 4240.92, "text": " And you can see that these sentence structures are pretty different.", "tokens": [400, 291, 393, 536, 300, 613, 8174, 9227, 366, 1238, 819, 13], "temperature": 0.0, "avg_logprob": -0.21575990423455937, "compression_ratio": 1.758308157099698, "no_speech_prob": 2.5461014956817962e-05}, {"id": 1145, "seek": 422468, "start": 4240.92, "end": 4244.84, "text": " So the model might have seen the first fact, but the sentence structure difference is actually", "tokens": [407, 264, 2316, 1062, 362, 1612, 264, 700, 1186, 11, 457, 264, 8174, 3877, 2649, 307, 767], "temperature": 0.0, "avg_logprob": -0.21575990423455937, "compression_ratio": 1.758308157099698, "no_speech_prob": 2.5461014956817962e-05}, {"id": 1146, "seek": 422468, "start": 4244.84, "end": 4249.52, "text": " enough to confuse it so it can't answer this query.", "tokens": [1547, 281, 28584, 309, 370, 309, 393, 380, 1867, 341, 14581, 13], "temperature": 0.0, "avg_logprob": -0.21575990423455937, "compression_ratio": 1.758308157099698, "no_speech_prob": 2.5461014956817962e-05}, {"id": 1147, "seek": 422468, "start": 4249.52, "end": 4253.08, "text": " So what they do is they generate a lot more of these prompts by mining templates from", "tokens": [407, 437, 436, 360, 307, 436, 8460, 257, 688, 544, 295, 613, 41095, 538, 15512, 21165, 490], "temperature": 0.0, "avg_logprob": -0.21575990423455937, "compression_ratio": 1.758308157099698, "no_speech_prob": 2.5461014956817962e-05}, {"id": 1148, "seek": 425308, "start": 4253.08, "end": 4257.58, "text": " with Kapedia, one of their techniques actually uses dependency parsing, and also generating", "tokens": [365, 21216, 14212, 11, 472, 295, 641, 7512, 767, 4960, 33621, 21156, 278, 11, 293, 611, 17746], "temperature": 0.0, "avg_logprob": -0.1850347878797999, "compression_ratio": 1.795053003533569, "no_speech_prob": 2.6272542527294718e-05}, {"id": 1149, "seek": 425308, "start": 4257.58, "end": 4263.48, "text": " paraphrased prompts by taking inspiration from the machine translation literature and using", "tokens": [36992, 1703, 1937, 41095, 538, 1940, 10249, 490, 264, 3479, 12853, 10394, 293, 1228], "temperature": 0.0, "avg_logprob": -0.1850347878797999, "compression_ratio": 1.795053003533569, "no_speech_prob": 2.6272542527294718e-05}, {"id": 1150, "seek": 425308, "start": 4263.48, "end": 4265.2, "text": " back translation.", "tokens": [646, 12853, 13], "temperature": 0.0, "avg_logprob": -0.1850347878797999, "compression_ratio": 1.795053003533569, "no_speech_prob": 2.6272542527294718e-05}, {"id": 1151, "seek": 425308, "start": 4265.2, "end": 4268.96, "text": " So they generate a lot more prompts to try to query the language models and figure out", "tokens": [407, 436, 8460, 257, 688, 544, 41095, 281, 853, 281, 14581, 264, 2856, 5245, 293, 2573, 484], "temperature": 0.0, "avg_logprob": -0.1850347878797999, "compression_ratio": 1.795053003533569, "no_speech_prob": 2.6272542527294718e-05}, {"id": 1152, "seek": 425308, "start": 4268.96, "end": 4273.84, "text": " do small variations in the prompt, trigger the correct prediction from the language model.", "tokens": [360, 1359, 17840, 294, 264, 12391, 11, 7875, 264, 3006, 17630, 490, 264, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1850347878797999, "compression_ratio": 1.795053003533569, "no_speech_prob": 2.6272542527294718e-05}, {"id": 1153, "seek": 425308, "start": 4273.84, "end": 4276.84, "text": " They also experiment the sampling prompts.", "tokens": [814, 611, 5120, 264, 21179, 41095, 13], "temperature": 0.0, "avg_logprob": -0.1850347878797999, "compression_ratio": 1.795053003533569, "no_speech_prob": 2.6272542527294718e-05}, {"id": 1154, "seek": 425308, "start": 4276.84, "end": 4281.08, "text": " So if we give the model multiple prompts and then take some probability averaged over", "tokens": [407, 498, 321, 976, 264, 2316, 3866, 41095, 293, 550, 747, 512, 8482, 18247, 2980, 670], "temperature": 0.0, "avg_logprob": -0.1850347878797999, "compression_ratio": 1.795053003533569, "no_speech_prob": 2.6272542527294718e-05}, {"id": 1155, "seek": 428108, "start": 4281.08, "end": 4285.96, "text": " these different prompts, can we improve the performance on the model returning the correct", "tokens": [613, 819, 41095, 11, 393, 321, 3470, 264, 3389, 322, 264, 2316, 12678, 264, 3006], "temperature": 0.0, "avg_logprob": -0.17866208818223742, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.813213985878974e-05}, {"id": 1156, "seek": 428108, "start": 4285.96, "end": 4286.96, "text": " prediction?", "tokens": [17630, 30], "temperature": 0.0, "avg_logprob": -0.17866208818223742, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.813213985878974e-05}, {"id": 1157, "seek": 428108, "start": 4286.96, "end": 4290.04, "text": " So we give it a higher chance of seeing a context that it might have actually seen during", "tokens": [407, 321, 976, 309, 257, 2946, 2931, 295, 2577, 257, 4319, 300, 309, 1062, 362, 767, 1612, 1830], "temperature": 0.0, "avg_logprob": -0.17866208818223742, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.813213985878974e-05}, {"id": 1158, "seek": 428108, "start": 4290.04, "end": 4292.72, "text": " pre-training.", "tokens": [659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.17866208818223742, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.813213985878974e-05}, {"id": 1159, "seek": 428108, "start": 4292.72, "end": 4297.6, "text": " They find that the performance on Lama increases when they either use a top performing prompt", "tokens": [814, 915, 300, 264, 3389, 322, 441, 2404, 8637, 562, 436, 2139, 764, 257, 1192, 10205, 12391], "temperature": 0.0, "avg_logprob": -0.17866208818223742, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.813213985878974e-05}, {"id": 1160, "seek": 428108, "start": 4297.6, "end": 4299.92, "text": " or when they use this sampling approach.", "tokens": [420, 562, 436, 764, 341, 21179, 3109, 13], "temperature": 0.0, "avg_logprob": -0.17866208818223742, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.813213985878974e-05}, {"id": 1161, "seek": 428108, "start": 4299.92, "end": 4303.44, "text": " So it suggests that the original Lama really was a lower bound on the amount of knowledge", "tokens": [407, 309, 13409, 300, 264, 3380, 441, 2404, 534, 390, 257, 3126, 5472, 322, 264, 2372, 295, 3601], "temperature": 0.0, "avg_logprob": -0.17866208818223742, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.813213985878974e-05}, {"id": 1162, "seek": 428108, "start": 4303.44, "end": 4305.88, "text": " encoded in these language models.", "tokens": [2058, 12340, 294, 613, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17866208818223742, "compression_ratio": 1.7547169811320755, "no_speech_prob": 6.813213985878974e-05}, {"id": 1163, "seek": 430588, "start": 4305.88, "end": 4312.96, "text": " And changing the phrasing can actually help the model recall the correct answer.", "tokens": [400, 4473, 264, 7636, 3349, 393, 767, 854, 264, 2316, 9901, 264, 3006, 1867, 13], "temperature": 0.0, "avg_logprob": -0.13334272542131057, "compression_ratio": 1.8196078431372549, "no_speech_prob": 3.943906631320715e-05}, {"id": 1164, "seek": 430588, "start": 4312.96, "end": 4316.4400000000005, "text": " This table is a bit frightening, but they find that small changes in the query can lead", "tokens": [639, 3199, 307, 257, 857, 31043, 11, 457, 436, 915, 300, 1359, 2962, 294, 264, 14581, 393, 1477], "temperature": 0.0, "avg_logprob": -0.13334272542131057, "compression_ratio": 1.8196078431372549, "no_speech_prob": 3.943906631320715e-05}, {"id": 1165, "seek": 430588, "start": 4316.4400000000005, "end": 4318.88, "text": " to really large gains on performance.", "tokens": [281, 534, 2416, 16823, 322, 3389, 13], "temperature": 0.0, "avg_logprob": -0.13334272542131057, "compression_ratio": 1.8196078431372549, "no_speech_prob": 3.943906631320715e-05}, {"id": 1166, "seek": 430588, "start": 4318.88, "end": 4323.96, "text": " So if you just have a query like x plays in y position, and then you change that to x plays", "tokens": [407, 498, 291, 445, 362, 257, 14581, 411, 2031, 5749, 294, 288, 2535, 11, 293, 550, 291, 1319, 300, 281, 2031, 5749], "temperature": 0.0, "avg_logprob": -0.13334272542131057, "compression_ratio": 1.8196078431372549, "no_speech_prob": 3.943906631320715e-05}, {"id": 1167, "seek": 430588, "start": 4323.96, "end": 4328.32, "text": " at y position, this can actually lead to like a 23% accuracy gain on this particular", "tokens": [412, 288, 2535, 11, 341, 393, 767, 1477, 281, 411, 257, 6673, 4, 14170, 6052, 322, 341, 1729], "temperature": 0.0, "avg_logprob": -0.13334272542131057, "compression_ratio": 1.8196078431372549, "no_speech_prob": 3.943906631320715e-05}, {"id": 1168, "seek": 430588, "start": 4328.32, "end": 4333.56, "text": " relation in terms of the model actually being able to recall the correct answer.", "tokens": [9721, 294, 2115, 295, 264, 2316, 767, 885, 1075, 281, 9901, 264, 3006, 1867, 13], "temperature": 0.0, "avg_logprob": -0.13334272542131057, "compression_ratio": 1.8196078431372549, "no_speech_prob": 3.943906631320715e-05}, {"id": 1169, "seek": 433356, "start": 4333.56, "end": 4339.72, "text": " Or even just x was created in y to x is created in y 10% accuracy gain.", "tokens": [1610, 754, 445, 2031, 390, 2942, 294, 288, 281, 2031, 307, 2942, 294, 288, 1266, 4, 14170, 6052, 13], "temperature": 0.0, "avg_logprob": -0.12789237742521326, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.618675196368713e-05}, {"id": 1170, "seek": 433356, "start": 4339.72, "end": 4344.200000000001, "text": " So I think this motivates the need to not only develop better ways to query these models,", "tokens": [407, 286, 519, 341, 42569, 264, 643, 281, 406, 787, 1499, 1101, 2098, 281, 14581, 613, 5245, 11], "temperature": 0.0, "avg_logprob": -0.12789237742521326, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.618675196368713e-05}, {"id": 1171, "seek": 433356, "start": 4344.200000000001, "end": 4351.68, "text": " but probably also build language models that are actually more robust to the query itself.", "tokens": [457, 1391, 611, 1322, 2856, 5245, 300, 366, 767, 544, 13956, 281, 264, 14581, 2564, 13], "temperature": 0.0, "avg_logprob": -0.12789237742521326, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.618675196368713e-05}, {"id": 1172, "seek": 433356, "start": 4351.68, "end": 4355.56, "text": " So in addition to probes, another way to evaluate these language models is by looking", "tokens": [407, 294, 4500, 281, 1239, 279, 11, 1071, 636, 281, 13059, 613, 2856, 5245, 307, 538, 1237], "temperature": 0.0, "avg_logprob": -0.12789237742521326, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.618675196368713e-05}, {"id": 1173, "seek": 433356, "start": 4355.56, "end": 4362.400000000001, "text": " at how well they transfer from the pre-trained representation to downstream tasks.", "tokens": [412, 577, 731, 436, 5003, 490, 264, 659, 12, 17227, 2001, 10290, 281, 30621, 9608, 13], "temperature": 0.0, "avg_logprob": -0.12789237742521326, "compression_ratio": 1.6772908366533865, "no_speech_prob": 1.618675196368713e-05}, {"id": 1174, "seek": 436240, "start": 4362.4, "end": 4365.5599999999995, "text": " And so the idea here is you're actually going to find two in the pre-trained representation", "tokens": [400, 370, 264, 1558, 510, 307, 291, 434, 767, 516, 281, 915, 732, 294, 264, 659, 12, 17227, 2001, 10290], "temperature": 0.0, "avg_logprob": -0.1798341093944902, "compression_ratio": 1.7697594501718212, "no_speech_prob": 3.6119038213655585e-06}, {"id": 1175, "seek": 436240, "start": 4365.5599999999995, "end": 4371.679999999999, "text": " on different downstream tasks, similar to how you evaluate Bert on glue tasks.", "tokens": [322, 819, 30621, 9608, 11, 2531, 281, 577, 291, 13059, 29594, 322, 8998, 9608, 13], "temperature": 0.0, "avg_logprob": -0.1798341093944902, "compression_ratio": 1.7697594501718212, "no_speech_prob": 3.6119038213655585e-06}, {"id": 1176, "seek": 436240, "start": 4371.679999999999, "end": 4376.92, "text": " So common tasks that are used for this are relation extraction, entity typing, and question", "tokens": [407, 2689, 9608, 300, 366, 1143, 337, 341, 366, 9721, 30197, 11, 13977, 18444, 11, 293, 1168], "temperature": 0.0, "avg_logprob": -0.1798341093944902, "compression_ratio": 1.7697594501718212, "no_speech_prob": 3.6119038213655585e-06}, {"id": 1177, "seek": 436240, "start": 4376.92, "end": 4377.92, "text": " answering.", "tokens": [13430, 13], "temperature": 0.0, "avg_logprob": -0.1798341093944902, "compression_ratio": 1.7697594501718212, "no_speech_prob": 3.6119038213655585e-06}, {"id": 1178, "seek": 436240, "start": 4377.92, "end": 4381.759999999999, "text": " Relation extraction is where you want to predict the relation between two entities.", "tokens": [8738, 399, 30197, 307, 689, 291, 528, 281, 6069, 264, 9721, 1296, 732, 16667, 13], "temperature": 0.0, "avg_logprob": -0.1798341093944902, "compression_ratio": 1.7697594501718212, "no_speech_prob": 3.6119038213655585e-06}, {"id": 1179, "seek": 436240, "start": 4381.759999999999, "end": 4385.28, "text": " So this is getting back at one of the questions earlier in this talk in terms of well, how", "tokens": [407, 341, 307, 1242, 646, 412, 472, 295, 264, 1651, 3071, 294, 341, 751, 294, 2115, 295, 731, 11, 577], "temperature": 0.0, "avg_logprob": -0.1798341093944902, "compression_ratio": 1.7697594501718212, "no_speech_prob": 3.6119038213655585e-06}, {"id": 1180, "seek": 436240, "start": 4385.28, "end": 4388.32, "text": " do you get the relation that's the edges in these knowledge bases?", "tokens": [360, 291, 483, 264, 9721, 300, 311, 264, 8819, 294, 613, 3601, 17949, 30], "temperature": 0.0, "avg_logprob": -0.1798341093944902, "compression_ratio": 1.7697594501718212, "no_speech_prob": 3.6119038213655585e-06}, {"id": 1181, "seek": 438832, "start": 4388.32, "end": 4393.4, "text": " So given two entities, you learn a model to predict what is a relation between them.", "tokens": [407, 2212, 732, 16667, 11, 291, 1466, 257, 2316, 281, 6069, 437, 307, 257, 9721, 1296, 552, 13], "temperature": 0.0, "avg_logprob": -0.18481493804414392, "compression_ratio": 1.7446808510638299, "no_speech_prob": 4.494992481340887e-06}, {"id": 1182, "seek": 438832, "start": 4393.4, "end": 4396.759999999999, "text": " Entity typing is a task of giving an entity what is the type of the entity.", "tokens": [3951, 507, 18444, 307, 257, 5633, 295, 2902, 364, 13977, 437, 307, 264, 2010, 295, 264, 13977, 13], "temperature": 0.0, "avg_logprob": -0.18481493804414392, "compression_ratio": 1.7446808510638299, "no_speech_prob": 4.494992481340887e-06}, {"id": 1183, "seek": 438832, "start": 4396.759999999999, "end": 4400.08, "text": " So here, Alice Rob the bank, you want to predict her as a criminal.", "tokens": [407, 510, 11, 16004, 5424, 264, 3765, 11, 291, 528, 281, 6069, 720, 382, 257, 8628, 13], "temperature": 0.0, "avg_logprob": -0.18481493804414392, "compression_ratio": 1.7446808510638299, "no_speech_prob": 4.494992481340887e-06}, {"id": 1184, "seek": 438832, "start": 4400.08, "end": 4403.5599999999995, "text": " And then you guys are very familiar with question answering.", "tokens": [400, 550, 291, 1074, 366, 588, 4963, 365, 1168, 13430, 13], "temperature": 0.0, "avg_logprob": -0.18481493804414392, "compression_ratio": 1.7446808510638299, "no_speech_prob": 4.494992481340887e-06}, {"id": 1185, "seek": 438832, "start": 4403.5599999999995, "end": 4408.4, "text": " So the idea of these tasks is that they're knowledge intensive, so they're good candidates", "tokens": [407, 264, 1558, 295, 613, 9608, 307, 300, 436, 434, 3601, 18957, 11, 370, 436, 434, 665, 11255], "temperature": 0.0, "avg_logprob": -0.18481493804414392, "compression_ratio": 1.7446808510638299, "no_speech_prob": 4.494992481340887e-06}, {"id": 1186, "seek": 438832, "start": 4408.4, "end": 4412.92, "text": " to see how well do these pre-trained representations actually transfer the knowledge to these downstream", "tokens": [281, 536, 577, 731, 360, 613, 659, 12, 17227, 2001, 33358, 767, 5003, 264, 3601, 281, 613, 30621], "temperature": 0.0, "avg_logprob": -0.18481493804414392, "compression_ratio": 1.7446808510638299, "no_speech_prob": 4.494992481340887e-06}, {"id": 1187, "seek": 438832, "start": 4412.92, "end": 4416.5599999999995, "text": " tasks.", "tokens": [9608, 13], "temperature": 0.0, "avg_logprob": -0.18481493804414392, "compression_ratio": 1.7446808510638299, "no_speech_prob": 4.494992481340887e-06}, {"id": 1188, "seek": 441656, "start": 4416.56, "end": 4421.0, "text": " So we look at the performance on a relation extraction benchmark called tackred and all", "tokens": [407, 321, 574, 412, 264, 3389, 322, 257, 9721, 30197, 18927, 1219, 9426, 986, 293, 439], "temperature": 0.0, "avg_logprob": -0.259368000608502, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.00011772396101150662}, {"id": 1189, "seek": 441656, "start": 4421.0, "end": 4425.360000000001, "text": " the models that we show here for one point stay the art on tackred.", "tokens": [264, 5245, 300, 321, 855, 510, 337, 472, 935, 1754, 264, 1523, 322, 9426, 986, 13], "temperature": 0.0, "avg_logprob": -0.259368000608502, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.00011772396101150662}, {"id": 1190, "seek": 441656, "start": 4425.360000000001, "end": 4430.72, "text": " So this CGCN is a graph convolutional neural network over dependency trees.", "tokens": [407, 341, 383, 38, 34, 45, 307, 257, 4295, 45216, 304, 18161, 3209, 670, 33621, 5852, 13], "temperature": 0.0, "avg_logprob": -0.259368000608502, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.00011772396101150662}, {"id": 1191, "seek": 441656, "start": 4430.72, "end": 4435.080000000001, "text": " The Bert LSTM base is a, it's one of the first works that showed that you could actually", "tokens": [440, 29594, 441, 6840, 44, 3096, 307, 257, 11, 309, 311, 472, 295, 264, 700, 1985, 300, 4712, 300, 291, 727, 767], "temperature": 0.0, "avg_logprob": -0.259368000608502, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.00011772396101150662}, {"id": 1192, "seek": 441656, "start": 4435.080000000001, "end": 4438.280000000001, "text": " get state of the art performance with Bert on relation extraction.", "tokens": [483, 1785, 295, 264, 1523, 3389, 365, 29594, 322, 9721, 30197, 13], "temperature": 0.0, "avg_logprob": -0.259368000608502, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.00011772396101150662}, {"id": 1193, "seek": 441656, "start": 4438.280000000001, "end": 4442.04, "text": " And this is just putting LSTM layer over Bert's output.", "tokens": [400, 341, 307, 445, 3372, 441, 6840, 44, 4583, 670, 29594, 311, 5598, 13], "temperature": 0.0, "avg_logprob": -0.259368000608502, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.00011772396101150662}, {"id": 1194, "seek": 441656, "start": 4442.04, "end": 4445.6, "text": " Ernie is the work that we talked about with the pre-trained entity embeddings.", "tokens": [3300, 2766, 307, 264, 589, 300, 321, 2825, 466, 365, 264, 659, 12, 17227, 2001, 13977, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.259368000608502, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.00011772396101150662}, {"id": 1195, "seek": 444560, "start": 4445.6, "end": 4449.160000000001, "text": " Actually the blanks we didn't get to today, but it's a really interesting work about learning", "tokens": [5135, 264, 8247, 82, 321, 994, 380, 483, 281, 965, 11, 457, 309, 311, 257, 534, 1880, 589, 466, 2539], "temperature": 0.0, "avg_logprob": -0.19294034020375397, "compression_ratio": 1.7441077441077442, "no_speech_prob": 9.026636689668521e-05}, {"id": 1196, "seek": 444560, "start": 4449.160000000001, "end": 4451.52, "text": " meaningful relation representations.", "tokens": [10995, 9721, 33358, 13], "temperature": 0.0, "avg_logprob": -0.19294034020375397, "compression_ratio": 1.7441077441077442, "no_speech_prob": 9.026636689668521e-05}, {"id": 1197, "seek": 444560, "start": 4451.52, "end": 4456.400000000001, "text": " And it falls more into the training data modification approaches and that they are actually", "tokens": [400, 309, 8804, 544, 666, 264, 3097, 1412, 26747, 11587, 293, 300, 436, 366, 767], "temperature": 0.0, "avg_logprob": -0.19294034020375397, "compression_ratio": 1.7441077441077442, "no_speech_prob": 9.026636689668521e-05}, {"id": 1198, "seek": 444560, "start": 4456.400000000001, "end": 4459.160000000001, "text": " masking out entities again.", "tokens": [31226, 484, 16667, 797, 13], "temperature": 0.0, "avg_logprob": -0.19294034020375397, "compression_ratio": 1.7441077441077442, "no_speech_prob": 9.026636689668521e-05}, {"id": 1199, "seek": 444560, "start": 4459.160000000001, "end": 4462.200000000001, "text": " And then no Bert is what we talked about.", "tokens": [400, 550, 572, 29594, 307, 437, 321, 2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.19294034020375397, "compression_ratio": 1.7441077441077442, "no_speech_prob": 9.026636689668521e-05}, {"id": 1200, "seek": 444560, "start": 4462.200000000001, "end": 4466.120000000001, "text": " The W and W here means the action code two knowledge bases in no Bert.", "tokens": [440, 343, 293, 343, 510, 1355, 264, 3069, 3089, 732, 3601, 17949, 294, 572, 29594, 13], "temperature": 0.0, "avg_logprob": -0.19294034020375397, "compression_ratio": 1.7441077441077442, "no_speech_prob": 9.026636689668521e-05}, {"id": 1201, "seek": 444560, "start": 4466.120000000001, "end": 4470.360000000001, "text": " So they're encoding wordnet and they're also encoding Wikipedia.", "tokens": [407, 436, 434, 43430, 1349, 7129, 293, 436, 434, 611, 43430, 28999, 13], "temperature": 0.0, "avg_logprob": -0.19294034020375397, "compression_ratio": 1.7441077441077442, "no_speech_prob": 9.026636689668521e-05}, {"id": 1202, "seek": 444560, "start": 4470.360000000001, "end": 4474.0, "text": " And the high level takeaway from this table is that you can see that the recent knowledge", "tokens": [400, 264, 1090, 1496, 30681, 490, 341, 3199, 307, 300, 291, 393, 536, 300, 264, 5162, 3601], "temperature": 0.0, "avg_logprob": -0.19294034020375397, "compression_ratio": 1.7441077441077442, "no_speech_prob": 9.026636689668521e-05}, {"id": 1203, "seek": 447400, "start": 4474.0, "end": 4479.12, "text": " enhanced models have achieved state of the art over the original models that once performed", "tokens": [21191, 5245, 362, 11042, 1785, 295, 264, 1523, 670, 264, 3380, 5245, 300, 1564, 10332], "temperature": 0.0, "avg_logprob": -0.1905202119246773, "compression_ratio": 1.724907063197026, "no_speech_prob": 6.502069300040603e-05}, {"id": 1204, "seek": 447400, "start": 4479.12, "end": 4480.12, "text": " very well on tackred.", "tokens": [588, 731, 322, 9426, 986, 13], "temperature": 0.0, "avg_logprob": -0.1905202119246773, "compression_ratio": 1.724907063197026, "no_speech_prob": 6.502069300040603e-05}, {"id": 1205, "seek": 447400, "start": 4480.12, "end": 4484.0, "text": " And we have about five F1 gains here.", "tokens": [400, 321, 362, 466, 1732, 479, 16, 16823, 510, 13], "temperature": 0.0, "avg_logprob": -0.1905202119246773, "compression_ratio": 1.724907063197026, "no_speech_prob": 6.502069300040603e-05}, {"id": 1206, "seek": 447400, "start": 4484.0, "end": 4487.4, "text": " Another interesting takeaway from this table is there seems to be a trade-off in the size", "tokens": [3996, 1880, 30681, 490, 341, 3199, 307, 456, 2544, 281, 312, 257, 4923, 12, 4506, 294, 264, 2744], "temperature": 0.0, "avg_logprob": -0.1905202119246773, "compression_ratio": 1.724907063197026, "no_speech_prob": 6.502069300040603e-05}, {"id": 1207, "seek": 447400, "start": 4487.4, "end": 4490.96, "text": " of a language model that's necessary to get a certain performance.", "tokens": [295, 257, 2856, 2316, 300, 311, 4818, 281, 483, 257, 1629, 3389, 13], "temperature": 0.0, "avg_logprob": -0.1905202119246773, "compression_ratio": 1.724907063197026, "no_speech_prob": 6.502069300040603e-05}, {"id": 1208, "seek": 447400, "start": 4490.96, "end": 4495.24, "text": " So if you just consider the size of a language model, then no Bert forms the best.", "tokens": [407, 498, 291, 445, 1949, 264, 2744, 295, 257, 2856, 2316, 11, 550, 572, 29594, 6422, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.1905202119246773, "compression_ratio": 1.724907063197026, "no_speech_prob": 6.502069300040603e-05}, {"id": 1209, "seek": 447400, "start": 4495.24, "end": 4500.88, "text": " But if you don't consider that, then it's high with matching the blanks.", "tokens": [583, 498, 291, 500, 380, 1949, 300, 11, 550, 309, 311, 1090, 365, 14324, 264, 8247, 82, 13], "temperature": 0.0, "avg_logprob": -0.1905202119246773, "compression_ratio": 1.724907063197026, "no_speech_prob": 6.502069300040603e-05}, {"id": 1210, "seek": 450088, "start": 4500.88, "end": 4505.12, "text": " So overall, this is pretty good evidence that these knowledge enhanced methods are in", "tokens": [407, 4787, 11, 341, 307, 1238, 665, 4467, 300, 613, 3601, 21191, 7150, 366, 294], "temperature": 0.0, "avg_logprob": -0.17045459747314454, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.468021739332471e-05}, {"id": 1211, "seek": 450088, "start": 4505.12, "end": 4510.0, "text": " fact transferring to these knowledge intensive downstream tasks that can really take advantage", "tokens": [1186, 31437, 281, 613, 3601, 18957, 30621, 9608, 300, 393, 534, 747, 5002], "temperature": 0.0, "avg_logprob": -0.17045459747314454, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.468021739332471e-05}, {"id": 1212, "seek": 450088, "start": 4510.0, "end": 4514.2, "text": " of these pre-trained representations.", "tokens": [295, 613, 659, 12, 17227, 2001, 33358, 13], "temperature": 0.0, "avg_logprob": -0.17045459747314454, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.468021739332471e-05}, {"id": 1213, "seek": 450088, "start": 4514.2, "end": 4516.16, "text": " We also have results on entity typing.", "tokens": [492, 611, 362, 3542, 322, 13977, 18444, 13], "temperature": 0.0, "avg_logprob": -0.17045459747314454, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.468021739332471e-05}, {"id": 1214, "seek": 450088, "start": 4516.16, "end": 4518.88, "text": " So here we're comparing to slightly different set of models.", "tokens": [407, 510, 321, 434, 15763, 281, 4748, 819, 992, 295, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17045459747314454, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.468021739332471e-05}, {"id": 1215, "seek": 450088, "start": 4518.88, "end": 4523.16, "text": " Some of the base signs are LSTM models that were designed for entity typing.", "tokens": [2188, 295, 264, 3096, 7880, 366, 441, 6840, 44, 5245, 300, 645, 4761, 337, 13977, 18444, 13], "temperature": 0.0, "avg_logprob": -0.17045459747314454, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.468021739332471e-05}, {"id": 1216, "seek": 450088, "start": 4523.16, "end": 4529.04, "text": " And we have Ernie and Nobert leading the, I guess, leaderboard here on the entity typing", "tokens": [400, 321, 362, 3300, 2766, 293, 883, 4290, 5775, 264, 11, 286, 2041, 11, 5263, 3787, 510, 322, 264, 13977, 18444], "temperature": 0.0, "avg_logprob": -0.17045459747314454, "compression_ratio": 1.7163120567375887, "no_speech_prob": 2.468021739332471e-05}, {"id": 1217, "seek": 452904, "start": 4529.04, "end": 4530.8, "text": " task of open entity.", "tokens": [5633, 295, 1269, 13977, 13], "temperature": 0.0, "avg_logprob": -0.1495175442453158, "compression_ratio": 1.6732673267326732, "no_speech_prob": 4.907537731924094e-05}, {"id": 1218, "seek": 452904, "start": 4530.8, "end": 4534.64, "text": " And we see gains of about 15 F1 points with Ernie and Nobert.", "tokens": [400, 321, 536, 16823, 295, 466, 2119, 479, 16, 2793, 365, 3300, 2766, 293, 883, 4290, 13], "temperature": 0.0, "avg_logprob": -0.1495175442453158, "compression_ratio": 1.6732673267326732, "no_speech_prob": 4.907537731924094e-05}, {"id": 1219, "seek": 452904, "start": 4534.64, "end": 4539.12, "text": " So once again, we really do see that these knowledge-rich pre-trained representations", "tokens": [407, 1564, 797, 11, 321, 534, 360, 536, 300, 613, 3601, 12, 10794, 659, 12, 17227, 2001, 33358], "temperature": 0.0, "avg_logprob": -0.1495175442453158, "compression_ratio": 1.6732673267326732, "no_speech_prob": 4.907537731924094e-05}, {"id": 1220, "seek": 452904, "start": 4539.12, "end": 4545.96, "text": " are transferring and helping on these knowledge intensive downstream tasks.", "tokens": [366, 31437, 293, 4315, 322, 613, 3601, 18957, 30621, 9608, 13], "temperature": 0.0, "avg_logprob": -0.1495175442453158, "compression_ratio": 1.6732673267326732, "no_speech_prob": 4.907537731924094e-05}, {"id": 1221, "seek": 452904, "start": 4545.96, "end": 4550.68, "text": " So just to recap, we talked about probes which evaluate the knowledge already present in models.", "tokens": [407, 445, 281, 20928, 11, 321, 2825, 466, 1239, 279, 597, 13059, 264, 3601, 1217, 1974, 294, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1495175442453158, "compression_ratio": 1.6732673267326732, "no_speech_prob": 4.907537731924094e-05}, {"id": 1222, "seek": 452904, "start": 4550.68, "end": 4552.88, "text": " These don't require any more training.", "tokens": [1981, 500, 380, 3651, 604, 544, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1495175442453158, "compression_ratio": 1.6732673267326732, "no_speech_prob": 4.907537731924094e-05}, {"id": 1223, "seek": 452904, "start": 4552.88, "end": 4557.36, "text": " But it can be challenging to construct benchmarks to actually make sure you're testing the knowledge", "tokens": [583, 309, 393, 312, 7595, 281, 7690, 43751, 281, 767, 652, 988, 291, 434, 4997, 264, 3601], "temperature": 0.0, "avg_logprob": -0.1495175442453158, "compression_ratio": 1.6732673267326732, "no_speech_prob": 4.907537731924094e-05}, {"id": 1224, "seek": 452904, "start": 4557.36, "end": 4558.36, "text": " in these language models.", "tokens": [294, 613, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1495175442453158, "compression_ratio": 1.6732673267326732, "no_speech_prob": 4.907537731924094e-05}, {"id": 1225, "seek": 455836, "start": 4558.36, "end": 4563.12, "text": " It can also be challenging to construct the queries used in the probe.", "tokens": [467, 393, 611, 312, 7595, 281, 7690, 264, 24109, 1143, 294, 264, 22715, 13], "temperature": 0.0, "avg_logprob": -0.1273611443383353, "compression_ratio": 1.6405693950177935, "no_speech_prob": 4.132923641009256e-05}, {"id": 1226, "seek": 455836, "start": 4563.12, "end": 4565.36, "text": " We then talked about downstream tasks.", "tokens": [492, 550, 2825, 466, 30621, 9608, 13], "temperature": 0.0, "avg_logprob": -0.1273611443383353, "compression_ratio": 1.6405693950177935, "no_speech_prob": 4.132923641009256e-05}, {"id": 1227, "seek": 455836, "start": 4565.36, "end": 4568.5599999999995, "text": " These are a bit of an indirect way to evaluate knowledge in that they have this extra component", "tokens": [1981, 366, 257, 857, 295, 364, 19523, 636, 281, 13059, 3601, 294, 300, 436, 362, 341, 2857, 6542], "temperature": 0.0, "avg_logprob": -0.1273611443383353, "compression_ratio": 1.6405693950177935, "no_speech_prob": 4.132923641009256e-05}, {"id": 1228, "seek": 455836, "start": 4568.5599999999995, "end": 4569.5599999999995, "text": " of fine tuning.", "tokens": [295, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.1273611443383353, "compression_ratio": 1.6405693950177935, "no_speech_prob": 4.132923641009256e-05}, {"id": 1229, "seek": 455836, "start": 4569.5599999999995, "end": 4574.48, "text": " But it's a good way to evaluate how useful is this knowledge-rich pre-trained representation", "tokens": [583, 309, 311, 257, 665, 636, 281, 13059, 577, 4420, 307, 341, 3601, 12, 10794, 659, 12, 17227, 2001, 10290], "temperature": 0.0, "avg_logprob": -0.1273611443383353, "compression_ratio": 1.6405693950177935, "no_speech_prob": 4.132923641009256e-05}, {"id": 1230, "seek": 455836, "start": 4574.48, "end": 4579.0, "text": " in actual applications.", "tokens": [294, 3539, 5821, 13], "temperature": 0.0, "avg_logprob": -0.1273611443383353, "compression_ratio": 1.6405693950177935, "no_speech_prob": 4.132923641009256e-05}, {"id": 1231, "seek": 455836, "start": 4579.0, "end": 4583.599999999999, "text": " So I just touched on the exciting work in this area, but there's many other objections", "tokens": [407, 286, 445, 9828, 322, 264, 4670, 589, 294, 341, 1859, 11, 457, 456, 311, 867, 661, 44649], "temperature": 0.0, "avg_logprob": -0.1273611443383353, "compression_ratio": 1.6405693950177935, "no_speech_prob": 4.132923641009256e-05}, {"id": 1232, "seek": 455836, "start": 4583.599999999999, "end": 4585.799999999999, "text": " if you want to dive more into this.", "tokens": [498, 291, 528, 281, 9192, 544, 666, 341, 13], "temperature": 0.0, "avg_logprob": -0.1273611443383353, "compression_ratio": 1.6405693950177935, "no_speech_prob": 4.132923641009256e-05}, {"id": 1233, "seek": 458580, "start": 4585.8, "end": 4590.16, "text": " So there's retrieval augmented language models which learn knowledge retrievers to figure", "tokens": [407, 456, 311, 19817, 3337, 36155, 2856, 5245, 597, 1466, 3601, 19817, 840, 281, 2573], "temperature": 0.0, "avg_logprob": -0.1418578140134734, "compression_ratio": 1.8821428571428571, "no_speech_prob": 7.140200614230707e-05}, {"id": 1234, "seek": 458580, "start": 4590.16, "end": 4594.0, "text": " out what documents might be relevant for predicting the next word.", "tokens": [484, 437, 8512, 1062, 312, 7340, 337, 32884, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1418578140134734, "compression_ratio": 1.8821428571428571, "no_speech_prob": 7.140200614230707e-05}, {"id": 1235, "seek": 458580, "start": 4594.0, "end": 4597.0, "text": " There's work in modifying the knowledge in language models.", "tokens": [821, 311, 589, 294, 42626, 264, 3601, 294, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1418578140134734, "compression_ratio": 1.8821428571428571, "no_speech_prob": 7.140200614230707e-05}, {"id": 1236, "seek": 458580, "start": 4597.0, "end": 4601.400000000001, "text": " So I talked about how this is one of the obstacles and challenges to using language models", "tokens": [407, 286, 2825, 466, 577, 341, 307, 472, 295, 264, 17735, 293, 4759, 281, 1228, 2856, 5245], "temperature": 0.0, "avg_logprob": -0.1418578140134734, "compression_ratio": 1.8821428571428571, "no_speech_prob": 7.140200614230707e-05}, {"id": 1237, "seek": 458580, "start": 4601.400000000001, "end": 4602.400000000001, "text": " as knowledge bases.", "tokens": [382, 3601, 17949, 13], "temperature": 0.0, "avg_logprob": -0.1418578140134734, "compression_ratio": 1.8821428571428571, "no_speech_prob": 7.140200614230707e-05}, {"id": 1238, "seek": 458580, "start": 4602.400000000001, "end": 4605.28, "text": " So there's been recent work in this area.", "tokens": [407, 456, 311, 668, 5162, 589, 294, 341, 1859, 13], "temperature": 0.0, "avg_logprob": -0.1418578140134734, "compression_ratio": 1.8821428571428571, "no_speech_prob": 7.140200614230707e-05}, {"id": 1239, "seek": 458580, "start": 4605.28, "end": 4608.88, "text": " We also saw how important the knowledge-pre-training task was.", "tokens": [492, 611, 1866, 577, 1021, 264, 3601, 12, 3712, 12, 17227, 1760, 5633, 390, 13], "temperature": 0.0, "avg_logprob": -0.1418578140134734, "compression_ratio": 1.8821428571428571, "no_speech_prob": 7.140200614230707e-05}, {"id": 1240, "seek": 458580, "start": 4608.88, "end": 4613.4400000000005, "text": " Well, there's many papers that are proposing different tasks to do the knowledge-pre-training.", "tokens": [1042, 11, 456, 311, 867, 10577, 300, 366, 29939, 819, 9608, 281, 360, 264, 3601, 12, 3712, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.1418578140134734, "compression_ratio": 1.8821428571428571, "no_speech_prob": 7.140200614230707e-05}, {"id": 1241, "seek": 461344, "start": 4613.44, "end": 4617.919999999999, "text": " So it's still an open question in terms of what tasks are best to add to and code more", "tokens": [407, 309, 311, 920, 364, 1269, 1168, 294, 2115, 295, 437, 9608, 366, 1151, 281, 909, 281, 293, 3089, 544], "temperature": 0.0, "avg_logprob": -0.267227705534514, "compression_ratio": 1.7890625, "no_speech_prob": 9.515245437796693e-06}, {"id": 1242, "seek": 461344, "start": 4617.919999999999, "end": 4618.919999999999, "text": " knowledge.", "tokens": [3601, 13], "temperature": 0.0, "avg_logprob": -0.267227705534514, "compression_ratio": 1.7890625, "no_speech_prob": 9.515245437796693e-06}, {"id": 1243, "seek": 461344, "start": 4618.919999999999, "end": 4622.32, "text": " And there's also been work on more efficient knowledge systems.", "tokens": [400, 456, 311, 611, 668, 589, 322, 544, 7148, 3601, 3652, 13], "temperature": 0.0, "avg_logprob": -0.267227705534514, "compression_ratio": 1.7890625, "no_speech_prob": 9.515245437796693e-06}, {"id": 1244, "seek": 461344, "start": 4622.32, "end": 4627.36, "text": " So add an search-mounted efficient QA challenge which aims at building the smallest QA system.", "tokens": [407, 909, 364, 3164, 12, 45275, 292, 7148, 1249, 32, 3430, 597, 24683, 412, 2390, 264, 16998, 1249, 32, 1185, 13], "temperature": 0.0, "avg_logprob": -0.267227705534514, "compression_ratio": 1.7890625, "no_speech_prob": 9.515245437796693e-06}, {"id": 1245, "seek": 461344, "start": 4627.36, "end": 4632.16, "text": " And then finally, there's been work on building better knowledge benchmarks that build on the", "tokens": [400, 550, 2721, 11, 456, 311, 668, 589, 322, 2390, 1101, 3601, 43751, 300, 1322, 322, 264], "temperature": 0.0, "avg_logprob": -0.267227705534514, "compression_ratio": 1.7890625, "no_speech_prob": 9.515245437796693e-06}, {"id": 1246, "seek": 461344, "start": 4632.16, "end": 4636.12, "text": " benchmarks that we saw today.", "tokens": [43751, 300, 321, 1866, 965, 13], "temperature": 0.0, "avg_logprob": -0.267227705534514, "compression_ratio": 1.7890625, "no_speech_prob": 9.515245437796693e-06}, {"id": 1247, "seek": 463612, "start": 4636.12, "end": 4646.12, "text": " So that's all I have for today and I hope your final projects are going well.", "tokens": [50364, 407, 300, 311, 439, 286, 362, 337, 965, 293, 286, 1454, 428, 2572, 4455, 366, 516, 731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.27148328508649555, "compression_ratio": 1.0, "no_speech_prob": 4.607918162946589e-05}], "language": "en"}