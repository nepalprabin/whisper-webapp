{"text": " Okay, hi everyone. So we'll get started again. We're now into week seven of CS224N. If you're following along the syllabus really closely, we actually did a little bit of a rearrangement in classes. And so today it's me and I'm going to talk about co-reference resolution, which is another chance we get to take a deeper dive into a more linguistic topic. They will also show you a couple of new things for deep learning models at the same time. And then the lecture that previously been scheduled at this point, which was going to be John on explanation in neural models, has been shifted later down into week nine, I think it is. But you'll still get him later. Before getting underway, just a couple of announcements on things. Well, first of all, congratulations on surviving assignment five, I hope. I know it was a bit of a challenge for some of you, but I hope it was a rewarding state of the art learning experience on the latest in neural nets. And in any rate, you know, this was a brand new assignment that we used for the first time this year. So we'll really appreciate later on when we do the second survey taking your feedback on it. We've been busy reading people's final project proposals. Thanks lots of interesting stuff there. Our goal is to get them back to you tomorrow. But you know, as soon as you've had a good night's sleep after assignment five, now is also a great time to get started working on your final projects because there's just not that much time to the end of quarter. And I particularly want to encourage all of you to chat to your mentor, regularly go and visit office hours and keep in touch, get advice, just talking through things is a good way to keep you on track. We also plan to begin back assignment four grades later this week. There's some of the work never stops at this point. So the next thing for the final project is the final project milestone. So that we handed out the details of that last Friday and it's due a week from today. So the idea of this final project milestone is really to help keep you on track and keep things moving towards having a successful final project. So our hope is that sort of most of what you write for the final project milestone is material you can also include in your final project, except for a few paragraphs of years exactly where I'm up to now. So the overall hope is that doing this in two parts and having a milestone before the final thing is just making you make progress and be on track for having a successful final project. Finally, the next class on Thursday is going to be Colin Rafellen. This is going to be super exciting. So he's going to be talking more about the very latest in large pre-trained language models, both what some of their successes are and also what some of the disconcerting, not quite so good aspects of those models are. So that should be a really good interesting lecture when we had him come and talk to our NLP seminar. We had several hundred people come along for that. And so for this talk again, we're asking that you write a reaction paragraph following the same instructions as last time about what's in this lecture. And someone that asks in the questions, well, what about last Thursdays? The answer that is no. So the distinction here is we're only doing the reaction paragraphs for outside guest speakers. And although it was great to have on Trian Bosleau for last Thursdays lecture, he's a postdoc at Stanford. So we don't count him as an outside guest speaker. And so nothing needs to be done for that one. So there are three classes for which you need to do it. So there was the one before from Duncishan, Colin Rafelle, which is Thursday, and then towards the end of the course, there's Yulitz Vettkov. Okay, so this is the plan today. So in the first part of it, I'm actually going to spend a bit of time talking about what co-reference is, what different kinds of reference and language are. And then I'm going to move on and talk about some of the kind of methods that people have used to solving co-reference resolution. Now there's one bug in our course design, which was a lot of years we've had a whole lecture on doing convolutional neural nets for language applications. And that slight bug appeared the other day when Duncish talked about the VDF model because she sort of slipped in all this character CNN representation of words. And we haven't actually covered that. And so that was a slight upsee. I mean, actually for applications in co-reference as well, people commonly make use of character level confidence. So I wanted to sort of spend a few minutes sort of doing basics of confidence for language. The sort of reality here is that given that there's no exam week this year to give people more time for final projects, we sort of shorten the content by a week this year and so you're getting a little bit less of that content. Then going on from there, say some stuff about a state of the art neural co-reference system and write at the end talk about how co-reference evaluated and what some of the results are. Yeah. So first of all, what is this co-reference resolution term that I've been talking about a lot? So co-reference resolution is meaning to find all the mentions and a piece of text that refer to the same entity. And sorry, that's a typo. It should be in the world, not in the word. So let's make this concrete. So here's part of a short story by Struz Hirao called the star. Now I have to make a confession here because this is an NLP class, not a literature class. I crudely made some cuts to the story to be able to have relevant parts appear on my slide in a decent size font for illustrating co-reference. So it's not quite the full original text, but it basically is a piece of this story. So what we're doing in co-reference resolution is we're working out what people are mentioned. So here's a mention of a person, Vanajah, and here's a mention of another person, Akila. And well, mentions don't have to be people. So the local park that's also a mention. And then here's Akila again and Akila's son. And then there's Pradhwal. Then there's another son here and then her son and Akash. And they both went to the same school. And then there's a preschool play. And there's Pradhwal again. And then there's a naughty child, Lord Krishna. And there's some that are a bit complicated. Like the lead role is that a mention. It's sort of more of a functional specification of something in the play. There's Akash and it's a tree. I won't go through the whole thing yet. But I mean, in general, there are noun phrases that are mentioning things in the world. And so then what we want to do for co-reference resolution is work out which of these mentions are talking about the same real world entity. So if we start off, so there's Banadja. And so Banadja is the same person as her there. And then we could read through. She resigned herself. So that's both Banadja. She bought him a brown T-shirt and brown trousers. And then she made a large cut out tree. She attached. So all of that's about Banadja. But then we can have another person. So here's a killer. And here's a killer. Maybe those are the only mentions of a killer. So then we can go on from there. Okay. And so then there's Pradjwal. But note the Pradjwal. Note that Pradjwal is also a killer's son. So really a killer's son is also Pradjwal. And so an interesting thing here is that you can get nested syntactic structure so that we have the sort of noun phrases. So that just overall we have sort of this noun phrase, a killer's son Pradjwal, which consists of two noun phrases in an opposition. He is Pradjwal. And then for the noun phrase, a killer's son, it sort of breaks down to itself having an extra possessive noun phrase in it. And then a noun so that you have a killer's and then this is son. So that you have these multiple noun phrases. And so that you can then be sort of having different parts of this be one person in the co-reference. But this noun phrase here referring to a different person in the co-reference. Okay, so back to Pradjwal. All right, so well there's some easy other Pradjwals, right? So there's Pradjwal here. And then you've got some more complicated things. So one of the complicated cases here is that we have they went to the same school. So that they there is what gets referred to as split antecedents. Because the they refers to both Pradjwal and the Kash. And that's an interesting phenomenon that and so I could try and show that somehow I could put some splashes in or something. And if I get a different color, a Kash, we have a Kash and her son. And then this one sort of both of them at once. Right, so human languages have this phenomenon of split antecedents. But you know, one of the things that you should notice when we start talking about algorithms that people use for doing co-reference resolution is that they make some simplified assumptions as they how they go about treating the problem. And one of the simplifications that most algorithms make is for any noun phrase like this pronoun they that's trying to work out what is a co-reference with. And the answer is one thing. And so actually most NLP algorithms for co-reference resolution just cannot get split and antecedents write. Any time it occurs in the text they guess something and they always get it wrong. So that's a sort of a bit of a sad state of affairs, but that's the truth of how it is. Okay, so then going ahead we have Akash here. And then we have another tricky one. So moving on from there, we then have this a tree. So well, in this context of this story Akash is going to be the tree. So you could feel that it was okay to say, well this tree is also Akash. You could also feel that that's a little bit weird and not want to do that. And I mean actually different people's co-reference datasets differ in this. So really that you know that we're predicating identity relationship here between Akash and the property of being a tree. So do we regard the tree as the same as Akash or not? And people make different decisions there. Okay, but then going ahead we have here's Akash and she bought him. So that's Akash. And then we have Akash here. And so then we go on. Okay, so then if we don't regard the tree as the same as Akash, we have a tree here. But then note that the next place over here where we have a mention of a tree, the best tree. But that's sort of really a functional description of you know of possible trees making someone the best tree. It's not really referential to a tree. And so it seems like that's not really co-referent. But if we go on, there's definitely more mention of a tree. So when she she has made the tree truly the nicest tree or well I'm not sure. Is that one co-referent? It is definitely referring to our tree. And maybe this one again is a sort of a functional description that isn't referring to the tree. Okay, there's definitely. And so maybe this one though where it's a tree is referring to the tree. But I hope to have illustrated from this is you know most of the time when we do co-reference in NLP, we just make it look sort of like the conceptual phenomenon is you know kind of obvious that there's a mention of Sarah and then it says she and you say oh they're co-referent. This is easy. But if you actually start looking at real text especially when you're looking at something like this that is a piece of literature, the kind of phenomena you get for co-reference and overlapping reference and it varies out the phenomena that I'll talk about you know they actually get pretty complex and it's not you know there are a lot of hard cases that you actually have to think about as to what things you think about as co-referent or not. Okay, but basically we do want to be able to do something with co-reference because it's useful for a lot of things that we'd like to do in natural language processing. So for one task that we've already talked about question answering but equally for other tasks such as summarization information extraction, if you're doing something like reading through a piece of text and you've got a sentence like he was born in 1961. You really want to know who he refers to to know if this is a good answer to the question of you know when was Barack Obama born or something like that. It turns out also that it's useful in machine translation. So in most languages pronouns have features for gender and number and in quite a lot of languages nouns and adjectives also show features of gender, number and case. And so when you're translating a sentence you want to be aware of these features and what is co-referent as what to be able to get the translations correct. So you know if you want to be able to work out a translation and know whether it's saying Alicia likes Juan because he's smart or Alicia likes Juan because she's smart then you have to be sensitive to co-reference relationships to be able to choose the right translation. For people who build dialogue systems dialogue systems also have issues of co-reference a lot at the time. So you know if it sort of book tickets to see James Bond and the system applies spectra is playing near you at two and three today. Well there's actually co-reference relation. Oh sorry there's a reference relation between spectra and James Bond because spectra is a James Bond film. I'll come back to that one in a minute. But then it's how many tickets would you like two tickets for the showing at three? That three is not just the number three. That three is then a co-reference relationship back to the 3PM showing that was mentioned by the agents in the dialogue system. So again to understand these we need to be understanding the co-reference relationships. So how now can you go about doing co-reference? So the standard traditional answer which I'll present first is co-reference is done in two steps. On the first step what we do is detect mentions in a piece of text and that's actually a pretty easy problem. And then in the second step we work out how to cluster the mentions. So as in my example from the Shruti Rau text basically what you're doing with co-reference is you're building up these clusters sets of mentions that refer to the same entity in the world. So if we explore a little how we could do that as a two step solution the first part was detecting the mentions. And so pretty much there are three kinds of things, different kinds of noun phrases that can be mentions. There are pronouns like I, you're itchy hymn and also some demonstrative pronouns like this and that and things like that. There are explicitly name things so things like Paris, Joe Biden, Nike and then there are plain noun phrases that describe things. So a dog, the big fluffy cat stuck in the tree. And so all of these things that we'd like to identify as mentions. And all the straightforward way to identify these mentions is to use natural language processing tools several of which we've talked about already. So to work out pronouns we can use what's called a part of speech tagger. We can use a part of speech tagger which we haven't really explicitly talked about but we used when you built dependency parsers. So that first of all assigns parts of speech to each word and so that we can just find the words that are pronouns. For named entities we did talk just a little bit about named entity recognizers as a use of sequence models for neural networks. So we can pick out things like person names and company names. And then for the ones like the big fluffy a big fluffy dog we could then be sort of picking out from syntactic structure noun phrases and regarding them as descriptions of things. So that we could use all of these tools and those would give us basically our mentions. It's a little bit more subtle than that because it turns out there are some noun phrases and things of all of those kinds which don't actually refer so that they're not referential in the world. So when you say it is sunny it doesn't really refer. When you make universal claims like every student well every student isn't referring to something you can point to in the world. And more dramatically when you have no student and making a negative universal claim it's not referential to anything. There are also things that you can describe functionally which don't have any clear reference. So if I say the best doughnut in the world that that's a functional claim but it doesn't necessarily have reference. Like if I've established that I think a particular kind of doughnut is the best doughnut in the world I could then say to you I hate the best doughnut in the world yesterday and you know what I mean it might have reference. But if I say something like I'm going around to all the doughnut stores trying to find the best doughnut in the world then it doesn't have any reference yet it's just a sort of a functional description I'm trying to satisfy. You also then have things like quantities, 100 miles it's a quantity that's not really something that has any particular reference. You can mark out 100 miles or sorts of places. So how do we deal with those things that aren't really mentions? Well one way is we could train a machine learning classifier to get rid of those curious mentions that actually mostly people don't do that. Most commonly if you're using this kind of pipeline model where you use a parser and a named NT recognizer you regard everything as you've found as a candidate mention and then you try and run your co-ref system and some of them like those ones hopefully aren't make a referent with anything else and so then you just discard them at the end of the process. Secret? Yeah. I've got an interesting question that linguist experienced on this. A student asks can we say that it is sunny? Has it's referring to the weather? I think so. That's a fair question. People have actually tried to suggest that when you say it is sunny it means the weather is sunny but I guess the majority opinion at least is that isn't plausible. I guess many of you aren't native speakers of English but similar phenomena occur in many other languages. I mean it just intuitively doesn't seem plausible when you say it's sunny or it's raining today that you're really saying that as a shortcut for the weather is raining today it just seems like really what the case is is English likes to have something filling the subject position and when there's nothing better to fill the subject position you stick it in there and get it's raining and so in general it's believed that you get this phenomenon of having these empty dummy it's that appear in various places. I mean another place in which it seems like you clearly get dummy it's is that when you have clauses that are subjects of a verb you can move them to the end of the sentence. So if you have a sentence where you put a clause in the subject position they normally in English sound fairly awkward so it's you have a sentence something like that CS24N is a lot of work is known by all students. People don't normally say that the normal thing to do is to shift the clause to the end of the sentence but when you do that you stick in the dummy it to fill the subject position so you then have it is known by all students that CS224N is a lot of work. So that's the general feeling that this is a dummy it that doesn't have any reference. Okay there's one more question so if someone says it is sunny and like other things and we ask how is the weather. Okay good point you've got me on that one right so someone says how is the weather and you answer it is sunny it then does seem like the it is in reference to the weather. Oh by that well you know I guess this is what our co-reference systems are built trying to do in situations like that they're making a decision of co-reference or not and I guess what you'd want to say in that case is it seems reasonable to regard this one as co-reference to that weather that did appear before it. I mean but that also indicates another reason to think that in the normal cases not co-reference right because normally pronouns are only used when their references establish that you've referred to now like John is answering questions and then you can say he types really quickly and it seemed odd to just sort of start the conversation by he types really quickly because it doesn't have any established reference whereas that doesn't seem to be the case it seems like you can just sort of start a conversation by saying it's raining really hard today and that doesn't sound odd at all. Okay so I've sort of there presented the traditional picture but you know this traditional picture doesn't mean something that was done last millennium before you were born I mean essentially that was the picture until about 2016 that essentially every co-reference system that was built used tools like part of speech tag as any of our systems and parsers to analyze sentences to identify mentions and to give you features for co-reference resolution and I'll show a bit more about that later but more recently in our neural systems people have moved to avoiding traditional pipeline systems and of doing one shot end to end co-reference resolution systems so if I skip directly to the second bullet there's a new generation of neural systems where you just start with your sequence of words and you do the maximally done thing you just say let's take all spans commonly with some heuristics for efficiency but you know conceptually all subsequences of this sentence they might be mentions let's feed them in to a neural network which will simultaneously do mention detection and co-reference resolution end to end in one model and I'll give an example of that kind of system later in the lecture. Okay is everything good to there and I should go on. Okay so I'm going to get on to how to do co-reference resolution systems but before I do that I do actually want to show a little bit more the linguistics of co-reference because there's actually a few more interesting things to understand and know here I mean when we say co-reference resolution we really confuse together two linguistic things which are overlapping but different and so it's really actually good to understand the difference between these things so there are two things that can happen one is that you can have mentions which are essentially standalone but happen to refer to the same entity in the world so if I have a piece of text that said Barack Obama traveled yesterday to Nebraska Obama was there to open a new meat processing plant or something like that I've mentioned with Barack Obama and Obama there are two mentions there they refer to the same person in the world they are co-referent so that is true co-reference but there's a different the related linguistic concept called a naffra and a naffra is when you have a textual dependence of an anaphora on another term which is the antecedent and in this case the meaning of the anaphora is determined by the antecedent in a textual context and the canonical case of this is pronouns so when it's Barack Obama said he would sign the bill he is an anaphora it's not a word that independently we can work out what it's meaning is in the world apart from knowing the vagus feature that it's referring to something probably male but in the context of this text we have that this anaphora is textually dependent on Barack Obama and so then we have an anaphora relationship which sort of means they refer to the same thing in the world and so therefore you can say they're co-referent so the picture we have is like this right so for co-reference we have these separate textual mentions which are basically standalone which refer to the same thing in the world whereas in an affra we actually have a textual relationship and you know you essentially have to use pronouns like he and she in legitimate ways in which the heerer can reconstruct a relationship from the text because they can't work out what he refers to if that's not there and so that's a fair bit of the distinction but it's actually a little bit more to realize because there are more complex forms of an affra which aren't co-reference because you have a textual dependence but it's not actually one of reference and so this comes back to things like these quantifying noun phrases that don't have reference so when you have sentences like these ones every dancer twisted her knee well this her here has an anaphora dependency on every dancer or even more clearly with no dancer twisted her knee the her here has an anaphora dependence on no dancer but for no dancer twisted her knee no dancer isn't referential it's not referring to anything in the world and so there's no co-reference or relationship because there's no reference relationship but there's still an anaphora relationship between these two noun phrases and then you have this other complex case that turns up quite a bit where you can have where the things being talked about do have reference but an anaphora relationship is more subtle than identity so you commonly get the constructions like this one we went to a concert last night the tickets were really expensive well the concert and the tickets are two different things they're not co-reference co-referential but in interpreting this sentence what this really means is the tickets of tickets to the tickets to the concert right and so there's sort of this hidden not not said dependence where this is referring back to the concert and so what we say is that these the tickets does have an anaphora dependence on the concert but they're not co-referential and so that's referred to as bridging an aphra and so overall there's the simple case and the common case which is pronominal anaphora where it's both co-reference and anaphora you then have other cases of co-reference such as every time you see a mention of the every time the United States has said it's co-referential with every other mention of the United States but those don't have any textual dependence on each other and then you have textual dependencies like bridging anaphora which aren't co-reference. Phew that's probably about as mm now I was going to say that's probably as as much linguistics as you wanted to hear but actually I have one more point of linguistics one or two of you but probably not many might have been troubled by the fact that the the term anaphora as a classical term means that you're looking backward for your antecedent that the anapart of anaphora means that you're looking backward for your antecedent and in sort of classical terminology you have both anaphora and cataphora and it's cataphora where you look forward for your antecedent. Cataphora isn't that common but it does occur here's a beautiful example of cataphora so this is from Oscar Wilde from the corner of the divine of Persian saddlebags on which he was lying smoking as was his custom and numerous cigarettes Lord Henry Watten could just catch the gleam of a honey sweet and honey cut of the honey sweet and honey colored blossoms of a labyrinth. Okay so in this example here right the he and then this he is are actually referring to Lord Henry Watten and so these are both examples of cataphora but in in modern linguistics even though most reference to pronouns is backwards we don't distinguish on in terms of order and so the term anaphora and anaphora is used for a textual dependence regardless of whether it's forward or backwards. Okay a lot of details there but taking stock of this so the basic observation is languages interpreted in context that in general you can't work out the meaning or reference of things without looking at the context of the linguistic utterance. So we've seen some simple examples before so for something like words since disambiguation you've if you see just the words but bank you don't know what it means and you need to look at a context to get some senses to whether it means a financial institution or the bank of a river or something like that and so anaphora and co-reference give us additional examples where you need to be doing contextual interpretation of language so when you see a pronoun you need to be looking at the context to see what it refers to and so if you think about text understanding as a human being does it reading a story or an article that we progress through the article from beginning to end and as we do it we build up a pretty complex discourse model in which new entities are introduced by mentions and then they're referred back to and relationships between them are established and they take actions and things like that and it sort of seems like in our head that we sort of build up a kind of a complex graph-like discourse representation of a piece of text with all these relationships and so part of that is these anaphora relationships and co-reference that we're talking about here and indeed in terms of CS224N the only kind of whole discourse meaning that we're going to look at is looking a bit at anaphora and co-reference but if you want to see more about higher level natural language understanding you can get more of this next quarter in CS224U so I want to tell you a bit about several different ways of doing co-reference so broadly there are four different kinds of co-reference models so the traditional old way of doing it was rule-based systems and this isn't the topic of this class and this is pretty archaic at this point this is stuff from last millennium but I wanted to say a little bit about it because it's actually kind of interesting as sort of food for thought as to how far along we are aren't in solving you know artificial intelligence and really being out to understand texts then there are sort of classic machine learning methods of doing it which you can sort of divide up as mention pair methods mention ranking methods and really clustering methods and I'm sort of going to skip the clustering methods today because most of the work especially most of the recent work implicitly makes clusters by using even mention pair or mention ranking methods and so I'm going to talk about a couple of neural methods for doing that okay but first of all let me just tell you a little bit about rule-based co-reference so there's a famous historical algorithm in NLP for doing pronoun and affer a resolution which is referred to as the Hobbs algorithm so everyone just refers to it as the Hobbs algorithm and if you sort of look up a textbook like Draftski and Martin's textbook it's referred to as the Hobbs algorithm but you know actually if you go back to Jerry Hobbs that's his picture over there in the corner if you actually go back to his original paper he refers to it as the naive algorithm and then his naive algorithm for pronoun co-reference was this sort of intricate handwritten set of rules to work out co-reference so this is the start of the set of the rules but there are more rules or more clauses of these rules for working out co-reference and you know this looks like a hot mess but the funny thing was that this set of rules for determining co-reference were actually pretty good and so in the sort of 1990s and 2000s decade even when people were using machine learning base systems for doing co-reference they had hide into those machine learning base systems that one of their features was the Hobbs algorithm and that the predictions it made with a certain weight was then a feature in making your final decisions and it's only really in the last five years that people have moved away from using the Hobbs algorithm let me give you a little bit of a sense of how it works okay so the Hobbs algorithm here's our example this is an example from a Guardian book review Nile Ferguson is prolific well-paid and a snappy dresser Steven Moss heated him okay so what the Hobbs algorithm does is we start with a pronoun oops we start with a pronoun and then it says step one go to the NP that's immediately dominating the pronoun and then it says go up to the first NP or S call this X and the path P then it says traverse all branches below X the left of P left to right bread first so then it's saying to go left to right for other branches below bread first so that's sort of working down the tree so we're going down and left to right and look for an NP okay and here's an NP but then we have to read more carefully and say propose as antecedent any NP that has an NP or S between it and X well this NP here has no NP or S between NP and X so this isn't a possible antecedent so this is all very you know complex and handwritten but basically he's sort of fit into the clauses of this kind of a lot of facts about how the grammar of English works and so what this is capturing is if you imagine a different sentence you know if you imagine the sentence Steven Moss's brother hated him well then Steven Moss would naturally be co-referent with him and in that case well precisely what you'd have is the noun phrase with well the noun brother and you'd have another noun phrase inside it for the Steven Moss and then that would go up to the sentence so in the case of Steven Moss's brother when you looked at this noun phrase there would be an intervening noun phrase before you got to the note X and therefore Steven Moss is a possible and in fact good antecedent of him and the algorithm would choose Steven Moss but the algorithm correctly captures that when you have the sentence Steven Moss hated him that him cannot refer to Steven Moss okay so having worked that out it then says if X is the highest S in the sentence okay so my X here is definitely the highest S in the sentence because I've got the whole sentence what you should do is then traverse the parse trees of previous sentences in the order of recency so what I should not do now is sort of work backwards in the text one sentence at a time going backwards looking for an antecedent and then for each tree traverse each tree left or right bread first so then within each tree I'm doing the same of going bread first so sort of working down and then going left or right with an equal bread and so hidden inside these clauses it's capturing a lot of the facts of how co-reference typically works so what you find in English I'll stay but in general this is true of lots of languages is that there are general preferences and tendencies for co-reference so a lot of the time a pronoun will be co-referent with something in the same sentence like Steven's Moss's brother heeded him but it can't be if it's too close to it so you can't say Steven Moss heeded him and have the him be Steven Moss and if you're then looking for co-reference it's further away the thing it's co-referent with is normally close by and so that's why you work backwards through sentences one by one but then once you're looking within a particular sentence the most likely thing that's going to be co-referent too is a topical noun phrase and default topics in English subjects so by doing things bread first left or right a preferred antecedent is then a subject and so this algorithm I won't go through all the complex clauses 529 ends up saying okay what you should do is propose Nile Ferguson as what is co-referent to him which is the obvious correct reading in this example okay you probably didn't want to know that and in some sense the details of that aren't interesting but what is I think actually still interesting in 2021 is what points Jerry Hobbes was actually trying to make last millennium and the point he was trying to make was the following so Jerry Hobbes wrote this algorithm the naive algorithm because what he said was well look if you want to try and crudely determine co-reference well there are these various preferences right there's the preference for same sentence there's the preference for recency there's a preference for topical things like subject and there are things where you know if it has gender it has to agree in gender so there are sort of strong constraints of that sort so I can write an algorithm using my linguistic mouse which captures all the main preferences and actually it works pretty well doing that is a pretty strong baseline system but what Jerry Hobbes wanted to argue is that this algorithm just isn't something you should believe in this isn't a solution to the problem this is just sort of you know making a best guess according to the preferences of what's most likely without actually understanding what's going on in the text at all and so actually what Jerry Hobbes wanted to argue was the so-called Hobbes algorithm now he wasn't a fan of the Hobbes algorithm he was wanting to argue that the Hobbes algorithm is completely inadequate as a solution to the problem and the only way we'll actually make progress in natural language understanding is by building systems that actually really understand the text and this is actually something that has come to the fore again more recently so the suggestion is that in general you can't work out co-reference or phenomenal in that for in particular unless you're really understanding the meaning of the text and people look at pairs of examples like these ones so she poured water from the picture into the cup until it was full so think for just half a moment well what is it in that example that is full so that what's full there is the cup but then if I say she poured water from the picture into the cup until it was empty well what's empty well that's the picture and the point that is being made with this example is the only thing that's been changed in these examples is the adjective right here so these two examples have exactly the same grammatical structure so in terms of the Hobbes naive algorithm the Hobbes naive algorithm necessarily has to predict the same answer for both of these but that's wrong you just cannot determine the correct pronoun antecedent based on grammatical preferences of the kind that are used in the naive algorithm you actually have to conceptually understand about pictures and cups and water and full and empty to be able to choose the right antecedent here's another famous example that goes along the same lines so Terry Winnegrad shown here as a young man so long long ago Terry Winnegrad came to Stanford as the natural language processing faculty and Terry Winnegrad became disillusioned with the symbolic AI of those days and just gave it up altogether and he reinvented himself as being an HCI person and so Terry was then essentially the person who established the HCI program at Stanford but before he lost faith in symbolic AI he talked about the co-reference problem and pointed out a similar pair of examples here so we have the city council refused the women a permit because they feared violence versus the city council refused the women a permit because they advocated violence so again you have this situation where these two sentences have identical syntactic structure and they differ only in the choice of verb here but once you add knowledge common sense knowledge of how the human world works well what how this should pretty obviously be interpreted in the first one that they is referring to the city council whereas in the second one that they is referring to the women and so coming off of that example of Terry these have been preferred to as Winnegrad schemers so Winnegrad schema challenges sort of choosing the right reference here and so it's basically just doing pronominal and afra but you know the interesting thing is people have been interested you know what a test of general intelligence and one famous general test of intelligence so I won't talk about now is the Turing test and there's been a lot of debate about problems with the Turing test and is it good and so in particular Hector Levesque is a very well-known senior AI person he actually proposed that a better alternative to the Turing test might be to do what he then dubbed Winnegrad schema and Winnegrad schema is just solving pronominal co-reference in cases like this where you have to have knowledge about the situation the world to get the answer right and so he's basically arguing that you know you can review really solving co-reference as solving artificial intelligence and that's sort of what the position that Hobbes wanted to advocate so what he actually said about his algorithm was that the naive approach is quite good computationally speaking it will be a long time before a semantically-based algorithm is sophisticated enough to perform as well and these results set a very high standard for any other approached way in for and he was proven right about that because there was sort of really talked to around 2015 before people thought they could do without the Hobbes algorithm but then he notes yet there is every reason to pursue a semantically-based approach the naive algorithm does not work anyone can think of examples where it fails in these cases it not only fails it gives no indication that it has failed and offers no help in finding the real antecedent and so I think this is actually still interesting stuff to think about because you know really for the kind of machine learning-based co-reference systems that we're building you know they're not a hot mess of rules like the Hobbes algorithm but basically they're still sort of working out statistical preferences of what patterns are most likely and choosing the antecedent that way they really have exactly the same deficiencies still that Hobbes was talking about right that they fail in various cases it's easy to find places where they fail the algorithms give you no idea when they fail they're not really understanding the text in a way that a human does to determine the antecedent so we still actually have a lot more work to do before we're really doing full artificial intelligence but I best get on now and actually tell you a bit about some co-reference algorithms right so the simple way of thinking about co-reference is to say that you're making just a binary decision about a reference pair so if you have your mentions you can then say well I've come to my next mention she I want to work out what it's co-referent with and I can just look at all of the mentions that came before it and say is it co-referent or not and do a binary decision so at training time I'll be able to say I have positive examples assuming I've got some data labeled for what's co-referent of what as to these ones are co-referent and I've got some negative examples of these ones are not co-referent and what I want to do is build a model that learns to predict co-referent things and I can do that fairly straightforwardly in the kind of ways that we have talked about so I train with the regular kind of cross entropy loss where I'm now summing over every pairwise binary decision as to whether two mentions are co-referent to each other or not and so then when I'm at test time what I want to do is cluster the mentions that correspond to the same entity and I do that by making use for my pairwise score so I can run my pairwise score and it will give a probability or a score that any two mentions are co-referent so by picking some threshold like 0.5 I can add co-reference links for when the classifier says it's above the threshold and then I do one more step to give me a clustering I then say okay let's also make the transitive closure to give me clusters so it thought that I and she were co-referent and my and she were co-referent therefore I also have to regard I and my as co-referent and so that's sort of the completion by transitivity and so since we always complete by transitivity note that this algorithm is very sensitive to making any mistake in a positive sense because if you make one mistake for example you say that he and my a co-referent and then by transitivity all of the mentions in these sentence become one big cluster and that they're all co-referent with each other so that's a workable algorithm and people have often used it but often people go a little bit beyond that and prefer a mention ranking model so it we just explain the advantages of that that normally if you have a long document where it's Ralph Nader and he did this and some of them did something to him and we visited his house and blah blah blah blah and then somebody voted for Nader because he in terms of building a co-reference classifier it seems like it's easy and reasonable it's easy and reasonable to be able to recover that this he refers to Nader but in terms of building a classifier for it to recognize that this he should be referring to this Nader which might be three paragraphs back seems kind of unreasonable how you're going to recover that so those far away ones might be almost impossible to get correct and so that suggests that maybe we should have a different way of configuring this task so instead of doing it that way what we should say is well this he here has various possible antecedents and our job is to just choose one of them and that's almost sufficient apart from we need to add one more choice which is well some mentions won't be co-referent with anything that proceeds because we're introducing a new entity into the discourse so we can add one more dummy mention the N.A. mention so it doesn't refer to anything previously in the discourse discourse and then our job at each point is to do mention ranking to choose which one of these she refers to and then at that point rather than doing binary yes no classifiers that what we can do is say aha this is choose one classification and then we can use the kind of softmax classifiers that we've seen at many points previously okay so that gets us in business for building systems and for either of these kind of models there are several ways in which we can build the system we could use any kind of traditional machine learning classifier we could use simple neural network we can use more advanced ones with all of the tools that we've been learning about more recently let me just quickly show you a simple neural network way of doing it so this is a model that my PhD student Kevin Clark did in 2015 so not that long ago but what he was doing was doing co-reference resolution based on the mentions with a simple feed forward neural network kind of in some sense like we did dependency parsing with a simple feed forward neural network so for for the mention it had word embeddings and seed and had word embeddings there were some additional features of each of the mention and candidate and a seed and and then there were some final additional features that captured things like distance away which you can't see from either the mention or the candidate and they were all of those features were just fed into several feed forward layers of a neural network and it gave you a score of are these things um co-referent or not and that by itself um just worked pretty well um and I won't say more details about that um but what I do want to show is sort of a more advanced um and modern neural co-reference system but before I do that I want to take a digression and sort of say a few words about convolutional neural networks so um the idea of when you apply a convolutional neural network to language i.e. to sequences is that what you're going to do is you're going to compute vectors features effectively for every possible words sub sequence of a certain length so that if you have a piece of text like tentative deal reach to keep government open you might say I'm going to take every three words of that I tentative deal reached deal reached to reach to keep and I'm going to compute a vector based on that sub sequence of words and use those computed vectors in my model by somehow grouping them together so the canonical um case of convolutional neural networks um is in vision and so if after this next quarter um you go along to CS231 and um you'll be able to um spend weeks doing convolutional neural networks for vision and so the idea there is that you've got these convolutional filters that you sort of slide over an image and you compute a function of each place so the sort of little red um numbers are showing you what you're computing but then you'll slide it over to the next position and fill in this cell and then you'll slide over the next position and fill in this cell and then you'll slide it down and fill in this cell and so you've got this sort of little function of a patch which you're sliding over your image and computing a convolution which is just um a dot product effectively um that you're then using to get an extra layer of representation and so by sliding things over you can pick out features and you've got a sort of a feature identifier that runs across every piece of the image um well for language we've just got a sequence but you can do basically the same thing and what you then have is a 1D convolution for text so if here's my sentence tentative deal reach to keep the government open that what I can do is have um so these words have uh word representation which so this is my vector for each word and then I can have a filter sometimes called a kernel which I use for my convolution and what I'm going to do is slide that down the text so I can start with it with the first three words and then I sort of treat them as sort of elements I can dot product and some and then I can compute a value as to what they all add up to which is minus one it turns out um and so then I might have a bias that I add on and get an updated value if my bias is plus one and then I'd run it through a nonlinearity and that will give me a final value um and then I'll slide my filter down and I'd work out um a computation for this window of three words and take 0.5 times 3 plus 0.2 times 1 etc and that comes out as this value um I add the bias I put it I'm going to put it through my nonlinearity and then I keep on sliding down and I'll do the next three words and keep on going down and so that gives me a 1D convolution and computes a representation of the text um you might have noticed in the previous example that I started here with seven words but because I wanted to have a window of three for my convolution the end result is that things shrunk so in the output I only had five things that's not necessarily desirable so commonly people will deal with that um with padding so if I put padding on both sides I can then start my 3 by 3 convolution my 3 so not 3 by 3 my 3 convolution um here and compute this one and then slide it down one and compute this one and so now my output is the same size as my real input and so that's a convolution with padding um okay so that was the start of things but you know how you get more power of the convolutional network is you don't only have one filter you have several filters so if I have three filters each of which will have their own bias nonlinearity I can then get a three-dimensional representation coming out the end and sort of you can think of these as conceptually computing different features of your text okay so that gives us a kind of a sort of a new feature re-representation of our text but commonly we then want to somehow summarize what we have and a very common way of summarizing what we have is to then do pooling so if we sort of think of these features as detecting different things in the text so you know they might even be high-level features like you know does this show signs of toxicity or hate speech is there reference to something so if you want to be interested and doesn't occur anywhere in the text what people often then do as a max pooling operation where for each feature they simply sort of compute the maximum value it ever achieved in any position as you went through the text and say that this vector ends up as the sentence representation sometimes for other purposes rather than max pooling people use average pooling where you take the averages of the different vectors to get the sentence representation then general max pooling has been found to be more successful and that's kind of because if you think of it as feature detectors that are wanting to detect was this present somewhere then it you know something like positive sentiment isn't going to be present in every three word subsequent you choose before us there somewhere is there and so often max pooling works better and so that's a very quick look at convolutional neural networks except to say this example was doing 1D convolutions with words but a very common place that convolutional neural networks being used in natural language is actually using them with characters and so what you can do is you can do convolutions over subsequences of the characters in the same way and if you do that this allows you to compute a representation for any sequence of characters so you don't have any problems with being out of vocabulary or anything like that because for any sequence of characters you just compute your convolutional representation and max pool word and so quite commonly people use a character convolution to give a representation of words perhaps as the only representation of words but otherwise is something that you use in addition to a word vector and so in both bydaph in the model I'm about to show that at the base level it makes use of both a word vector representation that we see saw right at the beginning of the text and a character level convolutional representation of the words okay with that said I now want to show you before time runs out an end-to-end neural co-refer system model so the model I'm going to show you is Kenton Lee's one from so Dunn University of Washington in 2017 this is no longer the state of the art I'll mention the state of the art at the end but this was the first model that really said get rid of all of that old stuff of having pipelines and mention detection first just build one end-to-end big model that does everything in returns co-reference so it's a good one to show so compared to the earliest simple thing I saw we're now going to process the text with bylistians we're going to make use of attention and we're going to do all of mention detection co-reference in one step end-to-end and the way it does that is by considering every span of the text up to a certain length as a candidate mentioned and just figures out a representation for it and whether it's co-referent to other things so what we do at the start is we start with the sequence of words and we calculate from those standard word embedding and a character level CNNs embedding we then feed those as inputs into a bidirectional LSTM of the kind that we saw quite a lot of before but then after this what we do is we compute representations for spans so when we have a sequence of words we're then going to work out a representation of a sequence of words which we can then put into our co-reference model so that we I can't fully illustrate in this picture but so sub sequences of different lengths so like general electric general electric said we'll all have a span representation which I've only shown a subset of them in green so how are those computed well the way they're computed is that the span representation is a vector that can catenate several vectors and it consists of four parts it consists of the representation that was computed for the start of the span from the bio LSTM the representation of the end from the bio LSTM that's over here and then it has a third part that's kind of interesting this is an intention based representation that is calculated from the whole span but particularly sort of looks for the head of a span and then there are still a few additional features so it turns out that you know some of these additional things like length and so on is still a bit useful so to work out the the final part is not the beginning and the end what's done is to calculate an intention weighted average of the word embeddings so what you're doing is you're taking the x star representation of the final word of the span and you're feeding that into a neural network to get attention scores for every word in the span which are these three and that's giving you an attention distribution as we've seen previously and then you're calculating the third component of this as an attention weighted sum of the different words in the span and so therefore you've got the sort of a sort of a soft average of the representations of the words of the span okay so then once you've got that what you're doing is then feeding these representations into having scores for whether spans are co-referent mentions so you have a representation of the two spans you have a score that's calculated for whether two different spans look co-referent and that overall you're getting a score for our different spans looking co-referent or not and so this model is just run into a non-all spans now it sort of would get intractable if you scored literally every span in a long piece of text so they do some pruning they sort of only allow spans up to a certain maximum size they only consider pairs of spans that aren't too distant from each other etc etc but basically it's in sort of an approximation just a complete comparison of spans and this turns into a very effective co-reference resolution algorithm today it's not the best co-reference resolution algorithm because maybe not surprisingly like everything else that we've been dealing with there's now been these transformer models like BERT have come along and that they produce even better results so the best co-reference systems now have you make use of BERT in particular when Danchi spoke she briefly mentioned span BERT which was a variant of BERT which constructs blanks out for reconstruction sub sequences of words rather than just a single word and span BERT has actually proven to be very effective for doing co-reference perhaps because you can blank out whole mentions people have also gotten gains actually funnily by treating co-reference a question answering task so effectively you can find a mention like he or the person and say what is it's there to see then and get in question answering answer and that's a good way to do co-reference so if we put that together as time is running out let me just sort of give you some sense of how results come out for co-reference systems so I'm skipping a bit actually which you can find in the slides which is how co-references scored but essentially it's scored on a clustering metric so a perfect clustering and give you a hundred and something that makes no correct decisions would give you zero and so this is sort of how the co-reference numbers have been panning out so back in 2010 actually this was a Stanford system this was a state of the art system for co-reference and one competition it was actually a non-machine learning model because again we were wanting to so prove how these rule based methods and practice work kind of well and so its accuracy was around 55 English 50 for Chinese then gradually machine learning these were sort of statistical machine learning models got a bit better wiseman was the very first neural co-reference system and that gave some gains he has a system that Kevin Clark and I did which gave a little bit further gains so Lee is the model that I've just shown you as the end-win model and it got a bit of further gains but then again you know what gave the huge breakthrough just like question answering was that the use of spanbert so once we moved it here we're now using spanbert that's giving you about an extra 10 percent or so the co-ref QA technique proved to be useful and then the very latest best results are effectively combining together spanbert and or larger version of spanbert and co-ref QA and getting up to 83 so you might think from that that co-ref is sort of doing really well and is getting close to solve like other NLP tasks well it's so me true that in neural times the results have been getting way way better than they had been before but I would caution you that these results that I just showed were on a corpus called onto notes which is mainly newswire and it turns out that newswire co-reference is pretty easy I mean in particular there's a lot of mention of the same entities right so the newspaper articles are full of mentions of the United States and China and leaders of the different countries and it's sort of very easy to work out what their co-reference to and so the co-reference scores are fairly high whereas if what you do is take something like a page of dialogue from a novel and feed that into a system and say okay do the co-reference correctly you'll find pretty rapidly that the performance of the models is much more modest if you'd like to try out a co-reference system for yourself there are pointers to a couple of them here where the top ones are ours from the Southern Kevin Clark's neural co-reference and this is one that goes with the hugging face repository that we've mentioned", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.08, "text": " Okay, hi everyone. So we'll get started again. We're now into week seven of CS224N. If", "tokens": [1033, 11, 4879, 1518, 13, 407, 321, 603, 483, 1409, 797, 13, 492, 434, 586, 666, 1243, 3407, 295, 9460, 7490, 19, 45, 13, 759], "temperature": 0.0, "avg_logprob": -0.25768134329054093, "compression_ratio": 1.361256544502618, "no_speech_prob": 0.04749716818332672}, {"id": 1, "seek": 0, "start": 14.08, "end": 18.56, "text": " you're following along the syllabus really closely, we actually did a little bit of", "tokens": [291, 434, 3480, 2051, 264, 48077, 534, 8185, 11, 321, 767, 630, 257, 707, 857, 295], "temperature": 0.0, "avg_logprob": -0.25768134329054093, "compression_ratio": 1.361256544502618, "no_speech_prob": 0.04749716818332672}, {"id": 2, "seek": 0, "start": 18.56, "end": 25.400000000000002, "text": " a rearrangement in classes. And so today it's me and I'm going to talk about co-reference", "tokens": [257, 39568, 518, 294, 5359, 13, 400, 370, 965, 309, 311, 385, 293, 286, 478, 516, 281, 751, 466, 598, 12, 265, 5158], "temperature": 0.0, "avg_logprob": -0.25768134329054093, "compression_ratio": 1.361256544502618, "no_speech_prob": 0.04749716818332672}, {"id": 3, "seek": 2540, "start": 25.4, "end": 30.72, "text": " resolution, which is another chance we get to take a deeper dive into a more linguistic", "tokens": [8669, 11, 597, 307, 1071, 2931, 321, 483, 281, 747, 257, 7731, 9192, 666, 257, 544, 43002], "temperature": 0.0, "avg_logprob": -0.18105196952819824, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0011901883408427238}, {"id": 4, "seek": 2540, "start": 30.72, "end": 35.6, "text": " topic. They will also show you a couple of new things for deep learning models at the", "tokens": [4829, 13, 814, 486, 611, 855, 291, 257, 1916, 295, 777, 721, 337, 2452, 2539, 5245, 412, 264], "temperature": 0.0, "avg_logprob": -0.18105196952819824, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0011901883408427238}, {"id": 5, "seek": 2540, "start": 35.6, "end": 41.8, "text": " same time. And then the lecture that previously been scheduled at this point, which was going", "tokens": [912, 565, 13, 400, 550, 264, 7991, 300, 8046, 668, 15678, 412, 341, 935, 11, 597, 390, 516], "temperature": 0.0, "avg_logprob": -0.18105196952819824, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0011901883408427238}, {"id": 6, "seek": 2540, "start": 41.8, "end": 49.239999999999995, "text": " to be John on explanation in neural models, has been shifted later down into week nine,", "tokens": [281, 312, 2619, 322, 10835, 294, 18161, 5245, 11, 575, 668, 18892, 1780, 760, 666, 1243, 4949, 11], "temperature": 0.0, "avg_logprob": -0.18105196952819824, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0011901883408427238}, {"id": 7, "seek": 2540, "start": 49.239999999999995, "end": 52.92, "text": " I think it is. But you'll still get him later.", "tokens": [286, 519, 309, 307, 13, 583, 291, 603, 920, 483, 796, 1780, 13], "temperature": 0.0, "avg_logprob": -0.18105196952819824, "compression_ratio": 1.576470588235294, "no_speech_prob": 0.0011901883408427238}, {"id": 8, "seek": 5292, "start": 52.92, "end": 59.400000000000006, "text": " Before getting underway, just a couple of announcements on things. Well, first of all, congratulations", "tokens": [4546, 1242, 27534, 11, 445, 257, 1916, 295, 23785, 322, 721, 13, 1042, 11, 700, 295, 439, 11, 13568], "temperature": 0.0, "avg_logprob": -0.16462854905561966, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0005720210610888898}, {"id": 9, "seek": 5292, "start": 59.400000000000006, "end": 66.12, "text": " on surviving assignment five, I hope. I know it was a bit of a challenge for some of you,", "tokens": [322, 24948, 15187, 1732, 11, 286, 1454, 13, 286, 458, 309, 390, 257, 857, 295, 257, 3430, 337, 512, 295, 291, 11], "temperature": 0.0, "avg_logprob": -0.16462854905561966, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0005720210610888898}, {"id": 10, "seek": 5292, "start": 66.12, "end": 70.44, "text": " but I hope it was a rewarding state of the art learning experience on the latest in neural", "tokens": [457, 286, 1454, 309, 390, 257, 20063, 1785, 295, 264, 1523, 2539, 1752, 322, 264, 6792, 294, 18161], "temperature": 0.0, "avg_logprob": -0.16462854905561966, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0005720210610888898}, {"id": 11, "seek": 5292, "start": 70.44, "end": 75.44, "text": " nets. And in any rate, you know, this was a brand new assignment that we used for the", "tokens": [36170, 13, 400, 294, 604, 3314, 11, 291, 458, 11, 341, 390, 257, 3360, 777, 15187, 300, 321, 1143, 337, 264], "temperature": 0.0, "avg_logprob": -0.16462854905561966, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0005720210610888898}, {"id": 12, "seek": 5292, "start": 75.44, "end": 80.36, "text": " first time this year. So we'll really appreciate later on when we do the second survey", "tokens": [700, 565, 341, 1064, 13, 407, 321, 603, 534, 4449, 1780, 322, 562, 321, 360, 264, 1150, 8984], "temperature": 0.0, "avg_logprob": -0.16462854905561966, "compression_ratio": 1.6826568265682658, "no_speech_prob": 0.0005720210610888898}, {"id": 13, "seek": 8036, "start": 80.36, "end": 86.6, "text": " taking your feedback on it. We've been busy reading people's final project proposals.", "tokens": [1940, 428, 5824, 322, 309, 13, 492, 600, 668, 5856, 3760, 561, 311, 2572, 1716, 20198, 13], "temperature": 0.0, "avg_logprob": -0.15636376354181877, "compression_ratio": 1.6268115942028984, "no_speech_prob": 0.0008763714577071369}, {"id": 14, "seek": 8036, "start": 86.6, "end": 92.48, "text": " Thanks lots of interesting stuff there. Our goal is to get them back to you tomorrow. But", "tokens": [2561, 3195, 295, 1880, 1507, 456, 13, 2621, 3387, 307, 281, 483, 552, 646, 281, 291, 4153, 13, 583], "temperature": 0.0, "avg_logprob": -0.15636376354181877, "compression_ratio": 1.6268115942028984, "no_speech_prob": 0.0008763714577071369}, {"id": 15, "seek": 8036, "start": 92.48, "end": 97.4, "text": " you know, as soon as you've had a good night's sleep after assignment five, now is also a", "tokens": [291, 458, 11, 382, 2321, 382, 291, 600, 632, 257, 665, 1818, 311, 2817, 934, 15187, 1732, 11, 586, 307, 611, 257], "temperature": 0.0, "avg_logprob": -0.15636376354181877, "compression_ratio": 1.6268115942028984, "no_speech_prob": 0.0008763714577071369}, {"id": 16, "seek": 8036, "start": 97.4, "end": 102.12, "text": " great time to get started working on your final projects because there's just not that", "tokens": [869, 565, 281, 483, 1409, 1364, 322, 428, 2572, 4455, 570, 456, 311, 445, 406, 300], "temperature": 0.0, "avg_logprob": -0.15636376354181877, "compression_ratio": 1.6268115942028984, "no_speech_prob": 0.0008763714577071369}, {"id": 17, "seek": 8036, "start": 102.12, "end": 107.72, "text": " much time to the end of quarter. And I particularly want to encourage all of you to chat to your", "tokens": [709, 565, 281, 264, 917, 295, 6555, 13, 400, 286, 4098, 528, 281, 5373, 439, 295, 291, 281, 5081, 281, 428], "temperature": 0.0, "avg_logprob": -0.15636376354181877, "compression_ratio": 1.6268115942028984, "no_speech_prob": 0.0008763714577071369}, {"id": 18, "seek": 10772, "start": 107.72, "end": 114.12, "text": " mentor, regularly go and visit office hours and keep in touch, get advice, just talking through", "tokens": [14478, 11, 11672, 352, 293, 3441, 3398, 2496, 293, 1066, 294, 2557, 11, 483, 5192, 11, 445, 1417, 807], "temperature": 0.0, "avg_logprob": -0.10384165303090985, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003533878189045936}, {"id": 19, "seek": 10772, "start": 114.12, "end": 119.96, "text": " things is a good way to keep you on track. We also plan to begin back assignment four grades", "tokens": [721, 307, 257, 665, 636, 281, 1066, 291, 322, 2837, 13, 492, 611, 1393, 281, 1841, 646, 15187, 1451, 18041], "temperature": 0.0, "avg_logprob": -0.10384165303090985, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003533878189045936}, {"id": 20, "seek": 10772, "start": 119.96, "end": 127.08, "text": " later this week. There's some of the work never stops at this point. So the next thing for the", "tokens": [1780, 341, 1243, 13, 821, 311, 512, 295, 264, 589, 1128, 10094, 412, 341, 935, 13, 407, 264, 958, 551, 337, 264], "temperature": 0.0, "avg_logprob": -0.10384165303090985, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003533878189045936}, {"id": 21, "seek": 10772, "start": 127.08, "end": 134.12, "text": " final project is the final project milestone. So that we handed out the details of that last", "tokens": [2572, 1716, 307, 264, 2572, 1716, 28048, 13, 407, 300, 321, 16013, 484, 264, 4365, 295, 300, 1036], "temperature": 0.0, "avg_logprob": -0.10384165303090985, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.0003533878189045936}, {"id": 22, "seek": 13412, "start": 134.12, "end": 142.84, "text": " Friday and it's due a week from today. So the idea of this final project milestone is really to", "tokens": [6984, 293, 309, 311, 3462, 257, 1243, 490, 965, 13, 407, 264, 1558, 295, 341, 2572, 1716, 28048, 307, 534, 281], "temperature": 0.0, "avg_logprob": -0.08635940108188363, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.00017858664796222}, {"id": 23, "seek": 13412, "start": 142.84, "end": 148.84, "text": " help keep you on track and keep things moving towards having a successful final project.", "tokens": [854, 1066, 291, 322, 2837, 293, 1066, 721, 2684, 3030, 1419, 257, 4406, 2572, 1716, 13], "temperature": 0.0, "avg_logprob": -0.08635940108188363, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.00017858664796222}, {"id": 24, "seek": 13412, "start": 148.84, "end": 154.68, "text": " So our hope is that sort of most of what you write for the final project milestone is material", "tokens": [407, 527, 1454, 307, 300, 1333, 295, 881, 295, 437, 291, 2464, 337, 264, 2572, 1716, 28048, 307, 2527], "temperature": 0.0, "avg_logprob": -0.08635940108188363, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.00017858664796222}, {"id": 25, "seek": 13412, "start": 154.68, "end": 160.28, "text": " you can also include in your final project, except for a few paragraphs of years exactly where I'm", "tokens": [291, 393, 611, 4090, 294, 428, 2572, 1716, 11, 3993, 337, 257, 1326, 48910, 295, 924, 2293, 689, 286, 478], "temperature": 0.0, "avg_logprob": -0.08635940108188363, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.00017858664796222}, {"id": 26, "seek": 16028, "start": 160.28, "end": 166.84, "text": " up to now. So the overall hope is that doing this in two parts and having a milestone before", "tokens": [493, 281, 586, 13, 407, 264, 4787, 1454, 307, 300, 884, 341, 294, 732, 3166, 293, 1419, 257, 28048, 949], "temperature": 0.0, "avg_logprob": -0.107762449531145, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.00017297618614975363}, {"id": 27, "seek": 16028, "start": 166.84, "end": 172.84, "text": " the final thing is just making you make progress and be on track for having a successful final project.", "tokens": [264, 2572, 551, 307, 445, 1455, 291, 652, 4205, 293, 312, 322, 2837, 337, 1419, 257, 4406, 2572, 1716, 13], "temperature": 0.0, "avg_logprob": -0.107762449531145, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.00017297618614975363}, {"id": 28, "seek": 16028, "start": 174.04, "end": 180.84, "text": " Finally, the next class on Thursday is going to be Colin Rafellen. This is going to be super", "tokens": [6288, 11, 264, 958, 1508, 322, 10383, 307, 516, 281, 312, 29253, 7591, 2106, 19191, 13, 639, 307, 516, 281, 312, 1687], "temperature": 0.0, "avg_logprob": -0.107762449531145, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.00017297618614975363}, {"id": 29, "seek": 16028, "start": 180.84, "end": 187.0, "text": " exciting. So he's going to be talking more about the very latest in large pre-trained language", "tokens": [4670, 13, 407, 415, 311, 516, 281, 312, 1417, 544, 466, 264, 588, 6792, 294, 2416, 659, 12, 17227, 2001, 2856], "temperature": 0.0, "avg_logprob": -0.107762449531145, "compression_ratio": 1.648068669527897, "no_speech_prob": 0.00017297618614975363}, {"id": 30, "seek": 18700, "start": 187.0, "end": 193.0, "text": " models, both what some of their successes are and also what some of the disconcerting, not quite", "tokens": [5245, 11, 1293, 437, 512, 295, 641, 26101, 366, 293, 611, 437, 512, 295, 264, 717, 1671, 1776, 783, 11, 406, 1596], "temperature": 0.0, "avg_logprob": -0.12386940337799407, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0003297921794001013}, {"id": 31, "seek": 18700, "start": 193.0, "end": 199.64, "text": " so good aspects of those models are. So that should be a really good interesting lecture when we", "tokens": [370, 665, 7270, 295, 729, 5245, 366, 13, 407, 300, 820, 312, 257, 534, 665, 1880, 7991, 562, 321], "temperature": 0.0, "avg_logprob": -0.12386940337799407, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0003297921794001013}, {"id": 32, "seek": 18700, "start": 199.64, "end": 205.96, "text": " had him come and talk to our NLP seminar. We had several hundred people come along for that.", "tokens": [632, 796, 808, 293, 751, 281, 527, 426, 45196, 29235, 13, 492, 632, 2940, 3262, 561, 808, 2051, 337, 300, 13], "temperature": 0.0, "avg_logprob": -0.12386940337799407, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0003297921794001013}, {"id": 33, "seek": 18700, "start": 207.4, "end": 215.0, "text": " And so for this talk again, we're asking that you write a reaction paragraph following the same", "tokens": [400, 370, 337, 341, 751, 797, 11, 321, 434, 3365, 300, 291, 2464, 257, 5480, 18865, 3480, 264, 912], "temperature": 0.0, "avg_logprob": -0.12386940337799407, "compression_ratio": 1.625531914893617, "no_speech_prob": 0.0003297921794001013}, {"id": 34, "seek": 21500, "start": 215.0, "end": 223.16, "text": " instructions as last time about what's in this lecture. And someone that asks in the questions,", "tokens": [9415, 382, 1036, 565, 466, 437, 311, 294, 341, 7991, 13, 400, 1580, 300, 8962, 294, 264, 1651, 11], "temperature": 0.0, "avg_logprob": -0.19273862150526538, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.0002346801629755646}, {"id": 35, "seek": 21500, "start": 223.16, "end": 229.64, "text": " well, what about last Thursdays? The answer that is no. So the distinction here is we're only doing", "tokens": [731, 11, 437, 466, 1036, 10383, 82, 30, 440, 1867, 300, 307, 572, 13, 407, 264, 16844, 510, 307, 321, 434, 787, 884], "temperature": 0.0, "avg_logprob": -0.19273862150526538, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.0002346801629755646}, {"id": 36, "seek": 21500, "start": 229.64, "end": 238.04, "text": " the reaction paragraphs for outside guest speakers. And although it was great to have on Trian Bosleau", "tokens": [264, 5480, 48910, 337, 2380, 8341, 9518, 13, 400, 4878, 309, 390, 869, 281, 362, 322, 314, 5501, 22264, 306, 1459], "temperature": 0.0, "avg_logprob": -0.19273862150526538, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.0002346801629755646}, {"id": 37, "seek": 21500, "start": 238.04, "end": 244.2, "text": " for last Thursdays lecture, he's a postdoc at Stanford. So we don't count him as an outside guest", "tokens": [337, 1036, 10383, 82, 7991, 11, 415, 311, 257, 2183, 39966, 412, 20374, 13, 407, 321, 500, 380, 1207, 796, 382, 364, 2380, 8341], "temperature": 0.0, "avg_logprob": -0.19273862150526538, "compression_ratio": 1.6708860759493671, "no_speech_prob": 0.0002346801629755646}, {"id": 38, "seek": 24420, "start": 244.2, "end": 250.44, "text": " speaker. And so nothing needs to be done for that one. So there are three classes for which you", "tokens": [8145, 13, 400, 370, 1825, 2203, 281, 312, 1096, 337, 300, 472, 13, 407, 456, 366, 1045, 5359, 337, 597, 291], "temperature": 0.0, "avg_logprob": -0.18345864613850912, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.00027437275275588036}, {"id": 39, "seek": 24420, "start": 250.44, "end": 258.59999999999997, "text": " need to do it. So there was the one before from Duncishan, Colin Rafelle, which is Thursday,", "tokens": [643, 281, 360, 309, 13, 407, 456, 390, 264, 472, 949, 490, 11959, 66, 742, 282, 11, 29253, 7591, 2106, 2447, 11, 597, 307, 10383, 11], "temperature": 0.0, "avg_logprob": -0.18345864613850912, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.00027437275275588036}, {"id": 40, "seek": 24420, "start": 258.59999999999997, "end": 262.76, "text": " and then towards the end of the course, there's Yulitz Vettkov.", "tokens": [293, 550, 3030, 264, 917, 295, 264, 1164, 11, 456, 311, 398, 425, 6862, 691, 3093, 33516, 13], "temperature": 0.0, "avg_logprob": -0.18345864613850912, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.00027437275275588036}, {"id": 41, "seek": 24420, "start": 265.64, "end": 272.12, "text": " Okay, so this is the plan today. So in the first part of it, I'm actually going to spend a bit of time", "tokens": [1033, 11, 370, 341, 307, 264, 1393, 965, 13, 407, 294, 264, 700, 644, 295, 309, 11, 286, 478, 767, 516, 281, 3496, 257, 857, 295, 565], "temperature": 0.0, "avg_logprob": -0.18345864613850912, "compression_ratio": 1.5367965367965368, "no_speech_prob": 0.00027437275275588036}, {"id": 42, "seek": 27212, "start": 272.12, "end": 277.08, "text": " talking about what co-reference is, what different kinds of reference and language are.", "tokens": [1417, 466, 437, 598, 12, 265, 5158, 307, 11, 437, 819, 3685, 295, 6408, 293, 2856, 366, 13], "temperature": 0.0, "avg_logprob": -0.07771511607699924, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00014141059364192188}, {"id": 43, "seek": 27212, "start": 278.04, "end": 282.92, "text": " And then I'm going to move on and talk about some of the kind of methods that people have used", "tokens": [400, 550, 286, 478, 516, 281, 1286, 322, 293, 751, 466, 512, 295, 264, 733, 295, 7150, 300, 561, 362, 1143], "temperature": 0.0, "avg_logprob": -0.07771511607699924, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00014141059364192188}, {"id": 44, "seek": 27212, "start": 282.92, "end": 292.28000000000003, "text": " to solving co-reference resolution. Now there's one bug in our course design, which was a lot of years", "tokens": [281, 12606, 598, 12, 265, 5158, 8669, 13, 823, 456, 311, 472, 7426, 294, 527, 1164, 1715, 11, 597, 390, 257, 688, 295, 924], "temperature": 0.0, "avg_logprob": -0.07771511607699924, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00014141059364192188}, {"id": 45, "seek": 27212, "start": 292.92, "end": 298.52, "text": " we've had a whole lecture on doing convolutional neural nets for language applications. And", "tokens": [321, 600, 632, 257, 1379, 7991, 322, 884, 45216, 304, 18161, 36170, 337, 2856, 5821, 13, 400], "temperature": 0.0, "avg_logprob": -0.07771511607699924, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.00014141059364192188}, {"id": 46, "seek": 29852, "start": 298.52, "end": 309.0, "text": " that slight bug appeared the other day when Duncish talked about the VDF model because she sort of", "tokens": [300, 4036, 7426, 8516, 264, 661, 786, 562, 11959, 66, 742, 2825, 466, 264, 691, 35, 37, 2316, 570, 750, 1333, 295], "temperature": 0.0, "avg_logprob": -0.20274099412855212, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.00010693598596844822}, {"id": 47, "seek": 29852, "start": 309.0, "end": 315.96, "text": " slipped in all this character CNN representation of words. And we haven't actually covered that.", "tokens": [28989, 294, 439, 341, 2517, 24859, 10290, 295, 2283, 13, 400, 321, 2378, 380, 767, 5343, 300, 13], "temperature": 0.0, "avg_logprob": -0.20274099412855212, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.00010693598596844822}, {"id": 48, "seek": 29852, "start": 315.96, "end": 321.47999999999996, "text": " And so that was a slight upsee. I mean, actually for applications in co-reference as well,", "tokens": [400, 370, 300, 390, 257, 4036, 493, 17109, 13, 286, 914, 11, 767, 337, 5821, 294, 598, 12, 265, 5158, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.20274099412855212, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.00010693598596844822}, {"id": 49, "seek": 29852, "start": 321.47999999999996, "end": 327.15999999999997, "text": " people commonly make use of character level confidence. So I wanted to sort of spend a few", "tokens": [561, 12719, 652, 764, 295, 2517, 1496, 6687, 13, 407, 286, 1415, 281, 1333, 295, 3496, 257, 1326], "temperature": 0.0, "avg_logprob": -0.20274099412855212, "compression_ratio": 1.5643153526970954, "no_speech_prob": 0.00010693598596844822}, {"id": 50, "seek": 32716, "start": 327.16, "end": 332.84000000000003, "text": " minutes sort of doing basics of confidence for language. The sort of reality here is that", "tokens": [2077, 1333, 295, 884, 14688, 295, 6687, 337, 2856, 13, 440, 1333, 295, 4103, 510, 307, 300], "temperature": 0.0, "avg_logprob": -0.10562513115700711, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.00022618687944486737}, {"id": 51, "seek": 32716, "start": 333.8, "end": 338.92, "text": " given that there's no exam week this year to give people more time for final projects,", "tokens": [2212, 300, 456, 311, 572, 1139, 1243, 341, 1064, 281, 976, 561, 544, 565, 337, 2572, 4455, 11], "temperature": 0.0, "avg_logprob": -0.10562513115700711, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.00022618687944486737}, {"id": 52, "seek": 32716, "start": 338.92, "end": 344.36, "text": " we sort of shorten the content by a week this year and so you're getting a little bit less of", "tokens": [321, 1333, 295, 39632, 264, 2701, 538, 257, 1243, 341, 1064, 293, 370, 291, 434, 1242, 257, 707, 857, 1570, 295], "temperature": 0.0, "avg_logprob": -0.10562513115700711, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.00022618687944486737}, {"id": 53, "seek": 32716, "start": 344.36, "end": 352.12, "text": " that content. Then going on from there, say some stuff about a state of the art neural co-reference", "tokens": [300, 2701, 13, 1396, 516, 322, 490, 456, 11, 584, 512, 1507, 466, 257, 1785, 295, 264, 1523, 18161, 598, 12, 265, 5158], "temperature": 0.0, "avg_logprob": -0.10562513115700711, "compression_ratio": 1.6894977168949772, "no_speech_prob": 0.00022618687944486737}, {"id": 54, "seek": 35212, "start": 352.12, "end": 358.12, "text": " system and write at the end talk about how co-reference evaluated and what some of the results are.", "tokens": [1185, 293, 2464, 412, 264, 917, 751, 466, 577, 598, 12, 265, 5158, 25509, 293, 437, 512, 295, 264, 3542, 366, 13], "temperature": 0.0, "avg_logprob": -0.11689964775900238, "compression_ratio": 1.7219730941704037, "no_speech_prob": 2.967707951029297e-05}, {"id": 55, "seek": 35212, "start": 358.84000000000003, "end": 365.32, "text": " Yeah. So first of all, what is this co-reference resolution term that I've been talking about a lot?", "tokens": [865, 13, 407, 700, 295, 439, 11, 437, 307, 341, 598, 12, 265, 5158, 8669, 1433, 300, 286, 600, 668, 1417, 466, 257, 688, 30], "temperature": 0.0, "avg_logprob": -0.11689964775900238, "compression_ratio": 1.7219730941704037, "no_speech_prob": 2.967707951029297e-05}, {"id": 56, "seek": 35212, "start": 365.32, "end": 375.16, "text": " So co-reference resolution is meaning to find all the mentions and a piece of text that refer", "tokens": [407, 598, 12, 265, 5158, 8669, 307, 3620, 281, 915, 439, 264, 23844, 293, 257, 2522, 295, 2487, 300, 2864], "temperature": 0.0, "avg_logprob": -0.11689964775900238, "compression_ratio": 1.7219730941704037, "no_speech_prob": 2.967707951029297e-05}, {"id": 57, "seek": 35212, "start": 375.16, "end": 379.96, "text": " to the same entity. And sorry, that's a typo. It should be in the world, not in the word.", "tokens": [281, 264, 912, 13977, 13, 400, 2597, 11, 300, 311, 257, 2125, 78, 13, 467, 820, 312, 294, 264, 1002, 11, 406, 294, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.11689964775900238, "compression_ratio": 1.7219730941704037, "no_speech_prob": 2.967707951029297e-05}, {"id": 58, "seek": 37996, "start": 379.96, "end": 387.96, "text": " So let's make this concrete. So here's part of a short story by Struz Hirao called the star.", "tokens": [407, 718, 311, 652, 341, 9859, 13, 407, 510, 311, 644, 295, 257, 2099, 1657, 538, 745, 894, 89, 2421, 424, 78, 1219, 264, 3543, 13], "temperature": 0.0, "avg_logprob": -0.2232347376206342, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0001986868737731129}, {"id": 59, "seek": 37996, "start": 387.96, "end": 396.52, "text": " Now I have to make a confession here because this is an NLP class, not a literature class. I", "tokens": [823, 286, 362, 281, 652, 257, 29154, 510, 570, 341, 307, 364, 426, 45196, 1508, 11, 406, 257, 10394, 1508, 13, 286], "temperature": 0.0, "avg_logprob": -0.2232347376206342, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0001986868737731129}, {"id": 60, "seek": 37996, "start": 396.52, "end": 403.64, "text": " crudely made some cuts to the story to be able to have relevant parts appear on my slide in", "tokens": [941, 532, 736, 1027, 512, 9992, 281, 264, 1657, 281, 312, 1075, 281, 362, 7340, 3166, 4204, 322, 452, 4137, 294], "temperature": 0.0, "avg_logprob": -0.2232347376206342, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0001986868737731129}, {"id": 61, "seek": 37996, "start": 403.64, "end": 409.56, "text": " a decent size font for illustrating co-reference. So it's not quite the full original text, but", "tokens": [257, 8681, 2744, 10703, 337, 8490, 8754, 598, 12, 265, 5158, 13, 407, 309, 311, 406, 1596, 264, 1577, 3380, 2487, 11, 457], "temperature": 0.0, "avg_logprob": -0.2232347376206342, "compression_ratio": 1.5349794238683128, "no_speech_prob": 0.0001986868737731129}, {"id": 62, "seek": 40956, "start": 409.56, "end": 418.6, "text": " it basically is a piece of this story. So what we're doing in co-reference resolution is we're", "tokens": [309, 1936, 307, 257, 2522, 295, 341, 1657, 13, 407, 437, 321, 434, 884, 294, 598, 12, 265, 5158, 8669, 307, 321, 434], "temperature": 0.0, "avg_logprob": -0.21857132911682128, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.0002562855079304427}, {"id": 63, "seek": 40956, "start": 418.6, "end": 426.44, "text": " working out what people are mentioned. So here's a mention of a person, Vanajah, and here's a", "tokens": [1364, 484, 437, 561, 366, 2835, 13, 407, 510, 311, 257, 2152, 295, 257, 954, 11, 8979, 1805, 545, 11, 293, 510, 311, 257], "temperature": 0.0, "avg_logprob": -0.21857132911682128, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.0002562855079304427}, {"id": 64, "seek": 40956, "start": 426.44, "end": 433.56, "text": " mention of another person, Akila. And well, mentions don't have to be people. So the local park that's", "tokens": [2152, 295, 1071, 954, 11, 9629, 7371, 13, 400, 731, 11, 23844, 500, 380, 362, 281, 312, 561, 13, 407, 264, 2654, 3884, 300, 311], "temperature": 0.0, "avg_logprob": -0.21857132911682128, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.0002562855079304427}, {"id": 65, "seek": 43356, "start": 433.56, "end": 443.96, "text": " also a mention. And then here's Akila again and Akila's son. And then there's Pradhwal. Then there's", "tokens": [611, 257, 2152, 13, 400, 550, 510, 311, 9629, 7371, 797, 293, 9629, 7371, 311, 1872, 13, 400, 550, 456, 311, 2114, 345, 71, 86, 304, 13, 1396, 456, 311], "temperature": 0.0, "avg_logprob": -0.19585624234429722, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.00017147573817055672}, {"id": 66, "seek": 43356, "start": 444.84, "end": 454.92, "text": " another son here and then her son and Akash. And they both went to the same school. And then", "tokens": [1071, 1872, 510, 293, 550, 720, 1872, 293, 9629, 1299, 13, 400, 436, 1293, 1437, 281, 264, 912, 1395, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.19585624234429722, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.00017147573817055672}, {"id": 67, "seek": 45492, "start": 454.92, "end": 467.0, "text": " there's a preschool play. And there's Pradhwal again. And then there's a naughty child, Lord", "tokens": [456, 311, 257, 39809, 862, 13, 400, 456, 311, 2114, 345, 71, 86, 304, 797, 13, 400, 550, 456, 311, 257, 32154, 1440, 11, 3257], "temperature": 0.0, "avg_logprob": -0.16277502760102477, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.00010552423191256821}, {"id": 68, "seek": 45492, "start": 467.0, "end": 475.24, "text": " Krishna. And there's some that are a bit complicated. Like the lead role is that a mention. It's sort", "tokens": [27153, 13, 400, 456, 311, 512, 300, 366, 257, 857, 6179, 13, 1743, 264, 1477, 3090, 307, 300, 257, 2152, 13, 467, 311, 1333], "temperature": 0.0, "avg_logprob": -0.16277502760102477, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.00010552423191256821}, {"id": 69, "seek": 45492, "start": 475.24, "end": 483.96000000000004, "text": " of more of a functional specification of something in the play. There's Akash and it's a tree.", "tokens": [295, 544, 295, 257, 11745, 31256, 295, 746, 294, 264, 862, 13, 821, 311, 9629, 1299, 293, 309, 311, 257, 4230, 13], "temperature": 0.0, "avg_logprob": -0.16277502760102477, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.00010552423191256821}, {"id": 70, "seek": 48396, "start": 483.96, "end": 489.79999999999995, "text": " I won't go through the whole thing yet. But I mean, in general, there are noun phrases that are", "tokens": [286, 1582, 380, 352, 807, 264, 1379, 551, 1939, 13, 583, 286, 914, 11, 294, 2674, 11, 456, 366, 23307, 20312, 300, 366], "temperature": 0.0, "avg_logprob": -0.09223170476417018, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0002633697586134076}, {"id": 71, "seek": 48396, "start": 489.79999999999995, "end": 497.96, "text": " mentioning things in the world. And so then what we want to do for co-reference resolution is", "tokens": [18315, 721, 294, 264, 1002, 13, 400, 370, 550, 437, 321, 528, 281, 360, 337, 598, 12, 265, 5158, 8669, 307], "temperature": 0.0, "avg_logprob": -0.09223170476417018, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0002633697586134076}, {"id": 72, "seek": 48396, "start": 497.96, "end": 505.4, "text": " work out which of these mentions are talking about the same real world entity. So if we start off,", "tokens": [589, 484, 597, 295, 613, 23844, 366, 1417, 466, 264, 912, 957, 1002, 13977, 13, 407, 498, 321, 722, 766, 11], "temperature": 0.0, "avg_logprob": -0.09223170476417018, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0002633697586134076}, {"id": 73, "seek": 50540, "start": 505.4, "end": 520.12, "text": " so there's Banadja. And so Banadja is the same person as her there. And then we could read through.", "tokens": [370, 456, 311, 13850, 345, 2938, 13, 400, 370, 13850, 345, 2938, 307, 264, 912, 954, 382, 720, 456, 13, 400, 550, 321, 727, 1401, 807, 13], "temperature": 0.0, "avg_logprob": -0.27665449443616363, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.00020545779261738062}, {"id": 74, "seek": 50540, "start": 521.64, "end": 533.16, "text": " She resigned herself. So that's both Banadja. She bought him a brown T-shirt and brown trousers.", "tokens": [1240, 41180, 7530, 13, 407, 300, 311, 1293, 13850, 345, 2938, 13, 1240, 4243, 796, 257, 6292, 314, 12, 15313, 293, 6292, 41463, 13], "temperature": 0.0, "avg_logprob": -0.27665449443616363, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.00020545779261738062}, {"id": 75, "seek": 53316, "start": 533.16, "end": 544.6, "text": " And then she made a large cut out tree. She attached. So all of that's about Banadja.", "tokens": [400, 550, 750, 1027, 257, 2416, 1723, 484, 4230, 13, 1240, 8570, 13, 407, 439, 295, 300, 311, 466, 13850, 345, 2938, 13], "temperature": 0.0, "avg_logprob": -0.21679564884730748, "compression_ratio": 1.393162393162393, "no_speech_prob": 0.00041506948764435947}, {"id": 76, "seek": 53316, "start": 545.56, "end": 554.52, "text": " But then we can have another person. So here's a killer. And here's a killer.", "tokens": [583, 550, 321, 393, 362, 1071, 954, 13, 407, 510, 311, 257, 13364, 13, 400, 510, 311, 257, 13364, 13], "temperature": 0.0, "avg_logprob": -0.21679564884730748, "compression_ratio": 1.393162393162393, "no_speech_prob": 0.00041506948764435947}, {"id": 77, "seek": 55452, "start": 554.52, "end": 567.56, "text": " Maybe those are the only mentions of a killer. So then we can go on from there.", "tokens": [2704, 729, 366, 264, 787, 23844, 295, 257, 13364, 13, 407, 550, 321, 393, 352, 322, 490, 456, 13], "temperature": 0.0, "avg_logprob": -0.3252501386277219, "compression_ratio": 1.2777777777777777, "no_speech_prob": 0.00010429129179101437}, {"id": 78, "seek": 55452, "start": 569.3199999999999, "end": 576.12, "text": " Okay. And so then there's Pradjwal. But note the Pradjwal.", "tokens": [1033, 13, 400, 370, 550, 456, 311, 2114, 345, 73, 86, 304, 13, 583, 3637, 264, 2114, 345, 73, 86, 304, 13], "temperature": 0.0, "avg_logprob": -0.3252501386277219, "compression_ratio": 1.2777777777777777, "no_speech_prob": 0.00010429129179101437}, {"id": 79, "seek": 57612, "start": 576.12, "end": 589.32, "text": " Note that Pradjwal is also a killer's son. So really a killer's son is also Pradjwal.", "tokens": [11633, 300, 2114, 345, 73, 86, 304, 307, 611, 257, 13364, 311, 1872, 13, 407, 534, 257, 13364, 311, 1872, 307, 611, 2114, 345, 73, 86, 304, 13], "temperature": 0.0, "avg_logprob": -0.14658658592789262, "compression_ratio": 1.4365079365079365, "no_speech_prob": 2.9729413654422387e-05}, {"id": 80, "seek": 57612, "start": 589.32, "end": 597.64, "text": " And so an interesting thing here is that you can get nested syntactic structure so that we have", "tokens": [400, 370, 364, 1880, 551, 510, 307, 300, 291, 393, 483, 15646, 292, 23980, 19892, 3877, 370, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.14658658592789262, "compression_ratio": 1.4365079365079365, "no_speech_prob": 2.9729413654422387e-05}, {"id": 81, "seek": 59764, "start": 597.64, "end": 608.76, "text": " the sort of noun phrases. So that just overall we have sort of this noun phrase, a killer's son", "tokens": [264, 1333, 295, 23307, 20312, 13, 407, 300, 445, 4787, 321, 362, 1333, 295, 341, 23307, 9535, 11, 257, 13364, 311, 1872], "temperature": 0.0, "avg_logprob": -0.19697297977495798, "compression_ratio": 1.7065868263473054, "no_speech_prob": 2.7047402909374796e-05}, {"id": 82, "seek": 59764, "start": 608.76, "end": 617.4, "text": " Pradjwal, which consists of two noun phrases in an opposition. He is Pradjwal. And then for the", "tokens": [2114, 345, 73, 86, 304, 11, 597, 14689, 295, 732, 23307, 20312, 294, 364, 13504, 13, 634, 307, 2114, 345, 73, 86, 304, 13, 400, 550, 337, 264], "temperature": 0.0, "avg_logprob": -0.19697297977495798, "compression_ratio": 1.7065868263473054, "no_speech_prob": 2.7047402909374796e-05}, {"id": 83, "seek": 59764, "start": 617.4, "end": 624.28, "text": " noun phrase, a killer's son, it sort of breaks down to itself having an extra possessive noun", "tokens": [23307, 9535, 11, 257, 13364, 311, 1872, 11, 309, 1333, 295, 9857, 760, 281, 2564, 1419, 364, 2857, 17490, 488, 23307], "temperature": 0.0, "avg_logprob": -0.19697297977495798, "compression_ratio": 1.7065868263473054, "no_speech_prob": 2.7047402909374796e-05}, {"id": 84, "seek": 62428, "start": 624.28, "end": 632.8399999999999, "text": " phrase in it. And then a noun so that you have a killer's and then this is son. So that you have", "tokens": [9535, 294, 309, 13, 400, 550, 257, 23307, 370, 300, 291, 362, 257, 13364, 311, 293, 550, 341, 307, 1872, 13, 407, 300, 291, 362], "temperature": 0.0, "avg_logprob": -0.09535566303465101, "compression_ratio": 1.7592592592592593, "no_speech_prob": 9.444161696592346e-05}, {"id": 85, "seek": 62428, "start": 632.8399999999999, "end": 641.64, "text": " these multiple noun phrases. And so that you can then be sort of having different parts of this", "tokens": [613, 3866, 23307, 20312, 13, 400, 370, 300, 291, 393, 550, 312, 1333, 295, 1419, 819, 3166, 295, 341], "temperature": 0.0, "avg_logprob": -0.09535566303465101, "compression_ratio": 1.7592592592592593, "no_speech_prob": 9.444161696592346e-05}, {"id": 86, "seek": 62428, "start": 643.0, "end": 649.72, "text": " be one person in the co-reference. But this noun phrase here referring to a different person", "tokens": [312, 472, 954, 294, 264, 598, 12, 265, 5158, 13, 583, 341, 23307, 9535, 510, 13761, 281, 257, 819, 954], "temperature": 0.0, "avg_logprob": -0.09535566303465101, "compression_ratio": 1.7592592592592593, "no_speech_prob": 9.444161696592346e-05}, {"id": 87, "seek": 64972, "start": 649.72, "end": 663.88, "text": " in the co-reference. Okay, so back to Pradjwal. All right, so well there's some easy other", "tokens": [294, 264, 598, 12, 265, 5158, 13, 1033, 11, 370, 646, 281, 2114, 345, 73, 86, 304, 13, 1057, 558, 11, 370, 731, 456, 311, 512, 1858, 661], "temperature": 0.0, "avg_logprob": -0.14629264104933964, "compression_ratio": 1.406015037593985, "no_speech_prob": 1.7756325178197585e-05}, {"id": 88, "seek": 64972, "start": 663.88, "end": 675.5600000000001, "text": " Pradjwals, right? So there's Pradjwal here. And then you've got some more complicated things. So", "tokens": [2114, 345, 73, 86, 1124, 11, 558, 30, 407, 456, 311, 2114, 345, 73, 86, 304, 510, 13, 400, 550, 291, 600, 658, 512, 544, 6179, 721, 13, 407], "temperature": 0.0, "avg_logprob": -0.14629264104933964, "compression_ratio": 1.406015037593985, "no_speech_prob": 1.7756325178197585e-05}, {"id": 89, "seek": 67556, "start": 675.56, "end": 685.0799999999999, "text": " one of the complicated cases here is that we have they went to the same school. So that they there", "tokens": [472, 295, 264, 6179, 3331, 510, 307, 300, 321, 362, 436, 1437, 281, 264, 912, 1395, 13, 407, 300, 436, 456], "temperature": 0.0, "avg_logprob": -0.09413336772544711, "compression_ratio": 1.4696969696969697, "no_speech_prob": 1.7274303900194354e-05}, {"id": 90, "seek": 67556, "start": 685.9599999999999, "end": 699.16, "text": " is what gets referred to as split antecedents. Because the they refers to both Pradjwal and the", "tokens": [307, 437, 2170, 10839, 281, 382, 7472, 23411, 1232, 791, 13, 1436, 264, 436, 14942, 281, 1293, 2114, 345, 73, 86, 304, 293, 264], "temperature": 0.0, "avg_logprob": -0.09413336772544711, "compression_ratio": 1.4696969696969697, "no_speech_prob": 1.7274303900194354e-05}, {"id": 91, "seek": 69916, "start": 699.16, "end": 707.4, "text": " Kash. And that's an interesting phenomenon that and so I could try and show that somehow I could", "tokens": [591, 1299, 13, 400, 300, 311, 364, 1880, 14029, 300, 293, 370, 286, 727, 853, 293, 855, 300, 6063, 286, 727], "temperature": 0.0, "avg_logprob": -0.2076719479683118, "compression_ratio": 1.5978260869565217, "no_speech_prob": 3.520047539495863e-05}, {"id": 92, "seek": 69916, "start": 707.4, "end": 716.8399999999999, "text": " put some splashes in or something. And if I get a different color, a Kash, we have a Kash and her son.", "tokens": [829, 512, 4732, 12808, 294, 420, 746, 13, 400, 498, 286, 483, 257, 819, 2017, 11, 257, 591, 1299, 11, 321, 362, 257, 591, 1299, 293, 720, 1872, 13], "temperature": 0.0, "avg_logprob": -0.2076719479683118, "compression_ratio": 1.5978260869565217, "no_speech_prob": 3.520047539495863e-05}, {"id": 93, "seek": 69916, "start": 716.8399999999999, "end": 724.6, "text": " And then this one sort of both of them at once. Right, so human languages have this phenomenon", "tokens": [400, 550, 341, 472, 1333, 295, 1293, 295, 552, 412, 1564, 13, 1779, 11, 370, 1952, 8650, 362, 341, 14029], "temperature": 0.0, "avg_logprob": -0.2076719479683118, "compression_ratio": 1.5978260869565217, "no_speech_prob": 3.520047539495863e-05}, {"id": 94, "seek": 72460, "start": 724.6, "end": 732.2, "text": " of split antecedents. But you know, one of the things that you should notice when we start talking", "tokens": [295, 7472, 23411, 1232, 791, 13, 583, 291, 458, 11, 472, 295, 264, 721, 300, 291, 820, 3449, 562, 321, 722, 1417], "temperature": 0.0, "avg_logprob": -0.11124475919283354, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.00011026598076568916}, {"id": 95, "seek": 72460, "start": 732.2, "end": 739.16, "text": " about algorithms that people use for doing co-reference resolution is that they make some", "tokens": [466, 14642, 300, 561, 764, 337, 884, 598, 12, 265, 5158, 8669, 307, 300, 436, 652, 512], "temperature": 0.0, "avg_logprob": -0.11124475919283354, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.00011026598076568916}, {"id": 96, "seek": 72460, "start": 739.16, "end": 747.0, "text": " simplified assumptions as they how they go about treating the problem. And one of the simplifications", "tokens": [26335, 17695, 382, 436, 577, 436, 352, 466, 15083, 264, 1154, 13, 400, 472, 295, 264, 6883, 7833], "temperature": 0.0, "avg_logprob": -0.11124475919283354, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.00011026598076568916}, {"id": 97, "seek": 74700, "start": 747.0, "end": 756.52, "text": " that most algorithms make is for any noun phrase like this pronoun they that's trying to work out", "tokens": [300, 881, 14642, 652, 307, 337, 604, 23307, 9535, 411, 341, 14144, 436, 300, 311, 1382, 281, 589, 484], "temperature": 0.0, "avg_logprob": -0.14109132707733468, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.0001231701608048752}, {"id": 98, "seek": 74700, "start": 756.52, "end": 763.96, "text": " what is a co-reference with. And the answer is one thing. And so actually most NLP algorithms", "tokens": [437, 307, 257, 598, 12, 265, 5158, 365, 13, 400, 264, 1867, 307, 472, 551, 13, 400, 370, 767, 881, 426, 45196, 14642], "temperature": 0.0, "avg_logprob": -0.14109132707733468, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.0001231701608048752}, {"id": 99, "seek": 74700, "start": 763.96, "end": 770.2, "text": " for co-reference resolution just cannot get split and antecedents write. Any time it occurs in", "tokens": [337, 598, 12, 265, 5158, 8669, 445, 2644, 483, 7472, 293, 23411, 1232, 791, 2464, 13, 2639, 565, 309, 11843, 294], "temperature": 0.0, "avg_logprob": -0.14109132707733468, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.0001231701608048752}, {"id": 100, "seek": 74700, "start": 770.2, "end": 776.12, "text": " the text they guess something and they always get it wrong. So that's a sort of a bit of a sad state", "tokens": [264, 2487, 436, 2041, 746, 293, 436, 1009, 483, 309, 2085, 13, 407, 300, 311, 257, 1333, 295, 257, 857, 295, 257, 4227, 1785], "temperature": 0.0, "avg_logprob": -0.14109132707733468, "compression_ratio": 1.668103448275862, "no_speech_prob": 0.0001231701608048752}, {"id": 101, "seek": 77612, "start": 776.12, "end": 784.12, "text": " of affairs, but that's the truth of how it is. Okay, so then going ahead we have Akash here.", "tokens": [295, 17478, 11, 457, 300, 311, 264, 3494, 295, 577, 309, 307, 13, 1033, 11, 370, 550, 516, 2286, 321, 362, 9629, 1299, 510, 13], "temperature": 0.0, "avg_logprob": -0.1467123398414025, "compression_ratio": 1.3636363636363635, "no_speech_prob": 7.706544420216233e-05}, {"id": 102, "seek": 77612, "start": 785.48, "end": 793.64, "text": " And then we have another tricky one. So moving on from there, we then have this a tree.", "tokens": [400, 550, 321, 362, 1071, 12414, 472, 13, 407, 2684, 322, 490, 456, 11, 321, 550, 362, 341, 257, 4230, 13], "temperature": 0.0, "avg_logprob": -0.1467123398414025, "compression_ratio": 1.3636363636363635, "no_speech_prob": 7.706544420216233e-05}, {"id": 103, "seek": 79364, "start": 793.64, "end": 808.68, "text": " So well, in this context of this story Akash is going to be the tree. So you could feel that it was", "tokens": [407, 731, 11, 294, 341, 4319, 295, 341, 1657, 9629, 1299, 307, 516, 281, 312, 264, 4230, 13, 407, 291, 727, 841, 300, 309, 390], "temperature": 0.0, "avg_logprob": -0.11220435742978696, "compression_ratio": 1.4621212121212122, "no_speech_prob": 4.674348747357726e-05}, {"id": 104, "seek": 79364, "start": 808.68, "end": 817.88, "text": " okay to say, well this tree is also Akash. You could also feel that that's a little bit weird", "tokens": [1392, 281, 584, 11, 731, 341, 4230, 307, 611, 9629, 1299, 13, 509, 727, 611, 841, 300, 300, 311, 257, 707, 857, 3657], "temperature": 0.0, "avg_logprob": -0.11220435742978696, "compression_ratio": 1.4621212121212122, "no_speech_prob": 4.674348747357726e-05}, {"id": 105, "seek": 81788, "start": 817.88, "end": 827.4, "text": " and not want to do that. And I mean actually different people's co-reference datasets differ in", "tokens": [293, 406, 528, 281, 360, 300, 13, 400, 286, 914, 767, 819, 561, 311, 598, 12, 265, 5158, 42856, 743, 294], "temperature": 0.0, "avg_logprob": -0.13162426326585852, "compression_ratio": 1.663716814159292, "no_speech_prob": 6.377485260600224e-05}, {"id": 106, "seek": 81788, "start": 827.4, "end": 834.36, "text": " this. So really that you know that we're predicating identity relationship here between Akash", "tokens": [341, 13, 407, 534, 300, 291, 458, 300, 321, 434, 3852, 30541, 6575, 2480, 510, 1296, 9629, 1299], "temperature": 0.0, "avg_logprob": -0.13162426326585852, "compression_ratio": 1.663716814159292, "no_speech_prob": 6.377485260600224e-05}, {"id": 107, "seek": 81788, "start": 834.36, "end": 839.96, "text": " and the property of being a tree. So do we regard the tree as the same as Akash or not? And people", "tokens": [293, 264, 4707, 295, 885, 257, 4230, 13, 407, 360, 321, 3843, 264, 4230, 382, 264, 912, 382, 9629, 1299, 420, 406, 30, 400, 561], "temperature": 0.0, "avg_logprob": -0.13162426326585852, "compression_ratio": 1.663716814159292, "no_speech_prob": 6.377485260600224e-05}, {"id": 108, "seek": 83996, "start": 839.96, "end": 849.24, "text": " make different decisions there. Okay, but then going ahead we have here's Akash and she bought him.", "tokens": [652, 819, 5327, 456, 13, 1033, 11, 457, 550, 516, 2286, 321, 362, 510, 311, 9629, 1299, 293, 750, 4243, 796, 13], "temperature": 0.0, "avg_logprob": -0.10266841085333574, "compression_ratio": 1.496124031007752, "no_speech_prob": 0.00011353506852174178}, {"id": 109, "seek": 83996, "start": 849.24, "end": 862.9200000000001, "text": " So that's Akash. And then we have Akash here. And so then we go on. Okay, so then if we don't", "tokens": [407, 300, 311, 9629, 1299, 13, 400, 550, 321, 362, 9629, 1299, 510, 13, 400, 370, 550, 321, 352, 322, 13, 1033, 11, 370, 550, 498, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.10266841085333574, "compression_ratio": 1.496124031007752, "no_speech_prob": 0.00011353506852174178}, {"id": 110, "seek": 86292, "start": 862.92, "end": 877.0, "text": " regard the tree as the same as Akash, we have a tree here. But then note that the next place over here", "tokens": [3843, 264, 4230, 382, 264, 912, 382, 9629, 1299, 11, 321, 362, 257, 4230, 510, 13, 583, 550, 3637, 300, 264, 958, 1081, 670, 510], "temperature": 0.0, "avg_logprob": -0.10484582073283645, "compression_ratio": 1.5615384615384615, "no_speech_prob": 3.186906178598292e-05}, {"id": 111, "seek": 86292, "start": 877.0, "end": 887.16, "text": " where we have a mention of a tree, the best tree. But that's sort of really a functional description", "tokens": [689, 321, 362, 257, 2152, 295, 257, 4230, 11, 264, 1151, 4230, 13, 583, 300, 311, 1333, 295, 534, 257, 11745, 3855], "temperature": 0.0, "avg_logprob": -0.10484582073283645, "compression_ratio": 1.5615384615384615, "no_speech_prob": 3.186906178598292e-05}, {"id": 112, "seek": 88716, "start": 887.16, "end": 894.6, "text": " of you know of possible trees making someone the best tree. It's not really referential to a tree.", "tokens": [295, 291, 458, 295, 1944, 5852, 1455, 1580, 264, 1151, 4230, 13, 467, 311, 406, 534, 2864, 2549, 281, 257, 4230, 13], "temperature": 0.0, "avg_logprob": -0.12652446605541087, "compression_ratio": 1.6, "no_speech_prob": 0.00019258374231867492}, {"id": 113, "seek": 88716, "start": 896.12, "end": 905.64, "text": " And so it seems like that's not really co-referent. But if we go on, there's definitely more mention", "tokens": [400, 370, 309, 2544, 411, 300, 311, 406, 534, 598, 12, 265, 612, 317, 13, 583, 498, 321, 352, 322, 11, 456, 311, 2138, 544, 2152], "temperature": 0.0, "avg_logprob": -0.12652446605541087, "compression_ratio": 1.6, "no_speech_prob": 0.00019258374231867492}, {"id": 114, "seek": 88716, "start": 905.64, "end": 914.6, "text": " of a tree. So when she she has made the tree truly the nicest tree or well I'm not sure. Is that", "tokens": [295, 257, 4230, 13, 407, 562, 750, 750, 575, 1027, 264, 4230, 4908, 264, 45516, 4230, 420, 731, 286, 478, 406, 988, 13, 1119, 300], "temperature": 0.0, "avg_logprob": -0.12652446605541087, "compression_ratio": 1.6, "no_speech_prob": 0.00019258374231867492}, {"id": 115, "seek": 91460, "start": 914.6, "end": 920.76, "text": " one co-referent? It is definitely referring to our tree. And maybe this one again is a sort of a", "tokens": [472, 598, 12, 265, 612, 317, 30, 467, 307, 2138, 13761, 281, 527, 4230, 13, 400, 1310, 341, 472, 797, 307, 257, 1333, 295, 257], "temperature": 0.0, "avg_logprob": -0.1390062305662367, "compression_ratio": 1.7048192771084338, "no_speech_prob": 8.631512173451483e-05}, {"id": 116, "seek": 91460, "start": 920.76, "end": 931.48, "text": " functional description that isn't referring to the tree. Okay, there's definitely. And so", "tokens": [11745, 3855, 300, 1943, 380, 13761, 281, 264, 4230, 13, 1033, 11, 456, 311, 2138, 13, 400, 370], "temperature": 0.0, "avg_logprob": -0.1390062305662367, "compression_ratio": 1.7048192771084338, "no_speech_prob": 8.631512173451483e-05}, {"id": 117, "seek": 91460, "start": 934.2, "end": 940.6800000000001, "text": " maybe this one though where it's a tree is referring to the tree. But I hope to have illustrated", "tokens": [1310, 341, 472, 1673, 689, 309, 311, 257, 4230, 307, 13761, 281, 264, 4230, 13, 583, 286, 1454, 281, 362, 33875], "temperature": 0.0, "avg_logprob": -0.1390062305662367, "compression_ratio": 1.7048192771084338, "no_speech_prob": 8.631512173451483e-05}, {"id": 118, "seek": 94068, "start": 940.68, "end": 948.8399999999999, "text": " from this is you know most of the time when we do co-reference in NLP, we just make it look sort of", "tokens": [490, 341, 307, 291, 458, 881, 295, 264, 565, 562, 321, 360, 598, 12, 265, 5158, 294, 426, 45196, 11, 321, 445, 652, 309, 574, 1333, 295], "temperature": 0.0, "avg_logprob": -0.10647375155717899, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.0001931993174366653}, {"id": 119, "seek": 94068, "start": 950.52, "end": 959.64, "text": " like the conceptual phenomenon is you know kind of obvious that there's a mention of Sarah", "tokens": [411, 264, 24106, 14029, 307, 291, 458, 733, 295, 6322, 300, 456, 311, 257, 2152, 295, 9519], "temperature": 0.0, "avg_logprob": -0.10647375155717899, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.0001931993174366653}, {"id": 120, "seek": 94068, "start": 959.64, "end": 967.0799999999999, "text": " and then it says she and you say oh they're co-referent. This is easy. But if you actually start", "tokens": [293, 550, 309, 1619, 750, 293, 291, 584, 1954, 436, 434, 598, 12, 265, 612, 317, 13, 639, 307, 1858, 13, 583, 498, 291, 767, 722], "temperature": 0.0, "avg_logprob": -0.10647375155717899, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.0001931993174366653}, {"id": 121, "seek": 96708, "start": 967.08, "end": 974.0400000000001, "text": " looking at real text especially when you're looking at something like this that is a piece of literature,", "tokens": [1237, 412, 957, 2487, 2318, 562, 291, 434, 1237, 412, 746, 411, 341, 300, 307, 257, 2522, 295, 10394, 11], "temperature": 0.0, "avg_logprob": -0.13686728203433685, "compression_ratio": 1.7990654205607477, "no_speech_prob": 6.006878174957819e-05}, {"id": 122, "seek": 96708, "start": 974.0400000000001, "end": 980.6800000000001, "text": " the kind of phenomena you get for co-reference and overlapping reference and it varies", "tokens": [264, 733, 295, 22004, 291, 483, 337, 598, 12, 265, 5158, 293, 33535, 6408, 293, 309, 21716], "temperature": 0.0, "avg_logprob": -0.13686728203433685, "compression_ratio": 1.7990654205607477, "no_speech_prob": 6.006878174957819e-05}, {"id": 123, "seek": 96708, "start": 980.6800000000001, "end": 987.48, "text": " out the phenomena that I'll talk about you know they actually get pretty complex and it's not", "tokens": [484, 264, 22004, 300, 286, 603, 751, 466, 291, 458, 436, 767, 483, 1238, 3997, 293, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.13686728203433685, "compression_ratio": 1.7990654205607477, "no_speech_prob": 6.006878174957819e-05}, {"id": 124, "seek": 96708, "start": 987.48, "end": 992.2, "text": " you know there are a lot of hard cases that you actually have to think about as to what things you", "tokens": [291, 458, 456, 366, 257, 688, 295, 1152, 3331, 300, 291, 767, 362, 281, 519, 466, 382, 281, 437, 721, 291], "temperature": 0.0, "avg_logprob": -0.13686728203433685, "compression_ratio": 1.7990654205607477, "no_speech_prob": 6.006878174957819e-05}, {"id": 125, "seek": 99220, "start": 992.2, "end": 1002.2, "text": " think about as co-referent or not. Okay, but basically we do want to be able to do something with", "tokens": [519, 466, 382, 598, 12, 265, 612, 317, 420, 406, 13, 1033, 11, 457, 1936, 321, 360, 528, 281, 312, 1075, 281, 360, 746, 365], "temperature": 0.0, "avg_logprob": -0.09224053488837348, "compression_ratio": 1.658008658008658, "no_speech_prob": 1.274706664844416e-05}, {"id": 126, "seek": 99220, "start": 1002.2, "end": 1007.96, "text": " co-reference because it's useful for a lot of things that we'd like to do in natural language", "tokens": [598, 12, 265, 5158, 570, 309, 311, 4420, 337, 257, 688, 295, 721, 300, 321, 1116, 411, 281, 360, 294, 3303, 2856], "temperature": 0.0, "avg_logprob": -0.09224053488837348, "compression_ratio": 1.658008658008658, "no_speech_prob": 1.274706664844416e-05}, {"id": 127, "seek": 99220, "start": 1007.96, "end": 1013.96, "text": " processing. So for one task that we've already talked about question answering but equally for", "tokens": [9007, 13, 407, 337, 472, 5633, 300, 321, 600, 1217, 2825, 466, 1168, 13430, 457, 12309, 337], "temperature": 0.0, "avg_logprob": -0.09224053488837348, "compression_ratio": 1.658008658008658, "no_speech_prob": 1.274706664844416e-05}, {"id": 128, "seek": 99220, "start": 1013.96, "end": 1020.44, "text": " other tasks such as summarization information extraction, if you're doing something like reading", "tokens": [661, 9608, 1270, 382, 14611, 2144, 1589, 30197, 11, 498, 291, 434, 884, 746, 411, 3760], "temperature": 0.0, "avg_logprob": -0.09224053488837348, "compression_ratio": 1.658008658008658, "no_speech_prob": 1.274706664844416e-05}, {"id": 129, "seek": 102044, "start": 1020.44, "end": 1028.76, "text": " through a piece of text and you've got a sentence like he was born in 1961. You really want to know", "tokens": [807, 257, 2522, 295, 2487, 293, 291, 600, 658, 257, 8174, 411, 415, 390, 4232, 294, 41720, 13, 509, 534, 528, 281, 458], "temperature": 0.0, "avg_logprob": -0.08752298355102539, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.00011914588685613126}, {"id": 130, "seek": 102044, "start": 1028.76, "end": 1036.8400000000001, "text": " who he refers to to know if this is a good answer to the question of you know when was Barack", "tokens": [567, 415, 14942, 281, 281, 458, 498, 341, 307, 257, 665, 1867, 281, 264, 1168, 295, 291, 458, 562, 390, 31705], "temperature": 0.0, "avg_logprob": -0.08752298355102539, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.00011914588685613126}, {"id": 131, "seek": 102044, "start": 1036.8400000000001, "end": 1045.48, "text": " Obama born or something like that. It turns out also that it's useful in machine translation.", "tokens": [9560, 4232, 420, 746, 411, 300, 13, 467, 4523, 484, 611, 300, 309, 311, 4420, 294, 3479, 12853, 13], "temperature": 0.0, "avg_logprob": -0.08752298355102539, "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.00011914588685613126}, {"id": 132, "seek": 104548, "start": 1045.48, "end": 1056.04, "text": " So in most languages pronouns have features for gender and number and in quite a lot of languages", "tokens": [407, 294, 881, 8650, 35883, 362, 4122, 337, 7898, 293, 1230, 293, 294, 1596, 257, 688, 295, 8650], "temperature": 0.0, "avg_logprob": -0.08933979814702814, "compression_ratio": 1.4961832061068703, "no_speech_prob": 2.073072209896054e-05}, {"id": 133, "seek": 104548, "start": 1057.16, "end": 1065.64, "text": " nouns and adjectives also show features of gender, number and case. And so when you're translating", "tokens": [48184, 293, 29378, 1539, 611, 855, 4122, 295, 7898, 11, 1230, 293, 1389, 13, 400, 370, 562, 291, 434, 35030], "temperature": 0.0, "avg_logprob": -0.08933979814702814, "compression_ratio": 1.4961832061068703, "no_speech_prob": 2.073072209896054e-05}, {"id": 134, "seek": 106564, "start": 1065.64, "end": 1076.6000000000001, "text": " a sentence you want to be aware of these features and what is co-referent as what to be able to", "tokens": [257, 8174, 291, 528, 281, 312, 3650, 295, 613, 4122, 293, 437, 307, 598, 12, 265, 612, 317, 382, 437, 281, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.08485818254774895, "compression_ratio": 1.7134146341463414, "no_speech_prob": 4.043735680170357e-05}, {"id": 135, "seek": 106564, "start": 1076.6000000000001, "end": 1085.0, "text": " get the translations correct. So you know if you want to be able to work out a translation", "tokens": [483, 264, 37578, 3006, 13, 407, 291, 458, 498, 291, 528, 281, 312, 1075, 281, 589, 484, 257, 12853], "temperature": 0.0, "avg_logprob": -0.08485818254774895, "compression_ratio": 1.7134146341463414, "no_speech_prob": 4.043735680170357e-05}, {"id": 136, "seek": 106564, "start": 1085.0, "end": 1091.16, "text": " and know whether it's saying Alicia likes Juan because he's smart or Alicia likes Juan because", "tokens": [293, 458, 1968, 309, 311, 1566, 38153, 5902, 17064, 570, 415, 311, 4069, 420, 38153, 5902, 17064, 570], "temperature": 0.0, "avg_logprob": -0.08485818254774895, "compression_ratio": 1.7134146341463414, "no_speech_prob": 4.043735680170357e-05}, {"id": 137, "seek": 109116, "start": 1091.16, "end": 1097.48, "text": " she's smart then you have to be sensitive to co-reference relationships to be able to choose", "tokens": [750, 311, 4069, 550, 291, 362, 281, 312, 9477, 281, 598, 12, 265, 5158, 6159, 281, 312, 1075, 281, 2826], "temperature": 0.0, "avg_logprob": -0.10994766719305693, "compression_ratio": 1.603448275862069, "no_speech_prob": 4.183032433502376e-05}, {"id": 138, "seek": 109116, "start": 1097.48, "end": 1107.4, "text": " the right translation. For people who build dialogue systems dialogue systems also have issues", "tokens": [264, 558, 12853, 13, 1171, 561, 567, 1322, 10221, 3652, 10221, 3652, 611, 362, 2663], "temperature": 0.0, "avg_logprob": -0.10994766719305693, "compression_ratio": 1.603448275862069, "no_speech_prob": 4.183032433502376e-05}, {"id": 139, "seek": 109116, "start": 1107.4, "end": 1115.96, "text": " of co-reference a lot at the time. So you know if it sort of book tickets to see James Bond", "tokens": [295, 598, 12, 265, 5158, 257, 688, 412, 264, 565, 13, 407, 291, 458, 498, 309, 1333, 295, 1446, 12628, 281, 536, 5678, 23604], "temperature": 0.0, "avg_logprob": -0.10994766719305693, "compression_ratio": 1.603448275862069, "no_speech_prob": 4.183032433502376e-05}, {"id": 140, "seek": 111596, "start": 1115.96, "end": 1121.56, "text": " and the system applies spectra is playing near you at two and three today. Well there's actually", "tokens": [293, 264, 1185, 13165, 6177, 424, 307, 2433, 2651, 291, 412, 732, 293, 1045, 965, 13, 1042, 456, 311, 767], "temperature": 0.0, "avg_logprob": -0.15025560317500944, "compression_ratio": 1.6755555555555555, "no_speech_prob": 4.126505518797785e-05}, {"id": 141, "seek": 111596, "start": 1121.56, "end": 1127.24, "text": " co-reference relation. Oh sorry there's a reference relation between spectra and James Bond", "tokens": [598, 12, 265, 5158, 9721, 13, 876, 2597, 456, 311, 257, 6408, 9721, 1296, 6177, 424, 293, 5678, 23604], "temperature": 0.0, "avg_logprob": -0.15025560317500944, "compression_ratio": 1.6755555555555555, "no_speech_prob": 4.126505518797785e-05}, {"id": 142, "seek": 111596, "start": 1127.24, "end": 1132.8400000000001, "text": " because spectra is a James Bond film. I'll come back to that one in a minute. But then it's", "tokens": [570, 6177, 424, 307, 257, 5678, 23604, 2007, 13, 286, 603, 808, 646, 281, 300, 472, 294, 257, 3456, 13, 583, 550, 309, 311], "temperature": 0.0, "avg_logprob": -0.15025560317500944, "compression_ratio": 1.6755555555555555, "no_speech_prob": 4.126505518797785e-05}, {"id": 143, "seek": 111596, "start": 1132.8400000000001, "end": 1139.72, "text": " how many tickets would you like two tickets for the showing at three? That three is not just the", "tokens": [577, 867, 12628, 576, 291, 411, 732, 12628, 337, 264, 4099, 412, 1045, 30, 663, 1045, 307, 406, 445, 264], "temperature": 0.0, "avg_logprob": -0.15025560317500944, "compression_ratio": 1.6755555555555555, "no_speech_prob": 4.126505518797785e-05}, {"id": 144, "seek": 113972, "start": 1139.72, "end": 1148.68, "text": " number three. That three is then a co-reference relationship back to the 3PM showing that was", "tokens": [1230, 1045, 13, 663, 1045, 307, 550, 257, 598, 12, 265, 5158, 2480, 646, 281, 264, 805, 18819, 4099, 300, 390], "temperature": 0.0, "avg_logprob": -0.08624608080152055, "compression_ratio": 1.6704545454545454, "no_speech_prob": 5.720640911022201e-05}, {"id": 145, "seek": 113972, "start": 1148.68, "end": 1156.1200000000001, "text": " mentioned by the agents in the dialogue system. So again to understand these we need to be understanding", "tokens": [2835, 538, 264, 12554, 294, 264, 10221, 1185, 13, 407, 797, 281, 1223, 613, 321, 643, 281, 312, 3701], "temperature": 0.0, "avg_logprob": -0.08624608080152055, "compression_ratio": 1.6704545454545454, "no_speech_prob": 5.720640911022201e-05}, {"id": 146, "seek": 113972, "start": 1156.1200000000001, "end": 1165.08, "text": " the co-reference relationships. So how now can you go about doing co-reference? So the standard", "tokens": [264, 598, 12, 265, 5158, 6159, 13, 407, 577, 586, 393, 291, 352, 466, 884, 598, 12, 265, 5158, 30, 407, 264, 3832], "temperature": 0.0, "avg_logprob": -0.08624608080152055, "compression_ratio": 1.6704545454545454, "no_speech_prob": 5.720640911022201e-05}, {"id": 147, "seek": 116508, "start": 1165.08, "end": 1172.9199999999998, "text": " traditional answer which I'll present first is co-reference is done in two steps. On the first", "tokens": [5164, 1867, 597, 286, 603, 1974, 700, 307, 598, 12, 265, 5158, 307, 1096, 294, 732, 4439, 13, 1282, 264, 700], "temperature": 0.0, "avg_logprob": -0.12427321076393127, "compression_ratio": 1.6406926406926408, "no_speech_prob": 3.99962009396404e-05}, {"id": 148, "seek": 116508, "start": 1172.9199999999998, "end": 1180.6799999999998, "text": " step what we do is detect mentions in a piece of text and that's actually a pretty easy problem.", "tokens": [1823, 437, 321, 360, 307, 5531, 23844, 294, 257, 2522, 295, 2487, 293, 300, 311, 767, 257, 1238, 1858, 1154, 13], "temperature": 0.0, "avg_logprob": -0.12427321076393127, "compression_ratio": 1.6406926406926408, "no_speech_prob": 3.99962009396404e-05}, {"id": 149, "seek": 116508, "start": 1180.6799999999998, "end": 1188.4399999999998, "text": " And then in the second step we work out how to cluster the mentions. So as in my example from", "tokens": [400, 550, 294, 264, 1150, 1823, 321, 589, 484, 577, 281, 13630, 264, 23844, 13, 407, 382, 294, 452, 1365, 490], "temperature": 0.0, "avg_logprob": -0.12427321076393127, "compression_ratio": 1.6406926406926408, "no_speech_prob": 3.99962009396404e-05}, {"id": 150, "seek": 116508, "start": 1188.4399999999998, "end": 1194.36, "text": " the Shruti Rau text basically what you're doing with co-reference is you're building up these", "tokens": [264, 1160, 24316, 72, 497, 1459, 2487, 1936, 437, 291, 434, 884, 365, 598, 12, 265, 5158, 307, 291, 434, 2390, 493, 613], "temperature": 0.0, "avg_logprob": -0.12427321076393127, "compression_ratio": 1.6406926406926408, "no_speech_prob": 3.99962009396404e-05}, {"id": 151, "seek": 119436, "start": 1194.36, "end": 1203.6399999999999, "text": " clusters sets of mentions that refer to the same entity in the world. So if we explore a little", "tokens": [23313, 6352, 295, 23844, 300, 2864, 281, 264, 912, 13977, 294, 264, 1002, 13, 407, 498, 321, 6839, 257, 707], "temperature": 0.0, "avg_logprob": -0.09599007421465063, "compression_ratio": 1.6179775280898876, "no_speech_prob": 2.4637205569888465e-05}, {"id": 152, "seek": 119436, "start": 1203.6399999999999, "end": 1211.08, "text": " how we could do that as a two step solution the first part was detecting the mentions. And so pretty", "tokens": [577, 321, 727, 360, 300, 382, 257, 732, 1823, 3827, 264, 700, 644, 390, 40237, 264, 23844, 13, 400, 370, 1238], "temperature": 0.0, "avg_logprob": -0.09599007421465063, "compression_ratio": 1.6179775280898876, "no_speech_prob": 2.4637205569888465e-05}, {"id": 153, "seek": 119436, "start": 1211.08, "end": 1220.84, "text": " much there are three kinds of things, different kinds of noun phrases that can be mentions.", "tokens": [709, 456, 366, 1045, 3685, 295, 721, 11, 819, 3685, 295, 23307, 20312, 300, 393, 312, 23844, 13], "temperature": 0.0, "avg_logprob": -0.09599007421465063, "compression_ratio": 1.6179775280898876, "no_speech_prob": 2.4637205569888465e-05}, {"id": 154, "seek": 122084, "start": 1220.84, "end": 1227.48, "text": " There are pronouns like I, you're itchy hymn and also some demonstrative pronouns like this and", "tokens": [821, 366, 35883, 411, 286, 11, 291, 434, 309, 28629, 2477, 40459, 293, 611, 512, 5516, 30457, 35883, 411, 341, 293], "temperature": 0.0, "avg_logprob": -0.1828738273458278, "compression_ratio": 1.7217391304347827, "no_speech_prob": 6.088852751418017e-05}, {"id": 155, "seek": 122084, "start": 1227.48, "end": 1234.28, "text": " that and things like that. There are explicitly name things so things like Paris, Joe Biden, Nike", "tokens": [300, 293, 721, 411, 300, 13, 821, 366, 20803, 1315, 721, 370, 721, 411, 8380, 11, 6807, 9877, 11, 30397], "temperature": 0.0, "avg_logprob": -0.1828738273458278, "compression_ratio": 1.7217391304347827, "no_speech_prob": 6.088852751418017e-05}, {"id": 156, "seek": 122084, "start": 1235.3999999999999, "end": 1242.84, "text": " and then there are plain noun phrases that describe things. So a dog, the big fluffy cat stuck in the", "tokens": [293, 550, 456, 366, 11121, 23307, 20312, 300, 6786, 721, 13, 407, 257, 3000, 11, 264, 955, 22778, 3857, 5541, 294, 264], "temperature": 0.0, "avg_logprob": -0.1828738273458278, "compression_ratio": 1.7217391304347827, "no_speech_prob": 6.088852751418017e-05}, {"id": 157, "seek": 122084, "start": 1242.84, "end": 1250.36, "text": " tree. And so all of these things that we'd like to identify as mentions. And all the straightforward", "tokens": [4230, 13, 400, 370, 439, 295, 613, 721, 300, 321, 1116, 411, 281, 5876, 382, 23844, 13, 400, 439, 264, 15325], "temperature": 0.0, "avg_logprob": -0.1828738273458278, "compression_ratio": 1.7217391304347827, "no_speech_prob": 6.088852751418017e-05}, {"id": 158, "seek": 125036, "start": 1250.36, "end": 1258.52, "text": " way to identify these mentions is to use natural language processing tools several of which we've", "tokens": [636, 281, 5876, 613, 23844, 307, 281, 764, 3303, 2856, 9007, 3873, 2940, 295, 597, 321, 600], "temperature": 0.0, "avg_logprob": -0.10417571708337585, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.80680711613968e-05}, {"id": 159, "seek": 125036, "start": 1258.52, "end": 1268.76, "text": " talked about already. So to work out pronouns we can use what's called a part of speech tagger.", "tokens": [2825, 466, 1217, 13, 407, 281, 589, 484, 35883, 321, 393, 764, 437, 311, 1219, 257, 644, 295, 6218, 6162, 1321, 13], "temperature": 0.0, "avg_logprob": -0.10417571708337585, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.80680711613968e-05}, {"id": 160, "seek": 125036, "start": 1272.52, "end": 1278.76, "text": " We can use a part of speech tagger which we haven't really explicitly talked about but we used", "tokens": [492, 393, 764, 257, 644, 295, 6218, 6162, 1321, 597, 321, 2378, 380, 534, 20803, 2825, 466, 457, 321, 1143], "temperature": 0.0, "avg_logprob": -0.10417571708337585, "compression_ratio": 1.6363636363636365, "no_speech_prob": 7.80680711613968e-05}, {"id": 161, "seek": 127876, "start": 1278.76, "end": 1285.72, "text": " when you built dependency parsers. So that first of all assigns parts of speech to each word and so", "tokens": [562, 291, 3094, 33621, 21156, 433, 13, 407, 300, 700, 295, 439, 6269, 82, 3166, 295, 6218, 281, 1184, 1349, 293, 370], "temperature": 0.0, "avg_logprob": -0.12147620519002279, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.710756810614839e-05}, {"id": 162, "seek": 127876, "start": 1285.72, "end": 1293.4, "text": " that we can just find the words that are pronouns. For named entities we did talk just a little bit", "tokens": [300, 321, 393, 445, 915, 264, 2283, 300, 366, 35883, 13, 1171, 4926, 16667, 321, 630, 751, 445, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.12147620519002279, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.710756810614839e-05}, {"id": 163, "seek": 127876, "start": 1293.4, "end": 1299.48, "text": " about named entity recognizers as a use of sequence models for neural networks. So we can pick out", "tokens": [466, 4926, 13977, 3068, 22525, 382, 257, 764, 295, 8310, 5245, 337, 18161, 9590, 13, 407, 321, 393, 1888, 484], "temperature": 0.0, "avg_logprob": -0.12147620519002279, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.710756810614839e-05}, {"id": 164, "seek": 127876, "start": 1299.48, "end": 1305.8799999999999, "text": " things like person names and company names. And then for the ones like the big fluffy", "tokens": [721, 411, 954, 5288, 293, 2237, 5288, 13, 400, 550, 337, 264, 2306, 411, 264, 955, 22778], "temperature": 0.0, "avg_logprob": -0.12147620519002279, "compression_ratio": 1.6842105263157894, "no_speech_prob": 8.710756810614839e-05}, {"id": 165, "seek": 130588, "start": 1305.88, "end": 1314.0400000000002, "text": " a big fluffy dog we could then be sort of picking out from syntactic structure noun phrases", "tokens": [257, 955, 22778, 3000, 321, 727, 550, 312, 1333, 295, 8867, 484, 490, 23980, 19892, 3877, 23307, 20312], "temperature": 0.0, "avg_logprob": -0.1156168717604417, "compression_ratio": 1.545945945945946, "no_speech_prob": 3.4787110053002834e-05}, {"id": 166, "seek": 130588, "start": 1314.0400000000002, "end": 1320.92, "text": " and regarding them as descriptions of things. So that we could use all of these tools and those", "tokens": [293, 8595, 552, 382, 24406, 295, 721, 13, 407, 300, 321, 727, 764, 439, 295, 613, 3873, 293, 729], "temperature": 0.0, "avg_logprob": -0.1156168717604417, "compression_ratio": 1.545945945945946, "no_speech_prob": 3.4787110053002834e-05}, {"id": 167, "seek": 130588, "start": 1320.92, "end": 1326.7600000000002, "text": " would give us basically our mentions. It's a little bit more subtle than that because it turns out", "tokens": [576, 976, 505, 1936, 527, 23844, 13, 467, 311, 257, 707, 857, 544, 13743, 813, 300, 570, 309, 4523, 484], "temperature": 0.0, "avg_logprob": -0.1156168717604417, "compression_ratio": 1.545945945945946, "no_speech_prob": 3.4787110053002834e-05}, {"id": 168, "seek": 132676, "start": 1326.76, "end": 1336.36, "text": " there are some noun phrases and things of all of those kinds which don't actually refer so that", "tokens": [456, 366, 512, 23307, 20312, 293, 721, 295, 439, 295, 729, 3685, 597, 500, 380, 767, 2864, 370, 300], "temperature": 0.0, "avg_logprob": -0.09079897945577448, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.0002329250128241256}, {"id": 169, "seek": 132676, "start": 1336.36, "end": 1342.84, "text": " they're not referential in the world. So when you say it is sunny it doesn't really refer. When you", "tokens": [436, 434, 406, 2864, 2549, 294, 264, 1002, 13, 407, 562, 291, 584, 309, 307, 20412, 309, 1177, 380, 534, 2864, 13, 1133, 291], "temperature": 0.0, "avg_logprob": -0.09079897945577448, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.0002329250128241256}, {"id": 170, "seek": 132676, "start": 1342.84, "end": 1350.04, "text": " make universal claims like every student well every student isn't referring to something you can", "tokens": [652, 11455, 9441, 411, 633, 3107, 731, 633, 3107, 1943, 380, 13761, 281, 746, 291, 393], "temperature": 0.0, "avg_logprob": -0.09079897945577448, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.0002329250128241256}, {"id": 171, "seek": 132676, "start": 1350.04, "end": 1355.8799999999999, "text": " point to in the world. And more dramatically when you have no student and making a negative universal", "tokens": [935, 281, 294, 264, 1002, 13, 400, 544, 17548, 562, 291, 362, 572, 3107, 293, 1455, 257, 3671, 11455], "temperature": 0.0, "avg_logprob": -0.09079897945577448, "compression_ratio": 1.7747747747747749, "no_speech_prob": 0.0002329250128241256}, {"id": 172, "seek": 135588, "start": 1355.88, "end": 1363.0800000000002, "text": " claim it's not referential to anything. There are also things that you can describe functionally", "tokens": [3932, 309, 311, 406, 2864, 2549, 281, 1340, 13, 821, 366, 611, 721, 300, 291, 393, 6786, 2445, 379], "temperature": 0.0, "avg_logprob": -0.11176231172349718, "compression_ratio": 1.778301886792453, "no_speech_prob": 7.110981096047908e-05}, {"id": 173, "seek": 135588, "start": 1363.88, "end": 1371.48, "text": " which don't have any clear reference. So if I say the best doughnut in the world that that's", "tokens": [597, 500, 380, 362, 604, 1850, 6408, 13, 407, 498, 286, 584, 264, 1151, 7984, 18316, 294, 264, 1002, 300, 300, 311], "temperature": 0.0, "avg_logprob": -0.11176231172349718, "compression_ratio": 1.778301886792453, "no_speech_prob": 7.110981096047908e-05}, {"id": 174, "seek": 135588, "start": 1371.48, "end": 1377.24, "text": " a functional claim but it doesn't necessarily have reference. Like if I've established", "tokens": [257, 11745, 3932, 457, 309, 1177, 380, 4725, 362, 6408, 13, 1743, 498, 286, 600, 7545], "temperature": 0.0, "avg_logprob": -0.11176231172349718, "compression_ratio": 1.778301886792453, "no_speech_prob": 7.110981096047908e-05}, {"id": 175, "seek": 135588, "start": 1378.2800000000002, "end": 1384.2800000000002, "text": " that I think a particular kind of doughnut is the best doughnut in the world I could then say to you", "tokens": [300, 286, 519, 257, 1729, 733, 295, 7984, 18316, 307, 264, 1151, 7984, 18316, 294, 264, 1002, 286, 727, 550, 584, 281, 291], "temperature": 0.0, "avg_logprob": -0.11176231172349718, "compression_ratio": 1.778301886792453, "no_speech_prob": 7.110981096047908e-05}, {"id": 176, "seek": 138428, "start": 1384.28, "end": 1392.36, "text": " I hate the best doughnut in the world yesterday and you know what I mean it might have reference.", "tokens": [286, 4700, 264, 1151, 7984, 18316, 294, 264, 1002, 5186, 293, 291, 458, 437, 286, 914, 309, 1062, 362, 6408, 13], "temperature": 0.0, "avg_logprob": -0.12185614903767904, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.00013868935639038682}, {"id": 177, "seek": 138428, "start": 1392.36, "end": 1397.3999999999999, "text": " But if I say something like I'm going around to all the doughnut stores trying to find the best", "tokens": [583, 498, 286, 584, 746, 411, 286, 478, 516, 926, 281, 439, 264, 7984, 18316, 9512, 1382, 281, 915, 264, 1151], "temperature": 0.0, "avg_logprob": -0.12185614903767904, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.00013868935639038682}, {"id": 178, "seek": 138428, "start": 1397.3999999999999, "end": 1402.04, "text": " doughnut in the world then it doesn't have any reference yet it's just a sort of a functional", "tokens": [7984, 18316, 294, 264, 1002, 550, 309, 1177, 380, 362, 604, 6408, 1939, 309, 311, 445, 257, 1333, 295, 257, 11745], "temperature": 0.0, "avg_logprob": -0.12185614903767904, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.00013868935639038682}, {"id": 179, "seek": 138428, "start": 1402.04, "end": 1408.12, "text": " description I'm trying to satisfy. You also then have things like quantities, 100 miles", "tokens": [3855, 286, 478, 1382, 281, 19319, 13, 509, 611, 550, 362, 721, 411, 22927, 11, 2319, 6193], "temperature": 0.0, "avg_logprob": -0.12185614903767904, "compression_ratio": 1.6968325791855203, "no_speech_prob": 0.00013868935639038682}, {"id": 180, "seek": 140812, "start": 1408.12, "end": 1414.84, "text": " it's a quantity that's not really something that has any particular reference. You can mark out 100", "tokens": [309, 311, 257, 11275, 300, 311, 406, 534, 746, 300, 575, 604, 1729, 6408, 13, 509, 393, 1491, 484, 2319], "temperature": 0.0, "avg_logprob": -0.15785796606718605, "compression_ratio": 1.5078534031413613, "no_speech_prob": 6.39490972389467e-05}, {"id": 181, "seek": 140812, "start": 1414.84, "end": 1423.7199999999998, "text": " miles or sorts of places. So how do we deal with those things that aren't really mentions?", "tokens": [6193, 420, 7527, 295, 3190, 13, 407, 577, 360, 321, 2028, 365, 729, 721, 300, 3212, 380, 534, 23844, 30], "temperature": 0.0, "avg_logprob": -0.15785796606718605, "compression_ratio": 1.5078534031413613, "no_speech_prob": 6.39490972389467e-05}, {"id": 182, "seek": 140812, "start": 1423.7199999999998, "end": 1431.08, "text": " Well one way is we could train a machine learning classifier to get rid of those curious mentions", "tokens": [1042, 472, 636, 307, 321, 727, 3847, 257, 3479, 2539, 1508, 9902, 281, 483, 3973, 295, 729, 6369, 23844], "temperature": 0.0, "avg_logprob": -0.15785796606718605, "compression_ratio": 1.5078534031413613, "no_speech_prob": 6.39490972389467e-05}, {"id": 183, "seek": 143108, "start": 1431.08, "end": 1439.96, "text": " that actually mostly people don't do that. Most commonly if you're using this kind of pipeline model", "tokens": [300, 767, 5240, 561, 500, 380, 360, 300, 13, 4534, 12719, 498, 291, 434, 1228, 341, 733, 295, 15517, 2316], "temperature": 0.0, "avg_logprob": -0.15645031134287515, "compression_ratio": 1.52, "no_speech_prob": 5.4690492106601596e-05}, {"id": 184, "seek": 143108, "start": 1439.96, "end": 1447.0, "text": " where you use a parser and a named NT recognizer you regard everything as you've found as a candidate", "tokens": [689, 291, 764, 257, 21156, 260, 293, 257, 4926, 43452, 3068, 6545, 291, 3843, 1203, 382, 291, 600, 1352, 382, 257, 11532], "temperature": 0.0, "avg_logprob": -0.15645031134287515, "compression_ratio": 1.52, "no_speech_prob": 5.4690492106601596e-05}, {"id": 185, "seek": 143108, "start": 1447.0, "end": 1453.96, "text": " mention and then you try and run your co-ref system and some of them like those ones hopefully aren't", "tokens": [2152, 293, 550, 291, 853, 293, 1190, 428, 598, 12, 33115, 1185, 293, 512, 295, 552, 411, 729, 2306, 4696, 3212, 380], "temperature": 0.0, "avg_logprob": -0.15645031134287515, "compression_ratio": 1.52, "no_speech_prob": 5.4690492106601596e-05}, {"id": 186, "seek": 145396, "start": 1453.96, "end": 1461.64, "text": " make a referent with anything else and so then you just discard them at the end of the process.", "tokens": [652, 257, 2864, 317, 365, 1340, 1646, 293, 370, 550, 291, 445, 31597, 552, 412, 264, 917, 295, 264, 1399, 13], "temperature": 0.0, "avg_logprob": -0.3992824261005108, "compression_ratio": 1.4545454545454546, "no_speech_prob": 7.942084630485624e-05}, {"id": 187, "seek": 145396, "start": 1461.64, "end": 1465.96, "text": " Secret? Yeah. I've got an interesting question that linguist", "tokens": [7400, 30, 865, 13, 286, 600, 658, 364, 1880, 1168, 300, 21766, 468], "temperature": 0.0, "avg_logprob": -0.3992824261005108, "compression_ratio": 1.4545454545454546, "no_speech_prob": 7.942084630485624e-05}, {"id": 188, "seek": 145396, "start": 1465.96, "end": 1473.0, "text": " experienced on this. A student asks can we say that it is sunny? Has it's referring to the weather?", "tokens": [6751, 322, 341, 13, 316, 3107, 8962, 393, 321, 584, 300, 309, 307, 20412, 30, 8646, 309, 311, 13761, 281, 264, 5503, 30], "temperature": 0.0, "avg_logprob": -0.3992824261005108, "compression_ratio": 1.4545454545454546, "no_speech_prob": 7.942084630485624e-05}, {"id": 189, "seek": 147300, "start": 1473.0, "end": 1486.84, "text": " I think so. That's a fair question. People have actually tried to suggest that when you say it is sunny", "tokens": [286, 519, 370, 13, 663, 311, 257, 3143, 1168, 13, 3432, 362, 767, 3031, 281, 3402, 300, 562, 291, 584, 309, 307, 20412], "temperature": 0.0, "avg_logprob": -0.24613333721550143, "compression_ratio": 1.3986013986013985, "no_speech_prob": 0.00026345925289206207}, {"id": 190, "seek": 147300, "start": 1486.84, "end": 1497.48, "text": " it means the weather is sunny but I guess the majority opinion at least is that isn't plausible.", "tokens": [309, 1355, 264, 5503, 307, 20412, 457, 286, 2041, 264, 6286, 4800, 412, 1935, 307, 300, 1943, 380, 39925, 13], "temperature": 0.0, "avg_logprob": -0.24613333721550143, "compression_ratio": 1.3986013986013985, "no_speech_prob": 0.00026345925289206207}, {"id": 191, "seek": 149748, "start": 1497.48, "end": 1507.64, "text": " I guess many of you aren't native speakers of English but similar phenomena occur in many other", "tokens": [286, 2041, 867, 295, 291, 3212, 380, 8470, 9518, 295, 3669, 457, 2531, 22004, 5160, 294, 867, 661], "temperature": 0.0, "avg_logprob": -0.10965875423315799, "compression_ratio": 1.5978260869565217, "no_speech_prob": 0.0004908204427920282}, {"id": 192, "seek": 149748, "start": 1507.64, "end": 1516.52, "text": " languages. I mean it just intuitively doesn't seem plausible when you say it's sunny or it's raining", "tokens": [8650, 13, 286, 914, 309, 445, 46506, 1177, 380, 1643, 39925, 562, 291, 584, 309, 311, 20412, 420, 309, 311, 18441], "temperature": 0.0, "avg_logprob": -0.10965875423315799, "compression_ratio": 1.5978260869565217, "no_speech_prob": 0.0004908204427920282}, {"id": 193, "seek": 149748, "start": 1516.52, "end": 1524.1200000000001, "text": " today that you're really saying that as a shortcut for the weather is raining today it just seems", "tokens": [965, 300, 291, 434, 534, 1566, 300, 382, 257, 24822, 337, 264, 5503, 307, 18441, 965, 309, 445, 2544], "temperature": 0.0, "avg_logprob": -0.10965875423315799, "compression_ratio": 1.5978260869565217, "no_speech_prob": 0.0004908204427920282}, {"id": 194, "seek": 152412, "start": 1524.12, "end": 1530.28, "text": " like really what the case is is English likes to have something filling the subject position", "tokens": [411, 534, 437, 264, 1389, 307, 307, 3669, 5902, 281, 362, 746, 10623, 264, 3983, 2535], "temperature": 0.0, "avg_logprob": -0.07032516526012886, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00023241832968778908}, {"id": 195, "seek": 152412, "start": 1530.28, "end": 1537.0, "text": " and when there's nothing better to fill the subject position you stick it in there and get", "tokens": [293, 562, 456, 311, 1825, 1101, 281, 2836, 264, 3983, 2535, 291, 2897, 309, 294, 456, 293, 483], "temperature": 0.0, "avg_logprob": -0.07032516526012886, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00023241832968778908}, {"id": 196, "seek": 152412, "start": 1537.0, "end": 1543.3999999999999, "text": " it's raining and so in general it's believed that you get this phenomenon of having these", "tokens": [309, 311, 18441, 293, 370, 294, 2674, 309, 311, 7847, 300, 291, 483, 341, 14029, 295, 1419, 613], "temperature": 0.0, "avg_logprob": -0.07032516526012886, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00023241832968778908}, {"id": 197, "seek": 152412, "start": 1543.3999999999999, "end": 1549.3999999999999, "text": " empty dummy it's that appear in various places. I mean another place in which it seems like you", "tokens": [6707, 35064, 309, 311, 300, 4204, 294, 3683, 3190, 13, 286, 914, 1071, 1081, 294, 597, 309, 2544, 411, 291], "temperature": 0.0, "avg_logprob": -0.07032516526012886, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00023241832968778908}, {"id": 198, "seek": 154940, "start": 1549.4, "end": 1556.68, "text": " clearly get dummy it's is that when you have clauses that are subjects of a verb you can move", "tokens": [4448, 483, 35064, 309, 311, 307, 300, 562, 291, 362, 49072, 300, 366, 13066, 295, 257, 9595, 291, 393, 1286], "temperature": 0.0, "avg_logprob": -0.09399441014165463, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.00018965666822623461}, {"id": 199, "seek": 154940, "start": 1556.68, "end": 1562.3600000000001, "text": " them to the end of the sentence. So if you have a sentence where you put a clause in the subject", "tokens": [552, 281, 264, 917, 295, 264, 8174, 13, 407, 498, 291, 362, 257, 8174, 689, 291, 829, 257, 25925, 294, 264, 3983], "temperature": 0.0, "avg_logprob": -0.09399441014165463, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.00018965666822623461}, {"id": 200, "seek": 154940, "start": 1562.3600000000001, "end": 1570.0400000000002, "text": " position they normally in English sound fairly awkward so it's you have a sentence something like", "tokens": [2535, 436, 5646, 294, 3669, 1626, 6457, 11411, 370, 309, 311, 291, 362, 257, 8174, 746, 411], "temperature": 0.0, "avg_logprob": -0.09399441014165463, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.00018965666822623461}, {"id": 201, "seek": 154940, "start": 1571.4, "end": 1579.3200000000002, "text": " that CS24N is a lot of work is known by all students. People don't normally say that the normal", "tokens": [300, 9460, 7911, 45, 307, 257, 688, 295, 589, 307, 2570, 538, 439, 1731, 13, 3432, 500, 380, 5646, 584, 300, 264, 2710], "temperature": 0.0, "avg_logprob": -0.09399441014165463, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.00018965666822623461}, {"id": 202, "seek": 157932, "start": 1579.32, "end": 1583.8799999999999, "text": " thing to do is to shift the clause to the end of the sentence but when you do that you stick in", "tokens": [551, 281, 360, 307, 281, 5513, 264, 25925, 281, 264, 917, 295, 264, 8174, 457, 562, 291, 360, 300, 291, 2897, 294], "temperature": 0.0, "avg_logprob": -0.09908763885498047, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.00013281320570968091}, {"id": 203, "seek": 157932, "start": 1583.8799999999999, "end": 1591.32, "text": " the dummy it to fill the subject position so you then have it is known by all students that CS224N", "tokens": [264, 35064, 309, 281, 2836, 264, 3983, 2535, 370, 291, 550, 362, 309, 307, 2570, 538, 439, 1731, 300, 9460, 7490, 19, 45], "temperature": 0.0, "avg_logprob": -0.09908763885498047, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.00013281320570968091}, {"id": 204, "seek": 157932, "start": 1591.32, "end": 1598.4399999999998, "text": " is a lot of work. So that's the general feeling that this is a dummy it that doesn't have any reference.", "tokens": [307, 257, 688, 295, 589, 13, 407, 300, 311, 264, 2674, 2633, 300, 341, 307, 257, 35064, 309, 300, 1177, 380, 362, 604, 6408, 13], "temperature": 0.0, "avg_logprob": -0.09908763885498047, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.00013281320570968091}, {"id": 205, "seek": 157932, "start": 1602.36, "end": 1608.84, "text": " Okay there's one more question so if someone says it is sunny and like other things and we ask", "tokens": [1033, 456, 311, 472, 544, 1168, 370, 498, 1580, 1619, 309, 307, 20412, 293, 411, 661, 721, 293, 321, 1029], "temperature": 0.0, "avg_logprob": -0.09908763885498047, "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.00013281320570968091}, {"id": 206, "seek": 160884, "start": 1608.84, "end": 1615.8799999999999, "text": " how is the weather. Okay good point you've got me on that one right so someone says how is the", "tokens": [577, 307, 264, 5503, 13, 1033, 665, 935, 291, 600, 658, 385, 322, 300, 472, 558, 370, 1580, 1619, 577, 307, 264], "temperature": 0.0, "avg_logprob": -0.12889034231913457, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.00024337510694749653}, {"id": 207, "seek": 160884, "start": 1615.8799999999999, "end": 1622.76, "text": " weather and you answer it is sunny it then does seem like the it is in reference to the weather.", "tokens": [5503, 293, 291, 1867, 309, 307, 20412, 309, 550, 775, 1643, 411, 264, 309, 307, 294, 6408, 281, 264, 5503, 13], "temperature": 0.0, "avg_logprob": -0.12889034231913457, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.00024337510694749653}, {"id": 208, "seek": 160884, "start": 1622.76, "end": 1629.6399999999999, "text": " Oh by that well you know I guess this is what our co-reference systems are built trying to do", "tokens": [876, 538, 300, 731, 291, 458, 286, 2041, 341, 307, 437, 527, 598, 12, 265, 5158, 3652, 366, 3094, 1382, 281, 360], "temperature": 0.0, "avg_logprob": -0.12889034231913457, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.00024337510694749653}, {"id": 209, "seek": 160884, "start": 1629.6399999999999, "end": 1635.1599999999999, "text": " in situations like that they're making a decision of co-reference or not and I guess what you'd", "tokens": [294, 6851, 411, 300, 436, 434, 1455, 257, 3537, 295, 598, 12, 265, 5158, 420, 406, 293, 286, 2041, 437, 291, 1116], "temperature": 0.0, "avg_logprob": -0.12889034231913457, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.00024337510694749653}, {"id": 210, "seek": 163516, "start": 1635.16, "end": 1640.68, "text": " want to say in that case is it seems reasonable to regard this one as co-reference to that weather", "tokens": [528, 281, 584, 294, 300, 1389, 307, 309, 2544, 10585, 281, 3843, 341, 472, 382, 598, 12, 265, 5158, 281, 300, 5503], "temperature": 0.0, "avg_logprob": -0.09606682170521129, "compression_ratio": 1.737991266375546, "no_speech_prob": 6.0837068303953856e-05}, {"id": 211, "seek": 163516, "start": 1640.68, "end": 1648.6000000000001, "text": " that did appear before it. I mean but that also indicates another reason to think that in the normal", "tokens": [300, 630, 4204, 949, 309, 13, 286, 914, 457, 300, 611, 16203, 1071, 1778, 281, 519, 300, 294, 264, 2710], "temperature": 0.0, "avg_logprob": -0.09606682170521129, "compression_ratio": 1.737991266375546, "no_speech_prob": 6.0837068303953856e-05}, {"id": 212, "seek": 163516, "start": 1648.6000000000001, "end": 1654.92, "text": " cases not co-reference right because normally pronouns are only used when their references establish", "tokens": [3331, 406, 598, 12, 265, 5158, 558, 570, 5646, 35883, 366, 787, 1143, 562, 641, 15400, 8327], "temperature": 0.0, "avg_logprob": -0.09606682170521129, "compression_ratio": 1.737991266375546, "no_speech_prob": 6.0837068303953856e-05}, {"id": 213, "seek": 163516, "start": 1654.92, "end": 1663.88, "text": " that you've referred to now like John is answering questions and then you can say he types really", "tokens": [300, 291, 600, 10839, 281, 586, 411, 2619, 307, 13430, 1651, 293, 550, 291, 393, 584, 415, 3467, 534], "temperature": 0.0, "avg_logprob": -0.09606682170521129, "compression_ratio": 1.737991266375546, "no_speech_prob": 6.0837068303953856e-05}, {"id": 214, "seek": 166388, "start": 1663.88, "end": 1670.5200000000002, "text": " quickly and it seemed odd to just sort of start the conversation by he types really quickly because", "tokens": [2661, 293, 309, 6576, 7401, 281, 445, 1333, 295, 722, 264, 3761, 538, 415, 3467, 534, 2661, 570], "temperature": 0.0, "avg_logprob": -0.07188405687846834, "compression_ratio": 1.7560975609756098, "no_speech_prob": 4.81514616694767e-05}, {"id": 215, "seek": 166388, "start": 1670.5200000000002, "end": 1675.4, "text": " it doesn't have any established reference whereas that doesn't seem to be the case it seems like", "tokens": [309, 1177, 380, 362, 604, 7545, 6408, 9735, 300, 1177, 380, 1643, 281, 312, 264, 1389, 309, 2544, 411], "temperature": 0.0, "avg_logprob": -0.07188405687846834, "compression_ratio": 1.7560975609756098, "no_speech_prob": 4.81514616694767e-05}, {"id": 216, "seek": 166388, "start": 1675.4, "end": 1681.8000000000002, "text": " you can just sort of start a conversation by saying it's raining really hard today and that", "tokens": [291, 393, 445, 1333, 295, 722, 257, 3761, 538, 1566, 309, 311, 18441, 534, 1152, 965, 293, 300], "temperature": 0.0, "avg_logprob": -0.07188405687846834, "compression_ratio": 1.7560975609756098, "no_speech_prob": 4.81514616694767e-05}, {"id": 217, "seek": 168180, "start": 1681.8, "end": 1694.28, "text": " doesn't sound odd at all. Okay so I've sort of there presented the traditional picture but you know", "tokens": [1177, 380, 1626, 7401, 412, 439, 13, 1033, 370, 286, 600, 1333, 295, 456, 8212, 264, 5164, 3036, 457, 291, 458], "temperature": 0.0, "avg_logprob": -0.07085302140977648, "compression_ratio": 1.598901098901099, "no_speech_prob": 3.578278483473696e-05}, {"id": 218, "seek": 168180, "start": 1694.28, "end": 1699.72, "text": " this traditional picture doesn't mean something that was done last millennium before you were born", "tokens": [341, 5164, 3036, 1177, 380, 914, 746, 300, 390, 1096, 1036, 21362, 2197, 949, 291, 645, 4232], "temperature": 0.0, "avg_logprob": -0.07085302140977648, "compression_ratio": 1.598901098901099, "no_speech_prob": 3.578278483473696e-05}, {"id": 219, "seek": 168180, "start": 1699.72, "end": 1710.84, "text": " I mean essentially that was the picture until about 2016 that essentially every co-reference", "tokens": [286, 914, 4476, 300, 390, 264, 3036, 1826, 466, 6549, 300, 4476, 633, 598, 12, 265, 5158], "temperature": 0.0, "avg_logprob": -0.07085302140977648, "compression_ratio": 1.598901098901099, "no_speech_prob": 3.578278483473696e-05}, {"id": 220, "seek": 171084, "start": 1710.84, "end": 1717.0, "text": " system that was built used tools like part of speech tag as any of our systems and parsers to", "tokens": [1185, 300, 390, 3094, 1143, 3873, 411, 644, 295, 6218, 6162, 382, 604, 295, 527, 3652, 293, 21156, 433, 281], "temperature": 0.0, "avg_logprob": -0.1640835553407669, "compression_ratio": 1.574585635359116, "no_speech_prob": 5.139639324625023e-05}, {"id": 221, "seek": 171084, "start": 1717.0, "end": 1723.6399999999999, "text": " analyze sentences to identify mentions and to give you features for co-reference resolution and", "tokens": [12477, 16579, 281, 5876, 23844, 293, 281, 976, 291, 4122, 337, 598, 12, 265, 5158, 8669, 293], "temperature": 0.0, "avg_logprob": -0.1640835553407669, "compression_ratio": 1.574585635359116, "no_speech_prob": 5.139639324625023e-05}, {"id": 222, "seek": 171084, "start": 1723.6399999999999, "end": 1732.12, "text": " I'll show a bit more about that later but more recently in our neural systems people have moved", "tokens": [286, 603, 855, 257, 857, 544, 466, 300, 1780, 457, 544, 3938, 294, 527, 18161, 3652, 561, 362, 4259], "temperature": 0.0, "avg_logprob": -0.1640835553407669, "compression_ratio": 1.574585635359116, "no_speech_prob": 5.139639324625023e-05}, {"id": 223, "seek": 173212, "start": 1732.12, "end": 1741.1599999999999, "text": " to avoiding traditional pipeline systems and of doing one shot end to end co-reference resolution", "tokens": [281, 20220, 5164, 15517, 3652, 293, 295, 884, 472, 3347, 917, 281, 917, 598, 12, 265, 5158, 8669], "temperature": 0.0, "avg_logprob": -0.07668779835556493, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.887982762127649e-05}, {"id": 224, "seek": 173212, "start": 1741.1599999999999, "end": 1749.56, "text": " systems so if I skip directly to the second bullet there's a new generation of neural systems where", "tokens": [3652, 370, 498, 286, 10023, 3838, 281, 264, 1150, 11632, 456, 311, 257, 777, 5125, 295, 18161, 3652, 689], "temperature": 0.0, "avg_logprob": -0.07668779835556493, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.887982762127649e-05}, {"id": 225, "seek": 173212, "start": 1749.56, "end": 1756.36, "text": " you just start with your sequence of words and you do the maximally done thing you just say let's", "tokens": [291, 445, 722, 365, 428, 8310, 295, 2283, 293, 291, 360, 264, 5138, 379, 1096, 551, 291, 445, 584, 718, 311], "temperature": 0.0, "avg_logprob": -0.07668779835556493, "compression_ratio": 1.5945945945945945, "no_speech_prob": 1.887982762127649e-05}, {"id": 226, "seek": 175636, "start": 1756.36, "end": 1764.04, "text": " take all spans commonly with some heuristics for efficiency but you know conceptually all subsequences", "tokens": [747, 439, 44086, 12719, 365, 512, 415, 374, 6006, 337, 10493, 457, 291, 458, 3410, 671, 439, 1422, 11834, 2667], "temperature": 0.0, "avg_logprob": -0.09608329027548604, "compression_ratio": 1.5860655737704918, "no_speech_prob": 2.6663987227948382e-05}, {"id": 227, "seek": 175636, "start": 1764.04, "end": 1771.0, "text": " of this sentence they might be mentions let's feed them in to a neural network which will", "tokens": [295, 341, 8174, 436, 1062, 312, 23844, 718, 311, 3154, 552, 294, 281, 257, 18161, 3209, 597, 486], "temperature": 0.0, "avg_logprob": -0.09608329027548604, "compression_ratio": 1.5860655737704918, "no_speech_prob": 2.6663987227948382e-05}, {"id": 228, "seek": 175636, "start": 1771.0, "end": 1777.6399999999999, "text": " simultaneously do mention detection and co-reference resolution end to end in one model and I'll", "tokens": [16561, 360, 2152, 17784, 293, 598, 12, 265, 5158, 8669, 917, 281, 917, 294, 472, 2316, 293, 286, 603], "temperature": 0.0, "avg_logprob": -0.09608329027548604, "compression_ratio": 1.5860655737704918, "no_speech_prob": 2.6663987227948382e-05}, {"id": 229, "seek": 175636, "start": 1777.6399999999999, "end": 1786.12, "text": " give an example of that kind of system later in the lecture. Okay is everything good to there and", "tokens": [976, 364, 1365, 295, 300, 733, 295, 1185, 1780, 294, 264, 7991, 13, 1033, 307, 1203, 665, 281, 456, 293], "temperature": 0.0, "avg_logprob": -0.09608329027548604, "compression_ratio": 1.5860655737704918, "no_speech_prob": 2.6663987227948382e-05}, {"id": 230, "seek": 178612, "start": 1786.12, "end": 1798.6799999999998, "text": " I should go on. Okay so I'm going to get on to how to do co-reference resolution systems but before", "tokens": [286, 820, 352, 322, 13, 1033, 370, 286, 478, 516, 281, 483, 322, 281, 577, 281, 360, 598, 12, 265, 5158, 8669, 3652, 457, 949], "temperature": 0.0, "avg_logprob": -0.12076769789604291, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0001070543221430853}, {"id": 231, "seek": 178612, "start": 1798.6799999999998, "end": 1805.4799999999998, "text": " I do that I do actually want to show a little bit more the linguistics of co-reference because", "tokens": [286, 360, 300, 286, 360, 767, 528, 281, 855, 257, 707, 857, 544, 264, 21766, 6006, 295, 598, 12, 265, 5158, 570], "temperature": 0.0, "avg_logprob": -0.12076769789604291, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0001070543221430853}, {"id": 232, "seek": 178612, "start": 1805.4799999999998, "end": 1812.52, "text": " there's actually a few more interesting things to understand and know here I mean when we say", "tokens": [456, 311, 767, 257, 1326, 544, 1880, 721, 281, 1223, 293, 458, 510, 286, 914, 562, 321, 584], "temperature": 0.0, "avg_logprob": -0.12076769789604291, "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.0001070543221430853}, {"id": 233, "seek": 181252, "start": 1812.52, "end": 1821.8799999999999, "text": " co-reference resolution we really confuse together two linguistic things which are overlapping", "tokens": [598, 12, 265, 5158, 8669, 321, 534, 28584, 1214, 732, 43002, 721, 597, 366, 33535], "temperature": 0.0, "avg_logprob": -0.049886569642184075, "compression_ratio": 1.698224852071006, "no_speech_prob": 7.598255615448579e-05}, {"id": 234, "seek": 181252, "start": 1821.8799999999999, "end": 1827.08, "text": " but different and so it's really actually good to understand the difference between these things", "tokens": [457, 819, 293, 370, 309, 311, 534, 767, 665, 281, 1223, 264, 2649, 1296, 613, 721], "temperature": 0.0, "avg_logprob": -0.049886569642184075, "compression_ratio": 1.698224852071006, "no_speech_prob": 7.598255615448579e-05}, {"id": 235, "seek": 181252, "start": 1827.08, "end": 1834.92, "text": " so there are two things that can happen one is that you can have mentions which are essentially", "tokens": [370, 456, 366, 732, 721, 300, 393, 1051, 472, 307, 300, 291, 393, 362, 23844, 597, 366, 4476], "temperature": 0.0, "avg_logprob": -0.049886569642184075, "compression_ratio": 1.698224852071006, "no_speech_prob": 7.598255615448579e-05}, {"id": 236, "seek": 183492, "start": 1834.92, "end": 1843.72, "text": " standalone but happen to refer to the same entity in the world so if I have a piece of text that", "tokens": [37454, 457, 1051, 281, 2864, 281, 264, 912, 13977, 294, 264, 1002, 370, 498, 286, 362, 257, 2522, 295, 2487, 300], "temperature": 0.0, "avg_logprob": -0.09177232167077443, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.00012104550842195749}, {"id": 237, "seek": 183492, "start": 1843.72, "end": 1852.28, "text": " said Barack Obama traveled yesterday to Nebraska Obama was there to open a new meat processing", "tokens": [848, 31705, 9560, 16147, 5186, 281, 27171, 9560, 390, 456, 281, 1269, 257, 777, 4615, 9007], "temperature": 0.0, "avg_logprob": -0.09177232167077443, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.00012104550842195749}, {"id": 238, "seek": 183492, "start": 1852.28, "end": 1859.0, "text": " plant or something like that I've mentioned with Barack Obama and Obama there are two mentions there", "tokens": [3709, 420, 746, 411, 300, 286, 600, 2835, 365, 31705, 9560, 293, 9560, 456, 366, 732, 23844, 456], "temperature": 0.0, "avg_logprob": -0.09177232167077443, "compression_ratio": 1.6043956043956045, "no_speech_prob": 0.00012104550842195749}, {"id": 239, "seek": 185900, "start": 1859.0, "end": 1865.24, "text": " they refer to the same person in the world they are co-referent so that is true co-reference but there's", "tokens": [436, 2864, 281, 264, 912, 954, 294, 264, 1002, 436, 366, 598, 12, 265, 612, 317, 370, 300, 307, 2074, 598, 12, 265, 5158, 457, 456, 311], "temperature": 0.0, "avg_logprob": -0.12379990372003294, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.184031498269178e-05}, {"id": 240, "seek": 185900, "start": 1865.24, "end": 1871.16, "text": " a different the related linguistic concept called a naffra and a naffra is when you have a", "tokens": [257, 819, 264, 4077, 43002, 3410, 1219, 257, 1667, 602, 424, 293, 257, 1667, 602, 424, 307, 562, 291, 362, 257], "temperature": 0.0, "avg_logprob": -0.12379990372003294, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.184031498269178e-05}, {"id": 241, "seek": 185900, "start": 1871.16, "end": 1879.64, "text": " textual dependence of an anaphora on another term which is the antecedent and in this case the", "tokens": [2487, 901, 31704, 295, 364, 364, 569, 7013, 322, 1071, 1433, 597, 307, 264, 23411, 1232, 317, 293, 294, 341, 1389, 264], "temperature": 0.0, "avg_logprob": -0.12379990372003294, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.184031498269178e-05}, {"id": 242, "seek": 185900, "start": 1879.64, "end": 1887.48, "text": " meaning of the anaphora is determined by the antecedent in a textual context and the canonical case", "tokens": [3620, 295, 264, 364, 569, 7013, 307, 9540, 538, 264, 23411, 1232, 317, 294, 257, 2487, 901, 4319, 293, 264, 46491, 1389], "temperature": 0.0, "avg_logprob": -0.12379990372003294, "compression_ratio": 1.8309859154929577, "no_speech_prob": 1.184031498269178e-05}, {"id": 243, "seek": 188748, "start": 1887.48, "end": 1895.96, "text": " of this is pronouns so when it's Barack Obama said he would sign the bill he is an anaphora it's not a", "tokens": [295, 341, 307, 35883, 370, 562, 309, 311, 31705, 9560, 848, 415, 576, 1465, 264, 2961, 415, 307, 364, 364, 569, 7013, 309, 311, 406, 257], "temperature": 0.0, "avg_logprob": -0.0951258833979217, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.00010703362931963056}, {"id": 244, "seek": 188748, "start": 1895.96, "end": 1901.88, "text": " word that independently we can work out what it's meaning is in the world apart from knowing the", "tokens": [1349, 300, 21761, 321, 393, 589, 484, 437, 309, 311, 3620, 307, 294, 264, 1002, 4936, 490, 5276, 264], "temperature": 0.0, "avg_logprob": -0.0951258833979217, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.00010703362931963056}, {"id": 245, "seek": 188748, "start": 1901.88, "end": 1910.2, "text": " vagus feature that it's referring to something probably male but in the context of this text", "tokens": [13501, 301, 4111, 300, 309, 311, 13761, 281, 746, 1391, 7133, 457, 294, 264, 4319, 295, 341, 2487], "temperature": 0.0, "avg_logprob": -0.0951258833979217, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.00010703362931963056}, {"id": 246, "seek": 191020, "start": 1910.2, "end": 1918.92, "text": " we have that this anaphora is textually dependent on Barack Obama and so then we have an anaphora", "tokens": [321, 362, 300, 341, 364, 569, 7013, 307, 2487, 671, 12334, 322, 31705, 9560, 293, 370, 550, 321, 362, 364, 364, 569, 7013], "temperature": 0.0, "avg_logprob": -0.0890701663109564, "compression_ratio": 1.8428571428571427, "no_speech_prob": 7.1335380198434e-05}, {"id": 247, "seek": 191020, "start": 1918.92, "end": 1925.48, "text": " relationship which sort of means they refer to the same thing in the world and so therefore you", "tokens": [2480, 597, 1333, 295, 1355, 436, 2864, 281, 264, 912, 551, 294, 264, 1002, 293, 370, 4412, 291], "temperature": 0.0, "avg_logprob": -0.0890701663109564, "compression_ratio": 1.8428571428571427, "no_speech_prob": 7.1335380198434e-05}, {"id": 248, "seek": 191020, "start": 1925.48, "end": 1933.64, "text": " can say they're co-referent so the picture we have is like this right so for co-reference we have", "tokens": [393, 584, 436, 434, 598, 12, 265, 612, 317, 370, 264, 3036, 321, 362, 307, 411, 341, 558, 370, 337, 598, 12, 265, 5158, 321, 362], "temperature": 0.0, "avg_logprob": -0.0890701663109564, "compression_ratio": 1.8428571428571427, "no_speech_prob": 7.1335380198434e-05}, {"id": 249, "seek": 191020, "start": 1933.64, "end": 1939.72, "text": " these separate textual mentions which are basically standalone which refer to the same thing in", "tokens": [613, 4994, 2487, 901, 23844, 597, 366, 1936, 37454, 597, 2864, 281, 264, 912, 551, 294], "temperature": 0.0, "avg_logprob": -0.0890701663109564, "compression_ratio": 1.8428571428571427, "no_speech_prob": 7.1335380198434e-05}, {"id": 250, "seek": 193972, "start": 1939.72, "end": 1947.88, "text": " the world whereas in an affra we actually have a textual relationship and you know you essentially", "tokens": [264, 1002, 9735, 294, 364, 257, 602, 424, 321, 767, 362, 257, 2487, 901, 2480, 293, 291, 458, 291, 4476], "temperature": 0.0, "avg_logprob": -0.1290431663171569, "compression_ratio": 1.6193181818181819, "no_speech_prob": 2.835228224284947e-05}, {"id": 251, "seek": 193972, "start": 1947.88, "end": 1955.96, "text": " have to use pronouns like he and she in legitimate ways in which the heerer can reconstruct a", "tokens": [362, 281, 764, 35883, 411, 415, 293, 750, 294, 17956, 2098, 294, 597, 264, 415, 260, 260, 393, 31499, 257], "temperature": 0.0, "avg_logprob": -0.1290431663171569, "compression_ratio": 1.6193181818181819, "no_speech_prob": 2.835228224284947e-05}, {"id": 252, "seek": 193972, "start": 1955.96, "end": 1962.76, "text": " relationship from the text because they can't work out what he refers to if that's not there", "tokens": [2480, 490, 264, 2487, 570, 436, 393, 380, 589, 484, 437, 415, 14942, 281, 498, 300, 311, 406, 456], "temperature": 0.0, "avg_logprob": -0.1290431663171569, "compression_ratio": 1.6193181818181819, "no_speech_prob": 2.835228224284947e-05}, {"id": 253, "seek": 196276, "start": 1962.76, "end": 1973.4, "text": " and so that's a fair bit of the distinction but it's actually a little bit more to realize because", "tokens": [293, 370, 300, 311, 257, 3143, 857, 295, 264, 16844, 457, 309, 311, 767, 257, 707, 857, 544, 281, 4325, 570], "temperature": 0.0, "avg_logprob": -0.07707415989467076, "compression_ratio": 1.672514619883041, "no_speech_prob": 1.8332726540393196e-05}, {"id": 254, "seek": 196276, "start": 1973.4, "end": 1981.08, "text": " there are more complex forms of an affra which aren't co-reference because you have a textual", "tokens": [456, 366, 544, 3997, 6422, 295, 364, 257, 602, 424, 597, 3212, 380, 598, 12, 265, 5158, 570, 291, 362, 257, 2487, 901], "temperature": 0.0, "avg_logprob": -0.07707415989467076, "compression_ratio": 1.672514619883041, "no_speech_prob": 1.8332726540393196e-05}, {"id": 255, "seek": 196276, "start": 1981.08, "end": 1989.08, "text": " dependence but it's not actually one of reference and so this comes back to things like these", "tokens": [31704, 457, 309, 311, 406, 767, 472, 295, 6408, 293, 370, 341, 1487, 646, 281, 721, 411, 613], "temperature": 0.0, "avg_logprob": -0.07707415989467076, "compression_ratio": 1.672514619883041, "no_speech_prob": 1.8332726540393196e-05}, {"id": 256, "seek": 198908, "start": 1989.08, "end": 1997.3999999999999, "text": " quantifying noun phrases that don't have reference so when you have sentences like these ones", "tokens": [4426, 5489, 23307, 20312, 300, 500, 380, 362, 6408, 370, 562, 291, 362, 16579, 411, 613, 2306], "temperature": 0.0, "avg_logprob": -0.08988057242499457, "compression_ratio": 1.8609271523178808, "no_speech_prob": 2.7435042284196243e-05}, {"id": 257, "seek": 198908, "start": 1997.3999999999999, "end": 2006.76, "text": " every dancer twisted her knee well this her here has an anaphora dependency on every dancer", "tokens": [633, 21621, 23057, 720, 9434, 731, 341, 720, 510, 575, 364, 364, 569, 7013, 33621, 322, 633, 21621], "temperature": 0.0, "avg_logprob": -0.08988057242499457, "compression_ratio": 1.8609271523178808, "no_speech_prob": 2.7435042284196243e-05}, {"id": 258, "seek": 198908, "start": 2006.76, "end": 2013.8, "text": " or even more clearly with no dancer twisted her knee the her here has an anaphora dependence on", "tokens": [420, 754, 544, 4448, 365, 572, 21621, 23057, 720, 9434, 264, 720, 510, 575, 364, 364, 569, 7013, 31704, 322], "temperature": 0.0, "avg_logprob": -0.08988057242499457, "compression_ratio": 1.8609271523178808, "no_speech_prob": 2.7435042284196243e-05}, {"id": 259, "seek": 201380, "start": 2013.8, "end": 2022.28, "text": " no dancer but for no dancer twisted her knee no dancer isn't referential it's not referring to", "tokens": [572, 21621, 457, 337, 572, 21621, 23057, 720, 9434, 572, 21621, 1943, 380, 2864, 2549, 309, 311, 406, 13761, 281], "temperature": 0.0, "avg_logprob": -0.1067538791232639, "compression_ratio": 1.8355263157894737, "no_speech_prob": 3.503111656755209e-05}, {"id": 260, "seek": 201380, "start": 2022.28, "end": 2028.28, "text": " anything in the world and so there's no co-reference or relationship because there's no reference", "tokens": [1340, 294, 264, 1002, 293, 370, 456, 311, 572, 598, 12, 265, 5158, 420, 2480, 570, 456, 311, 572, 6408], "temperature": 0.0, "avg_logprob": -0.1067538791232639, "compression_ratio": 1.8355263157894737, "no_speech_prob": 3.503111656755209e-05}, {"id": 261, "seek": 201380, "start": 2028.28, "end": 2034.28, "text": " relationship but there's still an anaphora relationship between these two noun phrases", "tokens": [2480, 457, 456, 311, 920, 364, 364, 569, 7013, 2480, 1296, 613, 732, 23307, 20312], "temperature": 0.0, "avg_logprob": -0.1067538791232639, "compression_ratio": 1.8355263157894737, "no_speech_prob": 3.503111656755209e-05}, {"id": 262, "seek": 203428, "start": 2034.28, "end": 2044.36, "text": " and then you have this other complex case that turns up quite a bit where you can have where the", "tokens": [293, 550, 291, 362, 341, 661, 3997, 1389, 300, 4523, 493, 1596, 257, 857, 689, 291, 393, 362, 689, 264], "temperature": 0.0, "avg_logprob": -0.07062912732362747, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.320186053519137e-05}, {"id": 263, "seek": 203428, "start": 2044.36, "end": 2051.96, "text": " things being talked about do have reference but an anaphora relationship is more subtle than", "tokens": [721, 885, 2825, 466, 360, 362, 6408, 457, 364, 364, 569, 7013, 2480, 307, 544, 13743, 813], "temperature": 0.0, "avg_logprob": -0.07062912732362747, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.320186053519137e-05}, {"id": 264, "seek": 203428, "start": 2051.96, "end": 2060.84, "text": " identity so you commonly get the constructions like this one we went to a concert last night the", "tokens": [6575, 370, 291, 12719, 483, 264, 7690, 626, 411, 341, 472, 321, 1437, 281, 257, 8543, 1036, 1818, 264], "temperature": 0.0, "avg_logprob": -0.07062912732362747, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.320186053519137e-05}, {"id": 265, "seek": 206084, "start": 2060.84, "end": 2068.6000000000004, "text": " tickets were really expensive well the concert and the tickets are two different things they're not", "tokens": [12628, 645, 534, 5124, 731, 264, 8543, 293, 264, 12628, 366, 732, 819, 721, 436, 434, 406], "temperature": 0.0, "avg_logprob": -0.10407020106460109, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.485587800852954e-05}, {"id": 266, "seek": 206084, "start": 2069.32, "end": 2077.48, "text": " co-reference co-referential but in interpreting this sentence what this really means is the tickets", "tokens": [598, 12, 265, 5158, 598, 12, 265, 612, 2549, 457, 294, 37395, 341, 8174, 437, 341, 534, 1355, 307, 264, 12628], "temperature": 0.0, "avg_logprob": -0.10407020106460109, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.485587800852954e-05}, {"id": 267, "seek": 206084, "start": 2077.48, "end": 2085.4, "text": " of tickets to the tickets to the concert right and so there's sort of this hidden not not said", "tokens": [295, 12628, 281, 264, 12628, 281, 264, 8543, 558, 293, 370, 456, 311, 1333, 295, 341, 7633, 406, 406, 848], "temperature": 0.0, "avg_logprob": -0.10407020106460109, "compression_ratio": 1.826086956521739, "no_speech_prob": 6.485587800852954e-05}, {"id": 268, "seek": 208540, "start": 2085.4, "end": 2092.76, "text": " dependence where this is referring back to the concert and so what we say is that these the tickets", "tokens": [31704, 689, 341, 307, 13761, 646, 281, 264, 8543, 293, 370, 437, 321, 584, 307, 300, 613, 264, 12628], "temperature": 0.0, "avg_logprob": -0.11515567037794325, "compression_ratio": 1.7058823529411764, "no_speech_prob": 8.196951966965571e-05}, {"id": 269, "seek": 208540, "start": 2093.96, "end": 2099.88, "text": " does have an anaphora dependence on the concert but they're not co-referential and so that's", "tokens": [775, 362, 364, 364, 569, 7013, 31704, 322, 264, 8543, 457, 436, 434, 406, 598, 12, 265, 612, 2549, 293, 370, 300, 311], "temperature": 0.0, "avg_logprob": -0.11515567037794325, "compression_ratio": 1.7058823529411764, "no_speech_prob": 8.196951966965571e-05}, {"id": 270, "seek": 208540, "start": 2099.88, "end": 2107.2400000000002, "text": " referred to as bridging an aphra and so overall there's the simple case and the common case which", "tokens": [10839, 281, 382, 16362, 3249, 364, 257, 950, 424, 293, 370, 4787, 456, 311, 264, 2199, 1389, 293, 264, 2689, 1389, 597], "temperature": 0.0, "avg_logprob": -0.11515567037794325, "compression_ratio": 1.7058823529411764, "no_speech_prob": 8.196951966965571e-05}, {"id": 271, "seek": 210724, "start": 2107.24, "end": 2116.7599999999998, "text": " is pronominal anaphora where it's both co-reference and anaphora you then have other cases of", "tokens": [307, 7569, 298, 2071, 364, 569, 7013, 689, 309, 311, 1293, 598, 12, 265, 5158, 293, 364, 569, 7013, 291, 550, 362, 661, 3331, 295], "temperature": 0.0, "avg_logprob": -0.06280397375424702, "compression_ratio": 1.898477157360406, "no_speech_prob": 2.1039206330897287e-05}, {"id": 272, "seek": 210724, "start": 2116.7599999999998, "end": 2123.24, "text": " co-reference such as every time you see a mention of the every time the United States has said", "tokens": [598, 12, 265, 5158, 1270, 382, 633, 565, 291, 536, 257, 2152, 295, 264, 633, 565, 264, 2824, 3040, 575, 848], "temperature": 0.0, "avg_logprob": -0.06280397375424702, "compression_ratio": 1.898477157360406, "no_speech_prob": 2.1039206330897287e-05}, {"id": 273, "seek": 210724, "start": 2123.24, "end": 2128.8399999999997, "text": " it's co-referential with every other mention of the United States but those don't have any", "tokens": [309, 311, 598, 12, 265, 612, 2549, 365, 633, 661, 2152, 295, 264, 2824, 3040, 457, 729, 500, 380, 362, 604], "temperature": 0.0, "avg_logprob": -0.06280397375424702, "compression_ratio": 1.898477157360406, "no_speech_prob": 2.1039206330897287e-05}, {"id": 274, "seek": 210724, "start": 2128.8399999999997, "end": 2134.12, "text": " textual dependence on each other and then you have textual dependencies like bridging anaphora", "tokens": [2487, 901, 31704, 322, 1184, 661, 293, 550, 291, 362, 2487, 901, 36606, 411, 16362, 3249, 364, 569, 7013], "temperature": 0.0, "avg_logprob": -0.06280397375424702, "compression_ratio": 1.898477157360406, "no_speech_prob": 2.1039206330897287e-05}, {"id": 275, "seek": 213412, "start": 2134.12, "end": 2141.72, "text": " which aren't co-reference. Phew that's probably about as mm now I was going to say that's probably", "tokens": [597, 3212, 380, 598, 12, 265, 5158, 13, 46679, 300, 311, 1391, 466, 382, 275, 76, 586, 286, 390, 516, 281, 584, 300, 311, 1391], "temperature": 0.0, "avg_logprob": -0.18268143640805598, "compression_ratio": 1.6551724137931034, "no_speech_prob": 3.865422331728041e-05}, {"id": 276, "seek": 213412, "start": 2141.72, "end": 2146.8399999999997, "text": " as as much linguistics as you wanted to hear but actually I have one more point of linguistics", "tokens": [382, 382, 709, 21766, 6006, 382, 291, 1415, 281, 1568, 457, 767, 286, 362, 472, 544, 935, 295, 21766, 6006], "temperature": 0.0, "avg_logprob": -0.18268143640805598, "compression_ratio": 1.6551724137931034, "no_speech_prob": 3.865422331728041e-05}, {"id": 277, "seek": 213412, "start": 2149.16, "end": 2157.7999999999997, "text": " one or two of you but probably not many might have been troubled by the fact that the the term", "tokens": [472, 420, 732, 295, 291, 457, 1391, 406, 867, 1062, 362, 668, 29402, 538, 264, 1186, 300, 264, 264, 1433], "temperature": 0.0, "avg_logprob": -0.18268143640805598, "compression_ratio": 1.6551724137931034, "no_speech_prob": 3.865422331728041e-05}, {"id": 278, "seek": 215780, "start": 2157.8, "end": 2165.7200000000003, "text": " anaphora as a classical term means that you're looking backward for your antecedent", "tokens": [364, 569, 7013, 382, 257, 13735, 1433, 1355, 300, 291, 434, 1237, 23897, 337, 428, 23411, 1232, 317], "temperature": 0.0, "avg_logprob": -0.13852314388050752, "compression_ratio": 2.047244094488189, "no_speech_prob": 2.706688792386558e-05}, {"id": 279, "seek": 215780, "start": 2166.44, "end": 2172.84, "text": " that the anapart of anaphora means that you're looking backward for your antecedent and in", "tokens": [300, 264, 364, 569, 446, 295, 364, 569, 7013, 1355, 300, 291, 434, 1237, 23897, 337, 428, 23411, 1232, 317, 293, 294], "temperature": 0.0, "avg_logprob": -0.13852314388050752, "compression_ratio": 2.047244094488189, "no_speech_prob": 2.706688792386558e-05}, {"id": 280, "seek": 215780, "start": 2173.8, "end": 2181.7200000000003, "text": " sort of classical terminology you have both anaphora and cataphora and it's cataphora", "tokens": [1333, 295, 13735, 27575, 291, 362, 1293, 364, 569, 7013, 293, 3857, 569, 7013, 293, 309, 311, 3857, 569, 7013], "temperature": 0.0, "avg_logprob": -0.13852314388050752, "compression_ratio": 2.047244094488189, "no_speech_prob": 2.706688792386558e-05}, {"id": 281, "seek": 218172, "start": 2181.72, "end": 2187.64, "text": " where you look forward for your antecedent. Cataphora isn't that common but it does occur", "tokens": [689, 291, 574, 2128, 337, 428, 23411, 1232, 317, 13, 9565, 569, 7013, 1943, 380, 300, 2689, 457, 309, 775, 5160], "temperature": 0.0, "avg_logprob": -0.13497395567841583, "compression_ratio": 1.5798319327731092, "no_speech_prob": 7.947383710416034e-05}, {"id": 282, "seek": 218172, "start": 2188.2, "end": 2194.52, "text": " here's a beautiful example of cataphora so this is from Oscar Wilde from the corner of the", "tokens": [510, 311, 257, 2238, 1365, 295, 3857, 569, 7013, 370, 341, 307, 490, 20718, 10904, 68, 490, 264, 4538, 295, 264], "temperature": 0.0, "avg_logprob": -0.13497395567841583, "compression_ratio": 1.5798319327731092, "no_speech_prob": 7.947383710416034e-05}, {"id": 283, "seek": 218172, "start": 2194.52, "end": 2201.72, "text": " divine of Persian saddlebags on which he was lying smoking as was his custom and numerous cigarettes", "tokens": [13678, 295, 30699, 30459, 44047, 322, 597, 415, 390, 8493, 14055, 382, 390, 702, 2375, 293, 12546, 29244], "temperature": 0.0, "avg_logprob": -0.13497395567841583, "compression_ratio": 1.5798319327731092, "no_speech_prob": 7.947383710416034e-05}, {"id": 284, "seek": 218172, "start": 2201.72, "end": 2207.72, "text": " Lord Henry Watten could just catch the gleam of a honey sweet and honey cut of the honey sweet", "tokens": [3257, 11085, 12593, 1147, 727, 445, 3745, 264, 48956, 335, 295, 257, 8330, 3844, 293, 8330, 1723, 295, 264, 8330, 3844], "temperature": 0.0, "avg_logprob": -0.13497395567841583, "compression_ratio": 1.5798319327731092, "no_speech_prob": 7.947383710416034e-05}, {"id": 285, "seek": 220772, "start": 2207.72, "end": 2216.68, "text": " and honey colored blossoms of a labyrinth. Okay so in this example here right the he and then this", "tokens": [293, 8330, 14332, 47789, 295, 257, 287, 46800, 392, 13, 1033, 370, 294, 341, 1365, 510, 558, 264, 415, 293, 550, 341], "temperature": 0.0, "avg_logprob": -0.19096751440139043, "compression_ratio": 1.5, "no_speech_prob": 8.667662768857554e-05}, {"id": 286, "seek": 220772, "start": 2216.68, "end": 2226.2799999999997, "text": " he is are actually referring to Lord Henry Watten and so these are both examples of cataphora", "tokens": [415, 307, 366, 767, 13761, 281, 3257, 11085, 12593, 1147, 293, 370, 613, 366, 1293, 5110, 295, 3857, 569, 7013], "temperature": 0.0, "avg_logprob": -0.19096751440139043, "compression_ratio": 1.5, "no_speech_prob": 8.667662768857554e-05}, {"id": 287, "seek": 222628, "start": 2226.28, "end": 2238.44, "text": " but in in modern linguistics even though most reference to pronouns is backwards we don't", "tokens": [457, 294, 294, 4363, 21766, 6006, 754, 1673, 881, 6408, 281, 35883, 307, 12204, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.1308010369539261, "compression_ratio": 1.569767441860465, "no_speech_prob": 7.009781256783754e-05}, {"id": 288, "seek": 222628, "start": 2239.6400000000003, "end": 2246.6800000000003, "text": " distinguish on in terms of order and so the term anaphora and anaphora is used for", "tokens": [20206, 322, 294, 2115, 295, 1668, 293, 370, 264, 1433, 364, 569, 7013, 293, 364, 569, 7013, 307, 1143, 337], "temperature": 0.0, "avg_logprob": -0.1308010369539261, "compression_ratio": 1.569767441860465, "no_speech_prob": 7.009781256783754e-05}, {"id": 289, "seek": 222628, "start": 2246.6800000000003, "end": 2253.32, "text": " a textual dependence regardless of whether it's forward or backwards. Okay a lot of details there", "tokens": [257, 2487, 901, 31704, 10060, 295, 1968, 309, 311, 2128, 420, 12204, 13, 1033, 257, 688, 295, 4365, 456], "temperature": 0.0, "avg_logprob": -0.1308010369539261, "compression_ratio": 1.569767441860465, "no_speech_prob": 7.009781256783754e-05}, {"id": 290, "seek": 225332, "start": 2253.32, "end": 2264.84, "text": " but taking stock of this so the basic observation is languages interpreted in context that in general", "tokens": [457, 1940, 4127, 295, 341, 370, 264, 3875, 14816, 307, 8650, 26749, 294, 4319, 300, 294, 2674], "temperature": 0.0, "avg_logprob": -0.11624146302541097, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0002119075070368126}, {"id": 291, "seek": 225332, "start": 2264.84, "end": 2271.8, "text": " you can't work out the meaning or reference of things without looking at the context of the", "tokens": [291, 393, 380, 589, 484, 264, 3620, 420, 6408, 295, 721, 1553, 1237, 412, 264, 4319, 295, 264], "temperature": 0.0, "avg_logprob": -0.11624146302541097, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0002119075070368126}, {"id": 292, "seek": 225332, "start": 2271.8, "end": 2278.44, "text": " linguistic utterance. So we've seen some simple examples before so for something like words", "tokens": [43002, 17567, 719, 13, 407, 321, 600, 1612, 512, 2199, 5110, 949, 370, 337, 746, 411, 2283], "temperature": 0.0, "avg_logprob": -0.11624146302541097, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0002119075070368126}, {"id": 293, "seek": 227844, "start": 2278.44, "end": 2285.08, "text": " since disambiguation you've if you see just the words but bank you don't know what it means", "tokens": [1670, 717, 2173, 328, 16073, 291, 600, 498, 291, 536, 445, 264, 2283, 457, 3765, 291, 500, 380, 458, 437, 309, 1355], "temperature": 0.0, "avg_logprob": -0.11375859019520519, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.367241727886721e-05}, {"id": 294, "seek": 227844, "start": 2285.08, "end": 2290.6, "text": " and you need to look at a context to get some senses to whether it means a financial institution", "tokens": [293, 291, 643, 281, 574, 412, 257, 4319, 281, 483, 512, 17057, 281, 1968, 309, 1355, 257, 4669, 7818], "temperature": 0.0, "avg_logprob": -0.11375859019520519, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.367241727886721e-05}, {"id": 295, "seek": 227844, "start": 2290.6, "end": 2298.6, "text": " or the bank of a river or something like that and so anaphora and co-reference give us additional", "tokens": [420, 264, 3765, 295, 257, 6810, 420, 746, 411, 300, 293, 370, 364, 569, 7013, 293, 598, 12, 265, 5158, 976, 505, 4497], "temperature": 0.0, "avg_logprob": -0.11375859019520519, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.367241727886721e-05}, {"id": 296, "seek": 227844, "start": 2298.6, "end": 2305.88, "text": " examples where you need to be doing contextual interpretation of language so when you see a pronoun", "tokens": [5110, 689, 291, 643, 281, 312, 884, 35526, 14174, 295, 2856, 370, 562, 291, 536, 257, 14144], "temperature": 0.0, "avg_logprob": -0.11375859019520519, "compression_ratio": 1.6929824561403508, "no_speech_prob": 9.367241727886721e-05}, {"id": 297, "seek": 230588, "start": 2305.88, "end": 2313.56, "text": " you need to be looking at the context to see what it refers to and so if you think about text", "tokens": [291, 643, 281, 312, 1237, 412, 264, 4319, 281, 536, 437, 309, 14942, 281, 293, 370, 498, 291, 519, 466, 2487], "temperature": 0.0, "avg_logprob": -0.064041154725211, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.72984271356836e-05}, {"id": 298, "seek": 230588, "start": 2313.56, "end": 2320.12, "text": " understanding as a human being does it reading a story or an article that we progress through the", "tokens": [3701, 382, 257, 1952, 885, 775, 309, 3760, 257, 1657, 420, 364, 7222, 300, 321, 4205, 807, 264], "temperature": 0.0, "avg_logprob": -0.064041154725211, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.72984271356836e-05}, {"id": 299, "seek": 230588, "start": 2320.12, "end": 2327.08, "text": " article from beginning to end and as we do it we build up a pretty complex discourse model in which", "tokens": [7222, 490, 2863, 281, 917, 293, 382, 321, 360, 309, 321, 1322, 493, 257, 1238, 3997, 23938, 2316, 294, 597], "temperature": 0.0, "avg_logprob": -0.064041154725211, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.72984271356836e-05}, {"id": 300, "seek": 230588, "start": 2327.08, "end": 2334.12, "text": " new entities are introduced by mentions and then they're referred back to and relationships", "tokens": [777, 16667, 366, 7268, 538, 23844, 293, 550, 436, 434, 10839, 646, 281, 293, 6159], "temperature": 0.0, "avg_logprob": -0.064041154725211, "compression_ratio": 1.6724890829694323, "no_speech_prob": 9.72984271356836e-05}, {"id": 301, "seek": 233412, "start": 2334.12, "end": 2339.88, "text": " between them are established and they take actions and things like that and it sort of seems like", "tokens": [1296, 552, 366, 7545, 293, 436, 747, 5909, 293, 721, 411, 300, 293, 309, 1333, 295, 2544, 411], "temperature": 0.0, "avg_logprob": -0.08423052162959657, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.2458457755856216e-05}, {"id": 302, "seek": 233412, "start": 2339.88, "end": 2345.4, "text": " in our head that we sort of build up a kind of a complex graph-like discourse representation", "tokens": [294, 527, 1378, 300, 321, 1333, 295, 1322, 493, 257, 733, 295, 257, 3997, 4295, 12, 4092, 23938, 10290], "temperature": 0.0, "avg_logprob": -0.08423052162959657, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.2458457755856216e-05}, {"id": 303, "seek": 233412, "start": 2345.4, "end": 2351.56, "text": " of a piece of text with all these relationships and so part of that is these anaphora", "tokens": [295, 257, 2522, 295, 2487, 365, 439, 613, 6159, 293, 370, 644, 295, 300, 307, 613, 364, 569, 7013], "temperature": 0.0, "avg_logprob": -0.08423052162959657, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.2458457755856216e-05}, {"id": 304, "seek": 233412, "start": 2351.56, "end": 2357.96, "text": " relationships and co-reference that we're talking about here and indeed in terms of CS224N", "tokens": [6159, 293, 598, 12, 265, 5158, 300, 321, 434, 1417, 466, 510, 293, 6451, 294, 2115, 295, 9460, 7490, 19, 45], "temperature": 0.0, "avg_logprob": -0.08423052162959657, "compression_ratio": 1.6990740740740742, "no_speech_prob": 4.2458457755856216e-05}, {"id": 305, "seek": 235796, "start": 2357.96, "end": 2365.56, "text": " the only kind of whole discourse meaning that we're going to look at is looking a bit at anaphora", "tokens": [264, 787, 733, 295, 1379, 23938, 3620, 300, 321, 434, 516, 281, 574, 412, 307, 1237, 257, 857, 412, 364, 569, 7013], "temperature": 0.0, "avg_logprob": -0.08365543683369954, "compression_ratio": 1.5129533678756477, "no_speech_prob": 3.975205618189648e-05}, {"id": 306, "seek": 235796, "start": 2365.56, "end": 2371.56, "text": " and co-reference but if you want to see more about higher level natural language understanding", "tokens": [293, 598, 12, 265, 5158, 457, 498, 291, 528, 281, 536, 544, 466, 2946, 1496, 3303, 2856, 3701], "temperature": 0.0, "avg_logprob": -0.08365543683369954, "compression_ratio": 1.5129533678756477, "no_speech_prob": 3.975205618189648e-05}, {"id": 307, "seek": 235796, "start": 2371.56, "end": 2381.16, "text": " you can get more of this next quarter in CS224U so I want to tell you a bit about several different", "tokens": [291, 393, 483, 544, 295, 341, 958, 6555, 294, 9460, 7490, 19, 52, 370, 286, 528, 281, 980, 291, 257, 857, 466, 2940, 819], "temperature": 0.0, "avg_logprob": -0.08365543683369954, "compression_ratio": 1.5129533678756477, "no_speech_prob": 3.975205618189648e-05}, {"id": 308, "seek": 238116, "start": 2381.16, "end": 2389.8799999999997, "text": " ways of doing co-reference so broadly there are four different kinds of co-reference models", "tokens": [2098, 295, 884, 598, 12, 265, 5158, 370, 19511, 456, 366, 1451, 819, 3685, 295, 598, 12, 265, 5158, 5245], "temperature": 0.0, "avg_logprob": -0.12602049774593777, "compression_ratio": 1.6457142857142857, "no_speech_prob": 6.089653470553458e-05}, {"id": 309, "seek": 238116, "start": 2390.92, "end": 2398.44, "text": " so the traditional old way of doing it was rule-based systems and this isn't the topic of this class", "tokens": [370, 264, 5164, 1331, 636, 295, 884, 309, 390, 4978, 12, 6032, 3652, 293, 341, 1943, 380, 264, 4829, 295, 341, 1508], "temperature": 0.0, "avg_logprob": -0.12602049774593777, "compression_ratio": 1.6457142857142857, "no_speech_prob": 6.089653470553458e-05}, {"id": 310, "seek": 238116, "start": 2398.8399999999997, "end": 2405.0, "text": " and this is pretty archaic at this point this is stuff from last millennium but I wanted to say", "tokens": [293, 341, 307, 1238, 3912, 64, 299, 412, 341, 935, 341, 307, 1507, 490, 1036, 21362, 2197, 457, 286, 1415, 281, 584], "temperature": 0.0, "avg_logprob": -0.12602049774593777, "compression_ratio": 1.6457142857142857, "no_speech_prob": 6.089653470553458e-05}, {"id": 311, "seek": 240500, "start": 2405.0, "end": 2411.48, "text": " a little bit about it because it's actually kind of interesting as sort of food for thought as to", "tokens": [257, 707, 857, 466, 309, 570, 309, 311, 767, 733, 295, 1880, 382, 1333, 295, 1755, 337, 1194, 382, 281], "temperature": 0.0, "avg_logprob": -0.11395077827649239, "compression_ratio": 1.7571428571428571, "no_speech_prob": 6.19306811131537e-05}, {"id": 312, "seek": 240500, "start": 2411.48, "end": 2418.12, "text": " how far along we are aren't in solving you know artificial intelligence and really being out", "tokens": [577, 1400, 2051, 321, 366, 3212, 380, 294, 12606, 291, 458, 11677, 7599, 293, 534, 885, 484], "temperature": 0.0, "avg_logprob": -0.11395077827649239, "compression_ratio": 1.7571428571428571, "no_speech_prob": 6.19306811131537e-05}, {"id": 313, "seek": 240500, "start": 2418.12, "end": 2424.36, "text": " to understand texts then there are sort of classic machine learning methods of doing it", "tokens": [281, 1223, 15765, 550, 456, 366, 1333, 295, 7230, 3479, 2539, 7150, 295, 884, 309], "temperature": 0.0, "avg_logprob": -0.11395077827649239, "compression_ratio": 1.7571428571428571, "no_speech_prob": 6.19306811131537e-05}, {"id": 314, "seek": 240500, "start": 2424.36, "end": 2429.96, "text": " which you can sort of divide up as mention pair methods mention ranking methods and really", "tokens": [597, 291, 393, 1333, 295, 9845, 493, 382, 2152, 6119, 7150, 2152, 17833, 7150, 293, 534], "temperature": 0.0, "avg_logprob": -0.11395077827649239, "compression_ratio": 1.7571428571428571, "no_speech_prob": 6.19306811131537e-05}, {"id": 315, "seek": 242996, "start": 2429.96, "end": 2435.4, "text": " clustering methods and I'm sort of going to skip the clustering methods today because most of", "tokens": [596, 48673, 7150, 293, 286, 478, 1333, 295, 516, 281, 10023, 264, 596, 48673, 7150, 965, 570, 881, 295], "temperature": 0.0, "avg_logprob": -0.06432723469204372, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.313854383828584e-05}, {"id": 316, "seek": 242996, "start": 2435.4, "end": 2441.8, "text": " the work especially most of the recent work implicitly makes clusters by using even mention pair", "tokens": [264, 589, 2318, 881, 295, 264, 5162, 589, 26947, 356, 1669, 23313, 538, 1228, 754, 2152, 6119], "temperature": 0.0, "avg_logprob": -0.06432723469204372, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.313854383828584e-05}, {"id": 317, "seek": 242996, "start": 2441.8, "end": 2447.08, "text": " or mention ranking methods and so I'm going to talk about a couple of neural methods for doing that", "tokens": [420, 2152, 17833, 7150, 293, 370, 286, 478, 516, 281, 751, 466, 257, 1916, 295, 18161, 7150, 337, 884, 300], "temperature": 0.0, "avg_logprob": -0.06432723469204372, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.313854383828584e-05}, {"id": 318, "seek": 242996, "start": 2449.16, "end": 2455.88, "text": " okay but first of all let me just tell you a little bit about rule-based co-reference so there's", "tokens": [1392, 457, 700, 295, 439, 718, 385, 445, 980, 291, 257, 707, 857, 466, 4978, 12, 6032, 598, 12, 265, 5158, 370, 456, 311], "temperature": 0.0, "avg_logprob": -0.06432723469204372, "compression_ratio": 1.7916666666666667, "no_speech_prob": 2.313854383828584e-05}, {"id": 319, "seek": 245588, "start": 2455.88, "end": 2466.36, "text": " a famous historical algorithm in NLP for doing pronoun and affer a resolution which is referred", "tokens": [257, 4618, 8584, 9284, 294, 426, 45196, 337, 884, 14144, 293, 2096, 260, 257, 8669, 597, 307, 10839], "temperature": 0.0, "avg_logprob": -0.15739368110574703, "compression_ratio": 1.794392523364486, "no_speech_prob": 2.709387626964599e-05}, {"id": 320, "seek": 245588, "start": 2466.36, "end": 2472.6800000000003, "text": " to as the Hobbs algorithm so everyone just refers to it as the Hobbs algorithm and if you sort of", "tokens": [281, 382, 264, 22966, 929, 9284, 370, 1518, 445, 14942, 281, 309, 382, 264, 22966, 929, 9284, 293, 498, 291, 1333, 295], "temperature": 0.0, "avg_logprob": -0.15739368110574703, "compression_ratio": 1.794392523364486, "no_speech_prob": 2.709387626964599e-05}, {"id": 321, "seek": 245588, "start": 2472.6800000000003, "end": 2479.4, "text": " look up a textbook like Draftski and Martin's textbook it's referred to as the Hobbs algorithm", "tokens": [574, 493, 257, 25591, 411, 413, 4469, 82, 2984, 293, 9184, 311, 25591, 309, 311, 10839, 281, 382, 264, 22966, 929, 9284], "temperature": 0.0, "avg_logprob": -0.15739368110574703, "compression_ratio": 1.794392523364486, "no_speech_prob": 2.709387626964599e-05}, {"id": 322, "seek": 245588, "start": 2479.4, "end": 2484.36, "text": " but you know actually if you go back to Jerry Hobbs that's his picture over there in the corner", "tokens": [457, 291, 458, 767, 498, 291, 352, 646, 281, 17454, 22966, 929, 300, 311, 702, 3036, 670, 456, 294, 264, 4538], "temperature": 0.0, "avg_logprob": -0.15739368110574703, "compression_ratio": 1.794392523364486, "no_speech_prob": 2.709387626964599e-05}, {"id": 323, "seek": 248436, "start": 2484.36, "end": 2493.32, "text": " if you actually go back to his original paper he refers to it as the naive algorithm and then his", "tokens": [498, 291, 767, 352, 646, 281, 702, 3380, 3035, 415, 14942, 281, 309, 382, 264, 29052, 9284, 293, 550, 702], "temperature": 0.0, "avg_logprob": -0.12023341137429942, "compression_ratio": 1.78125, "no_speech_prob": 2.351887997065205e-05}, {"id": 324, "seek": 248436, "start": 2493.32, "end": 2501.32, "text": " naive algorithm for pronoun co-reference was this sort of intricate handwritten set of rules", "tokens": [29052, 9284, 337, 14144, 598, 12, 265, 5158, 390, 341, 1333, 295, 38015, 1011, 26859, 992, 295, 4474], "temperature": 0.0, "avg_logprob": -0.12023341137429942, "compression_ratio": 1.78125, "no_speech_prob": 2.351887997065205e-05}, {"id": 325, "seek": 248436, "start": 2501.32, "end": 2507.96, "text": " to work out co-reference so this is the start of the set of the rules but there are more rules", "tokens": [281, 589, 484, 598, 12, 265, 5158, 370, 341, 307, 264, 722, 295, 264, 992, 295, 264, 4474, 457, 456, 366, 544, 4474], "temperature": 0.0, "avg_logprob": -0.12023341137429942, "compression_ratio": 1.78125, "no_speech_prob": 2.351887997065205e-05}, {"id": 326, "seek": 250796, "start": 2507.96, "end": 2515.96, "text": " or more clauses of these rules for working out co-reference and you know this looks like a hot mess", "tokens": [420, 544, 49072, 295, 613, 4474, 337, 1364, 484, 598, 12, 265, 5158, 293, 291, 458, 341, 1542, 411, 257, 2368, 2082], "temperature": 0.0, "avg_logprob": -0.08163148078365602, "compression_ratio": 1.5815217391304348, "no_speech_prob": 1.3001795196032617e-05}, {"id": 327, "seek": 250796, "start": 2516.6, "end": 2523.8, "text": " but the funny thing was that this set of rules for determining co-reference were actually pretty good", "tokens": [457, 264, 4074, 551, 390, 300, 341, 992, 295, 4474, 337, 23751, 598, 12, 265, 5158, 645, 767, 1238, 665], "temperature": 0.0, "avg_logprob": -0.08163148078365602, "compression_ratio": 1.5815217391304348, "no_speech_prob": 1.3001795196032617e-05}, {"id": 328, "seek": 250796, "start": 2523.8, "end": 2532.28, "text": " and so in the sort of 1990s and 2000s decade even when people were using machine learning", "tokens": [293, 370, 294, 264, 1333, 295, 13384, 82, 293, 8132, 82, 10378, 754, 562, 561, 645, 1228, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.08163148078365602, "compression_ratio": 1.5815217391304348, "no_speech_prob": 1.3001795196032617e-05}, {"id": 329, "seek": 253228, "start": 2532.28, "end": 2538.36, "text": " base systems for doing co-reference they had hide into those machine learning base systems", "tokens": [3096, 3652, 337, 884, 598, 12, 265, 5158, 436, 632, 6479, 666, 729, 3479, 2539, 3096, 3652], "temperature": 0.0, "avg_logprob": -0.08840804876283158, "compression_ratio": 1.6946902654867257, "no_speech_prob": 1.4274599379859865e-05}, {"id": 330, "seek": 253228, "start": 2538.36, "end": 2544.92, "text": " that one of their features was the Hobbs algorithm and that the predictions it made with a certain", "tokens": [300, 472, 295, 641, 4122, 390, 264, 22966, 929, 9284, 293, 300, 264, 21264, 309, 1027, 365, 257, 1629], "temperature": 0.0, "avg_logprob": -0.08840804876283158, "compression_ratio": 1.6946902654867257, "no_speech_prob": 1.4274599379859865e-05}, {"id": 331, "seek": 253228, "start": 2544.92, "end": 2551.8, "text": " weight was then a feature in making your final decisions and it's only really in the last five", "tokens": [3364, 390, 550, 257, 4111, 294, 1455, 428, 2572, 5327, 293, 309, 311, 787, 534, 294, 264, 1036, 1732], "temperature": 0.0, "avg_logprob": -0.08840804876283158, "compression_ratio": 1.6946902654867257, "no_speech_prob": 1.4274599379859865e-05}, {"id": 332, "seek": 253228, "start": 2551.8, "end": 2557.4, "text": " years that people have moved away from using the Hobbs algorithm let me give you a little bit of a", "tokens": [924, 300, 561, 362, 4259, 1314, 490, 1228, 264, 22966, 929, 9284, 718, 385, 976, 291, 257, 707, 857, 295, 257], "temperature": 0.0, "avg_logprob": -0.08840804876283158, "compression_ratio": 1.6946902654867257, "no_speech_prob": 1.4274599379859865e-05}, {"id": 333, "seek": 255740, "start": 2557.4, "end": 2565.0, "text": " sense of how it works okay so the Hobbs algorithm here's our example this is an example from a", "tokens": [2020, 295, 577, 309, 1985, 1392, 370, 264, 22966, 929, 9284, 510, 311, 527, 1365, 341, 307, 364, 1365, 490, 257], "temperature": 0.0, "avg_logprob": -0.20871327140114523, "compression_ratio": 1.5056818181818181, "no_speech_prob": 3.3560612791916355e-05}, {"id": 334, "seek": 255740, "start": 2565.0, "end": 2571.88, "text": " Guardian book review Nile Ferguson is prolific well-paid and a snappy dresser Steven Moss heated", "tokens": [27684, 1446, 3131, 426, 794, 40823, 307, 24398, 1089, 731, 12, 35035, 293, 257, 14528, 7966, 5231, 260, 12754, 39591, 18806], "temperature": 0.0, "avg_logprob": -0.20871327140114523, "compression_ratio": 1.5056818181818181, "no_speech_prob": 3.3560612791916355e-05}, {"id": 335, "seek": 255740, "start": 2571.88, "end": 2576.6800000000003, "text": " him okay so what the Hobbs algorithm does is we start with a pronoun oops", "tokens": [796, 1392, 370, 437, 264, 22966, 929, 9284, 775, 307, 321, 722, 365, 257, 14144, 34166], "temperature": 0.0, "avg_logprob": -0.20871327140114523, "compression_ratio": 1.5056818181818181, "no_speech_prob": 3.3560612791916355e-05}, {"id": 336, "seek": 257668, "start": 2576.68, "end": 2588.2799999999997, "text": " we start with a pronoun and then it says step one go to the NP that's immediately dominating the", "tokens": [321, 722, 365, 257, 14144, 293, 550, 309, 1619, 1823, 472, 352, 281, 264, 38611, 300, 311, 4258, 43306, 264], "temperature": 0.0, "avg_logprob": -0.15535611522441006, "compression_ratio": 1.5666666666666667, "no_speech_prob": 3.531092079356313e-05}, {"id": 337, "seek": 257668, "start": 2588.2799999999997, "end": 2599.48, "text": " pronoun and then it says go up to the first NP or S call this X and the path P then it says", "tokens": [14144, 293, 550, 309, 1619, 352, 493, 281, 264, 700, 38611, 420, 318, 818, 341, 1783, 293, 264, 3100, 430, 550, 309, 1619], "temperature": 0.0, "avg_logprob": -0.15535611522441006, "compression_ratio": 1.5666666666666667, "no_speech_prob": 3.531092079356313e-05}, {"id": 338, "seek": 259948, "start": 2599.48, "end": 2607.32, "text": " traverse all branches below X the left of P left to right bread first so then it's saying to go", "tokens": [45674, 439, 14770, 2507, 1783, 264, 1411, 295, 430, 1411, 281, 558, 5961, 700, 370, 550, 309, 311, 1566, 281, 352], "temperature": 0.0, "avg_logprob": -0.0751039883861803, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.7200582078658044e-05}, {"id": 339, "seek": 259948, "start": 2607.32, "end": 2615.16, "text": " left to right for other branches below bread first so that's sort of working down the tree so we're", "tokens": [1411, 281, 558, 337, 661, 14770, 2507, 5961, 700, 370, 300, 311, 1333, 295, 1364, 760, 264, 4230, 370, 321, 434], "temperature": 0.0, "avg_logprob": -0.0751039883861803, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.7200582078658044e-05}, {"id": 340, "seek": 259948, "start": 2615.16, "end": 2626.28, "text": " going down and left to right and look for an NP okay and here's an NP but then we have to read", "tokens": [516, 760, 293, 1411, 281, 558, 293, 574, 337, 364, 38611, 1392, 293, 510, 311, 364, 38611, 457, 550, 321, 362, 281, 1401], "temperature": 0.0, "avg_logprob": -0.0751039883861803, "compression_ratio": 1.858974358974359, "no_speech_prob": 1.7200582078658044e-05}, {"id": 341, "seek": 262628, "start": 2626.28, "end": 2636.1200000000003, "text": " more carefully and say propose as antecedent any NP that has an NP or S between it and X well", "tokens": [544, 7500, 293, 584, 17421, 382, 23411, 1232, 317, 604, 38611, 300, 575, 364, 38611, 420, 318, 1296, 309, 293, 1783, 731], "temperature": 0.0, "avg_logprob": -0.09743982392388421, "compression_ratio": 1.6171428571428572, "no_speech_prob": 4.1325169149786234e-05}, {"id": 342, "seek": 262628, "start": 2636.1200000000003, "end": 2647.0, "text": " this NP here has no NP or S between NP and X so this isn't a possible antecedent so this is", "tokens": [341, 38611, 510, 575, 572, 38611, 420, 318, 1296, 38611, 293, 1783, 370, 341, 1943, 380, 257, 1944, 23411, 1232, 317, 370, 341, 307], "temperature": 0.0, "avg_logprob": -0.09743982392388421, "compression_ratio": 1.6171428571428572, "no_speech_prob": 4.1325169149786234e-05}, {"id": 343, "seek": 262628, "start": 2647.0, "end": 2655.0800000000004, "text": " all very you know complex and handwritten but basically he's sort of fit into the clauses of this", "tokens": [439, 588, 291, 458, 3997, 293, 1011, 26859, 457, 1936, 415, 311, 1333, 295, 3318, 666, 264, 49072, 295, 341], "temperature": 0.0, "avg_logprob": -0.09743982392388421, "compression_ratio": 1.6171428571428572, "no_speech_prob": 4.1325169149786234e-05}, {"id": 344, "seek": 265508, "start": 2655.08, "end": 2662.36, "text": " kind of a lot of facts about how the grammar of English works and so what this is capturing is", "tokens": [733, 295, 257, 688, 295, 9130, 466, 577, 264, 22317, 295, 3669, 1985, 293, 370, 437, 341, 307, 23384, 307], "temperature": 0.0, "avg_logprob": -0.1213612409738394, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.693880585662555e-05}, {"id": 345, "seek": 265508, "start": 2662.36, "end": 2668.68, "text": " if you imagine a different sentence you know if you imagine the sentence Steven Moss's", "tokens": [498, 291, 3811, 257, 819, 8174, 291, 458, 498, 291, 3811, 264, 8174, 12754, 39591, 311], "temperature": 0.0, "avg_logprob": -0.1213612409738394, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.693880585662555e-05}, {"id": 346, "seek": 265508, "start": 2670.92, "end": 2681.7999999999997, "text": " brother hated him well then Steven Moss would naturally be co-referent with him and in that case", "tokens": [3708, 17398, 796, 731, 550, 12754, 39591, 576, 8195, 312, 598, 12, 265, 612, 317, 365, 796, 293, 294, 300, 1389], "temperature": 0.0, "avg_logprob": -0.1213612409738394, "compression_ratio": 1.5706214689265536, "no_speech_prob": 1.693880585662555e-05}, {"id": 347, "seek": 268180, "start": 2681.8, "end": 2693.32, "text": " well precisely what you'd have is the noun phrase with well the noun brother and you'd have another", "tokens": [731, 13402, 437, 291, 1116, 362, 307, 264, 23307, 9535, 365, 731, 264, 23307, 3708, 293, 291, 1116, 362, 1071], "temperature": 0.0, "avg_logprob": -0.0885377437510389, "compression_ratio": 1.546875, "no_speech_prob": 1.7491131075075828e-05}, {"id": 348, "seek": 268180, "start": 2693.32, "end": 2703.32, "text": " noun phrase inside it for the Steven Moss and then that would go up to the sentence so in the case", "tokens": [23307, 9535, 1854, 309, 337, 264, 12754, 39591, 293, 550, 300, 576, 352, 493, 281, 264, 8174, 370, 294, 264, 1389], "temperature": 0.0, "avg_logprob": -0.0885377437510389, "compression_ratio": 1.546875, "no_speech_prob": 1.7491131075075828e-05}, {"id": 349, "seek": 270332, "start": 2703.32, "end": 2711.96, "text": " of Steven Moss's brother when you looked at this noun phrase there would be an intervening noun phrase", "tokens": [295, 12754, 39591, 311, 3708, 562, 291, 2956, 412, 341, 23307, 9535, 456, 576, 312, 364, 17104, 278, 23307, 9535], "temperature": 0.0, "avg_logprob": -0.0678312008197491, "compression_ratio": 1.7109826589595376, "no_speech_prob": 2.4247696273960173e-05}, {"id": 350, "seek": 270332, "start": 2711.96, "end": 2721.96, "text": " before you got to the note X and therefore Steven Moss is a possible and in fact good antecedent", "tokens": [949, 291, 658, 281, 264, 3637, 1783, 293, 4412, 12754, 39591, 307, 257, 1944, 293, 294, 1186, 665, 23411, 1232, 317], "temperature": 0.0, "avg_logprob": -0.0678312008197491, "compression_ratio": 1.7109826589595376, "no_speech_prob": 2.4247696273960173e-05}, {"id": 351, "seek": 270332, "start": 2721.96, "end": 2729.0800000000004, "text": " of him and the algorithm would choose Steven Moss but the algorithm correctly captures that when", "tokens": [295, 796, 293, 264, 9284, 576, 2826, 12754, 39591, 457, 264, 9284, 8944, 27986, 300, 562], "temperature": 0.0, "avg_logprob": -0.0678312008197491, "compression_ratio": 1.7109826589595376, "no_speech_prob": 2.4247696273960173e-05}, {"id": 352, "seek": 272908, "start": 2729.08, "end": 2736.92, "text": " you have the sentence Steven Moss hated him that him cannot refer to Steven Moss okay so having", "tokens": [291, 362, 264, 8174, 12754, 39591, 17398, 796, 300, 796, 2644, 2864, 281, 12754, 39591, 1392, 370, 1419], "temperature": 0.0, "avg_logprob": -0.08071548483344947, "compression_ratio": 1.8647342995169083, "no_speech_prob": 2.3528591555077583e-05}, {"id": 353, "seek": 272908, "start": 2736.92, "end": 2744.12, "text": " worked that out it then says if X is the highest S in the sentence okay so my X here is definitely", "tokens": [2732, 300, 484, 309, 550, 1619, 498, 1783, 307, 264, 6343, 318, 294, 264, 8174, 1392, 370, 452, 1783, 510, 307, 2138], "temperature": 0.0, "avg_logprob": -0.08071548483344947, "compression_ratio": 1.8647342995169083, "no_speech_prob": 2.3528591555077583e-05}, {"id": 354, "seek": 272908, "start": 2744.12, "end": 2750.7599999999998, "text": " the highest S in the sentence because I've got the whole sentence what you should do is then", "tokens": [264, 6343, 318, 294, 264, 8174, 570, 286, 600, 658, 264, 1379, 8174, 437, 291, 820, 360, 307, 550], "temperature": 0.0, "avg_logprob": -0.08071548483344947, "compression_ratio": 1.8647342995169083, "no_speech_prob": 2.3528591555077583e-05}, {"id": 355, "seek": 272908, "start": 2750.7599999999998, "end": 2758.2, "text": " traverse the parse trees of previous sentences in the order of recency so what I should not do now", "tokens": [45674, 264, 48377, 5852, 295, 3894, 16579, 294, 264, 1668, 295, 850, 3020, 370, 437, 286, 820, 406, 360, 586], "temperature": 0.0, "avg_logprob": -0.08071548483344947, "compression_ratio": 1.8647342995169083, "no_speech_prob": 2.3528591555077583e-05}, {"id": 356, "seek": 275820, "start": 2758.2, "end": 2766.2799999999997, "text": " is sort of work backwards in the text one sentence at a time going backwards looking for an antecedent", "tokens": [307, 1333, 295, 589, 12204, 294, 264, 2487, 472, 8174, 412, 257, 565, 516, 12204, 1237, 337, 364, 23411, 1232, 317], "temperature": 0.0, "avg_logprob": -0.06753878095256749, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.752113454742357e-05}, {"id": 357, "seek": 275820, "start": 2767.56, "end": 2775.24, "text": " and then for each tree traverse each tree left or right bread first so then within each tree", "tokens": [293, 550, 337, 1184, 4230, 45674, 1184, 4230, 1411, 420, 558, 5961, 700, 370, 550, 1951, 1184, 4230], "temperature": 0.0, "avg_logprob": -0.06753878095256749, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.752113454742357e-05}, {"id": 358, "seek": 275820, "start": 2775.24, "end": 2782.2, "text": " I'm doing the same of going bread first so sort of working down and then going left or right", "tokens": [286, 478, 884, 264, 912, 295, 516, 5961, 700, 370, 1333, 295, 1364, 760, 293, 550, 516, 1411, 420, 558], "temperature": 0.0, "avg_logprob": -0.06753878095256749, "compression_ratio": 1.8461538461538463, "no_speech_prob": 4.752113454742357e-05}, {"id": 359, "seek": 278220, "start": 2782.2, "end": 2790.68, "text": " with an equal bread and so hidden inside these clauses it's capturing a lot of the facts of how", "tokens": [365, 364, 2681, 5961, 293, 370, 7633, 1854, 613, 49072, 309, 311, 23384, 257, 688, 295, 264, 9130, 295, 577], "temperature": 0.0, "avg_logprob": -0.063450965014371, "compression_ratio": 1.5524861878453038, "no_speech_prob": 3.366702003404498e-05}, {"id": 360, "seek": 278220, "start": 2790.68, "end": 2799.3999999999996, "text": " co-reference typically works so what you find in English I'll stay but in general this is true", "tokens": [598, 12, 265, 5158, 5850, 1985, 370, 437, 291, 915, 294, 3669, 286, 603, 1754, 457, 294, 2674, 341, 307, 2074], "temperature": 0.0, "avg_logprob": -0.063450965014371, "compression_ratio": 1.5524861878453038, "no_speech_prob": 3.366702003404498e-05}, {"id": 361, "seek": 278220, "start": 2799.3999999999996, "end": 2806.4399999999996, "text": " of lots of languages is that there are general preferences and tendencies for co-reference", "tokens": [295, 3195, 295, 8650, 307, 300, 456, 366, 2674, 21910, 293, 45488, 337, 598, 12, 265, 5158], "temperature": 0.0, "avg_logprob": -0.063450965014371, "compression_ratio": 1.5524861878453038, "no_speech_prob": 3.366702003404498e-05}, {"id": 362, "seek": 280644, "start": 2806.44, "end": 2813.08, "text": " so a lot of the time a pronoun will be co-referent with something in the same sentence like Steven's", "tokens": [370, 257, 688, 295, 264, 565, 257, 14144, 486, 312, 598, 12, 265, 612, 317, 365, 746, 294, 264, 912, 8174, 411, 12754, 311], "temperature": 0.0, "avg_logprob": -0.09382982343156761, "compression_ratio": 1.7927927927927927, "no_speech_prob": 2.2370388251147233e-05}, {"id": 363, "seek": 280644, "start": 2813.08, "end": 2819.2400000000002, "text": " Moss's brother heeded him but it can't be if it's too close to it so you can't say Steven Moss heeded", "tokens": [39591, 311, 3708, 415, 37679, 796, 457, 309, 393, 380, 312, 498, 309, 311, 886, 1998, 281, 309, 370, 291, 393, 380, 584, 12754, 39591, 415, 37679], "temperature": 0.0, "avg_logprob": -0.09382982343156761, "compression_ratio": 1.7927927927927927, "no_speech_prob": 2.2370388251147233e-05}, {"id": 364, "seek": 280644, "start": 2819.2400000000002, "end": 2825.0, "text": " him and have the him be Steven Moss and if you're then looking for co-reference it's further away", "tokens": [796, 293, 362, 264, 796, 312, 12754, 39591, 293, 498, 291, 434, 550, 1237, 337, 598, 12, 265, 5158, 309, 311, 3052, 1314], "temperature": 0.0, "avg_logprob": -0.09382982343156761, "compression_ratio": 1.7927927927927927, "no_speech_prob": 2.2370388251147233e-05}, {"id": 365, "seek": 280644, "start": 2826.36, "end": 2832.84, "text": " the thing it's co-referent with is normally close by and so that's why you work backwards through", "tokens": [264, 551, 309, 311, 598, 12, 265, 612, 317, 365, 307, 5646, 1998, 538, 293, 370, 300, 311, 983, 291, 589, 12204, 807], "temperature": 0.0, "avg_logprob": -0.09382982343156761, "compression_ratio": 1.7927927927927927, "no_speech_prob": 2.2370388251147233e-05}, {"id": 366, "seek": 283284, "start": 2832.84, "end": 2840.04, "text": " sentences one by one but then once you're looking within a particular sentence the most likely", "tokens": [16579, 472, 538, 472, 457, 550, 1564, 291, 434, 1237, 1951, 257, 1729, 8174, 264, 881, 3700], "temperature": 0.0, "avg_logprob": -0.09711616340724902, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.0520651711849496e-05}, {"id": 367, "seek": 283284, "start": 2840.04, "end": 2847.0, "text": " thing that's going to be co-referent too is a topical noun phrase and default topics in English", "tokens": [551, 300, 311, 516, 281, 312, 598, 12, 265, 612, 317, 886, 307, 257, 1192, 804, 23307, 9535, 293, 7576, 8378, 294, 3669], "temperature": 0.0, "avg_logprob": -0.09711616340724902, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.0520651711849496e-05}, {"id": 368, "seek": 283284, "start": 2847.6400000000003, "end": 2856.28, "text": " subjects so by doing things bread first left or right a preferred antecedent is then a subject", "tokens": [13066, 370, 538, 884, 721, 5961, 700, 1411, 420, 558, 257, 16494, 23411, 1232, 317, 307, 550, 257, 3983], "temperature": 0.0, "avg_logprob": -0.09711616340724902, "compression_ratio": 1.5714285714285714, "no_speech_prob": 5.0520651711849496e-05}, {"id": 369, "seek": 285628, "start": 2856.28, "end": 2863.8, "text": " and so this algorithm I won't go through all the complex clauses 529 ends up saying okay what you", "tokens": [293, 370, 341, 9284, 286, 1582, 380, 352, 807, 439, 264, 3997, 49072, 1025, 11871, 5314, 493, 1566, 1392, 437, 291], "temperature": 0.0, "avg_logprob": -0.09938434097501966, "compression_ratio": 1.48, "no_speech_prob": 5.98355291003827e-05}, {"id": 370, "seek": 285628, "start": 2863.8, "end": 2871.0800000000004, "text": " should do is propose Nile Ferguson as what is co-referent to him which is the obvious correct reading", "tokens": [820, 360, 307, 17421, 426, 794, 40823, 382, 437, 307, 598, 12, 265, 612, 317, 281, 796, 597, 307, 264, 6322, 3006, 3760], "temperature": 0.0, "avg_logprob": -0.09938434097501966, "compression_ratio": 1.48, "no_speech_prob": 5.98355291003827e-05}, {"id": 371, "seek": 285628, "start": 2871.0800000000004, "end": 2878.1200000000003, "text": " in this example okay you probably didn't want to know that and in some sense the details of that", "tokens": [294, 341, 1365, 1392, 291, 1391, 994, 380, 528, 281, 458, 300, 293, 294, 512, 2020, 264, 4365, 295, 300], "temperature": 0.0, "avg_logprob": -0.09938434097501966, "compression_ratio": 1.48, "no_speech_prob": 5.98355291003827e-05}, {"id": 372, "seek": 287812, "start": 2878.12, "end": 2888.6, "text": " aren't interesting but what is I think actually still interesting in 2021 is what points Jerry Hobbes", "tokens": [3212, 380, 1880, 457, 437, 307, 286, 519, 767, 920, 1880, 294, 7201, 307, 437, 2793, 17454, 22966, 6446], "temperature": 0.0, "avg_logprob": -0.11280945214358243, "compression_ratio": 1.5426356589147288, "no_speech_prob": 0.00017073331400752068}, {"id": 373, "seek": 287812, "start": 2888.6, "end": 2899.16, "text": " was actually trying to make last millennium and the point he was trying to make was the following", "tokens": [390, 767, 1382, 281, 652, 1036, 21362, 2197, 293, 264, 935, 415, 390, 1382, 281, 652, 390, 264, 3480], "temperature": 0.0, "avg_logprob": -0.11280945214358243, "compression_ratio": 1.5426356589147288, "no_speech_prob": 0.00017073331400752068}, {"id": 374, "seek": 289916, "start": 2899.16, "end": 2909.3999999999996, "text": " so Jerry Hobbes wrote this algorithm the naive algorithm because what he said was well look if you", "tokens": [370, 17454, 22966, 6446, 4114, 341, 9284, 264, 29052, 9284, 570, 437, 415, 848, 390, 731, 574, 498, 291], "temperature": 0.0, "avg_logprob": -0.07728374963519218, "compression_ratio": 1.8571428571428572, "no_speech_prob": 2.7825921279145405e-05}, {"id": 375, "seek": 289916, "start": 2909.3999999999996, "end": 2917.8799999999997, "text": " want to try and crudely determine co-reference well there are these various preferences right there's", "tokens": [528, 281, 853, 293, 941, 532, 736, 6997, 598, 12, 265, 5158, 731, 456, 366, 613, 3683, 21910, 558, 456, 311], "temperature": 0.0, "avg_logprob": -0.07728374963519218, "compression_ratio": 1.8571428571428572, "no_speech_prob": 2.7825921279145405e-05}, {"id": 376, "seek": 289916, "start": 2917.8799999999997, "end": 2923.0, "text": " the preference for same sentence there's the preference for recency there's a preference for", "tokens": [264, 17502, 337, 912, 8174, 456, 311, 264, 17502, 337, 850, 3020, 456, 311, 257, 17502, 337], "temperature": 0.0, "avg_logprob": -0.07728374963519218, "compression_ratio": 1.8571428571428572, "no_speech_prob": 2.7825921279145405e-05}, {"id": 377, "seek": 289916, "start": 2923.0, "end": 2928.52, "text": " topical things like subject and there are things where you know if it has gender it has to agree", "tokens": [1192, 804, 721, 411, 3983, 293, 456, 366, 721, 689, 291, 458, 498, 309, 575, 7898, 309, 575, 281, 3986], "temperature": 0.0, "avg_logprob": -0.07728374963519218, "compression_ratio": 1.8571428571428572, "no_speech_prob": 2.7825921279145405e-05}, {"id": 378, "seek": 292852, "start": 2928.52, "end": 2936.92, "text": " in gender so there are sort of strong constraints of that sort so I can write an algorithm using my", "tokens": [294, 7898, 370, 456, 366, 1333, 295, 2068, 18491, 295, 300, 1333, 370, 286, 393, 2464, 364, 9284, 1228, 452], "temperature": 0.0, "avg_logprob": -0.08937983434708392, "compression_ratio": 1.5769230769230769, "no_speech_prob": 3.3690808777464554e-05}, {"id": 379, "seek": 292852, "start": 2936.92, "end": 2945.16, "text": " linguistic mouse which captures all the main preferences and actually it works pretty well", "tokens": [43002, 9719, 597, 27986, 439, 264, 2135, 21910, 293, 767, 309, 1985, 1238, 731], "temperature": 0.0, "avg_logprob": -0.08937983434708392, "compression_ratio": 1.5769230769230769, "no_speech_prob": 3.3690808777464554e-05}, {"id": 380, "seek": 292852, "start": 2945.88, "end": 2955.56, "text": " doing that is a pretty strong baseline system but what Jerry Hobbes wanted to argue is that this", "tokens": [884, 300, 307, 257, 1238, 2068, 20518, 1185, 457, 437, 17454, 22966, 6446, 1415, 281, 9695, 307, 300, 341], "temperature": 0.0, "avg_logprob": -0.08937983434708392, "compression_ratio": 1.5769230769230769, "no_speech_prob": 3.3690808777464554e-05}, {"id": 381, "seek": 295556, "start": 2955.56, "end": 2963.56, "text": " algorithm just isn't something you should believe in this isn't a solution to the problem this is", "tokens": [9284, 445, 1943, 380, 746, 291, 820, 1697, 294, 341, 1943, 380, 257, 3827, 281, 264, 1154, 341, 307], "temperature": 0.0, "avg_logprob": -0.05151814594864845, "compression_ratio": 1.6292134831460674, "no_speech_prob": 2.5428897060919553e-05}, {"id": 382, "seek": 295556, "start": 2963.56, "end": 2972.84, "text": " just sort of you know making a best guess according to the preferences of what's most likely", "tokens": [445, 1333, 295, 291, 458, 1455, 257, 1151, 2041, 4650, 281, 264, 21910, 295, 437, 311, 881, 3700], "temperature": 0.0, "avg_logprob": -0.05151814594864845, "compression_ratio": 1.6292134831460674, "no_speech_prob": 2.5428897060919553e-05}, {"id": 383, "seek": 295556, "start": 2972.84, "end": 2980.2, "text": " without actually understanding what's going on in the text at all and so actually what Jerry Hobbes", "tokens": [1553, 767, 3701, 437, 311, 516, 322, 294, 264, 2487, 412, 439, 293, 370, 767, 437, 17454, 22966, 6446], "temperature": 0.0, "avg_logprob": -0.05151814594864845, "compression_ratio": 1.6292134831460674, "no_speech_prob": 2.5428897060919553e-05}, {"id": 384, "seek": 298020, "start": 2980.2, "end": 2986.4399999999996, "text": " wanted to argue was the so-called Hobbes algorithm now he wasn't a fan of the Hobbes algorithm he", "tokens": [1415, 281, 9695, 390, 264, 370, 12, 11880, 22966, 6446, 9284, 586, 415, 2067, 380, 257, 3429, 295, 264, 22966, 6446, 9284, 415], "temperature": 0.0, "avg_logprob": -0.0810857380137724, "compression_ratio": 1.8558139534883722, "no_speech_prob": 5.300857446854934e-05}, {"id": 385, "seek": 298020, "start": 2986.4399999999996, "end": 2992.7599999999998, "text": " was wanting to argue that the Hobbes algorithm is completely inadequate as a solution to the problem", "tokens": [390, 7935, 281, 9695, 300, 264, 22966, 6446, 9284, 307, 2584, 42107, 382, 257, 3827, 281, 264, 1154], "temperature": 0.0, "avg_logprob": -0.0810857380137724, "compression_ratio": 1.8558139534883722, "no_speech_prob": 5.300857446854934e-05}, {"id": 386, "seek": 298020, "start": 2992.7599999999998, "end": 2998.7599999999998, "text": " and the only way we'll actually make progress in natural language understanding is by building systems", "tokens": [293, 264, 787, 636, 321, 603, 767, 652, 4205, 294, 3303, 2856, 3701, 307, 538, 2390, 3652], "temperature": 0.0, "avg_logprob": -0.0810857380137724, "compression_ratio": 1.8558139534883722, "no_speech_prob": 5.300857446854934e-05}, {"id": 387, "seek": 298020, "start": 2998.7599999999998, "end": 3006.2799999999997, "text": " that actually really understand the text and this is actually something that has come to the fore", "tokens": [300, 767, 534, 1223, 264, 2487, 293, 341, 307, 767, 746, 300, 575, 808, 281, 264, 2091], "temperature": 0.0, "avg_logprob": -0.0810857380137724, "compression_ratio": 1.8558139534883722, "no_speech_prob": 5.300857446854934e-05}, {"id": 388, "seek": 300628, "start": 3006.28, "end": 3016.2000000000003, "text": " again more recently so the suggestion is that in general you can't work out co-reference or", "tokens": [797, 544, 3938, 370, 264, 16541, 307, 300, 294, 2674, 291, 393, 380, 589, 484, 598, 12, 265, 5158, 420], "temperature": 0.0, "avg_logprob": -0.14202933013439178, "compression_ratio": 1.5675675675675675, "no_speech_prob": 9.387107274960726e-05}, {"id": 389, "seek": 300628, "start": 3016.2000000000003, "end": 3021.5600000000004, "text": " phenomenal in that for in particular unless you're really understanding the meaning of the text", "tokens": [17778, 294, 300, 337, 294, 1729, 5969, 291, 434, 534, 3701, 264, 3620, 295, 264, 2487], "temperature": 0.0, "avg_logprob": -0.14202933013439178, "compression_ratio": 1.5675675675675675, "no_speech_prob": 9.387107274960726e-05}, {"id": 390, "seek": 300628, "start": 3021.5600000000004, "end": 3028.36, "text": " and people look at pairs of examples like these ones so she poured water from the picture into the cup", "tokens": [293, 561, 574, 412, 15494, 295, 5110, 411, 613, 2306, 370, 750, 23270, 1281, 490, 264, 3036, 666, 264, 4414], "temperature": 0.0, "avg_logprob": -0.14202933013439178, "compression_ratio": 1.5675675675675675, "no_speech_prob": 9.387107274960726e-05}, {"id": 391, "seek": 302836, "start": 3028.36, "end": 3037.08, "text": " until it was full so think for just half a moment well what is it in that example that is full", "tokens": [1826, 309, 390, 1577, 370, 519, 337, 445, 1922, 257, 1623, 731, 437, 307, 309, 294, 300, 1365, 300, 307, 1577], "temperature": 0.0, "avg_logprob": -0.06512763534767041, "compression_ratio": 1.8278145695364238, "no_speech_prob": 2.533204860810656e-05}, {"id": 392, "seek": 302836, "start": 3038.6, "end": 3046.04, "text": " so that what's full there is the cup but then if I say she poured water from the picture into the", "tokens": [370, 300, 437, 311, 1577, 456, 307, 264, 4414, 457, 550, 498, 286, 584, 750, 23270, 1281, 490, 264, 3036, 666, 264], "temperature": 0.0, "avg_logprob": -0.06512763534767041, "compression_ratio": 1.8278145695364238, "no_speech_prob": 2.533204860810656e-05}, {"id": 393, "seek": 302836, "start": 3046.04, "end": 3053.7200000000003, "text": " cup until it was empty well what's empty well that's the picture and the point that", "tokens": [4414, 1826, 309, 390, 6707, 731, 437, 311, 6707, 731, 300, 311, 264, 3036, 293, 264, 935, 300], "temperature": 0.0, "avg_logprob": -0.06512763534767041, "compression_ratio": 1.8278145695364238, "no_speech_prob": 2.533204860810656e-05}, {"id": 394, "seek": 305372, "start": 3053.72, "end": 3061.8799999999997, "text": " is being made with this example is the only thing that's been changed in these examples is", "tokens": [307, 885, 1027, 365, 341, 1365, 307, 264, 787, 551, 300, 311, 668, 3105, 294, 613, 5110, 307], "temperature": 0.0, "avg_logprob": -0.0936493489050096, "compression_ratio": 1.7654320987654322, "no_speech_prob": 3.005359394592233e-05}, {"id": 395, "seek": 305372, "start": 3062.8399999999997, "end": 3071.7999999999997, "text": " the adjective right here so these two examples have exactly the same grammatical structure so in", "tokens": [264, 44129, 558, 510, 370, 613, 732, 5110, 362, 2293, 264, 912, 17570, 267, 804, 3877, 370, 294], "temperature": 0.0, "avg_logprob": -0.0936493489050096, "compression_ratio": 1.7654320987654322, "no_speech_prob": 3.005359394592233e-05}, {"id": 396, "seek": 305372, "start": 3071.7999999999997, "end": 3080.8399999999997, "text": " terms of the Hobbes naive algorithm the Hobbes naive algorithm necessarily has to predict the same", "tokens": [2115, 295, 264, 22966, 6446, 29052, 9284, 264, 22966, 6446, 29052, 9284, 4725, 575, 281, 6069, 264, 912], "temperature": 0.0, "avg_logprob": -0.0936493489050096, "compression_ratio": 1.7654320987654322, "no_speech_prob": 3.005359394592233e-05}, {"id": 397, "seek": 308084, "start": 3080.84, "end": 3087.8, "text": " answer for both of these but that's wrong you just cannot determine the correct pronoun", "tokens": [1867, 337, 1293, 295, 613, 457, 300, 311, 2085, 291, 445, 2644, 6997, 264, 3006, 14144], "temperature": 0.0, "avg_logprob": -0.06737228761236351, "compression_ratio": 1.7008928571428572, "no_speech_prob": 6.475036207120866e-05}, {"id": 398, "seek": 308084, "start": 3087.8, "end": 3094.36, "text": " antecedent based on grammatical preferences of the kind that are used in the naive algorithm", "tokens": [23411, 1232, 317, 2361, 322, 17570, 267, 804, 21910, 295, 264, 733, 300, 366, 1143, 294, 264, 29052, 9284], "temperature": 0.0, "avg_logprob": -0.06737228761236351, "compression_ratio": 1.7008928571428572, "no_speech_prob": 6.475036207120866e-05}, {"id": 399, "seek": 308084, "start": 3094.36, "end": 3100.92, "text": " you actually have to conceptually understand about pictures and cups and water and full and empty", "tokens": [291, 767, 362, 281, 3410, 671, 1223, 466, 5242, 293, 13381, 293, 1281, 293, 1577, 293, 6707], "temperature": 0.0, "avg_logprob": -0.06737228761236351, "compression_ratio": 1.7008928571428572, "no_speech_prob": 6.475036207120866e-05}, {"id": 400, "seek": 308084, "start": 3100.92, "end": 3109.08, "text": " to be able to choose the right antecedent here's another famous example that goes along the same lines", "tokens": [281, 312, 1075, 281, 2826, 264, 558, 23411, 1232, 317, 510, 311, 1071, 4618, 1365, 300, 1709, 2051, 264, 912, 3876], "temperature": 0.0, "avg_logprob": -0.06737228761236351, "compression_ratio": 1.7008928571428572, "no_speech_prob": 6.475036207120866e-05}, {"id": 401, "seek": 310908, "start": 3109.08, "end": 3117.72, "text": " so Terry Winnegrad shown here as a young man so long long ago Terry Winnegrad came to Stanford as", "tokens": [370, 21983, 10427, 716, 861, 345, 4898, 510, 382, 257, 2037, 587, 370, 938, 938, 2057, 21983, 10427, 716, 861, 345, 1361, 281, 20374, 382], "temperature": 0.0, "avg_logprob": -0.12797534787977063, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.0002810463774949312}, {"id": 402, "seek": 310908, "start": 3117.72, "end": 3126.2799999999997, "text": " the natural language processing faculty and Terry Winnegrad became disillusioned with the symbolic AI", "tokens": [264, 3303, 2856, 9007, 6389, 293, 21983, 10427, 716, 861, 345, 3062, 717, 373, 5704, 292, 365, 264, 25755, 7318], "temperature": 0.0, "avg_logprob": -0.12797534787977063, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.0002810463774949312}, {"id": 403, "seek": 310908, "start": 3126.2799999999997, "end": 3134.04, "text": " of those days and just gave it up altogether and he reinvented himself as being an HCI person and", "tokens": [295, 729, 1708, 293, 445, 2729, 309, 493, 19051, 293, 415, 33477, 292, 3647, 382, 885, 364, 389, 25240, 954, 293], "temperature": 0.0, "avg_logprob": -0.12797534787977063, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.0002810463774949312}, {"id": 404, "seek": 313404, "start": 3134.04, "end": 3141.16, "text": " so Terry was then essentially the person who established the HCI program at Stanford but before", "tokens": [370, 21983, 390, 550, 4476, 264, 954, 567, 7545, 264, 389, 25240, 1461, 412, 20374, 457, 949], "temperature": 0.0, "avg_logprob": -0.09264116883277893, "compression_ratio": 1.7757009345794392, "no_speech_prob": 3.2962321711238474e-05}, {"id": 405, "seek": 313404, "start": 3141.88, "end": 3149.96, "text": " he lost faith in symbolic AI he talked about the co-reference problem and pointed out a", "tokens": [415, 2731, 4522, 294, 25755, 7318, 415, 2825, 466, 264, 598, 12, 265, 5158, 1154, 293, 10932, 484, 257], "temperature": 0.0, "avg_logprob": -0.09264116883277893, "compression_ratio": 1.7757009345794392, "no_speech_prob": 3.2962321711238474e-05}, {"id": 406, "seek": 313404, "start": 3149.96, "end": 3156.84, "text": " similar pair of examples here so we have the city council refused the women a permit because they", "tokens": [2531, 6119, 295, 5110, 510, 370, 321, 362, 264, 2307, 9209, 14654, 264, 2266, 257, 13423, 570, 436], "temperature": 0.0, "avg_logprob": -0.09264116883277893, "compression_ratio": 1.7757009345794392, "no_speech_prob": 3.2962321711238474e-05}, {"id": 407, "seek": 313404, "start": 3156.84, "end": 3163.96, "text": " feared violence versus the city council refused the women a permit because they advocated violence", "tokens": [30629, 6270, 5717, 264, 2307, 9209, 14654, 264, 2266, 257, 13423, 570, 436, 7915, 770, 6270], "temperature": 0.0, "avg_logprob": -0.09264116883277893, "compression_ratio": 1.7757009345794392, "no_speech_prob": 3.2962321711238474e-05}, {"id": 408, "seek": 316396, "start": 3163.96, "end": 3170.68, "text": " so again you have this situation where these two sentences have identical syntactic structure", "tokens": [370, 797, 291, 362, 341, 2590, 689, 613, 732, 16579, 362, 14800, 23980, 19892, 3877], "temperature": 0.0, "avg_logprob": -0.09569901433484308, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00015217573672998697}, {"id": 409, "seek": 316396, "start": 3170.68, "end": 3178.36, "text": " and they differ only in the choice of verb here but once you add knowledge common sense knowledge", "tokens": [293, 436, 743, 787, 294, 264, 3922, 295, 9595, 510, 457, 1564, 291, 909, 3601, 2689, 2020, 3601], "temperature": 0.0, "avg_logprob": -0.09569901433484308, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00015217573672998697}, {"id": 410, "seek": 316396, "start": 3178.36, "end": 3187.8, "text": " of how the human world works well what how this should pretty obviously be interpreted in the", "tokens": [295, 577, 264, 1952, 1002, 1985, 731, 437, 577, 341, 820, 1238, 2745, 312, 26749, 294, 264], "temperature": 0.0, "avg_logprob": -0.09569901433484308, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.00015217573672998697}, {"id": 411, "seek": 318780, "start": 3187.8, "end": 3194.44, "text": " first one that they is referring to the city council whereas in the second one that they", "tokens": [700, 472, 300, 436, 307, 13761, 281, 264, 2307, 9209, 9735, 294, 264, 1150, 472, 300, 436], "temperature": 0.0, "avg_logprob": -0.10230779086842257, "compression_ratio": 1.7881773399014778, "no_speech_prob": 2.9169852496124804e-05}, {"id": 412, "seek": 318780, "start": 3194.44, "end": 3201.88, "text": " is referring to the women and so coming off of that example of Terry these have been", "tokens": [307, 13761, 281, 264, 2266, 293, 370, 1348, 766, 295, 300, 1365, 295, 21983, 613, 362, 668], "temperature": 0.0, "avg_logprob": -0.10230779086842257, "compression_ratio": 1.7881773399014778, "no_speech_prob": 2.9169852496124804e-05}, {"id": 413, "seek": 318780, "start": 3203.0, "end": 3210.28, "text": " preferred to as Winnegrad schemers so Winnegrad schema challenges sort of choosing the right", "tokens": [16494, 281, 382, 10427, 716, 861, 345, 22627, 433, 370, 10427, 716, 861, 345, 34078, 4759, 1333, 295, 10875, 264, 558], "temperature": 0.0, "avg_logprob": -0.10230779086842257, "compression_ratio": 1.7881773399014778, "no_speech_prob": 2.9169852496124804e-05}, {"id": 414, "seek": 318780, "start": 3210.28, "end": 3217.1600000000003, "text": " reference here and so it's basically just doing pronominal and afra but you know the interesting", "tokens": [6408, 510, 293, 370, 309, 311, 1936, 445, 884, 7569, 298, 2071, 293, 3238, 424, 457, 291, 458, 264, 1880], "temperature": 0.0, "avg_logprob": -0.10230779086842257, "compression_ratio": 1.7881773399014778, "no_speech_prob": 2.9169852496124804e-05}, {"id": 415, "seek": 321716, "start": 3217.16, "end": 3224.2799999999997, "text": " thing is people have been interested you know what a test of general intelligence and one famous", "tokens": [551, 307, 561, 362, 668, 3102, 291, 458, 437, 257, 1500, 295, 2674, 7599, 293, 472, 4618], "temperature": 0.0, "avg_logprob": -0.15857693423395572, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.00024184305220842361}, {"id": 416, "seek": 321716, "start": 3224.2799999999997, "end": 3229.3199999999997, "text": " general test of intelligence so I won't talk about now is the Turing test and there's been a lot of", "tokens": [2674, 1500, 295, 7599, 370, 286, 1582, 380, 751, 466, 586, 307, 264, 314, 1345, 1500, 293, 456, 311, 668, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.15857693423395572, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.00024184305220842361}, {"id": 417, "seek": 321716, "start": 3229.3199999999997, "end": 3235.64, "text": " debate about problems with the Turing test and is it good and so in particular Hector Levesque is a", "tokens": [7958, 466, 2740, 365, 264, 314, 1345, 1500, 293, 307, 309, 665, 293, 370, 294, 1729, 389, 20814, 1456, 977, 1077, 307, 257], "temperature": 0.0, "avg_logprob": -0.15857693423395572, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.00024184305220842361}, {"id": 418, "seek": 321716, "start": 3236.68, "end": 3243.8799999999997, "text": " very well-known senior AI person he actually proposed that a better alternative to the Turing test", "tokens": [588, 731, 12, 6861, 7965, 7318, 954, 415, 767, 10348, 300, 257, 1101, 8535, 281, 264, 314, 1345, 1500], "temperature": 0.0, "avg_logprob": -0.15857693423395572, "compression_ratio": 1.724890829694323, "no_speech_prob": 0.00024184305220842361}, {"id": 419, "seek": 324388, "start": 3243.88, "end": 3250.2000000000003, "text": " might be to do what he then dubbed Winnegrad schema and Winnegrad schema is just solving", "tokens": [1062, 312, 281, 360, 437, 415, 550, 43686, 10427, 716, 861, 345, 34078, 293, 10427, 716, 861, 345, 34078, 307, 445, 12606], "temperature": 0.0, "avg_logprob": -0.07650389296285222, "compression_ratio": 1.7293577981651376, "no_speech_prob": 4.9747759476304054e-05}, {"id": 420, "seek": 324388, "start": 3250.2000000000003, "end": 3255.96, "text": " pronominal co-reference in cases like this where you have to have knowledge about the situation", "tokens": [7569, 298, 2071, 598, 12, 265, 5158, 294, 3331, 411, 341, 689, 291, 362, 281, 362, 3601, 466, 264, 2590], "temperature": 0.0, "avg_logprob": -0.07650389296285222, "compression_ratio": 1.7293577981651376, "no_speech_prob": 4.9747759476304054e-05}, {"id": 421, "seek": 324388, "start": 3255.96, "end": 3262.52, "text": " the world to get the answer right and so he's basically arguing that you know you can review", "tokens": [264, 1002, 281, 483, 264, 1867, 558, 293, 370, 415, 311, 1936, 19697, 300, 291, 458, 291, 393, 3131], "temperature": 0.0, "avg_logprob": -0.07650389296285222, "compression_ratio": 1.7293577981651376, "no_speech_prob": 4.9747759476304054e-05}, {"id": 422, "seek": 324388, "start": 3262.52, "end": 3270.36, "text": " really solving co-reference as solving artificial intelligence and that's sort of what the position", "tokens": [534, 12606, 598, 12, 265, 5158, 382, 12606, 11677, 7599, 293, 300, 311, 1333, 295, 437, 264, 2535], "temperature": 0.0, "avg_logprob": -0.07650389296285222, "compression_ratio": 1.7293577981651376, "no_speech_prob": 4.9747759476304054e-05}, {"id": 423, "seek": 327036, "start": 3270.36, "end": 3276.92, "text": " that Hobbes wanted to advocate so what he actually said about his algorithm was that the naive", "tokens": [300, 22966, 6446, 1415, 281, 14608, 370, 437, 415, 767, 848, 466, 702, 9284, 390, 300, 264, 29052], "temperature": 0.0, "avg_logprob": -0.09109726905822754, "compression_ratio": 1.7391304347826086, "no_speech_prob": 9.895141556626186e-05}, {"id": 424, "seek": 327036, "start": 3276.92, "end": 3282.52, "text": " approach is quite good computationally speaking it will be a long time before a semantically-based", "tokens": [3109, 307, 1596, 665, 24903, 379, 4124, 309, 486, 312, 257, 938, 565, 949, 257, 4361, 49505, 12, 6032], "temperature": 0.0, "avg_logprob": -0.09109726905822754, "compression_ratio": 1.7391304347826086, "no_speech_prob": 9.895141556626186e-05}, {"id": 425, "seek": 327036, "start": 3282.52, "end": 3288.36, "text": " algorithm is sophisticated enough to perform as well and these results set a very high standard", "tokens": [9284, 307, 16950, 1547, 281, 2042, 382, 731, 293, 613, 3542, 992, 257, 588, 1090, 3832], "temperature": 0.0, "avg_logprob": -0.09109726905822754, "compression_ratio": 1.7391304347826086, "no_speech_prob": 9.895141556626186e-05}, {"id": 426, "seek": 327036, "start": 3288.36, "end": 3293.08, "text": " for any other approached way in for and he was proven right about that because there was sort of", "tokens": [337, 604, 661, 17247, 636, 294, 337, 293, 415, 390, 12785, 558, 466, 300, 570, 456, 390, 1333, 295], "temperature": 0.0, "avg_logprob": -0.09109726905822754, "compression_ratio": 1.7391304347826086, "no_speech_prob": 9.895141556626186e-05}, {"id": 427, "seek": 327036, "start": 3293.08, "end": 3298.52, "text": " really talked to around 2015 before people thought they could do without the Hobbes algorithm", "tokens": [534, 2825, 281, 926, 7546, 949, 561, 1194, 436, 727, 360, 1553, 264, 22966, 6446, 9284], "temperature": 0.0, "avg_logprob": -0.09109726905822754, "compression_ratio": 1.7391304347826086, "no_speech_prob": 9.895141556626186e-05}, {"id": 428, "seek": 329852, "start": 3298.52, "end": 3304.84, "text": " but then he notes yet there is every reason to pursue a semantically-based approach the naive", "tokens": [457, 550, 415, 5570, 1939, 456, 307, 633, 1778, 281, 12392, 257, 4361, 49505, 12, 6032, 3109, 264, 29052], "temperature": 0.0, "avg_logprob": -0.08553239356639773, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.0001605243160156533}, {"id": 429, "seek": 329852, "start": 3304.84, "end": 3311.8, "text": " algorithm does not work anyone can think of examples where it fails in these cases it not only fails", "tokens": [9284, 775, 406, 589, 2878, 393, 519, 295, 5110, 689, 309, 18199, 294, 613, 3331, 309, 406, 787, 18199], "temperature": 0.0, "avg_logprob": -0.08553239356639773, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.0001605243160156533}, {"id": 430, "seek": 329852, "start": 3311.8, "end": 3317.32, "text": " it gives no indication that it has failed and offers no help in finding the real antecedent", "tokens": [309, 2709, 572, 18877, 300, 309, 575, 7612, 293, 7736, 572, 854, 294, 5006, 264, 957, 23411, 1232, 317], "temperature": 0.0, "avg_logprob": -0.08553239356639773, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.0001605243160156533}, {"id": 431, "seek": 329852, "start": 3318.44, "end": 3323.8, "text": " and so I think this is actually still interesting stuff to think about because you know really for the", "tokens": [293, 370, 286, 519, 341, 307, 767, 920, 1880, 1507, 281, 519, 466, 570, 291, 458, 534, 337, 264], "temperature": 0.0, "avg_logprob": -0.08553239356639773, "compression_ratio": 1.6767241379310345, "no_speech_prob": 0.0001605243160156533}, {"id": 432, "seek": 332380, "start": 3323.8, "end": 3330.2000000000003, "text": " kind of machine learning-based co-reference systems that we're building you know they're not a", "tokens": [733, 295, 3479, 2539, 12, 6032, 598, 12, 265, 5158, 3652, 300, 321, 434, 2390, 291, 458, 436, 434, 406, 257], "temperature": 0.0, "avg_logprob": -0.09363832002804603, "compression_ratio": 1.669683257918552, "no_speech_prob": 1.0124465006811079e-05}, {"id": 433, "seek": 332380, "start": 3330.2000000000003, "end": 3337.4, "text": " hot mess of rules like the Hobbes algorithm but basically they're still sort of working out", "tokens": [2368, 2082, 295, 4474, 411, 264, 22966, 6446, 9284, 457, 1936, 436, 434, 920, 1333, 295, 1364, 484], "temperature": 0.0, "avg_logprob": -0.09363832002804603, "compression_ratio": 1.669683257918552, "no_speech_prob": 1.0124465006811079e-05}, {"id": 434, "seek": 332380, "start": 3337.4, "end": 3345.8, "text": " statistical preferences of what patterns are most likely and choosing the antecedent that way", "tokens": [22820, 21910, 295, 437, 8294, 366, 881, 3700, 293, 10875, 264, 23411, 1232, 317, 300, 636], "temperature": 0.0, "avg_logprob": -0.09363832002804603, "compression_ratio": 1.669683257918552, "no_speech_prob": 1.0124465006811079e-05}, {"id": 435, "seek": 332380, "start": 3346.6000000000004, "end": 3353.7200000000003, "text": " they really have exactly the same deficiencies still that Hobbes was talking about right", "tokens": [436, 534, 362, 2293, 264, 912, 19248, 31294, 920, 300, 22966, 6446, 390, 1417, 466, 558], "temperature": 0.0, "avg_logprob": -0.09363832002804603, "compression_ratio": 1.669683257918552, "no_speech_prob": 1.0124465006811079e-05}, {"id": 436, "seek": 335372, "start": 3353.72, "end": 3361.72, "text": " that they fail in various cases it's easy to find places where they fail the algorithms give you", "tokens": [300, 436, 3061, 294, 3683, 3331, 309, 311, 1858, 281, 915, 3190, 689, 436, 3061, 264, 14642, 976, 291], "temperature": 0.0, "avg_logprob": -0.07230504353841145, "compression_ratio": 1.6964285714285714, "no_speech_prob": 3.87592262995895e-05}, {"id": 437, "seek": 335372, "start": 3361.72, "end": 3367.72, "text": " no idea when they fail they're not really understanding the text in a way that a human does to", "tokens": [572, 1558, 562, 436, 3061, 436, 434, 406, 534, 3701, 264, 2487, 294, 257, 636, 300, 257, 1952, 775, 281], "temperature": 0.0, "avg_logprob": -0.07230504353841145, "compression_ratio": 1.6964285714285714, "no_speech_prob": 3.87592262995895e-05}, {"id": 438, "seek": 335372, "start": 3367.72, "end": 3373.3999999999996, "text": " determine the antecedent so we still actually have a lot more work to do before we're really doing", "tokens": [6997, 264, 23411, 1232, 317, 370, 321, 920, 767, 362, 257, 688, 544, 589, 281, 360, 949, 321, 434, 534, 884], "temperature": 0.0, "avg_logprob": -0.07230504353841145, "compression_ratio": 1.6964285714285714, "no_speech_prob": 3.87592262995895e-05}, {"id": 439, "seek": 335372, "start": 3373.3999999999996, "end": 3380.12, "text": " full artificial intelligence but I best get on now and actually tell you a bit about some", "tokens": [1577, 11677, 7599, 457, 286, 1151, 483, 322, 586, 293, 767, 980, 291, 257, 857, 466, 512], "temperature": 0.0, "avg_logprob": -0.07230504353841145, "compression_ratio": 1.6964285714285714, "no_speech_prob": 3.87592262995895e-05}, {"id": 440, "seek": 338012, "start": 3380.12, "end": 3388.2, "text": " co-reference algorithms right so the simple way of thinking about co-reference is to say", "tokens": [598, 12, 265, 5158, 14642, 558, 370, 264, 2199, 636, 295, 1953, 466, 598, 12, 265, 5158, 307, 281, 584], "temperature": 0.0, "avg_logprob": -0.10852009720272487, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.116395237157121e-05}, {"id": 441, "seek": 338012, "start": 3388.2, "end": 3396.7599999999998, "text": " that you're making just a binary decision about a reference pair so if you have your mentions", "tokens": [300, 291, 434, 1455, 445, 257, 17434, 3537, 466, 257, 6408, 6119, 370, 498, 291, 362, 428, 23844], "temperature": 0.0, "avg_logprob": -0.10852009720272487, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.116395237157121e-05}, {"id": 442, "seek": 338012, "start": 3398.2799999999997, "end": 3406.52, "text": " you can then say well I've come to my next mention she I want to work out what it's co-referent", "tokens": [291, 393, 550, 584, 731, 286, 600, 808, 281, 452, 958, 2152, 750, 286, 528, 281, 589, 484, 437, 309, 311, 598, 12, 265, 612, 317], "temperature": 0.0, "avg_logprob": -0.10852009720272487, "compression_ratio": 1.6352941176470588, "no_speech_prob": 3.116395237157121e-05}, {"id": 443, "seek": 340652, "start": 3406.52, "end": 3413.72, "text": " with and I can just look at all of the mentions that came before it and say is it co-referent or not", "tokens": [365, 293, 286, 393, 445, 574, 412, 439, 295, 264, 23844, 300, 1361, 949, 309, 293, 584, 307, 309, 598, 12, 265, 612, 317, 420, 406], "temperature": 0.0, "avg_logprob": -0.06041052582067087, "compression_ratio": 1.8387096774193548, "no_speech_prob": 7.46663790778257e-05}, {"id": 444, "seek": 340652, "start": 3413.72, "end": 3420.7599999999998, "text": " and do a binary decision so at training time I'll be able to say I have positive examples assuming", "tokens": [293, 360, 257, 17434, 3537, 370, 412, 3097, 565, 286, 603, 312, 1075, 281, 584, 286, 362, 3353, 5110, 11926], "temperature": 0.0, "avg_logprob": -0.06041052582067087, "compression_ratio": 1.8387096774193548, "no_speech_prob": 7.46663790778257e-05}, {"id": 445, "seek": 340652, "start": 3420.7599999999998, "end": 3426.28, "text": " I've got some data labeled for what's co-referent of what as to these ones are co-referent and I've", "tokens": [286, 600, 658, 512, 1412, 21335, 337, 437, 311, 598, 12, 265, 612, 317, 295, 437, 382, 281, 613, 2306, 366, 598, 12, 265, 612, 317, 293, 286, 600], "temperature": 0.0, "avg_logprob": -0.06041052582067087, "compression_ratio": 1.8387096774193548, "no_speech_prob": 7.46663790778257e-05}, {"id": 446, "seek": 340652, "start": 3426.28, "end": 3433.8, "text": " got some negative examples of these ones are not co-referent and what I want to do is build a model", "tokens": [658, 512, 3671, 5110, 295, 613, 2306, 366, 406, 598, 12, 265, 612, 317, 293, 437, 286, 528, 281, 360, 307, 1322, 257, 2316], "temperature": 0.0, "avg_logprob": -0.06041052582067087, "compression_ratio": 1.8387096774193548, "no_speech_prob": 7.46663790778257e-05}, {"id": 447, "seek": 343380, "start": 3433.8, "end": 3439.2400000000002, "text": " that learns to predict co-referent things and I can do that fairly straightforwardly in the", "tokens": [300, 27152, 281, 6069, 598, 12, 265, 612, 317, 721, 293, 286, 393, 360, 300, 6457, 15325, 356, 294, 264], "temperature": 0.0, "avg_logprob": -0.06833398860433827, "compression_ratio": 1.6306306306306306, "no_speech_prob": 1.6157238860614598e-05}, {"id": 448, "seek": 343380, "start": 3439.2400000000002, "end": 3447.0800000000004, "text": " kind of ways that we have talked about so I train with the regular kind of cross entropy loss", "tokens": [733, 295, 2098, 300, 321, 362, 2825, 466, 370, 286, 3847, 365, 264, 3890, 733, 295, 3278, 30867, 4470], "temperature": 0.0, "avg_logprob": -0.06833398860433827, "compression_ratio": 1.6306306306306306, "no_speech_prob": 1.6157238860614598e-05}, {"id": 449, "seek": 343380, "start": 3447.96, "end": 3454.76, "text": " where I'm now summing over every pairwise binary decision as to whether two mentions", "tokens": [689, 286, 478, 586, 2408, 2810, 670, 633, 6119, 3711, 17434, 3537, 382, 281, 1968, 732, 23844], "temperature": 0.0, "avg_logprob": -0.06833398860433827, "compression_ratio": 1.6306306306306306, "no_speech_prob": 1.6157238860614598e-05}, {"id": 450, "seek": 343380, "start": 3454.76, "end": 3462.52, "text": " are co-referent to each other or not and so then when I'm at test time what I want to do is", "tokens": [366, 598, 12, 265, 612, 317, 281, 1184, 661, 420, 406, 293, 370, 550, 562, 286, 478, 412, 1500, 565, 437, 286, 528, 281, 360, 307], "temperature": 0.0, "avg_logprob": -0.06833398860433827, "compression_ratio": 1.6306306306306306, "no_speech_prob": 1.6157238860614598e-05}, {"id": 451, "seek": 346252, "start": 3462.52, "end": 3468.84, "text": " cluster the mentions that correspond to the same entity and I do that by making use for my", "tokens": [13630, 264, 23844, 300, 6805, 281, 264, 912, 13977, 293, 286, 360, 300, 538, 1455, 764, 337, 452], "temperature": 0.0, "avg_logprob": -0.09353210501474878, "compression_ratio": 1.654970760233918, "no_speech_prob": 3.814429874182679e-05}, {"id": 452, "seek": 346252, "start": 3468.84, "end": 3478.7599999999998, "text": " pairwise score so I can run my pairwise score and it will give a probability or a score that any", "tokens": [6119, 3711, 6175, 370, 286, 393, 1190, 452, 6119, 3711, 6175, 293, 309, 486, 976, 257, 8482, 420, 257, 6175, 300, 604], "temperature": 0.0, "avg_logprob": -0.09353210501474878, "compression_ratio": 1.654970760233918, "no_speech_prob": 3.814429874182679e-05}, {"id": 453, "seek": 346252, "start": 3478.7599999999998, "end": 3485.8, "text": " two mentions are co-referent so by picking some threshold like 0.5 I can add co-reference links", "tokens": [732, 23844, 366, 598, 12, 265, 612, 317, 370, 538, 8867, 512, 14678, 411, 1958, 13, 20, 286, 393, 909, 598, 12, 265, 5158, 6123], "temperature": 0.0, "avg_logprob": -0.09353210501474878, "compression_ratio": 1.654970760233918, "no_speech_prob": 3.814429874182679e-05}, {"id": 454, "seek": 348580, "start": 3485.8, "end": 3493.96, "text": " for when the classifier says it's above the threshold and then I do one more step to give me a", "tokens": [337, 562, 264, 1508, 9902, 1619, 309, 311, 3673, 264, 14678, 293, 550, 286, 360, 472, 544, 1823, 281, 976, 385, 257], "temperature": 0.0, "avg_logprob": -0.07214097600234182, "compression_ratio": 1.715151515151515, "no_speech_prob": 3.4690063330344856e-05}, {"id": 455, "seek": 348580, "start": 3493.96, "end": 3501.32, "text": " clustering I then say okay let's also make the transitive closure to give me clusters so it", "tokens": [596, 48673, 286, 550, 584, 1392, 718, 311, 611, 652, 264, 1145, 2187, 24653, 281, 976, 385, 23313, 370, 309], "temperature": 0.0, "avg_logprob": -0.07214097600234182, "compression_ratio": 1.715151515151515, "no_speech_prob": 3.4690063330344856e-05}, {"id": 456, "seek": 348580, "start": 3501.32, "end": 3508.52, "text": " thought that I and she were co-referent and my and she were co-referent therefore I also have to", "tokens": [1194, 300, 286, 293, 750, 645, 598, 12, 265, 612, 317, 293, 452, 293, 750, 645, 598, 12, 265, 612, 317, 4412, 286, 611, 362, 281], "temperature": 0.0, "avg_logprob": -0.07214097600234182, "compression_ratio": 1.715151515151515, "no_speech_prob": 3.4690063330344856e-05}, {"id": 457, "seek": 350852, "start": 3508.52, "end": 3517.32, "text": " regard I and my as co-referent and so that's sort of the completion by transitivity and so since", "tokens": [3843, 286, 293, 452, 382, 598, 12, 265, 612, 317, 293, 370, 300, 311, 1333, 295, 264, 19372, 538, 1145, 270, 4253, 293, 370, 1670], "temperature": 0.0, "avg_logprob": -0.09845919291178386, "compression_ratio": 1.7470588235294118, "no_speech_prob": 1.7204049072461203e-05}, {"id": 458, "seek": 350852, "start": 3517.32, "end": 3525.24, "text": " we always complete by transitivity note that this algorithm is very sensitive to making any mistake", "tokens": [321, 1009, 3566, 538, 1145, 270, 4253, 3637, 300, 341, 9284, 307, 588, 9477, 281, 1455, 604, 6146], "temperature": 0.0, "avg_logprob": -0.09845919291178386, "compression_ratio": 1.7470588235294118, "no_speech_prob": 1.7204049072461203e-05}, {"id": 459, "seek": 350852, "start": 3525.24, "end": 3532.12, "text": " in a positive sense because if you make one mistake for example you say that he and my a co-referent", "tokens": [294, 257, 3353, 2020, 570, 498, 291, 652, 472, 6146, 337, 1365, 291, 584, 300, 415, 293, 452, 257, 598, 12, 265, 612, 317], "temperature": 0.0, "avg_logprob": -0.09845919291178386, "compression_ratio": 1.7470588235294118, "no_speech_prob": 1.7204049072461203e-05}, {"id": 460, "seek": 353212, "start": 3532.12, "end": 3540.2799999999997, "text": " and then by transitivity all of the mentions in these sentence become one big cluster and that", "tokens": [293, 550, 538, 1145, 270, 4253, 439, 295, 264, 23844, 294, 613, 8174, 1813, 472, 955, 13630, 293, 300], "temperature": 0.0, "avg_logprob": -0.11367720014908735, "compression_ratio": 1.6337209302325582, "no_speech_prob": 5.3798412409378216e-05}, {"id": 461, "seek": 353212, "start": 3540.2799999999997, "end": 3547.88, "text": " they're all co-referent with each other so that's a workable algorithm and people have often used it", "tokens": [436, 434, 439, 598, 12, 265, 612, 317, 365, 1184, 661, 370, 300, 311, 257, 589, 712, 9284, 293, 561, 362, 2049, 1143, 309], "temperature": 0.0, "avg_logprob": -0.11367720014908735, "compression_ratio": 1.6337209302325582, "no_speech_prob": 5.3798412409378216e-05}, {"id": 462, "seek": 353212, "start": 3547.88, "end": 3555.64, "text": " but often people go a little bit beyond that and prefer a mention ranking model so it", "tokens": [457, 2049, 561, 352, 257, 707, 857, 4399, 300, 293, 4382, 257, 2152, 17833, 2316, 370, 309], "temperature": 0.0, "avg_logprob": -0.11367720014908735, "compression_ratio": 1.6337209302325582, "no_speech_prob": 5.3798412409378216e-05}, {"id": 463, "seek": 355564, "start": 3555.64, "end": 3562.2, "text": " we just explain the advantages of that that normally if you have a long document where it's", "tokens": [321, 445, 2903, 264, 14906, 295, 300, 300, 5646, 498, 291, 362, 257, 938, 4166, 689, 309, 311], "temperature": 0.0, "avg_logprob": -0.12059570442546498, "compression_ratio": 1.7440758293838863, "no_speech_prob": 6.086851499276236e-05}, {"id": 464, "seek": 355564, "start": 3562.2, "end": 3567.16, "text": " Ralph Nader and he did this and some of them did something to him and we visited his house and", "tokens": [28131, 426, 8312, 293, 415, 630, 341, 293, 512, 295, 552, 630, 746, 281, 796, 293, 321, 11220, 702, 1782, 293], "temperature": 0.0, "avg_logprob": -0.12059570442546498, "compression_ratio": 1.7440758293838863, "no_speech_prob": 6.086851499276236e-05}, {"id": 465, "seek": 355564, "start": 3567.16, "end": 3574.3599999999997, "text": " blah blah blah blah and then somebody voted for Nader because he in terms of building a", "tokens": [12288, 12288, 12288, 12288, 293, 550, 2618, 13415, 337, 426, 8312, 570, 415, 294, 2115, 295, 2390, 257], "temperature": 0.0, "avg_logprob": -0.12059570442546498, "compression_ratio": 1.7440758293838863, "no_speech_prob": 6.086851499276236e-05}, {"id": 466, "seek": 355564, "start": 3574.3599999999997, "end": 3583.08, "text": " co-reference classifier it seems like it's easy and reasonable it's easy and reasonable to be", "tokens": [598, 12, 265, 5158, 1508, 9902, 309, 2544, 411, 309, 311, 1858, 293, 10585, 309, 311, 1858, 293, 10585, 281, 312], "temperature": 0.0, "avg_logprob": -0.12059570442546498, "compression_ratio": 1.7440758293838863, "no_speech_prob": 6.086851499276236e-05}, {"id": 467, "seek": 358308, "start": 3583.08, "end": 3591.7999999999997, "text": " able to recover that this he refers to Nader but in terms of building a classifier for it to", "tokens": [1075, 281, 8114, 300, 341, 415, 14942, 281, 426, 8312, 457, 294, 2115, 295, 2390, 257, 1508, 9902, 337, 309, 281], "temperature": 0.0, "avg_logprob": -0.08708551951817103, "compression_ratio": 1.7534883720930232, "no_speech_prob": 7.349493535002694e-05}, {"id": 468, "seek": 358308, "start": 3591.7999999999997, "end": 3598.04, "text": " recognize that this he should be referring to this Nader which might be three paragraphs back", "tokens": [5521, 300, 341, 415, 820, 312, 13761, 281, 341, 426, 8312, 597, 1062, 312, 1045, 48910, 646], "temperature": 0.0, "avg_logprob": -0.08708551951817103, "compression_ratio": 1.7534883720930232, "no_speech_prob": 7.349493535002694e-05}, {"id": 469, "seek": 358308, "start": 3598.04, "end": 3604.12, "text": " seems kind of unreasonable how you're going to recover that so those far away ones might be almost", "tokens": [2544, 733, 295, 41730, 577, 291, 434, 516, 281, 8114, 300, 370, 729, 1400, 1314, 2306, 1062, 312, 1920], "temperature": 0.0, "avg_logprob": -0.08708551951817103, "compression_ratio": 1.7534883720930232, "no_speech_prob": 7.349493535002694e-05}, {"id": 470, "seek": 358308, "start": 3604.12, "end": 3610.68, "text": " impossible to get correct and so that suggests that maybe we should have a different way of", "tokens": [6243, 281, 483, 3006, 293, 370, 300, 13409, 300, 1310, 321, 820, 362, 257, 819, 636, 295], "temperature": 0.0, "avg_logprob": -0.08708551951817103, "compression_ratio": 1.7534883720930232, "no_speech_prob": 7.349493535002694e-05}, {"id": 471, "seek": 361068, "start": 3610.68, "end": 3619.96, "text": " configuring this task so instead of doing it that way what we should say is well this he here", "tokens": [6662, 1345, 341, 5633, 370, 2602, 295, 884, 309, 300, 636, 437, 321, 820, 584, 307, 731, 341, 415, 510], "temperature": 0.0, "avg_logprob": -0.0890955924987793, "compression_ratio": 1.5771428571428572, "no_speech_prob": 5.056191730545834e-05}, {"id": 472, "seek": 361068, "start": 3619.96, "end": 3627.56, "text": " has various possible antecedents and our job is to just choose one of them and that's almost", "tokens": [575, 3683, 1944, 23411, 1232, 791, 293, 527, 1691, 307, 281, 445, 2826, 472, 295, 552, 293, 300, 311, 1920], "temperature": 0.0, "avg_logprob": -0.0890955924987793, "compression_ratio": 1.5771428571428572, "no_speech_prob": 5.056191730545834e-05}, {"id": 473, "seek": 361068, "start": 3627.56, "end": 3637.3999999999996, "text": " sufficient apart from we need to add one more choice which is well some mentions won't be", "tokens": [11563, 4936, 490, 321, 643, 281, 909, 472, 544, 3922, 597, 307, 731, 512, 23844, 1582, 380, 312], "temperature": 0.0, "avg_logprob": -0.0890955924987793, "compression_ratio": 1.5771428571428572, "no_speech_prob": 5.056191730545834e-05}, {"id": 474, "seek": 363740, "start": 3637.4, "end": 3643.4, "text": " co-referent with anything that proceeds because we're introducing a new entity into the discourse", "tokens": [598, 12, 265, 612, 317, 365, 1340, 300, 32280, 570, 321, 434, 15424, 257, 777, 13977, 666, 264, 23938], "temperature": 0.0, "avg_logprob": -0.1286312271566952, "compression_ratio": 1.6368715083798884, "no_speech_prob": 9.140426118392497e-05}, {"id": 475, "seek": 363740, "start": 3643.4, "end": 3652.12, "text": " so we can add one more dummy mention the N.A. mention so it doesn't refer to anything previously", "tokens": [370, 321, 393, 909, 472, 544, 35064, 2152, 264, 426, 13, 32, 13, 2152, 370, 309, 1177, 380, 2864, 281, 1340, 8046], "temperature": 0.0, "avg_logprob": -0.1286312271566952, "compression_ratio": 1.6368715083798884, "no_speech_prob": 9.140426118392497e-05}, {"id": 476, "seek": 363740, "start": 3652.12, "end": 3659.56, "text": " in the discourse discourse and then our job at each point is to do mention ranking to choose which", "tokens": [294, 264, 23938, 23938, 293, 550, 527, 1691, 412, 1184, 935, 307, 281, 360, 2152, 17833, 281, 2826, 597], "temperature": 0.0, "avg_logprob": -0.1286312271566952, "compression_ratio": 1.6368715083798884, "no_speech_prob": 9.140426118392497e-05}, {"id": 477, "seek": 365956, "start": 3659.56, "end": 3668.2799999999997, "text": " one of these she refers to and then at that point rather than doing binary yes no classifiers", "tokens": [472, 295, 613, 750, 14942, 281, 293, 550, 412, 300, 935, 2831, 813, 884, 17434, 2086, 572, 1508, 23463], "temperature": 0.0, "avg_logprob": -0.060348866590813025, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.7073945602751337e-05}, {"id": 478, "seek": 365956, "start": 3668.2799999999997, "end": 3674.84, "text": " that what we can do is say aha this is choose one classification and then we can use the kind of", "tokens": [300, 437, 321, 393, 360, 307, 584, 47340, 341, 307, 2826, 472, 21538, 293, 550, 321, 393, 764, 264, 733, 295], "temperature": 0.0, "avg_logprob": -0.060348866590813025, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.7073945602751337e-05}, {"id": 479, "seek": 365956, "start": 3674.84, "end": 3683.72, "text": " softmax classifiers that we've seen at many points previously okay so that gets us in business", "tokens": [2787, 41167, 1508, 23463, 300, 321, 600, 1612, 412, 867, 2793, 8046, 1392, 370, 300, 2170, 505, 294, 1606], "temperature": 0.0, "avg_logprob": -0.060348866590813025, "compression_ratio": 1.7272727272727273, "no_speech_prob": 2.7073945602751337e-05}, {"id": 480, "seek": 368372, "start": 3683.72, "end": 3690.8399999999997, "text": " for building systems and for either of these kind of models there are several ways in which we", "tokens": [337, 2390, 3652, 293, 337, 2139, 295, 613, 733, 295, 5245, 456, 366, 2940, 2098, 294, 597, 321], "temperature": 0.0, "avg_logprob": -0.07408664907727923, "compression_ratio": 1.8093023255813954, "no_speech_prob": 5.9037000028183684e-05}, {"id": 481, "seek": 368372, "start": 3690.8399999999997, "end": 3697.0, "text": " can build the system we could use any kind of traditional machine learning classifier we could use", "tokens": [393, 1322, 264, 1185, 321, 727, 764, 604, 733, 295, 5164, 3479, 2539, 1508, 9902, 321, 727, 764], "temperature": 0.0, "avg_logprob": -0.07408664907727923, "compression_ratio": 1.8093023255813954, "no_speech_prob": 5.9037000028183684e-05}, {"id": 482, "seek": 368372, "start": 3697.72, "end": 3703.3999999999996, "text": " simple neural network we can use more advanced ones with all of the tools that we've been learning", "tokens": [2199, 18161, 3209, 321, 393, 764, 544, 7339, 2306, 365, 439, 295, 264, 3873, 300, 321, 600, 668, 2539], "temperature": 0.0, "avg_logprob": -0.07408664907727923, "compression_ratio": 1.8093023255813954, "no_speech_prob": 5.9037000028183684e-05}, {"id": 483, "seek": 368372, "start": 3703.3999999999996, "end": 3711.72, "text": " about more recently let me just quickly show you a simple neural network way of doing it so this", "tokens": [466, 544, 3938, 718, 385, 445, 2661, 855, 291, 257, 2199, 18161, 3209, 636, 295, 884, 309, 370, 341], "temperature": 0.0, "avg_logprob": -0.07408664907727923, "compression_ratio": 1.8093023255813954, "no_speech_prob": 5.9037000028183684e-05}, {"id": 484, "seek": 371172, "start": 3711.72, "end": 3721.8799999999997, "text": " is a model that my PhD student Kevin Clark did in 2015 so not that long ago but what he was doing was", "tokens": [307, 257, 2316, 300, 452, 14476, 3107, 9954, 18572, 630, 294, 7546, 370, 406, 300, 938, 2057, 457, 437, 415, 390, 884, 390], "temperature": 0.0, "avg_logprob": -0.10811954386094037, "compression_ratio": 1.6141304347826086, "no_speech_prob": 9.14364936761558e-05}, {"id": 485, "seek": 371172, "start": 3721.8799999999997, "end": 3729.0, "text": " doing co-reference resolution based on the mentions with a simple feed forward neural network kind", "tokens": [884, 598, 12, 265, 5158, 8669, 2361, 322, 264, 23844, 365, 257, 2199, 3154, 2128, 18161, 3209, 733], "temperature": 0.0, "avg_logprob": -0.10811954386094037, "compression_ratio": 1.6141304347826086, "no_speech_prob": 9.14364936761558e-05}, {"id": 486, "seek": 371172, "start": 3729.0, "end": 3734.7599999999998, "text": " of in some sense like we did dependency parsing with a simple feed forward neural network so for", "tokens": [295, 294, 512, 2020, 411, 321, 630, 33621, 21156, 278, 365, 257, 2199, 3154, 2128, 18161, 3209, 370, 337], "temperature": 0.0, "avg_logprob": -0.10811954386094037, "compression_ratio": 1.6141304347826086, "no_speech_prob": 9.14364936761558e-05}, {"id": 487, "seek": 373476, "start": 3734.76, "end": 3743.48, "text": " for the mention it had word embeddings and seed and had word embeddings there were some additional", "tokens": [337, 264, 2152, 309, 632, 1349, 12240, 29432, 293, 8871, 293, 632, 1349, 12240, 29432, 456, 645, 512, 4497], "temperature": 0.0, "avg_logprob": -0.1384294867515564, "compression_ratio": 2.0597826086956523, "no_speech_prob": 3.1615174520993605e-05}, {"id": 488, "seek": 373476, "start": 3743.48, "end": 3748.84, "text": " features of each of the mention and candidate and a seed and and then there were some final", "tokens": [4122, 295, 1184, 295, 264, 2152, 293, 11532, 293, 257, 8871, 293, 293, 550, 456, 645, 512, 2572], "temperature": 0.0, "avg_logprob": -0.1384294867515564, "compression_ratio": 2.0597826086956523, "no_speech_prob": 3.1615174520993605e-05}, {"id": 489, "seek": 373476, "start": 3748.84, "end": 3754.5200000000004, "text": " additional features that captured things like distance away which you can't see from either", "tokens": [4497, 4122, 300, 11828, 721, 411, 4560, 1314, 597, 291, 393, 380, 536, 490, 2139], "temperature": 0.0, "avg_logprob": -0.1384294867515564, "compression_ratio": 2.0597826086956523, "no_speech_prob": 3.1615174520993605e-05}, {"id": 490, "seek": 373476, "start": 3754.5200000000004, "end": 3760.2000000000003, "text": " the mention or the candidate and they were all of those features were just fed into several feed", "tokens": [264, 2152, 420, 264, 11532, 293, 436, 645, 439, 295, 729, 4122, 645, 445, 4636, 666, 2940, 3154], "temperature": 0.0, "avg_logprob": -0.1384294867515564, "compression_ratio": 2.0597826086956523, "no_speech_prob": 3.1615174520993605e-05}, {"id": 491, "seek": 376020, "start": 3760.2, "end": 3768.68, "text": " forward layers of a neural network and it gave you a score of are these things um co-referent or not", "tokens": [2128, 7914, 295, 257, 18161, 3209, 293, 309, 2729, 291, 257, 6175, 295, 366, 613, 721, 1105, 598, 12, 265, 612, 317, 420, 406], "temperature": 0.0, "avg_logprob": -0.09518076244153474, "compression_ratio": 1.6108108108108108, "no_speech_prob": 2.2768090275349095e-05}, {"id": 492, "seek": 376020, "start": 3768.68, "end": 3778.4399999999996, "text": " and that by itself um just worked pretty well um and I won't say more details about that um but", "tokens": [293, 300, 538, 2564, 1105, 445, 2732, 1238, 731, 1105, 293, 286, 1582, 380, 584, 544, 4365, 466, 300, 1105, 457], "temperature": 0.0, "avg_logprob": -0.09518076244153474, "compression_ratio": 1.6108108108108108, "no_speech_prob": 2.2768090275349095e-05}, {"id": 493, "seek": 376020, "start": 3778.4399999999996, "end": 3786.68, "text": " what I do want to show is sort of a more advanced um and modern neural co-reference system but before", "tokens": [437, 286, 360, 528, 281, 855, 307, 1333, 295, 257, 544, 7339, 1105, 293, 4363, 18161, 598, 12, 265, 5158, 1185, 457, 949], "temperature": 0.0, "avg_logprob": -0.09518076244153474, "compression_ratio": 1.6108108108108108, "no_speech_prob": 2.2768090275349095e-05}, {"id": 494, "seek": 378668, "start": 3786.68, "end": 3793.8799999999997, "text": " I do that I want to take a digression and sort of say a few words about convolutional neural networks", "tokens": [286, 360, 300, 286, 528, 281, 747, 257, 2528, 2775, 293, 1333, 295, 584, 257, 1326, 2283, 466, 45216, 304, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.07990135866052964, "compression_ratio": 1.6453488372093024, "no_speech_prob": 2.390127701801248e-05}, {"id": 495, "seek": 378668, "start": 3796.2, "end": 3805.64, "text": " so um the idea of when you apply a convolutional neural network to language i.e. to sequences", "tokens": [370, 1105, 264, 1558, 295, 562, 291, 3079, 257, 45216, 304, 18161, 3209, 281, 2856, 741, 13, 68, 13, 281, 22978], "temperature": 0.0, "avg_logprob": -0.07990135866052964, "compression_ratio": 1.6453488372093024, "no_speech_prob": 2.390127701801248e-05}, {"id": 496, "seek": 378668, "start": 3805.64, "end": 3812.8399999999997, "text": " is that what you're going to do is you're going to compute vectors features effectively", "tokens": [307, 300, 437, 291, 434, 516, 281, 360, 307, 291, 434, 516, 281, 14722, 18875, 4122, 8659], "temperature": 0.0, "avg_logprob": -0.07990135866052964, "compression_ratio": 1.6453488372093024, "no_speech_prob": 2.390127701801248e-05}, {"id": 497, "seek": 381284, "start": 3812.84, "end": 3818.92, "text": " for every possible words sub sequence of a certain length so that if you have a piece of text", "tokens": [337, 633, 1944, 2283, 1422, 8310, 295, 257, 1629, 4641, 370, 300, 498, 291, 362, 257, 2522, 295, 2487], "temperature": 0.0, "avg_logprob": -0.11308876125291846, "compression_ratio": 1.853658536585366, "no_speech_prob": 4.6060340537223965e-05}, {"id": 498, "seek": 381284, "start": 3818.92, "end": 3825.56, "text": " like tentative deal reach to keep government open you might say I'm going to take every three words", "tokens": [411, 7054, 1166, 2028, 2524, 281, 1066, 2463, 1269, 291, 1062, 584, 286, 478, 516, 281, 747, 633, 1045, 2283], "temperature": 0.0, "avg_logprob": -0.11308876125291846, "compression_ratio": 1.853658536585366, "no_speech_prob": 4.6060340537223965e-05}, {"id": 499, "seek": 381284, "start": 3825.56, "end": 3833.7200000000003, "text": " of that I tentative deal reached deal reached to reach to keep and I'm going to compute a vector", "tokens": [295, 300, 286, 7054, 1166, 2028, 6488, 2028, 6488, 281, 2524, 281, 1066, 293, 286, 478, 516, 281, 14722, 257, 8062], "temperature": 0.0, "avg_logprob": -0.11308876125291846, "compression_ratio": 1.853658536585366, "no_speech_prob": 4.6060340537223965e-05}, {"id": 500, "seek": 381284, "start": 3833.7200000000003, "end": 3841.8, "text": " based on that sub sequence of words and use those computed vectors in my model by somehow", "tokens": [2361, 322, 300, 1422, 8310, 295, 2283, 293, 764, 729, 40610, 18875, 294, 452, 2316, 538, 6063], "temperature": 0.0, "avg_logprob": -0.11308876125291846, "compression_ratio": 1.853658536585366, "no_speech_prob": 4.6060340537223965e-05}, {"id": 501, "seek": 384180, "start": 3841.8, "end": 3851.1600000000003, "text": " grouping them together so the canonical um case of convolutional neural networks um is in vision", "tokens": [40149, 552, 1214, 370, 264, 46491, 1105, 1389, 295, 45216, 304, 18161, 9590, 1105, 307, 294, 5201], "temperature": 0.0, "avg_logprob": -0.11321352509891286, "compression_ratio": 1.6900584795321638, "no_speech_prob": 3.0212211640900932e-05}, {"id": 502, "seek": 384180, "start": 3851.1600000000003, "end": 3859.48, "text": " and so if after this next quarter um you go along to CS231 and um you'll be able to um spend weeks", "tokens": [293, 370, 498, 934, 341, 958, 6555, 1105, 291, 352, 2051, 281, 9460, 9356, 16, 293, 1105, 291, 603, 312, 1075, 281, 1105, 3496, 3259], "temperature": 0.0, "avg_logprob": -0.11321352509891286, "compression_ratio": 1.6900584795321638, "no_speech_prob": 3.0212211640900932e-05}, {"id": 503, "seek": 384180, "start": 3859.48, "end": 3866.6800000000003, "text": " doing convolutional neural networks for vision and so the idea there is that you've got these", "tokens": [884, 45216, 304, 18161, 9590, 337, 5201, 293, 370, 264, 1558, 456, 307, 300, 291, 600, 658, 613], "temperature": 0.0, "avg_logprob": -0.11321352509891286, "compression_ratio": 1.6900584795321638, "no_speech_prob": 3.0212211640900932e-05}, {"id": 504, "seek": 386668, "start": 3866.68, "end": 3875.0, "text": " convolutional filters that you sort of slide over an image and you compute a function of each", "tokens": [45216, 304, 15995, 300, 291, 1333, 295, 4137, 670, 364, 3256, 293, 291, 14722, 257, 2445, 295, 1184], "temperature": 0.0, "avg_logprob": -0.0751366933186849, "compression_ratio": 2.1714285714285713, "no_speech_prob": 7.29186740500154e-06}, {"id": 505, "seek": 386668, "start": 3875.0, "end": 3881.08, "text": " place so the sort of little red um numbers are showing you what you're computing but then you'll", "tokens": [1081, 370, 264, 1333, 295, 707, 2182, 1105, 3547, 366, 4099, 291, 437, 291, 434, 15866, 457, 550, 291, 603], "temperature": 0.0, "avg_logprob": -0.0751366933186849, "compression_ratio": 2.1714285714285713, "no_speech_prob": 7.29186740500154e-06}, {"id": 506, "seek": 386668, "start": 3881.08, "end": 3887.08, "text": " slide it over to the next position and fill in this cell and then you'll slide over the next", "tokens": [4137, 309, 670, 281, 264, 958, 2535, 293, 2836, 294, 341, 2815, 293, 550, 291, 603, 4137, 670, 264, 958], "temperature": 0.0, "avg_logprob": -0.0751366933186849, "compression_ratio": 2.1714285714285713, "no_speech_prob": 7.29186740500154e-06}, {"id": 507, "seek": 386668, "start": 3887.08, "end": 3893.8799999999997, "text": " position and fill in this cell and then you'll slide it down and fill in this cell and so you've", "tokens": [2535, 293, 2836, 294, 341, 2815, 293, 550, 291, 603, 4137, 309, 760, 293, 2836, 294, 341, 2815, 293, 370, 291, 600], "temperature": 0.0, "avg_logprob": -0.0751366933186849, "compression_ratio": 2.1714285714285713, "no_speech_prob": 7.29186740500154e-06}, {"id": 508, "seek": 389388, "start": 3893.88, "end": 3900.84, "text": " got this sort of little function of a patch which you're sliding over your image and computing a", "tokens": [658, 341, 1333, 295, 707, 2445, 295, 257, 9972, 597, 291, 434, 21169, 670, 428, 3256, 293, 15866, 257], "temperature": 0.0, "avg_logprob": -0.07201524113499841, "compression_ratio": 1.7244444444444444, "no_speech_prob": 8.933081517170649e-06}, {"id": 509, "seek": 389388, "start": 3900.84, "end": 3908.2000000000003, "text": " convolution which is just um a dot product effectively um that you're then using to get an extra", "tokens": [45216, 597, 307, 445, 1105, 257, 5893, 1674, 8659, 1105, 300, 291, 434, 550, 1228, 281, 483, 364, 2857], "temperature": 0.0, "avg_logprob": -0.07201524113499841, "compression_ratio": 1.7244444444444444, "no_speech_prob": 8.933081517170649e-06}, {"id": 510, "seek": 389388, "start": 3908.2000000000003, "end": 3915.08, "text": " layer of representation and so by sliding things over you can pick out features and you've got a", "tokens": [4583, 295, 10290, 293, 370, 538, 21169, 721, 670, 291, 393, 1888, 484, 4122, 293, 291, 600, 658, 257], "temperature": 0.0, "avg_logprob": -0.07201524113499841, "compression_ratio": 1.7244444444444444, "no_speech_prob": 8.933081517170649e-06}, {"id": 511, "seek": 389388, "start": 3915.08, "end": 3923.56, "text": " sort of a feature identifier that runs across every piece of the image um well for language we've", "tokens": [1333, 295, 257, 4111, 45690, 300, 6676, 2108, 633, 2522, 295, 264, 3256, 1105, 731, 337, 2856, 321, 600], "temperature": 0.0, "avg_logprob": -0.07201524113499841, "compression_ratio": 1.7244444444444444, "no_speech_prob": 8.933081517170649e-06}, {"id": 512, "seek": 392356, "start": 3923.56, "end": 3930.52, "text": " just got a sequence but you can do basically the same thing and what you then have is a 1D", "tokens": [445, 658, 257, 8310, 457, 291, 393, 360, 1936, 264, 912, 551, 293, 437, 291, 550, 362, 307, 257, 502, 35], "temperature": 0.0, "avg_logprob": -0.07002412739084728, "compression_ratio": 1.5706214689265536, "no_speech_prob": 3.102745904470794e-05}, {"id": 513, "seek": 392356, "start": 3930.52, "end": 3937.32, "text": " convolution for text so if here's my sentence tentative deal reach to keep the government open", "tokens": [45216, 337, 2487, 370, 498, 510, 311, 452, 8174, 7054, 1166, 2028, 2524, 281, 1066, 264, 2463, 1269], "temperature": 0.0, "avg_logprob": -0.07002412739084728, "compression_ratio": 1.5706214689265536, "no_speech_prob": 3.102745904470794e-05}, {"id": 514, "seek": 392356, "start": 3937.32, "end": 3947.56, "text": " that what I can do is have um so these words have uh word representation which so this is my", "tokens": [300, 437, 286, 393, 360, 307, 362, 1105, 370, 613, 2283, 362, 2232, 1349, 10290, 597, 370, 341, 307, 452], "temperature": 0.0, "avg_logprob": -0.07002412739084728, "compression_ratio": 1.5706214689265536, "no_speech_prob": 3.102745904470794e-05}, {"id": 515, "seek": 394756, "start": 3947.56, "end": 3956.36, "text": " vector for each word and then I can have a filter sometimes called a kernel which I use for my", "tokens": [8062, 337, 1184, 1349, 293, 550, 286, 393, 362, 257, 6608, 2171, 1219, 257, 28256, 597, 286, 764, 337, 452], "temperature": 0.0, "avg_logprob": -0.08402933245119841, "compression_ratio": 1.737327188940092, "no_speech_prob": 4.8264228098560125e-05}, {"id": 516, "seek": 394756, "start": 3956.36, "end": 3963.0, "text": " convolution and what I'm going to do is slide that down the text so I can start with it with the", "tokens": [45216, 293, 437, 286, 478, 516, 281, 360, 307, 4137, 300, 760, 264, 2487, 370, 286, 393, 722, 365, 309, 365, 264], "temperature": 0.0, "avg_logprob": -0.08402933245119841, "compression_ratio": 1.737327188940092, "no_speech_prob": 4.8264228098560125e-05}, {"id": 517, "seek": 394756, "start": 3963.0, "end": 3971.08, "text": " first three words and then I sort of treat them as sort of elements I can dot product and some", "tokens": [700, 1045, 2283, 293, 550, 286, 1333, 295, 2387, 552, 382, 1333, 295, 4959, 286, 393, 5893, 1674, 293, 512], "temperature": 0.0, "avg_logprob": -0.08402933245119841, "compression_ratio": 1.737327188940092, "no_speech_prob": 4.8264228098560125e-05}, {"id": 518, "seek": 397108, "start": 3971.08, "end": 3978.04, "text": " and then I can compute a value as to what they all add up to which is minus one it turns out um and", "tokens": [293, 550, 286, 393, 14722, 257, 2158, 382, 281, 437, 436, 439, 909, 493, 281, 597, 307, 3175, 472, 309, 4523, 484, 1105, 293], "temperature": 0.0, "avg_logprob": -0.09343467614589593, "compression_ratio": 1.7065868263473054, "no_speech_prob": 7.759813343000133e-06}, {"id": 519, "seek": 397108, "start": 3978.04, "end": 3984.92, "text": " so then I might have a bias that I add on and get an updated value if my bias is plus one", "tokens": [370, 550, 286, 1062, 362, 257, 12577, 300, 286, 909, 322, 293, 483, 364, 10588, 2158, 498, 452, 12577, 307, 1804, 472], "temperature": 0.0, "avg_logprob": -0.09343467614589593, "compression_ratio": 1.7065868263473054, "no_speech_prob": 7.759813343000133e-06}, {"id": 520, "seek": 397108, "start": 3987.7999999999997, "end": 3993.96, "text": " and then I'd run it through a nonlinearity and that will give me a final value um and then I'll", "tokens": [293, 550, 286, 1116, 1190, 309, 807, 257, 2107, 1889, 17409, 293, 300, 486, 976, 385, 257, 2572, 2158, 1105, 293, 550, 286, 603], "temperature": 0.0, "avg_logprob": -0.09343467614589593, "compression_ratio": 1.7065868263473054, "no_speech_prob": 7.759813343000133e-06}, {"id": 521, "seek": 399396, "start": 3993.96, "end": 4004.04, "text": " slide my filter down and I'd work out um a computation for this window of three words and take 0.5", "tokens": [4137, 452, 6608, 760, 293, 286, 1116, 589, 484, 1105, 257, 24903, 337, 341, 4910, 295, 1045, 2283, 293, 747, 1958, 13, 20], "temperature": 0.0, "avg_logprob": -0.07258785963058471, "compression_ratio": 1.5846994535519126, "no_speech_prob": 8.011537829588633e-06}, {"id": 522, "seek": 399396, "start": 4004.04, "end": 4013.32, "text": " times 3 plus 0.2 times 1 etc and that comes out as this value um I add the bias I put it I'm", "tokens": [1413, 805, 1804, 1958, 13, 17, 1413, 502, 5183, 293, 300, 1487, 484, 382, 341, 2158, 1105, 286, 909, 264, 12577, 286, 829, 309, 286, 478], "temperature": 0.0, "avg_logprob": -0.07258785963058471, "compression_ratio": 1.5846994535519126, "no_speech_prob": 8.011537829588633e-06}, {"id": 523, "seek": 399396, "start": 4013.32, "end": 4019.7200000000003, "text": " going to put it through my nonlinearity and then I keep on sliding down and I'll do the next three", "tokens": [516, 281, 829, 309, 807, 452, 2107, 1889, 17409, 293, 550, 286, 1066, 322, 21169, 760, 293, 286, 603, 360, 264, 958, 1045], "temperature": 0.0, "avg_logprob": -0.07258785963058471, "compression_ratio": 1.5846994535519126, "no_speech_prob": 8.011537829588633e-06}, {"id": 524, "seek": 401972, "start": 4019.72, "end": 4027.3999999999996, "text": " words and keep on going down and so that gives me a 1D convolution and computes a representation of", "tokens": [2283, 293, 1066, 322, 516, 760, 293, 370, 300, 2709, 385, 257, 502, 35, 45216, 293, 715, 1819, 257, 10290, 295], "temperature": 0.0, "avg_logprob": -0.09311353459077723, "compression_ratio": 1.6021505376344085, "no_speech_prob": 2.1748935978394002e-05}, {"id": 525, "seek": 401972, "start": 4027.3999999999996, "end": 4036.52, "text": " the text um you might have noticed in the previous example that I started here with seven words", "tokens": [264, 2487, 1105, 291, 1062, 362, 5694, 294, 264, 3894, 1365, 300, 286, 1409, 510, 365, 3407, 2283], "temperature": 0.0, "avg_logprob": -0.09311353459077723, "compression_ratio": 1.6021505376344085, "no_speech_prob": 2.1748935978394002e-05}, {"id": 526, "seek": 401972, "start": 4036.52, "end": 4044.4399999999996, "text": " but because I wanted to have a window of three for my convolution the end result is that things shrunk", "tokens": [457, 570, 286, 1415, 281, 362, 257, 4910, 295, 1045, 337, 452, 45216, 264, 917, 1874, 307, 300, 721, 9884, 3197], "temperature": 0.0, "avg_logprob": -0.09311353459077723, "compression_ratio": 1.6021505376344085, "no_speech_prob": 2.1748935978394002e-05}, {"id": 527, "seek": 404444, "start": 4044.44, "end": 4051.2400000000002, "text": " so in the output I only had five things that's not necessarily desirable so commonly people will", "tokens": [370, 294, 264, 5598, 286, 787, 632, 1732, 721, 300, 311, 406, 4725, 30533, 370, 12719, 561, 486], "temperature": 0.0, "avg_logprob": -0.11414980552565883, "compression_ratio": 1.6685714285714286, "no_speech_prob": 7.405072665278567e-06}, {"id": 528, "seek": 404444, "start": 4051.2400000000002, "end": 4059.88, "text": " deal with that um with padding so if I put padding on both sides I can then start my 3 by 3 convolution", "tokens": [2028, 365, 300, 1105, 365, 39562, 370, 498, 286, 829, 39562, 322, 1293, 4881, 286, 393, 550, 722, 452, 805, 538, 805, 45216], "temperature": 0.0, "avg_logprob": -0.11414980552565883, "compression_ratio": 1.6685714285714286, "no_speech_prob": 7.405072665278567e-06}, {"id": 529, "seek": 404444, "start": 4059.88, "end": 4068.68, "text": " my 3 so not 3 by 3 my 3 convolution um here and compute this one and then slide it down one", "tokens": [452, 805, 370, 406, 805, 538, 805, 452, 805, 45216, 1105, 510, 293, 14722, 341, 472, 293, 550, 4137, 309, 760, 472], "temperature": 0.0, "avg_logprob": -0.11414980552565883, "compression_ratio": 1.6685714285714286, "no_speech_prob": 7.405072665278567e-06}, {"id": 530, "seek": 406868, "start": 4068.68, "end": 4076.8399999999997, "text": " and compute this one and so now my output is the same size as my real input and so that's a", "tokens": [293, 14722, 341, 472, 293, 370, 586, 452, 5598, 307, 264, 912, 2744, 382, 452, 957, 4846, 293, 370, 300, 311, 257], "temperature": 0.0, "avg_logprob": -0.05201456440028859, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.968784979719203e-06}, {"id": 531, "seek": 406868, "start": 4076.8399999999997, "end": 4086.7599999999998, "text": " convolution with padding um okay so that was the start of things but you know how you get more", "tokens": [45216, 365, 39562, 1105, 1392, 370, 300, 390, 264, 722, 295, 721, 457, 291, 458, 577, 291, 483, 544], "temperature": 0.0, "avg_logprob": -0.05201456440028859, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.968784979719203e-06}, {"id": 532, "seek": 406868, "start": 4086.7599999999998, "end": 4093.48, "text": " power of the convolutional network is you don't only have one filter you have several filters", "tokens": [1347, 295, 264, 45216, 304, 3209, 307, 291, 500, 380, 787, 362, 472, 6608, 291, 362, 2940, 15995], "temperature": 0.0, "avg_logprob": -0.05201456440028859, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.968784979719203e-06}, {"id": 533, "seek": 409348, "start": 4093.48, "end": 4099.32, "text": " so if I have three filters each of which will have their own bias nonlinearity I can then get", "tokens": [370, 498, 286, 362, 1045, 15995, 1184, 295, 597, 486, 362, 641, 1065, 12577, 2107, 1889, 17409, 286, 393, 550, 483], "temperature": 0.0, "avg_logprob": -0.12914505871859464, "compression_ratio": 1.5932203389830508, "no_speech_prob": 9.362322998640593e-06}, {"id": 534, "seek": 409348, "start": 4099.96, "end": 4106.28, "text": " a three-dimensional representation coming out the end and sort of you can think of these as", "tokens": [257, 1045, 12, 18759, 10290, 1348, 484, 264, 917, 293, 1333, 295, 291, 393, 519, 295, 613, 382], "temperature": 0.0, "avg_logprob": -0.12914505871859464, "compression_ratio": 1.5932203389830508, "no_speech_prob": 9.362322998640593e-06}, {"id": 535, "seek": 409348, "start": 4106.28, "end": 4115.72, "text": " conceptually computing different features of your text okay so that gives us a kind of a sort of", "tokens": [3410, 671, 15866, 819, 4122, 295, 428, 2487, 1392, 370, 300, 2709, 505, 257, 733, 295, 257, 1333, 295], "temperature": 0.0, "avg_logprob": -0.12914505871859464, "compression_ratio": 1.5932203389830508, "no_speech_prob": 9.362322998640593e-06}, {"id": 536, "seek": 411572, "start": 4115.72, "end": 4125.320000000001, "text": " a new feature re-representation of our text but commonly we then want to somehow summarize", "tokens": [257, 777, 4111, 319, 12, 19919, 11662, 399, 295, 527, 2487, 457, 12719, 321, 550, 528, 281, 6063, 20858], "temperature": 0.0, "avg_logprob": -0.09888145105162663, "compression_ratio": 1.6467065868263473, "no_speech_prob": 8.930765943659935e-06}, {"id": 537, "seek": 411572, "start": 4125.320000000001, "end": 4133.96, "text": " what we have and a very common way of summarizing what we have is to then do pooling so", "tokens": [437, 321, 362, 293, 257, 588, 2689, 636, 295, 14611, 3319, 437, 321, 362, 307, 281, 550, 360, 7005, 278, 370], "temperature": 0.0, "avg_logprob": -0.09888145105162663, "compression_ratio": 1.6467065868263473, "no_speech_prob": 8.930765943659935e-06}, {"id": 538, "seek": 411572, "start": 4135.320000000001, "end": 4140.84, "text": " if we sort of think of these features as detecting different things in the text so you know they", "tokens": [498, 321, 1333, 295, 519, 295, 613, 4122, 382, 40237, 819, 721, 294, 264, 2487, 370, 291, 458, 436], "temperature": 0.0, "avg_logprob": -0.09888145105162663, "compression_ratio": 1.6467065868263473, "no_speech_prob": 8.930765943659935e-06}, {"id": 539, "seek": 414084, "start": 4140.84, "end": 4149.32, "text": " might even be high-level features like you know does this show signs of toxicity or hate speech", "tokens": [1062, 754, 312, 1090, 12, 12418, 4122, 411, 291, 458, 775, 341, 855, 7880, 295, 45866, 420, 4700, 6218], "temperature": 0.0, "avg_logprob": -0.10166972024100167, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.074438387149712e-05}, {"id": 540, "seek": 414084, "start": 4150.12, "end": 4155.88, "text": " is there reference to something so if you want to be interested and doesn't occur anywhere in the", "tokens": [307, 456, 6408, 281, 746, 370, 498, 291, 528, 281, 312, 3102, 293, 1177, 380, 5160, 4992, 294, 264], "temperature": 0.0, "avg_logprob": -0.10166972024100167, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.074438387149712e-05}, {"id": 541, "seek": 414084, "start": 4155.88, "end": 4163.24, "text": " text what people often then do as a max pooling operation where for each feature they simply", "tokens": [2487, 437, 561, 2049, 550, 360, 382, 257, 11469, 7005, 278, 6916, 689, 337, 1184, 4111, 436, 2935], "temperature": 0.0, "avg_logprob": -0.10166972024100167, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.074438387149712e-05}, {"id": 542, "seek": 414084, "start": 4163.24, "end": 4170.2, "text": " sort of compute the maximum value it ever achieved in any position as you went through the text", "tokens": [1333, 295, 14722, 264, 6674, 2158, 309, 1562, 11042, 294, 604, 2535, 382, 291, 1437, 807, 264, 2487], "temperature": 0.0, "avg_logprob": -0.10166972024100167, "compression_ratio": 1.6754385964912282, "no_speech_prob": 1.074438387149712e-05}, {"id": 543, "seek": 417020, "start": 4170.2, "end": 4176.92, "text": " and say that this vector ends up as the sentence representation sometimes for other purposes rather", "tokens": [293, 584, 300, 341, 8062, 5314, 493, 382, 264, 8174, 10290, 2171, 337, 661, 9932, 2831], "temperature": 0.0, "avg_logprob": -0.06818337440490722, "compression_ratio": 1.8169014084507042, "no_speech_prob": 1.5901665392448194e-05}, {"id": 544, "seek": 417020, "start": 4176.92, "end": 4182.36, "text": " than max pooling people use average pooling where you take the averages of the different vectors", "tokens": [813, 11469, 7005, 278, 561, 764, 4274, 7005, 278, 689, 291, 747, 264, 42257, 295, 264, 819, 18875], "temperature": 0.0, "avg_logprob": -0.06818337440490722, "compression_ratio": 1.8169014084507042, "no_speech_prob": 1.5901665392448194e-05}, {"id": 545, "seek": 417020, "start": 4183.32, "end": 4189.72, "text": " to get the sentence representation then general max pooling has been found to be more successful", "tokens": [281, 483, 264, 8174, 10290, 550, 2674, 11469, 7005, 278, 575, 668, 1352, 281, 312, 544, 4406], "temperature": 0.0, "avg_logprob": -0.06818337440490722, "compression_ratio": 1.8169014084507042, "no_speech_prob": 1.5901665392448194e-05}, {"id": 546, "seek": 417020, "start": 4189.72, "end": 4195.0, "text": " and that's kind of because if you think of it as feature detectors that are wanting to detect", "tokens": [293, 300, 311, 733, 295, 570, 498, 291, 519, 295, 309, 382, 4111, 46866, 300, 366, 7935, 281, 5531], "temperature": 0.0, "avg_logprob": -0.06818337440490722, "compression_ratio": 1.8169014084507042, "no_speech_prob": 1.5901665392448194e-05}, {"id": 547, "seek": 419500, "start": 4195.0, "end": 4201.24, "text": " was this present somewhere then it you know something like positive sentiment isn't going to be", "tokens": [390, 341, 1974, 4079, 550, 309, 291, 458, 746, 411, 3353, 16149, 1943, 380, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.12046080430348714, "compression_ratio": 1.654970760233918, "no_speech_prob": 4.6002398448763415e-05}, {"id": 548, "seek": 419500, "start": 4202.2, "end": 4209.48, "text": " present in every three word subsequent you choose before us there somewhere is there and so", "tokens": [1974, 294, 633, 1045, 1349, 19962, 291, 2826, 949, 505, 456, 4079, 307, 456, 293, 370], "temperature": 0.0, "avg_logprob": -0.12046080430348714, "compression_ratio": 1.654970760233918, "no_speech_prob": 4.6002398448763415e-05}, {"id": 549, "seek": 419500, "start": 4209.48, "end": 4219.24, "text": " often max pooling works better and so that's a very quick look at convolutional neural networks", "tokens": [2049, 11469, 7005, 278, 1985, 1101, 293, 370, 300, 311, 257, 588, 1702, 574, 412, 45216, 304, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.12046080430348714, "compression_ratio": 1.654970760233918, "no_speech_prob": 4.6002398448763415e-05}, {"id": 550, "seek": 421924, "start": 4219.24, "end": 4227.16, "text": " except to say this example was doing 1D convolutions with words but a very common place that", "tokens": [3993, 281, 584, 341, 1365, 390, 884, 502, 35, 3754, 15892, 365, 2283, 457, 257, 588, 2689, 1081, 300], "temperature": 0.0, "avg_logprob": -0.13045868506798378, "compression_ratio": 1.6704545454545454, "no_speech_prob": 2.31549893214833e-05}, {"id": 551, "seek": 421924, "start": 4227.16, "end": 4233.88, "text": " convolutional neural networks being used in natural language is actually using them with characters", "tokens": [45216, 304, 18161, 9590, 885, 1143, 294, 3303, 2856, 307, 767, 1228, 552, 365, 4342], "temperature": 0.0, "avg_logprob": -0.13045868506798378, "compression_ratio": 1.6704545454545454, "no_speech_prob": 2.31549893214833e-05}, {"id": 552, "seek": 421924, "start": 4233.88, "end": 4242.28, "text": " and so what you can do is you can do convolutions over subsequences of the characters in the same way", "tokens": [293, 370, 437, 291, 393, 360, 307, 291, 393, 360, 3754, 15892, 670, 1422, 11834, 2667, 295, 264, 4342, 294, 264, 912, 636], "temperature": 0.0, "avg_logprob": -0.13045868506798378, "compression_ratio": 1.6704545454545454, "no_speech_prob": 2.31549893214833e-05}, {"id": 553, "seek": 424228, "start": 4242.28, "end": 4249.32, "text": " and if you do that this allows you to compute a representation for any sequence of characters", "tokens": [293, 498, 291, 360, 300, 341, 4045, 291, 281, 14722, 257, 10290, 337, 604, 8310, 295, 4342], "temperature": 0.0, "avg_logprob": -0.16470688204222086, "compression_ratio": 1.8761904761904762, "no_speech_prob": 3.216114055248909e-05}, {"id": 554, "seek": 424228, "start": 4249.32, "end": 4256.36, "text": " so you don't have any problems with being out of vocabulary or anything like that because for any", "tokens": [370, 291, 500, 380, 362, 604, 2740, 365, 885, 484, 295, 19864, 420, 1340, 411, 300, 570, 337, 604], "temperature": 0.0, "avg_logprob": -0.16470688204222086, "compression_ratio": 1.8761904761904762, "no_speech_prob": 3.216114055248909e-05}, {"id": 555, "seek": 424228, "start": 4256.36, "end": 4262.679999999999, "text": " sequence of characters you just compute your convolutional representation and max pool word and so", "tokens": [8310, 295, 4342, 291, 445, 14722, 428, 45216, 304, 10290, 293, 11469, 7005, 1349, 293, 370], "temperature": 0.0, "avg_logprob": -0.16470688204222086, "compression_ratio": 1.8761904761904762, "no_speech_prob": 3.216114055248909e-05}, {"id": 556, "seek": 424228, "start": 4262.679999999999, "end": 4271.48, "text": " quite commonly people use a character convolution to give a representation of words perhaps as the only", "tokens": [1596, 12719, 561, 764, 257, 2517, 45216, 281, 976, 257, 10290, 295, 2283, 4317, 382, 264, 787], "temperature": 0.0, "avg_logprob": -0.16470688204222086, "compression_ratio": 1.8761904761904762, "no_speech_prob": 3.216114055248909e-05}, {"id": 557, "seek": 427148, "start": 4271.48, "end": 4280.2, "text": " representation of words but otherwise is something that you use in addition to a word vector and so", "tokens": [10290, 295, 2283, 457, 5911, 307, 746, 300, 291, 764, 294, 4500, 281, 257, 1349, 8062, 293, 370], "temperature": 0.0, "avg_logprob": -0.17265165873936245, "compression_ratio": 1.7039106145251397, "no_speech_prob": 7.023054786259308e-05}, {"id": 558, "seek": 427148, "start": 4280.2, "end": 4286.919999999999, "text": " in both bydaph in the model I'm about to show that at the base level it makes use of both a word vector", "tokens": [294, 1293, 538, 67, 13957, 294, 264, 2316, 286, 478, 466, 281, 855, 300, 412, 264, 3096, 1496, 309, 1669, 764, 295, 1293, 257, 1349, 8062], "temperature": 0.0, "avg_logprob": -0.17265165873936245, "compression_ratio": 1.7039106145251397, "no_speech_prob": 7.023054786259308e-05}, {"id": 559, "seek": 427148, "start": 4286.919999999999, "end": 4293.48, "text": " representation that we see saw right at the beginning of the text and a character level convolutional", "tokens": [10290, 300, 321, 536, 1866, 558, 412, 264, 2863, 295, 264, 2487, 293, 257, 2517, 1496, 45216, 304], "temperature": 0.0, "avg_logprob": -0.17265165873936245, "compression_ratio": 1.7039106145251397, "no_speech_prob": 7.023054786259308e-05}, {"id": 560, "seek": 429348, "start": 4293.48, "end": 4302.04, "text": " representation of the words okay with that said I now want to show you before time runs out", "tokens": [10290, 295, 264, 2283, 1392, 365, 300, 848, 286, 586, 528, 281, 855, 291, 949, 565, 6676, 484], "temperature": 0.0, "avg_logprob": -0.18175945281982422, "compression_ratio": 1.651063829787234, "no_speech_prob": 2.077277713397052e-05}, {"id": 561, "seek": 429348, "start": 4302.04, "end": 4308.04, "text": " an end-to-end neural co-refer system model so the model I'm going to show you is Kenton Lee's one", "tokens": [364, 917, 12, 1353, 12, 521, 18161, 598, 12, 265, 612, 1185, 2316, 370, 264, 2316, 286, 478, 516, 281, 855, 291, 307, 15843, 266, 6957, 311, 472], "temperature": 0.0, "avg_logprob": -0.18175945281982422, "compression_ratio": 1.651063829787234, "no_speech_prob": 2.077277713397052e-05}, {"id": 562, "seek": 429348, "start": 4308.04, "end": 4315.48, "text": " from so Dunn University of Washington in 2017 this is no longer the state of the art I'll mention", "tokens": [490, 370, 11959, 77, 3535, 295, 6149, 294, 6591, 341, 307, 572, 2854, 264, 1785, 295, 264, 1523, 286, 603, 2152], "temperature": 0.0, "avg_logprob": -0.18175945281982422, "compression_ratio": 1.651063829787234, "no_speech_prob": 2.077277713397052e-05}, {"id": 563, "seek": 429348, "start": 4315.48, "end": 4322.839999999999, "text": " the state of the art at the end but this was the first model that really said get rid of all of that", "tokens": [264, 1785, 295, 264, 1523, 412, 264, 917, 457, 341, 390, 264, 700, 2316, 300, 534, 848, 483, 3973, 295, 439, 295, 300], "temperature": 0.0, "avg_logprob": -0.18175945281982422, "compression_ratio": 1.651063829787234, "no_speech_prob": 2.077277713397052e-05}, {"id": 564, "seek": 432284, "start": 4322.84, "end": 4329.56, "text": " old stuff of having pipelines and mention detection first just build one end-to-end big model that", "tokens": [1331, 1507, 295, 1419, 40168, 293, 2152, 17784, 700, 445, 1322, 472, 917, 12, 1353, 12, 521, 955, 2316, 300], "temperature": 0.0, "avg_logprob": -0.13321152511908083, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.247694884426892e-05}, {"id": 565, "seek": 432284, "start": 4329.56, "end": 4336.4400000000005, "text": " does everything in returns co-reference so it's a good one to show so compared to the earliest", "tokens": [775, 1203, 294, 11247, 598, 12, 265, 5158, 370, 309, 311, 257, 665, 472, 281, 855, 370, 5347, 281, 264, 20573], "temperature": 0.0, "avg_logprob": -0.13321152511908083, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.247694884426892e-05}, {"id": 566, "seek": 432284, "start": 4336.4400000000005, "end": 4342.92, "text": " simple thing I saw we're now going to process the text with bylistians we're going to make use", "tokens": [2199, 551, 286, 1866, 321, 434, 586, 516, 281, 1399, 264, 2487, 365, 538, 75, 468, 2567, 321, 434, 516, 281, 652, 764], "temperature": 0.0, "avg_logprob": -0.13321152511908083, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.247694884426892e-05}, {"id": 567, "seek": 432284, "start": 4342.92, "end": 4349.64, "text": " of attention and we're going to do all of mention detection co-reference in one step end-to-end", "tokens": [295, 3202, 293, 321, 434, 516, 281, 360, 439, 295, 2152, 17784, 598, 12, 265, 5158, 294, 472, 1823, 917, 12, 1353, 12, 521], "temperature": 0.0, "avg_logprob": -0.13321152511908083, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.247694884426892e-05}, {"id": 568, "seek": 434964, "start": 4349.64, "end": 4357.4800000000005, "text": " and the way it does that is by considering every span of the text up to a certain length as a", "tokens": [293, 264, 636, 309, 775, 300, 307, 538, 8079, 633, 16174, 295, 264, 2487, 493, 281, 257, 1629, 4641, 382, 257], "temperature": 0.0, "avg_logprob": -0.09381112087978406, "compression_ratio": 1.6535087719298245, "no_speech_prob": 2.4670149286976084e-05}, {"id": 569, "seek": 434964, "start": 4357.4800000000005, "end": 4363.0, "text": " candidate mentioned and just figures out a representation for it and whether it's co-referent to", "tokens": [11532, 2835, 293, 445, 9624, 484, 257, 10290, 337, 309, 293, 1968, 309, 311, 598, 12, 265, 612, 317, 281], "temperature": 0.0, "avg_logprob": -0.09381112087978406, "compression_ratio": 1.6535087719298245, "no_speech_prob": 2.4670149286976084e-05}, {"id": 570, "seek": 434964, "start": 4363.0, "end": 4370.68, "text": " other things so what we do at the start is we start with the sequence of words and we calculate", "tokens": [661, 721, 370, 437, 321, 360, 412, 264, 722, 307, 321, 722, 365, 264, 8310, 295, 2283, 293, 321, 8873], "temperature": 0.0, "avg_logprob": -0.09381112087978406, "compression_ratio": 1.6535087719298245, "no_speech_prob": 2.4670149286976084e-05}, {"id": 571, "seek": 434964, "start": 4370.68, "end": 4379.240000000001, "text": " from those standard word embedding and a character level CNNs embedding we then feed those", "tokens": [490, 729, 3832, 1349, 12240, 3584, 293, 257, 2517, 1496, 24859, 82, 12240, 3584, 321, 550, 3154, 729], "temperature": 0.0, "avg_logprob": -0.09381112087978406, "compression_ratio": 1.6535087719298245, "no_speech_prob": 2.4670149286976084e-05}, {"id": 572, "seek": 437924, "start": 4379.24, "end": 4388.44, "text": " as inputs into a bidirectional LSTM of the kind that we saw quite a lot of before but then", "tokens": [382, 15743, 666, 257, 12957, 621, 41048, 441, 6840, 44, 295, 264, 733, 300, 321, 1866, 1596, 257, 688, 295, 949, 457, 550], "temperature": 0.0, "avg_logprob": -0.059543180465698245, "compression_ratio": 1.6705882352941177, "no_speech_prob": 3.694098995765671e-05}, {"id": 573, "seek": 437924, "start": 4388.44, "end": 4397.639999999999, "text": " after this what we do is we compute representations for spans so when we have a sequence of words", "tokens": [934, 341, 437, 321, 360, 307, 321, 14722, 33358, 337, 44086, 370, 562, 321, 362, 257, 8310, 295, 2283], "temperature": 0.0, "avg_logprob": -0.059543180465698245, "compression_ratio": 1.6705882352941177, "no_speech_prob": 3.694098995765671e-05}, {"id": 574, "seek": 437924, "start": 4398.679999999999, "end": 4404.2, "text": " we're then going to work out a representation of a sequence of words which we can then put into", "tokens": [321, 434, 550, 516, 281, 589, 484, 257, 10290, 295, 257, 8310, 295, 2283, 597, 321, 393, 550, 829, 666], "temperature": 0.0, "avg_logprob": -0.059543180465698245, "compression_ratio": 1.6705882352941177, "no_speech_prob": 3.694098995765671e-05}, {"id": 575, "seek": 440420, "start": 4404.2, "end": 4414.44, "text": " our co-reference model so that we I can't fully illustrate in this picture but so sub sequences of", "tokens": [527, 598, 12, 265, 5158, 2316, 370, 300, 321, 286, 393, 380, 4498, 23221, 294, 341, 3036, 457, 370, 1422, 22978, 295], "temperature": 0.0, "avg_logprob": -0.13005053107418232, "compression_ratio": 1.560846560846561, "no_speech_prob": 3.154161822749302e-05}, {"id": 576, "seek": 440420, "start": 4414.44, "end": 4421.24, "text": " different lengths so like general electric general electric said we'll all have a span representation", "tokens": [819, 26329, 370, 411, 2674, 5210, 2674, 5210, 848, 321, 603, 439, 362, 257, 16174, 10290], "temperature": 0.0, "avg_logprob": -0.13005053107418232, "compression_ratio": 1.560846560846561, "no_speech_prob": 3.154161822749302e-05}, {"id": 577, "seek": 440420, "start": 4421.24, "end": 4428.84, "text": " which I've only shown a subset of them in green so how are those computed well the way they're", "tokens": [597, 286, 600, 787, 4898, 257, 25993, 295, 552, 294, 3092, 370, 577, 366, 729, 40610, 731, 264, 636, 436, 434], "temperature": 0.0, "avg_logprob": -0.13005053107418232, "compression_ratio": 1.560846560846561, "no_speech_prob": 3.154161822749302e-05}, {"id": 578, "seek": 442884, "start": 4428.84, "end": 4436.68, "text": " computed is that the span representation is a vector that can catenate several vectors and it", "tokens": [40610, 307, 300, 264, 16174, 10290, 307, 257, 8062, 300, 393, 3857, 268, 473, 2940, 18875, 293, 309], "temperature": 0.0, "avg_logprob": -0.12491501552957884, "compression_ratio": 1.9294871794871795, "no_speech_prob": 1.9794526451732963e-05}, {"id": 579, "seek": 442884, "start": 4436.68, "end": 4447.96, "text": " consists of four parts it consists of the representation that was computed for the start of the span from", "tokens": [14689, 295, 1451, 3166, 309, 14689, 295, 264, 10290, 300, 390, 40610, 337, 264, 722, 295, 264, 16174, 490], "temperature": 0.0, "avg_logprob": -0.12491501552957884, "compression_ratio": 1.9294871794871795, "no_speech_prob": 1.9794526451732963e-05}, {"id": 580, "seek": 442884, "start": 4447.96, "end": 4457.0, "text": " the bio LSTM the representation of the end from the bio LSTM that's over here and then it has a third", "tokens": [264, 12198, 441, 6840, 44, 264, 10290, 295, 264, 917, 490, 264, 12198, 441, 6840, 44, 300, 311, 670, 510, 293, 550, 309, 575, 257, 2636], "temperature": 0.0, "avg_logprob": -0.12491501552957884, "compression_ratio": 1.9294871794871795, "no_speech_prob": 1.9794526451732963e-05}, {"id": 581, "seek": 445700, "start": 4457.0, "end": 4464.2, "text": " part that's kind of interesting this is an intention based representation that is calculated from", "tokens": [644, 300, 311, 733, 295, 1880, 341, 307, 364, 7789, 2361, 10290, 300, 307, 15598, 490], "temperature": 0.0, "avg_logprob": -0.12341539977026768, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.065306736971252e-05}, {"id": 582, "seek": 445700, "start": 4464.2, "end": 4469.72, "text": " the whole span but particularly sort of looks for the head of a span and then there are still a", "tokens": [264, 1379, 16174, 457, 4098, 1333, 295, 1542, 337, 264, 1378, 295, 257, 16174, 293, 550, 456, 366, 920, 257], "temperature": 0.0, "avg_logprob": -0.12341539977026768, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.065306736971252e-05}, {"id": 583, "seek": 445700, "start": 4469.72, "end": 4476.44, "text": " few additional features so it turns out that you know some of these additional things like length", "tokens": [1326, 4497, 4122, 370, 309, 4523, 484, 300, 291, 458, 512, 295, 613, 4497, 721, 411, 4641], "temperature": 0.0, "avg_logprob": -0.12341539977026768, "compression_ratio": 1.6348314606741574, "no_speech_prob": 3.065306736971252e-05}, {"id": 584, "seek": 447644, "start": 4476.44, "end": 4487.96, "text": " and so on is still a bit useful so to work out the the final part is not the beginning and the end", "tokens": [293, 370, 322, 307, 920, 257, 857, 4420, 370, 281, 589, 484, 264, 264, 2572, 644, 307, 406, 264, 2863, 293, 264, 917], "temperature": 0.0, "avg_logprob": -0.12206918171473911, "compression_ratio": 1.6781609195402298, "no_speech_prob": 1.749154216668103e-05}, {"id": 585, "seek": 447644, "start": 4487.96, "end": 4493.32, "text": " what's done is to calculate an intention weighted average of the word embeddings so what you're", "tokens": [437, 311, 1096, 307, 281, 8873, 364, 7789, 32807, 4274, 295, 264, 1349, 12240, 29432, 370, 437, 291, 434], "temperature": 0.0, "avg_logprob": -0.12206918171473911, "compression_ratio": 1.6781609195402298, "no_speech_prob": 1.749154216668103e-05}, {"id": 586, "seek": 447644, "start": 4493.32, "end": 4501.48, "text": " doing is you're taking the x star representation of the final word of the span and you're feeding", "tokens": [884, 307, 291, 434, 1940, 264, 2031, 3543, 10290, 295, 264, 2572, 1349, 295, 264, 16174, 293, 291, 434, 12919], "temperature": 0.0, "avg_logprob": -0.12206918171473911, "compression_ratio": 1.6781609195402298, "no_speech_prob": 1.749154216668103e-05}, {"id": 587, "seek": 450148, "start": 4501.48, "end": 4511.08, "text": " that into a neural network to get attention scores for every word in the span which are these three", "tokens": [300, 666, 257, 18161, 3209, 281, 483, 3202, 13444, 337, 633, 1349, 294, 264, 16174, 597, 366, 613, 1045], "temperature": 0.0, "avg_logprob": -0.05922495372711666, "compression_ratio": 1.6723163841807909, "no_speech_prob": 3.370556441950612e-05}, {"id": 588, "seek": 450148, "start": 4511.08, "end": 4517.719999999999, "text": " and that's giving you an attention distribution as we've seen previously and then you're calculating", "tokens": [293, 300, 311, 2902, 291, 364, 3202, 7316, 382, 321, 600, 1612, 8046, 293, 550, 291, 434, 28258], "temperature": 0.0, "avg_logprob": -0.05922495372711666, "compression_ratio": 1.6723163841807909, "no_speech_prob": 3.370556441950612e-05}, {"id": 589, "seek": 450148, "start": 4517.719999999999, "end": 4529.32, "text": " the third component of this as an attention weighted sum of the different words in the span and", "tokens": [264, 2636, 6542, 295, 341, 382, 364, 3202, 32807, 2408, 295, 264, 819, 2283, 294, 264, 16174, 293], "temperature": 0.0, "avg_logprob": -0.05922495372711666, "compression_ratio": 1.6723163841807909, "no_speech_prob": 3.370556441950612e-05}, {"id": 590, "seek": 452932, "start": 4529.32, "end": 4534.36, "text": " so therefore you've got the sort of a sort of a soft average of the representations of the words of", "tokens": [370, 4412, 291, 600, 658, 264, 1333, 295, 257, 1333, 295, 257, 2787, 4274, 295, 264, 33358, 295, 264, 2283, 295], "temperature": 0.0, "avg_logprob": -0.09444258106288625, "compression_ratio": 1.8838709677419354, "no_speech_prob": 8.93131709744921e-06}, {"id": 591, "seek": 452932, "start": 4534.36, "end": 4548.599999999999, "text": " the span okay so then once you've got that what you're doing is then feeding these representations", "tokens": [264, 16174, 1392, 370, 550, 1564, 291, 600, 658, 300, 437, 291, 434, 884, 307, 550, 12919, 613, 33358], "temperature": 0.0, "avg_logprob": -0.09444258106288625, "compression_ratio": 1.8838709677419354, "no_speech_prob": 8.93131709744921e-06}, {"id": 592, "seek": 452932, "start": 4548.599999999999, "end": 4557.32, "text": " into having scores for whether spans are co-referent mentions so you have a representation of", "tokens": [666, 1419, 13444, 337, 1968, 44086, 366, 598, 12, 265, 612, 317, 23844, 370, 291, 362, 257, 10290, 295], "temperature": 0.0, "avg_logprob": -0.09444258106288625, "compression_ratio": 1.8838709677419354, "no_speech_prob": 8.93131709744921e-06}, {"id": 593, "seek": 455732, "start": 4557.32, "end": 4568.599999999999, "text": " the two spans you have a score that's calculated for whether two different spans look co-referent", "tokens": [264, 732, 44086, 291, 362, 257, 6175, 300, 311, 15598, 337, 1968, 732, 819, 44086, 574, 598, 12, 265, 612, 317], "temperature": 0.0, "avg_logprob": -0.1388942640121669, "compression_ratio": 1.715151515151515, "no_speech_prob": 3.3709744457155466e-05}, {"id": 594, "seek": 455732, "start": 4568.599999999999, "end": 4574.679999999999, "text": " and that overall you're getting a score for our different spans looking co-referent or not", "tokens": [293, 300, 4787, 291, 434, 1242, 257, 6175, 337, 527, 819, 44086, 1237, 598, 12, 265, 612, 317, 420, 406], "temperature": 0.0, "avg_logprob": -0.1388942640121669, "compression_ratio": 1.715151515151515, "no_speech_prob": 3.3709744457155466e-05}, {"id": 595, "seek": 455732, "start": 4576.28, "end": 4584.679999999999, "text": " and so this model is just run into a non-all spans now it sort of would get intractable if you", "tokens": [293, 370, 341, 2316, 307, 445, 1190, 666, 257, 2107, 12, 336, 44086, 586, 309, 1333, 295, 576, 483, 560, 1897, 712, 498, 291], "temperature": 0.0, "avg_logprob": -0.1388942640121669, "compression_ratio": 1.715151515151515, "no_speech_prob": 3.3709744457155466e-05}, {"id": 596, "seek": 458468, "start": 4584.68, "end": 4590.84, "text": " scored literally every span in a long piece of text so they do some pruning they sort of", "tokens": [18139, 3736, 633, 16174, 294, 257, 938, 2522, 295, 2487, 370, 436, 360, 512, 582, 37726, 436, 1333, 295], "temperature": 0.0, "avg_logprob": -0.08758290802560202, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.313930781383533e-05}, {"id": 597, "seek": 458468, "start": 4590.84, "end": 4598.52, "text": " only allow spans up to a certain maximum size they only consider pairs of spans that aren't too", "tokens": [787, 2089, 44086, 493, 281, 257, 1629, 6674, 2744, 436, 787, 1949, 15494, 295, 44086, 300, 3212, 380, 886], "temperature": 0.0, "avg_logprob": -0.08758290802560202, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.313930781383533e-05}, {"id": 598, "seek": 458468, "start": 4598.52, "end": 4606.360000000001, "text": " distant from each other etc etc but basically it's in sort of an approximation just a complete", "tokens": [17275, 490, 1184, 661, 5183, 5183, 457, 1936, 309, 311, 294, 1333, 295, 364, 28023, 445, 257, 3566], "temperature": 0.0, "avg_logprob": -0.08758290802560202, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.313930781383533e-05}, {"id": 599, "seek": 458468, "start": 4606.360000000001, "end": 4612.52, "text": " comparison of spans and this turns into a very effective co-reference resolution algorithm", "tokens": [9660, 295, 44086, 293, 341, 4523, 666, 257, 588, 4942, 598, 12, 265, 5158, 8669, 9284], "temperature": 0.0, "avg_logprob": -0.08758290802560202, "compression_ratio": 1.6591928251121075, "no_speech_prob": 2.313930781383533e-05}, {"id": 600, "seek": 461252, "start": 4612.52, "end": 4619.4800000000005, "text": " today it's not the best co-reference resolution algorithm because maybe not surprisingly like", "tokens": [965, 309, 311, 406, 264, 1151, 598, 12, 265, 5158, 8669, 9284, 570, 1310, 406, 17600, 411], "temperature": 0.0, "avg_logprob": -0.13722967814250164, "compression_ratio": 1.6519823788546255, "no_speech_prob": 6.0622376622632146e-05}, {"id": 601, "seek": 461252, "start": 4619.4800000000005, "end": 4625.400000000001, "text": " everything else that we've been dealing with there's now been these transformer models like", "tokens": [1203, 1646, 300, 321, 600, 668, 6260, 365, 456, 311, 586, 668, 613, 31782, 5245, 411], "temperature": 0.0, "avg_logprob": -0.13722967814250164, "compression_ratio": 1.6519823788546255, "no_speech_prob": 6.0622376622632146e-05}, {"id": 602, "seek": 461252, "start": 4625.400000000001, "end": 4632.76, "text": " BERT have come along and that they produce even better results so the best co-reference systems now", "tokens": [363, 31479, 362, 808, 2051, 293, 300, 436, 5258, 754, 1101, 3542, 370, 264, 1151, 598, 12, 265, 5158, 3652, 586], "temperature": 0.0, "avg_logprob": -0.13722967814250164, "compression_ratio": 1.6519823788546255, "no_speech_prob": 6.0622376622632146e-05}, {"id": 603, "seek": 461252, "start": 4632.76, "end": 4640.360000000001, "text": " have you make use of BERT in particular when Danchi spoke she briefly mentioned span BERT", "tokens": [362, 291, 652, 764, 295, 363, 31479, 294, 1729, 562, 3394, 8036, 7179, 750, 10515, 2835, 16174, 363, 31479], "temperature": 0.0, "avg_logprob": -0.13722967814250164, "compression_ratio": 1.6519823788546255, "no_speech_prob": 6.0622376622632146e-05}, {"id": 604, "seek": 464036, "start": 4640.36, "end": 4648.04, "text": " which was a variant of BERT which constructs blanks out for reconstruction sub sequences of words", "tokens": [597, 390, 257, 17501, 295, 363, 31479, 597, 7690, 82, 8247, 82, 484, 337, 31565, 1422, 22978, 295, 2283], "temperature": 0.0, "avg_logprob": -0.12261209732446915, "compression_ratio": 1.6261682242990654, "no_speech_prob": 6.904744077473879e-05}, {"id": 605, "seek": 464036, "start": 4648.04, "end": 4653.639999999999, "text": " rather than just a single word and span BERT has actually proven to be very effective", "tokens": [2831, 813, 445, 257, 2167, 1349, 293, 16174, 363, 31479, 575, 767, 12785, 281, 312, 588, 4942], "temperature": 0.0, "avg_logprob": -0.12261209732446915, "compression_ratio": 1.6261682242990654, "no_speech_prob": 6.904744077473879e-05}, {"id": 606, "seek": 464036, "start": 4654.28, "end": 4658.44, "text": " for doing co-reference perhaps because you can blank out whole mentions", "tokens": [337, 884, 598, 12, 265, 5158, 4317, 570, 291, 393, 8247, 484, 1379, 23844], "temperature": 0.0, "avg_logprob": -0.12261209732446915, "compression_ratio": 1.6261682242990654, "no_speech_prob": 6.904744077473879e-05}, {"id": 607, "seek": 464036, "start": 4660.44, "end": 4666.36, "text": " people have also gotten gains actually funnily by treating co-reference a question answering", "tokens": [561, 362, 611, 5768, 16823, 767, 1019, 77, 953, 538, 15083, 598, 12, 265, 5158, 257, 1168, 13430], "temperature": 0.0, "avg_logprob": -0.12261209732446915, "compression_ratio": 1.6261682242990654, "no_speech_prob": 6.904744077473879e-05}, {"id": 608, "seek": 466636, "start": 4666.36, "end": 4676.679999999999, "text": " task so effectively you can find a mention like he or the person and say what is it's", "tokens": [5633, 370, 8659, 291, 393, 915, 257, 2152, 411, 415, 420, 264, 954, 293, 584, 437, 307, 309, 311], "temperature": 0.0, "avg_logprob": -0.1760094446294448, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.326825001044199e-05}, {"id": 609, "seek": 466636, "start": 4676.679999999999, "end": 4683.32, "text": " there to see then and get in question answering answer and that's a good way to do co-reference", "tokens": [456, 281, 536, 550, 293, 483, 294, 1168, 13430, 1867, 293, 300, 311, 257, 665, 636, 281, 360, 598, 12, 265, 5158], "temperature": 0.0, "avg_logprob": -0.1760094446294448, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.326825001044199e-05}, {"id": 610, "seek": 466636, "start": 4685.0, "end": 4690.44, "text": " so if we put that together as time is running out let me just sort of give you some", "tokens": [370, 498, 321, 829, 300, 1214, 382, 565, 307, 2614, 484, 718, 385, 445, 1333, 295, 976, 291, 512], "temperature": 0.0, "avg_logprob": -0.1760094446294448, "compression_ratio": 1.5773809523809523, "no_speech_prob": 4.326825001044199e-05}, {"id": 611, "seek": 469044, "start": 4690.44, "end": 4698.839999999999, "text": " sense of how results come out for co-reference systems so I'm skipping a bit actually which", "tokens": [2020, 295, 577, 3542, 808, 484, 337, 598, 12, 265, 5158, 3652, 370, 286, 478, 31533, 257, 857, 767, 597], "temperature": 0.0, "avg_logprob": -0.12966835839407784, "compression_ratio": 1.6174863387978142, "no_speech_prob": 7.477463077520952e-05}, {"id": 612, "seek": 469044, "start": 4698.839999999999, "end": 4706.36, "text": " you can find in the slides which is how co-references scored but essentially it's scored on a clustering", "tokens": [291, 393, 915, 294, 264, 9788, 597, 307, 577, 598, 12, 265, 612, 2667, 18139, 457, 4476, 309, 311, 18139, 322, 257, 596, 48673], "temperature": 0.0, "avg_logprob": -0.12966835839407784, "compression_ratio": 1.6174863387978142, "no_speech_prob": 7.477463077520952e-05}, {"id": 613, "seek": 469044, "start": 4706.36, "end": 4713.08, "text": " metric so a perfect clustering and give you a hundred and something that makes no correct decisions", "tokens": [20678, 370, 257, 2176, 596, 48673, 293, 976, 291, 257, 3262, 293, 746, 300, 1669, 572, 3006, 5327], "temperature": 0.0, "avg_logprob": -0.12966835839407784, "compression_ratio": 1.6174863387978142, "no_speech_prob": 7.477463077520952e-05}, {"id": 614, "seek": 471308, "start": 4713.08, "end": 4721.4, "text": " would give you zero and so this is sort of how the co-reference numbers have been panning out", "tokens": [576, 976, 291, 4018, 293, 370, 341, 307, 1333, 295, 577, 264, 598, 12, 265, 5158, 3547, 362, 668, 2462, 773, 484], "temperature": 0.0, "avg_logprob": -0.1430494645062615, "compression_ratio": 1.5852272727272727, "no_speech_prob": 6.496880087070167e-05}, {"id": 615, "seek": 471308, "start": 4721.4, "end": 4729.96, "text": " so back in 2010 actually this was a Stanford system this was a state of the art system for", "tokens": [370, 646, 294, 9657, 767, 341, 390, 257, 20374, 1185, 341, 390, 257, 1785, 295, 264, 1523, 1185, 337], "temperature": 0.0, "avg_logprob": -0.1430494645062615, "compression_ratio": 1.5852272727272727, "no_speech_prob": 6.496880087070167e-05}, {"id": 616, "seek": 471308, "start": 4729.96, "end": 4736.5199999999995, "text": " co-reference and one competition it was actually a non-machine learning model because again we", "tokens": [598, 12, 265, 5158, 293, 472, 6211, 309, 390, 767, 257, 2107, 12, 46061, 2539, 2316, 570, 797, 321], "temperature": 0.0, "avg_logprob": -0.1430494645062615, "compression_ratio": 1.5852272727272727, "no_speech_prob": 6.496880087070167e-05}, {"id": 617, "seek": 473652, "start": 4736.52, "end": 4743.64, "text": " were wanting to so prove how these rule based methods and practice work kind of well and so its accuracy", "tokens": [645, 7935, 281, 370, 7081, 577, 613, 4978, 2361, 7150, 293, 3124, 589, 733, 295, 731, 293, 370, 1080, 14170], "temperature": 0.0, "avg_logprob": -0.1534216039321002, "compression_ratio": 1.615702479338843, "no_speech_prob": 1.7177068002638407e-05}, {"id": 618, "seek": 473652, "start": 4743.64, "end": 4753.0, "text": " was around 55 English 50 for Chinese then gradually machine learning these were sort of statistical", "tokens": [390, 926, 12330, 3669, 2625, 337, 4649, 550, 13145, 3479, 2539, 613, 645, 1333, 295, 22820], "temperature": 0.0, "avg_logprob": -0.1534216039321002, "compression_ratio": 1.615702479338843, "no_speech_prob": 1.7177068002638407e-05}, {"id": 619, "seek": 473652, "start": 4753.0, "end": 4760.040000000001, "text": " machine learning models got a bit better wiseman was the very first neural co-reference system", "tokens": [3479, 2539, 5245, 658, 257, 857, 1101, 10829, 1601, 390, 264, 588, 700, 18161, 598, 12, 265, 5158, 1185], "temperature": 0.0, "avg_logprob": -0.1534216039321002, "compression_ratio": 1.615702479338843, "no_speech_prob": 1.7177068002638407e-05}, {"id": 620, "seek": 473652, "start": 4760.040000000001, "end": 4766.280000000001, "text": " and that gave some gains he has a system that Kevin Clark and I did which gave a little bit", "tokens": [293, 300, 2729, 512, 16823, 415, 575, 257, 1185, 300, 9954, 18572, 293, 286, 630, 597, 2729, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.1534216039321002, "compression_ratio": 1.615702479338843, "no_speech_prob": 1.7177068002638407e-05}, {"id": 621, "seek": 476628, "start": 4766.28, "end": 4775.32, "text": " further gains so Lee is the model that I've just shown you as the end-win model and it got", "tokens": [3052, 16823, 370, 6957, 307, 264, 2316, 300, 286, 600, 445, 4898, 291, 382, 264, 917, 12, 9136, 2316, 293, 309, 658], "temperature": 0.0, "avg_logprob": -0.11407411098480225, "compression_ratio": 1.5919540229885059, "no_speech_prob": 0.0001435008307453245}, {"id": 622, "seek": 476628, "start": 4775.32, "end": 4782.84, "text": " a bit of further gains but then again you know what gave the huge breakthrough just like question", "tokens": [257, 857, 295, 3052, 16823, 457, 550, 797, 291, 458, 437, 2729, 264, 2603, 22397, 445, 411, 1168], "temperature": 0.0, "avg_logprob": -0.11407411098480225, "compression_ratio": 1.5919540229885059, "no_speech_prob": 0.0001435008307453245}, {"id": 623, "seek": 476628, "start": 4782.84, "end": 4789.8, "text": " answering was that the use of spanbert so once we moved it here we're now using spanbert", "tokens": [13430, 390, 300, 264, 764, 295, 16174, 4290, 370, 1564, 321, 4259, 309, 510, 321, 434, 586, 1228, 16174, 4290], "temperature": 0.0, "avg_logprob": -0.11407411098480225, "compression_ratio": 1.5919540229885059, "no_speech_prob": 0.0001435008307453245}, {"id": 624, "seek": 478980, "start": 4789.8, "end": 4796.92, "text": " that's giving you about an extra 10 percent or so the co-ref QA technique proved to be useful", "tokens": [300, 311, 2902, 291, 466, 364, 2857, 1266, 3043, 420, 370, 264, 598, 12, 33115, 1249, 32, 6532, 14617, 281, 312, 4420], "temperature": 0.0, "avg_logprob": -0.12585390859575413, "compression_ratio": 1.483695652173913, "no_speech_prob": 1.7494901840109378e-05}, {"id": 625, "seek": 478980, "start": 4797.72, "end": 4803.320000000001, "text": " and then the very latest best results are effectively combining together spanbert and", "tokens": [293, 550, 264, 588, 6792, 1151, 3542, 366, 8659, 21928, 1214, 16174, 4290, 293], "temperature": 0.0, "avg_logprob": -0.12585390859575413, "compression_ratio": 1.483695652173913, "no_speech_prob": 1.7494901840109378e-05}, {"id": 626, "seek": 478980, "start": 4804.68, "end": 4813.24, "text": " or larger version of spanbert and co-ref QA and getting up to 83 so you might think from that", "tokens": [420, 4833, 3037, 295, 16174, 4290, 293, 598, 12, 33115, 1249, 32, 293, 1242, 493, 281, 30997, 370, 291, 1062, 519, 490, 300], "temperature": 0.0, "avg_logprob": -0.12585390859575413, "compression_ratio": 1.483695652173913, "no_speech_prob": 1.7494901840109378e-05}, {"id": 627, "seek": 481324, "start": 4813.24, "end": 4821.4, "text": " that co-ref is sort of doing really well and is getting close to solve like other NLP tasks", "tokens": [300, 598, 12, 33115, 307, 1333, 295, 884, 534, 731, 293, 307, 1242, 1998, 281, 5039, 411, 661, 426, 45196, 9608], "temperature": 0.0, "avg_logprob": -0.1357981951340385, "compression_ratio": 1.645021645021645, "no_speech_prob": 1.5188893485174049e-05}, {"id": 628, "seek": 481324, "start": 4822.36, "end": 4827.5599999999995, "text": " well it's so me true that in neural times the results have been getting way way better than they", "tokens": [731, 309, 311, 370, 385, 2074, 300, 294, 18161, 1413, 264, 3542, 362, 668, 1242, 636, 636, 1101, 813, 436], "temperature": 0.0, "avg_logprob": -0.1357981951340385, "compression_ratio": 1.645021645021645, "no_speech_prob": 1.5188893485174049e-05}, {"id": 629, "seek": 481324, "start": 4827.5599999999995, "end": 4834.28, "text": " had been before but I would caution you that these results that I just showed were on a corpus", "tokens": [632, 668, 949, 457, 286, 576, 23585, 291, 300, 613, 3542, 300, 286, 445, 4712, 645, 322, 257, 1181, 31624], "temperature": 0.0, "avg_logprob": -0.1357981951340385, "compression_ratio": 1.645021645021645, "no_speech_prob": 1.5188893485174049e-05}, {"id": 630, "seek": 481324, "start": 4834.28, "end": 4840.92, "text": " called onto notes which is mainly newswire and it turns out that newswire co-reference is pretty", "tokens": [1219, 3911, 5570, 597, 307, 8704, 2583, 42689, 293, 309, 4523, 484, 300, 2583, 42689, 598, 12, 265, 5158, 307, 1238], "temperature": 0.0, "avg_logprob": -0.1357981951340385, "compression_ratio": 1.645021645021645, "no_speech_prob": 1.5188893485174049e-05}, {"id": 631, "seek": 484092, "start": 4840.92, "end": 4847.4800000000005, "text": " easy I mean in particular there's a lot of mention of the same entities right so the newspaper", "tokens": [1858, 286, 914, 294, 1729, 456, 311, 257, 688, 295, 2152, 295, 264, 912, 16667, 558, 370, 264, 13669], "temperature": 0.0, "avg_logprob": -0.058452939987182616, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00011203977919649333}, {"id": 632, "seek": 484092, "start": 4847.4800000000005, "end": 4854.68, "text": " articles are full of mentions of the United States and China and leaders of the different countries", "tokens": [11290, 366, 1577, 295, 23844, 295, 264, 2824, 3040, 293, 3533, 293, 3523, 295, 264, 819, 3517], "temperature": 0.0, "avg_logprob": -0.058452939987182616, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00011203977919649333}, {"id": 633, "seek": 484092, "start": 4854.68, "end": 4860.6, "text": " and it's sort of very easy to work out what their co-reference to and so the co-reference scores", "tokens": [293, 309, 311, 1333, 295, 588, 1858, 281, 589, 484, 437, 641, 598, 12, 265, 5158, 281, 293, 370, 264, 598, 12, 265, 5158, 13444], "temperature": 0.0, "avg_logprob": -0.058452939987182616, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00011203977919649333}, {"id": 634, "seek": 484092, "start": 4861.32, "end": 4870.04, "text": " are fairly high whereas if what you do is take something like a page of dialogue from a novel", "tokens": [366, 6457, 1090, 9735, 498, 437, 291, 360, 307, 747, 746, 411, 257, 3028, 295, 10221, 490, 257, 7613], "temperature": 0.0, "avg_logprob": -0.058452939987182616, "compression_ratio": 1.711111111111111, "no_speech_prob": 0.00011203977919649333}, {"id": 635, "seek": 487004, "start": 4870.04, "end": 4876.84, "text": " and feed that into a system and say okay do the co-reference correctly you'll find pretty rapidly", "tokens": [293, 3154, 300, 666, 257, 1185, 293, 584, 1392, 360, 264, 598, 12, 265, 5158, 8944, 291, 603, 915, 1238, 12910], "temperature": 0.0, "avg_logprob": -0.13810301863628885, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00010687001486076042}, {"id": 636, "seek": 487004, "start": 4877.48, "end": 4883.72, "text": " that the performance of the models is much more modest if you'd like to try out a co-reference", "tokens": [300, 264, 3389, 295, 264, 5245, 307, 709, 544, 25403, 498, 291, 1116, 411, 281, 853, 484, 257, 598, 12, 265, 5158], "temperature": 0.0, "avg_logprob": -0.13810301863628885, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00010687001486076042}, {"id": 637, "seek": 487004, "start": 4883.72, "end": 4892.2, "text": " system for yourself there are pointers to a couple of them here where the top ones are ours from", "tokens": [1185, 337, 1803, 456, 366, 44548, 281, 257, 1916, 295, 552, 510, 689, 264, 1192, 2306, 366, 11896, 490], "temperature": 0.0, "avg_logprob": -0.13810301863628885, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00010687001486076042}, {"id": 638, "seek": 487004, "start": 4892.2, "end": 4898.04, "text": " the Southern Kevin Clark's neural co-reference and this is one that goes with the hugging face", "tokens": [264, 13724, 9954, 18572, 311, 18161, 598, 12, 265, 5158, 293, 341, 307, 472, 300, 1709, 365, 264, 41706, 1851], "temperature": 0.0, "avg_logprob": -0.13810301863628885, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00010687001486076042}, {"id": 639, "seek": 489804, "start": 4898.04, "end": 4901.96, "text": " repository that we've mentioned", "tokens": [50364, 25841, 300, 321, 600, 2835, 50560], "temperature": 0.0, "avg_logprob": -0.4358990490436554, "compression_ratio": 0.7948717948717948, "no_speech_prob": 0.0002356595650780946}], "language": "en"}