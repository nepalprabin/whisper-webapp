{"text": " Okay, so hi everyone. Welcome back to CS224N. So today we're get to have the second of our invited speakers for this quarter. And so given all of the excitement that there's been about transformer pre-trained language models and all the great things have been done with them. We're especially excited today to be able to welcome Colin Reffel, who's been one of the key people who've been pushing the exploration of large pre-trained language models in particular. He was very interested in the development of the T5 language model, which you'll be telling you plenty about today. But to tell you a few more sentences, so Colin worked for a number of years at Google Brain, including working with Jeff Hinton on Captural Networks. He then got interested in the effectiveness of transfer using pre-large pre-trained language models. And as part of the work on that, he started with, together with other people on building even bigger, large pre-trained language models and doing a lot of investigations of those, which led to the T5 papers that you'll be hearing about today. So welcome, Colin. Right, yeah, thanks so much for the introduction and for having me. It's definitely an honor to speak at the legendary CS224N class. So yeah, I'm going to be talking today about large language models kind of in general, but focusing specifically on this model T5 that we released about a year and a half ago. I'll be presenting five or so papers that kind of should represent the full spectrum of good things, bad things and ugly things about large language models. And actually, I'll talk a bit about a paper that just appeared on archive last night. So hopefully everyone will learn something new today, even if you're already familiar with some of these papers. So just to give you an idea of what I'll be covering in this talk, I kind of will be answering each of these questions in turn in the over the course of the presentation. And I should mention that this, since some of these papers are new and some of this material is new, this is the first time I'll be presenting these slides. So if anything is confusing, I understand there's a way for you to pin questions to me through the hosts and feel free to ask about anything that might turn out to be confusing. So yeah, the first question that I'll try to answer is kind of the answer we sought out to focus on in the T5 paper, which is, you know, which of the transfer learning methods people have proposed so far work best. And what happens when we scale them up? And then after the T5 paper, we decided to investigate non-English pre-trained language models. T5 is an English only model. There are lots of languages spoken in the world. So what happens when we modify T5 so that it's a massively multi-lingual model? And then I'll talk about another paper where we try to investigate what kinds of knowledge and how much knowledge a model picks up over the course of pre-training. And then, relatedly in a follow-up work, we tried to figure out if the model actually memorizes training data during pre-training. If it actually, if we can get it to spit out verbatim entries from the training data set after it's been trained. And then finally, I'll talk about a very recent work that's similar in spirit to the T5 paper where the goal is to answer not what transfer learning methods work best, but what modifications to the transformer architecture that people have proposed to work best. So just to motivate this, I actually looked through some of the lectures that you all have had so far just to get a sense of what you've learned already. And I know that you are already pretty familiar with this transfer learning paradigm that has kind of taken the field of natural language processing by storm. But just as a quick refresher in this style of transfer learning, what we typically do is take a bunch of unlabeled text data and we apply some unsupervised objective, where you might say a self-supervised objective, where you do something like you mask out words at random and then you train the model to predict the missing words. So you can see these blanks in the block of text in green and the missing words that the language model will be trained to predict in the yellow box at the bottom. And then after we do this pre-training for a while, we fine tune the model on some downstream supervised task. In this example, I'm showing a sentiment analysis task for movie reviews. And the upshot of all of this is that doing this first unsupervised pre-training step is just ridiculously helpful. Not only does it usually make the performance better, it often gets you very good performance with relatively little fine-tuning data compared to training from scratch. So this is really a very common, it's kind of the de facto standard way to attack many natural language processing problems now. And I think you've already reviewed some of these methods, but because of how effective the transfer learning recipe was, there really was kind of an explosion of work on transfer learning starting in maybe 2018 or so. There's some prior work on other approaches to doing transfer learning like word vectors, like word-to-vec, some sort of preliminary work that proposed the recipe that I just described called semi-supervised sequence learning, some work that suggested that this kind of stuff might be really possible, like the unsupervised sentiment on paper. But I would say around in 2018, there was kind of a string of papers that kicked off the excitement in the field, including the universal language model fine-tuning paper, how the Elmo paper, what we now call GPT-1, and of course the BERT paper in late 2018. And then starting in 2019, there really was just an incredible explosion of different methods for doing transfer learning, including new transfer learning, sorry, new pre-training objectives, new data sets, new ways of doing fine-tuning, and so on. And we started working on the T5 project in late 2018, and we noticed that as all these methods were coming up, it was getting harder and harder to figure out what actually works best. And part of the reason for that was just because there was so many methods that were being proposed kind of simultaneously. And when that happens, even when everyone is working in Ernest with good faith, you might have situations like this where one paper comes along, paper A that proposes a new unsupervised pre-training like technique called fancy learn, another paper comes along maybe around the same time with a pre-training technique called fancyer learn, and paper A pre-trains on Wikipedia for unlabeled data, while paper B uses Wikipedia and the Toronto Books Corpus was just a collection of novel text. And then the question obviously is, is fancyer learn better than fancy learn? Well it's hard to say because they use different sources of pre-training data. You might imagine that maybe they use different model sizes, maybe they pre-trained for a different amount of time, they use different optimizers, there are tons of design decisions that come into play here. And so given that these design decisions can make it hard to determine what worked best, our goal in the T5 paper was kind of to step back and just say, given the current landscape of transfer learning, you know, all of the methods that people have proposed, what actually works best when we compare them in a in the same exact setting. And once we know what works best, how far can we push these tools that we already have? And how much can we explore the limits and figure out how well these things work in scale? And so to attack this problem, the kind of the only thing that we introduced, since again, we're kind of exploring existing techniques, was this idea of treating all text problems in the same format. And this kind of approach, this dogma of treating every text problem in the same format gives rise to our model, which we call the text-to-text transfer transformer. And so to explain this format to you, the basic idea is that we cast every text-based NLP problem as a text-to-text task. And by that, I mean, the model takes text as input and it produces text as output. So in things like English to German translation, this is pretty typical. We feed in a English sentence on the input and we train the model to predict a German sentence on the output. And you'll notice in our case, we actually also feed what we call a task prefix, translate English to German. That just tells the model what we wanted to do with the input. Because if, especially if we're training a multi-task model, if you just feed the model, that is good. The model doesn't know what to do with it. It doesn't know if you're trying to do sentiment analysis or English to German translation or what. So this shouldn't be that surprising so far. You probably learned about encoder decoder model, sequence-to-sequence models. And that's basically all that we're doing here. It maybe gets a little more unusual when we start to tackle things like text classification tasks. So this is an example from the cola benchmark, which is the corpus of linguistic acceptability. And the goal here is to take a sentence and determine if the sentence is quote unquote acceptable, which kind of means whether it's grammatically correct and also if it's nonsense or not. And in this case, the sentence is the course is jumping well and of course, courses can't jump. So this sentence is not acceptable. But rather than training our model through a classifier layer that outputs a class label or a probability distribution over class indices, we actually train the model to output the literal text not acceptable. So it's outputting this text string token by token. And it can get even a little weirder. We can attack things like regression problems where effectively the model is supposed to be outputting a floating point value. And we actually just do this by taking the floating point numbers and converting them to strings and training the model to just predict the string. And it turns out that at least for this particular task, which is the STSB benchmark, it works perfectly fine. And we ultimately actually got state of the art on this benchmark. So doing this type of sort of float to string conversion with a little bit of quantization turns out to work fine for regression problems. And finally, you know, the main point of this is that there are a lot, there are really tons and tons of problems that we can cast into this format. So here's an example of abstractive summarization. We feed in a news article on the left and we predict that at this summary on the right. And again, we can really attack all of these problems in exactly the same way. So we're using exactly the same objective during training and exactly the same decoding procedure at test time to attack a huge variety of natural language processing problems. And the nice part about doing this is that as long as a transfer learning improvement is applicable to our model and to this text to text format, we can try it on a huge suite of downstream tasks while using exactly the same model, exactly the same learning rate optimizer training procedure, exactly the same inference procedure. So we can get a rid of a ton of the confounders that I mentioned earlier. Hey, Colin. Yeah. Can you take a second as we're discussing the formatting here to talk about how the 3.8 sort of regression example works and yeah, just go over that one more time. Yeah, absolutely. So in this particular task, STSB, the goal is to take in two sentences and predict a floating point number, which denotes how similar those two sentences are. And that floating point number ranges between 1.0 and 5.0. So we basically took the ground truth, annotated values that were supposed to regress. We quantized them to the nearest 0.2 and we cast it to a string. And now we have a string like three three period eight, a 3.8, but you could think of it like three period eight or four period zero. And we just train the model to predict that string, which ultimately it's a string. It's not a number anymore. We just predict it token by token. So in a sense, you can kind of think of it as converting the regression problem to a classification problem because you're doing this quantization, but you also more broadly can just think of it as converting the regression problem to a text to text problem, which is what we're doing. Thanks. Yeah. It's a little funky, but I promise it works. And great. So the nice thing again about using this sort of sequence to sequence text to text format is that we can actually just use the original vanilla transformer as it was proposed because if you remember, the transformer was actually proposed for English to, well, it was proposed for machine translation primarily, which is a sequence task, you know, taking a language in sentence and one language as input and producing the corresponding sentence in other language. And so really, I won't say a lot about the model that we use. There are relatively few changes that we made to the standard transformer architecture as originally proposed and attention is all you need when constructing our T5 model. I will towards the end of the talk discuss lots of architectural modifications that people have made sense, but for T5 or the T5 paper, we really basically stuck to the basics. So the next big question when you're attacking a transfer learning problem is what should my pre-training data set be? And because of the internet, there are lots of possible sources of unlabeled text data, one common source is Wikipedia. I'm displaying a ton of Wikipedia articles on the screen here. And in undertaking this project, one of the factors that we wanted to study was the effect of the pre-training data set itself. And so we actually constructed a new pre-training data set that would allow us to vary the size across many orders of magnitude, also that has a filtering pipeline that would allow us to control the quality and type of data that was pre-trained on. And I'll describe how we built that data set now. So the first thing we did is we wanted to source our data from a publicly available source. We didn't want to use some Google internal web scrape that we couldn't release. So we made use of these web scrapes by a nonprofit organization called Common Crawl, which is really just organization that sends web crawlers out through the internet and downloads as much text as they can. And every month they dump out what they call web extracted text, which is you can think of them as websites with all of the HTML and JavaScript ideally removed. And this produces text that has a pretty decent amount of natural language in it. Also a lot of boilerplate and menu text and also a little bit of gibberish. But as a whole, it's kind of a good starting point for constructing these pre-training data sets. And then we took a few steps to kind of try to make it so this data set was a little cleaner. So the first thing we did is we removed any lines that didn't end in a terminal punctuation mark. We used a language classifier to only retain English text. We removed anything that looked like placeholder text, like Laura Mipson text on the right. We removed anything that looked like code. We duplicated things on a sentence level. So any time that any chunk of text appeared on multiple pages, we only retained it on one of the pages and so on. And ultimately these heuristics were relatively simple but produced reasonably clean text. And I will discuss later the effect of these choices in cleaning. That was one of the experiments that we ran. And so after doing this, we created this data set called C4, which is the colossal clean crawl corpus. And it's available in TensorFlow data sets. You actually, you need to do the processing yourself, which is somewhat computationally expensive. But nevertheless, it is entirely possible. And it produces about 750 gigabytes of reasonably clean natural text data. Okay, so now we have our framework, our model, our pre-training data set. We need our pre-training objective. What are we going to do to train the model on our unlabeled text? And so just to explain the objective that we chose kind of for our baseline experimental procedure, which again we will experiment with different pre-training objectives later. Imagine that you have some original text sentence like thank you for inviting me to your party last week. And what we do is we basically choose some words at random and technically we're choosing tokens at random. But for now, it's just assumed that tokens are words. And we're going to drop those tokens out. And so we end up with something that looks like this. We, for every consecutive span of tokens that have been dropped out, we replace it with a sentinel token. And each sentinel token gets a unique index. So one of them we call it the sentinel x. And the other one will be the sentinel y. And so you can see that because the words for and inviting are subsequent words that we decided to mask out by randomly masking words, we're going to replace both of those words with a single sentinel, single unique sentinel token. And then the model's goal will just be to fill in the blanks. And so if you're familiar with the birth pre-training objective, this is somewhat similar. The fact that we're collapsing subsequent tokens into a span that we're going to be replacing words from is slightly different. And the fact that we're reconstructing just the missing words and not the entire sequence is maybe slightly different too. Okay, so now I'll kind of talk through our baseline experimental procedure that we're going to use. We're going to kind of tweak over time in very long various axes to explore the landscape of transfer learning, at least circa when this paper came out. So to pre-training the model, we're going to take a model that has a birth base size encoder and decoder. So technically has twice as many parameters as birth base because there's a birth base size encoder and a birth base size decoder. We're going to use the denoising objective, the sort of mass language modeling objective that I just described. And we're going to apply it on the C4 dataset that I mentioned earlier. We're going to pre-traine for about 34 billion tokens, which is about a quarter as long as birth base was trained. So it's not a ton of pre-training time, but because we're training on, we're doing so many experiments, we need to cut it back a little bit. We're going to use an inverse square learning rate schedule that turned out to work reasonably well in our setting, but it's not a terribly important design decision. And then we'll fine-tune on a variety of downstream tasks, kind of tasks that people care a lot about at the time. There's the glue benchmark, which is kind of a meta benchmark of many individual downstream tasks, like cola and STSB that I already mentioned. These are what some people might call natural language understanding tasks, but for the most part you can think of them as sentence classification, sentence pair classification, or regression tasks. We also consider the CNN Daily Mail Abstractive Summarization Corpus. This is a sequence-to-sequence problem where you're given a news article and you have to output the summary. The squad question answering benchmark, which is a reading comprehension benchmark where you're given a paragraph and you have to answer a question about the paragraph. You can either attack it in an extractive setting where you extract the answer from the paragraph or an abstractive setting where you just output the answer. We use the abstractive form because it's a text-to-text problem. We also included the super glue benchmark, which was a new benchmark at the time that was designed to essentially be a more difficult version of the glue benchmark. It has a new set of tasks that were hard for existing models. And then finally we included three translation data sets, English to German, English to French and English to Romanian translation. English to French being the largest, which was an extremely large data set in English to Romanian being many more to my attitude smaller. And we're going to fine tune on each of these tasks individually and separately. So we take the pre-trained model and separately fine tune on each of these downstream tasks. And we're going to fine tune for up to 17 billion tokens, but we're going to save checkpoints along the way, evaluate each checkpoint on the validation set and report the performance on the best checkpoint. And note that this is not an experimentally valid way to report your performance because you're basically doing model selection on the data set that you are, the data split that you are, you're reporting performance on the data set split that you're doing model selection on, which is not a good way to compare different methods, but to compare within methods. It's, it's, it's, we're reasonably comfortable doing this. So now I'm going to kind of give you a very high level overview of some of the experimental results in this paper. This paper is pretty huge in terms of just the number experiments we ran. And so if I was to really drill into this paper, it probably would take me the whole time, but there's other fun stuff that I want to tell you about. So, but, but anyways, the point is I will be showing you lots of tables like this one. And so in these tables on the columns, you have the performance on the various downstream tasks. And in the rows, you have different experimental settings that we considered. So to give you an example, here's kind of the scores that we got from our baseline, which is the exact experimental procedure that I must describe, that I just described. And we also ran that baseline 10 times in reporting with standard deviation on the second line. And then in this last line here, we're reporting the performance of the same model without any pre-training, just basically only trained separately with supervision on all of these downstream tasks. And just to point out a couple of things on this table, the first obvious thing is that in most cases, the pre-training setting is dramatically worse. So indeed, transfer learning does tend to be helpful. The place where that's not true is actually on this English difference translation task. And that's probably because it's such a big task that you actually don't really need pre-training to do well at it. We wanted to include this because if the performance regresses on this task, then that's something that we should be worried about. The next feature of this table to notice is this little star appeared. That star will appear anytime there's a row in the table that's equivalent to our baseline. And another little thing to note, maybe if you're familiar with the history, the score that we got on glue and squad was reasonably comparable to BERT. So it's a decent sanity check. We have a model that has more parameters, but it's only trained for a quarter as long. And it nevertheless got comparable performance, which using a similar objective, so we shouldn't be too surprised about that. And then the last thing to mention is that we're going to use this standard deviation over and over again so that we can bold entries in the table when there within one standard deviation of the maximum value for that data set in the table. So now, I'll just make a big disclaimer, which is we're going to compare lots of different things. We're going to run lots of experiments, but we're not going to tweak any hyperparameters because if we did, like, change the learning rate or whatever, it would be just too computationally expensive to do this for each individual methods. Our hope is that this is okay because we are treating all problems in exactly the same framework. We're always doing text to text maximum likelihood training. So hopefully we can keep hyperparameters fixed. And arguably, if you propose a new method that requires extensive hyperparameter tuning, it's not a very useful method for practitioners. And we'll get into that a little bit more later when I talk about architectural modifications too. The other thing I'll say is that while we did run lots of experiments, there's no way we could be comprehensive because there were so many methods out there. And the inclusion or exclusion of one particular method is not meant as a judgment on its quality. It's just what we were able to do given the constraints that we were working under. So the first set of experiments that we ran were to compare different model structures. So as I mentioned earlier, the main baseline T5 model is an encoder decoder model. And in this case, you have a separate layer stack for that encodes a sequence and a separate layer stack that decodes the target sequence. Basically, it generates the target sequence, one token by token, while attending back to the encoders output to figure out what it should condition on. The next setup that we considered is an encoder decoder model except that all of the relevant parameters in the encoder decoder are shared. So they're basically half as many parameters. And then finally, another variant that we considered is an encoder decoder model where the encoder and decoder have half as many layers as they do in the baseline. And that's because we're also considering single stack models, the language model and what we call a prefix language model. The language model is a model that models the sequence strictly from the left to right fashion in a causal fashion. It basically just ingests tokens one at a time and predicts the next token. And you can actually apply these to text to text problems by basically feeding the input as a prefix before you start predicting anything. Now, if you just use a language model in its strict format, then you still have to have what we would call a causal mask, so a causal attention pattern on the prefix. And that's actually how the GPT series models treat all of their problems. But because we are explicitly denoting part of the sequence as an input and the rest of the sequence as a target, we actually can allow the model to have full visibility, you know, a non-causal mask on the input region of the sequence. And when we make that change, we call that the prefix language model. And now the upshot of all of this really is that the encoder decoder model for our framework turns out to work best. You can see that when we share the parameters, it does hurt performance a little bit, but maybe a little less than you might expect. The prefix language model attains slightly worse performance, but significantly better performance than doing strictly causal left to right language modeling, which is what you see in the fourth row here. And finally, having the number parameters in the encoder and decoder is harm's performance significantly. One thing to note is that in all of these cases, we're processing the same total sequence length. It's the same input sequence in the same target sequence. So in most of these cases, the total number of flops required to process the sequences the same, even though the number parameters is twice as many in the baseline model. So the next thing we looked into were different variants on our pre-training objective. So the first thing we did was kind of compare different high level approaches, maybe just training the model to predict the next token, one token at a time. That's kind of a language modeling objective. Another would be to take the input sequence, shuffle it up and train the model to predict the unshuffled sequence, or to consider a mass language model style, a bird style objective, like the one that we that we mentioned earlier. And now on the second step that I'm showing here, the results for, we considered a bird style objective where the model is trained to predict the entire original uncorrupted input sequence, a mass style objective, which is quite similar. And then a replace corrupted spans objective, which is like the one that I described at the beginning that we're using in our baseline model. And finally, a variant where rather than replacing each token with a unique Sentinel token, we just dropped the mass tokens completely and train the model to predict the dropped tokens. And you can see that the latter two options work roughly as well as one another. But another pertinent difference between these these sets of objectives is that the first two involve predicting the entire input sequence. And the last two basically just involve predicting the massed out tokens. And when you only predict the massed out tokens, you have a much shorter target sequence. And so the overall cost is significantly lower for pre-training. So we decided that that was the best approach. And then we considered other hyper parameters in our masking strategy, such as how many tokens to to mask out. So the next thing we considered were different variants of a pre-training data set. In our baseline, we use the C4 data set that I proposed at the beginning of the talk. We also compared to pre-training only on unfiltered data from C4. So rather than doing all these heuristic filtering steps, we just take the raw web extracted text from C4 and pre-training on that. And you can see that that does uniformly worse. So it does seem to be true that these cleaning steps that we're doing are actually useful. The next four data sets were our attempt to pre-training similar data sets that had been used in password. The real news data set came from the Grover paper. It's essentially pre-training only on data from news sites. Web text is this data set that we used in the GPT2 paper where you only train on Web text that was linked to and received a reasonably high score on Reddit. And then the last two variants are either Wikipedia alone or as was used in the in the birth paper Wikipedia with the Toronto Books Corpus. And you might actually notice that some of these more specialized data sets we get better performance. So for example, you can see that on the Wikipedia and Toronto Books Corpus on the bottom row, we actually do much better on superglue with a score of a little over 73 compared to pre-training on C4. And it turns out this is because or we we conjecture that this is because superglue contains a task called multi-RC, which is a reading comprehension task on on on Wicc news sorry and cyclopedia articles and on novels. So the basic takeaway here is that when you pre-training data that's similar to your downstream task that has a similar domain, you often get a big boost in that downstream task and that's indeed what happened here. Interestingly, you can also see the opposite effect. So if you if you look on Wikipedia on the second to last row, if you only pre-training Wikipedia, you end up doing much worse on cola, which is the corpus of linguistic acceptability tasks that I mentioned early on. And we conjecture that this is because on Wikipedia has very little unacceptable text. You're basically only pre-training on clean text, whereas C4 has some ungrammatical texts, some nonsense in it. And so that actually can boost your performance a little bit on cola. The last thing to note is that while you do see some gain sometimes on using these smaller data sets, these data sets are about an order of magnitude smaller than C4. So then the natural question is does it actually hurt you to pre-traine on a smaller data set? So to answer that question, what we did is basically took C4 and artificially made it smaller so that it was repeated over the course of pre-training. And you can see here that when you repeat the data set 64 times, so it's 34 billion divided by 64 tokens, because that's how much pre-training we did, you actually don't sacrifice much performance. The performance is roughly the same. But if you repeat the data set 256 times, 1024 times or more, you actually start to see degradation. And the reason that we think this is happening is because you're basically overfitting during pre-training. And you can get a sense for whether that's true or not by looking, just looking at the training loss. You can see that the model attains a much, much smaller training loss as repeat the data set more and more times. So the upshot of this is that your data set should be at least as big that you don't see significant overfitting during pre-training. And later on, when we scale up these models and pre-training them on much more data, we would do enough repeats of the smaller sort of more domain specific data sets that we imagine we would see harmful effects. The next thing we experimented with were multi-task learning strategies. So when you're doing multi-task learning, you're essentially training the model on multiple tasks at once. And in most of the, in all the experiments I'm showing on this slide here, we're actually training on every single task at once. So the pre-training task and all of the downstream tasks together. And the most pertinent question when you're doing multi-task training like this is how often should I sample data from each task? So one approach is just to sample data at an equal rate across all of the tasks. Another case is to basically pretend like you just concatenated all the data sets. We call that examples proportional mixing because it's equivalent to sampling from the data set in accordance to how many examples there are in the data set. The difficult thing with that though is that our pre-training data set is so big that its proportion would be much, much, much bigger than every downstream task. And we basically would never train on any of the downstream data. So we introduced this hyper parameter K, which is a constant that basically is how big should we pretend that the pre-training data set is. The last thing you can do is take the number of examples in each data set and scale it by a temperature. The larger the temperature, the closer you get to equal mixing to uniform sampling from each data set. But at any rate, the main takeaway from this table is that you can get pretty close to the performance of separate pre-training and fine-tuning like we do on our baseline if you get the mixing strategy right. But ultimately, we found that you do tend to sacrifice some performance when doing multi-task trading and at least some of the tasks. Collin, there were lots of questions back on the choice of on the slide with the different data sets, one showing real news and C4 and so on. Sure. Everyone take a couple. Absolutely. Yeah, so firstly, if you just look at this, it still does kind of look like you can get great results with more than an order of magnitude, less text, and C4. Yeah. And it seemed like that's not the message you wanted to be putting forward. Yeah. So there's a little nuance here, which is basically that in our baseline, in these experiments that I've been running so far, we're actually not pre-training for that long. So as I mentioned earlier, we're actually pre-training for a quarter as long as BERT. And actually for us, I believe, one 256th as long as Excel net, for example. So and when later in the paper, we're going to pre-traine for much, much, much longer. And in that case, we would end up repeating these data sets many, many times over the course of pre-training and we'd start to see these negative effects that I explained on the next slide. Oh, so that's why you're then doing the repeat. Okay, so when they're asked about that as to why you train on the same stuff over and over again, that's the test set. Yeah. Okay, exactly. And then on the data sets, so to a first approximation, the C4 contain Wikipedia or very partially many pages of Wikipedia, but not all of Wikipedia. Common crawl is done by like is a sort of web crawl by following links at some priority. And ultimately, it didn't cover all of Wikipedia. I don't actually know the exact proportion of Wikipedia that's included in C4, but definitely when training on C4, you will see some Wikipedia text. It will be at a relatively low proportion compared to all of the other data that you'll see. Sure. And then someone wasn't quite convinced with your argument that the good quality of Wikipedia explained the worst performance on Cola because they thought, well, surely real news that's basically well edited text as well. And yet it seems to work fine. Yeah, it's a good point. I'm not sure, you know, it could be that real news because real news has cloaks in it or maybe real news ended up having some content from the common sections of sites. I should say that when I, the reason that they're real news like and web text like is that these are our own reproductions of them so they might not be exactly the same as the originally proposed variance because web text, for example, was never released. Yeah, but it's an interesting point. And that's also why I would say that it's a conjecture. It's not, you know, not something that I can make a rigorous claim about. Okay, maybe we should let you go on now. Great. Thanks. Yeah, thanks for those questions. So then the next thing after looking at these different multitask training strategies is to see if there's any way for us to close the gap between multitask training and this pre-training followed by separate fine tuning. And we experimented with many different strategies here, basically strict multitask training, doing multitask training followed by individual task fine tuning, doing multitask training, but without any unsupervised data. And really, the main takeaway from all of these experiments was that if you do the multitask training first, including the unsupervised task, and then you fine tune the model on each task separately, which is the third row here, you actually don't really sacrifice much performance at all. You are, you don't end up with a multitask model because you're fine tuning on each task individually. But the nice thing about this approach is that you can monitor the performance on your downstream tasks while you're doing pre-training. And you don't sacrifice much performance. One setting that we didn't consider is the unsupervised pre-training followed by supervised multitask training. I wish that we had run that, but we just didn't. So then sort of the last set of experiments we ran, try to answer the following question. Let's say that someone comes along and all of a sudden gives you four times as much compute. What should you do with it? And so there are a number of things you could do. You could increase the number of training steps by a factor of four. You could increase your batch size by a factor of four. You could make your model twice as big and train for twice as long. You can make your model four times as big. You could train four models separately and ensemble them. Or you could do this last thing which doesn't actually use four times as much compute where you pre-trained one model and you fine tune it four times separately and then ensemble those. And the main takeaway here is that scaling helps. This is, you know, very unsurprising, especially in 2021. But interestingly, you get significant gains whether you just increase the training time or if you increase the size. So you can see that along both of these access we get significant performance improvements, although the performance improvements are more dramatic when we increase the size. And in particular, you can see that we've gone from a score of about 71 and super glue to 78 just by making the model four times bigger. Okay, so let me just kind of give a quick recap of all of that and then use that recap to explain the design decisions that went into the final sort of T5 models. The first thing is that we're going to choose an encoder decoder architecture because that seemed to work best in our text to text format. The next thing is that we're going to use a span prediction objective which is ultimately quite similar to the baseline objective that I described earlier. We will use the C4 dataset because it did attain reasonable performance but was large enough that we didn't have to worry about repeating the data and seeing detrimental overfitting during pre-training when we scale up the number of pre-training steps. We actually decided to do multitask pre-training because we will be scaling up the amount of pre-training. Our longest training runs took about a month and we wanted to be able to monitor performance over the course of pre-training without doing fine tuning. So we're going to be doing this multitask pre-training followed by fine tuning and then the last thing of course is we're going to train bigger models for longer. Specifically the model sizes that we ended up releasing we call small base large 3B and 11B. The small model has 60 million parameters. It's about a quarter as big as our baseline which again was a birth-based size encoder, birth-based size decoder. We also trained a model that was a birth-large size encoder, birth-large size decoder and then we created two larger variants simply by scaling up the feed-forward dimension of the transformer and the number of attention heads in the transformer. You can see our largest model actually had a hidden dimensionality of 65,000 in the feed-forward layers. The reason that we did this kind of unusual way of scaling up the perimeter count is just because the feed-forward layers are just gigantic matrix multiplies and that's the best way to make use of hardware accelerators. I just stick in one more question. Someone was asking about how you did the multi-task training. Was that sticking a simple softmax classifier on top for each task? In our case because we're using this text-to-text format basically you train on exactly the same model no new classification heads for every task. The only difference is that each task gets its own task prefix. So if you remember all the way back at the beginning we say you know translate English to German colon English sentence or you know summarize colon English paragraph and that tells the model what it should do and then you just train the model to predict the corresponding target. Cool. So the last pertinent detail again is that we did scale up the amount of pre-training. We ended up pre-training on a trillion tokens of data rather than 34 billion tokens. So it's quite a lot more pre-training although it's still less pre-training that was used in excel net. I think by a factor of two if I don't remember incorrectly. So here are the results that and these were kind of the way that things stood at the time that we released the paper. We ended up getting state of the art results on the glue metabenschmark, CNN Daily Mail, abstract summarization, squad question answering. And we were actually quite excited to see how well we did on super glue. We ultimately came pretty close to the human score which was the score of 89.8 and we performed significantly better than Roberta. Super glue it turns out is a benchmark that benefits a lot from large models and so you can really see a dramatic increase in the model's performance as we scale the model up. On the other hand we did not obtain state of the art results on any of the translation data sets. And the reason that we think that this is true is because all of the state of the art results at the time on these translation data sets used back translation. And if you remember we did English only pre-training in our model and we expect that in terms of making use of unlabeled data it's more effective to use back translation for machine translation problems than to use this English only pre-training that we did. I should mention of course these results are now quite a bit stale and some of these scores have been beaten by subsequent models. So now I'll just quick make a plug that you know all of our code is released, our pre-trained models have been released. You can make use of them in our code base. They're also of course in the Hugging Face Transformers code base. We made a colab at the time that shows a pretty basic demo of how to take one of our pre-trained models and basically train it on a TSV file of inputs and targets. So because all problems are text to text problems you just need to give the model some input text and some target text and that's all you need to fine tune the model. And you can make you can actually fine tune up to the 3 billion parameter model on a free colab TPU using the the link at the bottom here. Great so so far we've been talking about an English only pre-training model. I mean we did apply it to machine translation in downstream tasks. So kind of a natural question is you know what about all the other languages? Why not train a multi-lingual model? And so that's something that we did more recently. And actually let me just pause because I did see a couple questions coming on. I want to make sure that I I'm not leaving anyone behind as I move to the next section. Sure so one of the questions is about the multitask set. So if you include an unknown task prefix does anything interesting happen? And if you don't include a prefix what what does it do? So if you if you include an unknown task prefix or if you don't include a prefix at all what it will probably do is apply the unsupervised objective because we actually didn't use a task prefix for the unsupervised objective. Well I guess I should say what if it's not quite that's not quite true because there won't be any Sentinel tokens in the input. So we actually see it it typically does is it outputs kind of some related words and some other Sentinel tokens in gibberish. It's it's not very useful as I guess the upshot. You have questions about back translation. I don't think they heard about back translation in the rest of the course. So back translation is a pretty straightforward method. The basic idea is that if I have unlabeled text data in one language I use my current model to translate that data to some particular language and I use that as training data then subsequently for my model. It's it's similar to self-training if you're familiar with it. Basically you're making predictions on unlabeled data and then using those predictions to train the model. Turns out to be helpful. Yep and then one one detailed question maximum input length and maximum output length. How did you choose them? Did you do a study on that as well? Yeah so most of the tasks we considered did not have input length significantly longer than 512 tokens most of the time using the tokenization strategy that that that we made use of and so we used a maximum input length of 512 but we used that position encoding scheme that allows arbitrary input lengths and we actually have in subsequent work fine tune T5 on sequences of of length 2048. Beyond that you start to get into memory issues because of attention's quadratic memory complexity but you in principle you can you can apply it to long sequences. And then maybe one last thing if you have a second is talking again about how translation again seems to be not the not the killer app for this and so what's your intuition as to why translations like does it not benefit from pre-training for for what reason? I'm not you know I'm really not sure. I yeah I can only conjecture. I think that pre-training helps the model learn the meaning of words. It helps the model learn some world knowledge which I'll talk about a little bit later and that's a that's a very loose concept. I think that for translation learning world knowledge is not very useful because everything all of the knowledge you need to translate a sentence for for the most part is in the sentence and so basically all of the sort of contextual knowledge style information that you need to produce the German sentence is in the input sentence. So gaining world knowledge during pre-training is not very useful. Of course it's useful to know what words mean but to a certain extent you that's kind of the easiest signal to pick up on during training and I imagine that would be my guess I don't have any rigorous proof of any of this. Thanks. Great so like I was saying we trained this English only model and we wanted to address the major shortcoming that really only can speak one language. So we introduced a model called MT5 multi-lingual T5 and really for the most part the if you remember one thing about MT5 it's basically that it's exactly the same model but trained on a multi-lingual corpus and the text to text format is the same you know we feed in task prefixes but we can feed in content to different languages and we can do classification tasks we can do question answering tasks with MT5 in exactly the same way that we can do with T5. So like I said the pertinent thing about MT5 was creating a multi-lingual variant of C4. Overall the process is very similar to the process we used for C4 except that it includes 101 languages that we detected using a open source language detector. We also extracted data from more common crawl dumps because especially for the low resource languages it was hard to get enough data from only one common crawl dump and you can see a list of the languages that we include here. Ultimately the data set ended up being about 27 terabytes in size. So here's a distribution of the number of pages in the MT4 training data set for various languages. You can see our most our highest resource language is English where we have about three billion pages with three trillion tokens total. The lowest resource language is your RUBA with only about 50,000 pages. So you can see the amount of data that we got for each language varies by many orders of magnitude. Because of that a common strategy is to use this sort of temperature scaling that I mentioned earlier where basically you sample from data in a particular language by using by scaling the number of examples in that language by a temperature. And as the I apologize that the temperature here is one over the temperature that I described previously. So in this case as the temperature gets smaller and smaller you get closer and closer to a uniform distribution. The net effect of this is that for very small temperatures you tend to do better on downstream tasks on low resource languages like Urdu but as you increase the temperature so that you get so you basically are doing examples proportional mixing of the different languages you do better on high resource languages like Russian. So we took MT4 we pre-trained MT5 again basically everything was kept the same we made the vocabulary a bit bigger to accommodate all the different languages but overall the amount of pre-training the model sizes etc are basically the same. And ultimately got state of the art on some of the tasks in the extreme benchmark. You'll notice that we don't report results for some of these tasks that's partially because extreme is designed for sentence encoders like Bert. T5 is an MT5 our encoder decoder models. We did not experiment with using the encoder on its own but in order to attack some of these problems like the sentence retrieval problems you need a model that can output a single vector representation of your sequence and we don't have that in T5 so we didn't apply it to those tasks. One interesting finding from this paper that I'll just quickly mention here is that the there's there are basically multiple settings that people consider multilingual benchmarks. One is the case the zero shot case and in that case you don't do any pre-training on a language of sorry you don't have any fine-tuning data on each particular language you only have pre-training data on those languages. So you fine-tune let's say only in English and then you feed the model some text in another language and see if it produces the right predictions. The next setting is the translate train setting that's where you take a machine translation model and translate the data in the English fine-tuning corpus into different languages and then the last setting is the in-language multitask setting that's the setting where you assume that you have gold standard ground-truth data in every language that you want the model to be able to process data in and the takeaway here actually is that the difference in performance between small models and our largest model as we go along the x-axis is much much bigger for the zero shot and translate train settings than it is for the in-language multitask setting. So what this suggests to us is basically that the model learns a much wider distribution of languages if it has a much larger amount of parameters and it is able to do this kind of like zero shot task learning, multi-lingual task learning much better when it has more parameters. So kind of along those lines you know larger models can maybe fit more knowledge about more languages. We had another paper where we basically tried to answer the question you know how much and what kind of knowledge does a model pick up during pre-training. And so to answer that question we took a we basically introduced a new variant of the question answering task. So the question answering task kind of comes in a couple of different flavors. The simplest flavor which I've mentioned already is reading comprehension and in that case the model is basically given a paragraph or an article and then it's asked a question about the paragraph or article and it has to basically extract the answer. So you can see if it's being asked what color is 11 it has to look in the in the in the paragraph that it's seen and see that it's the 11 has a yellow fruit and output the word yellow. So this is kind of the simplest form of the question answering task. A more difficult form is what people call open domain question answering. And in that case you assume that the model is given a question and has access to a large external database of of knowledge maybe all of Wikipedia. So the model has to do two things. If first has to find the article or the snippet of text that contains the answer in the database and then it has to extract the answer from the article. So there's this additional retrieval step that makes the problem quite a bit harder. But we introduced a sort of third variant of question answering that we call closed book question answering the name takes inspiration from closed book exams. The goal here is that you just feed the model the question. It does not have access to an external knowledge source. It cannot look up information anywhere. It could only answer the question based on the knowledge that picked up during pre-training. So if you feed the model the question what colors a lemon it has to output the model yellow correctly because it so so to speak knows that the that elements are yellow. So this is a good way we argue of testing the knowledge the amount of knowledge stored in the model during pre-training. So why would we expect that to work? Well you could imagine here we're doing our normal pre-training of T5. We're masking out words and training the model to predict the masked out spans of words. And you might imagine that somewhere during pre-training it sees a sentence that says President Franklin Blank born blank January 1882. The goal here would be to output D. Roosevelt was blank in. And then during fine tuning we train the model to predict when the year 1882 when it was asked the question when was Franklin D. Roosevelt born. And you might hope that it kind of recalls back to its pre-training task and recalls some knowledge that it picked up during pre-training in order to answer this question correctly. So we took some standard open domain question answering data sets, natural questions, web questions, and trivia QA. And basically removed all of the context and trained our model to predict the correct answer when asked some particular question and then evaluated its performance on the test set for each of these tasks. And in this table we're comparing the state of the art results for an open domain system. These are systems that explicitly retrieve knowledge from an external knowledge source compared to T5 when it's been trained in this closed book setting. You'll notice that we're actually using a slightly different version of T5 here using T5.1.1. The pertinent difference is just that T5.1.1 was not multitask pre-trained. It was only pre-trained using an unsupervised objective. The reason we did that again is because we want to measure the amount of knowledge that the model picked up during pre-training. And you can see we actually got, you know, reasonably strong performance, maybe respectable performance. I don't want to use the performance again, the accuracy basically on each of these data sets increases as the model size increases, which maybe at a in a loose way suggests that the larger models have picked up more knowledge during pre-training. But we ultimately lagged behind the state of the art results for open domain systems that explicitly retrieve knowledge. So to try to close this gap, we made use of this objective called salient span masking from a paper called retrieval augmented language model pre-training. And salient span masking is a very simple idea. The idea is that rather than masking outwards at random in your pre-training objective, you actually mask out entities explicitly. So that's people's names, places, dates, etc. And you basically just use an off the shelf named end-to-recognizere to figure out what entities are in your pre-training data set. And you train the model to fill in salient spans instead of random spans. So what we did is we took T5.1.1 after it was pre-trained and did continued pre-training on salient span masking and then measured the performance on our downstream tasks after fine tuning. And you can see that the more salient span mask pre-training we did, the better, excuse me, the better and better the performance got when we fine tune on the downstream tasks. And we ultimately were able to close some of these gaps significantly and actually outperform the best open domain system on web questions by adding salient span masking to T5.1.1. So this just is a message to tell you that the objective matters. And doing this kind of now people, at the time people didn't call it this, but now people call this domain adaptive or task adaptive pre-training. And this is a good way of getting better performance on your downstream tasks. So I've got a good set of questions here. It's now a good time. Yes, that'd be great. So for some context, the students in their most recent assignment had to make effectively a mini T5 thing were the only questions that were asked from a simple domain so that they could pre-traine on a single GPU. And one of the questions they have is, how can we be sure that the answer produced by the model is not made up? They were asked this on the assignment as well, I think. So if you don't have access to a ground-treat answer, it's actually very hard to know. There's a nice paper that came out after this paper called, how can we know when language models know? And what the goal of that paper is to make it so that T5 is what we call well calibrated. And when a model is well calibrated, it means that when it doesn't know the answer, it doesn't output highly confident predictions. And this paper explored various ways of calibrating T5 for close-up question answering. And they ultimately found that when the model doesn't know the answer, when it's outputting something made up, that they could effectively make it be very unconfident in its predictions. So that's one way to do it. I think you're actually our muted, John. Great. Thanks. And then another question is the knowledge that's necessary for doing these fine-tuning, like the QA on these fine-tuning datasets. Is it knowledge that is all present at pre-training time? Yeah, so that's also something that was explored by a subsequent paper. We're shown that actually there is a decent amount of training and test overlap in terms of knowledge in these datasets. So it's definitely possible in these cases that that the model is picking up knowledge during fine-tuning and not pre-training. However, just as kind of a side experimental note, we find that the performance of T5 actually plateaus before it makes a single pass over the fine-tuning dataset. So basically T5 will very, very quickly figure out what the heck you're trying to get it to do. It doesn't even see the full training set before it gets, basically it's maximum performance on the test set. So we don't actually think that that's a major factor for for these results. Great. And then last question, how did the if you studied it, how did the multitask model do? Yeah, those results are in the paper. The results are almost exactly the same. It's a little easier to explain the whole knowledge pre-training thing when you are talking about T5.1.1. In the interest of time, I might skip these next three slides, which are the short summary of these slides is just that the evaluation procedure that we use is unfairly penalizes close book question answering systems. If you want to learn a bit more about that, you can poke into the paper a bit. It doesn't really support the main point that I'm trying to make in any meaningful way. So, and I want to get to some of the more recent papers that and I should be able to have time to do that. Cool. So we kind of answered this question, you know, how much knowledge does a model pick up during pre-training? The answer is arguably a lot. So kind of a follow-up question is, does the model is kind of memorizing this knowledge, right? But does it also memorize, do large language models also memorize stuff that we don't want them to memorize, specifically like private data? Like you could imagine, let's say that somewhere in C4 is someone's social security number. We probably don't want our model to memorize that and spit it out when we're decoding from it. And we certainly don't want it to happen for unconditional models like GPT2 or GPT3. So in this next work, we try to answer this question, you know, do large language models memorize stuff from their pre-training data set? And we can first actually turn to experts and see what experts think. And here are two statements made by the EFF and OpenAI. There was sent to the US Patent and Trademark Office when there were a call for comments on, on basically exactly this question. And you can see that in both cases, these organizations basically say, you know, there's basically no reason to believe that a large language model would output, would copy data from its training data set. OpenAI, you know, kind of calls this a well-constructed AI system. And I think what they actually mean by that, if you read their statement a little longer, they kind of say, you know, if you construct an AI system appropriately, the AI system will not overfit to the training data set. And if it's not overfit, we don't expect them to actually output their, their, any of their training set in any non-trivial way. So these are kind of statements that were hunches. And in this work, we tried to investigate more rigorously whether they were true. And the way that we did that was basically by taking the pre-trained GPT2 model and feeding it prefixes. So you can imagine you, you take a causal language model like GPT2 that just predicts tokens auto-aggressively, you feed it a prefix in the basket to basically predict what comes next. And we're showing here this sort of odd prefix East Stroudsburg Stroudsburg. But we found when we fed this particular prefix into GPT2, it actually output for BADOM and address, name, email address, phone number, and fax number of a real person that appears on the internet. This actually example actually only appears six times on the entire public internet. So it's unlikely that GPT2 saw this address very many times. And the main point of this work is that yes, it does seem like GPT2 at least has memorized a significant amount of non-trivial information from its pre-training data set. So how did we undertake this study? We use this procedure that we are, that's shown on the screen here. We basically consider three different ways of sampling data from GPT2. The first is just to sample auto-aggressively from it. The next is to sample auto-aggressively but with a decaying temperature. This basically means that you want the model to become more and more confident in its predictions over the course of sampling. And the last option is to take random text from the internet and use that as conditioning to GPT2 before asking it to generate what comes next. So now for each of these, for each of these generations, each of these 200,000 generations we did for each of these sampling methods, we want a way of kind of trying to predict whether it might be memorized or not. And so we came up with six metrics to use to give us a notion of whether a particular sample from GPT2 might be memorized. All of these different metrics basically make use of GPT2's perplexity for the sample. The perplexity, as I think you probably learned, is basically a measure of how confident GPT2 was in generating this particular sample. You can also think of it as a measure of compression. So these metrics all make use of the perplexity. Either we just measure GPT2's perplexity for the thing that it generated or we compute the ratio of GPT2's perplexity to the perplexity for another variant of GPT or to a text compression library called ZLib. We also compared via a ratio of the perplexity of the original sample versus a lower case version of the sample and also a windowed perplexity where we only compute the perplexity over a small window of the sample instead of the whole thing. We take the top, we do some deduplication on the generation and then choose the top 100 generations according to each of these metrics. So that'll ultimately give us 600 possible memorized generations and then we actually just do a basic, excuse me, a basic Google search to see if we can find that text that GPT2 generated on the internet somewhere. And if we do find it on the internet somewhere, we asked that GPT2 authors was this in the training set or not and they checked for all of the examples that we found and let us know if GPT2 actually spit out something from its training dataset. So just to give you an idea of what these metrics are and why they might be helpful, this scatter plot is showing the perplexity assigned by GPT2 and the perplexity assigned by ZLib to 200,000 samples generated by GPT2. And you can see that most of them kind of fall on this line. There's a big clump of them on the right there in gray. But highlighted here in red and blue are samples that kind of are outliers. GPT2 is assigning a much lower perplexity to ZLib, sorry, to those samples than ZLib is. And what that means is that GPT2 is very, very good at predicting what comes next in these samples and ZLib is not. ZLib is kind of a unbiased source, right? It's not really pre-trained on a bunch of data, it's kind of data agnostic. So it might be the case that if GPT2 is very good at predicting what comes next in a given sequence, but ZLib is not, that GPT2 has memorized those samples. So all of these things kind of in the top left there are possible memorized samples. And actually we marked the ones in blue that it turned out were actually in the training data set and that GPT2 had memorized. Overall, we found many examples of verbatim text memorized from the training data set. This included news, log files, licenses, you know, pages from Wikipedia, URLs, and we highlight two types of data here that we found that GPT2 had memorized that, in our opinion, constitute private information, like named individuals from non-new samples or contact information like the example I showed early on. And so you might ask, okay, you know, maybe it's not that surprising that GPT2 memorized a news article, if that news article appears hundreds of times in the internet. We actually got lucky because it turned out there were a bunch of examples of memorized data that only appeared on one document on the entire public internet. It was basically a paste, like a paste of a bunch of URLs from Reddit, from a controversial subreddit called the Donald. And all of these URLs had exactly the same form. It was, you know, HTTP colon slash slash Reddit.com slash r slash the Donald slash a bunch of random numbers and letters slash the name of the thread. Now this random numbers and letters part is nice because it's equally hard to predict in all cases. It's basically a random hash. So we know that that part of the sequence should be equally hard for any model to memorize. And what that means is that we can measure how many times does a particular URL need to appear in this list of URLs because there were repetitions in the list in order for one of the particular GPT2 sized models to memorize it. And what we found was that the largest variant of GPT2, GPT2 XL, memorized a URL that appeared 33 times in this particular document, but not URLs that appeared 17 times or fewer. The medium size model was only really able to fully memorize a URL that appeared 56 times. And the small model really didn't memorize any. The a half basically means that we could get it to spit out the URL if we gave it some additional prompting. We basically hinted at what some of the numbers were. And what this take away of this is that we actually, by coincidence, because there is this particular document with this structure, we were able to say reasonably confidently that larger models tend to memorize more data. They need to see particular examples fewer times in order to memorize them, which we thought was an interesting finding. So, so far I have kind of mostly been talking about the benefits of larger models, right? Because larger models did better on super glue, larger models did better on close-up question answering. Of course, there is this caveat that larger models also seem to be better at memorizing their training data set. But larger models are also inconvenient. They are more computationally expensive to run. They consume more energy. And they don't fit on, for example, T511B doesn't fit on a single GPU unless you use kind of clever methods. So, the last paper that I'll discuss, which is super recent work, is can we basically close the performance gap between large and small models through improvements to the transformer architecture? So in this work, we take basically the same strategy that we took in the T5 paper, where we took sort of the landscape of existing modifications to the transformer architecture and evaluated them in the same exact setting. And there have been lots of variants proposed to the transformer architecture. In T5, we use basically the standard and coder decoder architecture from the attention as all you need paper that's visualized here. But there have been lots and lots of modifications that have been proposed since the transformer was released in 2017. For example, maybe people suggested that you should factorize your embedding matrix. You should share the embedding matrix and the softmax output layer. You should use different forms of softmax like a mixture of softmaxes or an adaptive softmax. Different ways of normalizing or initializing the model. Maybe different attention mechanisms, alternatives to attention mechanisms like lightweight and dynamical convolutions. Different nonlinearities in the feed forward layers. Different structures for the feed forward layers like mixture of expert to the switch transformer. Completely different architectures that were inspired by the transformer, like the funnel transformer of all transformer, the universal transformer and so on. Really, there have just been tons and tons and tons of them. And again, the goal in this paper was to take a bunch of these modifications and apply the same basic methodology from the T5 paper where we test them in the same experimental setting. Specifically, we basically tested them in exactly the T5 setting that I described at the beginning of the talk where we pre-trained a T5-based size model on C4 and then fine-tuned it on a few downstream tasks. I won't discuss that too much more because I gave a pretty thorough introduction to it at the beginning of the talk. And so here is a sort of a first set of results that I'll show you. Along the x-axis are different transformer modifications. I'm not labeling which one is which because I don't want to call any particular modification out. This is the validation loss attained by the model for the pre-training objective. So when we do pre-training on C4, we hold out some data from C4 and then we basically measure the validation loss on the held out data. So lower is better in this case. And you can see this dotted black line is the performance of the baseline model without any transformer modifications. It's basically of an L-transformer. And you can see that actually some of the transformer modifications did attain better performance, which was good. But a lot of them didn't and a lot of them actually got significantly worse performance. But maybe even worse, some of these variants of the transformer that attained better performance were pretty minor changes. Like for example, just taking the relu in the dense relu dense layer and swapping it with another non-linearity. It's a pretty minor change. And some of the other really highly performant ones were actually ultimately more expensive models. We did use the same base model. It was the same T5 base size model. But some of these methods, like the switch transformer, for example, increases the parameter counter-matically. So it has more expensive in terms of memory. Some of the other methods, by coincidence, maybe if you make the model deeper, it's not able to make use of the accelerator as efficiently. And so it makes the training time and inference time a little more expensive. So once you factor out the very simple changes to the transformer and the ones that ultimately made the model more expensive along some axis, there actually were very few, if any, modifications that improved performance meaningfully. And this is true on the pre-training task. It's also true on the downstream tasks we considered. So this is the Rouge 2 score. It's just one of the metrics people use on the X-Sum task. X-Sum is you can sort of think of it like a harder version of CNN Daily Mail via the Summization task. And you can see that the model variance that attained a better validation score tended to also attain a better X-Sum Rouge 2 score. But again, almost all of the variance we tried decreased the performance. And just as an aside, I kind of alluded to this a little bit. There is a reasonably good correlation between the validation loss and the superglue score. Although I'll just point out a couple of interesting points here. One is this method called transparent attention. It attained a pretty good validation loss, but ultimately a very bad superglue score, which was surprising to us. The switch transformer, which I'll highlight here, attained the best validation loss, but it did not get the best superglue score. On the closed book of variant of web questions, the switch transformer actually achieved merely the best validation accuracy. And this kind of supports a loose conjecture in the field that scaling up the number of parameters can prove the amount of knowledge that the model can internalize. But it doesn't help the model reason. So kind of very, very loosely speaking. Again, this is kind of a conjecture. Superglue requires deep reasoning capabilities. Web, close book web questions requires knowledge intensive capabilities. And so the switch transformer, which only scales up the parameter count without scaling up the processing maybe does better on this on the web questions task. So this kind of raises and should raise some red flags for you. Because this is a pretty bold claim that most of these things don't actually help that much. And there's kind of a couple of possible reasons that this could be the case. One is that our code base is just very unusual and non-standard. We don't think this is the case because the code base that we used was actually developed by one of the people who invented the transformer, a NOMA Shazir. And it's been used a lot. It's been used in lots of various papers. It's basically the same as the tensor to tensor code base. And so we think that arguably our code base and the implementation detail should be reasonably standard. Maybe the tasks we consider are non-standard. We think this is probably not true. Pre-training followed by fine tuning is pretty common. Basically, all the tasks we tested out on have state of the art results from transformers. And actually, we included separately supervised only training on WMT English German, which was the task that transformer was actually proposed on. Maybe we need more hyper parameter tuning because we didn't, again, we didn't do significant hyper parameter tuning for each of these methods. To test how true this was, we actually took one of the methods that we performed significantly worse than we expected. And we ran maybe a couple hundred trials of hyper parameter optimization. One of the researchers on this paper spent a long time trying to get hyper parameters right to make it work. And it ultimately never worked as well as the baseline method. Next possibilities that we implemented these modifications incorrectly. To sanity check this, we actually emailed the authors of all of the different modifications and asked them to check our implementation. All of the ones that got back to us said that it looked correct to them. And then finally, the last option is that maybe these modifications to the transformer don't really kind of transfer. They don't transfer across code bases and implementations and applications. And to us, at least based on the evidence that we have, this is a plausible possibility. In my opinion, the best way to control for this is if you're proposing a new modification to the transformer, try to apply it to as many code bases and tasks as you can without tweaking hyper parameters. And if it works in all of those settings, then your golden and then your thing probably will transfer. And we think that it's probably the case that simpler modifications like changing the non-ladyarity are not so dependent on hyper parameters and implementation details. And so they may be the ones that are more likely to transfer, so to speak. So that's all discussed in this talk. I recognize that I was kind of a whirlwind tour. So I've linked all of the papers that I discussed on this slide here. Of course, this was work done by a huge and truly amazing group of collaborators over the course of these five papers who I've listed on the screen here. And yeah, I'm happy to answer any additional questions that you all have. Okay, so thank you so much Colin for that great talk. And yeah, it was a bit of a fire hose of information. I realize also there was one thing I forgot to say in my introduction. So I guess I need to have an afterward as well, which is that Colin has now started as a professor at the University of North Carolina. So effectively, the University of North Carolina is playing a big part in this course because it was also the source of the Cherokee data that we use for assignment four for the Cherokee English translation. So go tar heels. But yeah, so yeah, so Colin's happy to stay and answer some questions. So if you'd like to have more questions, use the raise hand, and we'll then sort of invite you into the where people can see each other, zoom room, and you know, if you're up to it, it'd even be nice to turn on your video. So people can see who they're talking to. And yeah, maybe in the first instance, you should stop sharing the screen. And yeah, if there's something you want to show again, you can turn it back on. Yeah, maybe I'll just say while people are still around, on the point of me being a professor at UNC in the event that there are any masters or undergraduate students in the audience who are applying for PhD programs, the application deadline for UNC actually has not occurred yet. So if you maybe you wanted apply to another school, have another option, you're excited about the work that I presented, you can still apply to UNC. We have a remarkably late application deadline. So just a plug in case there's anyone who's looking for a PhD. And UNC is the oldest public university in the nation. And if we do the full, you would see the investment. And I think we have the second oldest CS department too, which yeah, it's been around for on. It's pretty small. So it's only about 50 faculty. So while we're waiting for someone to join it, we do have one question already actually from you know, hi, things for the lecture. I had an question about earlier when you discussed like the T-bind overfitting and how many passes you did as I took for it to overfit. So I'm curious as to if you think some of the larger models like the 3 billion, 11 billion, 11 like the scale before players are overfitting and kind of generally how do you know when a model is overfitting, especially on the scale. Yeah, so I mean, if you measure overfitting in sort of the standard way where you compare the loss on training data versus the loss on validation data, we see that even in the in the very, very large models, it's roughly the same, which suggests kind of in the traditional sense that there is no overfitting. One reason for that is that C4 is actually big enough that we do just over one pass over it when we train for a trillion tokens. And you know, you might hope that you see limited overfitting when you only see each piece of data basically once over the course of training. So it's sort of like every time you're seeing data, it's new data. So there's not a huge difference between the data on the training set and the validation set. Of course, there is also this notion of overfitting that's kind of like worst case overfitting, which ties into the memorization work I mentioned. It does seem that it's possible for language models to memorize data, even when they do relatively few passes over the training data set. And you don't see kind of average case overfitting by comparing the training loss and the validation loss. Yep. Okay, then as a question of two. Sure, sorry, I was trying to unmute my video, but I can't do that for forever reason. First of all, thank you so much. This is fantastic. I'm sure you're really enjoying it. One thing that I particularly enjoyed was the work that you guys did on your training data extraction attack, trying to identify, really test this hunch on open AI and the EFF support that these models don't actually memorize. Training data. I'm wondering if you have two questions. One, have open AI and EFF since they're telling, since you actually or has your DM actually published, there's also that and they since acknowledged that, you know, well, constructive models may actually do this. And two, with this approach actually work for detecting other say, accidentally encoded biases towards like extreme biases which are prevalent in some language models, could you be able to create packages that could send those sorts of attacks on these models and then determine some degree of accuracy on how much these biases actually are present. Yeah, so with regards to the first question, I don't know of any official statements that have been made by anyone, but I will say that actually on the memorization paper, we had multiple co-authors from open AI. So it was very much a cooperation with them. I mean, you know, we're all scientists and we're all, you know, we all kind of make hypotheses that sometimes turn out correctly and incorrectly. And so I think open AI is definitely aware and on the side of the fact that, yeah, it is possible that these models might memorize data even when they don't exhibit the traditional signs of overfitting. To the second point, I, the way that people have kind of measured this in an ad hoc way is by feeding a prefix into the model about a particular demographic group or type of person and see what the model says about that person. And I think, I think in principle, you can kind of think of our approach as related to that, except that we have this additional step that kind of measures whether the model is generating that text because it saw it in its training data, basically, because the perplexity is excessively low for some continuation compared to a model that wasn't trained on the same data. So it might be interesting, for example, if you, if you feed the model a prefix that you're asking it to fill in some offensive information about some demographic group, check whether the perplexity of the model for its continuation is dramatically lower than, you know, Z-lib, for example. And in that case, you might think that the bias actually, maybe this like bias that the model has picked up is because it saw some actually a sentence that looked just like that and its training data, or if it's a more kind of loose concept that the model has internalized over the course of training. Good, Dessert. Thank you very much for sharing. Yeah, thanks for the questions. Okay, so next up, I guess is... Right, thank you for the talk that was super interesting. So my question is sort of what are your thoughts on potentially doing like multiple rounds of pre-training? So to make it more concrete, you know, like let's say you have a task somewhere, like something like response generation, and that's very bespoke to the particular response generation data set that you use, but potentially you want to kind of spruce that up by bringing in some general dialogue data set that consists of naturalistic human data. So I'm wondering if you kind of have any thoughts or intuitions on how effective it is to maybe start with, you know, the general internet, and then fine tune on this dialogue, unstructured dialogue data set, and then fine tune on maybe a more kind of tightly-scoped response generation data set. Yeah, so the technique you're describing sounds pretty similar to this really excellent approach that people now call domain adaptive pre-training or task adaptive pre-training. I was introduced in a paper called Don't Stop Pre-Training, and then there's a less catchy subtitle. And the idea is very similar to what you proposed. Basically, you take a pre-trained model that was trained on generic text, you do what you might call intermediate task training, or you do continued pre-training on domain specific data, and then finally you do fine tuning on your specific fine tuning data set. In their case, they're considering things like, you know, fine, like doing a scientific text classification or biomedical text analysis, and when they do an intermediate pre-training step on in-domain data, or even just doing the pre-training objective on the data from the task, it definitely helps significantly. And yeah, so that's a very excellent intuition, and I think that's the most similar method to what you're describing. It does kind of raise a clear question that I don't think has been addressed to my knowledge in the literature, which is, you know, we usually think of transfer learning as pre-trained and then fine-tune. And now we're doing kind of like pre-trained, and then maybe some more pre-training and then fine-tuning. And there are other methods that kind of inject other steps along the way. And so there's this natural question of like, what should the curriculum of tasks be, you know, how many intermediate steps should there be? What should the intermediate steps be? What's the benefit of one domain versus the other? How much domain shift is there? And what are the corresponding benefits and so on? And I think there would be, there's a fascinating line of work that would be basically better answering those questions. And what was the acronym? Yeah, so it's called Adapt or Taped, Domain Adaptive Pre-training or Task Adaptive Pre-training. The other papers called Don't Stop Pre-training, which is easy to remember if you like the song Don't Stop Believe in, which is how I, I don't know if it's an intended reference to that song. I assume it must be. I think they should have done a Don't Stop Pre-training with an apostrophe if they really wanted to drive it home, but, you know, maybe that would have been too cheesy. But anyways, yeah, the papers called Don't Stop Pre-training. All right, that's great. Thank you so much. Yeah, absolutely. Okay, next question is, and I'm not sure quite what they know corresponds to. Yes, hi, thank you, Colin, for that part of the really interesting. I'm again kind of looking at the question, I'm really looking for some advice here. So it feels like the recent headline grabbing your dancements and the end of being the tree have been achieved by building these massive models, like GPT-3, with down to parameters that oftentimes cost millions of dollars you can. And these dancements are funded by like larger organizations like Google, Facebook, OpenAI, who are less have infinite resources in the way, right? So my question is, you know, as a sole practitioner with limited resources, but an infinite appetite for learning. What are some ways that I can participate in these advancements and kind of, you know, just rotate and we're top-ending in the industry. Yeah, absolutely. I mean, I, I, I actually totally sympathize with you and agree with you in the sense that, you know, most of the development of these models is taking place by small groups behind closed doors at at large corporations. And that's not usually how I like to see, you know, science developed. I like to see it as a community endeavor that involves, you know, all kinds of stakeholders with varying amounts of resources. And we're not really quite at that stage with with this work yet. I do think that to the extent that people are still releasing pre-trained models, which is true, for example, for T5, but, but not for GPT-3, there is a lot of work to be done on basically analysis. Some of the stuff that that we were discussing earlier, you know, even the, memorization work is basically, I would say it's like analysis work. Some of the stuff pertaining to bias involves in analyzing these models. And I think there's, you know, there's, there's so little that we actually know about how these models work and what makes them useful at scale. The, there's plenty of room for interesting analytical work, which requires significantly less compute. I guess I would say a couple other things. One is that I do really hope that the field moves more towards community development models and moves towards frameworks that allow people to collaboratively train a model, for example, like in a distributed fashion. I think that's an incredibly setting research direction. It's something that I'm working with my students on in my lab at UNC now. And, and the last thing I'll say, and I actually don't usually like saying this, but I'll say it anyways, I do think that our field often undergoes sort of a tick-tock pattern where we show something as possible at scale, and then we show that the scale is not necessary to achieve the same thing. And to some extent, you could argue that this has happened already for GPT-3, in the sense that we saw GPT-3 come along, get outstanding results on, for example, superglue, with only 32 examples per class. And then there was the paper that proposed this method called iPad, which I think is interactive, an iterative patterned, exploited training that obtained basically comparable performance in a dramatically smaller model with the same amount of data. And, and you know, I think you can point to other examples. I personally like to attribute the story of attentions invention to the fact that researchers at the Montreal Institute for Learning Algorithms couldn't afford an AGPU machine, so they couldn't run the giant LSTM in the sequence of sequence paper. So they needed to invent something that worked better, but didn't require such a big model, so they invented attention. Of course, it's not good advice to get to tell someone that they should just go invent something smaller, but I'm at least hopeful that some of these things that we've shown are possible at scale are also possible at a much smaller scale. Thank you. Yeah. Okay, I think there's no one else who's have a hand up at the moment. Maybe there's now some moment for John to ask his question, but if any other people have questions, now's a good point to jump in. I was just going to ask a question from the Q&A and came in and asked it live, so. Yeah, I will say one other thing just quickly, which is that, you know, T5, I was very, like I said, I was very excited that we achieved near-human performance on Super Glue. The model that came along and actually closed that 0.5 percent gap is a model that is about, you know, 10 times smaller in terms of parameter count, so that's like another reasonable example of, I mean, it's still quite big, but at least, you know, as you make algorithmic and architectural improvements, sometimes you can close these gaps. Well, thank you again, Colin, and let you, whatever, have a beer and go to bed or something. Yeah, yeah, sounds great. Yeah, thanks again for having me. It's such a pleasure. So, and I should say if anyone has any follow-up questions I think of later on, I'm always excited to get emails about this kind of stuff. It's, you know, this is stuff I like working on, so. Yeah, thank you again for the great informative talk.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.6, "text": " Okay, so hi everyone. Welcome back to CS224N. So today we're get to have the second of", "tokens": [1033, 11, 370, 4879, 1518, 13, 4027, 646, 281, 9460, 7490, 19, 45, 13, 407, 965, 321, 434, 483, 281, 362, 264, 1150, 295], "temperature": 0.0, "avg_logprob": -0.2807945554906672, "compression_ratio": 1.4935622317596566, "no_speech_prob": 0.1043248176574707}, {"id": 1, "seek": 0, "start": 12.6, "end": 17.6, "text": " our invited speakers for this quarter. And so given all of the excitement that there's", "tokens": [527, 9185, 9518, 337, 341, 6555, 13, 400, 370, 2212, 439, 295, 264, 14755, 300, 456, 311], "temperature": 0.0, "avg_logprob": -0.2807945554906672, "compression_ratio": 1.4935622317596566, "no_speech_prob": 0.1043248176574707}, {"id": 2, "seek": 0, "start": 17.6, "end": 22.76, "text": " been about transformer pre-trained language models and all the great things have been", "tokens": [668, 466, 31782, 659, 12, 17227, 2001, 2856, 5245, 293, 439, 264, 869, 721, 362, 668], "temperature": 0.0, "avg_logprob": -0.2807945554906672, "compression_ratio": 1.4935622317596566, "no_speech_prob": 0.1043248176574707}, {"id": 3, "seek": 0, "start": 22.76, "end": 28.080000000000002, "text": " done with them. We're especially excited today to be able to welcome Colin Reffel, who's", "tokens": [1096, 365, 552, 13, 492, 434, 2318, 2919, 965, 281, 312, 1075, 281, 2928, 29253, 1300, 602, 338, 11, 567, 311], "temperature": 0.0, "avg_logprob": -0.2807945554906672, "compression_ratio": 1.4935622317596566, "no_speech_prob": 0.1043248176574707}, {"id": 4, "seek": 2808, "start": 28.08, "end": 33.48, "text": " been one of the key people who've been pushing the exploration of large pre-trained language", "tokens": [668, 472, 295, 264, 2141, 561, 567, 600, 668, 7380, 264, 16197, 295, 2416, 659, 12, 17227, 2001, 2856], "temperature": 0.0, "avg_logprob": -0.14283726636101218, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.0005315285525284708}, {"id": 5, "seek": 2808, "start": 33.48, "end": 39.879999999999995, "text": " models in particular. He was very interested in the development of the T5 language model,", "tokens": [5245, 294, 1729, 13, 634, 390, 588, 3102, 294, 264, 3250, 295, 264, 314, 20, 2856, 2316, 11], "temperature": 0.0, "avg_logprob": -0.14283726636101218, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.0005315285525284708}, {"id": 6, "seek": 2808, "start": 39.879999999999995, "end": 46.599999999999994, "text": " which you'll be telling you plenty about today. But to tell you a few more sentences, so Colin", "tokens": [597, 291, 603, 312, 3585, 291, 7140, 466, 965, 13, 583, 281, 980, 291, 257, 1326, 544, 16579, 11, 370, 29253], "temperature": 0.0, "avg_logprob": -0.14283726636101218, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.0005315285525284708}, {"id": 7, "seek": 2808, "start": 46.599999999999994, "end": 51.28, "text": " worked for a number of years at Google Brain, including working with Jeff Hinton on", "tokens": [2732, 337, 257, 1230, 295, 924, 412, 3329, 29783, 11, 3009, 1364, 365, 7506, 389, 12442, 322], "temperature": 0.0, "avg_logprob": -0.14283726636101218, "compression_ratio": 1.5427350427350428, "no_speech_prob": 0.0005315285525284708}, {"id": 8, "seek": 5128, "start": 51.28, "end": 58.6, "text": " Captural Networks. He then got interested in the effectiveness of transfer using pre-large", "tokens": [9480, 1807, 12640, 82, 13, 634, 550, 658, 3102, 294, 264, 21208, 295, 5003, 1228, 659, 12, 2200, 432], "temperature": 0.0, "avg_logprob": -0.1758122797365542, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0008385718101635575}, {"id": 9, "seek": 5128, "start": 58.6, "end": 63.56, "text": " pre-trained language models. And as part of the work on that, he started with, together", "tokens": [659, 12, 17227, 2001, 2856, 5245, 13, 400, 382, 644, 295, 264, 589, 322, 300, 11, 415, 1409, 365, 11, 1214], "temperature": 0.0, "avg_logprob": -0.1758122797365542, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0008385718101635575}, {"id": 10, "seek": 5128, "start": 63.56, "end": 68.44, "text": " with other people on building even bigger, large pre-trained language models and doing", "tokens": [365, 661, 561, 322, 2390, 754, 3801, 11, 2416, 659, 12, 17227, 2001, 2856, 5245, 293, 884], "temperature": 0.0, "avg_logprob": -0.1758122797365542, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0008385718101635575}, {"id": 11, "seek": 5128, "start": 68.44, "end": 73.76, "text": " a lot of investigations of those, which led to the T5 papers that you'll be hearing", "tokens": [257, 688, 295, 25582, 295, 729, 11, 597, 4684, 281, 264, 314, 20, 10577, 300, 291, 603, 312, 4763], "temperature": 0.0, "avg_logprob": -0.1758122797365542, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0008385718101635575}, {"id": 12, "seek": 5128, "start": 73.76, "end": 78.72, "text": " about today. So welcome, Colin. Right, yeah, thanks so much for the introduction and for", "tokens": [466, 965, 13, 407, 2928, 11, 29253, 13, 1779, 11, 1338, 11, 3231, 370, 709, 337, 264, 9339, 293, 337], "temperature": 0.0, "avg_logprob": -0.1758122797365542, "compression_ratio": 1.6528301886792454, "no_speech_prob": 0.0008385718101635575}, {"id": 13, "seek": 7872, "start": 78.72, "end": 85.12, "text": " having me. It's definitely an honor to speak at the legendary CS224N class. So yeah, I'm", "tokens": [1419, 385, 13, 467, 311, 2138, 364, 5968, 281, 1710, 412, 264, 16698, 9460, 7490, 19, 45, 1508, 13, 407, 1338, 11, 286, 478], "temperature": 0.0, "avg_logprob": -0.127221003445712, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00022332627850119025}, {"id": 14, "seek": 7872, "start": 85.12, "end": 91.6, "text": " going to be talking today about large language models kind of in general, but focusing specifically", "tokens": [516, 281, 312, 1417, 965, 466, 2416, 2856, 5245, 733, 295, 294, 2674, 11, 457, 8416, 4682], "temperature": 0.0, "avg_logprob": -0.127221003445712, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00022332627850119025}, {"id": 15, "seek": 7872, "start": 91.6, "end": 97.64, "text": " on this model T5 that we released about a year and a half ago. I'll be presenting five", "tokens": [322, 341, 2316, 314, 20, 300, 321, 4736, 466, 257, 1064, 293, 257, 1922, 2057, 13, 286, 603, 312, 15578, 1732], "temperature": 0.0, "avg_logprob": -0.127221003445712, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00022332627850119025}, {"id": 16, "seek": 7872, "start": 97.64, "end": 103.0, "text": " or so papers that kind of should represent the full spectrum of good things, bad things", "tokens": [420, 370, 10577, 300, 733, 295, 820, 2906, 264, 1577, 11143, 295, 665, 721, 11, 1578, 721], "temperature": 0.0, "avg_logprob": -0.127221003445712, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00022332627850119025}, {"id": 17, "seek": 7872, "start": 103.0, "end": 107.08, "text": " and ugly things about large language models. And actually, I'll talk a bit about a paper", "tokens": [293, 12246, 721, 466, 2416, 2856, 5245, 13, 400, 767, 11, 286, 603, 751, 257, 857, 466, 257, 3035], "temperature": 0.0, "avg_logprob": -0.127221003445712, "compression_ratio": 1.692883895131086, "no_speech_prob": 0.00022332627850119025}, {"id": 18, "seek": 10708, "start": 107.08, "end": 112.56, "text": " that just appeared on archive last night. So hopefully everyone will learn something new", "tokens": [300, 445, 8516, 322, 23507, 1036, 1818, 13, 407, 4696, 1518, 486, 1466, 746, 777], "temperature": 0.0, "avg_logprob": -0.12968388887552115, "compression_ratio": 1.7244094488188977, "no_speech_prob": 8.217996946768835e-05}, {"id": 19, "seek": 10708, "start": 112.56, "end": 118.88, "text": " today, even if you're already familiar with some of these papers. So just to give you an", "tokens": [965, 11, 754, 498, 291, 434, 1217, 4963, 365, 512, 295, 613, 10577, 13, 407, 445, 281, 976, 291, 364], "temperature": 0.0, "avg_logprob": -0.12968388887552115, "compression_ratio": 1.7244094488188977, "no_speech_prob": 8.217996946768835e-05}, {"id": 20, "seek": 10708, "start": 118.88, "end": 122.67999999999999, "text": " idea of what I'll be covering in this talk, I kind of will be answering each of these", "tokens": [1558, 295, 437, 286, 603, 312, 10322, 294, 341, 751, 11, 286, 733, 295, 486, 312, 13430, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.12968388887552115, "compression_ratio": 1.7244094488188977, "no_speech_prob": 8.217996946768835e-05}, {"id": 21, "seek": 10708, "start": 122.67999999999999, "end": 129.84, "text": " questions in turn in the over the course of the presentation. And I should mention that", "tokens": [1651, 294, 1261, 294, 264, 670, 264, 1164, 295, 264, 5860, 13, 400, 286, 820, 2152, 300], "temperature": 0.0, "avg_logprob": -0.12968388887552115, "compression_ratio": 1.7244094488188977, "no_speech_prob": 8.217996946768835e-05}, {"id": 22, "seek": 10708, "start": 129.84, "end": 135.04, "text": " this, since some of these papers are new and some of this material is new, this is the", "tokens": [341, 11, 1670, 512, 295, 613, 10577, 366, 777, 293, 512, 295, 341, 2527, 307, 777, 11, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.12968388887552115, "compression_ratio": 1.7244094488188977, "no_speech_prob": 8.217996946768835e-05}, {"id": 23, "seek": 13504, "start": 135.04, "end": 139.07999999999998, "text": " first time I'll be presenting these slides. So if anything is confusing, I understand", "tokens": [700, 565, 286, 603, 312, 15578, 613, 9788, 13, 407, 498, 1340, 307, 13181, 11, 286, 1223], "temperature": 0.0, "avg_logprob": -0.1433913774579485, "compression_ratio": 1.6833976833976834, "no_speech_prob": 7.02950928825885e-05}, {"id": 24, "seek": 13504, "start": 139.07999999999998, "end": 143.76, "text": " there's a way for you to pin questions to me through the hosts and feel free to ask", "tokens": [456, 311, 257, 636, 337, 291, 281, 5447, 1651, 281, 385, 807, 264, 21573, 293, 841, 1737, 281, 1029], "temperature": 0.0, "avg_logprob": -0.1433913774579485, "compression_ratio": 1.6833976833976834, "no_speech_prob": 7.02950928825885e-05}, {"id": 25, "seek": 13504, "start": 143.76, "end": 148.95999999999998, "text": " about anything that might turn out to be confusing. So yeah, the first question that I'll try", "tokens": [466, 1340, 300, 1062, 1261, 484, 281, 312, 13181, 13, 407, 1338, 11, 264, 700, 1168, 300, 286, 603, 853], "temperature": 0.0, "avg_logprob": -0.1433913774579485, "compression_ratio": 1.6833976833976834, "no_speech_prob": 7.02950928825885e-05}, {"id": 26, "seek": 13504, "start": 148.95999999999998, "end": 157.23999999999998, "text": " to answer is kind of the answer we sought out to focus on in the T5 paper, which is,", "tokens": [281, 1867, 307, 733, 295, 264, 1867, 321, 17532, 484, 281, 1879, 322, 294, 264, 314, 20, 3035, 11, 597, 307, 11], "temperature": 0.0, "avg_logprob": -0.1433913774579485, "compression_ratio": 1.6833976833976834, "no_speech_prob": 7.02950928825885e-05}, {"id": 27, "seek": 13504, "start": 157.23999999999998, "end": 160.88, "text": " you know, which of the transfer learning methods people have proposed so far work best.", "tokens": [291, 458, 11, 597, 295, 264, 5003, 2539, 7150, 561, 362, 10348, 370, 1400, 589, 1151, 13], "temperature": 0.0, "avg_logprob": -0.1433913774579485, "compression_ratio": 1.6833976833976834, "no_speech_prob": 7.02950928825885e-05}, {"id": 28, "seek": 16088, "start": 160.88, "end": 168.28, "text": " And what happens when we scale them up? And then after the T5 paper, we decided to investigate", "tokens": [400, 437, 2314, 562, 321, 4373, 552, 493, 30, 400, 550, 934, 264, 314, 20, 3035, 11, 321, 3047, 281, 15013], "temperature": 0.0, "avg_logprob": -0.11878655696737356, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.00016859901370480657}, {"id": 29, "seek": 16088, "start": 168.28, "end": 172.88, "text": " non-English pre-trained language models. T5 is an English only model. There are lots", "tokens": [2107, 12, 31254, 1933, 659, 12, 17227, 2001, 2856, 5245, 13, 314, 20, 307, 364, 3669, 787, 2316, 13, 821, 366, 3195], "temperature": 0.0, "avg_logprob": -0.11878655696737356, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.00016859901370480657}, {"id": 30, "seek": 16088, "start": 172.88, "end": 177.8, "text": " of languages spoken in the world. So what happens when we modify T5 so that it's a massively", "tokens": [295, 8650, 10759, 294, 264, 1002, 13, 407, 437, 2314, 562, 321, 16927, 314, 20, 370, 300, 309, 311, 257, 29379], "temperature": 0.0, "avg_logprob": -0.11878655696737356, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.00016859901370480657}, {"id": 31, "seek": 16088, "start": 177.8, "end": 182.68, "text": " multi-lingual model? And then I'll talk about another paper where we try to investigate", "tokens": [4825, 12, 1688, 901, 2316, 30, 400, 550, 286, 603, 751, 466, 1071, 3035, 689, 321, 853, 281, 15013], "temperature": 0.0, "avg_logprob": -0.11878655696737356, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.00016859901370480657}, {"id": 32, "seek": 16088, "start": 182.68, "end": 188.32, "text": " what kinds of knowledge and how much knowledge a model picks up over the course of pre-training.", "tokens": [437, 3685, 295, 3601, 293, 577, 709, 3601, 257, 2316, 16137, 493, 670, 264, 1164, 295, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.11878655696737356, "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.00016859901370480657}, {"id": 33, "seek": 18832, "start": 188.32, "end": 192.64, "text": " And then, relatedly in a follow-up work, we tried to figure out if the model actually", "tokens": [400, 550, 11, 4077, 356, 294, 257, 1524, 12, 1010, 589, 11, 321, 3031, 281, 2573, 484, 498, 264, 2316, 767], "temperature": 0.0, "avg_logprob": -0.13630542405154727, "compression_ratio": 1.6539923954372624, "no_speech_prob": 3.4802571462932974e-05}, {"id": 34, "seek": 18832, "start": 192.64, "end": 197.32, "text": " memorizes training data during pre-training. If it actually, if we can get it to spit", "tokens": [10560, 5660, 3097, 1412, 1830, 659, 12, 17227, 1760, 13, 759, 309, 767, 11, 498, 321, 393, 483, 309, 281, 22127], "temperature": 0.0, "avg_logprob": -0.13630542405154727, "compression_ratio": 1.6539923954372624, "no_speech_prob": 3.4802571462932974e-05}, {"id": 35, "seek": 18832, "start": 197.32, "end": 203.16, "text": " out verbatim entries from the training data set after it's been trained. And then finally,", "tokens": [484, 9595, 267, 332, 23041, 490, 264, 3097, 1412, 992, 934, 309, 311, 668, 8895, 13, 400, 550, 2721, 11], "temperature": 0.0, "avg_logprob": -0.13630542405154727, "compression_ratio": 1.6539923954372624, "no_speech_prob": 3.4802571462932974e-05}, {"id": 36, "seek": 18832, "start": 203.16, "end": 208.16, "text": " I'll talk about a very recent work that's similar in spirit to the T5 paper where the", "tokens": [286, 603, 751, 466, 257, 588, 5162, 589, 300, 311, 2531, 294, 3797, 281, 264, 314, 20, 3035, 689, 264], "temperature": 0.0, "avg_logprob": -0.13630542405154727, "compression_ratio": 1.6539923954372624, "no_speech_prob": 3.4802571462932974e-05}, {"id": 37, "seek": 18832, "start": 208.16, "end": 212.84, "text": " goal is to answer not what transfer learning methods work best, but what modifications", "tokens": [3387, 307, 281, 1867, 406, 437, 5003, 2539, 7150, 589, 1151, 11, 457, 437, 26881], "temperature": 0.0, "avg_logprob": -0.13630542405154727, "compression_ratio": 1.6539923954372624, "no_speech_prob": 3.4802571462932974e-05}, {"id": 38, "seek": 21284, "start": 212.84, "end": 219.24, "text": " to the transformer architecture that people have proposed to work best. So just to motivate", "tokens": [281, 264, 31782, 9482, 300, 561, 362, 10348, 281, 589, 1151, 13, 407, 445, 281, 28497], "temperature": 0.0, "avg_logprob": -0.10511213302612304, "compression_ratio": 1.7300380228136882, "no_speech_prob": 7.365446799667552e-05}, {"id": 39, "seek": 21284, "start": 219.24, "end": 223.12, "text": " this, I actually looked through some of the lectures that you all have had so far just", "tokens": [341, 11, 286, 767, 2956, 807, 512, 295, 264, 16564, 300, 291, 439, 362, 632, 370, 1400, 445], "temperature": 0.0, "avg_logprob": -0.10511213302612304, "compression_ratio": 1.7300380228136882, "no_speech_prob": 7.365446799667552e-05}, {"id": 40, "seek": 21284, "start": 223.12, "end": 228.08, "text": " to get a sense of what you've learned already. And I know that you are already pretty familiar", "tokens": [281, 483, 257, 2020, 295, 437, 291, 600, 3264, 1217, 13, 400, 286, 458, 300, 291, 366, 1217, 1238, 4963], "temperature": 0.0, "avg_logprob": -0.10511213302612304, "compression_ratio": 1.7300380228136882, "no_speech_prob": 7.365446799667552e-05}, {"id": 41, "seek": 21284, "start": 228.08, "end": 232.4, "text": " with this transfer learning paradigm that has kind of taken the field of natural language", "tokens": [365, 341, 5003, 2539, 24709, 300, 575, 733, 295, 2726, 264, 2519, 295, 3303, 2856], "temperature": 0.0, "avg_logprob": -0.10511213302612304, "compression_ratio": 1.7300380228136882, "no_speech_prob": 7.365446799667552e-05}, {"id": 42, "seek": 21284, "start": 232.4, "end": 238.68, "text": " processing by storm. But just as a quick refresher in this style of transfer learning, what", "tokens": [9007, 538, 7679, 13, 583, 445, 382, 257, 1702, 17368, 511, 294, 341, 3758, 295, 5003, 2539, 11, 437], "temperature": 0.0, "avg_logprob": -0.10511213302612304, "compression_ratio": 1.7300380228136882, "no_speech_prob": 7.365446799667552e-05}, {"id": 43, "seek": 23868, "start": 238.68, "end": 245.28, "text": " we typically do is take a bunch of unlabeled text data and we apply some unsupervised", "tokens": [321, 5850, 360, 307, 747, 257, 3840, 295, 32118, 18657, 292, 2487, 1412, 293, 321, 3079, 512, 2693, 12879, 24420], "temperature": 0.0, "avg_logprob": -0.11533432453870773, "compression_ratio": 1.886120996441281, "no_speech_prob": 4.6828230551909655e-05}, {"id": 44, "seek": 23868, "start": 245.28, "end": 249.04000000000002, "text": " objective, where you might say a self-supervised objective, where you do something like you", "tokens": [10024, 11, 689, 291, 1062, 584, 257, 2698, 12, 48172, 24420, 10024, 11, 689, 291, 360, 746, 411, 291], "temperature": 0.0, "avg_logprob": -0.11533432453870773, "compression_ratio": 1.886120996441281, "no_speech_prob": 4.6828230551909655e-05}, {"id": 45, "seek": 23868, "start": 249.04000000000002, "end": 252.88, "text": " mask out words at random and then you train the model to predict the missing words. So", "tokens": [6094, 484, 2283, 412, 4974, 293, 550, 291, 3847, 264, 2316, 281, 6069, 264, 5361, 2283, 13, 407], "temperature": 0.0, "avg_logprob": -0.11533432453870773, "compression_ratio": 1.886120996441281, "no_speech_prob": 4.6828230551909655e-05}, {"id": 46, "seek": 23868, "start": 252.88, "end": 258.24, "text": " you can see these blanks in the block of text in green and the missing words that the", "tokens": [291, 393, 536, 613, 8247, 82, 294, 264, 3461, 295, 2487, 294, 3092, 293, 264, 5361, 2283, 300, 264], "temperature": 0.0, "avg_logprob": -0.11533432453870773, "compression_ratio": 1.886120996441281, "no_speech_prob": 4.6828230551909655e-05}, {"id": 47, "seek": 23868, "start": 258.24, "end": 262.24, "text": " language model will be trained to predict in the yellow box at the bottom. And then", "tokens": [2856, 2316, 486, 312, 8895, 281, 6069, 294, 264, 5566, 2424, 412, 264, 2767, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.11533432453870773, "compression_ratio": 1.886120996441281, "no_speech_prob": 4.6828230551909655e-05}, {"id": 48, "seek": 23868, "start": 262.24, "end": 267.24, "text": " after we do this pre-training for a while, we fine tune the model on some downstream supervised", "tokens": [934, 321, 360, 341, 659, 12, 17227, 1760, 337, 257, 1339, 11, 321, 2489, 10864, 264, 2316, 322, 512, 30621, 46533], "temperature": 0.0, "avg_logprob": -0.11533432453870773, "compression_ratio": 1.886120996441281, "no_speech_prob": 4.6828230551909655e-05}, {"id": 49, "seek": 26724, "start": 267.24, "end": 273.84000000000003, "text": " task. In this example, I'm showing a sentiment analysis task for movie reviews. And the", "tokens": [5633, 13, 682, 341, 1365, 11, 286, 478, 4099, 257, 16149, 5215, 5633, 337, 3169, 10229, 13, 400, 264], "temperature": 0.0, "avg_logprob": -0.10006455541814416, "compression_ratio": 1.6088560885608856, "no_speech_prob": 8.479339885525405e-05}, {"id": 50, "seek": 26724, "start": 273.84000000000003, "end": 278.44, "text": " upshot of all of this is that doing this first unsupervised pre-training step is just", "tokens": [493, 18402, 295, 439, 295, 341, 307, 300, 884, 341, 700, 2693, 12879, 24420, 659, 12, 17227, 1760, 1823, 307, 445], "temperature": 0.0, "avg_logprob": -0.10006455541814416, "compression_ratio": 1.6088560885608856, "no_speech_prob": 8.479339885525405e-05}, {"id": 51, "seek": 26724, "start": 278.44, "end": 282.68, "text": " ridiculously helpful. Not only does it usually make the performance better, it often gets", "tokens": [41358, 4961, 13, 1726, 787, 775, 309, 2673, 652, 264, 3389, 1101, 11, 309, 2049, 2170], "temperature": 0.0, "avg_logprob": -0.10006455541814416, "compression_ratio": 1.6088560885608856, "no_speech_prob": 8.479339885525405e-05}, {"id": 52, "seek": 26724, "start": 282.68, "end": 287.08, "text": " you very good performance with relatively little fine-tuning data compared to training", "tokens": [291, 588, 665, 3389, 365, 7226, 707, 2489, 12, 83, 37726, 1412, 5347, 281, 3097], "temperature": 0.0, "avg_logprob": -0.10006455541814416, "compression_ratio": 1.6088560885608856, "no_speech_prob": 8.479339885525405e-05}, {"id": 53, "seek": 26724, "start": 287.08, "end": 291.84000000000003, "text": " from scratch. So this is really a very common, it's kind of the de facto standard way", "tokens": [490, 8459, 13, 407, 341, 307, 534, 257, 588, 2689, 11, 309, 311, 733, 295, 264, 368, 42225, 3832, 636], "temperature": 0.0, "avg_logprob": -0.10006455541814416, "compression_ratio": 1.6088560885608856, "no_speech_prob": 8.479339885525405e-05}, {"id": 54, "seek": 29184, "start": 291.84, "end": 298.59999999999997, "text": " to attack many natural language processing problems now. And I think you've already reviewed", "tokens": [281, 2690, 867, 3303, 2856, 9007, 2740, 586, 13, 400, 286, 519, 291, 600, 1217, 18429], "temperature": 0.0, "avg_logprob": -0.1520374584197998, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.00010388659575255588}, {"id": 55, "seek": 29184, "start": 298.59999999999997, "end": 304.35999999999996, "text": " some of these methods, but because of how effective the transfer learning recipe was, there", "tokens": [512, 295, 613, 7150, 11, 457, 570, 295, 577, 4942, 264, 5003, 2539, 6782, 390, 11, 456], "temperature": 0.0, "avg_logprob": -0.1520374584197998, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.00010388659575255588}, {"id": 56, "seek": 29184, "start": 304.35999999999996, "end": 309.79999999999995, "text": " really was kind of an explosion of work on transfer learning starting in maybe 2018", "tokens": [534, 390, 733, 295, 364, 15673, 295, 589, 322, 5003, 2539, 2891, 294, 1310, 6096], "temperature": 0.0, "avg_logprob": -0.1520374584197998, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.00010388659575255588}, {"id": 57, "seek": 29184, "start": 309.79999999999995, "end": 315.55999999999995, "text": " or so. There's some prior work on other approaches to doing transfer learning like word vectors,", "tokens": [420, 370, 13, 821, 311, 512, 4059, 589, 322, 661, 11587, 281, 884, 5003, 2539, 411, 1349, 18875, 11], "temperature": 0.0, "avg_logprob": -0.1520374584197998, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.00010388659575255588}, {"id": 58, "seek": 29184, "start": 315.55999999999995, "end": 320.59999999999997, "text": " like word-to-vec, some sort of preliminary work that proposed the recipe that I just described", "tokens": [411, 1349, 12, 1353, 12, 303, 66, 11, 512, 1333, 295, 28817, 589, 300, 10348, 264, 6782, 300, 286, 445, 7619], "temperature": 0.0, "avg_logprob": -0.1520374584197998, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.00010388659575255588}, {"id": 59, "seek": 32060, "start": 320.6, "end": 325.04, "text": " called semi-supervised sequence learning, some work that suggested that this kind of stuff", "tokens": [1219, 12909, 12, 48172, 24420, 8310, 2539, 11, 512, 589, 300, 10945, 300, 341, 733, 295, 1507], "temperature": 0.0, "avg_logprob": -0.1971165452684675, "compression_ratio": 1.6245487364620939, "no_speech_prob": 8.09076736913994e-05}, {"id": 60, "seek": 32060, "start": 325.04, "end": 329.40000000000003, "text": " might be really possible, like the unsupervised sentiment on paper. But I would say around", "tokens": [1062, 312, 534, 1944, 11, 411, 264, 2693, 12879, 24420, 16149, 322, 3035, 13, 583, 286, 576, 584, 926], "temperature": 0.0, "avg_logprob": -0.1971165452684675, "compression_ratio": 1.6245487364620939, "no_speech_prob": 8.09076736913994e-05}, {"id": 61, "seek": 32060, "start": 329.40000000000003, "end": 335.32000000000005, "text": " in 2018, there was kind of a string of papers that kicked off the excitement in the field,", "tokens": [294, 6096, 11, 456, 390, 733, 295, 257, 6798, 295, 10577, 300, 14609, 766, 264, 14755, 294, 264, 2519, 11], "temperature": 0.0, "avg_logprob": -0.1971165452684675, "compression_ratio": 1.6245487364620939, "no_speech_prob": 8.09076736913994e-05}, {"id": 62, "seek": 32060, "start": 335.32000000000005, "end": 339.8, "text": " including the universal language model fine-tuning paper, how the Elmo paper, what we now", "tokens": [3009, 264, 11455, 2856, 2316, 2489, 12, 83, 37726, 3035, 11, 577, 264, 2699, 3280, 3035, 11, 437, 321, 586], "temperature": 0.0, "avg_logprob": -0.1971165452684675, "compression_ratio": 1.6245487364620939, "no_speech_prob": 8.09076736913994e-05}, {"id": 63, "seek": 32060, "start": 339.8, "end": 346.04, "text": " call GPT-1, and of course the BERT paper in late 2018. And then starting in 2019, there", "tokens": [818, 26039, 51, 12, 16, 11, 293, 295, 1164, 264, 363, 31479, 3035, 294, 3469, 6096, 13, 400, 550, 2891, 294, 6071, 11, 456], "temperature": 0.0, "avg_logprob": -0.1971165452684675, "compression_ratio": 1.6245487364620939, "no_speech_prob": 8.09076736913994e-05}, {"id": 64, "seek": 34604, "start": 346.04, "end": 351.16, "text": " really was just an incredible explosion of different methods for doing transfer learning, including", "tokens": [534, 390, 445, 364, 4651, 15673, 295, 819, 7150, 337, 884, 5003, 2539, 11, 3009], "temperature": 0.0, "avg_logprob": -0.14282558161184328, "compression_ratio": 1.736842105263158, "no_speech_prob": 5.224078267929144e-05}, {"id": 65, "seek": 34604, "start": 351.16, "end": 357.24, "text": " new transfer learning, sorry, new pre-training objectives, new data sets, new ways of doing", "tokens": [777, 5003, 2539, 11, 2597, 11, 777, 659, 12, 17227, 1760, 15961, 11, 777, 1412, 6352, 11, 777, 2098, 295, 884], "temperature": 0.0, "avg_logprob": -0.14282558161184328, "compression_ratio": 1.736842105263158, "no_speech_prob": 5.224078267929144e-05}, {"id": 66, "seek": 34604, "start": 357.24, "end": 365.6, "text": " fine-tuning, and so on. And we started working on the T5 project in late 2018, and we noticed", "tokens": [2489, 12, 83, 37726, 11, 293, 370, 322, 13, 400, 321, 1409, 1364, 322, 264, 314, 20, 1716, 294, 3469, 6096, 11, 293, 321, 5694], "temperature": 0.0, "avg_logprob": -0.14282558161184328, "compression_ratio": 1.736842105263158, "no_speech_prob": 5.224078267929144e-05}, {"id": 67, "seek": 34604, "start": 365.6, "end": 369.68, "text": " that as all these methods were coming up, it was getting harder and harder to figure out", "tokens": [300, 382, 439, 613, 7150, 645, 1348, 493, 11, 309, 390, 1242, 6081, 293, 6081, 281, 2573, 484], "temperature": 0.0, "avg_logprob": -0.14282558161184328, "compression_ratio": 1.736842105263158, "no_speech_prob": 5.224078267929144e-05}, {"id": 68, "seek": 34604, "start": 369.68, "end": 375.52000000000004, "text": " what actually works best. And part of the reason for that was just because there was so", "tokens": [437, 767, 1985, 1151, 13, 400, 644, 295, 264, 1778, 337, 300, 390, 445, 570, 456, 390, 370], "temperature": 0.0, "avg_logprob": -0.14282558161184328, "compression_ratio": 1.736842105263158, "no_speech_prob": 5.224078267929144e-05}, {"id": 69, "seek": 37552, "start": 375.52, "end": 380.35999999999996, "text": " many methods that were being proposed kind of simultaneously. And when that happens, even", "tokens": [867, 7150, 300, 645, 885, 10348, 733, 295, 16561, 13, 400, 562, 300, 2314, 11, 754], "temperature": 0.0, "avg_logprob": -0.16921601756926505, "compression_ratio": 1.8169491525423729, "no_speech_prob": 0.00021984329214319587}, {"id": 70, "seek": 37552, "start": 380.35999999999996, "end": 385.96, "text": " when everyone is working in Ernest with good faith, you might have situations like this", "tokens": [562, 1518, 307, 1364, 294, 24147, 377, 365, 665, 4522, 11, 291, 1062, 362, 6851, 411, 341], "temperature": 0.0, "avg_logprob": -0.16921601756926505, "compression_ratio": 1.8169491525423729, "no_speech_prob": 0.00021984329214319587}, {"id": 71, "seek": 37552, "start": 385.96, "end": 391.0, "text": " where one paper comes along, paper A that proposes a new unsupervised pre-training", "tokens": [689, 472, 3035, 1487, 2051, 11, 3035, 316, 300, 2365, 4201, 257, 777, 2693, 12879, 24420, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.16921601756926505, "compression_ratio": 1.8169491525423729, "no_speech_prob": 0.00021984329214319587}, {"id": 72, "seek": 37552, "start": 391.0, "end": 395.91999999999996, "text": " like technique called fancy learn, another paper comes along maybe around the same time", "tokens": [411, 6532, 1219, 10247, 1466, 11, 1071, 3035, 1487, 2051, 1310, 926, 264, 912, 565], "temperature": 0.0, "avg_logprob": -0.16921601756926505, "compression_ratio": 1.8169491525423729, "no_speech_prob": 0.00021984329214319587}, {"id": 73, "seek": 37552, "start": 395.91999999999996, "end": 400.47999999999996, "text": " with a pre-training technique called fancyer learn, and paper A pre-trains on Wikipedia", "tokens": [365, 257, 659, 12, 17227, 1760, 6532, 1219, 10247, 260, 1466, 11, 293, 3035, 316, 659, 12, 17227, 1292, 322, 28999], "temperature": 0.0, "avg_logprob": -0.16921601756926505, "compression_ratio": 1.8169491525423729, "no_speech_prob": 0.00021984329214319587}, {"id": 74, "seek": 37552, "start": 400.47999999999996, "end": 405.15999999999997, "text": " for unlabeled data, while paper B uses Wikipedia and the Toronto Books Corpus was just a collection", "tokens": [337, 32118, 18657, 292, 1412, 11, 1339, 3035, 363, 4960, 28999, 293, 264, 14140, 33843, 3925, 31624, 390, 445, 257, 5765], "temperature": 0.0, "avg_logprob": -0.16921601756926505, "compression_ratio": 1.8169491525423729, "no_speech_prob": 0.00021984329214319587}, {"id": 75, "seek": 40516, "start": 405.16, "end": 411.16, "text": " of novel text. And then the question obviously is, is fancyer learn better than fancy learn?", "tokens": [295, 7613, 2487, 13, 400, 550, 264, 1168, 2745, 307, 11, 307, 10247, 260, 1466, 1101, 813, 10247, 1466, 30], "temperature": 0.0, "avg_logprob": -0.12369708106631325, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.590989581425674e-05}, {"id": 76, "seek": 40516, "start": 411.16, "end": 414.96000000000004, "text": " Well it's hard to say because they use different sources of pre-training data. You might", "tokens": [1042, 309, 311, 1152, 281, 584, 570, 436, 764, 819, 7139, 295, 659, 12, 17227, 1760, 1412, 13, 509, 1062], "temperature": 0.0, "avg_logprob": -0.12369708106631325, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.590989581425674e-05}, {"id": 77, "seek": 40516, "start": 414.96000000000004, "end": 419.56, "text": " imagine that maybe they use different model sizes, maybe they pre-trained for a different", "tokens": [3811, 300, 1310, 436, 764, 819, 2316, 11602, 11, 1310, 436, 659, 12, 17227, 2001, 337, 257, 819], "temperature": 0.0, "avg_logprob": -0.12369708106631325, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.590989581425674e-05}, {"id": 78, "seek": 40516, "start": 419.56, "end": 424.48, "text": " amount of time, they use different optimizers, there are tons of design decisions that come", "tokens": [2372, 295, 565, 11, 436, 764, 819, 5028, 22525, 11, 456, 366, 9131, 295, 1715, 5327, 300, 808], "temperature": 0.0, "avg_logprob": -0.12369708106631325, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.590989581425674e-05}, {"id": 79, "seek": 40516, "start": 424.48, "end": 431.20000000000005, "text": " into play here. And so given that these design decisions can make it hard to determine", "tokens": [666, 862, 510, 13, 400, 370, 2212, 300, 613, 1715, 5327, 393, 652, 309, 1152, 281, 6997], "temperature": 0.0, "avg_logprob": -0.12369708106631325, "compression_ratio": 1.8072289156626506, "no_speech_prob": 3.590989581425674e-05}, {"id": 80, "seek": 43120, "start": 431.2, "end": 437.0, "text": " what worked best, our goal in the T5 paper was kind of to step back and just say, given", "tokens": [437, 2732, 1151, 11, 527, 3387, 294, 264, 314, 20, 3035, 390, 733, 295, 281, 1823, 646, 293, 445, 584, 11, 2212], "temperature": 0.0, "avg_logprob": -0.13796893755594888, "compression_ratio": 1.688212927756654, "no_speech_prob": 5.919991235714406e-05}, {"id": 81, "seek": 43120, "start": 437.0, "end": 440.56, "text": " the current landscape of transfer learning, you know, all of the methods that people have", "tokens": [264, 2190, 9661, 295, 5003, 2539, 11, 291, 458, 11, 439, 295, 264, 7150, 300, 561, 362], "temperature": 0.0, "avg_logprob": -0.13796893755594888, "compression_ratio": 1.688212927756654, "no_speech_prob": 5.919991235714406e-05}, {"id": 82, "seek": 43120, "start": 440.56, "end": 446.48, "text": " proposed, what actually works best when we compare them in a in the same exact setting.", "tokens": [10348, 11, 437, 767, 1985, 1151, 562, 321, 6794, 552, 294, 257, 294, 264, 912, 1900, 3287, 13], "temperature": 0.0, "avg_logprob": -0.13796893755594888, "compression_ratio": 1.688212927756654, "no_speech_prob": 5.919991235714406e-05}, {"id": 83, "seek": 43120, "start": 446.48, "end": 451.96, "text": " And once we know what works best, how far can we push these tools that we already have?", "tokens": [400, 1564, 321, 458, 437, 1985, 1151, 11, 577, 1400, 393, 321, 2944, 613, 3873, 300, 321, 1217, 362, 30], "temperature": 0.0, "avg_logprob": -0.13796893755594888, "compression_ratio": 1.688212927756654, "no_speech_prob": 5.919991235714406e-05}, {"id": 84, "seek": 43120, "start": 451.96, "end": 457.88, "text": " And how much can we explore the limits and figure out how well these things work in scale?", "tokens": [400, 577, 709, 393, 321, 6839, 264, 10406, 293, 2573, 484, 577, 731, 613, 721, 589, 294, 4373, 30], "temperature": 0.0, "avg_logprob": -0.13796893755594888, "compression_ratio": 1.688212927756654, "no_speech_prob": 5.919991235714406e-05}, {"id": 85, "seek": 45788, "start": 457.88, "end": 463.2, "text": " And so to attack this problem, the kind of the only thing that we introduced, since again,", "tokens": [400, 370, 281, 2690, 341, 1154, 11, 264, 733, 295, 264, 787, 551, 300, 321, 7268, 11, 1670, 797, 11], "temperature": 0.0, "avg_logprob": -0.13543032503676142, "compression_ratio": 1.7718446601941749, "no_speech_prob": 3.0714239983353764e-05}, {"id": 86, "seek": 45788, "start": 463.2, "end": 470.4, "text": " we're kind of exploring existing techniques, was this idea of treating all text problems", "tokens": [321, 434, 733, 295, 12736, 6741, 7512, 11, 390, 341, 1558, 295, 15083, 439, 2487, 2740], "temperature": 0.0, "avg_logprob": -0.13543032503676142, "compression_ratio": 1.7718446601941749, "no_speech_prob": 3.0714239983353764e-05}, {"id": 87, "seek": 45788, "start": 470.4, "end": 477.8, "text": " in the same format. And this kind of approach, this dogma of treating every text problem", "tokens": [294, 264, 912, 7877, 13, 400, 341, 733, 295, 3109, 11, 341, 3000, 1696, 295, 15083, 633, 2487, 1154], "temperature": 0.0, "avg_logprob": -0.13543032503676142, "compression_ratio": 1.7718446601941749, "no_speech_prob": 3.0714239983353764e-05}, {"id": 88, "seek": 45788, "start": 477.8, "end": 482.84, "text": " in the same format gives rise to our model, which we call the text-to-text transfer transformer.", "tokens": [294, 264, 912, 7877, 2709, 6272, 281, 527, 2316, 11, 597, 321, 818, 264, 2487, 12, 1353, 12, 25111, 5003, 31782, 13], "temperature": 0.0, "avg_logprob": -0.13543032503676142, "compression_ratio": 1.7718446601941749, "no_speech_prob": 3.0714239983353764e-05}, {"id": 89, "seek": 48284, "start": 482.84, "end": 488.64, "text": " And so to explain this format to you, the basic idea is that we cast every text-based", "tokens": [400, 370, 281, 2903, 341, 7877, 281, 291, 11, 264, 3875, 1558, 307, 300, 321, 4193, 633, 2487, 12, 6032], "temperature": 0.0, "avg_logprob": -0.11973952403110741, "compression_ratio": 1.685823754789272, "no_speech_prob": 7.719610584899783e-05}, {"id": 90, "seek": 48284, "start": 488.64, "end": 494.56, "text": " NLP problem as a text-to-text task. And by that, I mean, the model takes text as input", "tokens": [426, 45196, 1154, 382, 257, 2487, 12, 1353, 12, 25111, 5633, 13, 400, 538, 300, 11, 286, 914, 11, 264, 2316, 2516, 2487, 382, 4846], "temperature": 0.0, "avg_logprob": -0.11973952403110741, "compression_ratio": 1.685823754789272, "no_speech_prob": 7.719610584899783e-05}, {"id": 91, "seek": 48284, "start": 494.56, "end": 499.47999999999996, "text": " and it produces text as output. So in things like English to German translation, this is", "tokens": [293, 309, 14725, 2487, 382, 5598, 13, 407, 294, 721, 411, 3669, 281, 6521, 12853, 11, 341, 307], "temperature": 0.0, "avg_logprob": -0.11973952403110741, "compression_ratio": 1.685823754789272, "no_speech_prob": 7.719610584899783e-05}, {"id": 92, "seek": 48284, "start": 499.47999999999996, "end": 505.03999999999996, "text": " pretty typical. We feed in a English sentence on the input and we train the model to predict", "tokens": [1238, 7476, 13, 492, 3154, 294, 257, 3669, 8174, 322, 264, 4846, 293, 321, 3847, 264, 2316, 281, 6069], "temperature": 0.0, "avg_logprob": -0.11973952403110741, "compression_ratio": 1.685823754789272, "no_speech_prob": 7.719610584899783e-05}, {"id": 93, "seek": 48284, "start": 505.03999999999996, "end": 509.96, "text": " a German sentence on the output. And you'll notice in our case, we actually also feed", "tokens": [257, 6521, 8174, 322, 264, 5598, 13, 400, 291, 603, 3449, 294, 527, 1389, 11, 321, 767, 611, 3154], "temperature": 0.0, "avg_logprob": -0.11973952403110741, "compression_ratio": 1.685823754789272, "no_speech_prob": 7.719610584899783e-05}, {"id": 94, "seek": 50996, "start": 509.96, "end": 514.28, "text": " what we call a task prefix, translate English to German. That just tells the model what", "tokens": [437, 321, 818, 257, 5633, 46969, 11, 13799, 3669, 281, 6521, 13, 663, 445, 5112, 264, 2316, 437], "temperature": 0.0, "avg_logprob": -0.13873330870671058, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.025903323665261e-05}, {"id": 95, "seek": 50996, "start": 514.28, "end": 518.6, "text": " we wanted to do with the input. Because if, especially if we're training a multi-task model,", "tokens": [321, 1415, 281, 360, 365, 264, 4846, 13, 1436, 498, 11, 2318, 498, 321, 434, 3097, 257, 4825, 12, 83, 3863, 2316, 11], "temperature": 0.0, "avg_logprob": -0.13873330870671058, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.025903323665261e-05}, {"id": 96, "seek": 50996, "start": 518.6, "end": 521.8, "text": " if you just feed the model, that is good. The model doesn't know what to do with it.", "tokens": [498, 291, 445, 3154, 264, 2316, 11, 300, 307, 665, 13, 440, 2316, 1177, 380, 458, 437, 281, 360, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.13873330870671058, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.025903323665261e-05}, {"id": 97, "seek": 50996, "start": 521.8, "end": 525.64, "text": " It doesn't know if you're trying to do sentiment analysis or English to German translation", "tokens": [467, 1177, 380, 458, 498, 291, 434, 1382, 281, 360, 16149, 5215, 420, 3669, 281, 6521, 12853], "temperature": 0.0, "avg_logprob": -0.13873330870671058, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.025903323665261e-05}, {"id": 98, "seek": 50996, "start": 525.64, "end": 532.52, "text": " or what. So this shouldn't be that surprising so far. You probably learned about encoder", "tokens": [420, 437, 13, 407, 341, 4659, 380, 312, 300, 8830, 370, 1400, 13, 509, 1391, 3264, 466, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.13873330870671058, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.025903323665261e-05}, {"id": 99, "seek": 50996, "start": 532.52, "end": 535.68, "text": " decoder model, sequence-to-sequence models. And that's basically all that we're doing", "tokens": [979, 19866, 2316, 11, 8310, 12, 1353, 12, 11834, 655, 5245, 13, 400, 300, 311, 1936, 439, 300, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.13873330870671058, "compression_ratio": 1.7939189189189189, "no_speech_prob": 7.025903323665261e-05}, {"id": 100, "seek": 53568, "start": 535.68, "end": 540.9599999999999, "text": " here. It maybe gets a little more unusual when we start to tackle things like text classification", "tokens": [510, 13, 467, 1310, 2170, 257, 707, 544, 10901, 562, 321, 722, 281, 14896, 721, 411, 2487, 21538], "temperature": 0.0, "avg_logprob": -0.13566634724441085, "compression_ratio": 1.6879699248120301, "no_speech_prob": 4.399156750878319e-05}, {"id": 101, "seek": 53568, "start": 540.9599999999999, "end": 546.3599999999999, "text": " tasks. So this is an example from the cola benchmark, which is the corpus of linguistic", "tokens": [9608, 13, 407, 341, 307, 364, 1365, 490, 264, 40495, 18927, 11, 597, 307, 264, 1181, 31624, 295, 43002], "temperature": 0.0, "avg_logprob": -0.13566634724441085, "compression_ratio": 1.6879699248120301, "no_speech_prob": 4.399156750878319e-05}, {"id": 102, "seek": 53568, "start": 546.3599999999999, "end": 551.56, "text": " acceptability. And the goal here is to take a sentence and determine if the sentence is", "tokens": [3241, 2310, 13, 400, 264, 3387, 510, 307, 281, 747, 257, 8174, 293, 6997, 498, 264, 8174, 307], "temperature": 0.0, "avg_logprob": -0.13566634724441085, "compression_ratio": 1.6879699248120301, "no_speech_prob": 4.399156750878319e-05}, {"id": 103, "seek": 53568, "start": 551.56, "end": 556.12, "text": " quote unquote acceptable, which kind of means whether it's grammatically correct and also", "tokens": [6513, 37557, 15513, 11, 597, 733, 295, 1355, 1968, 309, 311, 17570, 5030, 3006, 293, 611], "temperature": 0.0, "avg_logprob": -0.13566634724441085, "compression_ratio": 1.6879699248120301, "no_speech_prob": 4.399156750878319e-05}, {"id": 104, "seek": 53568, "start": 556.12, "end": 561.68, "text": " if it's nonsense or not. And in this case, the sentence is the course is jumping well", "tokens": [498, 309, 311, 14925, 420, 406, 13, 400, 294, 341, 1389, 11, 264, 8174, 307, 264, 1164, 307, 11233, 731], "temperature": 0.0, "avg_logprob": -0.13566634724441085, "compression_ratio": 1.6879699248120301, "no_speech_prob": 4.399156750878319e-05}, {"id": 105, "seek": 56168, "start": 561.68, "end": 566.1999999999999, "text": " and of course, courses can't jump. So this sentence is not acceptable. But rather than", "tokens": [293, 295, 1164, 11, 7712, 393, 380, 3012, 13, 407, 341, 8174, 307, 406, 15513, 13, 583, 2831, 813], "temperature": 0.0, "avg_logprob": -0.13878544982598753, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00011233876284677535}, {"id": 106, "seek": 56168, "start": 566.1999999999999, "end": 571.4799999999999, "text": " training our model through a classifier layer that outputs a class label or a probability", "tokens": [3097, 527, 2316, 807, 257, 1508, 9902, 4583, 300, 23930, 257, 1508, 7645, 420, 257, 8482], "temperature": 0.0, "avg_logprob": -0.13878544982598753, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00011233876284677535}, {"id": 107, "seek": 56168, "start": 571.4799999999999, "end": 576.76, "text": " distribution over class indices, we actually train the model to output the literal text", "tokens": [7316, 670, 1508, 43840, 11, 321, 767, 3847, 264, 2316, 281, 5598, 264, 20411, 2487], "temperature": 0.0, "avg_logprob": -0.13878544982598753, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00011233876284677535}, {"id": 108, "seek": 56168, "start": 576.76, "end": 583.8, "text": " not acceptable. So it's outputting this text string token by token. And it can get even", "tokens": [406, 15513, 13, 407, 309, 311, 5598, 783, 341, 2487, 6798, 14862, 538, 14862, 13, 400, 309, 393, 483, 754], "temperature": 0.0, "avg_logprob": -0.13878544982598753, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00011233876284677535}, {"id": 109, "seek": 56168, "start": 583.8, "end": 588.4799999999999, "text": " a little weirder. We can attack things like regression problems where effectively the", "tokens": [257, 707, 321, 347, 1068, 13, 492, 393, 2690, 721, 411, 24590, 2740, 689, 8659, 264], "temperature": 0.0, "avg_logprob": -0.13878544982598753, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00011233876284677535}, {"id": 110, "seek": 58848, "start": 588.48, "end": 593.0, "text": " model is supposed to be outputting a floating point value. And we actually just do this", "tokens": [2316, 307, 3442, 281, 312, 5598, 783, 257, 12607, 935, 2158, 13, 400, 321, 767, 445, 360, 341], "temperature": 0.0, "avg_logprob": -0.1157186208677686, "compression_ratio": 1.8231292517006803, "no_speech_prob": 4.0061233448795974e-05}, {"id": 111, "seek": 58848, "start": 593.0, "end": 597.72, "text": " by taking the floating point numbers and converting them to strings and training the model to", "tokens": [538, 1940, 264, 12607, 935, 3547, 293, 29942, 552, 281, 13985, 293, 3097, 264, 2316, 281], "temperature": 0.0, "avg_logprob": -0.1157186208677686, "compression_ratio": 1.8231292517006803, "no_speech_prob": 4.0061233448795974e-05}, {"id": 112, "seek": 58848, "start": 597.72, "end": 602.08, "text": " just predict the string. And it turns out that at least for this particular task, which", "tokens": [445, 6069, 264, 6798, 13, 400, 309, 4523, 484, 300, 412, 1935, 337, 341, 1729, 5633, 11, 597], "temperature": 0.0, "avg_logprob": -0.1157186208677686, "compression_ratio": 1.8231292517006803, "no_speech_prob": 4.0061233448795974e-05}, {"id": 113, "seek": 58848, "start": 602.08, "end": 607.4, "text": " is the STSB benchmark, it works perfectly fine. And we ultimately actually got state", "tokens": [307, 264, 4904, 50, 33, 18927, 11, 309, 1985, 6239, 2489, 13, 400, 321, 6284, 767, 658, 1785], "temperature": 0.0, "avg_logprob": -0.1157186208677686, "compression_ratio": 1.8231292517006803, "no_speech_prob": 4.0061233448795974e-05}, {"id": 114, "seek": 58848, "start": 607.4, "end": 612.6, "text": " of the art on this benchmark. So doing this type of sort of float to string conversion", "tokens": [295, 264, 1523, 322, 341, 18927, 13, 407, 884, 341, 2010, 295, 1333, 295, 15706, 281, 6798, 14298], "temperature": 0.0, "avg_logprob": -0.1157186208677686, "compression_ratio": 1.8231292517006803, "no_speech_prob": 4.0061233448795974e-05}, {"id": 115, "seek": 58848, "start": 612.6, "end": 618.04, "text": " with a little bit of quantization turns out to work fine for regression problems. And finally,", "tokens": [365, 257, 707, 857, 295, 4426, 2144, 4523, 484, 281, 589, 2489, 337, 24590, 2740, 13, 400, 2721, 11], "temperature": 0.0, "avg_logprob": -0.1157186208677686, "compression_ratio": 1.8231292517006803, "no_speech_prob": 4.0061233448795974e-05}, {"id": 116, "seek": 61804, "start": 618.04, "end": 621.68, "text": " you know, the main point of this is that there are a lot, there are really tons and tons", "tokens": [291, 458, 11, 264, 2135, 935, 295, 341, 307, 300, 456, 366, 257, 688, 11, 456, 366, 534, 9131, 293, 9131], "temperature": 0.0, "avg_logprob": -0.12113587431205336, "compression_ratio": 1.8653198653198653, "no_speech_prob": 0.00013131956802681088}, {"id": 117, "seek": 61804, "start": 621.68, "end": 626.3199999999999, "text": " of problems that we can cast into this format. So here's an example of abstractive summarization.", "tokens": [295, 2740, 300, 321, 393, 4193, 666, 341, 7877, 13, 407, 510, 311, 364, 1365, 295, 12649, 488, 14611, 2144, 13], "temperature": 0.0, "avg_logprob": -0.12113587431205336, "compression_ratio": 1.8653198653198653, "no_speech_prob": 0.00013131956802681088}, {"id": 118, "seek": 61804, "start": 626.3199999999999, "end": 630.76, "text": " We feed in a news article on the left and we predict that at this summary on the right.", "tokens": [492, 3154, 294, 257, 2583, 7222, 322, 264, 1411, 293, 321, 6069, 300, 412, 341, 12691, 322, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.12113587431205336, "compression_ratio": 1.8653198653198653, "no_speech_prob": 0.00013131956802681088}, {"id": 119, "seek": 61804, "start": 630.76, "end": 635.28, "text": " And again, we can really attack all of these problems in exactly the same way. So we're", "tokens": [400, 797, 11, 321, 393, 534, 2690, 439, 295, 613, 2740, 294, 2293, 264, 912, 636, 13, 407, 321, 434], "temperature": 0.0, "avg_logprob": -0.12113587431205336, "compression_ratio": 1.8653198653198653, "no_speech_prob": 0.00013131956802681088}, {"id": 120, "seek": 61804, "start": 635.28, "end": 641.92, "text": " using exactly the same objective during training and exactly the same decoding procedure at test", "tokens": [1228, 2293, 264, 912, 10024, 1830, 3097, 293, 2293, 264, 912, 979, 8616, 10747, 412, 1500], "temperature": 0.0, "avg_logprob": -0.12113587431205336, "compression_ratio": 1.8653198653198653, "no_speech_prob": 0.00013131956802681088}, {"id": 121, "seek": 61804, "start": 641.92, "end": 647.52, "text": " time to attack a huge variety of natural language processing problems. And the nice part about", "tokens": [565, 281, 2690, 257, 2603, 5673, 295, 3303, 2856, 9007, 2740, 13, 400, 264, 1481, 644, 466], "temperature": 0.0, "avg_logprob": -0.12113587431205336, "compression_ratio": 1.8653198653198653, "no_speech_prob": 0.00013131956802681088}, {"id": 122, "seek": 64752, "start": 647.52, "end": 653.6, "text": " doing this is that as long as a transfer learning improvement is applicable to our model and to", "tokens": [884, 341, 307, 300, 382, 938, 382, 257, 5003, 2539, 10444, 307, 21142, 281, 527, 2316, 293, 281], "temperature": 0.0, "avg_logprob": -0.16854183799342104, "compression_ratio": 1.7008547008547008, "no_speech_prob": 5.3066647524246946e-05}, {"id": 123, "seek": 64752, "start": 653.6, "end": 660.6, "text": " this text to text format, we can try it on a huge suite of downstream tasks while using exactly", "tokens": [341, 2487, 281, 2487, 7877, 11, 321, 393, 853, 309, 322, 257, 2603, 14205, 295, 30621, 9608, 1339, 1228, 2293], "temperature": 0.0, "avg_logprob": -0.16854183799342104, "compression_ratio": 1.7008547008547008, "no_speech_prob": 5.3066647524246946e-05}, {"id": 124, "seek": 64752, "start": 660.6, "end": 665.84, "text": " the same model, exactly the same learning rate optimizer training procedure, exactly the same", "tokens": [264, 912, 2316, 11, 2293, 264, 912, 2539, 3314, 5028, 6545, 3097, 10747, 11, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.16854183799342104, "compression_ratio": 1.7008547008547008, "no_speech_prob": 5.3066647524246946e-05}, {"id": 125, "seek": 64752, "start": 665.84, "end": 670.0799999999999, "text": " inference procedure. So we can get a rid of a ton of the confounders that I mentioned earlier.", "tokens": [38253, 10747, 13, 407, 321, 393, 483, 257, 3973, 295, 257, 2952, 295, 264, 1497, 554, 433, 300, 286, 2835, 3071, 13], "temperature": 0.0, "avg_logprob": -0.16854183799342104, "compression_ratio": 1.7008547008547008, "no_speech_prob": 5.3066647524246946e-05}, {"id": 126, "seek": 64752, "start": 671.28, "end": 672.72, "text": " Hey, Colin. Yeah.", "tokens": [1911, 11, 29253, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.16854183799342104, "compression_ratio": 1.7008547008547008, "no_speech_prob": 5.3066647524246946e-05}, {"id": 127, "seek": 67272, "start": 672.72, "end": 680.36, "text": " Can you take a second as we're discussing the formatting here to talk about how the 3.8 sort of", "tokens": [1664, 291, 747, 257, 1150, 382, 321, 434, 10850, 264, 39366, 510, 281, 751, 466, 577, 264, 805, 13, 23, 1333, 295], "temperature": 0.0, "avg_logprob": -0.1800828809323518, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.00022336856636684388}, {"id": 128, "seek": 67272, "start": 680.36, "end": 686.64, "text": " regression example works and yeah, just go over that one more time. Yeah, absolutely. So in this", "tokens": [24590, 1365, 1985, 293, 1338, 11, 445, 352, 670, 300, 472, 544, 565, 13, 865, 11, 3122, 13, 407, 294, 341], "temperature": 0.0, "avg_logprob": -0.1800828809323518, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.00022336856636684388}, {"id": 129, "seek": 67272, "start": 686.64, "end": 692.32, "text": " particular task, STSB, the goal is to take in two sentences and predict a floating point number,", "tokens": [1729, 5633, 11, 4904, 50, 33, 11, 264, 3387, 307, 281, 747, 294, 732, 16579, 293, 6069, 257, 12607, 935, 1230, 11], "temperature": 0.0, "avg_logprob": -0.1800828809323518, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.00022336856636684388}, {"id": 130, "seek": 67272, "start": 692.32, "end": 697.2, "text": " which denotes how similar those two sentences are. And that floating point number ranges between", "tokens": [597, 1441, 17251, 577, 2531, 729, 732, 16579, 366, 13, 400, 300, 12607, 935, 1230, 22526, 1296], "temperature": 0.0, "avg_logprob": -0.1800828809323518, "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.00022336856636684388}, {"id": 131, "seek": 69720, "start": 697.2, "end": 704.44, "text": " 1.0 and 5.0. So we basically took the ground truth, annotated values that were supposed to", "tokens": [502, 13, 15, 293, 1025, 13, 15, 13, 407, 321, 1936, 1890, 264, 2727, 3494, 11, 25339, 770, 4190, 300, 645, 3442, 281], "temperature": 0.0, "avg_logprob": -0.16040809337909406, "compression_ratio": 1.6050420168067228, "no_speech_prob": 5.224710184847936e-05}, {"id": 132, "seek": 69720, "start": 704.44, "end": 711.0400000000001, "text": " regress. We quantized them to the nearest 0.2 and we cast it to a string. And now we have a string", "tokens": [1121, 735, 13, 492, 4426, 1602, 552, 281, 264, 23831, 1958, 13, 17, 293, 321, 4193, 309, 281, 257, 6798, 13, 400, 586, 321, 362, 257, 6798], "temperature": 0.0, "avg_logprob": -0.16040809337909406, "compression_ratio": 1.6050420168067228, "no_speech_prob": 5.224710184847936e-05}, {"id": 133, "seek": 69720, "start": 711.0400000000001, "end": 716.5600000000001, "text": " like three three period eight, a 3.8, but you could think of it like three period eight or four", "tokens": [411, 1045, 1045, 2896, 3180, 11, 257, 805, 13, 23, 11, 457, 291, 727, 519, 295, 309, 411, 1045, 2896, 3180, 420, 1451], "temperature": 0.0, "avg_logprob": -0.16040809337909406, "compression_ratio": 1.6050420168067228, "no_speech_prob": 5.224710184847936e-05}, {"id": 134, "seek": 69720, "start": 716.5600000000001, "end": 722.5600000000001, "text": " period zero. And we just train the model to predict that string, which ultimately it's a string.", "tokens": [2896, 4018, 13, 400, 321, 445, 3847, 264, 2316, 281, 6069, 300, 6798, 11, 597, 6284, 309, 311, 257, 6798, 13], "temperature": 0.0, "avg_logprob": -0.16040809337909406, "compression_ratio": 1.6050420168067228, "no_speech_prob": 5.224710184847936e-05}, {"id": 135, "seek": 72256, "start": 722.56, "end": 728.8, "text": " It's not a number anymore. We just predict it token by token. So in a sense, you can kind of think", "tokens": [467, 311, 406, 257, 1230, 3602, 13, 492, 445, 6069, 309, 14862, 538, 14862, 13, 407, 294, 257, 2020, 11, 291, 393, 733, 295, 519], "temperature": 0.0, "avg_logprob": -0.09959673477431476, "compression_ratio": 1.835820895522388, "no_speech_prob": 1.6700336345820688e-05}, {"id": 136, "seek": 72256, "start": 728.8, "end": 733.04, "text": " of it as converting the regression problem to a classification problem because you're doing this", "tokens": [295, 309, 382, 29942, 264, 24590, 1154, 281, 257, 21538, 1154, 570, 291, 434, 884, 341], "temperature": 0.0, "avg_logprob": -0.09959673477431476, "compression_ratio": 1.835820895522388, "no_speech_prob": 1.6700336345820688e-05}, {"id": 137, "seek": 72256, "start": 733.04, "end": 738.0799999999999, "text": " quantization, but you also more broadly can just think of it as converting the regression problem", "tokens": [4426, 2144, 11, 457, 291, 611, 544, 19511, 393, 445, 519, 295, 309, 382, 29942, 264, 24590, 1154], "temperature": 0.0, "avg_logprob": -0.09959673477431476, "compression_ratio": 1.835820895522388, "no_speech_prob": 1.6700336345820688e-05}, {"id": 138, "seek": 72256, "start": 738.0799999999999, "end": 745.52, "text": " to a text to text problem, which is what we're doing. Thanks. Yeah. It's a little funky, but I promise", "tokens": [281, 257, 2487, 281, 2487, 1154, 11, 597, 307, 437, 321, 434, 884, 13, 2561, 13, 865, 13, 467, 311, 257, 707, 33499, 11, 457, 286, 6228], "temperature": 0.0, "avg_logprob": -0.09959673477431476, "compression_ratio": 1.835820895522388, "no_speech_prob": 1.6700336345820688e-05}, {"id": 139, "seek": 72256, "start": 745.52, "end": 752.4799999999999, "text": " it works. And great. So the nice thing again about using this sort of sequence to sequence text", "tokens": [309, 1985, 13, 400, 869, 13, 407, 264, 1481, 551, 797, 466, 1228, 341, 1333, 295, 8310, 281, 8310, 2487], "temperature": 0.0, "avg_logprob": -0.09959673477431476, "compression_ratio": 1.835820895522388, "no_speech_prob": 1.6700336345820688e-05}, {"id": 140, "seek": 75248, "start": 752.48, "end": 758.16, "text": " to text format is that we can actually just use the original vanilla transformer as it was proposed", "tokens": [281, 2487, 7877, 307, 300, 321, 393, 767, 445, 764, 264, 3380, 17528, 31782, 382, 309, 390, 10348], "temperature": 0.0, "avg_logprob": -0.11379560140463021, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.00016341585433110595}, {"id": 141, "seek": 75248, "start": 758.16, "end": 763.12, "text": " because if you remember, the transformer was actually proposed for English to, well, it was", "tokens": [570, 498, 291, 1604, 11, 264, 31782, 390, 767, 10348, 337, 3669, 281, 11, 731, 11, 309, 390], "temperature": 0.0, "avg_logprob": -0.11379560140463021, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.00016341585433110595}, {"id": 142, "seek": 75248, "start": 763.12, "end": 768.4, "text": " proposed for machine translation primarily, which is a sequence task, you know, taking a language", "tokens": [10348, 337, 3479, 12853, 10029, 11, 597, 307, 257, 8310, 5633, 11, 291, 458, 11, 1940, 257, 2856], "temperature": 0.0, "avg_logprob": -0.11379560140463021, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.00016341585433110595}, {"id": 143, "seek": 75248, "start": 768.4, "end": 774.08, "text": " in sentence and one language as input and producing the corresponding sentence in other language.", "tokens": [294, 8174, 293, 472, 2856, 382, 4846, 293, 10501, 264, 11760, 8174, 294, 661, 2856, 13], "temperature": 0.0, "avg_logprob": -0.11379560140463021, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.00016341585433110595}, {"id": 144, "seek": 75248, "start": 774.08, "end": 778.16, "text": " And so really, I won't say a lot about the model that we use. There are relatively few changes", "tokens": [400, 370, 534, 11, 286, 1582, 380, 584, 257, 688, 466, 264, 2316, 300, 321, 764, 13, 821, 366, 7226, 1326, 2962], "temperature": 0.0, "avg_logprob": -0.11379560140463021, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.00016341585433110595}, {"id": 145, "seek": 77816, "start": 778.16, "end": 782.9599999999999, "text": " that we made to the standard transformer architecture as originally proposed and attention is", "tokens": [300, 321, 1027, 281, 264, 3832, 31782, 9482, 382, 7993, 10348, 293, 3202, 307], "temperature": 0.0, "avg_logprob": -0.09953864027814167, "compression_ratio": 1.6077586206896552, "no_speech_prob": 9.026596671901643e-05}, {"id": 146, "seek": 77816, "start": 782.9599999999999, "end": 790.3199999999999, "text": " all you need when constructing our T5 model. I will towards the end of the talk discuss lots of", "tokens": [439, 291, 643, 562, 39969, 527, 314, 20, 2316, 13, 286, 486, 3030, 264, 917, 295, 264, 751, 2248, 3195, 295], "temperature": 0.0, "avg_logprob": -0.09953864027814167, "compression_ratio": 1.6077586206896552, "no_speech_prob": 9.026596671901643e-05}, {"id": 147, "seek": 77816, "start": 790.3199999999999, "end": 795.68, "text": " architectural modifications that people have made sense, but for T5 or the T5 paper, we really", "tokens": [26621, 26881, 300, 561, 362, 1027, 2020, 11, 457, 337, 314, 20, 420, 264, 314, 20, 3035, 11, 321, 534], "temperature": 0.0, "avg_logprob": -0.09953864027814167, "compression_ratio": 1.6077586206896552, "no_speech_prob": 9.026596671901643e-05}, {"id": 148, "seek": 77816, "start": 795.68, "end": 802.24, "text": " basically stuck to the basics. So the next big question when you're attacking a transfer", "tokens": [1936, 5541, 281, 264, 14688, 13, 407, 264, 958, 955, 1168, 562, 291, 434, 15010, 257, 5003], "temperature": 0.0, "avg_logprob": -0.09953864027814167, "compression_ratio": 1.6077586206896552, "no_speech_prob": 9.026596671901643e-05}, {"id": 149, "seek": 80224, "start": 802.24, "end": 808.88, "text": " learning problem is what should my pre-training data set be? And because of the internet, there", "tokens": [2539, 1154, 307, 437, 820, 452, 659, 12, 17227, 1760, 1412, 992, 312, 30, 400, 570, 295, 264, 4705, 11, 456], "temperature": 0.0, "avg_logprob": -0.086542454171688, "compression_ratio": 1.6738197424892705, "no_speech_prob": 4.907778566121124e-05}, {"id": 150, "seek": 80224, "start": 808.88, "end": 814.24, "text": " are lots of possible sources of unlabeled text data, one common source is Wikipedia. I'm displaying", "tokens": [366, 3195, 295, 1944, 7139, 295, 32118, 18657, 292, 2487, 1412, 11, 472, 2689, 4009, 307, 28999, 13, 286, 478, 36834], "temperature": 0.0, "avg_logprob": -0.086542454171688, "compression_ratio": 1.6738197424892705, "no_speech_prob": 4.907778566121124e-05}, {"id": 151, "seek": 80224, "start": 814.24, "end": 821.52, "text": " a ton of Wikipedia articles on the screen here. And in undertaking this project, one of the factors", "tokens": [257, 2952, 295, 28999, 11290, 322, 264, 2568, 510, 13, 400, 294, 39250, 341, 1716, 11, 472, 295, 264, 6771], "temperature": 0.0, "avg_logprob": -0.086542454171688, "compression_ratio": 1.6738197424892705, "no_speech_prob": 4.907778566121124e-05}, {"id": 152, "seek": 80224, "start": 821.52, "end": 826.96, "text": " that we wanted to study was the effect of the pre-training data set itself. And so we actually", "tokens": [300, 321, 1415, 281, 2979, 390, 264, 1802, 295, 264, 659, 12, 17227, 1760, 1412, 992, 2564, 13, 400, 370, 321, 767], "temperature": 0.0, "avg_logprob": -0.086542454171688, "compression_ratio": 1.6738197424892705, "no_speech_prob": 4.907778566121124e-05}, {"id": 153, "seek": 82696, "start": 826.96, "end": 832.32, "text": " constructed a new pre-training data set that would allow us to vary the size across many orders of", "tokens": [17083, 257, 777, 659, 12, 17227, 1760, 1412, 992, 300, 576, 2089, 505, 281, 10559, 264, 2744, 2108, 867, 9470, 295], "temperature": 0.0, "avg_logprob": -0.07152329829701207, "compression_ratio": 1.7179487179487178, "no_speech_prob": 7.600502431159839e-05}, {"id": 154, "seek": 82696, "start": 832.32, "end": 837.52, "text": " magnitude, also that has a filtering pipeline that would allow us to control the quality and type", "tokens": [15668, 11, 611, 300, 575, 257, 30822, 15517, 300, 576, 2089, 505, 281, 1969, 264, 3125, 293, 2010], "temperature": 0.0, "avg_logprob": -0.07152329829701207, "compression_ratio": 1.7179487179487178, "no_speech_prob": 7.600502431159839e-05}, {"id": 155, "seek": 82696, "start": 837.52, "end": 842.24, "text": " of data that was pre-trained on. And I'll describe how we built that data set now.", "tokens": [295, 1412, 300, 390, 659, 12, 17227, 2001, 322, 13, 400, 286, 603, 6786, 577, 321, 3094, 300, 1412, 992, 586, 13], "temperature": 0.0, "avg_logprob": -0.07152329829701207, "compression_ratio": 1.7179487179487178, "no_speech_prob": 7.600502431159839e-05}, {"id": 156, "seek": 82696, "start": 843.6800000000001, "end": 849.76, "text": " So the first thing we did is we wanted to source our data from a publicly available source. We", "tokens": [407, 264, 700, 551, 321, 630, 307, 321, 1415, 281, 4009, 527, 1412, 490, 257, 14843, 2435, 4009, 13, 492], "temperature": 0.0, "avg_logprob": -0.07152329829701207, "compression_ratio": 1.7179487179487178, "no_speech_prob": 7.600502431159839e-05}, {"id": 157, "seek": 82696, "start": 849.76, "end": 854.5600000000001, "text": " didn't want to use some Google internal web scrape that we couldn't release. So we made use of", "tokens": [994, 380, 528, 281, 764, 512, 3329, 6920, 3670, 32827, 300, 321, 2809, 380, 4374, 13, 407, 321, 1027, 764, 295], "temperature": 0.0, "avg_logprob": -0.07152329829701207, "compression_ratio": 1.7179487179487178, "no_speech_prob": 7.600502431159839e-05}, {"id": 158, "seek": 85456, "start": 854.56, "end": 859.52, "text": " these web scrapes by a nonprofit organization called Common Crawl, which is really just", "tokens": [613, 3670, 23138, 279, 538, 257, 23348, 4475, 1219, 18235, 37877, 75, 11, 597, 307, 534, 445], "temperature": 0.0, "avg_logprob": -0.11871979035526874, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.00010389030649093911}, {"id": 159, "seek": 85456, "start": 860.2399999999999, "end": 865.8399999999999, "text": " organization that sends web crawlers out through the internet and downloads as much text as they", "tokens": [4475, 300, 14790, 3670, 13999, 11977, 484, 807, 264, 4705, 293, 36553, 382, 709, 2487, 382, 436], "temperature": 0.0, "avg_logprob": -0.11871979035526874, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.00010389030649093911}, {"id": 160, "seek": 85456, "start": 865.8399999999999, "end": 871.4399999999999, "text": " can. And every month they dump out what they call web extracted text, which is you can think of", "tokens": [393, 13, 400, 633, 1618, 436, 11430, 484, 437, 436, 818, 3670, 34086, 2487, 11, 597, 307, 291, 393, 519, 295], "temperature": 0.0, "avg_logprob": -0.11871979035526874, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.00010389030649093911}, {"id": 161, "seek": 85456, "start": 871.4399999999999, "end": 879.28, "text": " them as websites with all of the HTML and JavaScript ideally removed. And this produces text that", "tokens": [552, 382, 12891, 365, 439, 295, 264, 17995, 293, 15778, 22915, 7261, 13, 400, 341, 14725, 2487, 300], "temperature": 0.0, "avg_logprob": -0.11871979035526874, "compression_ratio": 1.6223175965665235, "no_speech_prob": 0.00010389030649093911}, {"id": 162, "seek": 87928, "start": 879.28, "end": 886.16, "text": " has a pretty decent amount of natural language in it. Also a lot of boilerplate and menu text", "tokens": [575, 257, 1238, 8681, 2372, 295, 3303, 2856, 294, 309, 13, 2743, 257, 688, 295, 39228, 37008, 293, 6510, 2487], "temperature": 0.0, "avg_logprob": -0.0853721539179484, "compression_ratio": 1.6539792387543253, "no_speech_prob": 8.885662100510672e-05}, {"id": 163, "seek": 87928, "start": 886.16, "end": 891.1999999999999, "text": " and also a little bit of gibberish. But as a whole, it's kind of a good starting point for", "tokens": [293, 611, 257, 707, 857, 295, 4553, 43189, 13, 583, 382, 257, 1379, 11, 309, 311, 733, 295, 257, 665, 2891, 935, 337], "temperature": 0.0, "avg_logprob": -0.0853721539179484, "compression_ratio": 1.6539792387543253, "no_speech_prob": 8.885662100510672e-05}, {"id": 164, "seek": 87928, "start": 891.1999999999999, "end": 897.12, "text": " constructing these pre-training data sets. And then we took a few steps to kind of try to make it", "tokens": [39969, 613, 659, 12, 17227, 1760, 1412, 6352, 13, 400, 550, 321, 1890, 257, 1326, 4439, 281, 733, 295, 853, 281, 652, 309], "temperature": 0.0, "avg_logprob": -0.0853721539179484, "compression_ratio": 1.6539792387543253, "no_speech_prob": 8.885662100510672e-05}, {"id": 165, "seek": 87928, "start": 897.12, "end": 902.16, "text": " so this data set was a little cleaner. So the first thing we did is we removed any lines that", "tokens": [370, 341, 1412, 992, 390, 257, 707, 16532, 13, 407, 264, 700, 551, 321, 630, 307, 321, 7261, 604, 3876, 300], "temperature": 0.0, "avg_logprob": -0.0853721539179484, "compression_ratio": 1.6539792387543253, "no_speech_prob": 8.885662100510672e-05}, {"id": 166, "seek": 90216, "start": 902.16, "end": 908.56, "text": " didn't end in a terminal punctuation mark. We used a language classifier to only retain English text.", "tokens": [994, 380, 917, 294, 257, 14709, 27006, 16073, 1491, 13, 492, 1143, 257, 2856, 1508, 9902, 281, 787, 18340, 3669, 2487, 13], "temperature": 0.0, "avg_logprob": -0.11923926699478014, "compression_ratio": 1.7581227436823104, "no_speech_prob": 3.1686427973909304e-05}, {"id": 167, "seek": 90216, "start": 909.36, "end": 914.64, "text": " We removed anything that looked like placeholder text, like Laura Mipson text on the right.", "tokens": [492, 7261, 1340, 300, 2956, 411, 1081, 20480, 2487, 11, 411, 13220, 376, 2600, 266, 2487, 322, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.11923926699478014, "compression_ratio": 1.7581227436823104, "no_speech_prob": 3.1686427973909304e-05}, {"id": 168, "seek": 90216, "start": 914.64, "end": 920.7199999999999, "text": " We removed anything that looked like code. We duplicated things on a sentence level. So any time that", "tokens": [492, 7261, 1340, 300, 2956, 411, 3089, 13, 492, 1581, 564, 3587, 721, 322, 257, 8174, 1496, 13, 407, 604, 565, 300], "temperature": 0.0, "avg_logprob": -0.11923926699478014, "compression_ratio": 1.7581227436823104, "no_speech_prob": 3.1686427973909304e-05}, {"id": 169, "seek": 90216, "start": 920.7199999999999, "end": 926.0799999999999, "text": " any chunk of text appeared on multiple pages, we only retained it on one of the pages and so on.", "tokens": [604, 16635, 295, 2487, 8516, 322, 3866, 7183, 11, 321, 787, 33438, 309, 322, 472, 295, 264, 7183, 293, 370, 322, 13], "temperature": 0.0, "avg_logprob": -0.11923926699478014, "compression_ratio": 1.7581227436823104, "no_speech_prob": 3.1686427973909304e-05}, {"id": 170, "seek": 92608, "start": 926.08, "end": 932.4000000000001, "text": " And ultimately these heuristics were relatively simple but produced reasonably clean text. And I", "tokens": [400, 6284, 613, 415, 374, 6006, 645, 7226, 2199, 457, 7126, 23551, 2541, 2487, 13, 400, 286], "temperature": 0.0, "avg_logprob": -0.1392419731223976, "compression_ratio": 1.5870445344129556, "no_speech_prob": 4.1980754758697e-05}, {"id": 171, "seek": 92608, "start": 932.4000000000001, "end": 937.5200000000001, "text": " will discuss later the effect of these choices in cleaning. That was one of the experiments that we ran.", "tokens": [486, 2248, 1780, 264, 1802, 295, 613, 7994, 294, 8924, 13, 663, 390, 472, 295, 264, 12050, 300, 321, 5872, 13], "temperature": 0.0, "avg_logprob": -0.1392419731223976, "compression_ratio": 1.5870445344129556, "no_speech_prob": 4.1980754758697e-05}, {"id": 172, "seek": 92608, "start": 939.2800000000001, "end": 944.4000000000001, "text": " And so after doing this, we created this data set called C4, which is the colossal clean crawl", "tokens": [400, 370, 934, 884, 341, 11, 321, 2942, 341, 1412, 992, 1219, 383, 19, 11, 597, 307, 264, 48683, 304, 2541, 24767], "temperature": 0.0, "avg_logprob": -0.1392419731223976, "compression_ratio": 1.5870445344129556, "no_speech_prob": 4.1980754758697e-05}, {"id": 173, "seek": 92608, "start": 944.4000000000001, "end": 950.5600000000001, "text": " corpus. And it's available in TensorFlow data sets. You actually, you need to do the processing", "tokens": [1181, 31624, 13, 400, 309, 311, 2435, 294, 37624, 1412, 6352, 13, 509, 767, 11, 291, 643, 281, 360, 264, 9007], "temperature": 0.0, "avg_logprob": -0.1392419731223976, "compression_ratio": 1.5870445344129556, "no_speech_prob": 4.1980754758697e-05}, {"id": 174, "seek": 95056, "start": 950.56, "end": 956.7199999999999, "text": " yourself, which is somewhat computationally expensive. But nevertheless, it is entirely possible.", "tokens": [1803, 11, 597, 307, 8344, 24903, 379, 5124, 13, 583, 26924, 11, 309, 307, 7696, 1944, 13], "temperature": 0.0, "avg_logprob": -0.09478190705016419, "compression_ratio": 1.5523012552301256, "no_speech_prob": 9.75998118519783e-05}, {"id": 175, "seek": 95056, "start": 956.7199999999999, "end": 962.64, "text": " And it produces about 750 gigabytes of reasonably clean natural text data.", "tokens": [400, 309, 14725, 466, 31682, 42741, 295, 23551, 2541, 3303, 2487, 1412, 13], "temperature": 0.0, "avg_logprob": -0.09478190705016419, "compression_ratio": 1.5523012552301256, "no_speech_prob": 9.75998118519783e-05}, {"id": 176, "seek": 95056, "start": 964.8, "end": 970.3199999999999, "text": " Okay, so now we have our framework, our model, our pre-training data set. We need our pre-training", "tokens": [1033, 11, 370, 586, 321, 362, 527, 8388, 11, 527, 2316, 11, 527, 659, 12, 17227, 1760, 1412, 992, 13, 492, 643, 527, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.09478190705016419, "compression_ratio": 1.5523012552301256, "no_speech_prob": 9.75998118519783e-05}, {"id": 177, "seek": 95056, "start": 970.3199999999999, "end": 978.0799999999999, "text": " objective. What are we going to do to train the model on our unlabeled text? And so just to explain", "tokens": [10024, 13, 708, 366, 321, 516, 281, 360, 281, 3847, 264, 2316, 322, 527, 32118, 18657, 292, 2487, 30, 400, 370, 445, 281, 2903], "temperature": 0.0, "avg_logprob": -0.09478190705016419, "compression_ratio": 1.5523012552301256, "no_speech_prob": 9.75998118519783e-05}, {"id": 178, "seek": 97808, "start": 978.08, "end": 982.32, "text": " the objective that we chose kind of for our baseline experimental procedure, which again we will", "tokens": [264, 10024, 300, 321, 5111, 733, 295, 337, 527, 20518, 17069, 10747, 11, 597, 797, 321, 486], "temperature": 0.0, "avg_logprob": -0.11188065894296235, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0001739359722705558}, {"id": 179, "seek": 97808, "start": 982.32, "end": 987.76, "text": " experiment with different pre-training objectives later. Imagine that you have some original text", "tokens": [5120, 365, 819, 659, 12, 17227, 1760, 15961, 1780, 13, 11739, 300, 291, 362, 512, 3380, 2487], "temperature": 0.0, "avg_logprob": -0.11188065894296235, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0001739359722705558}, {"id": 180, "seek": 97808, "start": 987.76, "end": 992.88, "text": " sentence like thank you for inviting me to your party last week. And what we do is we basically", "tokens": [8174, 411, 1309, 291, 337, 18202, 385, 281, 428, 3595, 1036, 1243, 13, 400, 437, 321, 360, 307, 321, 1936], "temperature": 0.0, "avg_logprob": -0.11188065894296235, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0001739359722705558}, {"id": 181, "seek": 97808, "start": 992.88, "end": 997.76, "text": " choose some words at random and technically we're choosing tokens at random. But for now,", "tokens": [2826, 512, 2283, 412, 4974, 293, 12120, 321, 434, 10875, 22667, 412, 4974, 13, 583, 337, 586, 11], "temperature": 0.0, "avg_logprob": -0.11188065894296235, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0001739359722705558}, {"id": 182, "seek": 97808, "start": 997.76, "end": 1003.6, "text": " it's just assumed that tokens are words. And we're going to drop those tokens out. And so we end", "tokens": [309, 311, 445, 15895, 300, 22667, 366, 2283, 13, 400, 321, 434, 516, 281, 3270, 729, 22667, 484, 13, 400, 370, 321, 917], "temperature": 0.0, "avg_logprob": -0.11188065894296235, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0001739359722705558}, {"id": 183, "seek": 100360, "start": 1003.6, "end": 1008.16, "text": " up with something that looks like this. We, for every consecutive span of tokens that have been dropped", "tokens": [493, 365, 746, 300, 1542, 411, 341, 13, 492, 11, 337, 633, 30497, 16174, 295, 22667, 300, 362, 668, 8119], "temperature": 0.0, "avg_logprob": -0.12326418451902245, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.00010717389523051679}, {"id": 184, "seek": 100360, "start": 1008.16, "end": 1013.6800000000001, "text": " out, we replace it with a sentinel token. And each sentinel token gets a unique index. So one of", "tokens": [484, 11, 321, 7406, 309, 365, 257, 2279, 40952, 14862, 13, 400, 1184, 2279, 40952, 14862, 2170, 257, 3845, 8186, 13, 407, 472, 295], "temperature": 0.0, "avg_logprob": -0.12326418451902245, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.00010717389523051679}, {"id": 185, "seek": 100360, "start": 1013.6800000000001, "end": 1019.12, "text": " them we call it the sentinel x. And the other one will be the sentinel y. And so you can see that", "tokens": [552, 321, 818, 309, 264, 2279, 40952, 2031, 13, 400, 264, 661, 472, 486, 312, 264, 2279, 40952, 288, 13, 400, 370, 291, 393, 536, 300], "temperature": 0.0, "avg_logprob": -0.12326418451902245, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.00010717389523051679}, {"id": 186, "seek": 100360, "start": 1019.12, "end": 1024.88, "text": " because the words for and inviting are subsequent words that we decided to mask out by randomly", "tokens": [570, 264, 2283, 337, 293, 18202, 366, 19962, 2283, 300, 321, 3047, 281, 6094, 484, 538, 16979], "temperature": 0.0, "avg_logprob": -0.12326418451902245, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.00010717389523051679}, {"id": 187, "seek": 100360, "start": 1024.88, "end": 1029.92, "text": " masking words, we're going to replace both of those words with a single sentinel, single unique", "tokens": [31226, 2283, 11, 321, 434, 516, 281, 7406, 1293, 295, 729, 2283, 365, 257, 2167, 2279, 40952, 11, 2167, 3845], "temperature": 0.0, "avg_logprob": -0.12326418451902245, "compression_ratio": 1.828358208955224, "no_speech_prob": 0.00010717389523051679}, {"id": 188, "seek": 102992, "start": 1029.92, "end": 1035.1200000000001, "text": " sentinel token. And then the model's goal will just be to fill in the blanks. And so if you're", "tokens": [2279, 40952, 14862, 13, 400, 550, 264, 2316, 311, 3387, 486, 445, 312, 281, 2836, 294, 264, 8247, 82, 13, 400, 370, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.06539773941040039, "compression_ratio": 1.7224199288256228, "no_speech_prob": 4.13329507864546e-05}, {"id": 189, "seek": 102992, "start": 1035.1200000000001, "end": 1039.6000000000001, "text": " familiar with the birth pre-training objective, this is somewhat similar. The fact that we're", "tokens": [4963, 365, 264, 3965, 659, 12, 17227, 1760, 10024, 11, 341, 307, 8344, 2531, 13, 440, 1186, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.06539773941040039, "compression_ratio": 1.7224199288256228, "no_speech_prob": 4.13329507864546e-05}, {"id": 190, "seek": 102992, "start": 1039.6000000000001, "end": 1045.44, "text": " collapsing subsequent tokens into a span that we're going to be replacing words from is slightly", "tokens": [45339, 19962, 22667, 666, 257, 16174, 300, 321, 434, 516, 281, 312, 19139, 2283, 490, 307, 4748], "temperature": 0.0, "avg_logprob": -0.06539773941040039, "compression_ratio": 1.7224199288256228, "no_speech_prob": 4.13329507864546e-05}, {"id": 191, "seek": 102992, "start": 1045.44, "end": 1050.3200000000002, "text": " different. And the fact that we're reconstructing just the missing words and not the entire sequence", "tokens": [819, 13, 400, 264, 1186, 300, 321, 434, 31499, 278, 445, 264, 5361, 2283, 293, 406, 264, 2302, 8310], "temperature": 0.0, "avg_logprob": -0.06539773941040039, "compression_ratio": 1.7224199288256228, "no_speech_prob": 4.13329507864546e-05}, {"id": 192, "seek": 102992, "start": 1050.3200000000002, "end": 1057.44, "text": " is maybe slightly different too. Okay, so now I'll kind of talk through our baseline experimental", "tokens": [307, 1310, 4748, 819, 886, 13, 1033, 11, 370, 586, 286, 603, 733, 295, 751, 807, 527, 20518, 17069], "temperature": 0.0, "avg_logprob": -0.06539773941040039, "compression_ratio": 1.7224199288256228, "no_speech_prob": 4.13329507864546e-05}, {"id": 193, "seek": 105744, "start": 1057.44, "end": 1062.3200000000002, "text": " procedure that we're going to use. We're going to kind of tweak over time in very long various", "tokens": [10747, 300, 321, 434, 516, 281, 764, 13, 492, 434, 516, 281, 733, 295, 29879, 670, 565, 294, 588, 938, 3683], "temperature": 0.0, "avg_logprob": -0.12316775525737013, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.9853410018840805e-05}, {"id": 194, "seek": 105744, "start": 1062.3200000000002, "end": 1068.3200000000002, "text": " axes to explore the landscape of transfer learning, at least circa when this paper came out.", "tokens": [35387, 281, 6839, 264, 9661, 295, 5003, 2539, 11, 412, 1935, 45972, 562, 341, 3035, 1361, 484, 13], "temperature": 0.0, "avg_logprob": -0.12316775525737013, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.9853410018840805e-05}, {"id": 195, "seek": 105744, "start": 1069.2, "end": 1074.16, "text": " So to pre-training the model, we're going to take a model that has a birth base size encoder", "tokens": [407, 281, 659, 12, 17227, 1760, 264, 2316, 11, 321, 434, 516, 281, 747, 257, 2316, 300, 575, 257, 3965, 3096, 2744, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.12316775525737013, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.9853410018840805e-05}, {"id": 196, "seek": 105744, "start": 1074.16, "end": 1078.64, "text": " and decoder. So technically has twice as many parameters as birth base because there's a", "tokens": [293, 979, 19866, 13, 407, 12120, 575, 6091, 382, 867, 9834, 382, 3965, 3096, 570, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.12316775525737013, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.9853410018840805e-05}, {"id": 197, "seek": 105744, "start": 1078.64, "end": 1083.44, "text": " birth base size encoder and a birth base size decoder. We're going to use the denoising objective,", "tokens": [3965, 3096, 2744, 2058, 19866, 293, 257, 3965, 3096, 2744, 979, 19866, 13, 492, 434, 516, 281, 764, 264, 1441, 78, 3436, 10024, 11], "temperature": 0.0, "avg_logprob": -0.12316775525737013, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.9853410018840805e-05}, {"id": 198, "seek": 108344, "start": 1083.44, "end": 1087.92, "text": " the sort of mass language modeling objective that I just described. And we're going to apply it on", "tokens": [264, 1333, 295, 2758, 2856, 15983, 10024, 300, 286, 445, 7619, 13, 400, 321, 434, 516, 281, 3079, 309, 322], "temperature": 0.0, "avg_logprob": -0.09229721788500177, "compression_ratio": 1.678082191780822, "no_speech_prob": 2.429769119771663e-05}, {"id": 199, "seek": 108344, "start": 1087.92, "end": 1094.0, "text": " the C4 dataset that I mentioned earlier. We're going to pre-traine for about 34 billion tokens,", "tokens": [264, 383, 19, 28872, 300, 286, 2835, 3071, 13, 492, 434, 516, 281, 659, 12, 17227, 533, 337, 466, 12790, 5218, 22667, 11], "temperature": 0.0, "avg_logprob": -0.09229721788500177, "compression_ratio": 1.678082191780822, "no_speech_prob": 2.429769119771663e-05}, {"id": 200, "seek": 108344, "start": 1094.0, "end": 1098.64, "text": " which is about a quarter as long as birth base was trained. So it's not a ton of pre-training time,", "tokens": [597, 307, 466, 257, 6555, 382, 938, 382, 3965, 3096, 390, 8895, 13, 407, 309, 311, 406, 257, 2952, 295, 659, 12, 17227, 1760, 565, 11], "temperature": 0.0, "avg_logprob": -0.09229721788500177, "compression_ratio": 1.678082191780822, "no_speech_prob": 2.429769119771663e-05}, {"id": 201, "seek": 108344, "start": 1098.64, "end": 1102.88, "text": " but because we're training on, we're doing so many experiments, we need to cut it back a little bit.", "tokens": [457, 570, 321, 434, 3097, 322, 11, 321, 434, 884, 370, 867, 12050, 11, 321, 643, 281, 1723, 309, 646, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.09229721788500177, "compression_ratio": 1.678082191780822, "no_speech_prob": 2.429769119771663e-05}, {"id": 202, "seek": 108344, "start": 1104.16, "end": 1108.24, "text": " We're going to use an inverse square learning rate schedule that turned out to work reasonably", "tokens": [492, 434, 516, 281, 764, 364, 17340, 3732, 2539, 3314, 7567, 300, 3574, 484, 281, 589, 23551], "temperature": 0.0, "avg_logprob": -0.09229721788500177, "compression_ratio": 1.678082191780822, "no_speech_prob": 2.429769119771663e-05}, {"id": 203, "seek": 110824, "start": 1108.24, "end": 1113.84, "text": " well in our setting, but it's not a terribly important design decision. And then we'll fine-tune on", "tokens": [731, 294, 527, 3287, 11, 457, 309, 311, 406, 257, 22903, 1021, 1715, 3537, 13, 400, 550, 321, 603, 2489, 12, 83, 2613, 322], "temperature": 0.0, "avg_logprob": -0.11666533324095579, "compression_ratio": 1.6482758620689655, "no_speech_prob": 5.3901963838143274e-05}, {"id": 204, "seek": 110824, "start": 1113.84, "end": 1118.88, "text": " a variety of downstream tasks, kind of tasks that people care a lot about at the time. There's", "tokens": [257, 5673, 295, 30621, 9608, 11, 733, 295, 9608, 300, 561, 1127, 257, 688, 466, 412, 264, 565, 13, 821, 311], "temperature": 0.0, "avg_logprob": -0.11666533324095579, "compression_ratio": 1.6482758620689655, "no_speech_prob": 5.3901963838143274e-05}, {"id": 205, "seek": 110824, "start": 1118.88, "end": 1123.6, "text": " the glue benchmark, which is kind of a meta benchmark of many individual downstream tasks,", "tokens": [264, 8998, 18927, 11, 597, 307, 733, 295, 257, 19616, 18927, 295, 867, 2609, 30621, 9608, 11], "temperature": 0.0, "avg_logprob": -0.11666533324095579, "compression_ratio": 1.6482758620689655, "no_speech_prob": 5.3901963838143274e-05}, {"id": 206, "seek": 110824, "start": 1123.6, "end": 1128.56, "text": " like cola and STSB that I already mentioned. These are what some people might call natural", "tokens": [411, 40495, 293, 4904, 50, 33, 300, 286, 1217, 2835, 13, 1981, 366, 437, 512, 561, 1062, 818, 3303], "temperature": 0.0, "avg_logprob": -0.11666533324095579, "compression_ratio": 1.6482758620689655, "no_speech_prob": 5.3901963838143274e-05}, {"id": 207, "seek": 110824, "start": 1128.56, "end": 1134.56, "text": " language understanding tasks, but for the most part you can think of them as sentence classification,", "tokens": [2856, 3701, 9608, 11, 457, 337, 264, 881, 644, 291, 393, 519, 295, 552, 382, 8174, 21538, 11], "temperature": 0.0, "avg_logprob": -0.11666533324095579, "compression_ratio": 1.6482758620689655, "no_speech_prob": 5.3901963838143274e-05}, {"id": 208, "seek": 113456, "start": 1134.56, "end": 1140.6399999999999, "text": " sentence pair classification, or regression tasks. We also consider the CNN Daily Mail", "tokens": [8174, 6119, 21538, 11, 420, 24590, 9608, 13, 492, 611, 1949, 264, 24859, 19685, 29164], "temperature": 0.0, "avg_logprob": -0.11334570900338595, "compression_ratio": 1.9106529209621994, "no_speech_prob": 3.119696702924557e-05}, {"id": 209, "seek": 113456, "start": 1140.6399999999999, "end": 1144.72, "text": " Abstractive Summarization Corpus. This is a sequence-to-sequence problem where you're given a", "tokens": [46853, 1897, 488, 8626, 6209, 2144, 3925, 31624, 13, 639, 307, 257, 8310, 12, 1353, 12, 11834, 655, 1154, 689, 291, 434, 2212, 257], "temperature": 0.0, "avg_logprob": -0.11334570900338595, "compression_ratio": 1.9106529209621994, "no_speech_prob": 3.119696702924557e-05}, {"id": 210, "seek": 113456, "start": 1144.72, "end": 1149.76, "text": " news article and you have to output the summary. The squad question answering benchmark, which is", "tokens": [2583, 7222, 293, 291, 362, 281, 5598, 264, 12691, 13, 440, 15310, 1168, 13430, 18927, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.11334570900338595, "compression_ratio": 1.9106529209621994, "no_speech_prob": 3.119696702924557e-05}, {"id": 211, "seek": 113456, "start": 1149.76, "end": 1154.1599999999999, "text": " a reading comprehension benchmark where you're given a paragraph and you have to answer a question", "tokens": [257, 3760, 44991, 18927, 689, 291, 434, 2212, 257, 18865, 293, 291, 362, 281, 1867, 257, 1168], "temperature": 0.0, "avg_logprob": -0.11334570900338595, "compression_ratio": 1.9106529209621994, "no_speech_prob": 3.119696702924557e-05}, {"id": 212, "seek": 113456, "start": 1154.1599999999999, "end": 1158.8, "text": " about the paragraph. You can either attack it in an extractive setting where you extract the", "tokens": [466, 264, 18865, 13, 509, 393, 2139, 2690, 309, 294, 364, 8947, 488, 3287, 689, 291, 8947, 264], "temperature": 0.0, "avg_logprob": -0.11334570900338595, "compression_ratio": 1.9106529209621994, "no_speech_prob": 3.119696702924557e-05}, {"id": 213, "seek": 113456, "start": 1158.8, "end": 1162.72, "text": " answer from the paragraph or an abstractive setting where you just output the answer.", "tokens": [1867, 490, 264, 18865, 420, 364, 12649, 488, 3287, 689, 291, 445, 5598, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.11334570900338595, "compression_ratio": 1.9106529209621994, "no_speech_prob": 3.119696702924557e-05}, {"id": 214, "seek": 116272, "start": 1162.72, "end": 1169.1200000000001, "text": " We use the abstractive form because it's a text-to-text problem. We also included the super glue", "tokens": [492, 764, 264, 12649, 488, 1254, 570, 309, 311, 257, 2487, 12, 1353, 12, 25111, 1154, 13, 492, 611, 5556, 264, 1687, 8998], "temperature": 0.0, "avg_logprob": -0.21201446533203125, "compression_ratio": 1.9409722222222223, "no_speech_prob": 0.0001794120471458882}, {"id": 215, "seek": 116272, "start": 1169.1200000000001, "end": 1172.64, "text": " benchmark, which was a new benchmark at the time that was designed to essentially be a more", "tokens": [18927, 11, 597, 390, 257, 777, 18927, 412, 264, 565, 300, 390, 4761, 281, 4476, 312, 257, 544], "temperature": 0.0, "avg_logprob": -0.21201446533203125, "compression_ratio": 1.9409722222222223, "no_speech_prob": 0.0001794120471458882}, {"id": 216, "seek": 116272, "start": 1172.64, "end": 1177.6000000000001, "text": " difficult version of the glue benchmark. It has a new set of tasks that were hard for existing", "tokens": [2252, 3037, 295, 264, 8998, 18927, 13, 467, 575, 257, 777, 992, 295, 9608, 300, 645, 1152, 337, 6741], "temperature": 0.0, "avg_logprob": -0.21201446533203125, "compression_ratio": 1.9409722222222223, "no_speech_prob": 0.0001794120471458882}, {"id": 217, "seek": 116272, "start": 1177.6000000000001, "end": 1182.56, "text": " models. And then finally we included three translation data sets, English to German,", "tokens": [5245, 13, 400, 550, 2721, 321, 5556, 1045, 12853, 1412, 6352, 11, 3669, 281, 6521, 11], "temperature": 0.0, "avg_logprob": -0.21201446533203125, "compression_ratio": 1.9409722222222223, "no_speech_prob": 0.0001794120471458882}, {"id": 218, "seek": 116272, "start": 1182.56, "end": 1186.88, "text": " English to French and English to Romanian translation. English to French being the largest,", "tokens": [3669, 281, 5522, 293, 3669, 281, 49963, 12853, 13, 3669, 281, 5522, 885, 264, 6443, 11], "temperature": 0.0, "avg_logprob": -0.21201446533203125, "compression_ratio": 1.9409722222222223, "no_speech_prob": 0.0001794120471458882}, {"id": 219, "seek": 118688, "start": 1186.88, "end": 1192.64, "text": " which was an extremely large data set in English to Romanian being many more to my attitude smaller.", "tokens": [597, 390, 364, 4664, 2416, 1412, 992, 294, 3669, 281, 49963, 885, 867, 544, 281, 452, 10157, 4356, 13], "temperature": 0.0, "avg_logprob": -0.19803072725023543, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.000141887430800125}, {"id": 220, "seek": 118688, "start": 1194.0, "end": 1198.48, "text": " And we're going to fine tune on each of these tasks individually and separately. So we take the", "tokens": [400, 321, 434, 516, 281, 2489, 10864, 322, 1184, 295, 613, 9608, 16652, 293, 14759, 13, 407, 321, 747, 264], "temperature": 0.0, "avg_logprob": -0.19803072725023543, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.000141887430800125}, {"id": 221, "seek": 118688, "start": 1198.48, "end": 1202.48, "text": " pre-trained model and separately fine tune on each of these downstream tasks. And we're going to", "tokens": [659, 12, 17227, 2001, 2316, 293, 14759, 2489, 10864, 322, 1184, 295, 613, 30621, 9608, 13, 400, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.19803072725023543, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.000141887430800125}, {"id": 222, "seek": 118688, "start": 1202.48, "end": 1208.4, "text": " fine tune for up to 17 billion tokens, but we're going to save checkpoints along the way, evaluate", "tokens": [2489, 10864, 337, 493, 281, 3282, 5218, 22667, 11, 457, 321, 434, 516, 281, 3155, 1520, 20552, 2051, 264, 636, 11, 13059], "temperature": 0.0, "avg_logprob": -0.19803072725023543, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.000141887430800125}, {"id": 223, "seek": 118688, "start": 1208.4, "end": 1214.16, "text": " each checkpoint on the validation set and report the performance on the best checkpoint. And note that", "tokens": [1184, 42269, 322, 264, 24071, 992, 293, 2275, 264, 3389, 322, 264, 1151, 42269, 13, 400, 3637, 300], "temperature": 0.0, "avg_logprob": -0.19803072725023543, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.000141887430800125}, {"id": 224, "seek": 121416, "start": 1214.16, "end": 1219.76, "text": " this is not an experimentally valid way to report your performance because you're basically doing", "tokens": [341, 307, 406, 364, 5120, 379, 7363, 636, 281, 2275, 428, 3389, 570, 291, 434, 1936, 884], "temperature": 0.0, "avg_logprob": -0.1288506330642025, "compression_ratio": 1.979253112033195, "no_speech_prob": 3.646945333457552e-05}, {"id": 225, "seek": 121416, "start": 1219.76, "end": 1224.8000000000002, "text": " model selection on the data set that you are, the data split that you are, you're reporting", "tokens": [2316, 9450, 322, 264, 1412, 992, 300, 291, 366, 11, 264, 1412, 7472, 300, 291, 366, 11, 291, 434, 10031], "temperature": 0.0, "avg_logprob": -0.1288506330642025, "compression_ratio": 1.979253112033195, "no_speech_prob": 3.646945333457552e-05}, {"id": 226, "seek": 121416, "start": 1224.8000000000002, "end": 1229.28, "text": " performance on the data set split that you're doing model selection on, which is not a good way to", "tokens": [3389, 322, 264, 1412, 992, 7472, 300, 291, 434, 884, 2316, 9450, 322, 11, 597, 307, 406, 257, 665, 636, 281], "temperature": 0.0, "avg_logprob": -0.1288506330642025, "compression_ratio": 1.979253112033195, "no_speech_prob": 3.646945333457552e-05}, {"id": 227, "seek": 121416, "start": 1229.28, "end": 1234.4, "text": " compare different methods, but to compare within methods. It's, it's, it's, we're reasonably", "tokens": [6794, 819, 7150, 11, 457, 281, 6794, 1951, 7150, 13, 467, 311, 11, 309, 311, 11, 309, 311, 11, 321, 434, 23551], "temperature": 0.0, "avg_logprob": -0.1288506330642025, "compression_ratio": 1.979253112033195, "no_speech_prob": 3.646945333457552e-05}, {"id": 228, "seek": 121416, "start": 1234.4, "end": 1241.76, "text": " comfortable doing this. So now I'm going to kind of give you a very high level overview of some", "tokens": [4619, 884, 341, 13, 407, 586, 286, 478, 516, 281, 733, 295, 976, 291, 257, 588, 1090, 1496, 12492, 295, 512], "temperature": 0.0, "avg_logprob": -0.1288506330642025, "compression_ratio": 1.979253112033195, "no_speech_prob": 3.646945333457552e-05}, {"id": 229, "seek": 124176, "start": 1241.76, "end": 1247.76, "text": " of the experimental results in this paper. This paper is pretty huge in terms of just the number", "tokens": [295, 264, 17069, 3542, 294, 341, 3035, 13, 639, 3035, 307, 1238, 2603, 294, 2115, 295, 445, 264, 1230], "temperature": 0.0, "avg_logprob": -0.07353259722391764, "compression_ratio": 1.8079710144927537, "no_speech_prob": 7.48209422454238e-05}, {"id": 230, "seek": 124176, "start": 1247.76, "end": 1252.96, "text": " experiments we ran. And so if I was to really drill into this paper, it probably would take me the", "tokens": [12050, 321, 5872, 13, 400, 370, 498, 286, 390, 281, 534, 11392, 666, 341, 3035, 11, 309, 1391, 576, 747, 385, 264], "temperature": 0.0, "avg_logprob": -0.07353259722391764, "compression_ratio": 1.8079710144927537, "no_speech_prob": 7.48209422454238e-05}, {"id": 231, "seek": 124176, "start": 1252.96, "end": 1257.76, "text": " whole time, but there's other fun stuff that I want to tell you about. So, but, but anyways, the point", "tokens": [1379, 565, 11, 457, 456, 311, 661, 1019, 1507, 300, 286, 528, 281, 980, 291, 466, 13, 407, 11, 457, 11, 457, 13448, 11, 264, 935], "temperature": 0.0, "avg_logprob": -0.07353259722391764, "compression_ratio": 1.8079710144927537, "no_speech_prob": 7.48209422454238e-05}, {"id": 232, "seek": 124176, "start": 1257.76, "end": 1262.64, "text": " is I will be showing you lots of tables like this one. And so in these tables on the columns, you", "tokens": [307, 286, 486, 312, 4099, 291, 3195, 295, 8020, 411, 341, 472, 13, 400, 370, 294, 613, 8020, 322, 264, 13766, 11, 291], "temperature": 0.0, "avg_logprob": -0.07353259722391764, "compression_ratio": 1.8079710144927537, "no_speech_prob": 7.48209422454238e-05}, {"id": 233, "seek": 124176, "start": 1262.64, "end": 1266.8799999999999, "text": " have the performance on the various downstream tasks. And in the rows, you have different experimental", "tokens": [362, 264, 3389, 322, 264, 3683, 30621, 9608, 13, 400, 294, 264, 13241, 11, 291, 362, 819, 17069], "temperature": 0.0, "avg_logprob": -0.07353259722391764, "compression_ratio": 1.8079710144927537, "no_speech_prob": 7.48209422454238e-05}, {"id": 234, "seek": 126688, "start": 1266.88, "end": 1271.8400000000001, "text": " settings that we considered. So to give you an example, here's kind of the scores that we got", "tokens": [6257, 300, 321, 4888, 13, 407, 281, 976, 291, 364, 1365, 11, 510, 311, 733, 295, 264, 13444, 300, 321, 658], "temperature": 0.0, "avg_logprob": -0.10058393932524182, "compression_ratio": 1.7216117216117217, "no_speech_prob": 6.70811059535481e-05}, {"id": 235, "seek": 126688, "start": 1273.7600000000002, "end": 1278.8000000000002, "text": " from our baseline, which is the exact experimental procedure that I must describe, that I just", "tokens": [490, 527, 20518, 11, 597, 307, 264, 1900, 17069, 10747, 300, 286, 1633, 6786, 11, 300, 286, 445], "temperature": 0.0, "avg_logprob": -0.10058393932524182, "compression_ratio": 1.7216117216117217, "no_speech_prob": 6.70811059535481e-05}, {"id": 236, "seek": 126688, "start": 1278.8000000000002, "end": 1284.4, "text": " described. And we also ran that baseline 10 times in reporting with standard deviation on the", "tokens": [7619, 13, 400, 321, 611, 5872, 300, 20518, 1266, 1413, 294, 10031, 365, 3832, 25163, 322, 264], "temperature": 0.0, "avg_logprob": -0.10058393932524182, "compression_ratio": 1.7216117216117217, "no_speech_prob": 6.70811059535481e-05}, {"id": 237, "seek": 126688, "start": 1284.4, "end": 1289.92, "text": " second line. And then in this last line here, we're reporting the performance of the same", "tokens": [1150, 1622, 13, 400, 550, 294, 341, 1036, 1622, 510, 11, 321, 434, 10031, 264, 3389, 295, 264, 912], "temperature": 0.0, "avg_logprob": -0.10058393932524182, "compression_ratio": 1.7216117216117217, "no_speech_prob": 6.70811059535481e-05}, {"id": 238, "seek": 126688, "start": 1289.92, "end": 1294.64, "text": " model without any pre-training, just basically only trained separately with supervision on all of", "tokens": [2316, 1553, 604, 659, 12, 17227, 1760, 11, 445, 1936, 787, 8895, 14759, 365, 32675, 322, 439, 295], "temperature": 0.0, "avg_logprob": -0.10058393932524182, "compression_ratio": 1.7216117216117217, "no_speech_prob": 6.70811059535481e-05}, {"id": 239, "seek": 129464, "start": 1294.64, "end": 1301.3600000000001, "text": " these downstream tasks. And just to point out a couple of things on this table, the first obvious", "tokens": [613, 30621, 9608, 13, 400, 445, 281, 935, 484, 257, 1916, 295, 721, 322, 341, 3199, 11, 264, 700, 6322], "temperature": 0.0, "avg_logprob": -0.08516910856803961, "compression_ratio": 1.7112676056338028, "no_speech_prob": 0.0001214545191032812}, {"id": 240, "seek": 129464, "start": 1301.3600000000001, "end": 1308.0, "text": " thing is that in most cases, the pre-training setting is dramatically worse. So indeed, transfer", "tokens": [551, 307, 300, 294, 881, 3331, 11, 264, 659, 12, 17227, 1760, 3287, 307, 17548, 5324, 13, 407, 6451, 11, 5003], "temperature": 0.0, "avg_logprob": -0.08516910856803961, "compression_ratio": 1.7112676056338028, "no_speech_prob": 0.0001214545191032812}, {"id": 241, "seek": 129464, "start": 1308.0, "end": 1313.6000000000001, "text": " learning does tend to be helpful. The place where that's not true is actually on this English", "tokens": [2539, 775, 3928, 281, 312, 4961, 13, 440, 1081, 689, 300, 311, 406, 2074, 307, 767, 322, 341, 3669], "temperature": 0.0, "avg_logprob": -0.08516910856803961, "compression_ratio": 1.7112676056338028, "no_speech_prob": 0.0001214545191032812}, {"id": 242, "seek": 129464, "start": 1313.6000000000001, "end": 1317.8400000000001, "text": " difference translation task. And that's probably because it's such a big task that you actually", "tokens": [2649, 12853, 5633, 13, 400, 300, 311, 1391, 570, 309, 311, 1270, 257, 955, 5633, 300, 291, 767], "temperature": 0.0, "avg_logprob": -0.08516910856803961, "compression_ratio": 1.7112676056338028, "no_speech_prob": 0.0001214545191032812}, {"id": 243, "seek": 129464, "start": 1317.8400000000001, "end": 1323.68, "text": " don't really need pre-training to do well at it. We wanted to include this because if the performance", "tokens": [500, 380, 534, 643, 659, 12, 17227, 1760, 281, 360, 731, 412, 309, 13, 492, 1415, 281, 4090, 341, 570, 498, 264, 3389], "temperature": 0.0, "avg_logprob": -0.08516910856803961, "compression_ratio": 1.7112676056338028, "no_speech_prob": 0.0001214545191032812}, {"id": 244, "seek": 132368, "start": 1323.68, "end": 1329.3600000000001, "text": " regresses on this task, then that's something that we should be worried about. The next feature of", "tokens": [1121, 40352, 322, 341, 5633, 11, 550, 300, 311, 746, 300, 321, 820, 312, 5804, 466, 13, 440, 958, 4111, 295], "temperature": 0.0, "avg_logprob": -0.11787970727231323, "compression_ratio": 1.6845637583892616, "no_speech_prob": 4.06894541811198e-05}, {"id": 245, "seek": 132368, "start": 1329.3600000000001, "end": 1334.72, "text": " this table to notice is this little star appeared. That star will appear anytime there's a row in the", "tokens": [341, 3199, 281, 3449, 307, 341, 707, 3543, 8516, 13, 663, 3543, 486, 4204, 13038, 456, 311, 257, 5386, 294, 264], "temperature": 0.0, "avg_logprob": -0.11787970727231323, "compression_ratio": 1.6845637583892616, "no_speech_prob": 4.06894541811198e-05}, {"id": 246, "seek": 132368, "start": 1334.72, "end": 1340.24, "text": " table that's equivalent to our baseline. And another little thing to note, maybe if you're familiar with", "tokens": [3199, 300, 311, 10344, 281, 527, 20518, 13, 400, 1071, 707, 551, 281, 3637, 11, 1310, 498, 291, 434, 4963, 365], "temperature": 0.0, "avg_logprob": -0.11787970727231323, "compression_ratio": 1.6845637583892616, "no_speech_prob": 4.06894541811198e-05}, {"id": 247, "seek": 132368, "start": 1340.24, "end": 1345.1200000000001, "text": " the history, the score that we got on glue and squad was reasonably comparable to BERT. So it's a", "tokens": [264, 2503, 11, 264, 6175, 300, 321, 658, 322, 8998, 293, 15310, 390, 23551, 25323, 281, 363, 31479, 13, 407, 309, 311, 257], "temperature": 0.0, "avg_logprob": -0.11787970727231323, "compression_ratio": 1.6845637583892616, "no_speech_prob": 4.06894541811198e-05}, {"id": 248, "seek": 132368, "start": 1345.1200000000001, "end": 1352.4, "text": " decent sanity check. We have a model that has more parameters, but it's only trained for a quarter", "tokens": [8681, 47892, 1520, 13, 492, 362, 257, 2316, 300, 575, 544, 9834, 11, 457, 309, 311, 787, 8895, 337, 257, 6555], "temperature": 0.0, "avg_logprob": -0.11787970727231323, "compression_ratio": 1.6845637583892616, "no_speech_prob": 4.06894541811198e-05}, {"id": 249, "seek": 135240, "start": 1352.4, "end": 1358.8000000000002, "text": " as long. And it nevertheless got comparable performance, which using a similar objective, so we", "tokens": [382, 938, 13, 400, 309, 26924, 658, 25323, 3389, 11, 597, 1228, 257, 2531, 10024, 11, 370, 321], "temperature": 0.0, "avg_logprob": -0.14206444445273858, "compression_ratio": 1.8092307692307692, "no_speech_prob": 8.34805759950541e-05}, {"id": 250, "seek": 135240, "start": 1358.8000000000002, "end": 1362.5600000000002, "text": " shouldn't be too surprised about that. And then the last thing to mention is that we're going to use", "tokens": [4659, 380, 312, 886, 6100, 466, 300, 13, 400, 550, 264, 1036, 551, 281, 2152, 307, 300, 321, 434, 516, 281, 764], "temperature": 0.0, "avg_logprob": -0.14206444445273858, "compression_ratio": 1.8092307692307692, "no_speech_prob": 8.34805759950541e-05}, {"id": 251, "seek": 135240, "start": 1362.5600000000002, "end": 1367.1200000000001, "text": " this standard deviation over and over again so that we can bold entries in the table when there", "tokens": [341, 3832, 25163, 670, 293, 670, 797, 370, 300, 321, 393, 11928, 23041, 294, 264, 3199, 562, 456], "temperature": 0.0, "avg_logprob": -0.14206444445273858, "compression_ratio": 1.8092307692307692, "no_speech_prob": 8.34805759950541e-05}, {"id": 252, "seek": 135240, "start": 1367.1200000000001, "end": 1372.4, "text": " within one standard deviation of the maximum value for that data set in the table. So now, I'll just", "tokens": [1951, 472, 3832, 25163, 295, 264, 6674, 2158, 337, 300, 1412, 992, 294, 264, 3199, 13, 407, 586, 11, 286, 603, 445], "temperature": 0.0, "avg_logprob": -0.14206444445273858, "compression_ratio": 1.8092307692307692, "no_speech_prob": 8.34805759950541e-05}, {"id": 253, "seek": 135240, "start": 1372.4, "end": 1376.24, "text": " make a big disclaimer, which is we're going to compare lots of different things. We're going to run", "tokens": [652, 257, 955, 40896, 11, 597, 307, 321, 434, 516, 281, 6794, 3195, 295, 819, 721, 13, 492, 434, 516, 281, 1190], "temperature": 0.0, "avg_logprob": -0.14206444445273858, "compression_ratio": 1.8092307692307692, "no_speech_prob": 8.34805759950541e-05}, {"id": 254, "seek": 135240, "start": 1376.24, "end": 1381.52, "text": " lots of experiments, but we're not going to tweak any hyperparameters because if we did, like,", "tokens": [3195, 295, 12050, 11, 457, 321, 434, 406, 516, 281, 29879, 604, 9848, 2181, 335, 6202, 570, 498, 321, 630, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.14206444445273858, "compression_ratio": 1.8092307692307692, "no_speech_prob": 8.34805759950541e-05}, {"id": 255, "seek": 138152, "start": 1381.52, "end": 1386.96, "text": " change the learning rate or whatever, it would be just too computationally expensive to do this", "tokens": [1319, 264, 2539, 3314, 420, 2035, 11, 309, 576, 312, 445, 886, 24903, 379, 5124, 281, 360, 341], "temperature": 0.0, "avg_logprob": -0.0886088362685195, "compression_ratio": 1.6282894736842106, "no_speech_prob": 8.612766396254301e-05}, {"id": 256, "seek": 138152, "start": 1386.96, "end": 1393.44, "text": " for each individual methods. Our hope is that this is okay because we are treating all problems in", "tokens": [337, 1184, 2609, 7150, 13, 2621, 1454, 307, 300, 341, 307, 1392, 570, 321, 366, 15083, 439, 2740, 294], "temperature": 0.0, "avg_logprob": -0.0886088362685195, "compression_ratio": 1.6282894736842106, "no_speech_prob": 8.612766396254301e-05}, {"id": 257, "seek": 138152, "start": 1393.44, "end": 1399.44, "text": " exactly the same framework. We're always doing text to text maximum likelihood training. So hopefully", "tokens": [2293, 264, 912, 8388, 13, 492, 434, 1009, 884, 2487, 281, 2487, 6674, 22119, 3097, 13, 407, 4696], "temperature": 0.0, "avg_logprob": -0.0886088362685195, "compression_ratio": 1.6282894736842106, "no_speech_prob": 8.612766396254301e-05}, {"id": 258, "seek": 138152, "start": 1399.44, "end": 1403.84, "text": " we can keep hyperparameters fixed. And arguably, if you propose a new method that requires extensive", "tokens": [321, 393, 1066, 9848, 2181, 335, 6202, 6806, 13, 400, 26771, 11, 498, 291, 17421, 257, 777, 3170, 300, 7029, 13246], "temperature": 0.0, "avg_logprob": -0.0886088362685195, "compression_ratio": 1.6282894736842106, "no_speech_prob": 8.612766396254301e-05}, {"id": 259, "seek": 138152, "start": 1403.84, "end": 1408.8, "text": " hyperparameter tuning, it's not a very useful method for practitioners. And we'll get into that a", "tokens": [9848, 2181, 335, 2398, 15164, 11, 309, 311, 406, 257, 588, 4420, 3170, 337, 25742, 13, 400, 321, 603, 483, 666, 300, 257], "temperature": 0.0, "avg_logprob": -0.0886088362685195, "compression_ratio": 1.6282894736842106, "no_speech_prob": 8.612766396254301e-05}, {"id": 260, "seek": 140880, "start": 1408.8, "end": 1413.68, "text": " little bit more later when I talk about architectural modifications too. The other thing I'll say is", "tokens": [707, 857, 544, 1780, 562, 286, 751, 466, 26621, 26881, 886, 13, 440, 661, 551, 286, 603, 584, 307], "temperature": 0.0, "avg_logprob": -0.10211497490559149, "compression_ratio": 1.6868686868686869, "no_speech_prob": 6.203904194990173e-05}, {"id": 261, "seek": 140880, "start": 1413.68, "end": 1418.08, "text": " that while we did run lots of experiments, there's no way we could be comprehensive because there were", "tokens": [300, 1339, 321, 630, 1190, 3195, 295, 12050, 11, 456, 311, 572, 636, 321, 727, 312, 13914, 570, 456, 645], "temperature": 0.0, "avg_logprob": -0.10211497490559149, "compression_ratio": 1.6868686868686869, "no_speech_prob": 6.203904194990173e-05}, {"id": 262, "seek": 140880, "start": 1418.08, "end": 1424.8, "text": " so many methods out there. And the inclusion or exclusion of one particular method is not meant as", "tokens": [370, 867, 7150, 484, 456, 13, 400, 264, 15874, 420, 33049, 295, 472, 1729, 3170, 307, 406, 4140, 382], "temperature": 0.0, "avg_logprob": -0.10211497490559149, "compression_ratio": 1.6868686868686869, "no_speech_prob": 6.203904194990173e-05}, {"id": 263, "seek": 140880, "start": 1424.8, "end": 1430.8, "text": " a judgment on its quality. It's just what we were able to do given the constraints that we were", "tokens": [257, 12216, 322, 1080, 3125, 13, 467, 311, 445, 437, 321, 645, 1075, 281, 360, 2212, 264, 18491, 300, 321, 645], "temperature": 0.0, "avg_logprob": -0.10211497490559149, "compression_ratio": 1.6868686868686869, "no_speech_prob": 6.203904194990173e-05}, {"id": 264, "seek": 143080, "start": 1430.8, "end": 1438.8, "text": " working under. So the first set of experiments that we ran were to compare different model structures.", "tokens": [1364, 833, 13, 407, 264, 700, 992, 295, 12050, 300, 321, 5872, 645, 281, 6794, 819, 2316, 9227, 13], "temperature": 0.0, "avg_logprob": -0.0934151869553786, "compression_ratio": 1.7443946188340806, "no_speech_prob": 1.7230726371053606e-05}, {"id": 265, "seek": 143080, "start": 1438.8, "end": 1445.04, "text": " So as I mentioned earlier, the main baseline T5 model is an encoder decoder model. And in this", "tokens": [407, 382, 286, 2835, 3071, 11, 264, 2135, 20518, 314, 20, 2316, 307, 364, 2058, 19866, 979, 19866, 2316, 13, 400, 294, 341], "temperature": 0.0, "avg_logprob": -0.0934151869553786, "compression_ratio": 1.7443946188340806, "no_speech_prob": 1.7230726371053606e-05}, {"id": 266, "seek": 143080, "start": 1445.04, "end": 1450.8799999999999, "text": " case, you have a separate layer stack for that encodes a sequence and a separate layer stack that", "tokens": [1389, 11, 291, 362, 257, 4994, 4583, 8630, 337, 300, 2058, 4789, 257, 8310, 293, 257, 4994, 4583, 8630, 300], "temperature": 0.0, "avg_logprob": -0.0934151869553786, "compression_ratio": 1.7443946188340806, "no_speech_prob": 1.7230726371053606e-05}, {"id": 267, "seek": 143080, "start": 1450.8799999999999, "end": 1456.0, "text": " decodes the target sequence. Basically, it generates the target sequence, one token by token,", "tokens": [979, 4789, 264, 3779, 8310, 13, 8537, 11, 309, 23815, 264, 3779, 8310, 11, 472, 14862, 538, 14862, 11], "temperature": 0.0, "avg_logprob": -0.0934151869553786, "compression_ratio": 1.7443946188340806, "no_speech_prob": 1.7230726371053606e-05}, {"id": 268, "seek": 145600, "start": 1456.0, "end": 1461.84, "text": " while attending back to the encoders output to figure out what it should condition on. The next", "tokens": [1339, 15862, 646, 281, 264, 2058, 378, 433, 5598, 281, 2573, 484, 437, 309, 820, 4188, 322, 13, 440, 958], "temperature": 0.0, "avg_logprob": -0.08733057975769043, "compression_ratio": 1.9635627530364372, "no_speech_prob": 4.907991024083458e-05}, {"id": 269, "seek": 145600, "start": 1462.48, "end": 1466.88, "text": " setup that we considered is an encoder decoder model except that all of the relevant parameters", "tokens": [8657, 300, 321, 4888, 307, 364, 2058, 19866, 979, 19866, 2316, 3993, 300, 439, 295, 264, 7340, 9834], "temperature": 0.0, "avg_logprob": -0.08733057975769043, "compression_ratio": 1.9635627530364372, "no_speech_prob": 4.907991024083458e-05}, {"id": 270, "seek": 145600, "start": 1466.88, "end": 1472.72, "text": " in the encoder decoder are shared. So they're basically half as many parameters. And then finally,", "tokens": [294, 264, 2058, 19866, 979, 19866, 366, 5507, 13, 407, 436, 434, 1936, 1922, 382, 867, 9834, 13, 400, 550, 2721, 11], "temperature": 0.0, "avg_logprob": -0.08733057975769043, "compression_ratio": 1.9635627530364372, "no_speech_prob": 4.907991024083458e-05}, {"id": 271, "seek": 145600, "start": 1472.72, "end": 1477.28, "text": " another variant that we considered is an encoder decoder model where the encoder and decoder have", "tokens": [1071, 17501, 300, 321, 4888, 307, 364, 2058, 19866, 979, 19866, 2316, 689, 264, 2058, 19866, 293, 979, 19866, 362], "temperature": 0.0, "avg_logprob": -0.08733057975769043, "compression_ratio": 1.9635627530364372, "no_speech_prob": 4.907991024083458e-05}, {"id": 272, "seek": 145600, "start": 1477.28, "end": 1482.72, "text": " half as many layers as they do in the baseline. And that's because we're also considering single", "tokens": [1922, 382, 867, 7914, 382, 436, 360, 294, 264, 20518, 13, 400, 300, 311, 570, 321, 434, 611, 8079, 2167], "temperature": 0.0, "avg_logprob": -0.08733057975769043, "compression_ratio": 1.9635627530364372, "no_speech_prob": 4.907991024083458e-05}, {"id": 273, "seek": 148272, "start": 1482.72, "end": 1488.64, "text": " stack models, the language model and what we call a prefix language model. The language model is a", "tokens": [8630, 5245, 11, 264, 2856, 2316, 293, 437, 321, 818, 257, 46969, 2856, 2316, 13, 440, 2856, 2316, 307, 257], "temperature": 0.0, "avg_logprob": -0.08649799173528498, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.00012337382941041142}, {"id": 274, "seek": 148272, "start": 1488.64, "end": 1493.68, "text": " model that models the sequence strictly from the left to right fashion in a causal fashion. It", "tokens": [2316, 300, 5245, 264, 8310, 20792, 490, 264, 1411, 281, 558, 6700, 294, 257, 38755, 6700, 13, 467], "temperature": 0.0, "avg_logprob": -0.08649799173528498, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.00012337382941041142}, {"id": 275, "seek": 148272, "start": 1493.68, "end": 1499.44, "text": " basically just ingests tokens one at a time and predicts the next token. And you can actually", "tokens": [1936, 445, 3957, 4409, 22667, 472, 412, 257, 565, 293, 6069, 82, 264, 958, 14862, 13, 400, 291, 393, 767], "temperature": 0.0, "avg_logprob": -0.08649799173528498, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.00012337382941041142}, {"id": 276, "seek": 148272, "start": 1499.44, "end": 1504.24, "text": " apply these to text to text problems by basically feeding the input as a prefix before you start", "tokens": [3079, 613, 281, 2487, 281, 2487, 2740, 538, 1936, 12919, 264, 4846, 382, 257, 46969, 949, 291, 722], "temperature": 0.0, "avg_logprob": -0.08649799173528498, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.00012337382941041142}, {"id": 277, "seek": 148272, "start": 1504.24, "end": 1510.88, "text": " predicting anything. Now, if you just use a language model in its strict format, then you still have to", "tokens": [32884, 1340, 13, 823, 11, 498, 291, 445, 764, 257, 2856, 2316, 294, 1080, 10910, 7877, 11, 550, 291, 920, 362, 281], "temperature": 0.0, "avg_logprob": -0.08649799173528498, "compression_ratio": 1.8625954198473282, "no_speech_prob": 0.00012337382941041142}, {"id": 278, "seek": 151088, "start": 1510.88, "end": 1516.5600000000002, "text": " have what we would call a causal mask, so a causal attention pattern on the prefix. And that's", "tokens": [362, 437, 321, 576, 818, 257, 38755, 6094, 11, 370, 257, 38755, 3202, 5102, 322, 264, 46969, 13, 400, 300, 311], "temperature": 0.0, "avg_logprob": -0.08085828122839464, "compression_ratio": 1.7846153846153847, "no_speech_prob": 8.348369010491297e-05}, {"id": 279, "seek": 151088, "start": 1516.5600000000002, "end": 1524.4, "text": " actually how the GPT series models treat all of their problems. But because we are explicitly denoting", "tokens": [767, 577, 264, 26039, 51, 2638, 5245, 2387, 439, 295, 641, 2740, 13, 583, 570, 321, 366, 20803, 1441, 17001], "temperature": 0.0, "avg_logprob": -0.08085828122839464, "compression_ratio": 1.7846153846153847, "no_speech_prob": 8.348369010491297e-05}, {"id": 280, "seek": 151088, "start": 1524.4, "end": 1528.48, "text": " part of the sequence as an input and the rest of the sequence as a target, we actually can allow", "tokens": [644, 295, 264, 8310, 382, 364, 4846, 293, 264, 1472, 295, 264, 8310, 382, 257, 3779, 11, 321, 767, 393, 2089], "temperature": 0.0, "avg_logprob": -0.08085828122839464, "compression_ratio": 1.7846153846153847, "no_speech_prob": 8.348369010491297e-05}, {"id": 281, "seek": 151088, "start": 1528.48, "end": 1533.5200000000002, "text": " the model to have full visibility, you know, a non-causal mask on the input region of the sequence.", "tokens": [264, 2316, 281, 362, 1577, 19883, 11, 291, 458, 11, 257, 2107, 12, 496, 11765, 6094, 322, 264, 4846, 4458, 295, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.08085828122839464, "compression_ratio": 1.7846153846153847, "no_speech_prob": 8.348369010491297e-05}, {"id": 282, "seek": 151088, "start": 1533.5200000000002, "end": 1536.3200000000002, "text": " And when we make that change, we call that the prefix language model.", "tokens": [400, 562, 321, 652, 300, 1319, 11, 321, 818, 300, 264, 46969, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.08085828122839464, "compression_ratio": 1.7846153846153847, "no_speech_prob": 8.348369010491297e-05}, {"id": 283, "seek": 153632, "start": 1536.32, "end": 1542.3999999999999, "text": " And now the upshot of all of this really is that the encoder decoder model for our framework turns", "tokens": [400, 586, 264, 493, 18402, 295, 439, 295, 341, 534, 307, 300, 264, 2058, 19866, 979, 19866, 2316, 337, 527, 8388, 4523], "temperature": 0.0, "avg_logprob": -0.08382894374706128, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00017948936147149652}, {"id": 284, "seek": 153632, "start": 1542.3999999999999, "end": 1546.72, "text": " out to work best. You can see that when we share the parameters, it does hurt performance a little", "tokens": [484, 281, 589, 1151, 13, 509, 393, 536, 300, 562, 321, 2073, 264, 9834, 11, 309, 775, 4607, 3389, 257, 707], "temperature": 0.0, "avg_logprob": -0.08382894374706128, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00017948936147149652}, {"id": 285, "seek": 153632, "start": 1546.72, "end": 1552.32, "text": " bit, but maybe a little less than you might expect. The prefix language model attains slightly", "tokens": [857, 11, 457, 1310, 257, 707, 1570, 813, 291, 1062, 2066, 13, 440, 46969, 2856, 2316, 951, 2315, 4748], "temperature": 0.0, "avg_logprob": -0.08382894374706128, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00017948936147149652}, {"id": 286, "seek": 153632, "start": 1552.32, "end": 1556.72, "text": " worse performance, but significantly better performance than doing strictly causal left to right", "tokens": [5324, 3389, 11, 457, 10591, 1101, 3389, 813, 884, 20792, 38755, 1411, 281, 558], "temperature": 0.0, "avg_logprob": -0.08382894374706128, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00017948936147149652}, {"id": 287, "seek": 153632, "start": 1556.72, "end": 1561.84, "text": " language modeling, which is what you see in the fourth row here. And finally, having the number", "tokens": [2856, 15983, 11, 597, 307, 437, 291, 536, 294, 264, 6409, 5386, 510, 13, 400, 2721, 11, 1419, 264, 1230], "temperature": 0.0, "avg_logprob": -0.08382894374706128, "compression_ratio": 1.7446043165467626, "no_speech_prob": 0.00017948936147149652}, {"id": 288, "seek": 156184, "start": 1561.84, "end": 1569.04, "text": " parameters in the encoder and decoder is harm's performance significantly. One thing to note is that", "tokens": [9834, 294, 264, 2058, 19866, 293, 979, 19866, 307, 6491, 311, 3389, 10591, 13, 1485, 551, 281, 3637, 307, 300], "temperature": 0.0, "avg_logprob": -0.11107176228573448, "compression_ratio": 1.8168498168498168, "no_speech_prob": 4.1984141716966406e-05}, {"id": 289, "seek": 156184, "start": 1569.04, "end": 1573.9199999999998, "text": " in all of these cases, we're processing the same total sequence length. It's the same input sequence", "tokens": [294, 439, 295, 613, 3331, 11, 321, 434, 9007, 264, 912, 3217, 8310, 4641, 13, 467, 311, 264, 912, 4846, 8310], "temperature": 0.0, "avg_logprob": -0.11107176228573448, "compression_ratio": 1.8168498168498168, "no_speech_prob": 4.1984141716966406e-05}, {"id": 290, "seek": 156184, "start": 1573.9199999999998, "end": 1579.36, "text": " in the same target sequence. So in most of these cases, the total number of flops required to process", "tokens": [294, 264, 912, 3779, 8310, 13, 407, 294, 881, 295, 613, 3331, 11, 264, 3217, 1230, 295, 932, 3370, 4739, 281, 1399], "temperature": 0.0, "avg_logprob": -0.11107176228573448, "compression_ratio": 1.8168498168498168, "no_speech_prob": 4.1984141716966406e-05}, {"id": 291, "seek": 156184, "start": 1579.36, "end": 1584.1599999999999, "text": " the sequences the same, even though the number parameters is twice as many in the baseline model.", "tokens": [264, 22978, 264, 912, 11, 754, 1673, 264, 1230, 9834, 307, 6091, 382, 867, 294, 264, 20518, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11107176228573448, "compression_ratio": 1.8168498168498168, "no_speech_prob": 4.1984141716966406e-05}, {"id": 292, "seek": 156184, "start": 1586.1599999999999, "end": 1590.6399999999999, "text": " So the next thing we looked into were different variants on our pre-training objective. So the", "tokens": [407, 264, 958, 551, 321, 2956, 666, 645, 819, 21669, 322, 527, 659, 12, 17227, 1760, 10024, 13, 407, 264], "temperature": 0.0, "avg_logprob": -0.11107176228573448, "compression_ratio": 1.8168498168498168, "no_speech_prob": 4.1984141716966406e-05}, {"id": 293, "seek": 159064, "start": 1590.64, "end": 1595.3600000000001, "text": " first thing we did was kind of compare different high level approaches, maybe just training the model", "tokens": [700, 551, 321, 630, 390, 733, 295, 6794, 819, 1090, 1496, 11587, 11, 1310, 445, 3097, 264, 2316], "temperature": 0.0, "avg_logprob": -0.1443377453347911, "compression_ratio": 1.7992700729927007, "no_speech_prob": 3.76278258045204e-05}, {"id": 294, "seek": 159064, "start": 1595.3600000000001, "end": 1600.0800000000002, "text": " to predict the next token, one token at a time. That's kind of a language modeling objective. Another", "tokens": [281, 6069, 264, 958, 14862, 11, 472, 14862, 412, 257, 565, 13, 663, 311, 733, 295, 257, 2856, 15983, 10024, 13, 3996], "temperature": 0.0, "avg_logprob": -0.1443377453347911, "compression_ratio": 1.7992700729927007, "no_speech_prob": 3.76278258045204e-05}, {"id": 295, "seek": 159064, "start": 1600.0800000000002, "end": 1604.72, "text": " would be to take the input sequence, shuffle it up and train the model to predict the unshuffled", "tokens": [576, 312, 281, 747, 264, 4846, 8310, 11, 39426, 309, 493, 293, 3847, 264, 2316, 281, 6069, 264, 2693, 71, 33974], "temperature": 0.0, "avg_logprob": -0.1443377453347911, "compression_ratio": 1.7992700729927007, "no_speech_prob": 3.76278258045204e-05}, {"id": 296, "seek": 159064, "start": 1604.72, "end": 1610.72, "text": " sequence, or to consider a mass language model style, a bird style objective, like the one that we", "tokens": [8310, 11, 420, 281, 1949, 257, 2758, 2856, 2316, 3758, 11, 257, 5255, 3758, 10024, 11, 411, 264, 472, 300, 321], "temperature": 0.0, "avg_logprob": -0.1443377453347911, "compression_ratio": 1.7992700729927007, "no_speech_prob": 3.76278258045204e-05}, {"id": 297, "seek": 159064, "start": 1610.72, "end": 1616.64, "text": " that we mentioned earlier. And now on the second step that I'm showing here, the results for,", "tokens": [300, 321, 2835, 3071, 13, 400, 586, 322, 264, 1150, 1823, 300, 286, 478, 4099, 510, 11, 264, 3542, 337, 11], "temperature": 0.0, "avg_logprob": -0.1443377453347911, "compression_ratio": 1.7992700729927007, "no_speech_prob": 3.76278258045204e-05}, {"id": 298, "seek": 161664, "start": 1616.64, "end": 1621.2, "text": " we considered a bird style objective where the model is trained to predict the entire", "tokens": [321, 4888, 257, 5255, 3758, 10024, 689, 264, 2316, 307, 8895, 281, 6069, 264, 2302], "temperature": 0.0, "avg_logprob": -0.12753422727289887, "compression_ratio": 1.7529411764705882, "no_speech_prob": 5.0637623644433916e-05}, {"id": 299, "seek": 161664, "start": 1622.48, "end": 1627.1200000000001, "text": " original uncorrupted input sequence, a mass style objective, which is quite similar.", "tokens": [3380, 6219, 284, 5428, 292, 4846, 8310, 11, 257, 2758, 3758, 10024, 11, 597, 307, 1596, 2531, 13], "temperature": 0.0, "avg_logprob": -0.12753422727289887, "compression_ratio": 1.7529411764705882, "no_speech_prob": 5.0637623644433916e-05}, {"id": 300, "seek": 161664, "start": 1628.48, "end": 1632.5600000000002, "text": " And then a replace corrupted spans objective, which is like the one that I described at the", "tokens": [400, 550, 257, 7406, 39480, 44086, 10024, 11, 597, 307, 411, 264, 472, 300, 286, 7619, 412, 264], "temperature": 0.0, "avg_logprob": -0.12753422727289887, "compression_ratio": 1.7529411764705882, "no_speech_prob": 5.0637623644433916e-05}, {"id": 301, "seek": 161664, "start": 1632.5600000000002, "end": 1636.88, "text": " beginning that we're using in our baseline model. And finally, a variant where rather than", "tokens": [2863, 300, 321, 434, 1228, 294, 527, 20518, 2316, 13, 400, 2721, 11, 257, 17501, 689, 2831, 813], "temperature": 0.0, "avg_logprob": -0.12753422727289887, "compression_ratio": 1.7529411764705882, "no_speech_prob": 5.0637623644433916e-05}, {"id": 302, "seek": 161664, "start": 1636.88, "end": 1643.1200000000001, "text": " replacing each token with a unique Sentinel token, we just dropped the mass tokens completely", "tokens": [19139, 1184, 14862, 365, 257, 3845, 49498, 14862, 11, 321, 445, 8119, 264, 2758, 22667, 2584], "temperature": 0.0, "avg_logprob": -0.12753422727289887, "compression_ratio": 1.7529411764705882, "no_speech_prob": 5.0637623644433916e-05}, {"id": 303, "seek": 164312, "start": 1643.12, "end": 1648.8799999999999, "text": " and train the model to predict the dropped tokens. And you can see that the latter two options", "tokens": [293, 3847, 264, 2316, 281, 6069, 264, 8119, 22667, 13, 400, 291, 393, 536, 300, 264, 18481, 732, 3956], "temperature": 0.0, "avg_logprob": -0.06502283573150634, "compression_ratio": 1.904564315352697, "no_speech_prob": 6.401221617124975e-05}, {"id": 304, "seek": 164312, "start": 1648.8799999999999, "end": 1653.6, "text": " work roughly as well as one another. But another pertinent difference between these", "tokens": [589, 9810, 382, 731, 382, 472, 1071, 13, 583, 1071, 13269, 11058, 2649, 1296, 613], "temperature": 0.0, "avg_logprob": -0.06502283573150634, "compression_ratio": 1.904564315352697, "no_speech_prob": 6.401221617124975e-05}, {"id": 305, "seek": 164312, "start": 1653.6, "end": 1659.28, "text": " these sets of objectives is that the first two involve predicting the entire input sequence.", "tokens": [613, 6352, 295, 15961, 307, 300, 264, 700, 732, 9494, 32884, 264, 2302, 4846, 8310, 13], "temperature": 0.0, "avg_logprob": -0.06502283573150634, "compression_ratio": 1.904564315352697, "no_speech_prob": 6.401221617124975e-05}, {"id": 306, "seek": 164312, "start": 1659.28, "end": 1664.6399999999999, "text": " And the last two basically just involve predicting the massed out tokens. And when you only", "tokens": [400, 264, 1036, 732, 1936, 445, 9494, 32884, 264, 2758, 292, 484, 22667, 13, 400, 562, 291, 787], "temperature": 0.0, "avg_logprob": -0.06502283573150634, "compression_ratio": 1.904564315352697, "no_speech_prob": 6.401221617124975e-05}, {"id": 307, "seek": 164312, "start": 1664.6399999999999, "end": 1668.9599999999998, "text": " predict the massed out tokens, you have a much shorter target sequence. And so the overall cost", "tokens": [6069, 264, 2758, 292, 484, 22667, 11, 291, 362, 257, 709, 11639, 3779, 8310, 13, 400, 370, 264, 4787, 2063], "temperature": 0.0, "avg_logprob": -0.06502283573150634, "compression_ratio": 1.904564315352697, "no_speech_prob": 6.401221617124975e-05}, {"id": 308, "seek": 166896, "start": 1668.96, "end": 1674.0, "text": " is significantly lower for pre-training. So we decided that that was the best approach.", "tokens": [307, 10591, 3126, 337, 659, 12, 17227, 1760, 13, 407, 321, 3047, 300, 300, 390, 264, 1151, 3109, 13], "temperature": 0.0, "avg_logprob": -0.11108796861436632, "compression_ratio": 1.6283185840707965, "no_speech_prob": 8.091329800663516e-05}, {"id": 309, "seek": 166896, "start": 1674.0, "end": 1680.24, "text": " And then we considered other hyper parameters in our masking strategy, such as how many tokens to", "tokens": [400, 550, 321, 4888, 661, 9848, 9834, 294, 527, 31226, 5206, 11, 1270, 382, 577, 867, 22667, 281], "temperature": 0.0, "avg_logprob": -0.11108796861436632, "compression_ratio": 1.6283185840707965, "no_speech_prob": 8.091329800663516e-05}, {"id": 310, "seek": 166896, "start": 1680.24, "end": 1687.52, "text": " to mask out. So the next thing we considered were different variants of a pre-training data set.", "tokens": [281, 6094, 484, 13, 407, 264, 958, 551, 321, 4888, 645, 819, 21669, 295, 257, 659, 12, 17227, 1760, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.11108796861436632, "compression_ratio": 1.6283185840707965, "no_speech_prob": 8.091329800663516e-05}, {"id": 311, "seek": 166896, "start": 1688.4, "end": 1693.8400000000001, "text": " In our baseline, we use the C4 data set that I proposed at the beginning of the talk.", "tokens": [682, 527, 20518, 11, 321, 764, 264, 383, 19, 1412, 992, 300, 286, 10348, 412, 264, 2863, 295, 264, 751, 13], "temperature": 0.0, "avg_logprob": -0.11108796861436632, "compression_ratio": 1.6283185840707965, "no_speech_prob": 8.091329800663516e-05}, {"id": 312, "seek": 169384, "start": 1693.84, "end": 1699.36, "text": " We also compared to pre-training only on unfiltered data from C4. So rather than doing all", "tokens": [492, 611, 5347, 281, 659, 12, 17227, 1760, 787, 322, 3971, 388, 40665, 1412, 490, 383, 19, 13, 407, 2831, 813, 884, 439], "temperature": 0.0, "avg_logprob": -0.09373711744944255, "compression_ratio": 1.8098859315589353, "no_speech_prob": 8.748211985221133e-05}, {"id": 313, "seek": 169384, "start": 1699.36, "end": 1703.9199999999998, "text": " these heuristic filtering steps, we just take the raw web extracted text from C4 and pre-training", "tokens": [613, 415, 374, 3142, 30822, 4439, 11, 321, 445, 747, 264, 8936, 3670, 34086, 2487, 490, 383, 19, 293, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.09373711744944255, "compression_ratio": 1.8098859315589353, "no_speech_prob": 8.748211985221133e-05}, {"id": 314, "seek": 169384, "start": 1703.9199999999998, "end": 1708.8, "text": " on that. And you can see that that does uniformly worse. So it does seem to be true that these", "tokens": [322, 300, 13, 400, 291, 393, 536, 300, 300, 775, 48806, 5324, 13, 407, 309, 775, 1643, 281, 312, 2074, 300, 613], "temperature": 0.0, "avg_logprob": -0.09373711744944255, "compression_ratio": 1.8098859315589353, "no_speech_prob": 8.748211985221133e-05}, {"id": 315, "seek": 169384, "start": 1708.8, "end": 1713.52, "text": " cleaning steps that we're doing are actually useful. The next four data sets were our attempt to", "tokens": [8924, 4439, 300, 321, 434, 884, 366, 767, 4420, 13, 440, 958, 1451, 1412, 6352, 645, 527, 5217, 281], "temperature": 0.0, "avg_logprob": -0.09373711744944255, "compression_ratio": 1.8098859315589353, "no_speech_prob": 8.748211985221133e-05}, {"id": 316, "seek": 169384, "start": 1714.1599999999999, "end": 1718.72, "text": " pre-training similar data sets that had been used in password. The real news data set came from", "tokens": [659, 12, 17227, 1760, 2531, 1412, 6352, 300, 632, 668, 1143, 294, 11524, 13, 440, 957, 2583, 1412, 992, 1361, 490], "temperature": 0.0, "avg_logprob": -0.09373711744944255, "compression_ratio": 1.8098859315589353, "no_speech_prob": 8.748211985221133e-05}, {"id": 317, "seek": 171872, "start": 1718.72, "end": 1724.72, "text": " the Grover paper. It's essentially pre-training only on data from news sites. Web text is this", "tokens": [264, 12981, 331, 3035, 13, 467, 311, 4476, 659, 12, 17227, 1760, 787, 322, 1412, 490, 2583, 7533, 13, 9573, 2487, 307, 341], "temperature": 0.0, "avg_logprob": -0.15456303528376988, "compression_ratio": 1.65625, "no_speech_prob": 4.133093170821667e-05}, {"id": 318, "seek": 171872, "start": 1724.72, "end": 1730.4, "text": " data set that we used in the GPT2 paper where you only train on Web text that was linked to and", "tokens": [1412, 992, 300, 321, 1143, 294, 264, 26039, 51, 17, 3035, 689, 291, 787, 3847, 322, 9573, 2487, 300, 390, 9408, 281, 293], "temperature": 0.0, "avg_logprob": -0.15456303528376988, "compression_ratio": 1.65625, "no_speech_prob": 4.133093170821667e-05}, {"id": 319, "seek": 171872, "start": 1730.4, "end": 1736.0, "text": " received a reasonably high score on Reddit. And then the last two variants are either Wikipedia", "tokens": [4613, 257, 23551, 1090, 6175, 322, 32210, 13, 400, 550, 264, 1036, 732, 21669, 366, 2139, 28999], "temperature": 0.0, "avg_logprob": -0.15456303528376988, "compression_ratio": 1.65625, "no_speech_prob": 4.133093170821667e-05}, {"id": 320, "seek": 171872, "start": 1736.0, "end": 1741.04, "text": " alone or as was used in the in the birth paper Wikipedia with the Toronto Books Corpus.", "tokens": [3312, 420, 382, 390, 1143, 294, 264, 294, 264, 3965, 3035, 28999, 365, 264, 14140, 33843, 3925, 31624, 13], "temperature": 0.0, "avg_logprob": -0.15456303528376988, "compression_ratio": 1.65625, "no_speech_prob": 4.133093170821667e-05}, {"id": 321, "seek": 171872, "start": 1742.0, "end": 1748.16, "text": " And you might actually notice that some of these more specialized data sets we get better performance.", "tokens": [400, 291, 1062, 767, 3449, 300, 512, 295, 613, 544, 19813, 1412, 6352, 321, 483, 1101, 3389, 13], "temperature": 0.0, "avg_logprob": -0.15456303528376988, "compression_ratio": 1.65625, "no_speech_prob": 4.133093170821667e-05}, {"id": 322, "seek": 174816, "start": 1748.16, "end": 1753.3600000000001, "text": " So for example, you can see that on the Wikipedia and Toronto Books Corpus on the bottom row,", "tokens": [407, 337, 1365, 11, 291, 393, 536, 300, 322, 264, 28999, 293, 14140, 33843, 3925, 31624, 322, 264, 2767, 5386, 11], "temperature": 0.0, "avg_logprob": -0.21163801193237305, "compression_ratio": 1.56198347107438, "no_speech_prob": 2.668337765499018e-05}, {"id": 323, "seek": 174816, "start": 1753.3600000000001, "end": 1759.2, "text": " we actually do much better on superglue with a score of a little over 73 compared to pre-training", "tokens": [321, 767, 360, 709, 1101, 322, 1687, 7191, 622, 365, 257, 6175, 295, 257, 707, 670, 28387, 5347, 281, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.21163801193237305, "compression_ratio": 1.56198347107438, "no_speech_prob": 2.668337765499018e-05}, {"id": 324, "seek": 174816, "start": 1759.2, "end": 1765.92, "text": " on C4. And it turns out this is because or we we conjecture that this is because superglue", "tokens": [322, 383, 19, 13, 400, 309, 4523, 484, 341, 307, 570, 420, 321, 321, 416, 1020, 540, 300, 341, 307, 570, 1687, 7191, 622], "temperature": 0.0, "avg_logprob": -0.21163801193237305, "compression_ratio": 1.56198347107438, "no_speech_prob": 2.668337765499018e-05}, {"id": 325, "seek": 174816, "start": 1765.92, "end": 1773.0400000000002, "text": " contains a task called multi-RC, which is a reading comprehension task on on on Wicc news sorry", "tokens": [8306, 257, 5633, 1219, 4825, 12, 28437, 11, 597, 307, 257, 3760, 44991, 5633, 322, 322, 322, 343, 299, 66, 2583, 2597], "temperature": 0.0, "avg_logprob": -0.21163801193237305, "compression_ratio": 1.56198347107438, "no_speech_prob": 2.668337765499018e-05}, {"id": 326, "seek": 177304, "start": 1773.04, "end": 1778.8799999999999, "text": " and cyclopedia articles and on novels. So the basic takeaway here is that when you pre-training", "tokens": [293, 19474, 47795, 11290, 293, 322, 24574, 13, 407, 264, 3875, 30681, 510, 307, 300, 562, 291, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.13640184483976445, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.005641312687658e-05}, {"id": 327, "seek": 177304, "start": 1779.76, "end": 1784.48, "text": " data that's similar to your downstream task that has a similar domain, you often get a big boost", "tokens": [1412, 300, 311, 2531, 281, 428, 30621, 5633, 300, 575, 257, 2531, 9274, 11, 291, 2049, 483, 257, 955, 9194], "temperature": 0.0, "avg_logprob": -0.13640184483976445, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.005641312687658e-05}, {"id": 328, "seek": 177304, "start": 1784.48, "end": 1789.44, "text": " in that downstream task and that's indeed what happened here. Interestingly, you can also see the", "tokens": [294, 300, 30621, 5633, 293, 300, 311, 6451, 437, 2011, 510, 13, 30564, 11, 291, 393, 611, 536, 264], "temperature": 0.0, "avg_logprob": -0.13640184483976445, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.005641312687658e-05}, {"id": 329, "seek": 177304, "start": 1789.44, "end": 1795.6, "text": " opposite effect. So if you if you look on Wikipedia on the second to last row, if you only pre-training", "tokens": [6182, 1802, 13, 407, 498, 291, 498, 291, 574, 322, 28999, 322, 264, 1150, 281, 1036, 5386, 11, 498, 291, 787, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.13640184483976445, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.005641312687658e-05}, {"id": 330, "seek": 177304, "start": 1795.6, "end": 1800.48, "text": " Wikipedia, you end up doing much worse on cola, which is the corpus of linguistic acceptability", "tokens": [28999, 11, 291, 917, 493, 884, 709, 5324, 322, 40495, 11, 597, 307, 264, 1181, 31624, 295, 43002, 3241, 2310], "temperature": 0.0, "avg_logprob": -0.13640184483976445, "compression_ratio": 1.7818181818181817, "no_speech_prob": 4.005641312687658e-05}, {"id": 331, "seek": 180048, "start": 1800.48, "end": 1806.0, "text": " tasks that I mentioned early on. And we conjecture that this is because on Wikipedia has very little", "tokens": [9608, 300, 286, 2835, 2440, 322, 13, 400, 321, 416, 1020, 540, 300, 341, 307, 570, 322, 28999, 575, 588, 707], "temperature": 0.0, "avg_logprob": -0.0979129774817105, "compression_ratio": 1.6724738675958188, "no_speech_prob": 4.985113992006518e-05}, {"id": 332, "seek": 180048, "start": 1806.0, "end": 1811.04, "text": " unacceptable text. You're basically only pre-training on clean text, whereas C4 has some", "tokens": [31812, 2487, 13, 509, 434, 1936, 787, 659, 12, 17227, 1760, 322, 2541, 2487, 11, 9735, 383, 19, 575, 512], "temperature": 0.0, "avg_logprob": -0.0979129774817105, "compression_ratio": 1.6724738675958188, "no_speech_prob": 4.985113992006518e-05}, {"id": 333, "seek": 180048, "start": 1811.04, "end": 1815.2, "text": " ungrammatical texts, some nonsense in it. And so that actually can boost your performance a little", "tokens": [517, 1342, 15677, 804, 15765, 11, 512, 14925, 294, 309, 13, 400, 370, 300, 767, 393, 9194, 428, 3389, 257, 707], "temperature": 0.0, "avg_logprob": -0.0979129774817105, "compression_ratio": 1.6724738675958188, "no_speech_prob": 4.985113992006518e-05}, {"id": 334, "seek": 180048, "start": 1815.2, "end": 1820.08, "text": " bit on cola. The last thing to note is that while you do see some gain sometimes on using these", "tokens": [857, 322, 40495, 13, 440, 1036, 551, 281, 3637, 307, 300, 1339, 291, 360, 536, 512, 6052, 2171, 322, 1228, 613], "temperature": 0.0, "avg_logprob": -0.0979129774817105, "compression_ratio": 1.6724738675958188, "no_speech_prob": 4.985113992006518e-05}, {"id": 335, "seek": 180048, "start": 1820.08, "end": 1825.2, "text": " smaller data sets, these data sets are about an order of magnitude smaller than C4. So then the", "tokens": [4356, 1412, 6352, 11, 613, 1412, 6352, 366, 466, 364, 1668, 295, 15668, 4356, 813, 383, 19, 13, 407, 550, 264], "temperature": 0.0, "avg_logprob": -0.0979129774817105, "compression_ratio": 1.6724738675958188, "no_speech_prob": 4.985113992006518e-05}, {"id": 336, "seek": 182520, "start": 1825.2, "end": 1830.64, "text": " natural question is does it actually hurt you to pre-traine on a smaller data set? So to answer", "tokens": [3303, 1168, 307, 775, 309, 767, 4607, 291, 281, 659, 12, 17227, 533, 322, 257, 4356, 1412, 992, 30, 407, 281, 1867], "temperature": 0.0, "avg_logprob": -0.06877823365040314, "compression_ratio": 1.7462686567164178, "no_speech_prob": 3.480300074443221e-05}, {"id": 337, "seek": 182520, "start": 1830.64, "end": 1836.24, "text": " that question, what we did is basically took C4 and artificially made it smaller so that it was", "tokens": [300, 1168, 11, 437, 321, 630, 307, 1936, 1890, 383, 19, 293, 39905, 2270, 1027, 309, 4356, 370, 300, 309, 390], "temperature": 0.0, "avg_logprob": -0.06877823365040314, "compression_ratio": 1.7462686567164178, "no_speech_prob": 3.480300074443221e-05}, {"id": 338, "seek": 182520, "start": 1836.24, "end": 1842.48, "text": " repeated over the course of pre-training. And you can see here that when you repeat the data set 64", "tokens": [10477, 670, 264, 1164, 295, 659, 12, 17227, 1760, 13, 400, 291, 393, 536, 510, 300, 562, 291, 7149, 264, 1412, 992, 12145], "temperature": 0.0, "avg_logprob": -0.06877823365040314, "compression_ratio": 1.7462686567164178, "no_speech_prob": 3.480300074443221e-05}, {"id": 339, "seek": 182520, "start": 1842.48, "end": 1848.56, "text": " times, so it's 34 billion divided by 64 tokens, because that's how much pre-training we did,", "tokens": [1413, 11, 370, 309, 311, 12790, 5218, 6666, 538, 12145, 22667, 11, 570, 300, 311, 577, 709, 659, 12, 17227, 1760, 321, 630, 11], "temperature": 0.0, "avg_logprob": -0.06877823365040314, "compression_ratio": 1.7462686567164178, "no_speech_prob": 3.480300074443221e-05}, {"id": 340, "seek": 182520, "start": 1849.6000000000001, "end": 1853.76, "text": " you actually don't sacrifice much performance. The performance is roughly the same.", "tokens": [291, 767, 500, 380, 11521, 709, 3389, 13, 440, 3389, 307, 9810, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.06877823365040314, "compression_ratio": 1.7462686567164178, "no_speech_prob": 3.480300074443221e-05}, {"id": 341, "seek": 185376, "start": 1853.76, "end": 1860.64, "text": " But if you repeat the data set 256 times, 1024 times or more, you actually start to see degradation.", "tokens": [583, 498, 291, 7149, 264, 1412, 992, 38882, 1413, 11, 1266, 7911, 1413, 420, 544, 11, 291, 767, 722, 281, 536, 40519, 13], "temperature": 0.0, "avg_logprob": -0.09740200363287405, "compression_ratio": 1.7703703703703704, "no_speech_prob": 9.607082756701857e-05}, {"id": 342, "seek": 185376, "start": 1860.64, "end": 1864.56, "text": " And the reason that we think this is happening is because you're basically overfitting during", "tokens": [400, 264, 1778, 300, 321, 519, 341, 307, 2737, 307, 570, 291, 434, 1936, 670, 69, 2414, 1830], "temperature": 0.0, "avg_logprob": -0.09740200363287405, "compression_ratio": 1.7703703703703704, "no_speech_prob": 9.607082756701857e-05}, {"id": 343, "seek": 185376, "start": 1864.56, "end": 1869.2, "text": " pre-training. And you can get a sense for whether that's true or not by looking, just looking at", "tokens": [659, 12, 17227, 1760, 13, 400, 291, 393, 483, 257, 2020, 337, 1968, 300, 311, 2074, 420, 406, 538, 1237, 11, 445, 1237, 412], "temperature": 0.0, "avg_logprob": -0.09740200363287405, "compression_ratio": 1.7703703703703704, "no_speech_prob": 9.607082756701857e-05}, {"id": 344, "seek": 185376, "start": 1869.2, "end": 1873.52, "text": " the training loss. You can see that the model attains a much, much smaller training loss as", "tokens": [264, 3097, 4470, 13, 509, 393, 536, 300, 264, 2316, 951, 2315, 257, 709, 11, 709, 4356, 3097, 4470, 382], "temperature": 0.0, "avg_logprob": -0.09740200363287405, "compression_ratio": 1.7703703703703704, "no_speech_prob": 9.607082756701857e-05}, {"id": 345, "seek": 185376, "start": 1873.52, "end": 1878.0, "text": " repeat the data set more and more times. So the upshot of this is that your data set should be", "tokens": [7149, 264, 1412, 992, 544, 293, 544, 1413, 13, 407, 264, 493, 18402, 295, 341, 307, 300, 428, 1412, 992, 820, 312], "temperature": 0.0, "avg_logprob": -0.09740200363287405, "compression_ratio": 1.7703703703703704, "no_speech_prob": 9.607082756701857e-05}, {"id": 346, "seek": 187800, "start": 1878.0, "end": 1884.8, "text": " at least as big that you don't see significant overfitting during pre-training. And later on,", "tokens": [412, 1935, 382, 955, 300, 291, 500, 380, 536, 4776, 670, 69, 2414, 1830, 659, 12, 17227, 1760, 13, 400, 1780, 322, 11], "temperature": 0.0, "avg_logprob": -0.08948475249270176, "compression_ratio": 1.6176470588235294, "no_speech_prob": 1.3630559806188103e-05}, {"id": 347, "seek": 187800, "start": 1884.8, "end": 1890.32, "text": " when we scale up these models and pre-training them on much more data, we would do enough repeats", "tokens": [562, 321, 4373, 493, 613, 5245, 293, 659, 12, 17227, 1760, 552, 322, 709, 544, 1412, 11, 321, 576, 360, 1547, 35038], "temperature": 0.0, "avg_logprob": -0.08948475249270176, "compression_ratio": 1.6176470588235294, "no_speech_prob": 1.3630559806188103e-05}, {"id": 348, "seek": 187800, "start": 1890.32, "end": 1895.84, "text": " of the smaller sort of more domain specific data sets that we imagine we would see harmful effects.", "tokens": [295, 264, 4356, 1333, 295, 544, 9274, 2685, 1412, 6352, 300, 321, 3811, 321, 576, 536, 19727, 5065, 13], "temperature": 0.0, "avg_logprob": -0.08948475249270176, "compression_ratio": 1.6176470588235294, "no_speech_prob": 1.3630559806188103e-05}, {"id": 349, "seek": 187800, "start": 1898.16, "end": 1903.44, "text": " The next thing we experimented with were multi-task learning strategies. So when you're doing", "tokens": [440, 958, 551, 321, 5120, 292, 365, 645, 4825, 12, 83, 3863, 2539, 9029, 13, 407, 562, 291, 434, 884], "temperature": 0.0, "avg_logprob": -0.08948475249270176, "compression_ratio": 1.6176470588235294, "no_speech_prob": 1.3630559806188103e-05}, {"id": 350, "seek": 190344, "start": 1903.44, "end": 1909.2, "text": " multi-task learning, you're essentially training the model on multiple tasks at once. And in most of", "tokens": [4825, 12, 83, 3863, 2539, 11, 291, 434, 4476, 3097, 264, 2316, 322, 3866, 9608, 412, 1564, 13, 400, 294, 881, 295], "temperature": 0.0, "avg_logprob": -0.09885266621907553, "compression_ratio": 1.8154981549815499, "no_speech_prob": 5.0638813263503835e-05}, {"id": 351, "seek": 190344, "start": 1909.2, "end": 1914.96, "text": " the, in all the experiments I'm showing on this slide here, we're actually training on every single", "tokens": [264, 11, 294, 439, 264, 12050, 286, 478, 4099, 322, 341, 4137, 510, 11, 321, 434, 767, 3097, 322, 633, 2167], "temperature": 0.0, "avg_logprob": -0.09885266621907553, "compression_ratio": 1.8154981549815499, "no_speech_prob": 5.0638813263503835e-05}, {"id": 352, "seek": 190344, "start": 1914.96, "end": 1920.0800000000002, "text": " task at once. So the pre-training task and all of the downstream tasks together. And the most", "tokens": [5633, 412, 1564, 13, 407, 264, 659, 12, 17227, 1760, 5633, 293, 439, 295, 264, 30621, 9608, 1214, 13, 400, 264, 881], "temperature": 0.0, "avg_logprob": -0.09885266621907553, "compression_ratio": 1.8154981549815499, "no_speech_prob": 5.0638813263503835e-05}, {"id": 353, "seek": 190344, "start": 1920.0800000000002, "end": 1924.96, "text": " pertinent question when you're doing multi-task training like this is how often should I sample data", "tokens": [13269, 11058, 1168, 562, 291, 434, 884, 4825, 12, 83, 3863, 3097, 411, 341, 307, 577, 2049, 820, 286, 6889, 1412], "temperature": 0.0, "avg_logprob": -0.09885266621907553, "compression_ratio": 1.8154981549815499, "no_speech_prob": 5.0638813263503835e-05}, {"id": 354, "seek": 190344, "start": 1924.96, "end": 1931.52, "text": " from each task? So one approach is just to sample data at an equal rate across all of the tasks.", "tokens": [490, 1184, 5633, 30, 407, 472, 3109, 307, 445, 281, 6889, 1412, 412, 364, 2681, 3314, 2108, 439, 295, 264, 9608, 13], "temperature": 0.0, "avg_logprob": -0.09885266621907553, "compression_ratio": 1.8154981549815499, "no_speech_prob": 5.0638813263503835e-05}, {"id": 355, "seek": 193152, "start": 1931.52, "end": 1938.0, "text": " Another case is to basically pretend like you just concatenated all the data sets. We call that", "tokens": [3996, 1389, 307, 281, 1936, 11865, 411, 291, 445, 1588, 7186, 770, 439, 264, 1412, 6352, 13, 492, 818, 300], "temperature": 0.0, "avg_logprob": -0.11636897250338718, "compression_ratio": 1.800711743772242, "no_speech_prob": 6.81413512211293e-05}, {"id": 356, "seek": 193152, "start": 1938.0, "end": 1942.56, "text": " examples proportional mixing because it's equivalent to sampling from the data set in accordance to", "tokens": [5110, 24969, 11983, 570, 309, 311, 10344, 281, 21179, 490, 264, 1412, 992, 294, 31110, 281], "temperature": 0.0, "avg_logprob": -0.11636897250338718, "compression_ratio": 1.800711743772242, "no_speech_prob": 6.81413512211293e-05}, {"id": 357, "seek": 193152, "start": 1942.56, "end": 1947.2, "text": " how many examples there are in the data set. The difficult thing with that though is that our pre-training", "tokens": [577, 867, 5110, 456, 366, 294, 264, 1412, 992, 13, 440, 2252, 551, 365, 300, 1673, 307, 300, 527, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.11636897250338718, "compression_ratio": 1.800711743772242, "no_speech_prob": 6.81413512211293e-05}, {"id": 358, "seek": 193152, "start": 1947.2, "end": 1952.8, "text": " data set is so big that its proportion would be much, much, much bigger than every downstream task.", "tokens": [1412, 992, 307, 370, 955, 300, 1080, 16068, 576, 312, 709, 11, 709, 11, 709, 3801, 813, 633, 30621, 5633, 13], "temperature": 0.0, "avg_logprob": -0.11636897250338718, "compression_ratio": 1.800711743772242, "no_speech_prob": 6.81413512211293e-05}, {"id": 359, "seek": 193152, "start": 1952.8, "end": 1958.32, "text": " And we basically would never train on any of the downstream data. So we introduced this hyper parameter", "tokens": [400, 321, 1936, 576, 1128, 3847, 322, 604, 295, 264, 30621, 1412, 13, 407, 321, 7268, 341, 9848, 13075], "temperature": 0.0, "avg_logprob": -0.11636897250338718, "compression_ratio": 1.800711743772242, "no_speech_prob": 6.81413512211293e-05}, {"id": 360, "seek": 195832, "start": 1958.32, "end": 1964.8, "text": " K, which is a constant that basically is how big should we pretend that the pre-training data set is.", "tokens": [591, 11, 597, 307, 257, 5754, 300, 1936, 307, 577, 955, 820, 321, 11865, 300, 264, 659, 12, 17227, 1760, 1412, 992, 307, 13], "temperature": 0.0, "avg_logprob": -0.0714565135063009, "compression_ratio": 1.7123893805309736, "no_speech_prob": 6.502385076601058e-05}, {"id": 361, "seek": 195832, "start": 1965.6799999999998, "end": 1971.12, "text": " The last thing you can do is take the number of examples in each data set and scale it by a", "tokens": [440, 1036, 551, 291, 393, 360, 307, 747, 264, 1230, 295, 5110, 294, 1184, 1412, 992, 293, 4373, 309, 538, 257], "temperature": 0.0, "avg_logprob": -0.0714565135063009, "compression_ratio": 1.7123893805309736, "no_speech_prob": 6.502385076601058e-05}, {"id": 362, "seek": 195832, "start": 1971.12, "end": 1976.8, "text": " temperature. The larger the temperature, the closer you get to equal mixing to uniform sampling", "tokens": [4292, 13, 440, 4833, 264, 4292, 11, 264, 4966, 291, 483, 281, 2681, 11983, 281, 9452, 21179], "temperature": 0.0, "avg_logprob": -0.0714565135063009, "compression_ratio": 1.7123893805309736, "no_speech_prob": 6.502385076601058e-05}, {"id": 363, "seek": 195832, "start": 1976.8, "end": 1984.48, "text": " from each data set. But at any rate, the main takeaway from this table is that you can get pretty", "tokens": [490, 1184, 1412, 992, 13, 583, 412, 604, 3314, 11, 264, 2135, 30681, 490, 341, 3199, 307, 300, 291, 393, 483, 1238], "temperature": 0.0, "avg_logprob": -0.0714565135063009, "compression_ratio": 1.7123893805309736, "no_speech_prob": 6.502385076601058e-05}, {"id": 364, "seek": 198448, "start": 1984.48, "end": 1989.84, "text": " close to the performance of separate pre-training and fine-tuning like we do on our baseline", "tokens": [1998, 281, 264, 3389, 295, 4994, 659, 12, 17227, 1760, 293, 2489, 12, 83, 37726, 411, 321, 360, 322, 527, 20518], "temperature": 0.0, "avg_logprob": -0.16710428091195914, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.00012528833758551627}, {"id": 365, "seek": 198448, "start": 1989.84, "end": 1994.64, "text": " if you get the mixing strategy right. But ultimately, we found that you do tend to sacrifice", "tokens": [498, 291, 483, 264, 11983, 5206, 558, 13, 583, 6284, 11, 321, 1352, 300, 291, 360, 3928, 281, 11521], "temperature": 0.0, "avg_logprob": -0.16710428091195914, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.00012528833758551627}, {"id": 366, "seek": 198448, "start": 1994.64, "end": 1998.72, "text": " some performance when doing multi-task trading and at least some of the tasks.", "tokens": [512, 3389, 562, 884, 4825, 12, 83, 3863, 9529, 293, 412, 1935, 512, 295, 264, 9608, 13], "temperature": 0.0, "avg_logprob": -0.16710428091195914, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.00012528833758551627}, {"id": 367, "seek": 198448, "start": 1999.28, "end": 2003.84, "text": " Collin, there were lots of questions back on the choice of on the slide with the different", "tokens": [4586, 259, 11, 456, 645, 3195, 295, 1651, 646, 322, 264, 3922, 295, 322, 264, 4137, 365, 264, 819], "temperature": 0.0, "avg_logprob": -0.16710428091195914, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.00012528833758551627}, {"id": 368, "seek": 198448, "start": 2003.84, "end": 2008.24, "text": " data sets, one showing real news and C4 and so on. Sure.", "tokens": [1412, 6352, 11, 472, 4099, 957, 2583, 293, 383, 19, 293, 370, 322, 13, 4894, 13], "temperature": 0.0, "avg_logprob": -0.16710428091195914, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.00012528833758551627}, {"id": 369, "seek": 200824, "start": 2008.24, "end": 2016.0, "text": " Everyone take a couple. Absolutely. Yeah, so firstly, if you just look at this, it still does kind of", "tokens": [5198, 747, 257, 1916, 13, 7021, 13, 865, 11, 370, 27376, 11, 498, 291, 445, 574, 412, 341, 11, 309, 920, 775, 733, 295], "temperature": 0.0, "avg_logprob": -0.17488872210184733, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0001214299481944181}, {"id": 370, "seek": 200824, "start": 2016.0, "end": 2024.48, "text": " look like you can get great results with more than an order of magnitude, less text, and C4.", "tokens": [574, 411, 291, 393, 483, 869, 3542, 365, 544, 813, 364, 1668, 295, 15668, 11, 1570, 2487, 11, 293, 383, 19, 13], "temperature": 0.0, "avg_logprob": -0.17488872210184733, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0001214299481944181}, {"id": 371, "seek": 200824, "start": 2024.48, "end": 2028.32, "text": " Yeah. And it seemed like that's not the message you wanted to be putting forward.", "tokens": [865, 13, 400, 309, 6576, 411, 300, 311, 406, 264, 3636, 291, 1415, 281, 312, 3372, 2128, 13], "temperature": 0.0, "avg_logprob": -0.17488872210184733, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0001214299481944181}, {"id": 372, "seek": 200824, "start": 2029.2, "end": 2033.36, "text": " Yeah. So there's a little nuance here, which is basically that in our baseline, in these", "tokens": [865, 13, 407, 456, 311, 257, 707, 42625, 510, 11, 597, 307, 1936, 300, 294, 527, 20518, 11, 294, 613], "temperature": 0.0, "avg_logprob": -0.17488872210184733, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0001214299481944181}, {"id": 373, "seek": 200824, "start": 2033.36, "end": 2038.0, "text": " experiments that I've been running so far, we're actually not pre-training for that long. So as I", "tokens": [12050, 300, 286, 600, 668, 2614, 370, 1400, 11, 321, 434, 767, 406, 659, 12, 17227, 1760, 337, 300, 938, 13, 407, 382, 286], "temperature": 0.0, "avg_logprob": -0.17488872210184733, "compression_ratio": 1.602076124567474, "no_speech_prob": 0.0001214299481944181}, {"id": 374, "seek": 203800, "start": 2038.0, "end": 2043.36, "text": " mentioned earlier, we're actually pre-training for a quarter as long as BERT. And actually for us,", "tokens": [2835, 3071, 11, 321, 434, 767, 659, 12, 17227, 1760, 337, 257, 6555, 382, 938, 382, 363, 31479, 13, 400, 767, 337, 505, 11], "temperature": 0.0, "avg_logprob": -0.17869804799556732, "compression_ratio": 1.6920289855072463, "no_speech_prob": 3.6469584301812574e-05}, {"id": 375, "seek": 203800, "start": 2043.36, "end": 2052.96, "text": " I believe, one 256th as long as Excel net, for example. So and when later in the paper,", "tokens": [286, 1697, 11, 472, 3552, 21, 392, 382, 938, 382, 19060, 2533, 11, 337, 1365, 13, 407, 293, 562, 1780, 294, 264, 3035, 11], "temperature": 0.0, "avg_logprob": -0.17869804799556732, "compression_ratio": 1.6920289855072463, "no_speech_prob": 3.6469584301812574e-05}, {"id": 376, "seek": 203800, "start": 2052.96, "end": 2057.12, "text": " we're going to pre-traine for much, much, much longer. And in that case, we would end up", "tokens": [321, 434, 516, 281, 659, 12, 17227, 533, 337, 709, 11, 709, 11, 709, 2854, 13, 400, 294, 300, 1389, 11, 321, 576, 917, 493], "temperature": 0.0, "avg_logprob": -0.17869804799556732, "compression_ratio": 1.6920289855072463, "no_speech_prob": 3.6469584301812574e-05}, {"id": 377, "seek": 203800, "start": 2057.12, "end": 2061.6, "text": " repeating these data sets many, many times over the course of pre-training and we'd start to see", "tokens": [18617, 613, 1412, 6352, 867, 11, 867, 1413, 670, 264, 1164, 295, 659, 12, 17227, 1760, 293, 321, 1116, 722, 281, 536], "temperature": 0.0, "avg_logprob": -0.17869804799556732, "compression_ratio": 1.6920289855072463, "no_speech_prob": 3.6469584301812574e-05}, {"id": 378, "seek": 203800, "start": 2061.6, "end": 2065.12, "text": " these negative effects that I explained on the next slide. Oh, so that's why you're then doing", "tokens": [613, 3671, 5065, 300, 286, 8825, 322, 264, 958, 4137, 13, 876, 11, 370, 300, 311, 983, 291, 434, 550, 884], "temperature": 0.0, "avg_logprob": -0.17869804799556732, "compression_ratio": 1.6920289855072463, "no_speech_prob": 3.6469584301812574e-05}, {"id": 379, "seek": 206512, "start": 2065.12, "end": 2069.8399999999997, "text": " the repeat. Okay, so when they're asked about that as to why you train on the same stuff over and", "tokens": [264, 7149, 13, 1033, 11, 370, 562, 436, 434, 2351, 466, 300, 382, 281, 983, 291, 3847, 322, 264, 912, 1507, 670, 293], "temperature": 0.0, "avg_logprob": -0.27568478333322627, "compression_ratio": 1.5104166666666667, "no_speech_prob": 3.7043784686829895e-05}, {"id": 380, "seek": 206512, "start": 2069.8399999999997, "end": 2078.48, "text": " over again, that's the test set. Yeah. Okay, exactly. And then on the data sets, so to a first", "tokens": [670, 797, 11, 300, 311, 264, 1500, 992, 13, 865, 13, 1033, 11, 2293, 13, 400, 550, 322, 264, 1412, 6352, 11, 370, 281, 257, 700], "temperature": 0.0, "avg_logprob": -0.27568478333322627, "compression_ratio": 1.5104166666666667, "no_speech_prob": 3.7043784686829895e-05}, {"id": 381, "seek": 206512, "start": 2078.48, "end": 2086.24, "text": " approximation, the C4 contain Wikipedia or very partially many pages of Wikipedia, but not all of", "tokens": [28023, 11, 264, 383, 19, 5304, 28999, 420, 588, 18886, 867, 7183, 295, 28999, 11, 457, 406, 439, 295], "temperature": 0.0, "avg_logprob": -0.27568478333322627, "compression_ratio": 1.5104166666666667, "no_speech_prob": 3.7043784686829895e-05}, {"id": 382, "seek": 208624, "start": 2086.24, "end": 2096.3999999999996, "text": " Wikipedia. Common crawl is done by like is a sort of web crawl by following links at some priority.", "tokens": [28999, 13, 18235, 24767, 307, 1096, 538, 411, 307, 257, 1333, 295, 3670, 24767, 538, 3480, 6123, 412, 512, 9365, 13], "temperature": 0.0, "avg_logprob": -0.12905371707418692, "compression_ratio": 1.6566523605150214, "no_speech_prob": 3.535076029947959e-05}, {"id": 383, "seek": 208624, "start": 2096.3999999999996, "end": 2100.9599999999996, "text": " And ultimately, it didn't cover all of Wikipedia. I don't actually know the exact proportion of", "tokens": [400, 6284, 11, 309, 994, 380, 2060, 439, 295, 28999, 13, 286, 500, 380, 767, 458, 264, 1900, 16068, 295], "temperature": 0.0, "avg_logprob": -0.12905371707418692, "compression_ratio": 1.6566523605150214, "no_speech_prob": 3.535076029947959e-05}, {"id": 384, "seek": 208624, "start": 2100.9599999999996, "end": 2108.3999999999996, "text": " Wikipedia that's included in C4, but definitely when training on C4, you will see some Wikipedia text.", "tokens": [28999, 300, 311, 5556, 294, 383, 19, 11, 457, 2138, 562, 3097, 322, 383, 19, 11, 291, 486, 536, 512, 28999, 2487, 13], "temperature": 0.0, "avg_logprob": -0.12905371707418692, "compression_ratio": 1.6566523605150214, "no_speech_prob": 3.535076029947959e-05}, {"id": 385, "seek": 208624, "start": 2108.9599999999996, "end": 2113.68, "text": " It will be at a relatively low proportion compared to all of the other data that you'll", "tokens": [467, 486, 312, 412, 257, 7226, 2295, 16068, 5347, 281, 439, 295, 264, 661, 1412, 300, 291, 603], "temperature": 0.0, "avg_logprob": -0.12905371707418692, "compression_ratio": 1.6566523605150214, "no_speech_prob": 3.535076029947959e-05}, {"id": 386, "seek": 211368, "start": 2113.68, "end": 2121.6, "text": " see. Sure. And then someone wasn't quite convinced with your argument that the good quality of", "tokens": [536, 13, 4894, 13, 400, 550, 1580, 2067, 380, 1596, 12561, 365, 428, 6770, 300, 264, 665, 3125, 295], "temperature": 0.0, "avg_logprob": -0.1819313872944225, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.611207158537582e-05}, {"id": 387, "seek": 211368, "start": 2121.6, "end": 2129.9199999999996, "text": " Wikipedia explained the worst performance on Cola because they thought, well, surely real news", "tokens": [28999, 8825, 264, 5855, 3389, 322, 48037, 570, 436, 1194, 11, 731, 11, 11468, 957, 2583], "temperature": 0.0, "avg_logprob": -0.1819313872944225, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.611207158537582e-05}, {"id": 388, "seek": 211368, "start": 2129.9199999999996, "end": 2134.3199999999997, "text": " that's basically well edited text as well. And yet it seems to work fine.", "tokens": [300, 311, 1936, 731, 23016, 2487, 382, 731, 13, 400, 1939, 309, 2544, 281, 589, 2489, 13], "temperature": 0.0, "avg_logprob": -0.1819313872944225, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.611207158537582e-05}, {"id": 389, "seek": 211368, "start": 2135.04, "end": 2139.6, "text": " Yeah, it's a good point. I'm not sure, you know, it could be that real news because real news has", "tokens": [865, 11, 309, 311, 257, 665, 935, 13, 286, 478, 406, 988, 11, 291, 458, 11, 309, 727, 312, 300, 957, 2583, 570, 957, 2583, 575], "temperature": 0.0, "avg_logprob": -0.1819313872944225, "compression_ratio": 1.5833333333333333, "no_speech_prob": 8.611207158537582e-05}, {"id": 390, "seek": 213960, "start": 2139.6, "end": 2145.2799999999997, "text": " cloaks in it or maybe real news ended up having some content from the common sections of sites.", "tokens": [20123, 5461, 294, 309, 420, 1310, 957, 2583, 4590, 493, 1419, 512, 2701, 490, 264, 2689, 10863, 295, 7533, 13], "temperature": 0.0, "avg_logprob": -0.12449686980444538, "compression_ratio": 1.6898954703832754, "no_speech_prob": 6.603283691219985e-05}, {"id": 391, "seek": 213960, "start": 2145.8399999999997, "end": 2150.56, "text": " I should say that when I, the reason that they're real news like and web text like is that these", "tokens": [286, 820, 584, 300, 562, 286, 11, 264, 1778, 300, 436, 434, 957, 2583, 411, 293, 3670, 2487, 411, 307, 300, 613], "temperature": 0.0, "avg_logprob": -0.12449686980444538, "compression_ratio": 1.6898954703832754, "no_speech_prob": 6.603283691219985e-05}, {"id": 392, "seek": 213960, "start": 2150.56, "end": 2154.96, "text": " are our own reproductions of them so they might not be exactly the same as the originally proposed", "tokens": [366, 527, 1065, 11408, 3916, 295, 552, 370, 436, 1062, 406, 312, 2293, 264, 912, 382, 264, 7993, 10348], "temperature": 0.0, "avg_logprob": -0.12449686980444538, "compression_ratio": 1.6898954703832754, "no_speech_prob": 6.603283691219985e-05}, {"id": 393, "seek": 213960, "start": 2154.96, "end": 2161.52, "text": " variance because web text, for example, was never released. Yeah, but it's an interesting point.", "tokens": [21977, 570, 3670, 2487, 11, 337, 1365, 11, 390, 1128, 4736, 13, 865, 11, 457, 309, 311, 364, 1880, 935, 13], "temperature": 0.0, "avg_logprob": -0.12449686980444538, "compression_ratio": 1.6898954703832754, "no_speech_prob": 6.603283691219985e-05}, {"id": 394, "seek": 213960, "start": 2161.52, "end": 2165.7599999999998, "text": " And that's also why I would say that it's a conjecture. It's not, you know, not something that I", "tokens": [400, 300, 311, 611, 983, 286, 576, 584, 300, 309, 311, 257, 416, 1020, 540, 13, 467, 311, 406, 11, 291, 458, 11, 406, 746, 300, 286], "temperature": 0.0, "avg_logprob": -0.12449686980444538, "compression_ratio": 1.6898954703832754, "no_speech_prob": 6.603283691219985e-05}, {"id": 395, "seek": 216576, "start": 2165.76, "end": 2171.92, "text": " can make a rigorous claim about. Okay, maybe we should let you go on now. Great. Thanks. Yeah,", "tokens": [393, 652, 257, 29882, 3932, 466, 13, 1033, 11, 1310, 321, 820, 718, 291, 352, 322, 586, 13, 3769, 13, 2561, 13, 865, 11], "temperature": 0.0, "avg_logprob": -0.11596609751383463, "compression_ratio": 1.6, "no_speech_prob": 9.758371743373573e-05}, {"id": 396, "seek": 216576, "start": 2171.92, "end": 2177.84, "text": " thanks for those questions. So then the next thing after looking at these different multitask", "tokens": [3231, 337, 729, 1651, 13, 407, 550, 264, 958, 551, 934, 1237, 412, 613, 819, 42338, 3863], "temperature": 0.0, "avg_logprob": -0.11596609751383463, "compression_ratio": 1.6, "no_speech_prob": 9.758371743373573e-05}, {"id": 397, "seek": 216576, "start": 2177.84, "end": 2184.6400000000003, "text": " training strategies is to see if there's any way for us to close the gap between multitask training", "tokens": [3097, 9029, 307, 281, 536, 498, 456, 311, 604, 636, 337, 505, 281, 1998, 264, 7417, 1296, 42338, 3863, 3097], "temperature": 0.0, "avg_logprob": -0.11596609751383463, "compression_ratio": 1.6, "no_speech_prob": 9.758371743373573e-05}, {"id": 398, "seek": 216576, "start": 2184.6400000000003, "end": 2190.0800000000004, "text": " and this pre-training followed by separate fine tuning. And we experimented with many different", "tokens": [293, 341, 659, 12, 17227, 1760, 6263, 538, 4994, 2489, 15164, 13, 400, 321, 5120, 292, 365, 867, 819], "temperature": 0.0, "avg_logprob": -0.11596609751383463, "compression_ratio": 1.6, "no_speech_prob": 9.758371743373573e-05}, {"id": 399, "seek": 219008, "start": 2190.08, "end": 2195.84, "text": " strategies here, basically strict multitask training, doing multitask training followed by individual", "tokens": [9029, 510, 11, 1936, 10910, 42338, 3863, 3097, 11, 884, 42338, 3863, 3097, 6263, 538, 2609], "temperature": 0.0, "avg_logprob": -0.10436799893012413, "compression_ratio": 1.8650793650793651, "no_speech_prob": 6.501961615867913e-05}, {"id": 400, "seek": 219008, "start": 2195.84, "end": 2201.6, "text": " task fine tuning, doing multitask training, but without any unsupervised data. And really,", "tokens": [5633, 2489, 15164, 11, 884, 42338, 3863, 3097, 11, 457, 1553, 604, 2693, 12879, 24420, 1412, 13, 400, 534, 11], "temperature": 0.0, "avg_logprob": -0.10436799893012413, "compression_ratio": 1.8650793650793651, "no_speech_prob": 6.501961615867913e-05}, {"id": 401, "seek": 219008, "start": 2201.6, "end": 2207.52, "text": " the main takeaway from all of these experiments was that if you do the multitask training first,", "tokens": [264, 2135, 30681, 490, 439, 295, 613, 12050, 390, 300, 498, 291, 360, 264, 42338, 3863, 3097, 700, 11], "temperature": 0.0, "avg_logprob": -0.10436799893012413, "compression_ratio": 1.8650793650793651, "no_speech_prob": 6.501961615867913e-05}, {"id": 402, "seek": 219008, "start": 2207.52, "end": 2212.7999999999997, "text": " including the unsupervised task, and then you fine tune the model on each task separately,", "tokens": [3009, 264, 2693, 12879, 24420, 5633, 11, 293, 550, 291, 2489, 10864, 264, 2316, 322, 1184, 5633, 14759, 11], "temperature": 0.0, "avg_logprob": -0.10436799893012413, "compression_ratio": 1.8650793650793651, "no_speech_prob": 6.501961615867913e-05}, {"id": 403, "seek": 219008, "start": 2213.68, "end": 2217.6, "text": " which is the third row here, you actually don't really sacrifice much performance at all.", "tokens": [597, 307, 264, 2636, 5386, 510, 11, 291, 767, 500, 380, 534, 11521, 709, 3389, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.10436799893012413, "compression_ratio": 1.8650793650793651, "no_speech_prob": 6.501961615867913e-05}, {"id": 404, "seek": 221760, "start": 2217.6, "end": 2223.36, "text": " You are, you don't end up with a multitask model because you're fine tuning on each task individually.", "tokens": [509, 366, 11, 291, 500, 380, 917, 493, 365, 257, 42338, 3863, 2316, 570, 291, 434, 2489, 15164, 322, 1184, 5633, 16652, 13], "temperature": 0.0, "avg_logprob": -0.1346477579187464, "compression_ratio": 1.7768924302788844, "no_speech_prob": 4.984867337043397e-05}, {"id": 405, "seek": 221760, "start": 2223.36, "end": 2227.52, "text": " But the nice thing about this approach is that you can monitor the", "tokens": [583, 264, 1481, 551, 466, 341, 3109, 307, 300, 291, 393, 6002, 264], "temperature": 0.0, "avg_logprob": -0.1346477579187464, "compression_ratio": 1.7768924302788844, "no_speech_prob": 4.984867337043397e-05}, {"id": 406, "seek": 221760, "start": 2228.16, "end": 2233.68, "text": " performance on your downstream tasks while you're doing pre-training. And you don't sacrifice much", "tokens": [3389, 322, 428, 30621, 9608, 1339, 291, 434, 884, 659, 12, 17227, 1760, 13, 400, 291, 500, 380, 11521, 709], "temperature": 0.0, "avg_logprob": -0.1346477579187464, "compression_ratio": 1.7768924302788844, "no_speech_prob": 4.984867337043397e-05}, {"id": 407, "seek": 221760, "start": 2233.68, "end": 2240.48, "text": " performance. One setting that we didn't consider is the unsupervised pre-training followed by", "tokens": [3389, 13, 1485, 3287, 300, 321, 994, 380, 1949, 307, 264, 2693, 12879, 24420, 659, 12, 17227, 1760, 6263, 538], "temperature": 0.0, "avg_logprob": -0.1346477579187464, "compression_ratio": 1.7768924302788844, "no_speech_prob": 4.984867337043397e-05}, {"id": 408, "seek": 224048, "start": 2240.48, "end": 2248.48, "text": " supervised multitask training. I wish that we had run that, but we just didn't. So then sort of", "tokens": [1687, 24420, 42338, 3863, 3097, 13, 286, 3172, 300, 321, 632, 1190, 300, 11, 457, 321, 445, 994, 380, 13, 407, 550, 1333, 295], "temperature": 0.0, "avg_logprob": -0.1327864739202684, "compression_ratio": 1.7872340425531914, "no_speech_prob": 6.203269731486216e-05}, {"id": 409, "seek": 224048, "start": 2248.48, "end": 2253.76, "text": " the last set of experiments we ran, try to answer the following question. Let's say that someone comes", "tokens": [264, 1036, 992, 295, 12050, 321, 5872, 11, 853, 281, 1867, 264, 3480, 1168, 13, 961, 311, 584, 300, 1580, 1487], "temperature": 0.0, "avg_logprob": -0.1327864739202684, "compression_ratio": 1.7872340425531914, "no_speech_prob": 6.203269731486216e-05}, {"id": 410, "seek": 224048, "start": 2253.76, "end": 2258.64, "text": " along and all of a sudden gives you four times as much compute. What should you do with it? And so", "tokens": [2051, 293, 439, 295, 257, 3990, 2709, 291, 1451, 1413, 382, 709, 14722, 13, 708, 820, 291, 360, 365, 309, 30, 400, 370], "temperature": 0.0, "avg_logprob": -0.1327864739202684, "compression_ratio": 1.7872340425531914, "no_speech_prob": 6.203269731486216e-05}, {"id": 411, "seek": 224048, "start": 2258.64, "end": 2262.64, "text": " there are a number of things you could do. You could increase the number of training steps by a factor", "tokens": [456, 366, 257, 1230, 295, 721, 291, 727, 360, 13, 509, 727, 3488, 264, 1230, 295, 3097, 4439, 538, 257, 5952], "temperature": 0.0, "avg_logprob": -0.1327864739202684, "compression_ratio": 1.7872340425531914, "no_speech_prob": 6.203269731486216e-05}, {"id": 412, "seek": 224048, "start": 2262.64, "end": 2267.52, "text": " of four. You could increase your batch size by a factor of four. You could make your model twice as big", "tokens": [295, 1451, 13, 509, 727, 3488, 428, 15245, 2744, 538, 257, 5952, 295, 1451, 13, 509, 727, 652, 428, 2316, 6091, 382, 955], "temperature": 0.0, "avg_logprob": -0.1327864739202684, "compression_ratio": 1.7872340425531914, "no_speech_prob": 6.203269731486216e-05}, {"id": 413, "seek": 226752, "start": 2267.52, "end": 2272.72, "text": " and train for twice as long. You can make your model four times as big. You could train four models", "tokens": [293, 3847, 337, 6091, 382, 938, 13, 509, 393, 652, 428, 2316, 1451, 1413, 382, 955, 13, 509, 727, 3847, 1451, 5245], "temperature": 0.0, "avg_logprob": -0.11278858780860901, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.000146507823956199}, {"id": 414, "seek": 226752, "start": 2272.72, "end": 2277.52, "text": " separately and ensemble them. Or you could do this last thing which doesn't actually use four times", "tokens": [14759, 293, 19492, 552, 13, 1610, 291, 727, 360, 341, 1036, 551, 597, 1177, 380, 767, 764, 1451, 1413], "temperature": 0.0, "avg_logprob": -0.11278858780860901, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.000146507823956199}, {"id": 415, "seek": 226752, "start": 2277.52, "end": 2282.32, "text": " as much compute where you pre-trained one model and you fine tune it four times separately and then", "tokens": [382, 709, 14722, 689, 291, 659, 12, 17227, 2001, 472, 2316, 293, 291, 2489, 10864, 309, 1451, 1413, 14759, 293, 550], "temperature": 0.0, "avg_logprob": -0.11278858780860901, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.000146507823956199}, {"id": 416, "seek": 226752, "start": 2282.32, "end": 2289.36, "text": " ensemble those. And the main takeaway here is that scaling helps. This is, you know, very unsurprising,", "tokens": [19492, 729, 13, 400, 264, 2135, 30681, 510, 307, 300, 21589, 3665, 13, 639, 307, 11, 291, 458, 11, 588, 2693, 374, 26203, 11], "temperature": 0.0, "avg_logprob": -0.11278858780860901, "compression_ratio": 1.7521739130434784, "no_speech_prob": 0.000146507823956199}, {"id": 417, "seek": 228936, "start": 2289.36, "end": 2298.2400000000002, "text": " especially in 2021. But interestingly, you get significant gains whether you just increase", "tokens": [2318, 294, 7201, 13, 583, 25873, 11, 291, 483, 4776, 16823, 1968, 291, 445, 3488], "temperature": 0.0, "avg_logprob": -0.09147814173757295, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.72051207604818e-05}, {"id": 418, "seek": 228936, "start": 2298.2400000000002, "end": 2304.1600000000003, "text": " the training time or if you increase the size. So you can see that along both of these access we", "tokens": [264, 3097, 565, 420, 498, 291, 3488, 264, 2744, 13, 407, 291, 393, 536, 300, 2051, 1293, 295, 613, 2105, 321], "temperature": 0.0, "avg_logprob": -0.09147814173757295, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.72051207604818e-05}, {"id": 419, "seek": 228936, "start": 2304.1600000000003, "end": 2308.0, "text": " get significant performance improvements, although the performance improvements are more dramatic when", "tokens": [483, 4776, 3389, 13797, 11, 4878, 264, 3389, 13797, 366, 544, 12023, 562], "temperature": 0.0, "avg_logprob": -0.09147814173757295, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.72051207604818e-05}, {"id": 420, "seek": 228936, "start": 2308.0, "end": 2313.28, "text": " we increase the size. And in particular, you can see that we've gone from a score of about 71", "tokens": [321, 3488, 264, 2744, 13, 400, 294, 1729, 11, 291, 393, 536, 300, 321, 600, 2780, 490, 257, 6175, 295, 466, 30942], "temperature": 0.0, "avg_logprob": -0.09147814173757295, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.72051207604818e-05}, {"id": 421, "seek": 231328, "start": 2313.28, "end": 2320.32, "text": " and super glue to 78 just by making the model four times bigger. Okay, so let me just kind of give", "tokens": [293, 1687, 8998, 281, 26369, 445, 538, 1455, 264, 2316, 1451, 1413, 3801, 13, 1033, 11, 370, 718, 385, 445, 733, 295, 976], "temperature": 0.0, "avg_logprob": -0.08106337932118199, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0001038974296534434}, {"id": 422, "seek": 231328, "start": 2320.32, "end": 2325.2000000000003, "text": " a quick recap of all of that and then use that recap to explain the design decisions that went into", "tokens": [257, 1702, 20928, 295, 439, 295, 300, 293, 550, 764, 300, 20928, 281, 2903, 264, 1715, 5327, 300, 1437, 666], "temperature": 0.0, "avg_logprob": -0.08106337932118199, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0001038974296534434}, {"id": 423, "seek": 231328, "start": 2325.2000000000003, "end": 2330.7200000000003, "text": " the final sort of T5 models. The first thing is that we're going to choose an encoder decoder", "tokens": [264, 2572, 1333, 295, 314, 20, 5245, 13, 440, 700, 551, 307, 300, 321, 434, 516, 281, 2826, 364, 2058, 19866, 979, 19866], "temperature": 0.0, "avg_logprob": -0.08106337932118199, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0001038974296534434}, {"id": 424, "seek": 231328, "start": 2330.7200000000003, "end": 2335.28, "text": " architecture because that seemed to work best in our text to text format. The next thing is that", "tokens": [9482, 570, 300, 6576, 281, 589, 1151, 294, 527, 2487, 281, 2487, 7877, 13, 440, 958, 551, 307, 300], "temperature": 0.0, "avg_logprob": -0.08106337932118199, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0001038974296534434}, {"id": 425, "seek": 231328, "start": 2335.28, "end": 2339.28, "text": " we're going to use a span prediction objective which is ultimately quite similar to the baseline", "tokens": [321, 434, 516, 281, 764, 257, 16174, 17630, 10024, 597, 307, 6284, 1596, 2531, 281, 264, 20518], "temperature": 0.0, "avg_logprob": -0.08106337932118199, "compression_ratio": 1.7482014388489209, "no_speech_prob": 0.0001038974296534434}, {"id": 426, "seek": 233928, "start": 2339.28, "end": 2344.6400000000003, "text": " objective that I described earlier. We will use the C4 dataset because it did attain reasonable", "tokens": [10024, 300, 286, 7619, 3071, 13, 492, 486, 764, 264, 383, 19, 28872, 570, 309, 630, 23766, 10585], "temperature": 0.0, "avg_logprob": -0.05957896673857276, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.00019709706248249859}, {"id": 427, "seek": 233928, "start": 2344.6400000000003, "end": 2349.76, "text": " performance but was large enough that we didn't have to worry about repeating the data and seeing", "tokens": [3389, 457, 390, 2416, 1547, 300, 321, 994, 380, 362, 281, 3292, 466, 18617, 264, 1412, 293, 2577], "temperature": 0.0, "avg_logprob": -0.05957896673857276, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.00019709706248249859}, {"id": 428, "seek": 233928, "start": 2349.76, "end": 2355.52, "text": " detrimental overfitting during pre-training when we scale up the number of pre-training steps.", "tokens": [45694, 670, 69, 2414, 1830, 659, 12, 17227, 1760, 562, 321, 4373, 493, 264, 1230, 295, 659, 12, 17227, 1760, 4439, 13], "temperature": 0.0, "avg_logprob": -0.05957896673857276, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.00019709706248249859}, {"id": 429, "seek": 233928, "start": 2355.52, "end": 2359.84, "text": " We actually decided to do multitask pre-training because we will be scaling up the amount of", "tokens": [492, 767, 3047, 281, 360, 42338, 3863, 659, 12, 17227, 1760, 570, 321, 486, 312, 21589, 493, 264, 2372, 295], "temperature": 0.0, "avg_logprob": -0.05957896673857276, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.00019709706248249859}, {"id": 430, "seek": 233928, "start": 2359.84, "end": 2364.0, "text": " pre-training. Our longest training runs took about a month and we wanted to be able to monitor", "tokens": [659, 12, 17227, 1760, 13, 2621, 15438, 3097, 6676, 1890, 466, 257, 1618, 293, 321, 1415, 281, 312, 1075, 281, 6002], "temperature": 0.0, "avg_logprob": -0.05957896673857276, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.00019709706248249859}, {"id": 431, "seek": 233928, "start": 2364.0, "end": 2368.32, "text": " performance over the course of pre-training without doing fine tuning. So we're going to be doing", "tokens": [3389, 670, 264, 1164, 295, 659, 12, 17227, 1760, 1553, 884, 2489, 15164, 13, 407, 321, 434, 516, 281, 312, 884], "temperature": 0.0, "avg_logprob": -0.05957896673857276, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.00019709706248249859}, {"id": 432, "seek": 236832, "start": 2368.32, "end": 2372.6400000000003, "text": " this multitask pre-training followed by fine tuning and then the last thing of course is we're", "tokens": [341, 42338, 3863, 659, 12, 17227, 1760, 6263, 538, 2489, 15164, 293, 550, 264, 1036, 551, 295, 1164, 307, 321, 434], "temperature": 0.0, "avg_logprob": -0.137783940633138, "compression_ratio": 1.7640449438202248, "no_speech_prob": 4.468778206501156e-05}, {"id": 433, "seek": 236832, "start": 2372.6400000000003, "end": 2378.0800000000004, "text": " going to train bigger models for longer. Specifically the model sizes that we ended up", "tokens": [516, 281, 3847, 3801, 5245, 337, 2854, 13, 26058, 264, 2316, 11602, 300, 321, 4590, 493], "temperature": 0.0, "avg_logprob": -0.137783940633138, "compression_ratio": 1.7640449438202248, "no_speech_prob": 4.468778206501156e-05}, {"id": 434, "seek": 236832, "start": 2378.88, "end": 2385.6800000000003, "text": " releasing we call small base large 3B and 11B. The small model has 60 million parameters. It's", "tokens": [16327, 321, 818, 1359, 3096, 2416, 805, 33, 293, 2975, 33, 13, 440, 1359, 2316, 575, 4060, 2459, 9834, 13, 467, 311], "temperature": 0.0, "avg_logprob": -0.137783940633138, "compression_ratio": 1.7640449438202248, "no_speech_prob": 4.468778206501156e-05}, {"id": 435, "seek": 236832, "start": 2385.6800000000003, "end": 2390.4, "text": " about a quarter as big as our baseline which again was a birth-based size encoder, birth-based", "tokens": [466, 257, 6555, 382, 955, 382, 527, 20518, 597, 797, 390, 257, 3965, 12, 6032, 2744, 2058, 19866, 11, 3965, 12, 6032], "temperature": 0.0, "avg_logprob": -0.137783940633138, "compression_ratio": 1.7640449438202248, "no_speech_prob": 4.468778206501156e-05}, {"id": 436, "seek": 236832, "start": 2390.4, "end": 2396.0800000000004, "text": " size decoder. We also trained a model that was a birth-large size encoder, birth-large size decoder", "tokens": [2744, 979, 19866, 13, 492, 611, 8895, 257, 2316, 300, 390, 257, 3965, 12, 2200, 432, 2744, 2058, 19866, 11, 3965, 12, 2200, 432, 2744, 979, 19866], "temperature": 0.0, "avg_logprob": -0.137783940633138, "compression_ratio": 1.7640449438202248, "no_speech_prob": 4.468778206501156e-05}, {"id": 437, "seek": 239608, "start": 2396.08, "end": 2401.12, "text": " and then we created two larger variants simply by scaling up the feed-forward dimension of the", "tokens": [293, 550, 321, 2942, 732, 4833, 21669, 2935, 538, 21589, 493, 264, 3154, 12, 13305, 10139, 295, 264], "temperature": 0.0, "avg_logprob": -0.07571882671780056, "compression_ratio": 1.7830882352941178, "no_speech_prob": 4.756900671054609e-05}, {"id": 438, "seek": 239608, "start": 2401.12, "end": 2406.16, "text": " transformer and the number of attention heads in the transformer. You can see our largest model", "tokens": [31782, 293, 264, 1230, 295, 3202, 8050, 294, 264, 31782, 13, 509, 393, 536, 527, 6443, 2316], "temperature": 0.0, "avg_logprob": -0.07571882671780056, "compression_ratio": 1.7830882352941178, "no_speech_prob": 4.756900671054609e-05}, {"id": 439, "seek": 239608, "start": 2406.16, "end": 2412.72, "text": " actually had a hidden dimensionality of 65,000 in the feed-forward layers. The reason that we did this", "tokens": [767, 632, 257, 7633, 10139, 1860, 295, 11624, 11, 1360, 294, 264, 3154, 12, 13305, 7914, 13, 440, 1778, 300, 321, 630, 341], "temperature": 0.0, "avg_logprob": -0.07571882671780056, "compression_ratio": 1.7830882352941178, "no_speech_prob": 4.756900671054609e-05}, {"id": 440, "seek": 239608, "start": 2412.72, "end": 2418.16, "text": " kind of unusual way of scaling up the perimeter count is just because the feed-forward layers are", "tokens": [733, 295, 10901, 636, 295, 21589, 493, 264, 32404, 1207, 307, 445, 570, 264, 3154, 12, 13305, 7914, 366], "temperature": 0.0, "avg_logprob": -0.07571882671780056, "compression_ratio": 1.7830882352941178, "no_speech_prob": 4.756900671054609e-05}, {"id": 441, "seek": 239608, "start": 2418.16, "end": 2422.7999999999997, "text": " just gigantic matrix multiplies and that's the best way to make use of hardware accelerators.", "tokens": [445, 26800, 8141, 12788, 530, 293, 300, 311, 264, 1151, 636, 281, 652, 764, 295, 8837, 10172, 3391, 13], "temperature": 0.0, "avg_logprob": -0.07571882671780056, "compression_ratio": 1.7830882352941178, "no_speech_prob": 4.756900671054609e-05}, {"id": 442, "seek": 242280, "start": 2422.8, "end": 2434.1600000000003, "text": " I just stick in one more question. Someone was asking about how you did the multi-task training.", "tokens": [286, 445, 2897, 294, 472, 544, 1168, 13, 8734, 390, 3365, 466, 577, 291, 630, 264, 4825, 12, 83, 3863, 3097, 13], "temperature": 0.0, "avg_logprob": -0.29048986145944306, "compression_ratio": 1.45, "no_speech_prob": 0.00016084684466477484}, {"id": 443, "seek": 242280, "start": 2434.88, "end": 2440.6400000000003, "text": " Was that sticking a simple softmax classifier on top for each task?", "tokens": [3027, 300, 13465, 257, 2199, 2787, 41167, 1508, 9902, 322, 1192, 337, 1184, 5633, 30], "temperature": 0.0, "avg_logprob": -0.29048986145944306, "compression_ratio": 1.45, "no_speech_prob": 0.00016084684466477484}, {"id": 444, "seek": 242280, "start": 2440.6400000000003, "end": 2449.04, "text": " In our case because we're using this text-to-text format basically you train on exactly the same", "tokens": [682, 527, 1389, 570, 321, 434, 1228, 341, 2487, 12, 1353, 12, 25111, 7877, 1936, 291, 3847, 322, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.29048986145944306, "compression_ratio": 1.45, "no_speech_prob": 0.00016084684466477484}, {"id": 445, "seek": 244904, "start": 2449.04, "end": 2453.2799999999997, "text": " model no new classification heads for every task. The only difference is that each task gets its", "tokens": [2316, 572, 777, 21538, 8050, 337, 633, 5633, 13, 440, 787, 2649, 307, 300, 1184, 5633, 2170, 1080], "temperature": 0.0, "avg_logprob": -0.11088708594993309, "compression_ratio": 1.7052631578947368, "no_speech_prob": 8.08837721706368e-05}, {"id": 446, "seek": 244904, "start": 2453.2799999999997, "end": 2457.6, "text": " own task prefix. So if you remember all the way back at the beginning we say you know translate", "tokens": [1065, 5633, 46969, 13, 407, 498, 291, 1604, 439, 264, 636, 646, 412, 264, 2863, 321, 584, 291, 458, 13799], "temperature": 0.0, "avg_logprob": -0.11088708594993309, "compression_ratio": 1.7052631578947368, "no_speech_prob": 8.08837721706368e-05}, {"id": 447, "seek": 244904, "start": 2457.6, "end": 2463.2799999999997, "text": " English to German colon English sentence or you know summarize colon English paragraph and that", "tokens": [3669, 281, 6521, 8255, 3669, 8174, 420, 291, 458, 20858, 8255, 3669, 18865, 293, 300], "temperature": 0.0, "avg_logprob": -0.11088708594993309, "compression_ratio": 1.7052631578947368, "no_speech_prob": 8.08837721706368e-05}, {"id": 448, "seek": 244904, "start": 2463.2799999999997, "end": 2467.6, "text": " tells the model what it should do and then you just train the model to predict the corresponding target.", "tokens": [5112, 264, 2316, 437, 309, 820, 360, 293, 550, 291, 445, 3847, 264, 2316, 281, 6069, 264, 11760, 3779, 13], "temperature": 0.0, "avg_logprob": -0.11088708594993309, "compression_ratio": 1.7052631578947368, "no_speech_prob": 8.08837721706368e-05}, {"id": 449, "seek": 244904, "start": 2471.6, "end": 2477.92, "text": " Cool. So the last pertinent detail again is that we did scale up the amount of pre-training.", "tokens": [8561, 13, 407, 264, 1036, 13269, 11058, 2607, 797, 307, 300, 321, 630, 4373, 493, 264, 2372, 295, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.11088708594993309, "compression_ratio": 1.7052631578947368, "no_speech_prob": 8.08837721706368e-05}, {"id": 450, "seek": 247792, "start": 2477.92, "end": 2483.52, "text": " We ended up pre-training on a trillion tokens of data rather than 34 billion tokens. So it's", "tokens": [492, 4590, 493, 659, 12, 17227, 1760, 322, 257, 18723, 22667, 295, 1412, 2831, 813, 12790, 5218, 22667, 13, 407, 309, 311], "temperature": 0.0, "avg_logprob": -0.19129311836371987, "compression_ratio": 1.6851851851851851, "no_speech_prob": 6.40074722468853e-05}, {"id": 451, "seek": 247792, "start": 2483.52, "end": 2488.0, "text": " quite a lot more pre-training although it's still less pre-training that was used in excel", "tokens": [1596, 257, 688, 544, 659, 12, 17227, 1760, 4878, 309, 311, 920, 1570, 659, 12, 17227, 1760, 300, 390, 1143, 294, 24015], "temperature": 0.0, "avg_logprob": -0.19129311836371987, "compression_ratio": 1.6851851851851851, "no_speech_prob": 6.40074722468853e-05}, {"id": 452, "seek": 247792, "start": 2488.0, "end": 2494.4, "text": " net. I think by a factor of two if I don't remember incorrectly. So here are the results that", "tokens": [2533, 13, 286, 519, 538, 257, 5952, 295, 732, 498, 286, 500, 380, 1604, 42892, 13, 407, 510, 366, 264, 3542, 300], "temperature": 0.0, "avg_logprob": -0.19129311836371987, "compression_ratio": 1.6851851851851851, "no_speech_prob": 6.40074722468853e-05}, {"id": 453, "seek": 247792, "start": 2494.4, "end": 2497.76, "text": " and these were kind of the way that things stood at the time that we released the paper.", "tokens": [293, 613, 645, 733, 295, 264, 636, 300, 721, 9371, 412, 264, 565, 300, 321, 4736, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.19129311836371987, "compression_ratio": 1.6851851851851851, "no_speech_prob": 6.40074722468853e-05}, {"id": 454, "seek": 247792, "start": 2498.8, "end": 2504.16, "text": " We ended up getting state of the art results on the glue metabenschmark, CNN Daily Mail,", "tokens": [492, 4590, 493, 1242, 1785, 295, 264, 1523, 3542, 322, 264, 8998, 1131, 455, 26590, 5638, 11, 24859, 19685, 29164, 11], "temperature": 0.0, "avg_logprob": -0.19129311836371987, "compression_ratio": 1.6851851851851851, "no_speech_prob": 6.40074722468853e-05}, {"id": 455, "seek": 250416, "start": 2504.16, "end": 2510.0, "text": " abstract summarization, squad question answering. And we were actually quite excited to see how well", "tokens": [12649, 14611, 2144, 11, 15310, 1168, 13430, 13, 400, 321, 645, 767, 1596, 2919, 281, 536, 577, 731], "temperature": 0.0, "avg_logprob": -0.1285100541673265, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.0001140878812293522}, {"id": 456, "seek": 250416, "start": 2510.0, "end": 2516.0, "text": " we did on super glue. We ultimately came pretty close to the human score which was the score of 89.8", "tokens": [321, 630, 322, 1687, 8998, 13, 492, 6284, 1361, 1238, 1998, 281, 264, 1952, 6175, 597, 390, 264, 6175, 295, 31877, 13, 23], "temperature": 0.0, "avg_logprob": -0.1285100541673265, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.0001140878812293522}, {"id": 457, "seek": 250416, "start": 2516.0, "end": 2522.24, "text": " and we performed significantly better than Roberta. Super glue it turns out is a benchmark that", "tokens": [293, 321, 10332, 10591, 1101, 813, 15800, 1328, 13, 4548, 8998, 309, 4523, 484, 307, 257, 18927, 300], "temperature": 0.0, "avg_logprob": -0.1285100541673265, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.0001140878812293522}, {"id": 458, "seek": 250416, "start": 2522.24, "end": 2527.2, "text": " benefits a lot from large models and so you can really see a dramatic increase in the model's", "tokens": [5311, 257, 688, 490, 2416, 5245, 293, 370, 291, 393, 534, 536, 257, 12023, 3488, 294, 264, 2316, 311], "temperature": 0.0, "avg_logprob": -0.1285100541673265, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.0001140878812293522}, {"id": 459, "seek": 250416, "start": 2527.2, "end": 2533.12, "text": " performance as we scale the model up. On the other hand we did not obtain state of the art results", "tokens": [3389, 382, 321, 4373, 264, 2316, 493, 13, 1282, 264, 661, 1011, 321, 630, 406, 12701, 1785, 295, 264, 1523, 3542], "temperature": 0.0, "avg_logprob": -0.1285100541673265, "compression_ratio": 1.6442953020134228, "no_speech_prob": 0.0001140878812293522}, {"id": 460, "seek": 253312, "start": 2533.12, "end": 2537.8399999999997, "text": " on any of the translation data sets. And the reason that we think that this is true is because", "tokens": [322, 604, 295, 264, 12853, 1412, 6352, 13, 400, 264, 1778, 300, 321, 519, 300, 341, 307, 2074, 307, 570], "temperature": 0.0, "avg_logprob": -0.08142070432679843, "compression_ratio": 1.9137254901960785, "no_speech_prob": 9.912031964631751e-05}, {"id": 461, "seek": 253312, "start": 2537.8399999999997, "end": 2542.72, "text": " all of the state of the art results at the time on these translation data sets used back translation.", "tokens": [439, 295, 264, 1785, 295, 264, 1523, 3542, 412, 264, 565, 322, 613, 12853, 1412, 6352, 1143, 646, 12853, 13], "temperature": 0.0, "avg_logprob": -0.08142070432679843, "compression_ratio": 1.9137254901960785, "no_speech_prob": 9.912031964631751e-05}, {"id": 462, "seek": 253312, "start": 2542.72, "end": 2549.2799999999997, "text": " And if you remember we did English only pre-training in our model and we expect that in terms of", "tokens": [400, 498, 291, 1604, 321, 630, 3669, 787, 659, 12, 17227, 1760, 294, 527, 2316, 293, 321, 2066, 300, 294, 2115, 295], "temperature": 0.0, "avg_logprob": -0.08142070432679843, "compression_ratio": 1.9137254901960785, "no_speech_prob": 9.912031964631751e-05}, {"id": 463, "seek": 253312, "start": 2549.2799999999997, "end": 2555.04, "text": " making use of unlabeled data it's more effective to use back translation for machine translation", "tokens": [1455, 764, 295, 32118, 18657, 292, 1412, 309, 311, 544, 4942, 281, 764, 646, 12853, 337, 3479, 12853], "temperature": 0.0, "avg_logprob": -0.08142070432679843, "compression_ratio": 1.9137254901960785, "no_speech_prob": 9.912031964631751e-05}, {"id": 464, "seek": 253312, "start": 2555.04, "end": 2560.24, "text": " problems than to use this English only pre-training that we did. I should mention of course these", "tokens": [2740, 813, 281, 764, 341, 3669, 787, 659, 12, 17227, 1760, 300, 321, 630, 13, 286, 820, 2152, 295, 1164, 613], "temperature": 0.0, "avg_logprob": -0.08142070432679843, "compression_ratio": 1.9137254901960785, "no_speech_prob": 9.912031964631751e-05}, {"id": 465, "seek": 256024, "start": 2560.24, "end": 2565.52, "text": " results are now quite a bit stale and some of these scores have been beaten by subsequent models.", "tokens": [3542, 366, 586, 1596, 257, 857, 342, 1220, 293, 512, 295, 613, 13444, 362, 668, 17909, 538, 19962, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17365053384610923, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0001088690769392997}, {"id": 466, "seek": 256024, "start": 2567.9199999999996, "end": 2572.3999999999996, "text": " So now I'll just quick make a plug that you know all of our code is released, our pre-trained models", "tokens": [407, 586, 286, 603, 445, 1702, 652, 257, 5452, 300, 291, 458, 439, 295, 527, 3089, 307, 4736, 11, 527, 659, 12, 17227, 2001, 5245], "temperature": 0.0, "avg_logprob": -0.17365053384610923, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0001088690769392997}, {"id": 467, "seek": 256024, "start": 2572.3999999999996, "end": 2576.64, "text": " have been released. You can make use of them in our code base. They're also of course in the", "tokens": [362, 668, 4736, 13, 509, 393, 652, 764, 295, 552, 294, 527, 3089, 3096, 13, 814, 434, 611, 295, 1164, 294, 264], "temperature": 0.0, "avg_logprob": -0.17365053384610923, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0001088690769392997}, {"id": 468, "seek": 256024, "start": 2576.64, "end": 2585.4399999999996, "text": " Hugging Face Transformers code base. We made a colab at the time that shows a pretty basic demo of", "tokens": [389, 697, 3249, 4047, 27938, 433, 3089, 3096, 13, 492, 1027, 257, 1173, 455, 412, 264, 565, 300, 3110, 257, 1238, 3875, 10723, 295], "temperature": 0.0, "avg_logprob": -0.17365053384610923, "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0001088690769392997}, {"id": 469, "seek": 258544, "start": 2585.44, "end": 2591.68, "text": " how to take one of our pre-trained models and basically train it on a TSV file of inputs and", "tokens": [577, 281, 747, 472, 295, 527, 659, 12, 17227, 2001, 5245, 293, 1936, 3847, 309, 322, 257, 37645, 53, 3991, 295, 15743, 293], "temperature": 0.0, "avg_logprob": -0.1163532574971517, "compression_ratio": 1.7472527472527473, "no_speech_prob": 5.306770981405862e-05}, {"id": 470, "seek": 258544, "start": 2591.68, "end": 2596.0, "text": " targets. So because all problems are text to text problems you just need to give the model some", "tokens": [12911, 13, 407, 570, 439, 2740, 366, 2487, 281, 2487, 2740, 291, 445, 643, 281, 976, 264, 2316, 512], "temperature": 0.0, "avg_logprob": -0.1163532574971517, "compression_ratio": 1.7472527472527473, "no_speech_prob": 5.306770981405862e-05}, {"id": 471, "seek": 258544, "start": 2596.0, "end": 2601.68, "text": " input text and some target text and that's all you need to fine tune the model. And you can make", "tokens": [4846, 2487, 293, 512, 3779, 2487, 293, 300, 311, 439, 291, 643, 281, 2489, 10864, 264, 2316, 13, 400, 291, 393, 652], "temperature": 0.0, "avg_logprob": -0.1163532574971517, "compression_ratio": 1.7472527472527473, "no_speech_prob": 5.306770981405862e-05}, {"id": 472, "seek": 258544, "start": 2601.68, "end": 2607.52, "text": " you can actually fine tune up to the 3 billion parameter model on a free colab TPU using the", "tokens": [291, 393, 767, 2489, 10864, 493, 281, 264, 805, 5218, 13075, 2316, 322, 257, 1737, 1173, 455, 314, 8115, 1228, 264], "temperature": 0.0, "avg_logprob": -0.1163532574971517, "compression_ratio": 1.7472527472527473, "no_speech_prob": 5.306770981405862e-05}, {"id": 473, "seek": 258544, "start": 2607.52, "end": 2614.32, "text": " the link at the bottom here. Great so so far we've been talking about an English only pre-training", "tokens": [264, 2113, 412, 264, 2767, 510, 13, 3769, 370, 370, 1400, 321, 600, 668, 1417, 466, 364, 3669, 787, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.1163532574971517, "compression_ratio": 1.7472527472527473, "no_speech_prob": 5.306770981405862e-05}, {"id": 474, "seek": 261432, "start": 2614.32, "end": 2619.6000000000004, "text": " model. I mean we did apply it to machine translation in downstream tasks. So kind of a natural", "tokens": [2316, 13, 286, 914, 321, 630, 3079, 309, 281, 3479, 12853, 294, 30621, 9608, 13, 407, 733, 295, 257, 3303], "temperature": 0.0, "avg_logprob": -0.1593540593197471, "compression_ratio": 1.6714801444043321, "no_speech_prob": 9.313153714174405e-05}, {"id": 475, "seek": 261432, "start": 2619.6000000000004, "end": 2625.04, "text": " question is you know what about all the other languages? Why not train a multi-lingual model?", "tokens": [1168, 307, 291, 458, 437, 466, 439, 264, 661, 8650, 30, 1545, 406, 3847, 257, 4825, 12, 1688, 901, 2316, 30], "temperature": 0.0, "avg_logprob": -0.1593540593197471, "compression_ratio": 1.6714801444043321, "no_speech_prob": 9.313153714174405e-05}, {"id": 476, "seek": 261432, "start": 2625.04, "end": 2628.88, "text": " And so that's something that we did more recently. And actually let me just pause because I did", "tokens": [400, 370, 300, 311, 746, 300, 321, 630, 544, 3938, 13, 400, 767, 718, 385, 445, 10465, 570, 286, 630], "temperature": 0.0, "avg_logprob": -0.1593540593197471, "compression_ratio": 1.6714801444043321, "no_speech_prob": 9.313153714174405e-05}, {"id": 477, "seek": 261432, "start": 2628.88, "end": 2633.76, "text": " see a couple questions coming on. I want to make sure that I I'm not leaving anyone behind as I", "tokens": [536, 257, 1916, 1651, 1348, 322, 13, 286, 528, 281, 652, 988, 300, 286, 286, 478, 406, 5012, 2878, 2261, 382, 286], "temperature": 0.0, "avg_logprob": -0.1593540593197471, "compression_ratio": 1.6714801444043321, "no_speech_prob": 9.313153714174405e-05}, {"id": 478, "seek": 261432, "start": 2633.76, "end": 2644.2400000000002, "text": " move to the next section. Sure so one of the questions is about the multitask set.", "tokens": [1286, 281, 264, 958, 3541, 13, 4894, 370, 472, 295, 264, 1651, 307, 466, 264, 42338, 3863, 992, 13], "temperature": 0.0, "avg_logprob": -0.1593540593197471, "compression_ratio": 1.6714801444043321, "no_speech_prob": 9.313153714174405e-05}, {"id": 479, "seek": 264424, "start": 2644.24, "end": 2649.68, "text": " So if you include an unknown task prefix does anything interesting happen? And if you don't", "tokens": [407, 498, 291, 4090, 364, 9841, 5633, 46969, 775, 1340, 1880, 1051, 30, 400, 498, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.18465457119784512, "compression_ratio": 1.958974358974359, "no_speech_prob": 0.00013762277376372367}, {"id": 480, "seek": 264424, "start": 2649.68, "end": 2658.24, "text": " include a prefix what what does it do? So if you if you include an unknown task prefix or if you", "tokens": [4090, 257, 46969, 437, 437, 775, 309, 360, 30, 407, 498, 291, 498, 291, 4090, 364, 9841, 5633, 46969, 420, 498, 291], "temperature": 0.0, "avg_logprob": -0.18465457119784512, "compression_ratio": 1.958974358974359, "no_speech_prob": 0.00013762277376372367}, {"id": 481, "seek": 264424, "start": 2658.24, "end": 2663.9199999999996, "text": " don't include a prefix at all what it will probably do is apply the unsupervised objective because", "tokens": [500, 380, 4090, 257, 46969, 412, 439, 437, 309, 486, 1391, 360, 307, 3079, 264, 2693, 12879, 24420, 10024, 570], "temperature": 0.0, "avg_logprob": -0.18465457119784512, "compression_ratio": 1.958974358974359, "no_speech_prob": 0.00013762277376372367}, {"id": 482, "seek": 264424, "start": 2663.9199999999996, "end": 2670.16, "text": " we actually didn't use a task prefix for the unsupervised objective. Well I guess I should say", "tokens": [321, 767, 994, 380, 764, 257, 5633, 46969, 337, 264, 2693, 12879, 24420, 10024, 13, 1042, 286, 2041, 286, 820, 584], "temperature": 0.0, "avg_logprob": -0.18465457119784512, "compression_ratio": 1.958974358974359, "no_speech_prob": 0.00013762277376372367}, {"id": 483, "seek": 267016, "start": 2670.16, "end": 2677.3599999999997, "text": " what if it's not quite that's not quite true because there won't be any Sentinel tokens in the input.", "tokens": [437, 498, 309, 311, 406, 1596, 300, 311, 406, 1596, 2074, 570, 456, 1582, 380, 312, 604, 49498, 22667, 294, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.1335875771262429, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.00016089128621388227}, {"id": 484, "seek": 267016, "start": 2677.3599999999997, "end": 2682.24, "text": " So we actually see it it typically does is it outputs kind of some related words and some other", "tokens": [407, 321, 767, 536, 309, 309, 5850, 775, 307, 309, 23930, 733, 295, 512, 4077, 2283, 293, 512, 661], "temperature": 0.0, "avg_logprob": -0.1335875771262429, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.00016089128621388227}, {"id": 485, "seek": 267016, "start": 2682.24, "end": 2687.6, "text": " Sentinel tokens in gibberish. It's it's not very useful as I guess the upshot.", "tokens": [49498, 22667, 294, 4553, 43189, 13, 467, 311, 309, 311, 406, 588, 4420, 382, 286, 2041, 264, 493, 18402, 13], "temperature": 0.0, "avg_logprob": -0.1335875771262429, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.00016089128621388227}, {"id": 486, "seek": 267016, "start": 2689.8399999999997, "end": 2692.96, "text": " You have questions about back translation. I don't think they heard about back translation in", "tokens": [509, 362, 1651, 466, 646, 12853, 13, 286, 500, 380, 519, 436, 2198, 466, 646, 12853, 294], "temperature": 0.0, "avg_logprob": -0.1335875771262429, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.00016089128621388227}, {"id": 487, "seek": 267016, "start": 2692.96, "end": 2698.24, "text": " the rest of the course. So back translation is a pretty straightforward method. The basic idea is", "tokens": [264, 1472, 295, 264, 1164, 13, 407, 646, 12853, 307, 257, 1238, 15325, 3170, 13, 440, 3875, 1558, 307], "temperature": 0.0, "avg_logprob": -0.1335875771262429, "compression_ratio": 1.806949806949807, "no_speech_prob": 0.00016089128621388227}, {"id": 488, "seek": 269824, "start": 2698.24, "end": 2703.7599999999998, "text": " that if I have unlabeled text data in one language I use my current model to translate that data", "tokens": [300, 498, 286, 362, 32118, 18657, 292, 2487, 1412, 294, 472, 2856, 286, 764, 452, 2190, 2316, 281, 13799, 300, 1412], "temperature": 0.0, "avg_logprob": -0.0826365869123857, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0001465003442717716}, {"id": 489, "seek": 269824, "start": 2704.3999999999996, "end": 2712.0, "text": " to some particular language and I use that as training data then subsequently for my model.", "tokens": [281, 512, 1729, 2856, 293, 286, 764, 300, 382, 3097, 1412, 550, 26514, 337, 452, 2316, 13], "temperature": 0.0, "avg_logprob": -0.0826365869123857, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0001465003442717716}, {"id": 490, "seek": 269824, "start": 2712.0, "end": 2715.9199999999996, "text": " It's it's similar to self-training if you're familiar with it. Basically you're making predictions", "tokens": [467, 311, 309, 311, 2531, 281, 2698, 12, 17227, 1760, 498, 291, 434, 4963, 365, 309, 13, 8537, 291, 434, 1455, 21264], "temperature": 0.0, "avg_logprob": -0.0826365869123857, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0001465003442717716}, {"id": 491, "seek": 269824, "start": 2715.9199999999996, "end": 2720.8799999999997, "text": " on unlabeled data and then using those predictions to train the model. Turns out to be helpful.", "tokens": [322, 32118, 18657, 292, 1412, 293, 550, 1228, 729, 21264, 281, 3847, 264, 2316, 13, 29524, 484, 281, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.0826365869123857, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.0001465003442717716}, {"id": 492, "seek": 272088, "start": 2720.88, "end": 2728.6400000000003, "text": " Yep and then one one detailed question maximum input length and maximum output length. How did", "tokens": [7010, 293, 550, 472, 472, 9942, 1168, 6674, 4846, 4641, 293, 6674, 5598, 4641, 13, 1012, 630], "temperature": 0.0, "avg_logprob": -0.12448241975572374, "compression_ratio": 1.7706422018348624, "no_speech_prob": 7.720595749560744e-05}, {"id": 493, "seek": 272088, "start": 2728.6400000000003, "end": 2733.28, "text": " you choose them? Did you do a study on that as well? Yeah so most of the tasks we considered", "tokens": [291, 2826, 552, 30, 2589, 291, 360, 257, 2979, 322, 300, 382, 731, 30, 865, 370, 881, 295, 264, 9608, 321, 4888], "temperature": 0.0, "avg_logprob": -0.12448241975572374, "compression_ratio": 1.7706422018348624, "no_speech_prob": 7.720595749560744e-05}, {"id": 494, "seek": 272088, "start": 2734.08, "end": 2739.52, "text": " did not have input length significantly longer than 512 tokens most of the time using the tokenization", "tokens": [630, 406, 362, 4846, 4641, 10591, 2854, 813, 1025, 4762, 22667, 881, 295, 264, 565, 1228, 264, 14862, 2144], "temperature": 0.0, "avg_logprob": -0.12448241975572374, "compression_ratio": 1.7706422018348624, "no_speech_prob": 7.720595749560744e-05}, {"id": 495, "seek": 272088, "start": 2739.52, "end": 2747.04, "text": " strategy that that that we made use of and so we used a maximum input length of 512 but we used", "tokens": [5206, 300, 300, 300, 321, 1027, 764, 295, 293, 370, 321, 1143, 257, 6674, 4846, 4641, 295, 1025, 4762, 457, 321, 1143], "temperature": 0.0, "avg_logprob": -0.12448241975572374, "compression_ratio": 1.7706422018348624, "no_speech_prob": 7.720595749560744e-05}, {"id": 496, "seek": 274704, "start": 2747.04, "end": 2752.56, "text": " that position encoding scheme that allows arbitrary input lengths and we actually have in", "tokens": [300, 2535, 43430, 12232, 300, 4045, 23211, 4846, 26329, 293, 321, 767, 362, 294], "temperature": 0.0, "avg_logprob": -0.15064399242401122, "compression_ratio": 1.5726495726495726, "no_speech_prob": 9.168752876576036e-05}, {"id": 497, "seek": 274704, "start": 2752.56, "end": 2759.84, "text": " subsequent work fine tune T5 on sequences of of length 2048. Beyond that you start to get", "tokens": [19962, 589, 2489, 10864, 314, 20, 322, 22978, 295, 295, 4641, 945, 13318, 13, 19707, 300, 291, 722, 281, 483], "temperature": 0.0, "avg_logprob": -0.15064399242401122, "compression_ratio": 1.5726495726495726, "no_speech_prob": 9.168752876576036e-05}, {"id": 498, "seek": 274704, "start": 2759.84, "end": 2765.84, "text": " into memory issues because of attention's quadratic memory complexity but you in principle you", "tokens": [666, 4675, 2663, 570, 295, 3202, 311, 37262, 4675, 14024, 457, 291, 294, 8665, 291], "temperature": 0.0, "avg_logprob": -0.15064399242401122, "compression_ratio": 1.5726495726495726, "no_speech_prob": 9.168752876576036e-05}, {"id": 499, "seek": 274704, "start": 2765.84, "end": 2770.48, "text": " can you can apply it to long sequences. And then maybe one last thing if you have a second is", "tokens": [393, 291, 393, 3079, 309, 281, 938, 22978, 13, 400, 550, 1310, 472, 1036, 551, 498, 291, 362, 257, 1150, 307], "temperature": 0.0, "avg_logprob": -0.15064399242401122, "compression_ratio": 1.5726495726495726, "no_speech_prob": 9.168752876576036e-05}, {"id": 500, "seek": 277048, "start": 2770.48, "end": 2777.76, "text": " talking again about how translation again seems to be not the not the killer app for this and", "tokens": [1417, 797, 466, 577, 12853, 797, 2544, 281, 312, 406, 264, 406, 264, 13364, 724, 337, 341, 293], "temperature": 0.0, "avg_logprob": -0.1735104297069793, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.00013976788613945246}, {"id": 501, "seek": 277048, "start": 2777.76, "end": 2784.48, "text": " so what's your intuition as to why translations like does it not benefit from pre-training for", "tokens": [370, 437, 311, 428, 24002, 382, 281, 983, 37578, 411, 775, 309, 406, 5121, 490, 659, 12, 17227, 1760, 337], "temperature": 0.0, "avg_logprob": -0.1735104297069793, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.00013976788613945246}, {"id": 502, "seek": 277048, "start": 2784.48, "end": 2795.04, "text": " for what reason? I'm not you know I'm really not sure. I yeah I can only conjecture. I think that", "tokens": [337, 437, 1778, 30, 286, 478, 406, 291, 458, 286, 478, 534, 406, 988, 13, 286, 1338, 286, 393, 787, 416, 1020, 540, 13, 286, 519, 300], "temperature": 0.0, "avg_logprob": -0.1735104297069793, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.00013976788613945246}, {"id": 503, "seek": 277048, "start": 2795.04, "end": 2800.08, "text": " pre-training helps the model learn the meaning of words. It helps the model learn some", "tokens": [659, 12, 17227, 1760, 3665, 264, 2316, 1466, 264, 3620, 295, 2283, 13, 467, 3665, 264, 2316, 1466, 512], "temperature": 0.0, "avg_logprob": -0.1735104297069793, "compression_ratio": 1.7677725118483412, "no_speech_prob": 0.00013976788613945246}, {"id": 504, "seek": 280008, "start": 2800.08, "end": 2803.92, "text": " world knowledge which I'll talk about a little bit later and that's a that's a very loose concept.", "tokens": [1002, 3601, 597, 286, 603, 751, 466, 257, 707, 857, 1780, 293, 300, 311, 257, 300, 311, 257, 588, 9612, 3410, 13], "temperature": 0.0, "avg_logprob": -0.0888534340203977, "compression_ratio": 1.9196787148594376, "no_speech_prob": 4.0056718717096373e-05}, {"id": 505, "seek": 280008, "start": 2805.36, "end": 2811.36, "text": " I think that for translation learning world knowledge is not very useful because everything", "tokens": [286, 519, 300, 337, 12853, 2539, 1002, 3601, 307, 406, 588, 4420, 570, 1203], "temperature": 0.0, "avg_logprob": -0.0888534340203977, "compression_ratio": 1.9196787148594376, "no_speech_prob": 4.0056718717096373e-05}, {"id": 506, "seek": 280008, "start": 2811.36, "end": 2816.64, "text": " all of the knowledge you need to translate a sentence for for the most part is in the sentence", "tokens": [439, 295, 264, 3601, 291, 643, 281, 13799, 257, 8174, 337, 337, 264, 881, 644, 307, 294, 264, 8174], "temperature": 0.0, "avg_logprob": -0.0888534340203977, "compression_ratio": 1.9196787148594376, "no_speech_prob": 4.0056718717096373e-05}, {"id": 507, "seek": 280008, "start": 2818.56, "end": 2823.84, "text": " and so basically all of the sort of contextual knowledge style information that you need to", "tokens": [293, 370, 1936, 439, 295, 264, 1333, 295, 35526, 3601, 3758, 1589, 300, 291, 643, 281], "temperature": 0.0, "avg_logprob": -0.0888534340203977, "compression_ratio": 1.9196787148594376, "no_speech_prob": 4.0056718717096373e-05}, {"id": 508, "seek": 280008, "start": 2823.84, "end": 2828.56, "text": " produce the German sentence is in the input sentence. So gaining world knowledge during pre-training", "tokens": [5258, 264, 6521, 8174, 307, 294, 264, 4846, 8174, 13, 407, 19752, 1002, 3601, 1830, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.0888534340203977, "compression_ratio": 1.9196787148594376, "no_speech_prob": 4.0056718717096373e-05}, {"id": 509, "seek": 282856, "start": 2828.56, "end": 2834.48, "text": " is not very useful. Of course it's useful to know what words mean but to a certain extent you", "tokens": [307, 406, 588, 4420, 13, 2720, 1164, 309, 311, 4420, 281, 458, 437, 2283, 914, 457, 281, 257, 1629, 8396, 291], "temperature": 0.0, "avg_logprob": -0.08064719084854964, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.963339885463938e-05}, {"id": 510, "seek": 282856, "start": 2834.48, "end": 2840.0, "text": " that's kind of the easiest signal to pick up on during training and I imagine that would be my guess", "tokens": [300, 311, 733, 295, 264, 12889, 6358, 281, 1888, 493, 322, 1830, 3097, 293, 286, 3811, 300, 576, 312, 452, 2041], "temperature": 0.0, "avg_logprob": -0.08064719084854964, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.963339885463938e-05}, {"id": 511, "seek": 282856, "start": 2840.0, "end": 2850.24, "text": " I don't have any rigorous proof of any of this. Thanks. Great so like I was saying we trained", "tokens": [286, 500, 380, 362, 604, 29882, 8177, 295, 604, 295, 341, 13, 2561, 13, 3769, 370, 411, 286, 390, 1566, 321, 8895], "temperature": 0.0, "avg_logprob": -0.08064719084854964, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.963339885463938e-05}, {"id": 512, "seek": 282856, "start": 2850.24, "end": 2856.48, "text": " this English only model and we wanted to address the major shortcoming that really only can", "tokens": [341, 3669, 787, 2316, 293, 321, 1415, 281, 2985, 264, 2563, 2099, 6590, 300, 534, 787, 393], "temperature": 0.0, "avg_logprob": -0.08064719084854964, "compression_ratio": 1.5573770491803278, "no_speech_prob": 7.963339885463938e-05}, {"id": 513, "seek": 285648, "start": 2856.48, "end": 2861.84, "text": " speak one language. So we introduced a model called MT5 multi-lingual T5 and really for the", "tokens": [1710, 472, 2856, 13, 407, 321, 7268, 257, 2316, 1219, 37333, 20, 4825, 12, 1688, 901, 314, 20, 293, 534, 337, 264], "temperature": 0.0, "avg_logprob": -0.11078317626183774, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.00010886997188208625}, {"id": 514, "seek": 285648, "start": 2861.84, "end": 2867.52, "text": " most part the if you remember one thing about MT5 it's basically that it's exactly the same model", "tokens": [881, 644, 264, 498, 291, 1604, 472, 551, 466, 37333, 20, 309, 311, 1936, 300, 309, 311, 2293, 264, 912, 2316], "temperature": 0.0, "avg_logprob": -0.11078317626183774, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.00010886997188208625}, {"id": 515, "seek": 285648, "start": 2867.52, "end": 2873.52, "text": " but trained on a multi-lingual corpus and the text to text format is the same you know we feed in", "tokens": [457, 8895, 322, 257, 4825, 12, 1688, 901, 1181, 31624, 293, 264, 2487, 281, 2487, 7877, 307, 264, 912, 291, 458, 321, 3154, 294], "temperature": 0.0, "avg_logprob": -0.11078317626183774, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.00010886997188208625}, {"id": 516, "seek": 285648, "start": 2873.52, "end": 2879.6, "text": " task prefixes but we can feed in content to different languages and we can do classification tasks", "tokens": [5633, 18417, 36005, 457, 321, 393, 3154, 294, 2701, 281, 819, 8650, 293, 321, 393, 360, 21538, 9608], "temperature": 0.0, "avg_logprob": -0.11078317626183774, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.00010886997188208625}, {"id": 517, "seek": 285648, "start": 2879.6, "end": 2884.96, "text": " we can do question answering tasks with MT5 in exactly the same way that we can do with T5.", "tokens": [321, 393, 360, 1168, 13430, 9608, 365, 37333, 20, 294, 2293, 264, 912, 636, 300, 321, 393, 360, 365, 314, 20, 13], "temperature": 0.0, "avg_logprob": -0.11078317626183774, "compression_ratio": 1.859922178988327, "no_speech_prob": 0.00010886997188208625}, {"id": 518, "seek": 288496, "start": 2884.96, "end": 2891.92, "text": " So like I said the pertinent thing about MT5 was creating a multi-lingual variant of C4.", "tokens": [407, 411, 286, 848, 264, 13269, 11058, 551, 466, 37333, 20, 390, 4084, 257, 4825, 12, 1688, 901, 17501, 295, 383, 19, 13], "temperature": 0.0, "avg_logprob": -0.10045708550347222, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00014649210788775235}, {"id": 519, "seek": 288496, "start": 2891.92, "end": 2897.84, "text": " Overall the process is very similar to the process we used for C4 except that it includes 101", "tokens": [18420, 264, 1399, 307, 588, 2531, 281, 264, 1399, 321, 1143, 337, 383, 19, 3993, 300, 309, 5974, 21055], "temperature": 0.0, "avg_logprob": -0.10045708550347222, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00014649210788775235}, {"id": 520, "seek": 288496, "start": 2897.84, "end": 2905.52, "text": " languages that we detected using a open source language detector. We also extracted data from", "tokens": [8650, 300, 321, 21896, 1228, 257, 1269, 4009, 2856, 25712, 13, 492, 611, 34086, 1412, 490], "temperature": 0.0, "avg_logprob": -0.10045708550347222, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00014649210788775235}, {"id": 521, "seek": 288496, "start": 2905.52, "end": 2909.52, "text": " more common crawl dumps because especially for the low resource languages it was hard to get enough", "tokens": [544, 2689, 24767, 11430, 82, 570, 2318, 337, 264, 2295, 7684, 8650, 309, 390, 1152, 281, 483, 1547], "temperature": 0.0, "avg_logprob": -0.10045708550347222, "compression_ratio": 1.7657992565055762, "no_speech_prob": 0.00014649210788775235}, {"id": 522, "seek": 290952, "start": 2909.52, "end": 2915.04, "text": " data from only one common crawl dump and you can see a list of the languages that we include here.", "tokens": [1412, 490, 787, 472, 2689, 24767, 11430, 293, 291, 393, 536, 257, 1329, 295, 264, 8650, 300, 321, 4090, 510, 13], "temperature": 0.0, "avg_logprob": -0.14843342282356473, "compression_ratio": 1.6892857142857143, "no_speech_prob": 2.9305288990144618e-05}, {"id": 523, "seek": 290952, "start": 2915.04, "end": 2921.44, "text": " Ultimately the data set ended up being about 27 terabytes in size. So here's a distribution of", "tokens": [23921, 264, 1412, 992, 4590, 493, 885, 466, 7634, 1796, 24538, 294, 2744, 13, 407, 510, 311, 257, 7316, 295], "temperature": 0.0, "avg_logprob": -0.14843342282356473, "compression_ratio": 1.6892857142857143, "no_speech_prob": 2.9305288990144618e-05}, {"id": 524, "seek": 290952, "start": 2921.44, "end": 2926.72, "text": " the number of pages in the MT4 training data set for various languages. You can see our", "tokens": [264, 1230, 295, 7183, 294, 264, 37333, 19, 3097, 1412, 992, 337, 3683, 8650, 13, 509, 393, 536, 527], "temperature": 0.0, "avg_logprob": -0.14843342282356473, "compression_ratio": 1.6892857142857143, "no_speech_prob": 2.9305288990144618e-05}, {"id": 525, "seek": 290952, "start": 2926.72, "end": 2931.52, "text": " most our highest resource language is English where we have about three billion pages with", "tokens": [881, 527, 6343, 7684, 2856, 307, 3669, 689, 321, 362, 466, 1045, 5218, 7183, 365], "temperature": 0.0, "avg_logprob": -0.14843342282356473, "compression_ratio": 1.6892857142857143, "no_speech_prob": 2.9305288990144618e-05}, {"id": 526, "seek": 290952, "start": 2931.52, "end": 2937.04, "text": " three trillion tokens total. The lowest resource language is your RUBA with only about 50,000 pages.", "tokens": [1045, 18723, 22667, 3217, 13, 440, 12437, 7684, 2856, 307, 428, 497, 52, 9295, 365, 787, 466, 2625, 11, 1360, 7183, 13], "temperature": 0.0, "avg_logprob": -0.14843342282356473, "compression_ratio": 1.6892857142857143, "no_speech_prob": 2.9305288990144618e-05}, {"id": 527, "seek": 293704, "start": 2937.04, "end": 2942.32, "text": " So you can see the amount of data that we got for each language varies by many orders of magnitude.", "tokens": [407, 291, 393, 536, 264, 2372, 295, 1412, 300, 321, 658, 337, 1184, 2856, 21716, 538, 867, 9470, 295, 15668, 13], "temperature": 0.0, "avg_logprob": -0.1340801841334293, "compression_ratio": 1.7951807228915662, "no_speech_prob": 5.648800652124919e-05}, {"id": 528, "seek": 293704, "start": 2944.24, "end": 2948.32, "text": " Because of that a common strategy is to use this sort of temperature scaling that I mentioned", "tokens": [1436, 295, 300, 257, 2689, 5206, 307, 281, 764, 341, 1333, 295, 4292, 21589, 300, 286, 2835], "temperature": 0.0, "avg_logprob": -0.1340801841334293, "compression_ratio": 1.7951807228915662, "no_speech_prob": 5.648800652124919e-05}, {"id": 529, "seek": 293704, "start": 2948.32, "end": 2953.2, "text": " earlier where basically you sample from data in a particular language by using", "tokens": [3071, 689, 1936, 291, 6889, 490, 1412, 294, 257, 1729, 2856, 538, 1228], "temperature": 0.0, "avg_logprob": -0.1340801841334293, "compression_ratio": 1.7951807228915662, "no_speech_prob": 5.648800652124919e-05}, {"id": 530, "seek": 293704, "start": 2954.08, "end": 2959.36, "text": " by scaling the number of examples in that language by a temperature. And as the", "tokens": [538, 21589, 264, 1230, 295, 5110, 294, 300, 2856, 538, 257, 4292, 13, 400, 382, 264], "temperature": 0.0, "avg_logprob": -0.1340801841334293, "compression_ratio": 1.7951807228915662, "no_speech_prob": 5.648800652124919e-05}, {"id": 531, "seek": 293704, "start": 2960.08, "end": 2966.24, "text": " I apologize that the temperature here is one over the temperature that I described previously.", "tokens": [286, 12328, 300, 264, 4292, 510, 307, 472, 670, 264, 4292, 300, 286, 7619, 8046, 13], "temperature": 0.0, "avg_logprob": -0.1340801841334293, "compression_ratio": 1.7951807228915662, "no_speech_prob": 5.648800652124919e-05}, {"id": 532, "seek": 296624, "start": 2966.24, "end": 2970.64, "text": " So in this case as the temperature gets smaller and smaller you get closer and closer to a uniform", "tokens": [407, 294, 341, 1389, 382, 264, 4292, 2170, 4356, 293, 4356, 291, 483, 4966, 293, 4966, 281, 257, 9452], "temperature": 0.0, "avg_logprob": -0.10334417954930719, "compression_ratio": 1.8656716417910448, "no_speech_prob": 5.474833233165555e-05}, {"id": 533, "seek": 296624, "start": 2970.64, "end": 2976.3999999999996, "text": " distribution. The net effect of this is that for very small temperatures you tend to do better on", "tokens": [7316, 13, 440, 2533, 1802, 295, 341, 307, 300, 337, 588, 1359, 12633, 291, 3928, 281, 360, 1101, 322], "temperature": 0.0, "avg_logprob": -0.10334417954930719, "compression_ratio": 1.8656716417910448, "no_speech_prob": 5.474833233165555e-05}, {"id": 534, "seek": 296624, "start": 2976.3999999999996, "end": 2981.3599999999997, "text": " downstream tasks on low resource languages like Urdu but as you increase the temperature so that you", "tokens": [30621, 9608, 322, 2295, 7684, 8650, 411, 9533, 769, 457, 382, 291, 3488, 264, 4292, 370, 300, 291], "temperature": 0.0, "avg_logprob": -0.10334417954930719, "compression_ratio": 1.8656716417910448, "no_speech_prob": 5.474833233165555e-05}, {"id": 535, "seek": 296624, "start": 2981.3599999999997, "end": 2985.7599999999998, "text": " get so you basically are doing examples proportional mixing of the different languages you do better", "tokens": [483, 370, 291, 1936, 366, 884, 5110, 24969, 11983, 295, 264, 819, 8650, 291, 360, 1101], "temperature": 0.0, "avg_logprob": -0.10334417954930719, "compression_ratio": 1.8656716417910448, "no_speech_prob": 5.474833233165555e-05}, {"id": 536, "seek": 296624, "start": 2985.7599999999998, "end": 2995.7599999999998, "text": " on high resource languages like Russian. So we took MT4 we pre-trained MT5 again basically everything", "tokens": [322, 1090, 7684, 8650, 411, 7220, 13, 407, 321, 1890, 37333, 19, 321, 659, 12, 17227, 2001, 37333, 20, 797, 1936, 1203], "temperature": 0.0, "avg_logprob": -0.10334417954930719, "compression_ratio": 1.8656716417910448, "no_speech_prob": 5.474833233165555e-05}, {"id": 537, "seek": 299576, "start": 2995.76, "end": 3000.5600000000004, "text": " was kept the same we made the vocabulary a bit bigger to accommodate all the different languages", "tokens": [390, 4305, 264, 912, 321, 1027, 264, 19864, 257, 857, 3801, 281, 21410, 439, 264, 819, 8650], "temperature": 0.0, "avg_logprob": -0.12291269386764121, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00014879908121656626}, {"id": 538, "seek": 299576, "start": 3000.5600000000004, "end": 3006.7200000000003, "text": " but overall the amount of pre-training the model sizes etc are basically the same. And ultimately", "tokens": [457, 4787, 264, 2372, 295, 659, 12, 17227, 1760, 264, 2316, 11602, 5183, 366, 1936, 264, 912, 13, 400, 6284], "temperature": 0.0, "avg_logprob": -0.12291269386764121, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00014879908121656626}, {"id": 539, "seek": 299576, "start": 3007.36, "end": 3011.5200000000004, "text": " got state of the art on some of the tasks in the extreme benchmark. You'll notice that we don't", "tokens": [658, 1785, 295, 264, 1523, 322, 512, 295, 264, 9608, 294, 264, 8084, 18927, 13, 509, 603, 3449, 300, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.12291269386764121, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00014879908121656626}, {"id": 540, "seek": 299576, "start": 3011.5200000000004, "end": 3017.6800000000003, "text": " report results for some of these tasks that's partially because extreme is designed for sentence", "tokens": [2275, 3542, 337, 512, 295, 613, 9608, 300, 311, 18886, 570, 8084, 307, 4761, 337, 8174], "temperature": 0.0, "avg_logprob": -0.12291269386764121, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00014879908121656626}, {"id": 541, "seek": 299576, "start": 3017.6800000000003, "end": 3025.28, "text": " encoders like Bert. T5 is an MT5 our encoder decoder models. We did not experiment with using the", "tokens": [2058, 378, 433, 411, 29594, 13, 314, 20, 307, 364, 37333, 20, 527, 2058, 19866, 979, 19866, 5245, 13, 492, 630, 406, 5120, 365, 1228, 264], "temperature": 0.0, "avg_logprob": -0.12291269386764121, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.00014879908121656626}, {"id": 542, "seek": 302528, "start": 3025.28, "end": 3029.6800000000003, "text": " encoder on its own but in order to attack some of these problems like the sentence retrieval", "tokens": [2058, 19866, 322, 1080, 1065, 457, 294, 1668, 281, 2690, 512, 295, 613, 2740, 411, 264, 8174, 19817, 3337], "temperature": 0.0, "avg_logprob": -0.09541481419613487, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.133181209908798e-05}, {"id": 543, "seek": 302528, "start": 3029.6800000000003, "end": 3035.36, "text": " problems you need a model that can output a single vector representation of your sequence", "tokens": [2740, 291, 643, 257, 2316, 300, 393, 5598, 257, 2167, 8062, 10290, 295, 428, 8310], "temperature": 0.0, "avg_logprob": -0.09541481419613487, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.133181209908798e-05}, {"id": 544, "seek": 302528, "start": 3035.92, "end": 3039.6000000000004, "text": " and we don't have that in T5 so we didn't apply it to those tasks.", "tokens": [293, 321, 500, 380, 362, 300, 294, 314, 20, 370, 321, 994, 380, 3079, 309, 281, 729, 9608, 13], "temperature": 0.0, "avg_logprob": -0.09541481419613487, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.133181209908798e-05}, {"id": 545, "seek": 302528, "start": 3042.0, "end": 3046.88, "text": " One interesting finding from this paper that I'll just quickly mention here is that the", "tokens": [1485, 1880, 5006, 490, 341, 3035, 300, 286, 603, 445, 2661, 2152, 510, 307, 300, 264], "temperature": 0.0, "avg_logprob": -0.09541481419613487, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.133181209908798e-05}, {"id": 546, "seek": 302528, "start": 3048.4, "end": 3053.44, "text": " there's there are basically multiple settings that people consider multilingual benchmarks.", "tokens": [456, 311, 456, 366, 1936, 3866, 6257, 300, 561, 1949, 2120, 38219, 43751, 13], "temperature": 0.0, "avg_logprob": -0.09541481419613487, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.133181209908798e-05}, {"id": 547, "seek": 305344, "start": 3053.44, "end": 3059.68, "text": " One is the case the zero shot case and in that case you don't do any pre-training on a language of", "tokens": [1485, 307, 264, 1389, 264, 4018, 3347, 1389, 293, 294, 300, 1389, 291, 500, 380, 360, 604, 659, 12, 17227, 1760, 322, 257, 2856, 295], "temperature": 0.0, "avg_logprob": -0.10033425714215662, "compression_ratio": 1.921259842519685, "no_speech_prob": 0.0001970916346181184}, {"id": 548, "seek": 305344, "start": 3060.32, "end": 3065.12, "text": " sorry you don't have any fine-tuning data on each particular language you only have pre-training", "tokens": [2597, 291, 500, 380, 362, 604, 2489, 12, 83, 37726, 1412, 322, 1184, 1729, 2856, 291, 787, 362, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.10033425714215662, "compression_ratio": 1.921259842519685, "no_speech_prob": 0.0001970916346181184}, {"id": 549, "seek": 305344, "start": 3065.12, "end": 3069.76, "text": " data on those languages. So you fine-tune let's say only in English and then you feed the model", "tokens": [1412, 322, 729, 8650, 13, 407, 291, 2489, 12, 83, 2613, 718, 311, 584, 787, 294, 3669, 293, 550, 291, 3154, 264, 2316], "temperature": 0.0, "avg_logprob": -0.10033425714215662, "compression_ratio": 1.921259842519685, "no_speech_prob": 0.0001970916346181184}, {"id": 550, "seek": 305344, "start": 3069.76, "end": 3074.56, "text": " some text in another language and see if it produces the right predictions. The next setting is the", "tokens": [512, 2487, 294, 1071, 2856, 293, 536, 498, 309, 14725, 264, 558, 21264, 13, 440, 958, 3287, 307, 264], "temperature": 0.0, "avg_logprob": -0.10033425714215662, "compression_ratio": 1.921259842519685, "no_speech_prob": 0.0001970916346181184}, {"id": 551, "seek": 305344, "start": 3074.56, "end": 3079.04, "text": " translate train setting that's where you take a machine translation model and translate the data", "tokens": [13799, 3847, 3287, 300, 311, 689, 291, 747, 257, 3479, 12853, 2316, 293, 13799, 264, 1412], "temperature": 0.0, "avg_logprob": -0.10033425714215662, "compression_ratio": 1.921259842519685, "no_speech_prob": 0.0001970916346181184}, {"id": 552, "seek": 307904, "start": 3079.04, "end": 3083.7599999999998, "text": " in the English fine-tuning corpus into different languages and then the last setting is the", "tokens": [294, 264, 3669, 2489, 12, 83, 37726, 1181, 31624, 666, 819, 8650, 293, 550, 264, 1036, 3287, 307, 264], "temperature": 0.0, "avg_logprob": -0.0843744637831202, "compression_ratio": 1.8093385214007782, "no_speech_prob": 6.400721031241119e-05}, {"id": 553, "seek": 307904, "start": 3083.7599999999998, "end": 3088.32, "text": " in-language multitask setting that's the setting where you assume that you have gold standard", "tokens": [294, 12, 25241, 20473, 42338, 3863, 3287, 300, 311, 264, 3287, 689, 291, 6552, 300, 291, 362, 3821, 3832], "temperature": 0.0, "avg_logprob": -0.0843744637831202, "compression_ratio": 1.8093385214007782, "no_speech_prob": 6.400721031241119e-05}, {"id": 554, "seek": 307904, "start": 3088.32, "end": 3093.2799999999997, "text": " ground-truth data in every language that you want the model to be able to process data in", "tokens": [2727, 12, 6903, 2910, 1412, 294, 633, 2856, 300, 291, 528, 264, 2316, 281, 312, 1075, 281, 1399, 1412, 294], "temperature": 0.0, "avg_logprob": -0.0843744637831202, "compression_ratio": 1.8093385214007782, "no_speech_prob": 6.400721031241119e-05}, {"id": 555, "seek": 307904, "start": 3093.92, "end": 3100.8, "text": " and the takeaway here actually is that the difference in performance between small models and our", "tokens": [293, 264, 30681, 510, 767, 307, 300, 264, 2649, 294, 3389, 1296, 1359, 5245, 293, 527], "temperature": 0.0, "avg_logprob": -0.0843744637831202, "compression_ratio": 1.8093385214007782, "no_speech_prob": 6.400721031241119e-05}, {"id": 556, "seek": 307904, "start": 3100.8, "end": 3107.84, "text": " largest model as we go along the x-axis is much much bigger for the zero shot and translate", "tokens": [6443, 2316, 382, 321, 352, 2051, 264, 2031, 12, 24633, 307, 709, 709, 3801, 337, 264, 4018, 3347, 293, 13799], "temperature": 0.0, "avg_logprob": -0.0843744637831202, "compression_ratio": 1.8093385214007782, "no_speech_prob": 6.400721031241119e-05}, {"id": 557, "seek": 310784, "start": 3107.84, "end": 3112.56, "text": " train settings than it is for the in-language multitask setting. So what this suggests to us is", "tokens": [3847, 6257, 813, 309, 307, 337, 264, 294, 12, 25241, 20473, 42338, 3863, 3287, 13, 407, 437, 341, 13409, 281, 505, 307], "temperature": 0.0, "avg_logprob": -0.10925630126336608, "compression_ratio": 1.7958333333333334, "no_speech_prob": 4.908021219307557e-05}, {"id": 558, "seek": 310784, "start": 3112.56, "end": 3120.1600000000003, "text": " basically that the model learns a much wider distribution of languages if it has a much larger", "tokens": [1936, 300, 264, 2316, 27152, 257, 709, 11842, 7316, 295, 8650, 498, 309, 575, 257, 709, 4833], "temperature": 0.0, "avg_logprob": -0.10925630126336608, "compression_ratio": 1.7958333333333334, "no_speech_prob": 4.908021219307557e-05}, {"id": 559, "seek": 310784, "start": 3120.1600000000003, "end": 3126.32, "text": " amount of parameters and it is able to do this kind of like zero shot task learning,", "tokens": [2372, 295, 9834, 293, 309, 307, 1075, 281, 360, 341, 733, 295, 411, 4018, 3347, 5633, 2539, 11], "temperature": 0.0, "avg_logprob": -0.10925630126336608, "compression_ratio": 1.7958333333333334, "no_speech_prob": 4.908021219307557e-05}, {"id": 560, "seek": 310784, "start": 3126.32, "end": 3130.0, "text": " multi-lingual task learning much better when it has more parameters.", "tokens": [4825, 12, 1688, 901, 5633, 2539, 709, 1101, 562, 309, 575, 544, 9834, 13], "temperature": 0.0, "avg_logprob": -0.10925630126336608, "compression_ratio": 1.7958333333333334, "no_speech_prob": 4.908021219307557e-05}, {"id": 561, "seek": 310784, "start": 3132.48, "end": 3137.1200000000003, "text": " So kind of along those lines you know larger models can maybe fit more knowledge about", "tokens": [407, 733, 295, 2051, 729, 3876, 291, 458, 4833, 5245, 393, 1310, 3318, 544, 3601, 466], "temperature": 0.0, "avg_logprob": -0.10925630126336608, "compression_ratio": 1.7958333333333334, "no_speech_prob": 4.908021219307557e-05}, {"id": 562, "seek": 313712, "start": 3137.12, "end": 3142.0, "text": " more languages. We had another paper where we basically tried to answer the question you know", "tokens": [544, 8650, 13, 492, 632, 1071, 3035, 689, 321, 1936, 3031, 281, 1867, 264, 1168, 291, 458], "temperature": 0.0, "avg_logprob": -0.08660186505785175, "compression_ratio": 1.8199233716475096, "no_speech_prob": 4.3314161302987486e-05}, {"id": 563, "seek": 313712, "start": 3142.0, "end": 3147.68, "text": " how much and what kind of knowledge does a model pick up during pre-training. And so to answer", "tokens": [577, 709, 293, 437, 733, 295, 3601, 775, 257, 2316, 1888, 493, 1830, 659, 12, 17227, 1760, 13, 400, 370, 281, 1867], "temperature": 0.0, "avg_logprob": -0.08660186505785175, "compression_ratio": 1.8199233716475096, "no_speech_prob": 4.3314161302987486e-05}, {"id": 564, "seek": 313712, "start": 3147.68, "end": 3153.68, "text": " that question we took a we basically introduced a new variant of the question answering task.", "tokens": [300, 1168, 321, 1890, 257, 321, 1936, 7268, 257, 777, 17501, 295, 264, 1168, 13430, 5633, 13], "temperature": 0.0, "avg_logprob": -0.08660186505785175, "compression_ratio": 1.8199233716475096, "no_speech_prob": 4.3314161302987486e-05}, {"id": 565, "seek": 313712, "start": 3153.68, "end": 3158.4, "text": " So the question answering task kind of comes in a couple of different flavors. The simplest flavor", "tokens": [407, 264, 1168, 13430, 5633, 733, 295, 1487, 294, 257, 1916, 295, 819, 16303, 13, 440, 22811, 6813], "temperature": 0.0, "avg_logprob": -0.08660186505785175, "compression_ratio": 1.8199233716475096, "no_speech_prob": 4.3314161302987486e-05}, {"id": 566, "seek": 313712, "start": 3158.4, "end": 3163.04, "text": " which I've mentioned already is reading comprehension and in that case the model is basically", "tokens": [597, 286, 600, 2835, 1217, 307, 3760, 44991, 293, 294, 300, 1389, 264, 2316, 307, 1936], "temperature": 0.0, "avg_logprob": -0.08660186505785175, "compression_ratio": 1.8199233716475096, "no_speech_prob": 4.3314161302987486e-05}, {"id": 567, "seek": 316304, "start": 3163.04, "end": 3167.6, "text": " given a paragraph or an article and then it's asked a question about the paragraph or article", "tokens": [2212, 257, 18865, 420, 364, 7222, 293, 550, 309, 311, 2351, 257, 1168, 466, 264, 18865, 420, 7222], "temperature": 0.0, "avg_logprob": -0.09366127296730324, "compression_ratio": 1.9691780821917808, "no_speech_prob": 1.9222390619688667e-05}, {"id": 568, "seek": 316304, "start": 3167.6, "end": 3172.56, "text": " and it has to basically extract the answer. So you can see if it's being asked what color is", "tokens": [293, 309, 575, 281, 1936, 8947, 264, 1867, 13, 407, 291, 393, 536, 498, 309, 311, 885, 2351, 437, 2017, 307], "temperature": 0.0, "avg_logprob": -0.09366127296730324, "compression_ratio": 1.9691780821917808, "no_speech_prob": 1.9222390619688667e-05}, {"id": 569, "seek": 316304, "start": 3172.56, "end": 3179.12, "text": " 11 it has to look in the in the in the paragraph that it's seen and see that it's the 11 has a yellow", "tokens": [2975, 309, 575, 281, 574, 294, 264, 294, 264, 294, 264, 18865, 300, 309, 311, 1612, 293, 536, 300, 309, 311, 264, 2975, 575, 257, 5566], "temperature": 0.0, "avg_logprob": -0.09366127296730324, "compression_ratio": 1.9691780821917808, "no_speech_prob": 1.9222390619688667e-05}, {"id": 570, "seek": 316304, "start": 3179.12, "end": 3183.36, "text": " fruit and output the word yellow. So this is kind of the simplest form of the question answering task.", "tokens": [6773, 293, 5598, 264, 1349, 5566, 13, 407, 341, 307, 733, 295, 264, 22811, 1254, 295, 264, 1168, 13430, 5633, 13], "temperature": 0.0, "avg_logprob": -0.09366127296730324, "compression_ratio": 1.9691780821917808, "no_speech_prob": 1.9222390619688667e-05}, {"id": 571, "seek": 316304, "start": 3184.08, "end": 3188.48, "text": " A more difficult form is what people call open domain question answering. And in that case you", "tokens": [316, 544, 2252, 1254, 307, 437, 561, 818, 1269, 9274, 1168, 13430, 13, 400, 294, 300, 1389, 291], "temperature": 0.0, "avg_logprob": -0.09366127296730324, "compression_ratio": 1.9691780821917808, "no_speech_prob": 1.9222390619688667e-05}, {"id": 572, "seek": 316304, "start": 3188.48, "end": 3192.96, "text": " assume that the model is given a question and has access to a large external database of", "tokens": [6552, 300, 264, 2316, 307, 2212, 257, 1168, 293, 575, 2105, 281, 257, 2416, 8320, 8149, 295], "temperature": 0.0, "avg_logprob": -0.09366127296730324, "compression_ratio": 1.9691780821917808, "no_speech_prob": 1.9222390619688667e-05}, {"id": 573, "seek": 319296, "start": 3192.96, "end": 3199.12, "text": " of knowledge maybe all of Wikipedia. So the model has to do two things. If first has to find the", "tokens": [295, 3601, 1310, 439, 295, 28999, 13, 407, 264, 2316, 575, 281, 360, 732, 721, 13, 759, 700, 575, 281, 915, 264], "temperature": 0.0, "avg_logprob": -0.06941710136554859, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.061562306858832e-05}, {"id": 574, "seek": 319296, "start": 3199.12, "end": 3204.96, "text": " article or the snippet of text that contains the answer in the database and then it has to", "tokens": [7222, 420, 264, 35623, 302, 295, 2487, 300, 8306, 264, 1867, 294, 264, 8149, 293, 550, 309, 575, 281], "temperature": 0.0, "avg_logprob": -0.06941710136554859, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.061562306858832e-05}, {"id": 575, "seek": 319296, "start": 3204.96, "end": 3210.4, "text": " extract the answer from the article. So there's this additional retrieval step that makes the problem", "tokens": [8947, 264, 1867, 490, 264, 7222, 13, 407, 456, 311, 341, 4497, 19817, 3337, 1823, 300, 1669, 264, 1154], "temperature": 0.0, "avg_logprob": -0.06941710136554859, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.061562306858832e-05}, {"id": 576, "seek": 319296, "start": 3210.4, "end": 3215.12, "text": " quite a bit harder. But we introduced a sort of third variant of question answering that we call", "tokens": [1596, 257, 857, 6081, 13, 583, 321, 7268, 257, 1333, 295, 2636, 17501, 295, 1168, 13430, 300, 321, 818], "temperature": 0.0, "avg_logprob": -0.06941710136554859, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.061562306858832e-05}, {"id": 577, "seek": 319296, "start": 3215.12, "end": 3220.96, "text": " closed book question answering the name takes inspiration from closed book exams. The goal here is", "tokens": [5395, 1446, 1168, 13430, 264, 1315, 2516, 10249, 490, 5395, 1446, 20514, 13, 440, 3387, 510, 307], "temperature": 0.0, "avg_logprob": -0.06941710136554859, "compression_ratio": 1.830188679245283, "no_speech_prob": 1.061562306858832e-05}, {"id": 578, "seek": 322096, "start": 3220.96, "end": 3225.52, "text": " that you just feed the model the question. It does not have access to an external knowledge source.", "tokens": [300, 291, 445, 3154, 264, 2316, 264, 1168, 13, 467, 775, 406, 362, 2105, 281, 364, 8320, 3601, 4009, 13], "temperature": 0.0, "avg_logprob": -0.10428386926651001, "compression_ratio": 1.8764044943820224, "no_speech_prob": 3.822196595137939e-05}, {"id": 579, "seek": 322096, "start": 3225.52, "end": 3230.64, "text": " It cannot look up information anywhere. It could only answer the question based on the knowledge that", "tokens": [467, 2644, 574, 493, 1589, 4992, 13, 467, 727, 787, 1867, 264, 1168, 2361, 322, 264, 3601, 300], "temperature": 0.0, "avg_logprob": -0.10428386926651001, "compression_ratio": 1.8764044943820224, "no_speech_prob": 3.822196595137939e-05}, {"id": 580, "seek": 322096, "start": 3230.64, "end": 3235.68, "text": " picked up during pre-training. So if you feed the model the question what colors a lemon it has to", "tokens": [6183, 493, 1830, 659, 12, 17227, 1760, 13, 407, 498, 291, 3154, 264, 2316, 264, 1168, 437, 4577, 257, 11356, 309, 575, 281], "temperature": 0.0, "avg_logprob": -0.10428386926651001, "compression_ratio": 1.8764044943820224, "no_speech_prob": 3.822196595137939e-05}, {"id": 581, "seek": 322096, "start": 3235.68, "end": 3241.36, "text": " output the model yellow correctly because it so so to speak knows that the that elements are yellow.", "tokens": [5598, 264, 2316, 5566, 8944, 570, 309, 370, 370, 281, 1710, 3255, 300, 264, 300, 4959, 366, 5566, 13], "temperature": 0.0, "avg_logprob": -0.10428386926651001, "compression_ratio": 1.8764044943820224, "no_speech_prob": 3.822196595137939e-05}, {"id": 582, "seek": 322096, "start": 3242.08, "end": 3248.2400000000002, "text": " So this is a good way we argue of testing the knowledge the amount of knowledge stored in the model", "tokens": [407, 341, 307, 257, 665, 636, 321, 9695, 295, 4997, 264, 3601, 264, 2372, 295, 3601, 12187, 294, 264, 2316], "temperature": 0.0, "avg_logprob": -0.10428386926651001, "compression_ratio": 1.8764044943820224, "no_speech_prob": 3.822196595137939e-05}, {"id": 583, "seek": 324824, "start": 3248.24, "end": 3254.64, "text": " during pre-training. So why would we expect that to work? Well you could imagine here we're doing", "tokens": [1830, 659, 12, 17227, 1760, 13, 407, 983, 576, 321, 2066, 300, 281, 589, 30, 1042, 291, 727, 3811, 510, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.13365738442603578, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.00013338896678760648}, {"id": 584, "seek": 324824, "start": 3254.64, "end": 3259.4399999999996, "text": " our normal pre-training of T5. We're masking out words and training the model to predict the", "tokens": [527, 2710, 659, 12, 17227, 1760, 295, 314, 20, 13, 492, 434, 31226, 484, 2283, 293, 3097, 264, 2316, 281, 6069, 264], "temperature": 0.0, "avg_logprob": -0.13365738442603578, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.00013338896678760648}, {"id": 585, "seek": 324824, "start": 3259.4399999999996, "end": 3265.9199999999996, "text": " masked out spans of words. And you might imagine that somewhere during pre-training it sees a", "tokens": [45249, 484, 44086, 295, 2283, 13, 400, 291, 1062, 3811, 300, 4079, 1830, 659, 12, 17227, 1760, 309, 8194, 257], "temperature": 0.0, "avg_logprob": -0.13365738442603578, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.00013338896678760648}, {"id": 586, "seek": 324824, "start": 3265.9199999999996, "end": 3271.3599999999997, "text": " sentence that says President Franklin Blank born blank January 1882. The goal here would be to", "tokens": [8174, 300, 1619, 3117, 22010, 2177, 657, 4232, 8247, 7061, 2443, 32848, 13, 440, 3387, 510, 576, 312, 281], "temperature": 0.0, "avg_logprob": -0.13365738442603578, "compression_ratio": 1.6336206896551724, "no_speech_prob": 0.00013338896678760648}, {"id": 587, "seek": 327136, "start": 3271.36, "end": 3278.8, "text": " output D. Roosevelt was blank in. And then during fine tuning we train the model to predict when the", "tokens": [5598, 413, 13, 28515, 390, 8247, 294, 13, 400, 550, 1830, 2489, 15164, 321, 3847, 264, 2316, 281, 6069, 562, 264], "temperature": 0.0, "avg_logprob": -0.11936364275343875, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.331056697992608e-05}, {"id": 588, "seek": 327136, "start": 3280.0, "end": 3285.52, "text": " year 1882 when it was asked the question when was Franklin D. Roosevelt born. And you might", "tokens": [1064, 2443, 32848, 562, 309, 390, 2351, 264, 1168, 562, 390, 22010, 413, 13, 28515, 4232, 13, 400, 291, 1062], "temperature": 0.0, "avg_logprob": -0.11936364275343875, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.331056697992608e-05}, {"id": 589, "seek": 327136, "start": 3285.52, "end": 3292.4, "text": " hope that it kind of recalls back to its pre-training task and recalls some knowledge that it picked", "tokens": [1454, 300, 309, 733, 295, 9901, 82, 646, 281, 1080, 659, 12, 17227, 1760, 5633, 293, 9901, 82, 512, 3601, 300, 309, 6183], "temperature": 0.0, "avg_logprob": -0.11936364275343875, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.331056697992608e-05}, {"id": 590, "seek": 327136, "start": 3292.4, "end": 3299.52, "text": " up during pre-training in order to answer this question correctly. So we took some standard open", "tokens": [493, 1830, 659, 12, 17227, 1760, 294, 1668, 281, 1867, 341, 1168, 8944, 13, 407, 321, 1890, 512, 3832, 1269], "temperature": 0.0, "avg_logprob": -0.11936364275343875, "compression_ratio": 1.6956521739130435, "no_speech_prob": 4.331056697992608e-05}, {"id": 591, "seek": 329952, "start": 3299.52, "end": 3305.68, "text": " domain question answering data sets, natural questions, web questions, and trivia QA. And basically", "tokens": [9274, 1168, 13430, 1412, 6352, 11, 3303, 1651, 11, 3670, 1651, 11, 293, 48770, 1249, 32, 13, 400, 1936], "temperature": 0.0, "avg_logprob": -0.10830056190490722, "compression_ratio": 1.7463768115942029, "no_speech_prob": 4.1331571992486715e-05}, {"id": 592, "seek": 329952, "start": 3305.68, "end": 3312.48, "text": " removed all of the context and trained our model to predict the correct answer when asked some", "tokens": [7261, 439, 295, 264, 4319, 293, 8895, 527, 2316, 281, 6069, 264, 3006, 1867, 562, 2351, 512], "temperature": 0.0, "avg_logprob": -0.10830056190490722, "compression_ratio": 1.7463768115942029, "no_speech_prob": 4.1331571992486715e-05}, {"id": 593, "seek": 329952, "start": 3312.48, "end": 3316.88, "text": " particular question and then evaluated its performance on the test set for each of these tasks.", "tokens": [1729, 1168, 293, 550, 25509, 1080, 3389, 322, 264, 1500, 992, 337, 1184, 295, 613, 9608, 13], "temperature": 0.0, "avg_logprob": -0.10830056190490722, "compression_ratio": 1.7463768115942029, "no_speech_prob": 4.1331571992486715e-05}, {"id": 594, "seek": 329952, "start": 3317.52, "end": 3321.92, "text": " And in this table we're comparing the state of the art results for an open domain system. These", "tokens": [400, 294, 341, 3199, 321, 434, 15763, 264, 1785, 295, 264, 1523, 3542, 337, 364, 1269, 9274, 1185, 13, 1981], "temperature": 0.0, "avg_logprob": -0.10830056190490722, "compression_ratio": 1.7463768115942029, "no_speech_prob": 4.1331571992486715e-05}, {"id": 595, "seek": 329952, "start": 3321.92, "end": 3327.44, "text": " are systems that explicitly retrieve knowledge from an external knowledge source compared to T5", "tokens": [366, 3652, 300, 20803, 30254, 3601, 490, 364, 8320, 3601, 4009, 5347, 281, 314, 20], "temperature": 0.0, "avg_logprob": -0.10830056190490722, "compression_ratio": 1.7463768115942029, "no_speech_prob": 4.1331571992486715e-05}, {"id": 596, "seek": 332744, "start": 3327.44, "end": 3332.2400000000002, "text": " when it's been trained in this closed book setting. You'll notice that we're actually using a", "tokens": [562, 309, 311, 668, 8895, 294, 341, 5395, 1446, 3287, 13, 509, 603, 3449, 300, 321, 434, 767, 1228, 257], "temperature": 0.0, "avg_logprob": -0.08255382475814199, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.00019406848878134042}, {"id": 597, "seek": 332744, "start": 3332.2400000000002, "end": 3339.04, "text": " slightly different version of T5 here using T5.1.1. The pertinent difference is just that T5.1.1", "tokens": [4748, 819, 3037, 295, 314, 20, 510, 1228, 314, 20, 13, 16, 13, 16, 13, 440, 13269, 11058, 2649, 307, 445, 300, 314, 20, 13, 16, 13, 16], "temperature": 0.0, "avg_logprob": -0.08255382475814199, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.00019406848878134042}, {"id": 598, "seek": 332744, "start": 3339.04, "end": 3344.4, "text": " was not multitask pre-trained. It was only pre-trained using an unsupervised objective. The", "tokens": [390, 406, 42338, 3863, 659, 12, 17227, 2001, 13, 467, 390, 787, 659, 12, 17227, 2001, 1228, 364, 2693, 12879, 24420, 10024, 13, 440], "temperature": 0.0, "avg_logprob": -0.08255382475814199, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.00019406848878134042}, {"id": 599, "seek": 332744, "start": 3344.4, "end": 3347.84, "text": " reason we did that again is because we want to measure the amount of knowledge that the model", "tokens": [1778, 321, 630, 300, 797, 307, 570, 321, 528, 281, 3481, 264, 2372, 295, 3601, 300, 264, 2316], "temperature": 0.0, "avg_logprob": -0.08255382475814199, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.00019406848878134042}, {"id": 600, "seek": 332744, "start": 3347.84, "end": 3353.28, "text": " picked up during pre-training. And you can see we actually got, you know, reasonably strong", "tokens": [6183, 493, 1830, 659, 12, 17227, 1760, 13, 400, 291, 393, 536, 321, 767, 658, 11, 291, 458, 11, 23551, 2068], "temperature": 0.0, "avg_logprob": -0.08255382475814199, "compression_ratio": 1.6895306859205776, "no_speech_prob": 0.00019406848878134042}, {"id": 601, "seek": 335328, "start": 3353.28, "end": 3364.0800000000004, "text": " performance, maybe respectable performance. I don't want to use the performance again, the accuracy", "tokens": [3389, 11, 1310, 44279, 3389, 13, 286, 500, 380, 528, 281, 764, 264, 3389, 797, 11, 264, 14170], "temperature": 0.0, "avg_logprob": -0.1385096009955349, "compression_ratio": 1.6379310344827587, "no_speech_prob": 2.3549619072582573e-05}, {"id": 602, "seek": 335328, "start": 3364.0800000000004, "end": 3369.44, "text": " basically on each of these data sets increases as the model size increases, which maybe at a", "tokens": [1936, 322, 1184, 295, 613, 1412, 6352, 8637, 382, 264, 2316, 2744, 8637, 11, 597, 1310, 412, 257], "temperature": 0.0, "avg_logprob": -0.1385096009955349, "compression_ratio": 1.6379310344827587, "no_speech_prob": 2.3549619072582573e-05}, {"id": 603, "seek": 335328, "start": 3369.44, "end": 3373.84, "text": " in a loose way suggests that the larger models have picked up more knowledge during pre-training.", "tokens": [294, 257, 9612, 636, 13409, 300, 264, 4833, 5245, 362, 6183, 493, 544, 3601, 1830, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.1385096009955349, "compression_ratio": 1.6379310344827587, "no_speech_prob": 2.3549619072582573e-05}, {"id": 604, "seek": 335328, "start": 3373.84, "end": 3377.84, "text": " But we ultimately lagged behind the state of the art results for open domain systems that", "tokens": [583, 321, 6284, 8953, 3004, 2261, 264, 1785, 295, 264, 1523, 3542, 337, 1269, 9274, 3652, 300], "temperature": 0.0, "avg_logprob": -0.1385096009955349, "compression_ratio": 1.6379310344827587, "no_speech_prob": 2.3549619072582573e-05}, {"id": 605, "seek": 337784, "start": 3377.84, "end": 3385.76, "text": " explicitly retrieve knowledge. So to try to close this gap, we made use of this objective called", "tokens": [20803, 30254, 3601, 13, 407, 281, 853, 281, 1998, 341, 7417, 11, 321, 1027, 764, 295, 341, 10024, 1219], "temperature": 0.0, "avg_logprob": -0.10397080452211442, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.8824291550554335e-05}, {"id": 606, "seek": 337784, "start": 3385.76, "end": 3391.6800000000003, "text": " salient span masking from a paper called retrieval augmented language model pre-training. And salient", "tokens": [1845, 1196, 16174, 31226, 490, 257, 3035, 1219, 19817, 3337, 36155, 2856, 2316, 659, 12, 17227, 1760, 13, 400, 1845, 1196], "temperature": 0.0, "avg_logprob": -0.10397080452211442, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.8824291550554335e-05}, {"id": 607, "seek": 337784, "start": 3391.6800000000003, "end": 3397.44, "text": " span masking is a very simple idea. The idea is that rather than masking outwards at random in your", "tokens": [16174, 31226, 307, 257, 588, 2199, 1558, 13, 440, 1558, 307, 300, 2831, 813, 31226, 484, 2015, 412, 4974, 294, 428], "temperature": 0.0, "avg_logprob": -0.10397080452211442, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.8824291550554335e-05}, {"id": 608, "seek": 337784, "start": 3397.44, "end": 3404.96, "text": " pre-training objective, you actually mask out entities explicitly. So that's people's names, places,", "tokens": [659, 12, 17227, 1760, 10024, 11, 291, 767, 6094, 484, 16667, 20803, 13, 407, 300, 311, 561, 311, 5288, 11, 3190, 11], "temperature": 0.0, "avg_logprob": -0.10397080452211442, "compression_ratio": 1.7272727272727273, "no_speech_prob": 3.8824291550554335e-05}, {"id": 609, "seek": 340496, "start": 3404.96, "end": 3411.76, "text": " dates, etc. And you basically just use an off the shelf named end-to-recognizere to figure out", "tokens": [11691, 11, 5183, 13, 400, 291, 1936, 445, 764, 364, 766, 264, 15222, 4926, 917, 12, 1353, 12, 13867, 2912, 590, 323, 281, 2573, 484], "temperature": 0.0, "avg_logprob": -0.13502945005893707, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0001272859371965751}, {"id": 610, "seek": 340496, "start": 3411.76, "end": 3417.52, "text": " what entities are in your pre-training data set. And you train the model to fill in salient", "tokens": [437, 16667, 366, 294, 428, 659, 12, 17227, 1760, 1412, 992, 13, 400, 291, 3847, 264, 2316, 281, 2836, 294, 1845, 1196], "temperature": 0.0, "avg_logprob": -0.13502945005893707, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0001272859371965751}, {"id": 611, "seek": 340496, "start": 3417.52, "end": 3424.32, "text": " spans instead of random spans. So what we did is we took T5.1.1 after it was pre-trained and did", "tokens": [44086, 2602, 295, 4974, 44086, 13, 407, 437, 321, 630, 307, 321, 1890, 314, 20, 13, 16, 13, 16, 934, 309, 390, 659, 12, 17227, 2001, 293, 630], "temperature": 0.0, "avg_logprob": -0.13502945005893707, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0001272859371965751}, {"id": 612, "seek": 340496, "start": 3424.32, "end": 3429.2, "text": " continued pre-training on salient span masking and then measured the performance on our downstream", "tokens": [7014, 659, 12, 17227, 1760, 322, 1845, 1196, 16174, 31226, 293, 550, 12690, 264, 3389, 322, 527, 30621], "temperature": 0.0, "avg_logprob": -0.13502945005893707, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0001272859371965751}, {"id": 613, "seek": 340496, "start": 3429.2, "end": 3433.84, "text": " tasks after fine tuning. And you can see that the more salient span mask pre-training we did,", "tokens": [9608, 934, 2489, 15164, 13, 400, 291, 393, 536, 300, 264, 544, 1845, 1196, 16174, 6094, 659, 12, 17227, 1760, 321, 630, 11], "temperature": 0.0, "avg_logprob": -0.13502945005893707, "compression_ratio": 1.8307692307692307, "no_speech_prob": 0.0001272859371965751}, {"id": 614, "seek": 343384, "start": 3433.84, "end": 3439.76, "text": " the better, excuse me, the better and better the performance got when we fine tune on the downstream", "tokens": [264, 1101, 11, 8960, 385, 11, 264, 1101, 293, 1101, 264, 3389, 658, 562, 321, 2489, 10864, 322, 264, 30621], "temperature": 0.0, "avg_logprob": -0.1358607644620149, "compression_ratio": 1.5877551020408163, "no_speech_prob": 9.760365355759859e-05}, {"id": 615, "seek": 343384, "start": 3439.76, "end": 3445.76, "text": " tasks. And we ultimately were able to close some of these gaps significantly and actually outperform", "tokens": [9608, 13, 400, 321, 6284, 645, 1075, 281, 1998, 512, 295, 613, 15031, 10591, 293, 767, 484, 26765], "temperature": 0.0, "avg_logprob": -0.1358607644620149, "compression_ratio": 1.5877551020408163, "no_speech_prob": 9.760365355759859e-05}, {"id": 616, "seek": 343384, "start": 3445.76, "end": 3452.7200000000003, "text": " the best open domain system on web questions by adding salient span masking to T5.1.1.", "tokens": [264, 1151, 1269, 9274, 1185, 322, 3670, 1651, 538, 5127, 1845, 1196, 16174, 31226, 281, 314, 20, 13, 16, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.1358607644620149, "compression_ratio": 1.5877551020408163, "no_speech_prob": 9.760365355759859e-05}, {"id": 617, "seek": 343384, "start": 3454.56, "end": 3461.84, "text": " So this just is a message to tell you that the objective matters. And doing this kind of now people,", "tokens": [407, 341, 445, 307, 257, 3636, 281, 980, 291, 300, 264, 10024, 7001, 13, 400, 884, 341, 733, 295, 586, 561, 11], "temperature": 0.0, "avg_logprob": -0.1358607644620149, "compression_ratio": 1.5877551020408163, "no_speech_prob": 9.760365355759859e-05}, {"id": 618, "seek": 346184, "start": 3461.84, "end": 3465.92, "text": " at the time people didn't call it this, but now people call this domain adaptive or task adaptive", "tokens": [412, 264, 565, 561, 994, 380, 818, 309, 341, 11, 457, 586, 561, 818, 341, 9274, 27912, 420, 5633, 27912], "temperature": 0.0, "avg_logprob": -0.13900609369631167, "compression_ratio": 1.6452830188679246, "no_speech_prob": 8.886240539140999e-05}, {"id": 619, "seek": 346184, "start": 3465.92, "end": 3470.8, "text": " pre-training. And this is a good way of getting better performance on your downstream tasks.", "tokens": [659, 12, 17227, 1760, 13, 400, 341, 307, 257, 665, 636, 295, 1242, 1101, 3389, 322, 428, 30621, 9608, 13], "temperature": 0.0, "avg_logprob": -0.13900609369631167, "compression_ratio": 1.6452830188679246, "no_speech_prob": 8.886240539140999e-05}, {"id": 620, "seek": 346184, "start": 3472.56, "end": 3476.6400000000003, "text": " So I've got a good set of questions here. It's now a good time.", "tokens": [407, 286, 600, 658, 257, 665, 992, 295, 1651, 510, 13, 467, 311, 586, 257, 665, 565, 13], "temperature": 0.0, "avg_logprob": -0.13900609369631167, "compression_ratio": 1.6452830188679246, "no_speech_prob": 8.886240539140999e-05}, {"id": 621, "seek": 346184, "start": 3478.08, "end": 3483.2000000000003, "text": " Yes, that'd be great. So for some context, the students in their most recent assignment", "tokens": [1079, 11, 300, 1116, 312, 869, 13, 407, 337, 512, 4319, 11, 264, 1731, 294, 641, 881, 5162, 15187], "temperature": 0.0, "avg_logprob": -0.13900609369631167, "compression_ratio": 1.6452830188679246, "no_speech_prob": 8.886240539140999e-05}, {"id": 622, "seek": 346184, "start": 3483.76, "end": 3489.36, "text": " had to make effectively a mini T5 thing were the only questions that were asked from a simple", "tokens": [632, 281, 652, 8659, 257, 8382, 314, 20, 551, 645, 264, 787, 1651, 300, 645, 2351, 490, 257, 2199], "temperature": 0.0, "avg_logprob": -0.13900609369631167, "compression_ratio": 1.6452830188679246, "no_speech_prob": 8.886240539140999e-05}, {"id": 623, "seek": 348936, "start": 3489.36, "end": 3494.1600000000003, "text": " domain so that they could pre-traine on a single GPU. And one of the questions they have is,", "tokens": [9274, 370, 300, 436, 727, 659, 12, 17227, 533, 322, 257, 2167, 18407, 13, 400, 472, 295, 264, 1651, 436, 362, 307, 11], "temperature": 0.0, "avg_logprob": -0.13377707895606455, "compression_ratio": 1.5311203319502074, "no_speech_prob": 7.600471144542098e-05}, {"id": 624, "seek": 348936, "start": 3494.88, "end": 3499.92, "text": " how can we be sure that the answer produced by the model is not made up? They were asked", "tokens": [577, 393, 321, 312, 988, 300, 264, 1867, 7126, 538, 264, 2316, 307, 406, 1027, 493, 30, 814, 645, 2351], "temperature": 0.0, "avg_logprob": -0.13377707895606455, "compression_ratio": 1.5311203319502074, "no_speech_prob": 7.600471144542098e-05}, {"id": 625, "seek": 348936, "start": 3499.92, "end": 3508.6400000000003, "text": " this on the assignment as well, I think. So if you don't have access to a ground-treat answer,", "tokens": [341, 322, 264, 15187, 382, 731, 11, 286, 519, 13, 407, 498, 291, 500, 380, 362, 2105, 281, 257, 2727, 12, 83, 620, 1867, 11], "temperature": 0.0, "avg_logprob": -0.13377707895606455, "compression_ratio": 1.5311203319502074, "no_speech_prob": 7.600471144542098e-05}, {"id": 626, "seek": 348936, "start": 3508.6400000000003, "end": 3513.6800000000003, "text": " it's actually very hard to know. There's a nice paper that came out after this paper called,", "tokens": [309, 311, 767, 588, 1152, 281, 458, 13, 821, 311, 257, 1481, 3035, 300, 1361, 484, 934, 341, 3035, 1219, 11], "temperature": 0.0, "avg_logprob": -0.13377707895606455, "compression_ratio": 1.5311203319502074, "no_speech_prob": 7.600471144542098e-05}, {"id": 627, "seek": 351368, "start": 3513.68, "end": 3519.44, "text": " how can we know when language models know? And what the goal of that paper is to make it so that T5", "tokens": [577, 393, 321, 458, 562, 2856, 5245, 458, 30, 400, 437, 264, 3387, 295, 300, 3035, 307, 281, 652, 309, 370, 300, 314, 20], "temperature": 0.0, "avg_logprob": -0.10288688341776529, "compression_ratio": 1.8759398496240602, "no_speech_prob": 6.603877409361303e-05}, {"id": 628, "seek": 351368, "start": 3519.44, "end": 3524.64, "text": " is what we call well calibrated. And when a model is well calibrated, it means that when it doesn't", "tokens": [307, 437, 321, 818, 731, 21583, 5468, 13, 400, 562, 257, 2316, 307, 731, 21583, 5468, 11, 309, 1355, 300, 562, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.10288688341776529, "compression_ratio": 1.8759398496240602, "no_speech_prob": 6.603877409361303e-05}, {"id": 629, "seek": 351368, "start": 3524.64, "end": 3530.48, "text": " know the answer, it doesn't output highly confident predictions. And this paper explored various", "tokens": [458, 264, 1867, 11, 309, 1177, 380, 5598, 5405, 6679, 21264, 13, 400, 341, 3035, 24016, 3683], "temperature": 0.0, "avg_logprob": -0.10288688341776529, "compression_ratio": 1.8759398496240602, "no_speech_prob": 6.603877409361303e-05}, {"id": 630, "seek": 351368, "start": 3530.48, "end": 3535.6, "text": " ways of calibrating T5 for close-up question answering. And they ultimately found that when the model", "tokens": [2098, 295, 2104, 6414, 990, 314, 20, 337, 1998, 12, 1010, 1168, 13430, 13, 400, 436, 6284, 1352, 300, 562, 264, 2316], "temperature": 0.0, "avg_logprob": -0.10288688341776529, "compression_ratio": 1.8759398496240602, "no_speech_prob": 6.603877409361303e-05}, {"id": 631, "seek": 351368, "start": 3535.6, "end": 3541.52, "text": " doesn't know the answer, when it's outputting something made up, that they could effectively make it", "tokens": [1177, 380, 458, 264, 1867, 11, 562, 309, 311, 5598, 783, 746, 1027, 493, 11, 300, 436, 727, 8659, 652, 309], "temperature": 0.0, "avg_logprob": -0.10288688341776529, "compression_ratio": 1.8759398496240602, "no_speech_prob": 6.603877409361303e-05}, {"id": 632, "seek": 354152, "start": 3541.52, "end": 3547.6, "text": " be very unconfident in its predictions. So that's one way to do it.", "tokens": [312, 588, 517, 24697, 1078, 294, 1080, 21264, 13, 407, 300, 311, 472, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1804713199013158, "compression_ratio": 1.5829596412556053, "no_speech_prob": 5.9193222114117816e-05}, {"id": 633, "seek": 354152, "start": 3551.44, "end": 3557.68, "text": " I think you're actually our muted, John. Great. Thanks. And then another question is the knowledge", "tokens": [286, 519, 291, 434, 767, 527, 32808, 11, 2619, 13, 3769, 13, 2561, 13, 400, 550, 1071, 1168, 307, 264, 3601], "temperature": 0.0, "avg_logprob": -0.1804713199013158, "compression_ratio": 1.5829596412556053, "no_speech_prob": 5.9193222114117816e-05}, {"id": 634, "seek": 354152, "start": 3557.68, "end": 3563.04, "text": " that's necessary for doing these fine-tuning, like the QA on these fine-tuning datasets.", "tokens": [300, 311, 4818, 337, 884, 613, 2489, 12, 83, 37726, 11, 411, 264, 1249, 32, 322, 613, 2489, 12, 83, 37726, 42856, 13], "temperature": 0.0, "avg_logprob": -0.1804713199013158, "compression_ratio": 1.5829596412556053, "no_speech_prob": 5.9193222114117816e-05}, {"id": 635, "seek": 354152, "start": 3564.4, "end": 3570.8, "text": " Is it knowledge that is all present at pre-training time? Yeah, so that's also something that was", "tokens": [1119, 309, 3601, 300, 307, 439, 1974, 412, 659, 12, 17227, 1760, 565, 30, 865, 11, 370, 300, 311, 611, 746, 300, 390], "temperature": 0.0, "avg_logprob": -0.1804713199013158, "compression_ratio": 1.5829596412556053, "no_speech_prob": 5.9193222114117816e-05}, {"id": 636, "seek": 357080, "start": 3570.8, "end": 3575.04, "text": " explored by a subsequent paper. We're shown that actually there is a decent amount of", "tokens": [24016, 538, 257, 19962, 3035, 13, 492, 434, 4898, 300, 767, 456, 307, 257, 8681, 2372, 295], "temperature": 0.0, "avg_logprob": -0.10878845702770144, "compression_ratio": 1.605263157894737, "no_speech_prob": 6.604017835343257e-05}, {"id": 637, "seek": 357080, "start": 3575.04, "end": 3580.0800000000004, "text": " training and test overlap in terms of knowledge in these datasets. So it's definitely possible in", "tokens": [3097, 293, 1500, 19959, 294, 2115, 295, 3601, 294, 613, 42856, 13, 407, 309, 311, 2138, 1944, 294], "temperature": 0.0, "avg_logprob": -0.10878845702770144, "compression_ratio": 1.605263157894737, "no_speech_prob": 6.604017835343257e-05}, {"id": 638, "seek": 357080, "start": 3580.0800000000004, "end": 3585.76, "text": " these cases that that the model is picking up knowledge during fine-tuning and not pre-training.", "tokens": [613, 3331, 300, 300, 264, 2316, 307, 8867, 493, 3601, 1830, 2489, 12, 83, 37726, 293, 406, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.10878845702770144, "compression_ratio": 1.605263157894737, "no_speech_prob": 6.604017835343257e-05}, {"id": 639, "seek": 357080, "start": 3585.76, "end": 3595.6000000000004, "text": " However, just as kind of a side experimental note, we find that the performance of T5", "tokens": [2908, 11, 445, 382, 733, 295, 257, 1252, 17069, 3637, 11, 321, 915, 300, 264, 3389, 295, 314, 20], "temperature": 0.0, "avg_logprob": -0.10878845702770144, "compression_ratio": 1.605263157894737, "no_speech_prob": 6.604017835343257e-05}, {"id": 640, "seek": 359560, "start": 3595.6, "end": 3601.7599999999998, "text": " actually plateaus before it makes a single pass over the fine-tuning dataset. So basically T5", "tokens": [767, 5924, 8463, 949, 309, 1669, 257, 2167, 1320, 670, 264, 2489, 12, 83, 37726, 28872, 13, 407, 1936, 314, 20], "temperature": 0.0, "avg_logprob": -0.11639132292374321, "compression_ratio": 1.6691176470588236, "no_speech_prob": 8.479353709844872e-05}, {"id": 641, "seek": 359560, "start": 3601.7599999999998, "end": 3605.44, "text": " will very, very quickly figure out what the heck you're trying to get it to do. It doesn't even", "tokens": [486, 588, 11, 588, 2661, 2573, 484, 437, 264, 12872, 291, 434, 1382, 281, 483, 309, 281, 360, 13, 467, 1177, 380, 754], "temperature": 0.0, "avg_logprob": -0.11639132292374321, "compression_ratio": 1.6691176470588236, "no_speech_prob": 8.479353709844872e-05}, {"id": 642, "seek": 359560, "start": 3605.44, "end": 3610.48, "text": " see the full training set before it gets, basically it's maximum performance on the test set.", "tokens": [536, 264, 1577, 3097, 992, 949, 309, 2170, 11, 1936, 309, 311, 6674, 3389, 322, 264, 1500, 992, 13], "temperature": 0.0, "avg_logprob": -0.11639132292374321, "compression_ratio": 1.6691176470588236, "no_speech_prob": 8.479353709844872e-05}, {"id": 643, "seek": 359560, "start": 3610.48, "end": 3614.56, "text": " So we don't actually think that that's a major factor for for these results.", "tokens": [407, 321, 500, 380, 767, 519, 300, 300, 311, 257, 2563, 5952, 337, 337, 613, 3542, 13], "temperature": 0.0, "avg_logprob": -0.11639132292374321, "compression_ratio": 1.6691176470588236, "no_speech_prob": 8.479353709844872e-05}, {"id": 644, "seek": 359560, "start": 3615.2799999999997, "end": 3619.7599999999998, "text": " Great. And then last question, how did the if you studied it, how did the multitask model do?", "tokens": [3769, 13, 400, 550, 1036, 1168, 11, 577, 630, 264, 498, 291, 9454, 309, 11, 577, 630, 264, 42338, 3863, 2316, 360, 30], "temperature": 0.0, "avg_logprob": -0.11639132292374321, "compression_ratio": 1.6691176470588236, "no_speech_prob": 8.479353709844872e-05}, {"id": 645, "seek": 361976, "start": 3619.76, "end": 3626.4, "text": " Yeah, those results are in the paper. The results are almost exactly the same. It's a little easier", "tokens": [865, 11, 729, 3542, 366, 294, 264, 3035, 13, 440, 3542, 366, 1920, 2293, 264, 912, 13, 467, 311, 257, 707, 3571], "temperature": 0.0, "avg_logprob": -0.11437455052914827, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.00010551881860010326}, {"id": 646, "seek": 361976, "start": 3626.4, "end": 3631.84, "text": " to explain the whole knowledge pre-training thing when you are talking about T5.1.1.", "tokens": [281, 2903, 264, 1379, 3601, 659, 12, 17227, 1760, 551, 562, 291, 366, 1417, 466, 314, 20, 13, 16, 13, 16, 13], "temperature": 0.0, "avg_logprob": -0.11437455052914827, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.00010551881860010326}, {"id": 647, "seek": 361976, "start": 3632.96, "end": 3639.36, "text": " In the interest of time, I might skip these next three slides, which are the short summary of", "tokens": [682, 264, 1179, 295, 565, 11, 286, 1062, 10023, 613, 958, 1045, 9788, 11, 597, 366, 264, 2099, 12691, 295], "temperature": 0.0, "avg_logprob": -0.11437455052914827, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.00010551881860010326}, {"id": 648, "seek": 361976, "start": 3639.36, "end": 3644.8, "text": " these slides is just that the evaluation procedure that we use is unfairly penalizes close book", "tokens": [613, 9788, 307, 445, 300, 264, 13344, 10747, 300, 321, 764, 307, 17019, 356, 13661, 5660, 1998, 1446], "temperature": 0.0, "avg_logprob": -0.11437455052914827, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.00010551881860010326}, {"id": 649, "seek": 361976, "start": 3644.8, "end": 3648.6400000000003, "text": " question answering systems. If you want to learn a bit more about that, you can poke into the paper", "tokens": [1168, 13430, 3652, 13, 759, 291, 528, 281, 1466, 257, 857, 544, 466, 300, 11, 291, 393, 19712, 666, 264, 3035], "temperature": 0.0, "avg_logprob": -0.11437455052914827, "compression_ratio": 1.651567944250871, "no_speech_prob": 0.00010551881860010326}, {"id": 650, "seek": 364864, "start": 3648.64, "end": 3655.8399999999997, "text": " a bit. It doesn't really support the main point that I'm trying to make in any meaningful way.", "tokens": [257, 857, 13, 467, 1177, 380, 534, 1406, 264, 2135, 935, 300, 286, 478, 1382, 281, 652, 294, 604, 10995, 636, 13], "temperature": 0.0, "avg_logprob": -0.11299221402122861, "compression_ratio": 1.615702479338843, "no_speech_prob": 9.31302274693735e-05}, {"id": 651, "seek": 364864, "start": 3655.8399999999997, "end": 3660.64, "text": " So, and I want to get to some of the more recent papers that and I should be able to have time to do", "tokens": [407, 11, 293, 286, 528, 281, 483, 281, 512, 295, 264, 544, 5162, 10577, 300, 293, 286, 820, 312, 1075, 281, 362, 565, 281, 360], "temperature": 0.0, "avg_logprob": -0.11299221402122861, "compression_ratio": 1.615702479338843, "no_speech_prob": 9.31302274693735e-05}, {"id": 652, "seek": 364864, "start": 3660.64, "end": 3669.12, "text": " that. Cool. So we kind of answered this question, you know, how much knowledge does a model pick up", "tokens": [300, 13, 8561, 13, 407, 321, 733, 295, 10103, 341, 1168, 11, 291, 458, 11, 577, 709, 3601, 775, 257, 2316, 1888, 493], "temperature": 0.0, "avg_logprob": -0.11299221402122861, "compression_ratio": 1.615702479338843, "no_speech_prob": 9.31302274693735e-05}, {"id": 653, "seek": 364864, "start": 3669.12, "end": 3674.16, "text": " during pre-training? The answer is arguably a lot. So kind of a follow-up question is, does the", "tokens": [1830, 659, 12, 17227, 1760, 30, 440, 1867, 307, 26771, 257, 688, 13, 407, 733, 295, 257, 1524, 12, 1010, 1168, 307, 11, 775, 264], "temperature": 0.0, "avg_logprob": -0.11299221402122861, "compression_ratio": 1.615702479338843, "no_speech_prob": 9.31302274693735e-05}, {"id": 654, "seek": 367416, "start": 3674.16, "end": 3678.72, "text": " model is kind of memorizing this knowledge, right? But does it also memorize, do large language", "tokens": [2316, 307, 733, 295, 10560, 3319, 341, 3601, 11, 558, 30, 583, 775, 309, 611, 27478, 11, 360, 2416, 2856], "temperature": 0.0, "avg_logprob": -0.10287851147947058, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.00010888135875575244}, {"id": 655, "seek": 367416, "start": 3678.72, "end": 3683.92, "text": " models also memorize stuff that we don't want them to memorize, specifically like private data?", "tokens": [5245, 611, 27478, 1507, 300, 321, 500, 380, 528, 552, 281, 27478, 11, 4682, 411, 4551, 1412, 30], "temperature": 0.0, "avg_logprob": -0.10287851147947058, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.00010888135875575244}, {"id": 656, "seek": 367416, "start": 3683.92, "end": 3688.96, "text": " Like you could imagine, let's say that somewhere in C4 is someone's social security number.", "tokens": [1743, 291, 727, 3811, 11, 718, 311, 584, 300, 4079, 294, 383, 19, 307, 1580, 311, 2093, 3825, 1230, 13], "temperature": 0.0, "avg_logprob": -0.10287851147947058, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.00010888135875575244}, {"id": 657, "seek": 367416, "start": 3689.52, "end": 3694.64, "text": " We probably don't want our model to memorize that and spit it out when we're decoding from it.", "tokens": [492, 1391, 500, 380, 528, 527, 2316, 281, 27478, 300, 293, 22127, 309, 484, 562, 321, 434, 979, 8616, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.10287851147947058, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.00010888135875575244}, {"id": 658, "seek": 367416, "start": 3695.52, "end": 3700.08, "text": " And we certainly don't want it to happen for unconditional models like GPT2 or GPT3.", "tokens": [400, 321, 3297, 500, 380, 528, 309, 281, 1051, 337, 47916, 5245, 411, 26039, 51, 17, 420, 26039, 51, 18, 13], "temperature": 0.0, "avg_logprob": -0.10287851147947058, "compression_ratio": 1.6775362318840579, "no_speech_prob": 0.00010888135875575244}, {"id": 659, "seek": 370008, "start": 3700.08, "end": 3706.3199999999997, "text": " So in this next work, we try to answer this question, you know, do large language models memorize stuff", "tokens": [407, 294, 341, 958, 589, 11, 321, 853, 281, 1867, 341, 1168, 11, 291, 458, 11, 360, 2416, 2856, 5245, 27478, 1507], "temperature": 0.0, "avg_logprob": -0.15425751686096192, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00014650657249148935}, {"id": 660, "seek": 370008, "start": 3706.3199999999997, "end": 3713.52, "text": " from their pre-training data set? And we can first actually turn to experts and see what experts think.", "tokens": [490, 641, 659, 12, 17227, 1760, 1412, 992, 30, 400, 321, 393, 700, 767, 1261, 281, 8572, 293, 536, 437, 8572, 519, 13], "temperature": 0.0, "avg_logprob": -0.15425751686096192, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00014650657249148935}, {"id": 661, "seek": 370008, "start": 3713.52, "end": 3720.7999999999997, "text": " And here are two statements made by the EFF and OpenAI. There was sent to the US Patent and Trademark", "tokens": [400, 510, 366, 732, 12363, 1027, 538, 264, 462, 6345, 293, 7238, 48698, 13, 821, 390, 2279, 281, 264, 2546, 4379, 317, 293, 22017, 26721], "temperature": 0.0, "avg_logprob": -0.15425751686096192, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00014650657249148935}, {"id": 662, "seek": 370008, "start": 3720.7999999999997, "end": 3725.36, "text": " Office when there were a call for comments on, on basically exactly this question. And you can see", "tokens": [8935, 562, 456, 645, 257, 818, 337, 3053, 322, 11, 322, 1936, 2293, 341, 1168, 13, 400, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.15425751686096192, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.00014650657249148935}, {"id": 663, "seek": 372536, "start": 3725.36, "end": 3732.32, "text": " that in both cases, these organizations basically say, you know, there's basically no reason to", "tokens": [300, 294, 1293, 3331, 11, 613, 6150, 1936, 584, 11, 291, 458, 11, 456, 311, 1936, 572, 1778, 281], "temperature": 0.0, "avg_logprob": -0.10337493337433913, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.0001441986096324399}, {"id": 664, "seek": 372536, "start": 3732.32, "end": 3737.76, "text": " believe that a large language model would output, would copy data from its training data set.", "tokens": [1697, 300, 257, 2416, 2856, 2316, 576, 5598, 11, 576, 5055, 1412, 490, 1080, 3097, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.10337493337433913, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.0001441986096324399}, {"id": 665, "seek": 372536, "start": 3738.56, "end": 3743.76, "text": " OpenAI, you know, kind of calls this a well-constructed AI system. And I think what they actually", "tokens": [7238, 48698, 11, 291, 458, 11, 733, 295, 5498, 341, 257, 731, 12, 25279, 1757, 292, 7318, 1185, 13, 400, 286, 519, 437, 436, 767], "temperature": 0.0, "avg_logprob": -0.10337493337433913, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.0001441986096324399}, {"id": 666, "seek": 372536, "start": 3743.76, "end": 3747.1200000000003, "text": " mean by that, if you read their statement a little longer, they kind of say, you know, if you", "tokens": [914, 538, 300, 11, 498, 291, 1401, 641, 5629, 257, 707, 2854, 11, 436, 733, 295, 584, 11, 291, 458, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.10337493337433913, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.0001441986096324399}, {"id": 667, "seek": 372536, "start": 3747.1200000000003, "end": 3752.0, "text": " construct an AI system appropriately, the AI system will not overfit to the training data set.", "tokens": [7690, 364, 7318, 1185, 23505, 11, 264, 7318, 1185, 486, 406, 670, 6845, 281, 264, 3097, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.10337493337433913, "compression_ratio": 1.7827715355805243, "no_speech_prob": 0.0001441986096324399}, {"id": 668, "seek": 375200, "start": 3752.0, "end": 3757.28, "text": " And if it's not overfit, we don't expect them to actually output their, their, any of their training", "tokens": [400, 498, 309, 311, 406, 670, 6845, 11, 321, 500, 380, 2066, 552, 281, 767, 5598, 641, 11, 641, 11, 604, 295, 641, 3097], "temperature": 0.0, "avg_logprob": -0.09683308234581581, "compression_ratio": 1.6049382716049383, "no_speech_prob": 2.467877129674889e-05}, {"id": 669, "seek": 375200, "start": 3757.28, "end": 3766.32, "text": " set in any non-trivial way. So these are kind of statements that were hunches. And in this work,", "tokens": [992, 294, 604, 2107, 12, 83, 470, 22640, 636, 13, 407, 613, 366, 733, 295, 12363, 300, 645, 47630, 279, 13, 400, 294, 341, 589, 11], "temperature": 0.0, "avg_logprob": -0.09683308234581581, "compression_ratio": 1.6049382716049383, "no_speech_prob": 2.467877129674889e-05}, {"id": 670, "seek": 375200, "start": 3766.32, "end": 3771.92, "text": " we tried to investigate more rigorously whether they were true. And the way that we did that was", "tokens": [321, 3031, 281, 15013, 544, 42191, 5098, 1968, 436, 645, 2074, 13, 400, 264, 636, 300, 321, 630, 300, 390], "temperature": 0.0, "avg_logprob": -0.09683308234581581, "compression_ratio": 1.6049382716049383, "no_speech_prob": 2.467877129674889e-05}, {"id": 671, "seek": 375200, "start": 3771.92, "end": 3777.68, "text": " basically by taking the pre-trained GPT2 model and feeding it prefixes. So you can imagine you,", "tokens": [1936, 538, 1940, 264, 659, 12, 17227, 2001, 26039, 51, 17, 2316, 293, 12919, 309, 18417, 36005, 13, 407, 291, 393, 3811, 291, 11], "temperature": 0.0, "avg_logprob": -0.09683308234581581, "compression_ratio": 1.6049382716049383, "no_speech_prob": 2.467877129674889e-05}, {"id": 672, "seek": 377768, "start": 3777.68, "end": 3784.0, "text": " you take a causal language model like GPT2 that just predicts tokens auto-aggressively,", "tokens": [291, 747, 257, 38755, 2856, 2316, 411, 26039, 51, 17, 300, 445, 6069, 82, 22667, 8399, 12, 559, 3091, 3413, 11], "temperature": 0.0, "avg_logprob": -0.1795946439107259, "compression_ratio": 1.6517241379310346, "no_speech_prob": 3.88245825888589e-05}, {"id": 673, "seek": 377768, "start": 3784.0, "end": 3789.12, "text": " you feed it a prefix in the basket to basically predict what comes next. And we're showing here", "tokens": [291, 3154, 309, 257, 46969, 294, 264, 8390, 281, 1936, 6069, 437, 1487, 958, 13, 400, 321, 434, 4099, 510], "temperature": 0.0, "avg_logprob": -0.1795946439107259, "compression_ratio": 1.6517241379310346, "no_speech_prob": 3.88245825888589e-05}, {"id": 674, "seek": 377768, "start": 3789.12, "end": 3794.64, "text": " this sort of odd prefix East Stroudsburg Stroudsburg. But we found when we fed this particular prefix", "tokens": [341, 1333, 295, 7401, 46969, 6747, 8251, 1861, 82, 8342, 8251, 1861, 82, 8342, 13, 583, 321, 1352, 562, 321, 4636, 341, 1729, 46969], "temperature": 0.0, "avg_logprob": -0.1795946439107259, "compression_ratio": 1.6517241379310346, "no_speech_prob": 3.88245825888589e-05}, {"id": 675, "seek": 377768, "start": 3794.64, "end": 3801.3599999999997, "text": " into GPT2, it actually output for BADOM and address, name, email address, phone number, and fax", "tokens": [666, 26039, 51, 17, 11, 309, 767, 5598, 337, 363, 6112, 5251, 293, 2985, 11, 1315, 11, 3796, 2985, 11, 2593, 1230, 11, 293, 2050, 87], "temperature": 0.0, "avg_logprob": -0.1795946439107259, "compression_ratio": 1.6517241379310346, "no_speech_prob": 3.88245825888589e-05}, {"id": 676, "seek": 377768, "start": 3801.3599999999997, "end": 3806.3199999999997, "text": " number of a real person that appears on the internet. This actually example actually only appears", "tokens": [1230, 295, 257, 957, 954, 300, 7038, 322, 264, 4705, 13, 639, 767, 1365, 767, 787, 7038], "temperature": 0.0, "avg_logprob": -0.1795946439107259, "compression_ratio": 1.6517241379310346, "no_speech_prob": 3.88245825888589e-05}, {"id": 677, "seek": 380632, "start": 3806.32, "end": 3813.52, "text": " six times on the entire public internet. So it's unlikely that GPT2 saw this address very many times.", "tokens": [2309, 1413, 322, 264, 2302, 1908, 4705, 13, 407, 309, 311, 17518, 300, 26039, 51, 17, 1866, 341, 2985, 588, 867, 1413, 13], "temperature": 0.0, "avg_logprob": -0.07401554183204576, "compression_ratio": 1.5669291338582678, "no_speech_prob": 9.458453132538125e-05}, {"id": 678, "seek": 380632, "start": 3814.32, "end": 3820.6400000000003, "text": " And the main point of this work is that yes, it does seem like GPT2 at least has memorized a", "tokens": [400, 264, 2135, 935, 295, 341, 589, 307, 300, 2086, 11, 309, 775, 1643, 411, 26039, 51, 17, 412, 1935, 575, 46677, 257], "temperature": 0.0, "avg_logprob": -0.07401554183204576, "compression_ratio": 1.5669291338582678, "no_speech_prob": 9.458453132538125e-05}, {"id": 679, "seek": 380632, "start": 3820.6400000000003, "end": 3827.2000000000003, "text": " significant amount of non-trivial information from its pre-training data set. So how did we undertake", "tokens": [4776, 2372, 295, 2107, 12, 83, 470, 22640, 1589, 490, 1080, 659, 12, 17227, 1760, 1412, 992, 13, 407, 577, 630, 321, 37010], "temperature": 0.0, "avg_logprob": -0.07401554183204576, "compression_ratio": 1.5669291338582678, "no_speech_prob": 9.458453132538125e-05}, {"id": 680, "seek": 380632, "start": 3827.2000000000003, "end": 3834.8, "text": " this study? We use this procedure that we are, that's shown on the screen here. We basically consider", "tokens": [341, 2979, 30, 492, 764, 341, 10747, 300, 321, 366, 11, 300, 311, 4898, 322, 264, 2568, 510, 13, 492, 1936, 1949], "temperature": 0.0, "avg_logprob": -0.07401554183204576, "compression_ratio": 1.5669291338582678, "no_speech_prob": 9.458453132538125e-05}, {"id": 681, "seek": 383480, "start": 3834.8, "end": 3840.7200000000003, "text": " three different ways of sampling data from GPT2. The first is just to sample auto-aggressively from", "tokens": [1045, 819, 2098, 295, 21179, 1412, 490, 26039, 51, 17, 13, 440, 700, 307, 445, 281, 6889, 8399, 12, 559, 3091, 3413, 490], "temperature": 0.0, "avg_logprob": -0.06602785546900862, "compression_ratio": 1.8051470588235294, "no_speech_prob": 4.2639814637368545e-05}, {"id": 682, "seek": 383480, "start": 3840.7200000000003, "end": 3846.0800000000004, "text": " it. The next is to sample auto-aggressively but with a decaying temperature. This basically means", "tokens": [309, 13, 440, 958, 307, 281, 6889, 8399, 12, 559, 3091, 3413, 457, 365, 257, 21039, 278, 4292, 13, 639, 1936, 1355], "temperature": 0.0, "avg_logprob": -0.06602785546900862, "compression_ratio": 1.8051470588235294, "no_speech_prob": 4.2639814637368545e-05}, {"id": 683, "seek": 383480, "start": 3846.0800000000004, "end": 3851.6800000000003, "text": " that you want the model to become more and more confident in its predictions over the course of", "tokens": [300, 291, 528, 264, 2316, 281, 1813, 544, 293, 544, 6679, 294, 1080, 21264, 670, 264, 1164, 295], "temperature": 0.0, "avg_logprob": -0.06602785546900862, "compression_ratio": 1.8051470588235294, "no_speech_prob": 4.2639814637368545e-05}, {"id": 684, "seek": 383480, "start": 3851.6800000000003, "end": 3857.6000000000004, "text": " sampling. And the last option is to take random text from the internet and use that as conditioning", "tokens": [21179, 13, 400, 264, 1036, 3614, 307, 281, 747, 4974, 2487, 490, 264, 4705, 293, 764, 300, 382, 21901], "temperature": 0.0, "avg_logprob": -0.06602785546900862, "compression_ratio": 1.8051470588235294, "no_speech_prob": 4.2639814637368545e-05}, {"id": 685, "seek": 383480, "start": 3857.6000000000004, "end": 3864.32, "text": " to GPT2 before asking it to generate what comes next. So now for each of these, for each of these", "tokens": [281, 26039, 51, 17, 949, 3365, 309, 281, 8460, 437, 1487, 958, 13, 407, 586, 337, 1184, 295, 613, 11, 337, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.06602785546900862, "compression_ratio": 1.8051470588235294, "no_speech_prob": 4.2639814637368545e-05}, {"id": 686, "seek": 386432, "start": 3864.32, "end": 3869.36, "text": " generations, each of these 200,000 generations we did for each of these sampling methods,", "tokens": [10593, 11, 1184, 295, 613, 2331, 11, 1360, 10593, 321, 630, 337, 1184, 295, 613, 21179, 7150, 11], "temperature": 0.0, "avg_logprob": -0.07086233122158894, "compression_ratio": 1.7234848484848484, "no_speech_prob": 2.42939822783228e-05}, {"id": 687, "seek": 386432, "start": 3869.36, "end": 3873.52, "text": " we want a way of kind of trying to predict whether it might be memorized or not.", "tokens": [321, 528, 257, 636, 295, 733, 295, 1382, 281, 6069, 1968, 309, 1062, 312, 46677, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.07086233122158894, "compression_ratio": 1.7234848484848484, "no_speech_prob": 2.42939822783228e-05}, {"id": 688, "seek": 386432, "start": 3874.32, "end": 3880.48, "text": " And so we came up with six metrics to use to give us a notion of whether a particular sample from", "tokens": [400, 370, 321, 1361, 493, 365, 2309, 16367, 281, 764, 281, 976, 505, 257, 10710, 295, 1968, 257, 1729, 6889, 490], "temperature": 0.0, "avg_logprob": -0.07086233122158894, "compression_ratio": 1.7234848484848484, "no_speech_prob": 2.42939822783228e-05}, {"id": 689, "seek": 386432, "start": 3880.48, "end": 3886.6400000000003, "text": " GPT2 might be memorized. All of these different metrics basically make use of GPT2's perplexity", "tokens": [26039, 51, 17, 1062, 312, 46677, 13, 1057, 295, 613, 819, 16367, 1936, 652, 764, 295, 26039, 51, 17, 311, 680, 18945, 507], "temperature": 0.0, "avg_logprob": -0.07086233122158894, "compression_ratio": 1.7234848484848484, "no_speech_prob": 2.42939822783228e-05}, {"id": 690, "seek": 386432, "start": 3886.6400000000003, "end": 3893.6800000000003, "text": " for the sample. The perplexity, as I think you probably learned, is basically a measure of", "tokens": [337, 264, 6889, 13, 440, 680, 18945, 507, 11, 382, 286, 519, 291, 1391, 3264, 11, 307, 1936, 257, 3481, 295], "temperature": 0.0, "avg_logprob": -0.07086233122158894, "compression_ratio": 1.7234848484848484, "no_speech_prob": 2.42939822783228e-05}, {"id": 691, "seek": 389368, "start": 3893.68, "end": 3899.9199999999996, "text": " how confident GPT2 was in generating this particular sample. You can also think of it as a measure", "tokens": [577, 6679, 26039, 51, 17, 390, 294, 17746, 341, 1729, 6889, 13, 509, 393, 611, 519, 295, 309, 382, 257, 3481], "temperature": 0.0, "avg_logprob": -0.07923497959059111, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.295403065654682e-06}, {"id": 692, "seek": 389368, "start": 3899.9199999999996, "end": 3908.24, "text": " of compression. So these metrics all make use of the perplexity. Either we just measure GPT2's", "tokens": [295, 19355, 13, 407, 613, 16367, 439, 652, 764, 295, 264, 680, 18945, 507, 13, 13746, 321, 445, 3481, 26039, 51, 17, 311], "temperature": 0.0, "avg_logprob": -0.07923497959059111, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.295403065654682e-06}, {"id": 693, "seek": 389368, "start": 3908.24, "end": 3915.04, "text": " perplexity for the thing that it generated or we compute the ratio of GPT2's perplexity", "tokens": [680, 18945, 507, 337, 264, 551, 300, 309, 10833, 420, 321, 14722, 264, 8509, 295, 26039, 51, 17, 311, 680, 18945, 507], "temperature": 0.0, "avg_logprob": -0.07923497959059111, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.295403065654682e-06}, {"id": 694, "seek": 389368, "start": 3915.68, "end": 3920.72, "text": " to the perplexity for another variant of GPT or to a text compression library called ZLib.", "tokens": [281, 264, 680, 18945, 507, 337, 1071, 17501, 295, 26039, 51, 420, 281, 257, 2487, 19355, 6405, 1219, 1176, 43, 897, 13], "temperature": 0.0, "avg_logprob": -0.07923497959059111, "compression_ratio": 1.7142857142857142, "no_speech_prob": 7.295403065654682e-06}, {"id": 695, "seek": 392072, "start": 3920.72, "end": 3927.2, "text": " We also compared via a ratio of the perplexity of the original sample versus a lower case version", "tokens": [492, 611, 5347, 5766, 257, 8509, 295, 264, 680, 18945, 507, 295, 264, 3380, 6889, 5717, 257, 3126, 1389, 3037], "temperature": 0.0, "avg_logprob": -0.1057815247393669, "compression_ratio": 1.830188679245283, "no_speech_prob": 4.1981329559348524e-05}, {"id": 696, "seek": 392072, "start": 3927.2, "end": 3932.08, "text": " of the sample and also a windowed perplexity where we only compute the perplexity over a small", "tokens": [295, 264, 6889, 293, 611, 257, 4910, 292, 680, 18945, 507, 689, 321, 787, 14722, 264, 680, 18945, 507, 670, 257, 1359], "temperature": 0.0, "avg_logprob": -0.1057815247393669, "compression_ratio": 1.830188679245283, "no_speech_prob": 4.1981329559348524e-05}, {"id": 697, "seek": 392072, "start": 3932.08, "end": 3940.3199999999997, "text": " window of the sample instead of the whole thing. We take the top, we do some deduplication on the", "tokens": [4910, 295, 264, 6889, 2602, 295, 264, 1379, 551, 13, 492, 747, 264, 1192, 11, 321, 360, 512, 4172, 84, 4770, 399, 322, 264], "temperature": 0.0, "avg_logprob": -0.1057815247393669, "compression_ratio": 1.830188679245283, "no_speech_prob": 4.1981329559348524e-05}, {"id": 698, "seek": 392072, "start": 3940.3199999999997, "end": 3945.2, "text": " generation and then choose the top 100 generations according to each of these metrics. So that'll", "tokens": [5125, 293, 550, 2826, 264, 1192, 2319, 10593, 4650, 281, 1184, 295, 613, 16367, 13, 407, 300, 603], "temperature": 0.0, "avg_logprob": -0.1057815247393669, "compression_ratio": 1.830188679245283, "no_speech_prob": 4.1981329559348524e-05}, {"id": 699, "seek": 394520, "start": 3945.2, "end": 3951.04, "text": " ultimately give us 600 possible memorized generations and then we actually just do a basic, excuse me,", "tokens": [6284, 976, 505, 11849, 1944, 46677, 10593, 293, 550, 321, 767, 445, 360, 257, 3875, 11, 8960, 385, 11], "temperature": 0.0, "avg_logprob": -0.09945120931673451, "compression_ratio": 1.7614035087719297, "no_speech_prob": 8.091226482065395e-05}, {"id": 700, "seek": 394520, "start": 3951.04, "end": 3958.08, "text": " a basic Google search to see if we can find that text that GPT2 generated on the internet somewhere.", "tokens": [257, 3875, 3329, 3164, 281, 536, 498, 321, 393, 915, 300, 2487, 300, 26039, 51, 17, 10833, 322, 264, 4705, 4079, 13], "temperature": 0.0, "avg_logprob": -0.09945120931673451, "compression_ratio": 1.7614035087719297, "no_speech_prob": 8.091226482065395e-05}, {"id": 701, "seek": 394520, "start": 3958.08, "end": 3963.3599999999997, "text": " And if we do find it on the internet somewhere, we asked that GPT2 authors was this in the training", "tokens": [400, 498, 321, 360, 915, 309, 322, 264, 4705, 4079, 11, 321, 2351, 300, 26039, 51, 17, 16552, 390, 341, 294, 264, 3097], "temperature": 0.0, "avg_logprob": -0.09945120931673451, "compression_ratio": 1.7614035087719297, "no_speech_prob": 8.091226482065395e-05}, {"id": 702, "seek": 394520, "start": 3963.3599999999997, "end": 3968.8799999999997, "text": " set or not and they checked for all of the examples that we found and let us know if GPT2 actually", "tokens": [992, 420, 406, 293, 436, 10033, 337, 439, 295, 264, 5110, 300, 321, 1352, 293, 718, 505, 458, 498, 26039, 51, 17, 767], "temperature": 0.0, "avg_logprob": -0.09945120931673451, "compression_ratio": 1.7614035087719297, "no_speech_prob": 8.091226482065395e-05}, {"id": 703, "seek": 394520, "start": 3968.8799999999997, "end": 3974.7999999999997, "text": " spit out something from its training dataset. So just to give you an idea of what these metrics are", "tokens": [22127, 484, 746, 490, 1080, 3097, 28872, 13, 407, 445, 281, 976, 291, 364, 1558, 295, 437, 613, 16367, 366], "temperature": 0.0, "avg_logprob": -0.09945120931673451, "compression_ratio": 1.7614035087719297, "no_speech_prob": 8.091226482065395e-05}, {"id": 704, "seek": 397480, "start": 3974.8, "end": 3981.92, "text": " and why they might be helpful, this scatter plot is showing the perplexity assigned by GPT2", "tokens": [293, 983, 436, 1062, 312, 4961, 11, 341, 34951, 7542, 307, 4099, 264, 680, 18945, 507, 13279, 538, 26039, 51, 17], "temperature": 0.0, "avg_logprob": -0.0836334778712346, "compression_ratio": 1.6651982378854626, "no_speech_prob": 5.5614735174458474e-05}, {"id": 705, "seek": 397480, "start": 3981.92, "end": 3989.28, "text": " and the perplexity assigned by ZLib to 200,000 samples generated by GPT2. And you can see that most", "tokens": [293, 264, 680, 18945, 507, 13279, 538, 1176, 43, 897, 281, 2331, 11, 1360, 10938, 10833, 538, 26039, 51, 17, 13, 400, 291, 393, 536, 300, 881], "temperature": 0.0, "avg_logprob": -0.0836334778712346, "compression_ratio": 1.6651982378854626, "no_speech_prob": 5.5614735174458474e-05}, {"id": 706, "seek": 397480, "start": 3989.28, "end": 3994.5600000000004, "text": " of them kind of fall on this line. There's a big clump of them on the right there in gray.", "tokens": [295, 552, 733, 295, 2100, 322, 341, 1622, 13, 821, 311, 257, 955, 596, 1420, 295, 552, 322, 264, 558, 456, 294, 10855, 13], "temperature": 0.0, "avg_logprob": -0.0836334778712346, "compression_ratio": 1.6651982378854626, "no_speech_prob": 5.5614735174458474e-05}, {"id": 707, "seek": 397480, "start": 3995.52, "end": 4003.76, "text": " But highlighted here in red and blue are samples that kind of are outliers. GPT2 is assigning a", "tokens": [583, 17173, 510, 294, 2182, 293, 3344, 366, 10938, 300, 733, 295, 366, 484, 23646, 13, 26039, 51, 17, 307, 49602, 257], "temperature": 0.0, "avg_logprob": -0.0836334778712346, "compression_ratio": 1.6651982378854626, "no_speech_prob": 5.5614735174458474e-05}, {"id": 708, "seek": 400376, "start": 4003.76, "end": 4010.0, "text": " much lower perplexity to ZLib, sorry, to those samples than ZLib is. And what that means is that", "tokens": [709, 3126, 680, 18945, 507, 281, 1176, 43, 897, 11, 2597, 11, 281, 729, 10938, 813, 1176, 43, 897, 307, 13, 400, 437, 300, 1355, 307, 300], "temperature": 0.0, "avg_logprob": -0.1045908179164918, "compression_ratio": 1.7370689655172413, "no_speech_prob": 3.8825568481115624e-05}, {"id": 709, "seek": 400376, "start": 4010.0, "end": 4016.96, "text": " GPT2 is very, very good at predicting what comes next in these samples and ZLib is not. ZLib is kind of", "tokens": [26039, 51, 17, 307, 588, 11, 588, 665, 412, 32884, 437, 1487, 958, 294, 613, 10938, 293, 1176, 43, 897, 307, 406, 13, 1176, 43, 897, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.1045908179164918, "compression_ratio": 1.7370689655172413, "no_speech_prob": 3.8825568481115624e-05}, {"id": 710, "seek": 400376, "start": 4016.96, "end": 4024.32, "text": " a unbiased source, right? It's not really pre-trained on a bunch of data, it's kind of data agnostic.", "tokens": [257, 517, 5614, 1937, 4009, 11, 558, 30, 467, 311, 406, 534, 659, 12, 17227, 2001, 322, 257, 3840, 295, 1412, 11, 309, 311, 733, 295, 1412, 623, 77, 19634, 13], "temperature": 0.0, "avg_logprob": -0.1045908179164918, "compression_ratio": 1.7370689655172413, "no_speech_prob": 3.8825568481115624e-05}, {"id": 711, "seek": 400376, "start": 4024.32, "end": 4029.28, "text": " So it might be the case that if GPT2 is very good at predicting what comes next in a given sequence,", "tokens": [407, 309, 1062, 312, 264, 1389, 300, 498, 26039, 51, 17, 307, 588, 665, 412, 32884, 437, 1487, 958, 294, 257, 2212, 8310, 11], "temperature": 0.0, "avg_logprob": -0.1045908179164918, "compression_ratio": 1.7370689655172413, "no_speech_prob": 3.8825568481115624e-05}, {"id": 712, "seek": 402928, "start": 4029.28, "end": 4034.96, "text": " but ZLib is not, that GPT2 has memorized those samples. So all of these things kind of in the top", "tokens": [457, 1176, 43, 897, 307, 406, 11, 300, 26039, 51, 17, 575, 46677, 729, 10938, 13, 407, 439, 295, 613, 721, 733, 295, 294, 264, 1192], "temperature": 0.0, "avg_logprob": -0.10295401897627054, "compression_ratio": 1.7025862068965518, "no_speech_prob": 2.247103111585602e-05}, {"id": 713, "seek": 402928, "start": 4034.96, "end": 4040.32, "text": " left there are possible memorized samples. And actually we marked the ones in blue that it turned out", "tokens": [1411, 456, 366, 1944, 46677, 10938, 13, 400, 767, 321, 12658, 264, 2306, 294, 3344, 300, 309, 3574, 484], "temperature": 0.0, "avg_logprob": -0.10295401897627054, "compression_ratio": 1.7025862068965518, "no_speech_prob": 2.247103111585602e-05}, {"id": 714, "seek": 402928, "start": 4040.32, "end": 4049.1200000000003, "text": " were actually in the training data set and that GPT2 had memorized. Overall, we found many examples", "tokens": [645, 767, 294, 264, 3097, 1412, 992, 293, 300, 26039, 51, 17, 632, 46677, 13, 18420, 11, 321, 1352, 867, 5110], "temperature": 0.0, "avg_logprob": -0.10295401897627054, "compression_ratio": 1.7025862068965518, "no_speech_prob": 2.247103111585602e-05}, {"id": 715, "seek": 402928, "start": 4049.1200000000003, "end": 4055.84, "text": " of verbatim text memorized from the training data set. This included news, log files, licenses,", "tokens": [295, 9595, 267, 332, 2487, 46677, 490, 264, 3097, 1412, 992, 13, 639, 5556, 2583, 11, 3565, 7098, 11, 32821, 11], "temperature": 0.0, "avg_logprob": -0.10295401897627054, "compression_ratio": 1.7025862068965518, "no_speech_prob": 2.247103111585602e-05}, {"id": 716, "seek": 405584, "start": 4055.84, "end": 4063.6000000000004, "text": " you know, pages from Wikipedia, URLs, and we highlight two types of data here that we found", "tokens": [291, 458, 11, 7183, 490, 28999, 11, 43267, 11, 293, 321, 5078, 732, 3467, 295, 1412, 510, 300, 321, 1352], "temperature": 0.0, "avg_logprob": -0.11556265904353215, "compression_ratio": 1.5578512396694215, "no_speech_prob": 6.107746594352648e-05}, {"id": 717, "seek": 405584, "start": 4063.6000000000004, "end": 4070.1600000000003, "text": " that GPT2 had memorized that, in our opinion, constitute private information, like named individuals", "tokens": [300, 26039, 51, 17, 632, 46677, 300, 11, 294, 527, 4800, 11, 41658, 4551, 1589, 11, 411, 4926, 5346], "temperature": 0.0, "avg_logprob": -0.11556265904353215, "compression_ratio": 1.5578512396694215, "no_speech_prob": 6.107746594352648e-05}, {"id": 718, "seek": 405584, "start": 4070.1600000000003, "end": 4074.0, "text": " from non-new samples or contact information like the example I showed early on.", "tokens": [490, 2107, 12, 7686, 10938, 420, 3385, 1589, 411, 264, 1365, 286, 4712, 2440, 322, 13], "temperature": 0.0, "avg_logprob": -0.11556265904353215, "compression_ratio": 1.5578512396694215, "no_speech_prob": 6.107746594352648e-05}, {"id": 719, "seek": 405584, "start": 4076.32, "end": 4081.44, "text": " And so you might ask, okay, you know, maybe it's not that surprising that GPT2 memorized a news article,", "tokens": [400, 370, 291, 1062, 1029, 11, 1392, 11, 291, 458, 11, 1310, 309, 311, 406, 300, 8830, 300, 26039, 51, 17, 46677, 257, 2583, 7222, 11], "temperature": 0.0, "avg_logprob": -0.11556265904353215, "compression_ratio": 1.5578512396694215, "no_speech_prob": 6.107746594352648e-05}, {"id": 720, "seek": 408144, "start": 4081.44, "end": 4086.32, "text": " if that news article appears hundreds of times in the internet. We actually got lucky because it", "tokens": [498, 300, 2583, 7222, 7038, 6779, 295, 1413, 294, 264, 4705, 13, 492, 767, 658, 6356, 570, 309], "temperature": 0.0, "avg_logprob": -0.10327704568927208, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.5615597375435755e-05}, {"id": 721, "seek": 408144, "start": 4086.32, "end": 4092.08, "text": " turned out there were a bunch of examples of memorized data that only appeared on one document", "tokens": [3574, 484, 456, 645, 257, 3840, 295, 5110, 295, 46677, 1412, 300, 787, 8516, 322, 472, 4166], "temperature": 0.0, "avg_logprob": -0.10327704568927208, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.5615597375435755e-05}, {"id": 722, "seek": 408144, "start": 4092.08, "end": 4099.84, "text": " on the entire public internet. It was basically a paste, like a paste of a bunch of URLs from Reddit,", "tokens": [322, 264, 2302, 1908, 4705, 13, 467, 390, 1936, 257, 9163, 11, 411, 257, 9163, 295, 257, 3840, 295, 43267, 490, 32210, 11], "temperature": 0.0, "avg_logprob": -0.10327704568927208, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.5615597375435755e-05}, {"id": 723, "seek": 408144, "start": 4099.84, "end": 4105.28, "text": " from a controversial subreddit called the Donald. And all of these URLs had exactly the same form.", "tokens": [490, 257, 17323, 1422, 986, 17975, 1219, 264, 8632, 13, 400, 439, 295, 613, 43267, 632, 2293, 264, 912, 1254, 13], "temperature": 0.0, "avg_logprob": -0.10327704568927208, "compression_ratio": 1.6065573770491803, "no_speech_prob": 5.5615597375435755e-05}, {"id": 724, "seek": 410528, "start": 4105.28, "end": 4112.08, "text": " It was, you know, HTTP colon slash slash Reddit.com slash r slash the Donald slash a bunch of random", "tokens": [467, 390, 11, 291, 458, 11, 33283, 8255, 17330, 17330, 32210, 13, 1112, 17330, 367, 17330, 264, 8632, 17330, 257, 3840, 295, 4974], "temperature": 0.0, "avg_logprob": -0.09670703969103225, "compression_ratio": 1.7324561403508771, "no_speech_prob": 2.17808428715216e-05}, {"id": 725, "seek": 410528, "start": 4112.08, "end": 4118.4, "text": " numbers and letters slash the name of the thread. Now this random numbers and letters part is nice", "tokens": [3547, 293, 7825, 17330, 264, 1315, 295, 264, 7207, 13, 823, 341, 4974, 3547, 293, 7825, 644, 307, 1481], "temperature": 0.0, "avg_logprob": -0.09670703969103225, "compression_ratio": 1.7324561403508771, "no_speech_prob": 2.17808428715216e-05}, {"id": 726, "seek": 410528, "start": 4118.4, "end": 4124.16, "text": " because it's equally hard to predict in all cases. It's basically a random hash. So we know that", "tokens": [570, 309, 311, 12309, 1152, 281, 6069, 294, 439, 3331, 13, 467, 311, 1936, 257, 4974, 22019, 13, 407, 321, 458, 300], "temperature": 0.0, "avg_logprob": -0.09670703969103225, "compression_ratio": 1.7324561403508771, "no_speech_prob": 2.17808428715216e-05}, {"id": 727, "seek": 410528, "start": 4124.16, "end": 4129.92, "text": " that part of the sequence should be equally hard for any model to memorize. And what that means is", "tokens": [300, 644, 295, 264, 8310, 820, 312, 12309, 1152, 337, 604, 2316, 281, 27478, 13, 400, 437, 300, 1355, 307], "temperature": 0.0, "avg_logprob": -0.09670703969103225, "compression_ratio": 1.7324561403508771, "no_speech_prob": 2.17808428715216e-05}, {"id": 728, "seek": 412992, "start": 4129.92, "end": 4138.16, "text": " that we can measure how many times does a particular URL need to appear in this list of URLs", "tokens": [300, 321, 393, 3481, 577, 867, 1413, 775, 257, 1729, 12905, 643, 281, 4204, 294, 341, 1329, 295, 43267], "temperature": 0.0, "avg_logprob": -0.08722858428955078, "compression_ratio": 1.478021978021978, "no_speech_prob": 5.06390533701051e-05}, {"id": 729, "seek": 412992, "start": 4138.16, "end": 4144.32, "text": " because there were repetitions in the list in order for one of the particular GPT2 sized models", "tokens": [570, 456, 645, 13645, 2451, 294, 264, 1329, 294, 1668, 337, 472, 295, 264, 1729, 26039, 51, 17, 20004, 5245], "temperature": 0.0, "avg_logprob": -0.08722858428955078, "compression_ratio": 1.478021978021978, "no_speech_prob": 5.06390533701051e-05}, {"id": 730, "seek": 412992, "start": 4145.04, "end": 4151.36, "text": " to memorize it. And what we found was that the largest variant of GPT2, GPT2 XL,", "tokens": [281, 27478, 309, 13, 400, 437, 321, 1352, 390, 300, 264, 6443, 17501, 295, 26039, 51, 17, 11, 26039, 51, 17, 37210, 11], "temperature": 0.0, "avg_logprob": -0.08722858428955078, "compression_ratio": 1.478021978021978, "no_speech_prob": 5.06390533701051e-05}, {"id": 731, "seek": 415136, "start": 4151.36, "end": 4161.44, "text": " memorized a URL that appeared 33 times in this particular document, but not URLs that appeared 17 times", "tokens": [46677, 257, 12905, 300, 8516, 11816, 1413, 294, 341, 1729, 4166, 11, 457, 406, 43267, 300, 8516, 3282, 1413], "temperature": 0.0, "avg_logprob": -0.10306482417609102, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.480551458778791e-05}, {"id": 732, "seek": 415136, "start": 4161.44, "end": 4168.799999999999, "text": " or fewer. The medium size model was only really able to fully memorize a URL that appeared 56 times.", "tokens": [420, 13366, 13, 440, 6399, 2744, 2316, 390, 787, 534, 1075, 281, 4498, 27478, 257, 12905, 300, 8516, 19687, 1413, 13], "temperature": 0.0, "avg_logprob": -0.10306482417609102, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.480551458778791e-05}, {"id": 733, "seek": 415136, "start": 4168.799999999999, "end": 4173.2, "text": " And the small model really didn't memorize any. The a half basically means that we could get it to", "tokens": [400, 264, 1359, 2316, 534, 994, 380, 27478, 604, 13, 440, 257, 1922, 1936, 1355, 300, 321, 727, 483, 309, 281], "temperature": 0.0, "avg_logprob": -0.10306482417609102, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.480551458778791e-05}, {"id": 734, "seek": 415136, "start": 4173.2, "end": 4177.759999999999, "text": " spit out the URL if we gave it some additional prompting. We basically hinted at what some of the", "tokens": [22127, 484, 264, 12905, 498, 321, 2729, 309, 512, 4497, 12391, 278, 13, 492, 1936, 12075, 292, 412, 437, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.10306482417609102, "compression_ratio": 1.7210300429184548, "no_speech_prob": 3.480551458778791e-05}, {"id": 735, "seek": 417776, "start": 4177.76, "end": 4183.280000000001, "text": " numbers were. And what this take away of this is that we actually, by coincidence, because there", "tokens": [3547, 645, 13, 400, 437, 341, 747, 1314, 295, 341, 307, 300, 321, 767, 11, 538, 22137, 11, 570, 456], "temperature": 0.0, "avg_logprob": -0.13042909220645302, "compression_ratio": 1.640552995391705, "no_speech_prob": 3.535212090355344e-05}, {"id": 736, "seek": 417776, "start": 4183.280000000001, "end": 4188.64, "text": " is this particular document with this structure, we were able to say reasonably confidently that", "tokens": [307, 341, 1729, 4166, 365, 341, 3877, 11, 321, 645, 1075, 281, 584, 23551, 41956, 300], "temperature": 0.0, "avg_logprob": -0.13042909220645302, "compression_ratio": 1.640552995391705, "no_speech_prob": 3.535212090355344e-05}, {"id": 737, "seek": 417776, "start": 4188.64, "end": 4195.52, "text": " larger models tend to memorize more data. They need to see particular examples fewer times", "tokens": [4833, 5245, 3928, 281, 27478, 544, 1412, 13, 814, 643, 281, 536, 1729, 5110, 13366, 1413], "temperature": 0.0, "avg_logprob": -0.13042909220645302, "compression_ratio": 1.640552995391705, "no_speech_prob": 3.535212090355344e-05}, {"id": 738, "seek": 417776, "start": 4195.52, "end": 4199.280000000001, "text": " in order to memorize them, which we thought was an interesting finding.", "tokens": [294, 1668, 281, 27478, 552, 11, 597, 321, 1194, 390, 364, 1880, 5006, 13], "temperature": 0.0, "avg_logprob": -0.13042909220645302, "compression_ratio": 1.640552995391705, "no_speech_prob": 3.535212090355344e-05}, {"id": 739, "seek": 419928, "start": 4199.28, "end": 4208.24, "text": " So, so far I have kind of mostly been talking about the benefits of larger models, right? Because", "tokens": [407, 11, 370, 1400, 286, 362, 733, 295, 5240, 668, 1417, 466, 264, 5311, 295, 4833, 5245, 11, 558, 30, 1436], "temperature": 0.0, "avg_logprob": -0.15373673158533432, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00013761023001279682}, {"id": 740, "seek": 419928, "start": 4209.04, "end": 4214.0, "text": " larger models did better on super glue, larger models did better on close-up question answering.", "tokens": [4833, 5245, 630, 1101, 322, 1687, 8998, 11, 4833, 5245, 630, 1101, 322, 1998, 12, 1010, 1168, 13430, 13], "temperature": 0.0, "avg_logprob": -0.15373673158533432, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00013761023001279682}, {"id": 741, "seek": 419928, "start": 4214.0, "end": 4217.759999999999, "text": " Of course, there is this caveat that larger models also seem to be better at memorizing", "tokens": [2720, 1164, 11, 456, 307, 341, 43012, 300, 4833, 5245, 611, 1643, 281, 312, 1101, 412, 10560, 3319], "temperature": 0.0, "avg_logprob": -0.15373673158533432, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00013761023001279682}, {"id": 742, "seek": 419928, "start": 4217.759999999999, "end": 4225.2, "text": " their training data set. But larger models are also inconvenient. They are more computationally", "tokens": [641, 3097, 1412, 992, 13, 583, 4833, 5245, 366, 611, 46196, 13, 814, 366, 544, 24903, 379], "temperature": 0.0, "avg_logprob": -0.15373673158533432, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00013761023001279682}, {"id": 743, "seek": 422520, "start": 4225.2, "end": 4230.5599999999995, "text": " expensive to run. They consume more energy. And they don't fit on, for example, T511B doesn't fit", "tokens": [5124, 281, 1190, 13, 814, 14732, 544, 2281, 13, 400, 436, 500, 380, 3318, 322, 11, 337, 1365, 11, 314, 20, 5348, 33, 1177, 380, 3318], "temperature": 0.0, "avg_logprob": -0.10818164579329952, "compression_ratio": 1.498069498069498, "no_speech_prob": 4.9082002078648657e-05}, {"id": 744, "seek": 422520, "start": 4230.5599999999995, "end": 4239.599999999999, "text": " on a single GPU unless you use kind of clever methods. So, the last paper that I'll discuss,", "tokens": [322, 257, 2167, 18407, 5969, 291, 764, 733, 295, 13494, 7150, 13, 407, 11, 264, 1036, 3035, 300, 286, 603, 2248, 11], "temperature": 0.0, "avg_logprob": -0.10818164579329952, "compression_ratio": 1.498069498069498, "no_speech_prob": 4.9082002078648657e-05}, {"id": 745, "seek": 422520, "start": 4239.599999999999, "end": 4245.12, "text": " which is super recent work, is can we basically close the performance gap between large and small", "tokens": [597, 307, 1687, 5162, 589, 11, 307, 393, 321, 1936, 1998, 264, 3389, 7417, 1296, 2416, 293, 1359], "temperature": 0.0, "avg_logprob": -0.10818164579329952, "compression_ratio": 1.498069498069498, "no_speech_prob": 4.9082002078648657e-05}, {"id": 746, "seek": 422520, "start": 4245.12, "end": 4250.16, "text": " models through improvements to the transformer architecture? So in this work, we take basically the", "tokens": [5245, 807, 13797, 281, 264, 31782, 9482, 30, 407, 294, 341, 589, 11, 321, 747, 1936, 264], "temperature": 0.0, "avg_logprob": -0.10818164579329952, "compression_ratio": 1.498069498069498, "no_speech_prob": 4.9082002078648657e-05}, {"id": 747, "seek": 425016, "start": 4250.16, "end": 4255.5199999999995, "text": " same strategy that we took in the T5 paper, where we took sort of the landscape of existing", "tokens": [912, 5206, 300, 321, 1890, 294, 264, 314, 20, 3035, 11, 689, 321, 1890, 1333, 295, 264, 9661, 295, 6741], "temperature": 0.0, "avg_logprob": -0.11466862895701191, "compression_ratio": 1.902834008097166, "no_speech_prob": 5.737585888709873e-05}, {"id": 748, "seek": 425016, "start": 4255.5199999999995, "end": 4260.0, "text": " modifications to the transformer architecture and evaluated them in the same exact setting.", "tokens": [26881, 281, 264, 31782, 9482, 293, 25509, 552, 294, 264, 912, 1900, 3287, 13], "temperature": 0.0, "avg_logprob": -0.11466862895701191, "compression_ratio": 1.902834008097166, "no_speech_prob": 5.737585888709873e-05}, {"id": 749, "seek": 425016, "start": 4260.88, "end": 4265.68, "text": " And there have been lots of variants proposed to the transformer architecture. In T5, we use", "tokens": [400, 456, 362, 668, 3195, 295, 21669, 10348, 281, 264, 31782, 9482, 13, 682, 314, 20, 11, 321, 764], "temperature": 0.0, "avg_logprob": -0.11466862895701191, "compression_ratio": 1.902834008097166, "no_speech_prob": 5.737585888709873e-05}, {"id": 750, "seek": 425016, "start": 4265.68, "end": 4269.92, "text": " basically the standard and coder decoder architecture from the attention as all you need paper", "tokens": [1936, 264, 3832, 293, 17656, 260, 979, 19866, 9482, 490, 264, 3202, 382, 439, 291, 643, 3035], "temperature": 0.0, "avg_logprob": -0.11466862895701191, "compression_ratio": 1.902834008097166, "no_speech_prob": 5.737585888709873e-05}, {"id": 751, "seek": 425016, "start": 4270.5599999999995, "end": 4275.04, "text": " that's visualized here. But there have been lots and lots of modifications that have been proposed", "tokens": [300, 311, 5056, 1602, 510, 13, 583, 456, 362, 668, 3195, 293, 3195, 295, 26881, 300, 362, 668, 10348], "temperature": 0.0, "avg_logprob": -0.11466862895701191, "compression_ratio": 1.902834008097166, "no_speech_prob": 5.737585888709873e-05}, {"id": 752, "seek": 427504, "start": 4275.04, "end": 4281.76, "text": " since the transformer was released in 2017. For example, maybe people suggested that you should factorize", "tokens": [1670, 264, 31782, 390, 4736, 294, 6591, 13, 1171, 1365, 11, 1310, 561, 10945, 300, 291, 820, 5952, 1125], "temperature": 0.0, "avg_logprob": -0.11319221149791371, "compression_ratio": 1.789272030651341, "no_speech_prob": 6.603441579500213e-05}, {"id": 753, "seek": 427504, "start": 4281.76, "end": 4286.96, "text": " your embedding matrix. You should share the embedding matrix and the softmax output layer.", "tokens": [428, 12240, 3584, 8141, 13, 509, 820, 2073, 264, 12240, 3584, 8141, 293, 264, 2787, 41167, 5598, 4583, 13], "temperature": 0.0, "avg_logprob": -0.11319221149791371, "compression_ratio": 1.789272030651341, "no_speech_prob": 6.603441579500213e-05}, {"id": 754, "seek": 427504, "start": 4286.96, "end": 4291.68, "text": " You should use different forms of softmax like a mixture of softmaxes or an adaptive softmax.", "tokens": [509, 820, 764, 819, 6422, 295, 2787, 41167, 411, 257, 9925, 295, 2787, 41167, 279, 420, 364, 27912, 2787, 41167, 13], "temperature": 0.0, "avg_logprob": -0.11319221149791371, "compression_ratio": 1.789272030651341, "no_speech_prob": 6.603441579500213e-05}, {"id": 755, "seek": 427504, "start": 4292.32, "end": 4299.36, "text": " Different ways of normalizing or initializing the model. Maybe different attention mechanisms,", "tokens": [20825, 2098, 295, 2710, 3319, 420, 5883, 3319, 264, 2316, 13, 2704, 819, 3202, 15902, 11], "temperature": 0.0, "avg_logprob": -0.11319221149791371, "compression_ratio": 1.789272030651341, "no_speech_prob": 6.603441579500213e-05}, {"id": 756, "seek": 427504, "start": 4299.36, "end": 4302.96, "text": " alternatives to attention mechanisms like lightweight and dynamical convolutions.", "tokens": [20478, 281, 3202, 15902, 411, 22052, 293, 5999, 804, 3754, 15892, 13], "temperature": 0.0, "avg_logprob": -0.11319221149791371, "compression_ratio": 1.789272030651341, "no_speech_prob": 6.603441579500213e-05}, {"id": 757, "seek": 430296, "start": 4302.96, "end": 4307.68, "text": " Different nonlinearities in the feed forward layers. Different structures for the feed forward", "tokens": [20825, 2107, 28263, 1088, 294, 264, 3154, 2128, 7914, 13, 20825, 9227, 337, 264, 3154, 2128], "temperature": 0.0, "avg_logprob": -0.11050188836972576, "compression_ratio": 1.9240924092409242, "no_speech_prob": 9.914111433317885e-05}, {"id": 758, "seek": 430296, "start": 4307.68, "end": 4312.72, "text": " layers like mixture of expert to the switch transformer. Completely different architectures that", "tokens": [7914, 411, 9925, 295, 5844, 281, 264, 3679, 31782, 13, 39978, 819, 6331, 1303, 300], "temperature": 0.0, "avg_logprob": -0.11050188836972576, "compression_ratio": 1.9240924092409242, "no_speech_prob": 9.914111433317885e-05}, {"id": 759, "seek": 430296, "start": 4312.72, "end": 4317.2, "text": " were inspired by the transformer, like the funnel transformer of all transformer, the universal", "tokens": [645, 7547, 538, 264, 31782, 11, 411, 264, 24515, 31782, 295, 439, 31782, 11, 264, 11455], "temperature": 0.0, "avg_logprob": -0.11050188836972576, "compression_ratio": 1.9240924092409242, "no_speech_prob": 9.914111433317885e-05}, {"id": 760, "seek": 430296, "start": 4317.2, "end": 4321.76, "text": " transformer and so on. Really, there have just been tons and tons and tons of them. And again,", "tokens": [31782, 293, 370, 322, 13, 4083, 11, 456, 362, 445, 668, 9131, 293, 9131, 293, 9131, 295, 552, 13, 400, 797, 11], "temperature": 0.0, "avg_logprob": -0.11050188836972576, "compression_ratio": 1.9240924092409242, "no_speech_prob": 9.914111433317885e-05}, {"id": 761, "seek": 430296, "start": 4321.76, "end": 4326.88, "text": " the goal in this paper was to take a bunch of these modifications and apply the same basic methodology", "tokens": [264, 3387, 294, 341, 3035, 390, 281, 747, 257, 3840, 295, 613, 26881, 293, 3079, 264, 912, 3875, 24850], "temperature": 0.0, "avg_logprob": -0.11050188836972576, "compression_ratio": 1.9240924092409242, "no_speech_prob": 9.914111433317885e-05}, {"id": 762, "seek": 430296, "start": 4326.88, "end": 4332.24, "text": " from the T5 paper where we test them in the same experimental setting. Specifically, we basically", "tokens": [490, 264, 314, 20, 3035, 689, 321, 1500, 552, 294, 264, 912, 17069, 3287, 13, 26058, 11, 321, 1936], "temperature": 0.0, "avg_logprob": -0.11050188836972576, "compression_ratio": 1.9240924092409242, "no_speech_prob": 9.914111433317885e-05}, {"id": 763, "seek": 433224, "start": 4332.24, "end": 4337.36, "text": " tested them in exactly the T5 setting that I described at the beginning of the talk where we", "tokens": [8246, 552, 294, 2293, 264, 314, 20, 3287, 300, 286, 7619, 412, 264, 2863, 295, 264, 751, 689, 321], "temperature": 0.0, "avg_logprob": -0.0986278501607604, "compression_ratio": 1.6763636363636363, "no_speech_prob": 7.253658986883238e-05}, {"id": 764, "seek": 433224, "start": 4337.36, "end": 4344.639999999999, "text": " pre-trained a T5-based size model on C4 and then fine-tuned it on a few downstream tasks.", "tokens": [659, 12, 17227, 2001, 257, 314, 20, 12, 6032, 2744, 2316, 322, 383, 19, 293, 550, 2489, 12, 83, 43703, 309, 322, 257, 1326, 30621, 9608, 13], "temperature": 0.0, "avg_logprob": -0.0986278501607604, "compression_ratio": 1.6763636363636363, "no_speech_prob": 7.253658986883238e-05}, {"id": 765, "seek": 433224, "start": 4346.16, "end": 4349.679999999999, "text": " I won't discuss that too much more because I gave a pretty thorough introduction to it at the", "tokens": [286, 1582, 380, 2248, 300, 886, 709, 544, 570, 286, 2729, 257, 1238, 12934, 9339, 281, 309, 412, 264], "temperature": 0.0, "avg_logprob": -0.0986278501607604, "compression_ratio": 1.6763636363636363, "no_speech_prob": 7.253658986883238e-05}, {"id": 766, "seek": 433224, "start": 4349.679999999999, "end": 4356.8, "text": " beginning of the talk. And so here is a sort of a first set of results that I'll show you.", "tokens": [2863, 295, 264, 751, 13, 400, 370, 510, 307, 257, 1333, 295, 257, 700, 992, 295, 3542, 300, 286, 603, 855, 291, 13], "temperature": 0.0, "avg_logprob": -0.0986278501607604, "compression_ratio": 1.6763636363636363, "no_speech_prob": 7.253658986883238e-05}, {"id": 767, "seek": 433224, "start": 4357.599999999999, "end": 4361.92, "text": " Along the x-axis are different transformer modifications. I'm not labeling which one is which", "tokens": [17457, 264, 2031, 12, 24633, 366, 819, 31782, 26881, 13, 286, 478, 406, 40244, 597, 472, 307, 597], "temperature": 0.0, "avg_logprob": -0.0986278501607604, "compression_ratio": 1.6763636363636363, "no_speech_prob": 7.253658986883238e-05}, {"id": 768, "seek": 436192, "start": 4361.92, "end": 4368.32, "text": " because I don't want to call any particular modification out. This is the validation loss", "tokens": [570, 286, 500, 380, 528, 281, 818, 604, 1729, 26747, 484, 13, 639, 307, 264, 24071, 4470], "temperature": 0.0, "avg_logprob": -0.08227052328721532, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.00011771827848860994}, {"id": 769, "seek": 436192, "start": 4368.88, "end": 4373.6, "text": " attained by the model for the pre-training objective. So when we do pre-training on C4,", "tokens": [46633, 538, 264, 2316, 337, 264, 659, 12, 17227, 1760, 10024, 13, 407, 562, 321, 360, 659, 12, 17227, 1760, 322, 383, 19, 11], "temperature": 0.0, "avg_logprob": -0.08227052328721532, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.00011771827848860994}, {"id": 770, "seek": 436192, "start": 4373.6, "end": 4377.84, "text": " we hold out some data from C4 and then we basically measure the validation loss", "tokens": [321, 1797, 484, 512, 1412, 490, 383, 19, 293, 550, 321, 1936, 3481, 264, 24071, 4470], "temperature": 0.0, "avg_logprob": -0.08227052328721532, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.00011771827848860994}, {"id": 771, "seek": 436192, "start": 4378.64, "end": 4384.4, "text": " on the held out data. So lower is better in this case. And you can see this dotted black line is", "tokens": [322, 264, 5167, 484, 1412, 13, 407, 3126, 307, 1101, 294, 341, 1389, 13, 400, 291, 393, 536, 341, 37459, 2211, 1622, 307], "temperature": 0.0, "avg_logprob": -0.08227052328721532, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.00011771827848860994}, {"id": 772, "seek": 436192, "start": 4384.4, "end": 4389.2, "text": " the performance of the baseline model without any transformer modifications. It's basically", "tokens": [264, 3389, 295, 264, 20518, 2316, 1553, 604, 31782, 26881, 13, 467, 311, 1936], "temperature": 0.0, "avg_logprob": -0.08227052328721532, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.00011771827848860994}, {"id": 773, "seek": 438920, "start": 4389.2, "end": 4393.84, "text": " of an L-transformer. And you can see that actually some of the transformer modifications did", "tokens": [295, 364, 441, 12, 24999, 837, 260, 13, 400, 291, 393, 536, 300, 767, 512, 295, 264, 31782, 26881, 630], "temperature": 0.0, "avg_logprob": -0.18965742722997125, "compression_ratio": 1.844, "no_speech_prob": 3.2694777473807335e-05}, {"id": 774, "seek": 438920, "start": 4393.84, "end": 4398.88, "text": " attain better performance, which was good. But a lot of them didn't and a lot of them actually", "tokens": [23766, 1101, 3389, 11, 597, 390, 665, 13, 583, 257, 688, 295, 552, 994, 380, 293, 257, 688, 295, 552, 767], "temperature": 0.0, "avg_logprob": -0.18965742722997125, "compression_ratio": 1.844, "no_speech_prob": 3.2694777473807335e-05}, {"id": 775, "seek": 438920, "start": 4398.88, "end": 4405.36, "text": " got significantly worse performance. But maybe even worse, some of these variants of the", "tokens": [658, 10591, 5324, 3389, 13, 583, 1310, 754, 5324, 11, 512, 295, 613, 21669, 295, 264], "temperature": 0.0, "avg_logprob": -0.18965742722997125, "compression_ratio": 1.844, "no_speech_prob": 3.2694777473807335e-05}, {"id": 776, "seek": 438920, "start": 4405.36, "end": 4409.76, "text": " transformer that attained better performance were pretty minor changes. Like for example,", "tokens": [31782, 300, 46633, 1101, 3389, 645, 1238, 6696, 2962, 13, 1743, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.18965742722997125, "compression_ratio": 1.844, "no_speech_prob": 3.2694777473807335e-05}, {"id": 777, "seek": 438920, "start": 4409.76, "end": 4415.28, "text": " just taking the relu in the dense relu dense layer and swapping it with another non-linearity.", "tokens": [445, 1940, 264, 1039, 84, 294, 264, 18011, 1039, 84, 18011, 4583, 293, 1693, 10534, 309, 365, 1071, 2107, 12, 1889, 17409, 13], "temperature": 0.0, "avg_logprob": -0.18965742722997125, "compression_ratio": 1.844, "no_speech_prob": 3.2694777473807335e-05}, {"id": 778, "seek": 441528, "start": 4415.28, "end": 4421.04, "text": " It's a pretty minor change. And some of the other really highly performant ones were actually", "tokens": [467, 311, 257, 1238, 6696, 1319, 13, 400, 512, 295, 264, 661, 534, 5405, 2042, 394, 2306, 645, 767], "temperature": 0.0, "avg_logprob": -0.14361643559724382, "compression_ratio": 1.724, "no_speech_prob": 3.6476012610364705e-05}, {"id": 779, "seek": 441528, "start": 4421.04, "end": 4427.12, "text": " ultimately more expensive models. We did use the same base model. It was the same T5 base size model.", "tokens": [6284, 544, 5124, 5245, 13, 492, 630, 764, 264, 912, 3096, 2316, 13, 467, 390, 264, 912, 314, 20, 3096, 2744, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14361643559724382, "compression_ratio": 1.724, "no_speech_prob": 3.6476012610364705e-05}, {"id": 780, "seek": 441528, "start": 4428.0, "end": 4432.5599999999995, "text": " But some of these methods, like the switch transformer, for example,", "tokens": [583, 512, 295, 613, 7150, 11, 411, 264, 3679, 31782, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.14361643559724382, "compression_ratio": 1.724, "no_speech_prob": 3.6476012610364705e-05}, {"id": 781, "seek": 441528, "start": 4432.5599999999995, "end": 4437.679999999999, "text": " increases the parameter counter-matically. So it has more expensive in terms of memory.", "tokens": [8637, 264, 13075, 5682, 12, 76, 5030, 13, 407, 309, 575, 544, 5124, 294, 2115, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.14361643559724382, "compression_ratio": 1.724, "no_speech_prob": 3.6476012610364705e-05}, {"id": 782, "seek": 441528, "start": 4437.679999999999, "end": 4442.16, "text": " Some of the other methods, by coincidence, maybe if you make the model deeper,", "tokens": [2188, 295, 264, 661, 7150, 11, 538, 22137, 11, 1310, 498, 291, 652, 264, 2316, 7731, 11], "temperature": 0.0, "avg_logprob": -0.14361643559724382, "compression_ratio": 1.724, "no_speech_prob": 3.6476012610364705e-05}, {"id": 783, "seek": 444216, "start": 4442.16, "end": 4448.88, "text": " it's not able to make use of the accelerator as efficiently. And so it makes the", "tokens": [309, 311, 406, 1075, 281, 652, 764, 295, 264, 39889, 382, 19621, 13, 400, 370, 309, 1669, 264], "temperature": 0.0, "avg_logprob": -0.1045584369015384, "compression_ratio": 1.6497695852534562, "no_speech_prob": 5.224371489020996e-05}, {"id": 784, "seek": 444216, "start": 4449.76, "end": 4455.04, "text": " training time and inference time a little more expensive. So once you factor out the very simple", "tokens": [3097, 565, 293, 38253, 565, 257, 707, 544, 5124, 13, 407, 1564, 291, 5952, 484, 264, 588, 2199], "temperature": 0.0, "avg_logprob": -0.1045584369015384, "compression_ratio": 1.6497695852534562, "no_speech_prob": 5.224371489020996e-05}, {"id": 785, "seek": 444216, "start": 4455.04, "end": 4459.599999999999, "text": " changes to the transformer and the ones that ultimately made the model more expensive along some", "tokens": [2962, 281, 264, 31782, 293, 264, 2306, 300, 6284, 1027, 264, 2316, 544, 5124, 2051, 512], "temperature": 0.0, "avg_logprob": -0.1045584369015384, "compression_ratio": 1.6497695852534562, "no_speech_prob": 5.224371489020996e-05}, {"id": 786, "seek": 444216, "start": 4459.599999999999, "end": 4464.639999999999, "text": " axis, there actually were very few, if any, modifications that improved performance", "tokens": [10298, 11, 456, 767, 645, 588, 1326, 11, 498, 604, 11, 26881, 300, 9689, 3389], "temperature": 0.0, "avg_logprob": -0.1045584369015384, "compression_ratio": 1.6497695852534562, "no_speech_prob": 5.224371489020996e-05}, {"id": 787, "seek": 446464, "start": 4464.64, "end": 4472.56, "text": " meaningfully. And this is true on the pre-training task. It's also true on the downstream tasks we", "tokens": [3620, 2277, 13, 400, 341, 307, 2074, 322, 264, 659, 12, 17227, 1760, 5633, 13, 467, 311, 611, 2074, 322, 264, 30621, 9608, 321], "temperature": 0.0, "avg_logprob": -0.19061776307912973, "compression_ratio": 1.6470588235294117, "no_speech_prob": 7.366005593212321e-05}, {"id": 788, "seek": 446464, "start": 4472.56, "end": 4479.04, "text": " considered. So this is the Rouge 2 score. It's just one of the metrics people use on the X-Sum task.", "tokens": [4888, 13, 407, 341, 307, 264, 47607, 568, 6175, 13, 467, 311, 445, 472, 295, 264, 16367, 561, 764, 322, 264, 1783, 12, 50, 449, 5633, 13], "temperature": 0.0, "avg_logprob": -0.19061776307912973, "compression_ratio": 1.6470588235294117, "no_speech_prob": 7.366005593212321e-05}, {"id": 789, "seek": 446464, "start": 4479.04, "end": 4483.360000000001, "text": " X-Sum is you can sort of think of it like a harder version of CNN Daily Mail via the Summization", "tokens": [1783, 12, 50, 449, 307, 291, 393, 1333, 295, 519, 295, 309, 411, 257, 6081, 3037, 295, 24859, 19685, 29164, 5766, 264, 8626, 76, 2144], "temperature": 0.0, "avg_logprob": -0.19061776307912973, "compression_ratio": 1.6470588235294117, "no_speech_prob": 7.366005593212321e-05}, {"id": 790, "seek": 446464, "start": 4483.360000000001, "end": 4490.08, "text": " task. And you can see that the model variance that attained a better validation score tended to", "tokens": [5633, 13, 400, 291, 393, 536, 300, 264, 2316, 21977, 300, 46633, 257, 1101, 24071, 6175, 34732, 281], "temperature": 0.0, "avg_logprob": -0.19061776307912973, "compression_ratio": 1.6470588235294117, "no_speech_prob": 7.366005593212321e-05}, {"id": 791, "seek": 449008, "start": 4490.08, "end": 4496.72, "text": " also attain a better X-Sum Rouge 2 score. But again, almost all of the variance we tried", "tokens": [611, 23766, 257, 1101, 1783, 12, 50, 449, 47607, 568, 6175, 13, 583, 797, 11, 1920, 439, 295, 264, 21977, 321, 3031], "temperature": 0.0, "avg_logprob": -0.09944407145182292, "compression_ratio": 1.6476868327402134, "no_speech_prob": 1.862833414634224e-05}, {"id": 792, "seek": 449008, "start": 4496.72, "end": 4503.44, "text": " decreased the performance. And just as an aside, I kind of alluded to this a little bit.", "tokens": [24436, 264, 3389, 13, 400, 445, 382, 364, 7359, 11, 286, 733, 295, 33919, 281, 341, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.09944407145182292, "compression_ratio": 1.6476868327402134, "no_speech_prob": 1.862833414634224e-05}, {"id": 793, "seek": 449008, "start": 4503.44, "end": 4507.84, "text": " There is a reasonably good correlation between the validation loss and the superglue score.", "tokens": [821, 307, 257, 23551, 665, 20009, 1296, 264, 24071, 4470, 293, 264, 1687, 7191, 622, 6175, 13], "temperature": 0.0, "avg_logprob": -0.09944407145182292, "compression_ratio": 1.6476868327402134, "no_speech_prob": 1.862833414634224e-05}, {"id": 794, "seek": 449008, "start": 4508.96, "end": 4513.28, "text": " Although I'll just point out a couple of interesting points here. One is this method called", "tokens": [5780, 286, 603, 445, 935, 484, 257, 1916, 295, 1880, 2793, 510, 13, 1485, 307, 341, 3170, 1219], "temperature": 0.0, "avg_logprob": -0.09944407145182292, "compression_ratio": 1.6476868327402134, "no_speech_prob": 1.862833414634224e-05}, {"id": 795, "seek": 449008, "start": 4513.28, "end": 4519.44, "text": " transparent attention. It attained a pretty good validation loss, but ultimately a very bad superglue", "tokens": [12737, 3202, 13, 467, 46633, 257, 1238, 665, 24071, 4470, 11, 457, 6284, 257, 588, 1578, 1687, 7191, 622], "temperature": 0.0, "avg_logprob": -0.09944407145182292, "compression_ratio": 1.6476868327402134, "no_speech_prob": 1.862833414634224e-05}, {"id": 796, "seek": 451944, "start": 4519.44, "end": 4523.599999999999, "text": " score, which was surprising to us. The switch transformer, which I'll highlight here,", "tokens": [6175, 11, 597, 390, 8830, 281, 505, 13, 440, 3679, 31782, 11, 597, 286, 603, 5078, 510, 11], "temperature": 0.0, "avg_logprob": -0.10402363421870213, "compression_ratio": 1.695167286245353, "no_speech_prob": 4.399298268253915e-05}, {"id": 797, "seek": 451944, "start": 4524.16, "end": 4528.0, "text": " attained the best validation loss, but it did not get the best superglue score.", "tokens": [46633, 264, 1151, 24071, 4470, 11, 457, 309, 630, 406, 483, 264, 1151, 1687, 7191, 622, 6175, 13], "temperature": 0.0, "avg_logprob": -0.10402363421870213, "compression_ratio": 1.695167286245353, "no_speech_prob": 4.399298268253915e-05}, {"id": 798, "seek": 451944, "start": 4528.96, "end": 4534.5599999999995, "text": " On the closed book of variant of web questions, the switch transformer actually achieved merely", "tokens": [1282, 264, 5395, 1446, 295, 17501, 295, 3670, 1651, 11, 264, 3679, 31782, 767, 11042, 17003], "temperature": 0.0, "avg_logprob": -0.10402363421870213, "compression_ratio": 1.695167286245353, "no_speech_prob": 4.399298268253915e-05}, {"id": 799, "seek": 451944, "start": 4534.5599999999995, "end": 4541.759999999999, "text": " the best validation accuracy. And this kind of supports a loose conjecture in the field that", "tokens": [264, 1151, 24071, 14170, 13, 400, 341, 733, 295, 9346, 257, 9612, 416, 1020, 540, 294, 264, 2519, 300], "temperature": 0.0, "avg_logprob": -0.10402363421870213, "compression_ratio": 1.695167286245353, "no_speech_prob": 4.399298268253915e-05}, {"id": 800, "seek": 451944, "start": 4543.04, "end": 4548.0, "text": " scaling up the number of parameters can prove the amount of knowledge that the model can internalize.", "tokens": [21589, 493, 264, 1230, 295, 9834, 393, 7081, 264, 2372, 295, 3601, 300, 264, 2316, 393, 6920, 1125, 13], "temperature": 0.0, "avg_logprob": -0.10402363421870213, "compression_ratio": 1.695167286245353, "no_speech_prob": 4.399298268253915e-05}, {"id": 801, "seek": 454800, "start": 4548.0, "end": 4553.52, "text": " But it doesn't help the model reason. So kind of very, very loosely speaking. Again,", "tokens": [583, 309, 1177, 380, 854, 264, 2316, 1778, 13, 407, 733, 295, 588, 11, 588, 37966, 4124, 13, 3764, 11], "temperature": 0.0, "avg_logprob": -0.16642772341237486, "compression_ratio": 1.7026022304832713, "no_speech_prob": 5.920061084907502e-05}, {"id": 802, "seek": 454800, "start": 4553.52, "end": 4558.8, "text": " this is kind of a conjecture. Superglue requires deep reasoning capabilities.", "tokens": [341, 307, 733, 295, 257, 416, 1020, 540, 13, 4548, 7191, 622, 7029, 2452, 21577, 10862, 13], "temperature": 0.0, "avg_logprob": -0.16642772341237486, "compression_ratio": 1.7026022304832713, "no_speech_prob": 5.920061084907502e-05}, {"id": 803, "seek": 454800, "start": 4558.8, "end": 4564.16, "text": " Web, close book web questions requires knowledge intensive capabilities. And so the switch transformer,", "tokens": [9573, 11, 1998, 1446, 3670, 1651, 7029, 3601, 18957, 10862, 13, 400, 370, 264, 3679, 31782, 11], "temperature": 0.0, "avg_logprob": -0.16642772341237486, "compression_ratio": 1.7026022304832713, "no_speech_prob": 5.920061084907502e-05}, {"id": 804, "seek": 454800, "start": 4564.16, "end": 4570.16, "text": " which only scales up the parameter count without scaling up the processing maybe does better on this", "tokens": [597, 787, 17408, 493, 264, 13075, 1207, 1553, 21589, 493, 264, 9007, 1310, 775, 1101, 322, 341], "temperature": 0.0, "avg_logprob": -0.16642772341237486, "compression_ratio": 1.7026022304832713, "no_speech_prob": 5.920061084907502e-05}, {"id": 805, "seek": 454800, "start": 4570.16, "end": 4576.96, "text": " on the web questions task. So this kind of raises and should raise some red flags for you.", "tokens": [322, 264, 3670, 1651, 5633, 13, 407, 341, 733, 295, 19658, 293, 820, 5300, 512, 2182, 23265, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.16642772341237486, "compression_ratio": 1.7026022304832713, "no_speech_prob": 5.920061084907502e-05}, {"id": 806, "seek": 457696, "start": 4576.96, "end": 4581.36, "text": " Because this is a pretty bold claim that most of these things don't actually help that much.", "tokens": [1436, 341, 307, 257, 1238, 11928, 3932, 300, 881, 295, 613, 721, 500, 380, 767, 854, 300, 709, 13], "temperature": 0.0, "avg_logprob": -0.1701198609407283, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.00014647873467765749}, {"id": 807, "seek": 457696, "start": 4581.36, "end": 4586.72, "text": " And there's kind of a couple of possible reasons that this could be the case. One is that our", "tokens": [400, 456, 311, 733, 295, 257, 1916, 295, 1944, 4112, 300, 341, 727, 312, 264, 1389, 13, 1485, 307, 300, 527], "temperature": 0.0, "avg_logprob": -0.1701198609407283, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.00014647873467765749}, {"id": 808, "seek": 457696, "start": 4586.72, "end": 4592.24, "text": " code base is just very unusual and non-standard. We don't think this is the case because the code", "tokens": [3089, 3096, 307, 445, 588, 10901, 293, 2107, 12, 1115, 515, 13, 492, 500, 380, 519, 341, 307, 264, 1389, 570, 264, 3089], "temperature": 0.0, "avg_logprob": -0.1701198609407283, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.00014647873467765749}, {"id": 809, "seek": 457696, "start": 4592.24, "end": 4597.52, "text": " base that we used was actually developed by one of the people who invented the transformer,", "tokens": [3096, 300, 321, 1143, 390, 767, 4743, 538, 472, 295, 264, 561, 567, 14479, 264, 31782, 11], "temperature": 0.0, "avg_logprob": -0.1701198609407283, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.00014647873467765749}, {"id": 810, "seek": 457696, "start": 4597.52, "end": 4604.72, "text": " a NOMA Shazir. And it's been used a lot. It's been used in lots of various papers. It's basically", "tokens": [257, 426, 5251, 32, 1160, 921, 347, 13, 400, 309, 311, 668, 1143, 257, 688, 13, 467, 311, 668, 1143, 294, 3195, 295, 3683, 10577, 13, 467, 311, 1936], "temperature": 0.0, "avg_logprob": -0.1701198609407283, "compression_ratio": 1.7362637362637363, "no_speech_prob": 0.00014647873467765749}, {"id": 811, "seek": 460472, "start": 4604.72, "end": 4609.68, "text": " the same as the tensor to tensor code base. And so we think that arguably our code base and", "tokens": [264, 912, 382, 264, 40863, 281, 40863, 3089, 3096, 13, 400, 370, 321, 519, 300, 26771, 527, 3089, 3096, 293], "temperature": 0.0, "avg_logprob": -0.12115302386584582, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.00023763450735714287}, {"id": 812, "seek": 460472, "start": 4609.68, "end": 4615.360000000001, "text": " the implementation detail should be reasonably standard. Maybe the tasks we consider are non-standard.", "tokens": [264, 11420, 2607, 820, 312, 23551, 3832, 13, 2704, 264, 9608, 321, 1949, 366, 2107, 12, 1115, 515, 13], "temperature": 0.0, "avg_logprob": -0.12115302386584582, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.00023763450735714287}, {"id": 813, "seek": 460472, "start": 4615.360000000001, "end": 4621.52, "text": " We think this is probably not true. Pre-training followed by fine tuning is pretty common. Basically,", "tokens": [492, 519, 341, 307, 1391, 406, 2074, 13, 6001, 12, 17227, 1760, 6263, 538, 2489, 15164, 307, 1238, 2689, 13, 8537, 11], "temperature": 0.0, "avg_logprob": -0.12115302386584582, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.00023763450735714287}, {"id": 814, "seek": 460472, "start": 4621.52, "end": 4627.04, "text": " all the tasks we tested out on have state of the art results from transformers. And actually,", "tokens": [439, 264, 9608, 321, 8246, 484, 322, 362, 1785, 295, 264, 1523, 3542, 490, 4088, 433, 13, 400, 767, 11], "temperature": 0.0, "avg_logprob": -0.12115302386584582, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.00023763450735714287}, {"id": 815, "seek": 460472, "start": 4627.04, "end": 4633.84, "text": " we included separately supervised only training on WMT English German, which was the task that", "tokens": [321, 5556, 14759, 46533, 787, 3097, 322, 343, 44, 51, 3669, 6521, 11, 597, 390, 264, 5633, 300], "temperature": 0.0, "avg_logprob": -0.12115302386584582, "compression_ratio": 1.6898954703832754, "no_speech_prob": 0.00023763450735714287}, {"id": 816, "seek": 463384, "start": 4633.84, "end": 4640.96, "text": " transformer was actually proposed on. Maybe we need more hyper parameter tuning because we didn't,", "tokens": [31782, 390, 767, 10348, 322, 13, 2704, 321, 643, 544, 9848, 13075, 15164, 570, 321, 994, 380, 11], "temperature": 0.0, "avg_logprob": -0.09518461407355543, "compression_ratio": 1.8314606741573034, "no_speech_prob": 4.6106310037430376e-05}, {"id": 817, "seek": 463384, "start": 4640.96, "end": 4645.04, "text": " again, we didn't do significant hyper parameter tuning for each of these methods. To test how true", "tokens": [797, 11, 321, 994, 380, 360, 4776, 9848, 13075, 15164, 337, 1184, 295, 613, 7150, 13, 1407, 1500, 577, 2074], "temperature": 0.0, "avg_logprob": -0.09518461407355543, "compression_ratio": 1.8314606741573034, "no_speech_prob": 4.6106310037430376e-05}, {"id": 818, "seek": 463384, "start": 4645.04, "end": 4650.88, "text": " this was, we actually took one of the methods that we performed significantly worse than we expected.", "tokens": [341, 390, 11, 321, 767, 1890, 472, 295, 264, 7150, 300, 321, 10332, 10591, 5324, 813, 321, 5176, 13], "temperature": 0.0, "avg_logprob": -0.09518461407355543, "compression_ratio": 1.8314606741573034, "no_speech_prob": 4.6106310037430376e-05}, {"id": 819, "seek": 463384, "start": 4650.88, "end": 4655.52, "text": " And we ran maybe a couple hundred trials of hyper parameter optimization. One of the researchers", "tokens": [400, 321, 5872, 1310, 257, 1916, 3262, 12450, 295, 9848, 13075, 19618, 13, 1485, 295, 264, 10309], "temperature": 0.0, "avg_logprob": -0.09518461407355543, "compression_ratio": 1.8314606741573034, "no_speech_prob": 4.6106310037430376e-05}, {"id": 820, "seek": 463384, "start": 4655.52, "end": 4659.12, "text": " on this paper spent a long time trying to get hyper parameters right to make it work. And it", "tokens": [322, 341, 3035, 4418, 257, 938, 565, 1382, 281, 483, 9848, 9834, 558, 281, 652, 309, 589, 13, 400, 309], "temperature": 0.0, "avg_logprob": -0.09518461407355543, "compression_ratio": 1.8314606741573034, "no_speech_prob": 4.6106310037430376e-05}, {"id": 821, "seek": 465912, "start": 4659.12, "end": 4665.68, "text": " ultimately never worked as well as the baseline method. Next possibilities that we implemented", "tokens": [6284, 1128, 2732, 382, 731, 382, 264, 20518, 3170, 13, 3087, 12178, 300, 321, 12270], "temperature": 0.0, "avg_logprob": -0.09915197738493332, "compression_ratio": 1.821969696969697, "no_speech_prob": 4.98521258123219e-05}, {"id": 822, "seek": 465912, "start": 4665.68, "end": 4670.32, "text": " these modifications incorrectly. To sanity check this, we actually emailed the authors of all of", "tokens": [613, 26881, 42892, 13, 1407, 47892, 1520, 341, 11, 321, 767, 45460, 264, 16552, 295, 439, 295], "temperature": 0.0, "avg_logprob": -0.09915197738493332, "compression_ratio": 1.821969696969697, "no_speech_prob": 4.98521258123219e-05}, {"id": 823, "seek": 465912, "start": 4670.32, "end": 4674.5599999999995, "text": " the different modifications and asked them to check our implementation. All of the ones that got", "tokens": [264, 819, 26881, 293, 2351, 552, 281, 1520, 527, 11420, 13, 1057, 295, 264, 2306, 300, 658], "temperature": 0.0, "avg_logprob": -0.09915197738493332, "compression_ratio": 1.821969696969697, "no_speech_prob": 4.98521258123219e-05}, {"id": 824, "seek": 465912, "start": 4674.5599999999995, "end": 4678.72, "text": " back to us said that it looked correct to them. And then finally, the last option is that maybe", "tokens": [646, 281, 505, 848, 300, 309, 2956, 3006, 281, 552, 13, 400, 550, 2721, 11, 264, 1036, 3614, 307, 300, 1310], "temperature": 0.0, "avg_logprob": -0.09915197738493332, "compression_ratio": 1.821969696969697, "no_speech_prob": 4.98521258123219e-05}, {"id": 825, "seek": 465912, "start": 4678.72, "end": 4683.44, "text": " these modifications to the transformer don't really kind of transfer. They don't transfer across", "tokens": [613, 26881, 281, 264, 31782, 500, 380, 534, 733, 295, 5003, 13, 814, 500, 380, 5003, 2108], "temperature": 0.0, "avg_logprob": -0.09915197738493332, "compression_ratio": 1.821969696969697, "no_speech_prob": 4.98521258123219e-05}, {"id": 826, "seek": 468344, "start": 4683.44, "end": 4689.2, "text": " code bases and implementations and applications. And to us, at least based on the evidence that we", "tokens": [3089, 17949, 293, 4445, 763, 293, 5821, 13, 400, 281, 505, 11, 412, 1935, 2361, 322, 264, 4467, 300, 321], "temperature": 0.0, "avg_logprob": -0.08323843437328673, "compression_ratio": 1.7625899280575539, "no_speech_prob": 7.483065564883873e-05}, {"id": 827, "seek": 468344, "start": 4689.2, "end": 4695.2, "text": " have, this is a plausible possibility. In my opinion, the best way to control for this is if you're", "tokens": [362, 11, 341, 307, 257, 39925, 7959, 13, 682, 452, 4800, 11, 264, 1151, 636, 281, 1969, 337, 341, 307, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.08323843437328673, "compression_ratio": 1.7625899280575539, "no_speech_prob": 7.483065564883873e-05}, {"id": 828, "seek": 468344, "start": 4695.2, "end": 4700.799999999999, "text": " proposing a new modification to the transformer, try to apply it to as many code bases and tasks as", "tokens": [29939, 257, 777, 26747, 281, 264, 31782, 11, 853, 281, 3079, 309, 281, 382, 867, 3089, 17949, 293, 9608, 382], "temperature": 0.0, "avg_logprob": -0.08323843437328673, "compression_ratio": 1.7625899280575539, "no_speech_prob": 7.483065564883873e-05}, {"id": 829, "seek": 468344, "start": 4700.799999999999, "end": 4705.919999999999, "text": " you can without tweaking hyper parameters. And if it works in all of those settings, then your", "tokens": [291, 393, 1553, 6986, 2456, 9848, 9834, 13, 400, 498, 309, 1985, 294, 439, 295, 729, 6257, 11, 550, 428], "temperature": 0.0, "avg_logprob": -0.08323843437328673, "compression_ratio": 1.7625899280575539, "no_speech_prob": 7.483065564883873e-05}, {"id": 830, "seek": 468344, "start": 4705.919999999999, "end": 4710.16, "text": " golden and then your thing probably will transfer. And we think that it's probably the case that", "tokens": [9729, 293, 550, 428, 551, 1391, 486, 5003, 13, 400, 321, 519, 300, 309, 311, 1391, 264, 1389, 300], "temperature": 0.0, "avg_logprob": -0.08323843437328673, "compression_ratio": 1.7625899280575539, "no_speech_prob": 7.483065564883873e-05}, {"id": 831, "seek": 471016, "start": 4710.16, "end": 4716.32, "text": " simpler modifications like changing the non-ladyarity are not so dependent on hyper parameters and", "tokens": [18587, 26881, 411, 4473, 264, 2107, 12, 75, 880, 17409, 366, 406, 370, 12334, 322, 9848, 9834, 293], "temperature": 0.0, "avg_logprob": -0.12251463351042374, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.8823040085844696e-05}, {"id": 832, "seek": 471016, "start": 4716.32, "end": 4721.68, "text": " implementation details. And so they may be the ones that are more likely to transfer, so to speak.", "tokens": [11420, 4365, 13, 400, 370, 436, 815, 312, 264, 2306, 300, 366, 544, 3700, 281, 5003, 11, 370, 281, 1710, 13], "temperature": 0.0, "avg_logprob": -0.12251463351042374, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.8823040085844696e-05}, {"id": 833, "seek": 471016, "start": 4723.44, "end": 4727.5199999999995, "text": " So that's all discussed in this talk. I recognize that I was kind of a whirlwind tour. So I've", "tokens": [407, 300, 311, 439, 7152, 294, 341, 751, 13, 286, 5521, 300, 286, 390, 733, 295, 257, 35706, 12199, 3512, 13, 407, 286, 600], "temperature": 0.0, "avg_logprob": -0.12251463351042374, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.8823040085844696e-05}, {"id": 834, "seek": 471016, "start": 4727.5199999999995, "end": 4732.96, "text": " linked all of the papers that I discussed on this slide here. Of course, this was work done by", "tokens": [9408, 439, 295, 264, 10577, 300, 286, 7152, 322, 341, 4137, 510, 13, 2720, 1164, 11, 341, 390, 589, 1096, 538], "temperature": 0.0, "avg_logprob": -0.12251463351042374, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.8823040085844696e-05}, {"id": 835, "seek": 471016, "start": 4732.96, "end": 4738.72, "text": " a huge and truly amazing group of collaborators over the course of these five papers who I've listed", "tokens": [257, 2603, 293, 4908, 2243, 1594, 295, 39789, 670, 264, 1164, 295, 613, 1732, 10577, 567, 286, 600, 10052], "temperature": 0.0, "avg_logprob": -0.12251463351042374, "compression_ratio": 1.6827586206896552, "no_speech_prob": 3.8823040085844696e-05}, {"id": 836, "seek": 473872, "start": 4738.72, "end": 4743.84, "text": " on the screen here. And yeah, I'm happy to answer any additional questions that you all have.", "tokens": [322, 264, 2568, 510, 13, 400, 1338, 11, 286, 478, 2055, 281, 1867, 604, 4497, 1651, 300, 291, 439, 362, 13], "temperature": 0.0, "avg_logprob": -0.1262458192891088, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0001678623229963705}, {"id": 837, "seek": 473872, "start": 4744.64, "end": 4748.4800000000005, "text": " Okay, so thank you so much Colin for that great talk. And yeah, it was a bit of a", "tokens": [1033, 11, 370, 1309, 291, 370, 709, 29253, 337, 300, 869, 751, 13, 400, 1338, 11, 309, 390, 257, 857, 295, 257], "temperature": 0.0, "avg_logprob": -0.1262458192891088, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0001678623229963705}, {"id": 838, "seek": 473872, "start": 4748.4800000000005, "end": 4754.72, "text": " fire hose of information. I realize also there was one thing I forgot to say in my introduction. So", "tokens": [2610, 20061, 295, 1589, 13, 286, 4325, 611, 456, 390, 472, 551, 286, 5298, 281, 584, 294, 452, 9339, 13, 407], "temperature": 0.0, "avg_logprob": -0.1262458192891088, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0001678623229963705}, {"id": 839, "seek": 473872, "start": 4754.72, "end": 4762.64, "text": " I guess I need to have an afterward as well, which is that Colin has now started as a professor at", "tokens": [286, 2041, 286, 643, 281, 362, 364, 40411, 382, 731, 11, 597, 307, 300, 29253, 575, 586, 1409, 382, 257, 8304, 412], "temperature": 0.0, "avg_logprob": -0.1262458192891088, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0001678623229963705}, {"id": 840, "seek": 473872, "start": 4762.64, "end": 4767.4400000000005, "text": " the University of North Carolina. So effectively, the University of North Carolina is playing a big", "tokens": [264, 3535, 295, 4067, 11480, 13, 407, 8659, 11, 264, 3535, 295, 4067, 11480, 307, 2433, 257, 955], "temperature": 0.0, "avg_logprob": -0.1262458192891088, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.0001678623229963705}, {"id": 841, "seek": 476744, "start": 4767.44, "end": 4773.28, "text": " part in this course because it was also the source of the Cherokee data that we use for assignment", "tokens": [644, 294, 341, 1164, 570, 309, 390, 611, 264, 4009, 295, 264, 14825, 49319, 1412, 300, 321, 764, 337, 15187], "temperature": 0.0, "avg_logprob": -0.16345949042333316, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00027039198903366923}, {"id": 842, "seek": 476744, "start": 4773.28, "end": 4783.759999999999, "text": " four for the Cherokee English translation. So go tar heels. But yeah, so yeah, so Colin's happy", "tokens": [1451, 337, 264, 14825, 49319, 3669, 12853, 13, 407, 352, 3112, 19502, 13, 583, 1338, 11, 370, 1338, 11, 370, 29253, 311, 2055], "temperature": 0.0, "avg_logprob": -0.16345949042333316, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00027039198903366923}, {"id": 843, "seek": 476744, "start": 4783.759999999999, "end": 4790.32, "text": " to stay and answer some questions. So if you'd like to have more questions, use the raise hand,", "tokens": [281, 1754, 293, 1867, 512, 1651, 13, 407, 498, 291, 1116, 411, 281, 362, 544, 1651, 11, 764, 264, 5300, 1011, 11], "temperature": 0.0, "avg_logprob": -0.16345949042333316, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00027039198903366923}, {"id": 844, "seek": 479032, "start": 4790.32, "end": 4798.48, "text": " and we'll then sort of invite you into the where people can see each other, zoom room, and you know,", "tokens": [293, 321, 603, 550, 1333, 295, 7980, 291, 666, 264, 689, 561, 393, 536, 1184, 661, 11, 8863, 1808, 11, 293, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.12527219825815933, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.000181349299964495}, {"id": 845, "seek": 479032, "start": 4798.48, "end": 4804.0, "text": " if you're up to it, it'd even be nice to turn on your video. So people can see who they're talking to.", "tokens": [498, 291, 434, 493, 281, 309, 11, 309, 1116, 754, 312, 1481, 281, 1261, 322, 428, 960, 13, 407, 561, 393, 536, 567, 436, 434, 1417, 281, 13], "temperature": 0.0, "avg_logprob": -0.12527219825815933, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.000181349299964495}, {"id": 846, "seek": 479032, "start": 4805.2, "end": 4813.12, "text": " And yeah, maybe in the first instance, you should stop sharing the screen. And yeah, if there's", "tokens": [400, 1338, 11, 1310, 294, 264, 700, 5197, 11, 291, 820, 1590, 5414, 264, 2568, 13, 400, 1338, 11, 498, 456, 311], "temperature": 0.0, "avg_logprob": -0.12527219825815933, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.000181349299964495}, {"id": 847, "seek": 479032, "start": 4813.12, "end": 4818.719999999999, "text": " something you want to show again, you can turn it back on. Yeah, maybe I'll just say while people", "tokens": [746, 291, 528, 281, 855, 797, 11, 291, 393, 1261, 309, 646, 322, 13, 865, 11, 1310, 286, 603, 445, 584, 1339, 561], "temperature": 0.0, "avg_logprob": -0.12527219825815933, "compression_ratio": 1.6541666666666666, "no_speech_prob": 0.000181349299964495}, {"id": 848, "seek": 481872, "start": 4818.72, "end": 4824.88, "text": " are still around, on the point of me being a professor at UNC in the event that there are any", "tokens": [366, 920, 926, 11, 322, 264, 935, 295, 385, 885, 257, 8304, 412, 44886, 294, 264, 2280, 300, 456, 366, 604], "temperature": 0.0, "avg_logprob": -0.11282626143446914, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.00028676126385107636}, {"id": 849, "seek": 481872, "start": 4824.88, "end": 4830.240000000001, "text": " masters or undergraduate students in the audience who are applying for PhD programs, the application", "tokens": [19294, 420, 19113, 1731, 294, 264, 4034, 567, 366, 9275, 337, 14476, 4268, 11, 264, 3861], "temperature": 0.0, "avg_logprob": -0.11282626143446914, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.00028676126385107636}, {"id": 850, "seek": 481872, "start": 4830.240000000001, "end": 4836.4800000000005, "text": " deadline for UNC actually has not occurred yet. So if you maybe you wanted apply to another school,", "tokens": [20615, 337, 44886, 767, 575, 406, 11068, 1939, 13, 407, 498, 291, 1310, 291, 1415, 3079, 281, 1071, 1395, 11], "temperature": 0.0, "avg_logprob": -0.11282626143446914, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.00028676126385107636}, {"id": 851, "seek": 481872, "start": 4836.4800000000005, "end": 4840.8, "text": " have another option, you're excited about the work that I presented, you can still apply to UNC.", "tokens": [362, 1071, 3614, 11, 291, 434, 2919, 466, 264, 589, 300, 286, 8212, 11, 291, 393, 920, 3079, 281, 44886, 13], "temperature": 0.0, "avg_logprob": -0.11282626143446914, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.00028676126385107636}, {"id": 852, "seek": 481872, "start": 4840.8, "end": 4846.8, "text": " We have a remarkably late application deadline. So just a plug in case there's anyone who's looking for", "tokens": [492, 362, 257, 37381, 3469, 3861, 20615, 13, 407, 445, 257, 5452, 294, 1389, 456, 311, 2878, 567, 311, 1237, 337], "temperature": 0.0, "avg_logprob": -0.11282626143446914, "compression_ratio": 1.7307692307692308, "no_speech_prob": 0.00028676126385107636}, {"id": 853, "seek": 484680, "start": 4846.8, "end": 4854.88, "text": " a PhD. And UNC is the oldest public university in the nation. And if we do the full, you would see", "tokens": [257, 14476, 13, 400, 44886, 307, 264, 14026, 1908, 5454, 294, 264, 4790, 13, 400, 498, 321, 360, 264, 1577, 11, 291, 576, 536], "temperature": 0.0, "avg_logprob": -0.28601902133815893, "compression_ratio": 1.5129310344827587, "no_speech_prob": 0.0001607494632480666}, {"id": 854, "seek": 484680, "start": 4854.88, "end": 4860.4800000000005, "text": " the investment. And I think we have the second oldest CS department too, which yeah, it's been around", "tokens": [264, 6078, 13, 400, 286, 519, 321, 362, 264, 1150, 14026, 9460, 5882, 886, 11, 597, 1338, 11, 309, 311, 668, 926], "temperature": 0.0, "avg_logprob": -0.28601902133815893, "compression_ratio": 1.5129310344827587, "no_speech_prob": 0.0001607494632480666}, {"id": 855, "seek": 484680, "start": 4860.4800000000005, "end": 4867.92, "text": " for on. It's pretty small. So it's only about 50 faculty. So while we're waiting for someone to", "tokens": [337, 322, 13, 467, 311, 1238, 1359, 13, 407, 309, 311, 787, 466, 2625, 6389, 13, 407, 1339, 321, 434, 3806, 337, 1580, 281], "temperature": 0.0, "avg_logprob": -0.28601902133815893, "compression_ratio": 1.5129310344827587, "no_speech_prob": 0.0001607494632480666}, {"id": 856, "seek": 484680, "start": 4867.92, "end": 4871.4400000000005, "text": " join it, we do have one question already actually from", "tokens": [3917, 309, 11, 321, 360, 362, 472, 1168, 1217, 767, 490], "temperature": 0.0, "avg_logprob": -0.28601902133815893, "compression_ratio": 1.5129310344827587, "no_speech_prob": 0.0001607494632480666}, {"id": 857, "seek": 487144, "start": 4871.44, "end": 4881.679999999999, "text": " you know, hi, things for the lecture. I had an question about earlier when you discussed like", "tokens": [291, 458, 11, 4879, 11, 721, 337, 264, 7991, 13, 286, 632, 364, 1168, 466, 3071, 562, 291, 7152, 411], "temperature": 0.0, "avg_logprob": -0.4330064172614111, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.0002692987909540534}, {"id": 858, "seek": 487144, "start": 4883.5199999999995, "end": 4890.4, "text": " the T-bind overfitting and how many passes you did as I took for it to overfit. So", "tokens": [264, 314, 12, 65, 471, 670, 69, 2414, 293, 577, 867, 11335, 291, 630, 382, 286, 1890, 337, 309, 281, 670, 6845, 13, 407], "temperature": 0.0, "avg_logprob": -0.4330064172614111, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.0002692987909540534}, {"id": 859, "seek": 487144, "start": 4890.4, "end": 4897.04, "text": " I'm curious as to if you think some of the larger models like the 3 billion, 11 billion,", "tokens": [286, 478, 6369, 382, 281, 498, 291, 519, 512, 295, 264, 4833, 5245, 411, 264, 805, 5218, 11, 2975, 5218, 11], "temperature": 0.0, "avg_logprob": -0.4330064172614111, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.0002692987909540534}, {"id": 860, "seek": 489704, "start": 4897.04, "end": 4905.5199999999995, "text": " 11 like the scale before players are overfitting and kind of generally how do you know", "tokens": [2975, 411, 264, 4373, 949, 4150, 366, 670, 69, 2414, 293, 733, 295, 5101, 577, 360, 291, 458], "temperature": 0.0, "avg_logprob": -0.15889587998390198, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00011231432290514931}, {"id": 861, "seek": 489704, "start": 4906.32, "end": 4912.96, "text": " when a model is overfitting, especially on the scale. Yeah, so I mean, if you measure overfitting", "tokens": [562, 257, 2316, 307, 670, 69, 2414, 11, 2318, 322, 264, 4373, 13, 865, 11, 370, 286, 914, 11, 498, 291, 3481, 670, 69, 2414], "temperature": 0.0, "avg_logprob": -0.15889587998390198, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00011231432290514931}, {"id": 862, "seek": 489704, "start": 4912.96, "end": 4917.84, "text": " in sort of the standard way where you compare the loss on training data versus the loss on validation", "tokens": [294, 1333, 295, 264, 3832, 636, 689, 291, 6794, 264, 4470, 322, 3097, 1412, 5717, 264, 4470, 322, 24071], "temperature": 0.0, "avg_logprob": -0.15889587998390198, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00011231432290514931}, {"id": 863, "seek": 489704, "start": 4917.84, "end": 4924.16, "text": " data, we see that even in the in the very, very large models, it's roughly the same, which suggests", "tokens": [1412, 11, 321, 536, 300, 754, 294, 264, 294, 264, 588, 11, 588, 2416, 5245, 11, 309, 311, 9810, 264, 912, 11, 597, 13409], "temperature": 0.0, "avg_logprob": -0.15889587998390198, "compression_ratio": 1.7155555555555555, "no_speech_prob": 0.00011231432290514931}, {"id": 864, "seek": 492416, "start": 4924.16, "end": 4928.639999999999, "text": " kind of in the traditional sense that there is no overfitting. One reason for that is that C4", "tokens": [733, 295, 294, 264, 5164, 2020, 300, 456, 307, 572, 670, 69, 2414, 13, 1485, 1778, 337, 300, 307, 300, 383, 19], "temperature": 0.0, "avg_logprob": -0.05090223325716032, "compression_ratio": 1.8291139240506329, "no_speech_prob": 6.500714516732842e-05}, {"id": 865, "seek": 492416, "start": 4928.639999999999, "end": 4935.92, "text": " is actually big enough that we do just over one pass over it when we train for a trillion tokens.", "tokens": [307, 767, 955, 1547, 300, 321, 360, 445, 670, 472, 1320, 670, 309, 562, 321, 3847, 337, 257, 18723, 22667, 13], "temperature": 0.0, "avg_logprob": -0.05090223325716032, "compression_ratio": 1.8291139240506329, "no_speech_prob": 6.500714516732842e-05}, {"id": 866, "seek": 492416, "start": 4935.92, "end": 4940.72, "text": " And you know, you might hope that you see limited overfitting when you only see each piece of data", "tokens": [400, 291, 458, 11, 291, 1062, 1454, 300, 291, 536, 5567, 670, 69, 2414, 562, 291, 787, 536, 1184, 2522, 295, 1412], "temperature": 0.0, "avg_logprob": -0.05090223325716032, "compression_ratio": 1.8291139240506329, "no_speech_prob": 6.500714516732842e-05}, {"id": 867, "seek": 492416, "start": 4940.72, "end": 4944.72, "text": " basically once over the course of training. So it's sort of like every time you're seeing data,", "tokens": [1936, 1564, 670, 264, 1164, 295, 3097, 13, 407, 309, 311, 1333, 295, 411, 633, 565, 291, 434, 2577, 1412, 11], "temperature": 0.0, "avg_logprob": -0.05090223325716032, "compression_ratio": 1.8291139240506329, "no_speech_prob": 6.500714516732842e-05}, {"id": 868, "seek": 492416, "start": 4944.72, "end": 4948.16, "text": " it's new data. So there's not a huge difference between the data on the training set and the", "tokens": [309, 311, 777, 1412, 13, 407, 456, 311, 406, 257, 2603, 2649, 1296, 264, 1412, 322, 264, 3097, 992, 293, 264], "temperature": 0.0, "avg_logprob": -0.05090223325716032, "compression_ratio": 1.8291139240506329, "no_speech_prob": 6.500714516732842e-05}, {"id": 869, "seek": 492416, "start": 4948.16, "end": 4953.12, "text": " validation set. Of course, there is also this notion of overfitting that's kind of like worst case", "tokens": [24071, 992, 13, 2720, 1164, 11, 456, 307, 611, 341, 10710, 295, 670, 69, 2414, 300, 311, 733, 295, 411, 5855, 1389], "temperature": 0.0, "avg_logprob": -0.05090223325716032, "compression_ratio": 1.8291139240506329, "no_speech_prob": 6.500714516732842e-05}, {"id": 870, "seek": 495312, "start": 4953.12, "end": 4958.96, "text": " overfitting, which ties into the memorization work I mentioned. It does seem that it's possible", "tokens": [670, 69, 2414, 11, 597, 14039, 666, 264, 10560, 2144, 589, 286, 2835, 13, 467, 775, 1643, 300, 309, 311, 1944], "temperature": 0.0, "avg_logprob": -0.14679876617763354, "compression_ratio": 1.5756302521008403, "no_speech_prob": 6.399970152415335e-05}, {"id": 871, "seek": 495312, "start": 4958.96, "end": 4964.72, "text": " for language models to memorize data, even when they do relatively few passes over the training", "tokens": [337, 2856, 5245, 281, 27478, 1412, 11, 754, 562, 436, 360, 7226, 1326, 11335, 670, 264, 3097], "temperature": 0.0, "avg_logprob": -0.14679876617763354, "compression_ratio": 1.5756302521008403, "no_speech_prob": 6.399970152415335e-05}, {"id": 872, "seek": 495312, "start": 4964.72, "end": 4970.16, "text": " data set. And you don't see kind of average case overfitting by comparing the training loss and", "tokens": [1412, 992, 13, 400, 291, 500, 380, 536, 733, 295, 4274, 1389, 670, 69, 2414, 538, 15763, 264, 3097, 4470, 293], "temperature": 0.0, "avg_logprob": -0.14679876617763354, "compression_ratio": 1.5756302521008403, "no_speech_prob": 6.399970152415335e-05}, {"id": 873, "seek": 495312, "start": 4970.16, "end": 4982.24, "text": " the validation loss. Yep. Okay, then as a question of two. Sure, sorry, I was trying to", "tokens": [264, 24071, 4470, 13, 7010, 13, 1033, 11, 550, 382, 257, 1168, 295, 732, 13, 4894, 11, 2597, 11, 286, 390, 1382, 281], "temperature": 0.0, "avg_logprob": -0.14679876617763354, "compression_ratio": 1.5756302521008403, "no_speech_prob": 6.399970152415335e-05}, {"id": 874, "seek": 498224, "start": 4982.24, "end": 4988.719999999999, "text": " unmute my video, but I can't do that for forever reason. First of all, thank you so much.", "tokens": [41445, 452, 960, 11, 457, 286, 393, 380, 360, 300, 337, 5680, 1778, 13, 2386, 295, 439, 11, 1309, 291, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.24534986284044052, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0006259193178266287}, {"id": 875, "seek": 498224, "start": 4988.719999999999, "end": 4996.24, "text": " This is fantastic. I'm sure you're really enjoying it. One thing that I particularly enjoyed was", "tokens": [639, 307, 5456, 13, 286, 478, 988, 291, 434, 534, 9929, 309, 13, 1485, 551, 300, 286, 4098, 4626, 390], "temperature": 0.0, "avg_logprob": -0.24534986284044052, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0006259193178266287}, {"id": 876, "seek": 498224, "start": 4996.24, "end": 5003.12, "text": " the work that you guys did on your training data extraction attack, trying to identify,", "tokens": [264, 589, 300, 291, 1074, 630, 322, 428, 3097, 1412, 30197, 2690, 11, 1382, 281, 5876, 11], "temperature": 0.0, "avg_logprob": -0.24534986284044052, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0006259193178266287}, {"id": 877, "seek": 498224, "start": 5003.12, "end": 5009.12, "text": " really test this hunch on open AI and the EFF support that these models don't actually memorize.", "tokens": [534, 1500, 341, 47630, 322, 1269, 7318, 293, 264, 462, 6345, 1406, 300, 613, 5245, 500, 380, 767, 27478, 13], "temperature": 0.0, "avg_logprob": -0.24534986284044052, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.0006259193178266287}, {"id": 878, "seek": 500912, "start": 5009.12, "end": 5016.08, "text": " Training data. I'm wondering if you have two questions. One, have open AI and EFF", "tokens": [20620, 1412, 13, 286, 478, 6359, 498, 291, 362, 732, 1651, 13, 1485, 11, 362, 1269, 7318, 293, 462, 6345], "temperature": 0.0, "avg_logprob": -0.33391001224517824, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.00014197912241797894}, {"id": 879, "seek": 500912, "start": 5016.08, "end": 5022.08, "text": " since they're telling, since you actually or has your DM actually published, there's also that", "tokens": [1670, 436, 434, 3585, 11, 1670, 291, 767, 420, 575, 428, 15322, 767, 6572, 11, 456, 311, 611, 300], "temperature": 0.0, "avg_logprob": -0.33391001224517824, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.00014197912241797894}, {"id": 880, "seek": 500912, "start": 5022.08, "end": 5027.92, "text": " and they since acknowledged that, you know, well, constructive models may actually do this.", "tokens": [293, 436, 1670, 27262, 300, 11, 291, 458, 11, 731, 11, 30223, 5245, 815, 767, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.33391001224517824, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.00014197912241797894}, {"id": 881, "seek": 500912, "start": 5027.92, "end": 5032.72, "text": " And two, with this approach actually work for detecting other say,", "tokens": [400, 732, 11, 365, 341, 3109, 767, 589, 337, 40237, 661, 584, 11], "temperature": 0.0, "avg_logprob": -0.33391001224517824, "compression_ratio": 1.558139534883721, "no_speech_prob": 0.00014197912241797894}, {"id": 882, "seek": 503272, "start": 5032.72, "end": 5040.08, "text": " accidentally encoded biases towards like extreme biases which are prevalent in some language", "tokens": [15715, 2058, 12340, 32152, 3030, 411, 8084, 32152, 597, 366, 30652, 294, 512, 2856], "temperature": 0.0, "avg_logprob": -0.2172879086265081, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.964264659676701e-05}, {"id": 883, "seek": 503272, "start": 5040.08, "end": 5045.4400000000005, "text": " models, could you be able to create packages that could send those sorts of attacks on these", "tokens": [5245, 11, 727, 291, 312, 1075, 281, 1884, 17401, 300, 727, 2845, 729, 7527, 295, 8122, 322, 613], "temperature": 0.0, "avg_logprob": -0.2172879086265081, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.964264659676701e-05}, {"id": 884, "seek": 503272, "start": 5045.4400000000005, "end": 5051.92, "text": " models and then determine some degree of accuracy on how much these biases actually are present.", "tokens": [5245, 293, 550, 6997, 512, 4314, 295, 14170, 322, 577, 709, 613, 32152, 767, 366, 1974, 13], "temperature": 0.0, "avg_logprob": -0.2172879086265081, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.964264659676701e-05}, {"id": 885, "seek": 503272, "start": 5052.8, "end": 5058.16, "text": " Yeah, so with regards to the first question, I don't know of any official statements that have", "tokens": [865, 11, 370, 365, 14258, 281, 264, 700, 1168, 11, 286, 500, 380, 458, 295, 604, 4783, 12363, 300, 362], "temperature": 0.0, "avg_logprob": -0.2172879086265081, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.964264659676701e-05}, {"id": 886, "seek": 505816, "start": 5058.16, "end": 5063.68, "text": " been made by anyone, but I will say that actually on the memorization paper, we had multiple co-authors", "tokens": [668, 1027, 538, 2878, 11, 457, 286, 486, 584, 300, 767, 322, 264, 10560, 2144, 3035, 11, 321, 632, 3866, 598, 12, 40198, 830], "temperature": 0.0, "avg_logprob": -0.07655546665191651, "compression_ratio": 1.6291666666666667, "no_speech_prob": 4.4682699808618054e-05}, {"id": 887, "seek": 505816, "start": 5063.68, "end": 5069.2, "text": " from open AI. So it was very much a cooperation with them. I mean, you know, we're all scientists", "tokens": [490, 1269, 7318, 13, 407, 309, 390, 588, 709, 257, 14968, 365, 552, 13, 286, 914, 11, 291, 458, 11, 321, 434, 439, 7708], "temperature": 0.0, "avg_logprob": -0.07655546665191651, "compression_ratio": 1.6291666666666667, "no_speech_prob": 4.4682699808618054e-05}, {"id": 888, "seek": 505816, "start": 5069.2, "end": 5073.5199999999995, "text": " and we're all, you know, we all kind of make hypotheses that sometimes turn out correctly and", "tokens": [293, 321, 434, 439, 11, 291, 458, 11, 321, 439, 733, 295, 652, 49969, 300, 2171, 1261, 484, 8944, 293], "temperature": 0.0, "avg_logprob": -0.07655546665191651, "compression_ratio": 1.6291666666666667, "no_speech_prob": 4.4682699808618054e-05}, {"id": 889, "seek": 505816, "start": 5073.5199999999995, "end": 5081.44, "text": " incorrectly. And so I think open AI is definitely aware and on the side of the fact that, yeah,", "tokens": [42892, 13, 400, 370, 286, 519, 1269, 7318, 307, 2138, 3650, 293, 322, 264, 1252, 295, 264, 1186, 300, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.07655546665191651, "compression_ratio": 1.6291666666666667, "no_speech_prob": 4.4682699808618054e-05}, {"id": 890, "seek": 508144, "start": 5081.44, "end": 5089.28, "text": " it is possible that these models might memorize data even when they don't exhibit the traditional", "tokens": [309, 307, 1944, 300, 613, 5245, 1062, 27478, 1412, 754, 562, 436, 500, 380, 20487, 264, 5164], "temperature": 0.0, "avg_logprob": -0.10972953543943517, "compression_ratio": 1.4948979591836735, "no_speech_prob": 4.538902794593014e-05}, {"id": 891, "seek": 508144, "start": 5089.28, "end": 5097.679999999999, "text": " signs of overfitting. To the second point, I, the way that people have kind of measured this in an", "tokens": [7880, 295, 670, 69, 2414, 13, 1407, 264, 1150, 935, 11, 286, 11, 264, 636, 300, 561, 362, 733, 295, 12690, 341, 294, 364], "temperature": 0.0, "avg_logprob": -0.10972953543943517, "compression_ratio": 1.4948979591836735, "no_speech_prob": 4.538902794593014e-05}, {"id": 892, "seek": 508144, "start": 5097.679999999999, "end": 5104.0, "text": " ad hoc way is by feeding a prefix into the model about a particular demographic group or type of", "tokens": [614, 16708, 636, 307, 538, 12919, 257, 46969, 666, 264, 2316, 466, 257, 1729, 26331, 1594, 420, 2010, 295], "temperature": 0.0, "avg_logprob": -0.10972953543943517, "compression_ratio": 1.4948979591836735, "no_speech_prob": 4.538902794593014e-05}, {"id": 893, "seek": 510400, "start": 5104.0, "end": 5112.0, "text": " person and see what the model says about that person. And I think, I think in principle, you can kind", "tokens": [954, 293, 536, 437, 264, 2316, 1619, 466, 300, 954, 13, 400, 286, 519, 11, 286, 519, 294, 8665, 11, 291, 393, 733], "temperature": 0.0, "avg_logprob": -0.11145602215777387, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.0236702514230274e-05}, {"id": 894, "seek": 510400, "start": 5112.0, "end": 5118.4, "text": " of think of our approach as related to that, except that we have this additional step that kind of", "tokens": [295, 519, 295, 527, 3109, 382, 4077, 281, 300, 11, 3993, 300, 321, 362, 341, 4497, 1823, 300, 733, 295], "temperature": 0.0, "avg_logprob": -0.11145602215777387, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.0236702514230274e-05}, {"id": 895, "seek": 510400, "start": 5118.4, "end": 5125.28, "text": " measures whether the model is generating that text because it saw it in its training data, basically,", "tokens": [8000, 1968, 264, 2316, 307, 17746, 300, 2487, 570, 309, 1866, 309, 294, 1080, 3097, 1412, 11, 1936, 11], "temperature": 0.0, "avg_logprob": -0.11145602215777387, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.0236702514230274e-05}, {"id": 896, "seek": 510400, "start": 5126.08, "end": 5132.72, "text": " because the perplexity is excessively low for some continuation compared to a model that wasn't", "tokens": [570, 264, 680, 18945, 507, 307, 9310, 3413, 2295, 337, 512, 29357, 5347, 281, 257, 2316, 300, 2067, 380], "temperature": 0.0, "avg_logprob": -0.11145602215777387, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.0236702514230274e-05}, {"id": 897, "seek": 513272, "start": 5132.72, "end": 5139.12, "text": " trained on the same data. So it might be interesting, for example, if you, if you feed the model a prefix", "tokens": [8895, 322, 264, 912, 1412, 13, 407, 309, 1062, 312, 1880, 11, 337, 1365, 11, 498, 291, 11, 498, 291, 3154, 264, 2316, 257, 46969], "temperature": 0.0, "avg_logprob": -0.1458189222547743, "compression_ratio": 1.735191637630662, "no_speech_prob": 3.5350014513824135e-05}, {"id": 898, "seek": 513272, "start": 5139.12, "end": 5144.64, "text": " that you're asking it to fill in some offensive information about some demographic group,", "tokens": [300, 291, 434, 3365, 309, 281, 2836, 294, 512, 15710, 1589, 466, 512, 26331, 1594, 11], "temperature": 0.0, "avg_logprob": -0.1458189222547743, "compression_ratio": 1.735191637630662, "no_speech_prob": 3.5350014513824135e-05}, {"id": 899, "seek": 513272, "start": 5144.64, "end": 5151.4400000000005, "text": " check whether the perplexity of the model for its continuation is dramatically lower than, you know,", "tokens": [1520, 1968, 264, 680, 18945, 507, 295, 264, 2316, 337, 1080, 29357, 307, 17548, 3126, 813, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1458189222547743, "compression_ratio": 1.735191637630662, "no_speech_prob": 3.5350014513824135e-05}, {"id": 900, "seek": 513272, "start": 5151.4400000000005, "end": 5157.360000000001, "text": " Z-lib, for example. And in that case, you might think that the bias actually, maybe this like bias that", "tokens": [1176, 12, 38270, 11, 337, 1365, 13, 400, 294, 300, 1389, 11, 291, 1062, 519, 300, 264, 12577, 767, 11, 1310, 341, 411, 12577, 300], "temperature": 0.0, "avg_logprob": -0.1458189222547743, "compression_ratio": 1.735191637630662, "no_speech_prob": 3.5350014513824135e-05}, {"id": 901, "seek": 513272, "start": 5157.360000000001, "end": 5162.400000000001, "text": " the model has picked up is because it saw some actually a sentence that looked just like that and", "tokens": [264, 2316, 575, 6183, 493, 307, 570, 309, 1866, 512, 767, 257, 8174, 300, 2956, 445, 411, 300, 293], "temperature": 0.0, "avg_logprob": -0.1458189222547743, "compression_ratio": 1.735191637630662, "no_speech_prob": 3.5350014513824135e-05}, {"id": 902, "seek": 516240, "start": 5162.4, "end": 5169.36, "text": " its training data, or if it's a more kind of loose concept that the model has internalized over", "tokens": [1080, 3097, 1412, 11, 420, 498, 309, 311, 257, 544, 733, 295, 9612, 3410, 300, 264, 2316, 575, 6920, 1602, 670], "temperature": 0.0, "avg_logprob": -0.19863736002068771, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.00011056585208280012}, {"id": 903, "seek": 516240, "start": 5169.36, "end": 5175.679999999999, "text": " the course of training. Good, Dessert. Thank you very much for sharing. Yeah, thanks for the", "tokens": [264, 1164, 295, 3097, 13, 2205, 11, 413, 442, 911, 13, 1044, 291, 588, 709, 337, 5414, 13, 865, 11, 3231, 337, 264], "temperature": 0.0, "avg_logprob": -0.19863736002068771, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.00011056585208280012}, {"id": 904, "seek": 516240, "start": 5175.679999999999, "end": 5184.879999999999, "text": " questions. Okay, so next up, I guess is... Right, thank you for the talk that was super interesting.", "tokens": [1651, 13, 1033, 11, 370, 958, 493, 11, 286, 2041, 307, 485, 1779, 11, 1309, 291, 337, 264, 751, 300, 390, 1687, 1880, 13], "temperature": 0.0, "avg_logprob": -0.19863736002068771, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.00011056585208280012}, {"id": 905, "seek": 518488, "start": 5184.88, "end": 5194.72, "text": " So my question is sort of what are your thoughts on potentially doing like multiple rounds of", "tokens": [407, 452, 1168, 307, 1333, 295, 437, 366, 428, 4598, 322, 7263, 884, 411, 3866, 13757, 295], "temperature": 0.0, "avg_logprob": -0.197541973807595, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0002198637230321765}, {"id": 906, "seek": 518488, "start": 5194.72, "end": 5202.24, "text": " pre-training? So to make it more concrete, you know, like let's say you have a task somewhere,", "tokens": [659, 12, 17227, 1760, 30, 407, 281, 652, 309, 544, 9859, 11, 291, 458, 11, 411, 718, 311, 584, 291, 362, 257, 5633, 4079, 11], "temperature": 0.0, "avg_logprob": -0.197541973807595, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0002198637230321765}, {"id": 907, "seek": 518488, "start": 5202.24, "end": 5209.4400000000005, "text": " like something like response generation, and that's very bespoke to the particular response", "tokens": [411, 746, 411, 4134, 5125, 11, 293, 300, 311, 588, 4097, 48776, 281, 264, 1729, 4134], "temperature": 0.0, "avg_logprob": -0.197541973807595, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0002198637230321765}, {"id": 908, "seek": 520944, "start": 5209.44, "end": 5214.96, "text": " generation data set that you use, but potentially you want to kind of spruce that up by bringing in", "tokens": [5125, 1412, 992, 300, 291, 764, 11, 457, 7263, 291, 528, 281, 733, 295, 637, 41158, 300, 493, 538, 5062, 294], "temperature": 0.0, "avg_logprob": -0.10795599690983805, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.2472011551144533e-05}, {"id": 909, "seek": 520944, "start": 5215.679999999999, "end": 5222.08, "text": " some general dialogue data set that consists of naturalistic human data. So I'm wondering if you", "tokens": [512, 2674, 10221, 1412, 992, 300, 14689, 295, 3303, 3142, 1952, 1412, 13, 407, 286, 478, 6359, 498, 291], "temperature": 0.0, "avg_logprob": -0.10795599690983805, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.2472011551144533e-05}, {"id": 910, "seek": 520944, "start": 5222.08, "end": 5229.04, "text": " kind of have any thoughts or intuitions on how effective it is to maybe start with, you know,", "tokens": [733, 295, 362, 604, 4598, 420, 16224, 626, 322, 577, 4942, 309, 307, 281, 1310, 722, 365, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.10795599690983805, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.2472011551144533e-05}, {"id": 911, "seek": 520944, "start": 5229.04, "end": 5235.36, "text": " the general internet, and then fine tune on this dialogue, unstructured dialogue data set,", "tokens": [264, 2674, 4705, 11, 293, 550, 2489, 10864, 322, 341, 10221, 11, 18799, 46847, 10221, 1412, 992, 11], "temperature": 0.0, "avg_logprob": -0.10795599690983805, "compression_ratio": 1.7085201793721974, "no_speech_prob": 2.2472011551144533e-05}, {"id": 912, "seek": 523536, "start": 5235.36, "end": 5242.0, "text": " and then fine tune on maybe a more kind of tightly-scoped response generation data set.", "tokens": [293, 550, 2489, 10864, 322, 1310, 257, 544, 733, 295, 21952, 12, 4417, 27277, 4134, 5125, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.0961185006534352, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.5461817131144926e-05}, {"id": 913, "seek": 523536, "start": 5242.96, "end": 5249.599999999999, "text": " Yeah, so the technique you're describing sounds pretty similar to this really excellent", "tokens": [865, 11, 370, 264, 6532, 291, 434, 16141, 3263, 1238, 2531, 281, 341, 534, 7103], "temperature": 0.0, "avg_logprob": -0.0961185006534352, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.5461817131144926e-05}, {"id": 914, "seek": 523536, "start": 5250.799999999999, "end": 5255.2, "text": " approach that people now call domain adaptive pre-training or task adaptive pre-training.", "tokens": [3109, 300, 561, 586, 818, 9274, 27912, 659, 12, 17227, 1760, 420, 5633, 27912, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.0961185006534352, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.5461817131144926e-05}, {"id": 915, "seek": 523536, "start": 5255.2, "end": 5260.639999999999, "text": " I was introduced in a paper called Don't Stop Pre-Training, and then there's a less catchy", "tokens": [286, 390, 7268, 294, 257, 3035, 1219, 1468, 380, 5535, 6001, 12, 38971, 1760, 11, 293, 550, 456, 311, 257, 1570, 47168], "temperature": 0.0, "avg_logprob": -0.0961185006534352, "compression_ratio": 1.5682819383259912, "no_speech_prob": 2.5461817131144926e-05}, {"id": 916, "seek": 526064, "start": 5260.64, "end": 5267.6, "text": " subtitle. And the idea is very similar to what you proposed. Basically, you take a pre-trained", "tokens": [30706, 306, 13, 400, 264, 1558, 307, 588, 2531, 281, 437, 291, 10348, 13, 8537, 11, 291, 747, 257, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.14553681441715785, "compression_ratio": 1.8143939393939394, "no_speech_prob": 3.4262342524016276e-05}, {"id": 917, "seek": 526064, "start": 5267.6, "end": 5273.52, "text": " model that was trained on generic text, you do what you might call intermediate task training,", "tokens": [2316, 300, 390, 8895, 322, 19577, 2487, 11, 291, 360, 437, 291, 1062, 818, 19376, 5633, 3097, 11], "temperature": 0.0, "avg_logprob": -0.14553681441715785, "compression_ratio": 1.8143939393939394, "no_speech_prob": 3.4262342524016276e-05}, {"id": 918, "seek": 526064, "start": 5273.52, "end": 5278.4800000000005, "text": " or you do continued pre-training on domain specific data, and then finally you do fine tuning on", "tokens": [420, 291, 360, 7014, 659, 12, 17227, 1760, 322, 9274, 2685, 1412, 11, 293, 550, 2721, 291, 360, 2489, 15164, 322], "temperature": 0.0, "avg_logprob": -0.14553681441715785, "compression_ratio": 1.8143939393939394, "no_speech_prob": 3.4262342524016276e-05}, {"id": 919, "seek": 526064, "start": 5278.4800000000005, "end": 5284.08, "text": " your specific fine tuning data set. In their case, they're considering things like, you know,", "tokens": [428, 2685, 2489, 15164, 1412, 992, 13, 682, 641, 1389, 11, 436, 434, 8079, 721, 411, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.14553681441715785, "compression_ratio": 1.8143939393939394, "no_speech_prob": 3.4262342524016276e-05}, {"id": 920, "seek": 526064, "start": 5284.08, "end": 5289.76, "text": " fine, like doing a scientific text classification or biomedical text analysis, and when they do an", "tokens": [2489, 11, 411, 884, 257, 8134, 2487, 21538, 420, 49775, 2487, 5215, 11, 293, 562, 436, 360, 364], "temperature": 0.0, "avg_logprob": -0.14553681441715785, "compression_ratio": 1.8143939393939394, "no_speech_prob": 3.4262342524016276e-05}, {"id": 921, "seek": 528976, "start": 5289.76, "end": 5296.24, "text": " intermediate pre-training step on in-domain data, or even just doing the pre-training objective", "tokens": [19376, 659, 12, 17227, 1760, 1823, 322, 294, 12, 4121, 491, 1412, 11, 420, 754, 445, 884, 264, 659, 12, 17227, 1760, 10024], "temperature": 0.0, "avg_logprob": -0.07237381630755485, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.3057705372339115e-05}, {"id": 922, "seek": 528976, "start": 5296.24, "end": 5303.68, "text": " on the data from the task, it definitely helps significantly. And yeah, so that's a very excellent", "tokens": [322, 264, 1412, 490, 264, 5633, 11, 309, 2138, 3665, 10591, 13, 400, 1338, 11, 370, 300, 311, 257, 588, 7103], "temperature": 0.0, "avg_logprob": -0.07237381630755485, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.3057705372339115e-05}, {"id": 923, "seek": 528976, "start": 5303.68, "end": 5310.96, "text": " intuition, and I think that's the most similar method to what you're describing. It does kind of", "tokens": [24002, 11, 293, 286, 519, 300, 311, 264, 881, 2531, 3170, 281, 437, 291, 434, 16141, 13, 467, 775, 733, 295], "temperature": 0.0, "avg_logprob": -0.07237381630755485, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.3057705372339115e-05}, {"id": 924, "seek": 528976, "start": 5310.96, "end": 5315.6, "text": " raise a clear question that I don't think has been addressed to my knowledge in the literature,", "tokens": [5300, 257, 1850, 1168, 300, 286, 500, 380, 519, 575, 668, 13847, 281, 452, 3601, 294, 264, 10394, 11], "temperature": 0.0, "avg_logprob": -0.07237381630755485, "compression_ratio": 1.5925925925925926, "no_speech_prob": 5.3057705372339115e-05}, {"id": 925, "seek": 531560, "start": 5315.6, "end": 5320.4800000000005, "text": " which is, you know, we usually think of transfer learning as pre-trained and then fine-tune. And", "tokens": [597, 307, 11, 291, 458, 11, 321, 2673, 519, 295, 5003, 2539, 382, 659, 12, 17227, 2001, 293, 550, 2489, 12, 83, 2613, 13, 400], "temperature": 0.0, "avg_logprob": -0.11163280487060546, "compression_ratio": 1.9115384615384616, "no_speech_prob": 5.47470262972638e-05}, {"id": 926, "seek": 531560, "start": 5320.4800000000005, "end": 5325.84, "text": " now we're doing kind of like pre-trained, and then maybe some more pre-training and then fine-tuning.", "tokens": [586, 321, 434, 884, 733, 295, 411, 659, 12, 17227, 2001, 11, 293, 550, 1310, 512, 544, 659, 12, 17227, 1760, 293, 550, 2489, 12, 83, 37726, 13], "temperature": 0.0, "avg_logprob": -0.11163280487060546, "compression_ratio": 1.9115384615384616, "no_speech_prob": 5.47470262972638e-05}, {"id": 927, "seek": 531560, "start": 5325.84, "end": 5331.200000000001, "text": " And there are other methods that kind of inject other steps along the way. And so there's this", "tokens": [400, 456, 366, 661, 7150, 300, 733, 295, 10711, 661, 4439, 2051, 264, 636, 13, 400, 370, 456, 311, 341], "temperature": 0.0, "avg_logprob": -0.11163280487060546, "compression_ratio": 1.9115384615384616, "no_speech_prob": 5.47470262972638e-05}, {"id": 928, "seek": 531560, "start": 5331.200000000001, "end": 5338.4800000000005, "text": " natural question of like, what should the curriculum of tasks be, you know, how many intermediate steps", "tokens": [3303, 1168, 295, 411, 11, 437, 820, 264, 14302, 295, 9608, 312, 11, 291, 458, 11, 577, 867, 19376, 4439], "temperature": 0.0, "avg_logprob": -0.11163280487060546, "compression_ratio": 1.9115384615384616, "no_speech_prob": 5.47470262972638e-05}, {"id": 929, "seek": 531560, "start": 5338.4800000000005, "end": 5343.68, "text": " should there be? What should the intermediate steps be? What's the benefit of one domain versus the", "tokens": [820, 456, 312, 30, 708, 820, 264, 19376, 4439, 312, 30, 708, 311, 264, 5121, 295, 472, 9274, 5717, 264], "temperature": 0.0, "avg_logprob": -0.11163280487060546, "compression_ratio": 1.9115384615384616, "no_speech_prob": 5.47470262972638e-05}, {"id": 930, "seek": 534368, "start": 5343.68, "end": 5348.88, "text": " other? How much domain shift is there? And what are the corresponding benefits and so on? And I", "tokens": [661, 30, 1012, 709, 9274, 5513, 307, 456, 30, 400, 437, 366, 264, 11760, 5311, 293, 370, 322, 30, 400, 286], "temperature": 0.0, "avg_logprob": -0.18475569510946468, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.826580309076235e-05}, {"id": 931, "seek": 534368, "start": 5348.88, "end": 5353.92, "text": " think there would be, there's a fascinating line of work that would be basically better answering", "tokens": [519, 456, 576, 312, 11, 456, 311, 257, 10343, 1622, 295, 589, 300, 576, 312, 1936, 1101, 13430], "temperature": 0.0, "avg_logprob": -0.18475569510946468, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.826580309076235e-05}, {"id": 932, "seek": 534368, "start": 5353.92, "end": 5365.76, "text": " those questions. And what was the acronym? Yeah, so it's called Adapt or Taped, Domain Adaptive", "tokens": [729, 1651, 13, 400, 437, 390, 264, 39195, 30, 865, 11, 370, 309, 311, 1219, 49643, 420, 13445, 292, 11, 16674, 491, 49643, 488], "temperature": 0.0, "avg_logprob": -0.18475569510946468, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.826580309076235e-05}, {"id": 933, "seek": 534368, "start": 5365.76, "end": 5370.4800000000005, "text": " Pre-training or Task Adaptive Pre-training. The other papers called Don't Stop Pre-training,", "tokens": [6001, 12, 17227, 1760, 420, 30428, 49643, 488, 6001, 12, 17227, 1760, 13, 440, 661, 10577, 1219, 1468, 380, 5535, 6001, 12, 17227, 1760, 11], "temperature": 0.0, "avg_logprob": -0.18475569510946468, "compression_ratio": 1.6608695652173913, "no_speech_prob": 5.826580309076235e-05}, {"id": 934, "seek": 537048, "start": 5370.48, "end": 5377.679999999999, "text": " which is easy to remember if you like the song Don't Stop Believe in, which is how I, I don't know", "tokens": [597, 307, 1858, 281, 1604, 498, 291, 411, 264, 2153, 1468, 380, 5535, 21486, 294, 11, 597, 307, 577, 286, 11, 286, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.14796366218392176, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.00020323517674114555}, {"id": 935, "seek": 537048, "start": 5377.679999999999, "end": 5382.879999999999, "text": " if it's an intended reference to that song. I assume it must be. I think they should have done a", "tokens": [498, 309, 311, 364, 10226, 6408, 281, 300, 2153, 13, 286, 6552, 309, 1633, 312, 13, 286, 519, 436, 820, 362, 1096, 257], "temperature": 0.0, "avg_logprob": -0.14796366218392176, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.00020323517674114555}, {"id": 936, "seek": 537048, "start": 5382.879999999999, "end": 5387.2, "text": " Don't Stop Pre-training with an apostrophe if they really wanted to drive it home, but, you know,", "tokens": [1468, 380, 5535, 6001, 12, 17227, 1760, 365, 364, 19484, 27194, 498, 436, 534, 1415, 281, 3332, 309, 1280, 11, 457, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.14796366218392176, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.00020323517674114555}, {"id": 937, "seek": 537048, "start": 5387.2, "end": 5390.5599999999995, "text": " maybe that would have been too cheesy. But anyways, yeah, the papers called Don't Stop Pre-training.", "tokens": [1310, 300, 576, 362, 668, 886, 32549, 13, 583, 13448, 11, 1338, 11, 264, 10577, 1219, 1468, 380, 5535, 6001, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.14796366218392176, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.00020323517674114555}, {"id": 938, "seek": 537048, "start": 5391.679999999999, "end": 5397.36, "text": " All right, that's great. Thank you so much. Yeah, absolutely. Okay, next question is,", "tokens": [1057, 558, 11, 300, 311, 869, 13, 1044, 291, 370, 709, 13, 865, 11, 3122, 13, 1033, 11, 958, 1168, 307, 11], "temperature": 0.0, "avg_logprob": -0.14796366218392176, "compression_ratio": 1.643835616438356, "no_speech_prob": 0.00020323517674114555}, {"id": 939, "seek": 539736, "start": 5397.36, "end": 5404.96, "text": " and I'm not sure quite what they know corresponds to. Yes, hi, thank you, Colin, for that", "tokens": [293, 286, 478, 406, 988, 1596, 437, 436, 458, 23249, 281, 13, 1079, 11, 4879, 11, 1309, 291, 11, 29253, 11, 337, 300], "temperature": 0.0, "avg_logprob": -0.49785013114456583, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.00039683550130575895}, {"id": 940, "seek": 539736, "start": 5404.96, "end": 5410.4, "text": " part of the really interesting. I'm again kind of looking at the question, I'm really looking for", "tokens": [644, 295, 264, 534, 1880, 13, 286, 478, 797, 733, 295, 1237, 412, 264, 1168, 11, 286, 478, 534, 1237, 337], "temperature": 0.0, "avg_logprob": -0.49785013114456583, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.00039683550130575895}, {"id": 941, "seek": 539736, "start": 5410.4, "end": 5415.28, "text": " some advice here. So it feels like the recent headline grabbing your dancements and the end of", "tokens": [512, 5192, 510, 13, 407, 309, 3417, 411, 264, 5162, 28380, 23771, 428, 4489, 1117, 293, 264, 917, 295], "temperature": 0.0, "avg_logprob": -0.49785013114456583, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.00039683550130575895}, {"id": 942, "seek": 539736, "start": 5415.28, "end": 5420.719999999999, "text": " being the tree have been achieved by building these massive models, like GPT-3, with down to", "tokens": [885, 264, 4230, 362, 668, 11042, 538, 2390, 613, 5994, 5245, 11, 411, 26039, 51, 12, 18, 11, 365, 760, 281], "temperature": 0.0, "avg_logprob": -0.49785013114456583, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.00039683550130575895}, {"id": 943, "seek": 539736, "start": 5420.719999999999, "end": 5426.16, "text": " parameters that oftentimes cost millions of dollars you can. And these dancements are funded by", "tokens": [9834, 300, 18349, 2063, 6803, 295, 3808, 291, 393, 13, 400, 613, 4489, 1117, 366, 14385, 538], "temperature": 0.0, "avg_logprob": -0.49785013114456583, "compression_ratio": 1.6354166666666667, "no_speech_prob": 0.00039683550130575895}, {"id": 944, "seek": 542616, "start": 5426.16, "end": 5431.36, "text": " like larger organizations like Google, Facebook, OpenAI, who are less have infinite resources in", "tokens": [411, 4833, 6150, 411, 3329, 11, 4384, 11, 7238, 48698, 11, 567, 366, 1570, 362, 13785, 3593, 294], "temperature": 0.0, "avg_logprob": -0.26020515914511894, "compression_ratio": 1.6175438596491227, "no_speech_prob": 0.0002033174387179315}, {"id": 945, "seek": 542616, "start": 5431.36, "end": 5437.68, "text": " the way, right? So my question is, you know, as a sole practitioner with limited resources, but", "tokens": [264, 636, 11, 558, 30, 407, 452, 1168, 307, 11, 291, 458, 11, 382, 257, 12321, 32125, 365, 5567, 3593, 11, 457], "temperature": 0.0, "avg_logprob": -0.26020515914511894, "compression_ratio": 1.6175438596491227, "no_speech_prob": 0.0002033174387179315}, {"id": 946, "seek": 542616, "start": 5437.68, "end": 5442.88, "text": " an infinite appetite for learning. What are some ways that I can participate in these", "tokens": [364, 13785, 23996, 337, 2539, 13, 708, 366, 512, 2098, 300, 286, 393, 8197, 294, 613], "temperature": 0.0, "avg_logprob": -0.26020515914511894, "compression_ratio": 1.6175438596491227, "no_speech_prob": 0.0002033174387179315}, {"id": 947, "seek": 542616, "start": 5442.88, "end": 5448.4, "text": " advancements and kind of, you know, just rotate and we're top-ending in the industry. Yeah,", "tokens": [7295, 1117, 293, 733, 295, 11, 291, 458, 11, 445, 13121, 293, 321, 434, 1192, 12, 2029, 294, 264, 3518, 13, 865, 11], "temperature": 0.0, "avg_logprob": -0.26020515914511894, "compression_ratio": 1.6175438596491227, "no_speech_prob": 0.0002033174387179315}, {"id": 948, "seek": 542616, "start": 5448.4, "end": 5453.28, "text": " absolutely. I mean, I, I, I actually totally sympathize with you and agree with you in the", "tokens": [3122, 13, 286, 914, 11, 286, 11, 286, 11, 286, 767, 3879, 22276, 1125, 365, 291, 293, 3986, 365, 291, 294, 264], "temperature": 0.0, "avg_logprob": -0.26020515914511894, "compression_ratio": 1.6175438596491227, "no_speech_prob": 0.0002033174387179315}, {"id": 949, "seek": 545328, "start": 5453.28, "end": 5457.759999999999, "text": " sense that, you know, most of the development of these models is taking place by small groups", "tokens": [2020, 300, 11, 291, 458, 11, 881, 295, 264, 3250, 295, 613, 5245, 307, 1940, 1081, 538, 1359, 3935], "temperature": 0.0, "avg_logprob": -0.08854768150731136, "compression_ratio": 1.7321428571428572, "no_speech_prob": 8.34449238027446e-05}, {"id": 950, "seek": 545328, "start": 5458.32, "end": 5463.759999999999, "text": " behind closed doors at at large corporations. And that's not usually how I like to see, you know,", "tokens": [2261, 5395, 8077, 412, 412, 2416, 17676, 13, 400, 300, 311, 406, 2673, 577, 286, 411, 281, 536, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.08854768150731136, "compression_ratio": 1.7321428571428572, "no_speech_prob": 8.34449238027446e-05}, {"id": 951, "seek": 545328, "start": 5463.759999999999, "end": 5468.4, "text": " science developed. I like to see it as a community endeavor that involves, you know, all kinds of", "tokens": [3497, 4743, 13, 286, 411, 281, 536, 309, 382, 257, 1768, 34975, 300, 11626, 11, 291, 458, 11, 439, 3685, 295], "temperature": 0.0, "avg_logprob": -0.08854768150731136, "compression_ratio": 1.7321428571428572, "no_speech_prob": 8.34449238027446e-05}, {"id": 952, "seek": 545328, "start": 5468.4, "end": 5472.5599999999995, "text": " stakeholders with varying amounts of resources. And we're not really quite at that stage with", "tokens": [17779, 365, 22984, 11663, 295, 3593, 13, 400, 321, 434, 406, 534, 1596, 412, 300, 3233, 365], "temperature": 0.0, "avg_logprob": -0.08854768150731136, "compression_ratio": 1.7321428571428572, "no_speech_prob": 8.34449238027446e-05}, {"id": 953, "seek": 545328, "start": 5472.5599999999995, "end": 5480.0, "text": " with this work yet. I do think that to the extent that people are still releasing pre-trained models,", "tokens": [365, 341, 589, 1939, 13, 286, 360, 519, 300, 281, 264, 8396, 300, 561, 366, 920, 16327, 659, 12, 17227, 2001, 5245, 11], "temperature": 0.0, "avg_logprob": -0.08854768150731136, "compression_ratio": 1.7321428571428572, "no_speech_prob": 8.34449238027446e-05}, {"id": 954, "seek": 548000, "start": 5480.0, "end": 5486.88, "text": " which is true, for example, for T5, but, but not for GPT-3, there is a lot of work to be done on", "tokens": [597, 307, 2074, 11, 337, 1365, 11, 337, 314, 20, 11, 457, 11, 457, 406, 337, 26039, 51, 12, 18, 11, 456, 307, 257, 688, 295, 589, 281, 312, 1096, 322], "temperature": 0.0, "avg_logprob": -0.13588580147164767, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0001022537107928656}, {"id": 955, "seek": 548000, "start": 5486.88, "end": 5492.16, "text": " basically analysis. Some of the stuff that that we were discussing earlier, you know, even the,", "tokens": [1936, 5215, 13, 2188, 295, 264, 1507, 300, 300, 321, 645, 10850, 3071, 11, 291, 458, 11, 754, 264, 11], "temperature": 0.0, "avg_logprob": -0.13588580147164767, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0001022537107928656}, {"id": 956, "seek": 548000, "start": 5492.96, "end": 5497.28, "text": " memorization work is basically, I would say it's like analysis work. Some of the stuff", "tokens": [10560, 2144, 589, 307, 1936, 11, 286, 576, 584, 309, 311, 411, 5215, 589, 13, 2188, 295, 264, 1507], "temperature": 0.0, "avg_logprob": -0.13588580147164767, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0001022537107928656}, {"id": 957, "seek": 548000, "start": 5497.28, "end": 5502.48, "text": " pertaining to bias involves in analyzing these models. And I think there's, you know, there's,", "tokens": [49582, 281, 12577, 11626, 294, 23663, 613, 5245, 13, 400, 286, 519, 456, 311, 11, 291, 458, 11, 456, 311, 11], "temperature": 0.0, "avg_logprob": -0.13588580147164767, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0001022537107928656}, {"id": 958, "seek": 548000, "start": 5502.48, "end": 5509.36, "text": " there's so little that we actually know about how these models work and what makes them useful at", "tokens": [456, 311, 370, 707, 300, 321, 767, 458, 466, 577, 613, 5245, 589, 293, 437, 1669, 552, 4420, 412], "temperature": 0.0, "avg_logprob": -0.13588580147164767, "compression_ratio": 1.754646840148699, "no_speech_prob": 0.0001022537107928656}, {"id": 959, "seek": 550936, "start": 5509.36, "end": 5517.36, "text": " scale. The, there's plenty of room for interesting analytical work, which requires significantly", "tokens": [4373, 13, 440, 11, 456, 311, 7140, 295, 1808, 337, 1880, 29579, 589, 11, 597, 7029, 10591], "temperature": 0.0, "avg_logprob": -0.09823677653358096, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.473949931911193e-05}, {"id": 960, "seek": 550936, "start": 5517.36, "end": 5526.96, "text": " less compute. I guess I would say a couple other things. One is that I do really hope that the field", "tokens": [1570, 14722, 13, 286, 2041, 286, 576, 584, 257, 1916, 661, 721, 13, 1485, 307, 300, 286, 360, 534, 1454, 300, 264, 2519], "temperature": 0.0, "avg_logprob": -0.09823677653358096, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.473949931911193e-05}, {"id": 961, "seek": 550936, "start": 5526.96, "end": 5532.799999999999, "text": " moves more towards community development models and moves towards frameworks that allow people to", "tokens": [6067, 544, 3030, 1768, 3250, 5245, 293, 6067, 3030, 29834, 300, 2089, 561, 281], "temperature": 0.0, "avg_logprob": -0.09823677653358096, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.473949931911193e-05}, {"id": 962, "seek": 550936, "start": 5532.799999999999, "end": 5537.679999999999, "text": " collaboratively train a model, for example, like in a distributed fashion. I think that's an", "tokens": [16555, 356, 3847, 257, 2316, 11, 337, 1365, 11, 411, 294, 257, 12631, 6700, 13, 286, 519, 300, 311, 364], "temperature": 0.0, "avg_logprob": -0.09823677653358096, "compression_ratio": 1.5901639344262295, "no_speech_prob": 5.473949931911193e-05}, {"id": 963, "seek": 553768, "start": 5537.68, "end": 5541.84, "text": " incredibly setting research direction. It's something that I'm working with my students on in my", "tokens": [6252, 3287, 2132, 3513, 13, 467, 311, 746, 300, 286, 478, 1364, 365, 452, 1731, 322, 294, 452], "temperature": 0.0, "avg_logprob": -0.10239559266625381, "compression_ratio": 1.6542372881355931, "no_speech_prob": 6.812672654632479e-05}, {"id": 964, "seek": 553768, "start": 5541.84, "end": 5549.4400000000005, "text": " lab at UNC now. And, and the last thing I'll say, and I actually don't usually like saying this,", "tokens": [2715, 412, 44886, 586, 13, 400, 11, 293, 264, 1036, 551, 286, 603, 584, 11, 293, 286, 767, 500, 380, 2673, 411, 1566, 341, 11], "temperature": 0.0, "avg_logprob": -0.10239559266625381, "compression_ratio": 1.6542372881355931, "no_speech_prob": 6.812672654632479e-05}, {"id": 965, "seek": 553768, "start": 5549.4400000000005, "end": 5557.4400000000005, "text": " but I'll say it anyways, I do think that our field often undergoes sort of a tick-tock pattern", "tokens": [457, 286, 603, 584, 309, 13448, 11, 286, 360, 519, 300, 527, 2519, 2049, 26426, 279, 1333, 295, 257, 5204, 12, 1353, 547, 5102], "temperature": 0.0, "avg_logprob": -0.10239559266625381, "compression_ratio": 1.6542372881355931, "no_speech_prob": 6.812672654632479e-05}, {"id": 966, "seek": 553768, "start": 5557.4400000000005, "end": 5562.0, "text": " where we show something as possible at scale, and then we show that the scale is not necessary to", "tokens": [689, 321, 855, 746, 382, 1944, 412, 4373, 11, 293, 550, 321, 855, 300, 264, 4373, 307, 406, 4818, 281], "temperature": 0.0, "avg_logprob": -0.10239559266625381, "compression_ratio": 1.6542372881355931, "no_speech_prob": 6.812672654632479e-05}, {"id": 967, "seek": 553768, "start": 5562.0, "end": 5567.200000000001, "text": " achieve the same thing. And to some extent, you could argue that this has happened already for GPT-3,", "tokens": [4584, 264, 912, 551, 13, 400, 281, 512, 8396, 11, 291, 727, 9695, 300, 341, 575, 2011, 1217, 337, 26039, 51, 12, 18, 11], "temperature": 0.0, "avg_logprob": -0.10239559266625381, "compression_ratio": 1.6542372881355931, "no_speech_prob": 6.812672654632479e-05}, {"id": 968, "seek": 556720, "start": 5567.2, "end": 5572.16, "text": " in the sense that we saw GPT-3 come along, get outstanding results on, for example, superglue,", "tokens": [294, 264, 2020, 300, 321, 1866, 26039, 51, 12, 18, 808, 2051, 11, 483, 14485, 3542, 322, 11, 337, 1365, 11, 1687, 7191, 622, 11], "temperature": 0.0, "avg_logprob": -0.1832521687383237, "compression_ratio": 1.5776892430278884, "no_speech_prob": 0.0001272375084226951}, {"id": 969, "seek": 556720, "start": 5572.16, "end": 5576.96, "text": " with only 32 examples per class. And then there was the paper that proposed this method called", "tokens": [365, 787, 8858, 5110, 680, 1508, 13, 400, 550, 456, 390, 264, 3035, 300, 10348, 341, 3170, 1219], "temperature": 0.0, "avg_logprob": -0.1832521687383237, "compression_ratio": 1.5776892430278884, "no_speech_prob": 0.0001272375084226951}, {"id": 970, "seek": 556720, "start": 5576.96, "end": 5584.4, "text": " iPad, which I think is interactive, an iterative patterned, exploited training that obtained basically", "tokens": [12945, 11, 597, 286, 519, 307, 15141, 11, 364, 17138, 1166, 5102, 292, 11, 40918, 3097, 300, 14879, 1936], "temperature": 0.0, "avg_logprob": -0.1832521687383237, "compression_ratio": 1.5776892430278884, "no_speech_prob": 0.0001272375084226951}, {"id": 971, "seek": 556720, "start": 5584.4, "end": 5591.5199999999995, "text": " comparable performance in a dramatically smaller model with the same amount of data. And, and you know,", "tokens": [25323, 3389, 294, 257, 17548, 4356, 2316, 365, 264, 912, 2372, 295, 1412, 13, 400, 11, 293, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1832521687383237, "compression_ratio": 1.5776892430278884, "no_speech_prob": 0.0001272375084226951}, {"id": 972, "seek": 559152, "start": 5591.52, "end": 5597.120000000001, "text": " I think you can point to other examples. I personally like to attribute the story of", "tokens": [286, 519, 291, 393, 935, 281, 661, 5110, 13, 286, 5665, 411, 281, 19667, 264, 1657, 295], "temperature": 0.0, "avg_logprob": -0.1325725606969885, "compression_ratio": 1.6608996539792387, "no_speech_prob": 8.883928239811212e-05}, {"id": 973, "seek": 559152, "start": 5597.120000000001, "end": 5602.56, "text": " attentions invention to the fact that researchers at the Montreal Institute for Learning Algorithms", "tokens": [30980, 626, 22265, 281, 264, 1186, 300, 10309, 412, 264, 34180, 9446, 337, 15205, 35014, 6819, 2592], "temperature": 0.0, "avg_logprob": -0.1325725606969885, "compression_ratio": 1.6608996539792387, "no_speech_prob": 8.883928239811212e-05}, {"id": 974, "seek": 559152, "start": 5602.56, "end": 5607.280000000001, "text": " couldn't afford an AGPU machine, so they couldn't run the giant LSTM in the sequence of sequence", "tokens": [2809, 380, 6157, 364, 28406, 8115, 3479, 11, 370, 436, 2809, 380, 1190, 264, 7410, 441, 6840, 44, 294, 264, 8310, 295, 8310], "temperature": 0.0, "avg_logprob": -0.1325725606969885, "compression_ratio": 1.6608996539792387, "no_speech_prob": 8.883928239811212e-05}, {"id": 975, "seek": 559152, "start": 5607.280000000001, "end": 5611.68, "text": " paper. So they needed to invent something that worked better, but didn't require such a big model,", "tokens": [3035, 13, 407, 436, 2978, 281, 7962, 746, 300, 2732, 1101, 11, 457, 994, 380, 3651, 1270, 257, 955, 2316, 11], "temperature": 0.0, "avg_logprob": -0.1325725606969885, "compression_ratio": 1.6608996539792387, "no_speech_prob": 8.883928239811212e-05}, {"id": 976, "seek": 559152, "start": 5611.68, "end": 5617.040000000001, "text": " so they invented attention. Of course, it's not good advice to get to tell someone that they should", "tokens": [370, 436, 14479, 3202, 13, 2720, 1164, 11, 309, 311, 406, 665, 5192, 281, 483, 281, 980, 1580, 300, 436, 820], "temperature": 0.0, "avg_logprob": -0.1325725606969885, "compression_ratio": 1.6608996539792387, "no_speech_prob": 8.883928239811212e-05}, {"id": 977, "seek": 561704, "start": 5617.04, "end": 5621.68, "text": " just go invent something smaller, but I'm at least hopeful that some of these things that we've shown", "tokens": [445, 352, 7962, 746, 4356, 11, 457, 286, 478, 412, 1935, 20531, 300, 512, 295, 613, 721, 300, 321, 600, 4898], "temperature": 0.0, "avg_logprob": -0.11856601593342234, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.00018403391004540026}, {"id": 978, "seek": 561704, "start": 5621.68, "end": 5629.28, "text": " are possible at scale are also possible at a much smaller scale. Thank you. Yeah.", "tokens": [366, 1944, 412, 4373, 366, 611, 1944, 412, 257, 709, 4356, 4373, 13, 1044, 291, 13, 865, 13], "temperature": 0.0, "avg_logprob": -0.11856601593342234, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.00018403391004540026}, {"id": 979, "seek": 561704, "start": 5632.16, "end": 5637.36, "text": " Okay, I think there's no one else who's have a hand up at the moment. Maybe there's now some", "tokens": [1033, 11, 286, 519, 456, 311, 572, 472, 1646, 567, 311, 362, 257, 1011, 493, 412, 264, 1623, 13, 2704, 456, 311, 586, 512], "temperature": 0.0, "avg_logprob": -0.11856601593342234, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.00018403391004540026}, {"id": 980, "seek": 561704, "start": 5637.36, "end": 5643.5199999999995, "text": " moment for John to ask his question, but if any other people have questions, now's a good point", "tokens": [1623, 337, 2619, 281, 1029, 702, 1168, 11, 457, 498, 604, 661, 561, 362, 1651, 11, 586, 311, 257, 665, 935], "temperature": 0.0, "avg_logprob": -0.11856601593342234, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.00018403391004540026}, {"id": 981, "seek": 564352, "start": 5643.52, "end": 5650.400000000001, "text": " to jump in. I was just going to ask a question from the Q&A and came in and asked it live, so.", "tokens": [281, 3012, 294, 13, 286, 390, 445, 516, 281, 1029, 257, 1168, 490, 264, 1249, 5, 32, 293, 1361, 294, 293, 2351, 309, 1621, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.15189637078179252, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.0005522868596017361}, {"id": 982, "seek": 564352, "start": 5652.400000000001, "end": 5658.8, "text": " Yeah, I will say one other thing just quickly, which is that, you know, T5, I was very, like I", "tokens": [865, 11, 286, 486, 584, 472, 661, 551, 445, 2661, 11, 597, 307, 300, 11, 291, 458, 11, 314, 20, 11, 286, 390, 588, 11, 411, 286], "temperature": 0.0, "avg_logprob": -0.15189637078179252, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.0005522868596017361}, {"id": 983, "seek": 564352, "start": 5658.8, "end": 5663.68, "text": " said, I was very excited that we achieved near-human performance on Super Glue. The model that came", "tokens": [848, 11, 286, 390, 588, 2919, 300, 321, 11042, 2651, 12, 18796, 3389, 322, 4548, 49832, 13, 440, 2316, 300, 1361], "temperature": 0.0, "avg_logprob": -0.15189637078179252, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.0005522868596017361}, {"id": 984, "seek": 564352, "start": 5663.68, "end": 5669.84, "text": " along and actually closed that 0.5 percent gap is a model that is about, you know, 10 times smaller", "tokens": [2051, 293, 767, 5395, 300, 1958, 13, 20, 3043, 7417, 307, 257, 2316, 300, 307, 466, 11, 291, 458, 11, 1266, 1413, 4356], "temperature": 0.0, "avg_logprob": -0.15189637078179252, "compression_ratio": 1.58130081300813, "no_speech_prob": 0.0005522868596017361}, {"id": 985, "seek": 566984, "start": 5669.84, "end": 5674.88, "text": " in terms of parameter count, so that's like another reasonable example of, I mean, it's still quite", "tokens": [294, 2115, 295, 13075, 1207, 11, 370, 300, 311, 411, 1071, 10585, 1365, 295, 11, 286, 914, 11, 309, 311, 920, 1596], "temperature": 0.0, "avg_logprob": -0.18931234799898589, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00018511648522689939}, {"id": 986, "seek": 566984, "start": 5674.88, "end": 5682.32, "text": " big, but at least, you know, as you make algorithmic and architectural improvements, sometimes you can", "tokens": [955, 11, 457, 412, 1935, 11, 291, 458, 11, 382, 291, 652, 9284, 299, 293, 26621, 13797, 11, 2171, 291, 393], "temperature": 0.0, "avg_logprob": -0.18931234799898589, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00018511648522689939}, {"id": 987, "seek": 566984, "start": 5682.32, "end": 5689.2, "text": " close these gaps. Well, thank you again, Colin, and let you, whatever, have a beer and go to bed or something.", "tokens": [1998, 613, 15031, 13, 1042, 11, 1309, 291, 797, 11, 29253, 11, 293, 718, 291, 11, 2035, 11, 362, 257, 8795, 293, 352, 281, 2901, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.18931234799898589, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00018511648522689939}, {"id": 988, "seek": 566984, "start": 5690.96, "end": 5695.52, "text": " Yeah, yeah, sounds great. Yeah, thanks again for having me. It's such a pleasure. So,", "tokens": [865, 11, 1338, 11, 3263, 869, 13, 865, 11, 3231, 797, 337, 1419, 385, 13, 467, 311, 1270, 257, 6834, 13, 407, 11], "temperature": 0.0, "avg_logprob": -0.18931234799898589, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00018511648522689939}, {"id": 989, "seek": 569552, "start": 5695.52, "end": 5700.4800000000005, "text": " and I should say if anyone has any follow-up questions I think of later on, I'm always excited to", "tokens": [293, 286, 820, 584, 498, 2878, 575, 604, 1524, 12, 1010, 1651, 286, 519, 295, 1780, 322, 11, 286, 478, 1009, 2919, 281], "temperature": 0.0, "avg_logprob": -0.19780941615028988, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.00013322716404218227}, {"id": 990, "seek": 569552, "start": 5700.4800000000005, "end": 5704.080000000001, "text": " get emails about this kind of stuff. It's, you know, this is stuff I like working on, so.", "tokens": [483, 12524, 466, 341, 733, 295, 1507, 13, 467, 311, 11, 291, 458, 11, 341, 307, 1507, 286, 411, 1364, 322, 11, 370, 13], "temperature": 0.0, "avg_logprob": -0.19780941615028988, "compression_ratio": 1.407185628742515, "no_speech_prob": 0.00013322716404218227}, {"id": 991, "seek": 570408, "start": 5704.08, "end": 5733.04, "text": " Yeah, thank you again for the great informative talk.", "tokens": [50364, 865, 11, 1309, 291, 797, 337, 264, 869, 27759, 751, 13, 51812], "temperature": 0.0, "avg_logprob": -0.4585076740809849, "compression_ratio": 0.9137931034482759, "no_speech_prob": 0.00010453340655658394}], "language": "en"}