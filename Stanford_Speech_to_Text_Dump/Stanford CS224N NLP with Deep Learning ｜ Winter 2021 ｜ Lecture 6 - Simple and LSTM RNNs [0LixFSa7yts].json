{"text": " Okay, hi everyone. Welcome back to CS224N. So today is a pretty key lecture where we get through a number of important topics for neural networks, especially as supplied to natural language processing. So right at the end of last time I started through current neural networks. So we'll talk in detail more about the current neural networks in the first part of the class. And we'd emphasize language models, but then also getting a bit beyond that. And then look at it more advanced kinds of recurrent neural networks towards the end part of the class. I just wanted to sort of say a word before getting underway about the final project. So hopefully by now you've started looking at assignment three, which is the middle of the five assignments for the first half of the course. And then in the second half of the course most of your effort goes into a final project. So next week the first day lecture is going to be about final projects and choosing the final project and tips for final projects, etc. So it's fine to delay thinking about final projects until next week if you want, but you shouldn't delay it too long because we do want you to get underway with what topic you're going to do for your final project. If you are thinking about final projects you can find some info on the website, but note that the info that's there at the moment is still last year's information and it will be being updated over the coming week. We'll also talk about project mentors. If you've got ideas of people who on your own you can line up as a mentor that now would be a good time to ask them about what they're doing and we'll sort of talk about what the alternatives are. Okay so last lecture I introduced the idea of language models, so probabilistic models that predict the probability of next words after a word sequence and then we looked at end-gram language models and started into a current neural network models. So today we go to talk more about the simple RNNs we saw before talking about training RNNs and uses RNNs, but then we'll also look into the problems that occur with RNNs and how we might fix them. These will motivate a more sophisticated RNN architecture called LSTMs and we'll talk about other more complex RNN options by directional RNNs and multi-layer RNNs. Then next Tuesday we're essentially going to further exploit and build on the RNN based architectures that we've been looking at to discuss how to build a neural machine translation system with the sequence to sequence and model with attention and effectively as that model is what we'll use in assignment 4 but it also means that you'll be using all of the stuff that we're talking about today. Okay so if you remember from last time this was the idea of a simple recurrent neural network language model. So we had a sequence of words as our context for which we've looked up word embeddings and then the recurrent neural network model ran this recurrent layer where at each point we have a previous hidden state which can just be zero at the beginning of a sequence and you have feeding it in to the next hidden state, the previous hidden state and encoding and transform the coding of a word using this recurrent neural network equation that I have on the left that's very central and based on that you compute a new hidden representation for the next time step and you can repeat that along for successive time steps. Now we also usually want our recurrent neural networks to produce outputs. So I only show it at the end here but at each time step we're then also going to generate an output and so to do that we're feeding the hidden layer into a softmax layer so we're doing another matrix multiply add on a bias, put it through the softmax equation and that will then gives the probability distribution over words and we can use that to predict how likely it is that different words are going to occur after the students open there. Okay, so I didn't introduce that model but I hadn't really gone through the specifics of how we train this model, how we use it and evaluate it so let me go through this now. So here's how we train an RNN language model, we get a big corpus of text, just a lot of text and so in regard that is just a long sequence of words x1 to xt and what we're going to do is feed it into the RNN LM. So for each we're going to take prefixes of that sequence and based on each prefix we're going to want to predict the probability distribution for the word that comes next and then we're going to train our model by assessing how good a job we do about that and so the loss function we use is the loss function normally referred to as cross entropy loss in the literature which is this negative log likelihood loss. So we are going to predict some word to come next. Well we have a probability distribution over predictions of what word comes next and actually there was an actual next word in the text and so we say well what probability did you give to that word and maybe we gave it a probability estimate of.01. Well it would have been greater if we've given a probability estimate of almost one because that meant we've almost certain that what did come next in our model and so we'll take a loss to the extent that we're giving the actual next word a predicted probability of less than one. So then get an idea of how well we're doing over the entire corpus we work out that loss at each position and then we work out the average loss of the entire training set. So let's just go through that again more graphically in the next couple of slides. So down the bottom here's our corpus of text. We're running it through our simple or current neural network and at each position we've predicting a probability distribution over words. We then say well actually at each position we know what word is actually next. So when we're at time step one the actual next word is students because we can see it just to the right of us here and we say what probability estimate did you give to students and to the extent that it's not high it's not one we take a loss and then we go on to the time step two and we say well at time step two you predict the probability distribution over words. The actual next word is opened so to the extent that you haven't given high probability to open you take a loss and then that repeats in time step three we're hoping the model predict there at time step four we're hoping the model predict exams and then to work out our overall loss we're then averaging our per time step loss. So in a way this is a pretty obvious thing to do but note that there is a little subtlety here and in particular this algorithm is referred to in the literature as teacher forcing and so what does that mean? Well you know you can imagine what you can do with a recurrent neural network is say okay just start generating maybe I'll give you a hint as to where to start I'll say the sentence starts thus students and then let it run and see what it generates coming next it might start saying the students have been locked out in the classroom or whatever it is right and that we could say as well that's not very close to what the actual text says and somehow we want to learn from that and if you go in that direction there's a space of things you can do that leads into more complex algorithms such as reinforcement learning but from the perspective of training these neural models that's unjule complex and unnecessary so we have this very simple way of doing things which is what we do is just predict one time step forward so we say we know that the prefix is the students predict a probability distribution over the next word it's good to the extent that you give probability mass to open okay now the prefix is the students opened predict the a probability distribution over the next word it's good to the extent that you give probability mass to there and so effectively at each step we're resetting to what was actually in the corpus so you know it's possible after the students opened the model thinks that by far the most probable thing to come next is ah or thus say I mean we don't actually use what the model suggested we penalize the model for not having suggested there but then we just go with what's actually in the corpus and ask it to predict again this is just a little side thing but it's an important part to know if you're actually training your own neural language model I sort of presented as one huge corpus that we chug through but in practice we don't chug through a whole corpus one step at a time what we do is we cut the whole corpus into shorter pieces which might commonly be sentences or documents or sometimes they're literally just pieces that are chopped right so your recall that stochastic gradient send allows us to compute a lost ingredients from a small chunk of data and update so what we do is we take these small pieces compute gradients from those and update weights and repeat and in particular we get a lot more speed and efficiency in training if we aren't actually doing an update for just one sentence at a time but actually a batch of sentences so typically what we'll actually do is we'll feed to the model 32 sentences say of a similar length at the same time compute gradients for them update weights and then get another batch of sentences to train on how do we train I haven't sort of gone through the details of this I mean in one sense the answer is just like we talked about lecture three we use back propagation to get gradients and update parameters but let's take at least a minute to go through the differences and subtleties of the current neural network case and the central thing that's a bit you know as before we're going to take our loss and we're going to back propagate it to all of the parameters of the network everything from word embeddings to biases etc but the central bit that's a little bit different and is more complicated is that we have this WH matrix that runs along the sequence that we keep on applying to update our hidden state so what's the derivative of Jt of theta with respect to the repeated weight matrix WH and well the answer that is that what we do is we look at it in each position and work out what the particles are of Jt with respect to WH in position one or position two position three position four etc right along the sequence and we just sum up all of those particles and that gives us a partial for Jt with respect to WH overall so the answer for a current neural networks is the gradient with respect to a repeated weight in our current network is the sum of the gradient with respect to each time it appears and let me just then go through a little why that is the case but before I do that let me just note one gotcha I mean it's just not the case that this means it equals t times the partial of Jt with respect to WH because we're using WH here here here here here through the sequence and for each of the places we use it there's a different upstream gradient that's being fed into it so each of the values in this sum will be completely different from each other well why we get this answer is essentially a consequence of what we talked about in the third lecture so to take the simplest case of it right that if you have a multivariable function f of xy and you have two single variable functions x of t and y of t which are fed one input t well then the simple version of working out the derivative of this function is you take the derivative down one path and you take the derivative down the other path and so in the slides in lecture 3 that was what was summarized on a couple of slides by the slogan gradient sum at outward branches so t has outward branches and so you take gradient here on the left gradient on the right and you sum them together and so really what's happening with the recurrent neural network is just many pieces generalization of this so we have one WH matrix and we're using it to keep on updating the hidden state at time one time two time three right through time t and so what we're going to get is that this has a lot of outward branches and we're going to sum the gradient path at each one of them but what is this gradient path here it kind of goes down here and then goes down there but you know actually the bottom part is that we're just using WH at each position so we have the partial of WH used at position i with respect to the partial of WH which is just our weight matrix for our recurrent neural network so that's just one because you know we're just using the same matrix everywhere and so we are just then summing the partials in each position that we use it okay i'm practically what does that mean in terms of how you compute this um well if you're doing it by hand um what happens is you start at the end just like the general lecture three story you work out um derivatives um with respect to the hidden layer and then with respect to WH at the last time step and so that gives you one update for WH but then you continue passing the gradient back to the t minus one time step and after a couple more steps of the chain rule you get another update for WH and you simply sum that onto your previous update for WH and then you go to ht minus two you get another update for WH and you sum that onto your update for WH and you go back all the way um and you sum up the gradients as you go um and that gives you a total update um for WH um and so there's sort of two tricks here and i'll just mention um the two tricks you have to kind of separately sum the updates for WH and then once you finished um apply them all at once you don't want to actually be changing the WH matrix as you go because that's then invalid because the forward calculations were done with the constant WH that you had from the previous um state all through the network um the second trick is well if you're doing this for sentences you can normally just go back to the beginning of the sentence um but if you've got very long sequences this can really slow you down if you're having to sort of run this algorithm back for a huge amount of time so something people commonly do is what's called truncated back propagation through time where you choose some constants say 20 and you say well i'm just going to run this back propagation for 20 time steps some those 20 gradients and then i'm just done that's what i'll update um the WH matrix with um and that works just fine okay so now given a corpus um we can train uh uh simple RNN and so that's good progress um but um this is a model that can also generate text in general so how do we generate text well just like about n-gram language model we're going to generate text by repeated sampling so we're going to start off with an initial state um um and um yeah this slide is imperfect um so the initial state for the hidden state um is is normally just taken as a zero vector and well then we need to have something for a first input and on this slide the first input is shown as the first word my and if you want to feed a starting point you could feed my but a lot of the time you'd like to generate a sentence from nothing and if you want to do that what's conventional is to additionally have a beginning of sequence token which is a special token so you'll feed in the beginning of sequence token in at the beginning as the first token it has an embedding and then you use the um RNN update and then you generate using the soft max and next word and um well you generate a probability probability distribution over next words and then at that point you sample from that and it chooses some word like favorite and so then the trick is for doing generation that you take this word that you sampled and you copy it back down to the input and then you feed it as an in as an input next step if you are an N sample from the soft max get another word and just keep repeating this over and over again and you start generating the text and how you end is as well as having a beginning of sequence um special symbol you usually have an end of sequence special symbol and at some point um the recurrent neural network will generate the end of um sequence symbol and then you say okay I'm done I'm finished generating text um so before going on for the um more of the um difficult content of the lecture we can just have a little bit of fun with this and try um training up and generating text with a recurrent neural network model so you can generate you can train an RNN on any kind of text and so that means one of the fun things that you can do is generate text um in different styles based on what you could train it from um so here um Harry Potter as a there it's a fair amount of a corpus of text so you can train our NNLM on the Harry Potter box and then say go off and generate some text and it'll generate text like this sorry how Harry shouted panicking I'll leave those brooms in London are they no idea said nearly headless Nick casting low close by sadrick carrying the last bit of treacle charms from Harry's shoulder and to answer him the common room pushed upon it four arms held a shining knob from when the spider hadn't felt it seemed he reached the teams too um well so on the one hand that's still kind of a bit in coherent as a story on the other hand it sort of sounds like Harry Potter and certainly the kind of you know vocabulary and constructions it uses and like I think you'd agree that you know even though it gets sort of incoherent it's sort of more coherent than what we got from an in-gram language model when I showed a generation in the last um lecture um you can choose a very different style of text um so you could instead train the model on a bunch of cookbooks um and if you do that you can then say generate um based on what you've learned about cookbooks um and it'll just generate a recipe so here's a recipe um chocolate ranch barbecue um categories yield six servings two tablespoons of parmesan cheese chopped a one cup of coconut milk three eggs beaten place each pasta over layers of lumps shaped mixture into the moderate oven and simmer until firm serve hot and bodied fresh mustard orange and cheese combine the cheese and salt together the dough in a large skillet add the ingredients and stir in the chocolate and pepper so you know um this recipe makes um no sense and it's sufficiently um incoherent there's actually even no danger that you'll try cooking this at home um but you know something that's interesting is although you know this really just isn't a recipe and the things that are done in the instructions have no relation um to the ingredients that the thing that's interesting that it has learned as this recurrent your network model is that it's really mastered the overall structure of a recipe it knows that a recipe has a title it often tells you about how many people it serves at list the ingredients and then it has instructions um to make it so that's sort of fairly impressive in some sense the high level tech structuring um so the one other thing I wanted to mention was um when I say you can train and our an end language model and any kind of text the other difference from where we were in in-gram language models was on in-gram language models that just meant counting in grams and meant it took um two minutes or even on a large corpus with any modern computer training your iron and L.M. actually can then be a time intensive activity and you can spend hours doing that as you might find next week when you're training um machine translation models okay um how do we decide if our models are good or not um so the standard evaluation metric for language models is what's called perplexity um and what perplexity is is um kind of like when you were um training your model you use teacher forcing over a piece of text that's a different piece of test text which isn't text that was in the training data and you say well given a sequence of t words um what probability do you give to the actual t plus oneth word and you repeat that at each position and then you take the inverse of that probability and raise it to the one on t for the length of your test text sample and that number is the perplexity so it's a geometric mean of the inverse probabilities now um after that explanation perhaps an easier way to think of it is that the perplexity um is simply um the cross-entropy loss that I introduced before expenentiated um so um but you know it's now the other way around so low perplexity um is better so there's actually an interesting story about these perplexities um so a famous figure in the development of um probabilistic and machine learning approaches to natural language processing is Fred Jeleneck who died a few years ago um and he was trying to um interest people and the idea of using probability models and machine learning um for natural language processing at a time i this is the 1970s and early 1980s when nearly everyone in the field of AI um was still in the thrall of logic-based models and blackboard architectures and things like that for artificial intelligence systems and so Fred Jeleneck was actually an information theorist by background um and who um then got interest in working with speech and then language data um so at that time the stuff that's this sort of um exponential or using cross-entropy losses was completely bread and butter um to Fred Jeleneck but he'd found that no one in AI could understand the bottom half of the slide and so he wanted to come up with something simple that AI people at that time could understand and perplexity has a kind of a simple interpretation you can tell people so if you get a perplexity of 53 that means how uncertain you are um of the next word is equivalent to the uncertainty of that you're tossing a 53 sided dice and it coming up as a one right so um that was kind of an easy simple metric and so he introduced um that idea um but you know i guess things stick and to this day everyone evaluates their language models by providing perplexity numbers and so here are some perplexity numbers um so traditional n-gram language models commonly had perplexities over 100 but if you made them really big and really careful you carefully you could get them down into a number like 67 as people started to build more advanced recurrent neural networks especially as they move beyond the kind of simple RNNs which has all I've shown you so far which one of is in the second line of the slide into LSTMs which I talk about later in this course that people started producing much better perplexities and here we're getting perplexities down to 30 and this is results actually from a few years ago so nowadays people get perplexities even lower than 30 you have to be realistic and what you can expect right because if you're just generating a text some words are almost determined um so you know if it's something like um you know sum gave the man a napkin he said thank you know basically 100 percent you should be able to say the word that comes next is you um and so that you can predict really well but um you know if it's a lot of other sentences like um he looked out the window and saw uh something right no probability in the model or model in the world can give a very good estimate of what's actually going to be coming next to that point and so that gives us the sort of residual um uncertainty that leads to perplexities that on average might be around 20 or something okay um so we've talked a lot about language models now why should we care about language modeling you know well there's sort of an intellectual scientific answer that says this is a benchmark task right if we what we want to do is build machine learning models of language and our ability to predict what word will come next in the context that shows how well we understand both the structure of language and the structure of the human world that um language talks about um but there's a much more practical answer than that um which is you know language models are really the secret tool of natural language processing so if you're talking to any nlp person and you've got almost any task it's quite likely they'll say oh I bet we could use a language model for that and so language models are sort of used as a not the whole solution but a part of almost any task any task involves generating or estimating the probability of text so you can use it for predictive typing speech recognition grammar correction identifying authors machine translation summarization dialogue just that anything you do with natural language involves language models and we'll see examples of that in following classes including next Tuesday where we're using language models for machine translation okay so a language model is just a system that predicts the next word a recurrent neural network is a family of neural networks which can take sequential input of any length they reuse the same weights to generate a hidden state and optionally but commonly an output on each step um note that these two things are different um so we've talked about two ways that you could build language models but one of them is our ns being a great way but our ns can also be used for a lot of other things so let me just quickly preview a few other things you can do with our ns so there are lots of tasks that people want to do an nlp which are referred to as sequence taking tasks where we'd like to take words of text and do some kind of classification along the sequence so one simple common one is to give words parts of speech that is a determinar start orders an adjective cat is a noun not does a verb um and well you can do this straightforwardly by using a recurrent neural network as a sequential classifier whereas now going to generate parts of speech rather than the next word you can use a recurrent neural network the sentiment classification well this time we don't actually want to generate um an output at each word necessarily but we want to know what the overall sentiment looks like so somehow we want to get out a sentence encoding that we can perhaps put through another neural network layer to judge whether the sentence is positive or negative well the simplest way to do that is to think well after I've run my LSTM through the whole sentence actually this final hidden state it's encoded the whole sentence because remember I updated that hidden state based on each previous word and so you could say that this is the whole meaning of the sentence so let's just say that is the sentence encoding um and then put an extra um classifier layer on that with something like a softmax classifier um that method has been used and it actually works reasonably well and if you sort of train this model end to end well it's actually then motivated to preserve sentiment information in the hidden state of the recurrent neural network because that will allow it to better predict the sentiment of the whole sentence um which is the final task and hence loss function that we're giving the network but it turns out that you can commonly do better than that by actually doing things like feeding all hidden states into the sentence encoding perhaps by making the sentence encoding an element wise max or an element wise mean of all the hidden states because this then more symmetrically encodes the hidden state over each time step another big use of recurrent neural networks is what I'll call language encoder module uses so anytime you have some text for example here we have a question of what nationality was Beethoven we'd like to construct some kind of neural representation of this so one way to do it is to run recurrent neural network over it and then just like last time to either take the final hidden state or take some kind of um function of all the hidden states and say that's the sentence representation and we could do the same thing um for the context so for question answering we're going to build some more neural net structure um on top of that and we'll learn more about them a couple of weeks um when we have the question answering lecture but the key thing is what we built so far we used to get sentence representation so it's a language encoder module so that was the language encoding part we can also use RNNs to decode into language and that's commonly used in speech recognition machine translation summarization so if we have a speech recognizer the input is an audio signal and what we want to do is decode that into language well what we could do is use some function of the input which is probably itself going to be in your net as the initial um hidden state of our RNN LM and then we say start generating text based on that and so it should then um we generate word at a time by the method that we just looked at um we turn the speech into text so this is an example of a conditional language model because we're now generating text conditioned on the speech signal and a lot of the time you can do interesting more advanced things with the current neural networks by building conditional language models another place you can use conditional language models is for text classification tasks and including sentiment classification so if you can condition um your language model based on a kind of sentiment you can build a kind of classifier for that and another use that we'll see a lot of next class is for machine translation okay so that's the end of the intro um to um doing things with um recurrent neural networks and language models now I want to move on and tell you about the fact that everything is not perfect and these recurrent neural networks tend to have a couple of problems and we'll talk about those and then in part that'll then motivate coming up with a more advanced recurrent neural network architecture so the first problem to be mentioned is the idea of what's called vanishing gradients and what does that mean well at the end of our sequence we have some overall um loss that we're calculating and well what we want to do is back propagate that loss um and we want to back propagate it right along the sequence and so we're working out the partials of j4 um with respect to the hidden state at time one and when we have a longer sequence we'll be working out the partials of j20 with respect to the hidden state at time one and how do we do that well how we do it is by composition and the chain rule we've got a big long chain rule along the whole sequence um well if we're doing that um you know we're multiplying a ton of things together and so the danger of what tends to happen is that as we do these um multiplications a lot of time these partials between successive hidden states become small and so what happens is as we go along the gradient gets smaller and smaller and smaller and starts to peter out and to the extent that appears out um well then we've kind of got no upstream gradient and therefore we won't be changing the parameters at all and that turns out to be pretty problematic um so the next couple of slides sort of um say a little bit about the why and how this happens um what's presented here is a kind of only semi formal wave your hands at the kind of problems that you might expect um if you really want to sort of get into all the details of this um you should look at the couple of papers um than I mentioned a small print at the bottom of the slide but at any rate if you remember that this is our basic um my current neural network equation well let's consider an easy case suppose we sort of get rid of our non-linearity and just assume that it's an identity function okay so then when we're working out the partials of the hidden state with respect to the previous hidden state um we can work those out in the usual way according to the chain rule and then if um sigma is um simply the identity function um well then everything gets really easy for us so only the the sigma just goes away and only the first term involves um h at time t minus 1 so the later terms go away and so um our gradient ends up as wh well that's doing it for just one time step what happens when you want to work out these partials a number of time steps away so we want to work it out the partial of time step i with respect to j um well what we end up with is a product of the partials of successive time steps um and well each of those um is coming out as wh and so we end up um getting wh raised to the elf power and well our potential problem is that if wh is small in some sense then this term gets exponentially problematic i it becomes vanishingly small as our sequence length becomes long well what can we mean by small well a matrix is small if it's eigenvalues are all less than one so we can rewrite what's happening with this successor multiplication using eigenvalues and eigenvectors um and i should say that all eigenvector values less than one is sufficient but not necessary condition for what i'm about to say um right so we can rewrite um things using the eigenvectors as a basis and if we do that um um we end up getting um the eigenvalues being raised to the elf power and so if all of our eigen values are less than one if we're taking a number less than one um and raising it to the elf power that's going to approach zero as the sequence length grows and so the gradient vanishes okay now the reality is more complex than that because actually we always use a non-linear activation sigma but you know in principle it's sort of the same thing um apart from we have to consider in the effect of the non-linear activation okay so why is this a problem that the gradients disappear well suppose we're wanting to look at the influence of time steps well in the future uh on um the representations we want to have early in the sentence well um what's happening late in the sentence just isn't going to be giving much information about what we should be storing in the h at time one vector whereas on the other hand the loss at time step two is going to be giving a lot of information at what um should be stored in the hidden vector at time step one so the end result of that is that what happens is that these simple RNNs are very good at modeling nearby effects but they're not good at all at modeling long term effects because the gradient signal from far away is just lost too much and therefore the model never effectively gets to learn um what information from far away it would be useful to preserve into the future so let's consider that concretely um for the example of language models that we've worked on so here's a piece of text um when she tried to print her tickets she found that the printer was out of toner she went to the stationery store to buy more toner it was very overpriced after installing the toner into the printer she finally printed her and well you're all smart human beings i trust you can all guess what the word that comes next is it should be tickets um but well the problem is that for the RNN start to learn cases like this it would have to carry through in its hidden state a memory of the word tickets for sort of whatever it is about 30 hidden state updates and well we'll train on this um example and so we'll be wanting it to predict tickets um is the next word and so a gradient update will be sent right back through the hidden states of the LSTM corresponding to this sentence and that should tell the model um is good to preserve information about the word tickets because that might be useful in the future here it was useful in the future um but the problem is that the gradient signal will just become far too weak out after a bunch of words and it just never learns that dependency um and so what we find in practice is the model is just unable to predict similar long distance dependencies at test time i spent quite a long time on vanishing gradients and and really vanishing gradients are the big problem in practice um with using recurrent neural networks over long sequences um but you know i have to do justice to the fact that you could actually also have the opposite problem you can also have exploding gradients so if a gradient becomes too big that's also a problem and it's a problem because the secastic gradient update step becomes too big right so remember our parameter update is um based on the product of the learning rate and the gradient so if your gradient is huge right you've calculated oh it's got a lot of slope here this has a slope of 10,000 um then your parameter update can be arbitrarily large and that's potentially problematic that can cause a bad update where you take a huge step and you end up at a weird and bad parameter configuration so you sort of think you're coming up with a to a steep hill to climb and well you want to be climbing the hill to high likelihood but actually the gradient is so steep that you make an enormous um update and then suddenly your parameters are over an IOR and you've lost your hill altogether there's also the practical differently that we only have so much resolution now floating point numbers um so if your gradient gets too steep um you start getting um not a numbers in your calculations which ruin all your hard training work um we use a kind of an easy fix to this which is called gradient clipping um which is we choose some reasonable number and we say we're just not going to deal with gradients that bigger than this number um a commonly used number is 20 you know some the thing that's got a range of spread but not that high you know you can use 10,000 some where sort of in that range um and if the norm of the gradient is greater than that threshold we simply just scale it down which means that we then make a smaller gradient update so we're still moving in exactly the same um direction but we're taking a smaller step um so doing this gradient clipping is important um um you know but it's an easy problem to solve okay um so the thing that we've still got left to solve is how to really solve this problem of vanishing gradients um so the problem is yeah these RNNs just can't preserve information over many time steps and one way to think about that intuitively is at each time step we have a hidden state and the hidden state is being completely changed at each time step and it's being changed in a multiplicative manner by multiplying by wh and then putting it through um and nonlinearity like maybe we can make some more progress um if we could more flexibly maintain a memory in our recurrent neural network which we can manipulate in a more flexible manner that allows us to more easily preserve information and so this was an idea that people started thinking about and actually they started thinking about it a long time ago um in the late 1990s um and Huck Rydon Schmitt Hoover came up with this idea that got called long short term memory RNNs as a solution to the problem of vanishing gradients I mean so this 1997 paper is the paper you always see cited for LSTM's but you know actually in terms of what we now understand as an LSTM um it was missing part of it in fact it's missing what in retrospect has turned out to be the most important part of um the um modern LSTM so really in some sense the real paper that the modern LSTM is due to is this slightly later paper by Gerst, still Schmitt Hoover and Cummins from 2000 which additionally introduces the forget gate that I'll explain in a minute um yeah so um so this was some very clever stuff that was introduced and it turned out later to have an enormous impact um if I just diverge from the technical part for one more moment um that you know for those of you who these days um think that mastering your networks is the path to fame and fortune um the funny thing is you know at the time that this work was done that just was not true right very few people were interested in neural networks and although long short term memories have turned out to be one of the most important successful and influential ideas in neural networks for the following 25 years um really the original authors didn't get recognition for that so both of them are now professors at German universities um but Hock Rider um moved over into doing bioinformatics work um to find um something to do and Gerst actually is doing kind of multimedia studies um so um that's the fates of history um okay so what is an LSTM so the crucial innovation of an LSTM is to say well rather than just having one hidden vector in the recurrent model we're going to um build a model with two um hidden vectors at each time step one of which is still called the hidden state H and the other of which is called the cell state um now you know arguably in retrospect these were named wrongly because as you'll see when we look at in more detail in some sense the cell is more equivalent to the hidden state of the simple RNN than vice versa but we're just going with the names that everybody uses so both of these are vectors of length N um and it's going to be the cell that stores long term information and so we want to have something that's more like memory so the meaning like RAM and the computer um so the cell is designed so you can read from it you can erase parts of it and you can write new information to the cell um and the interesting part of an LSTM is then it's got control structures to decide how you do that so the selection of which information to erase write and read is controlled by probabilistic gates so the gates are also vectors of length N and on each time step um we work out a state for the gate vectors so each element of the gate vectors is a probability so they can be open probability one close probability zero or somewhere in between and their value will be saying how much do you erase how much do you write how much do you read and so these are dynamic gates with a value that's computed based on the current context okay so in this next slide we go through the equations of an LSTM but following this there are some more graphic slides which will probably be easier to absorb right so we again just like before it's a current neural network we have a sequence of inputs x um t and we're going to it each time step compute a cell state in the hidden state so how do we do that so firstly we're going to compute values of the three gates and so we're computing the gate values using an equation that's identical to the equation um for the simple recurrent neural network um but in particular um oops sorry how does just say what the gates are first so there's a forget gate um which we will control what is kept in the cell at the next time step versus what is forgotten there's an input gate which is going to determine which parts of a calculator new cell content get written to the cell memory and there's an output gate which is going to control what parts of the cell memory are moved over into the hidden state and so each of these is using the logistic function because we want them to be in each element of this vector a probability which will say whether to fully forget partially forget or fully fully remember yeah and the equation for each of these is exactly like the simple r and an equation but note of course that we've got different parameters for each one so we've got forgetting weight matrix w with a forgetting bias um and a forgetting um multiplier of the input okay so then we have the other equations that really are the mechanics of the LSTM so we have something that will calculate a new cell content so this is our candidate update and so for calculating the candidate update we're again essentially using exactly the same simple r and n equation apart from now it's usual to use 10h so you get something that I discussed last time is balanced around zero okay so then to actually update things we use our gates so for our new cell content what the idea is is that we want to remember some but probably not all of what we had in the cell from previous time steps and we want to store some but probably not all of the value that we've calculated as the new cell update and so the way we do that is we take the previous cell content and then we take its hard-amired product with the forget vector and then we add to it the hard-amired product of the input gate times the candidate cell update and then for working out the new hidden state we then work out which parts of the cell to expose in the hidden state and so after taking a 10h transform of the cell we then take the hard-amired product with the output gate and that gives us our hidden representation and if this hidden representation that we then put through a soft softmax layer to generate our next output of our LSTM or current neural network yeah so the gates and the things that they're put with our vectors of size n and what we're doing is we're taking each element of them and multiplying them element wise to work out a new vector and then we get two vectors and that we're adding together so this way of doing things element wise you sort of don't really see and standard linear algebra course it's referred to as the hard-amired product it's represented by some kind of circle I mean actually in more modern work it's been more usual to represent it with this slightly bigger circle with the dot at the middle as the hard-amired product symbol and someday I'll change these slides to be like that but I was lazy and redoing the equations but the other notation you do see quite often is just using the same little circle that you use for function composition to represent hard-amired product okay so all of these things are being done as vectors of the same length n and the other thing that you might notice is that the candidate update and forget import and output gates all have a very similar form the only difference is three logistics in one 10-h and none of them depend on each other so all four of those can be calculated parallel and if you want to have an efficient LSTM implementation that's what you do okay so here's the more graphical presentation of this so these pictures come from Chris Ola and I guess he did such a nice job at producing pictures for LSTMs that almost everyone uses them these days and so this sort of pulls apart the computation graph of an LSTM unit so blowing this up you've got from the previous time step both your cell and hidden recurrent vectors and so you feed the hidden vector from the previous time step and the new input xt into the computation of the gates which is happening down the bottom so you compute the forget gate and then you use the forget gate in a hard-amired product here drawn as a actually a time symbol so forget some cell content you work out the input gate and then using the input gate and a regular recurrent neural network like computation you can compute candidate new cell content and so then you add those two together to get the new cell content which then heads out as the new cell content at time t but then you also have worked out an output gate and so then you take the cell content put it through another non-linearity and multi-hard-amired product it with the output gate and that then gives you the new hidden state so this is all kind of complex but as to understanding why something is different as happening here the thing to notice is that the cell state from t minus 1 is passing right through this to be the cell state at time t without very much happening to it so some of it is being deleted by the forget gate and then some new stuff is being written to it as a result of using this candidate new cell content but the real secret of the LSTM is that new stuff is just being added to the cell with an addition right so in the simple RNN at each success of step you are doing a multiplication and that makes it incredibly difficult to learn to preserve information in the hidden state over a long period of time it's not completely impossible but it's a very difficult thing to learn whereas with this new LSTM architecture it's trivial to preserve information the cell from one time step to the next you just don't forget it and it'll carry right through with perhaps some new stuff added in to also remember and so that's the sense in which the cell behaves much more like RAM and a conventional computer that storing stuff and extra stuff can be stored into it and other stuff can be deleted from it as you go along. Okay so the LSTM architecture makes it much easier to preserve information from many time steps and I just right so in particular standard practice with LSTMs is to initialize the forget gate to a one vector which it's just so that a starting point is to say preserve everything from previous time steps and then it is then learning when it's appropriate to forget stuff and contrast is very hard to get or a simple RNN to preserve stuff for a very long time. I mean what does that actually mean? Well you know I've put down some numbers here I mean you know how what you get in practice you know depends on a million things it depends on the nature of your data and how much data you have and what dimensionality your hidden states are blurdy blurdy blur but just to give you some idea of what's going on is typically if you train a simple recurrent neural network that it's effective memory it's ability to be able to use things in the past to condition the future goes for about seven time steps you just really can't get it to remember stuff further back in the past than that whereas for the LSTM it's not complete magic it doesn't work forever but you know it's effectively able to remember and use things from much much further back so typically you find that with an LSTM you can effectively remember and use things about a hundred time steps back and that's just enormously more useful for a lot of the natural language understanding tasks that we want to do and so that was precisely what the LSTM was designed to do and I mean so in particular just going back to its name quite a few people miss paths its name the idea of its name was there's a concept of short term memory which comes from psychology and it'd been suggested for simple RNNs that the hidden state of the RNN could be a model of human short term memory and then there would be something somewhere else that would deal with human long term memory but while people had found that this only gave you a very short short term memory so what Hock-Rider and Schmidt who were interested in was how we could give construct models with a long short term memory and so that then gave us this name of LSTM. LSTMs don't guarantee that there are no vanishing exploding gradients but in practice they provide they they don't tend to explode nearly the same way again that plus sign is crucial rather than a multiplication and so they're a much more effective way of learning long distance dependencies. Okay so despite the fact that LSTMs were developed around 1997 2000 it was really only in the early 2010s that the world woke up to them and how successful they were so it was really around 2013 to 2015 that LSTMs sort of hit the world achieving state-of-the-art results on all kinds of problems. One of the first big demonstrations was for handwriting recognition then speech recognition and then going on to a lot of natural language tasks including machine translation, parsing, vision and language tasks like minskapshening as well of course using them for language models and around these years LSTMs became the dominant approach for most NLP tasks. The easiest way to build a good strong model was to approach the problem with an LSTM. So now in 2021 actually LSTMs are starting to be supplanted or have been supplanted by other approaches particularly transformer models which we'll get to in the class in a couple of weeks time. So this is the sort of picture you can see. So for many years there's been a machine translation conference and so a Bake Off competition called WMT workshop on machine translation. So if you look at the history of that in WMT 2014 there was zero neural machine translation systems in the competition. 2014 was actually the first year that the success of LSTMs for machine translation was proven in a conference paper but nothing occurred in this competition. By 2016 everyone had jumped on LSTMs as working great and lots of people including the winner of the competition was using an LSTM model. If you then jump ahead to 2019 then there's relatively little use of LSTMs and the vast majority of people are now using transformers. So things change quickly in your network land and I keep on having to rewrite these lectures. So quick further note on vanishing and exploding gradients. Is it only a problem with recurrent neural networks? It's not. It's actually a problem that also occurs anywhere where you have a lot of depth including feed forward and convolutional neural networks. As any time when you've got long sequences of chain rules which give you multiplications the gradient can become vanishingly small as it back propagates. And so generally sort of lower layers are learned very slowly in a hard to train. So there's been a lot of effort in other places as well to come up with different architectures that let you learn more efficiently in deep network. And the commonest way to do that is to add more direct connections that allow the gradient to flow. So the big thing in vision in the last few years has been resnets where the res stands for residual connections. And so the way they made this picture is upside down so the input is at the top is that you have these sort of two paths that are summed together. One path is just an identity path and the other one goes through some neural network layers. And so therefore it's default behavior is just to preserve the input which might sound a little bit like what we just saw for LSTMs. There are other methods that there have been dense nets where you add skip connections forward to every layer. Highway nets were also actually developed by Schmitt Hoover and sort of a reminiscent of what was done with LSTMs. So rather than just having an identity connection as a resnet has, it introduces an extra gate. So it looks more like an LSTM which says how much to send the input through the highway versus how much to put it through a neural net layer and those two are then combined into the output. So essentially this problem occurs anywhere when you have a lot of depth in your layers of neural network. But it first arose and turns out to be especially problematic with recurrent neural networks. They're particularly unstable because of the fact that you've got this one weight matrix that you're repeatedly using through the time sequence. Okay. So Chris, we've got a couple of questions more or less about whether you would ever want to use an RN like a simple RNN instead of an LSTM. How does the LSTM learn what to do with its gates? How can you apply in on those things? Sure. So I think basically the answer is you should never use a simple RNN these days. You should always use an LSTM. I mean, you know, obviously that depends on what you're doing. If you're wanting to do some kind of analytical paper or something, you might prefer a simple RNN. And it is the case that you can actually get decent results with simple RNNs providing you're very careful to make sure that things aren't exploding nor vanishing. But, you know, in practice, getting simple RNNs to work and preserve long contexts is incredibly difficult where you can train LSTMs and they will just work. So really, you should always just use an LSTM. Now wait, the second question was... I think there's a bit of confusion about like whether the gates are learning differently. Yeah. So the gates are also just learned. So if we go back to these equations, you know, this is the complete model. And when we're training the model, every one of these parameters, so all of these WU and B's, everything is simultaneously being trained by BackProp. So that what you hope and indeed it works is the model is learning what stuff should I remember for a long time versus what stuff should I forget, what things in the input are important versus what things in the input don't really matter. So it can learn things like function words like A and D, don't really matter even though everyone uses them in English. So you can just not worry about those. So all of this is learned. And the models do actually successfully learn gate values about what information is useful to preserve long term versus what information is really only useful short term for predicting the next one or two words. Finally, the gradient improvements due to the... So you said that the addition is really important between the New Cell candidate and the Cell State. I don't think at least a couple of students have sort of questioned that. So if you want to go over that again, then maybe useful. Sure. So what we would like is an easy way for memory to be preserved long term. And you know, one way, which is what ResNet's use is just to sort of completely have a direct path from CT minus one to CT and will preserve entirely the history. So there's kind of... There's the fault action of preserving information about the past long term. LSTMs don't quite do that, but they allow that function to be easy. So you start off with the previous Cell State and you can forget some of it by the Forget Gate, so you can delete stuff out of your memory that's used for operation. And then while you're going to be able to update the content of the Cell with this, the right operation that occurs in the plus where depending on the input gate, some parts of what's in the Cell will be added to. But you can think of that adding as overlaying extra information. Everything that was in the Cell that wasn't forgotten is still continuing on to the next time step. And in particular, when you're doing the back propagation through time, that there isn't... I want to say there isn't a multiplication between CT and CT minus one. And there's this unfortunate time symbol here, but remember that's the Hadamard product, which is zeroing out part of it with the Forget Gate. It's not a multiplication by a matrix like in the simple RNN. I hope that's good. Okay, so there are a couple of other things that I wanted to get through before the end. I guess I'm not going to have time to do both of them, I think, so I'll do the last one probably next time. So these are actually simple and easy things, but they complete our picture. So I sort of briefly alluded to this example of sentiment classification where what we could do is run an RNN, maybe an LSTM over a sentence, call this our representation of the sentence and you feed it into a softmax classifier to classify for sentiment. So what we are actually saying there is that we can regard the hidden state as a representation of a word in context, that below that we have just a word vector for terribly, but we then looked at our context and say, okay, we've now created a hidden state representation for the word terribly in the context of the movie was and that proves to be a really useful idea because words have different meanings in different contexts, but it seems like there's a defect of what we've done here because our context only contains information from the left. What about right context? Surely it also be useful to have the meaning of terribly depend on exciting because often words mean different things based on what follows them. So if you have something like red wine, it means something quite different from a red light. So how could we deal with that? Well, an easy way to deal with that would be to say, well, if we're just going to come up with a neural encoding of a sentence, we could have a second RNN with completely separate parameters learned and we could run it backwards through the sentence to get a backward representation of each word and then we can get an overall representation of each word in context by just concatenating those two representations and now we've got a representation of terribly that has both left and right context. So we're simply running a forward RNN and when I say RNN here, that just means any kind of recurrent neural network so commonly it'll be an LSTM and the backward one and then at each time step we just concatenating their representations with each of these having separate weights. And so then we regard this concatenated thing as the hidden state, the contextual representation of a token at a particular time that we pass forward. This is so common that people use a shortcut to denote that and now just draw this picture with two sided arrows and when you see that picture with two sided arrows it means that you're running two RNNs one in each direction and then concatenating their results at each time step and that's what you're going to use later in the model. Okay, but so if you're doing an encoding problem like for sentiment classification or question answering using bidirectional RNNs is a great thing to do but they're only applicable if you have access to the entire input sequence. They're not applicable to language modeling because in a language model necessarily you have to generate the next word based on only the preceding context. But if you do have the entire input sequence that bidirectionality gives you greater power and indeed that's been an idea that people have built on in subsequent work. So when we get to transformers in a couple of weeks we'll spend plenty of time on the BERT model where that acronym stands for bidirectional encoder representations from transformers. So part of what's important in that model is the transformer but really a central point of the paper was to say that you could build more powerful models using transformers by again exploiting bidirectionality. Okay, there's one teeny bit left on RNNs but I'll sneak it into next class and I'll call it the end for today and if there are other things you'd like to ask questions about you can find me on NOX again in just in just a minute. Okay, so see you again next Tuesday.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 14.200000000000001, "text": " Okay, hi everyone. Welcome back to CS224N. So today is a pretty key lecture where we get", "tokens": [1033, 11, 4879, 1518, 13, 4027, 646, 281, 9460, 17, 7911, 45, 13, 407, 965, 307, 257, 1238, 2141, 7991, 689, 321, 483], "temperature": 0.0, "avg_logprob": -0.2543658739254798, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.06331508606672287}, {"id": 1, "seek": 0, "start": 14.200000000000001, "end": 20.240000000000002, "text": " through a number of important topics for neural networks, especially as supplied to natural", "tokens": [807, 257, 1230, 295, 1021, 8378, 337, 18161, 9590, 11, 2318, 382, 27625, 281, 3303], "temperature": 0.0, "avg_logprob": -0.2543658739254798, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.06331508606672287}, {"id": 2, "seek": 0, "start": 20.240000000000002, "end": 24.92, "text": " language processing. So right at the end of last time I started through current neural", "tokens": [2856, 9007, 13, 407, 558, 412, 264, 917, 295, 1036, 565, 286, 1409, 807, 2190, 18161], "temperature": 0.0, "avg_logprob": -0.2543658739254798, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.06331508606672287}, {"id": 3, "seek": 0, "start": 24.92, "end": 29.44, "text": " networks. So we'll talk in detail more about the current neural networks in the first", "tokens": [9590, 13, 407, 321, 603, 751, 294, 2607, 544, 466, 264, 2190, 18161, 9590, 294, 264, 700], "temperature": 0.0, "avg_logprob": -0.2543658739254798, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.06331508606672287}, {"id": 4, "seek": 2944, "start": 29.44, "end": 36.72, "text": " part of the class. And we'd emphasize language models, but then also getting a bit beyond", "tokens": [644, 295, 264, 1508, 13, 400, 321, 1116, 16078, 2856, 5245, 11, 457, 550, 611, 1242, 257, 857, 4399], "temperature": 0.0, "avg_logprob": -0.16942161467017197, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.00027758648502640426}, {"id": 5, "seek": 2944, "start": 36.72, "end": 42.32, "text": " that. And then look at it more advanced kinds of recurrent neural networks towards the", "tokens": [300, 13, 400, 550, 574, 412, 309, 544, 7339, 3685, 295, 18680, 1753, 18161, 9590, 3030, 264], "temperature": 0.0, "avg_logprob": -0.16942161467017197, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.00027758648502640426}, {"id": 6, "seek": 2944, "start": 42.32, "end": 49.52, "text": " end part of the class. I just wanted to sort of say a word before getting underway about", "tokens": [917, 644, 295, 264, 1508, 13, 286, 445, 1415, 281, 1333, 295, 584, 257, 1349, 949, 1242, 27534, 466], "temperature": 0.0, "avg_logprob": -0.16942161467017197, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.00027758648502640426}, {"id": 7, "seek": 2944, "start": 49.52, "end": 55.16, "text": " the final project. So hopefully by now you've started looking at assignment three, which", "tokens": [264, 2572, 1716, 13, 407, 4696, 538, 586, 291, 600, 1409, 1237, 412, 15187, 1045, 11, 597], "temperature": 0.0, "avg_logprob": -0.16942161467017197, "compression_ratio": 1.5733333333333333, "no_speech_prob": 0.00027758648502640426}, {"id": 8, "seek": 5516, "start": 55.16, "end": 59.72, "text": " is the middle of the five assignments for the first half of the course. And then in the", "tokens": [307, 264, 2808, 295, 264, 1732, 22546, 337, 264, 700, 1922, 295, 264, 1164, 13, 400, 550, 294, 264], "temperature": 0.0, "avg_logprob": -0.13970648447672526, "compression_ratio": 1.9260869565217391, "no_speech_prob": 0.0002593896060716361}, {"id": 9, "seek": 5516, "start": 59.72, "end": 65.64, "text": " second half of the course most of your effort goes into a final project. So next week the", "tokens": [1150, 1922, 295, 264, 1164, 881, 295, 428, 4630, 1709, 666, 257, 2572, 1716, 13, 407, 958, 1243, 264], "temperature": 0.0, "avg_logprob": -0.13970648447672526, "compression_ratio": 1.9260869565217391, "no_speech_prob": 0.0002593896060716361}, {"id": 10, "seek": 5516, "start": 65.64, "end": 70.12, "text": " first day lecture is going to be about final projects and choosing the final project and", "tokens": [700, 786, 7991, 307, 516, 281, 312, 466, 2572, 4455, 293, 10875, 264, 2572, 1716, 293], "temperature": 0.0, "avg_logprob": -0.13970648447672526, "compression_ratio": 1.9260869565217391, "no_speech_prob": 0.0002593896060716361}, {"id": 11, "seek": 5516, "start": 70.12, "end": 75.72, "text": " tips for final projects, etc. So it's fine to delay thinking about final projects until", "tokens": [6082, 337, 2572, 4455, 11, 5183, 13, 407, 309, 311, 2489, 281, 8577, 1953, 466, 2572, 4455, 1826], "temperature": 0.0, "avg_logprob": -0.13970648447672526, "compression_ratio": 1.9260869565217391, "no_speech_prob": 0.0002593896060716361}, {"id": 12, "seek": 5516, "start": 75.72, "end": 80.72, "text": " next week if you want, but you shouldn't delay it too long because we do want you to get", "tokens": [958, 1243, 498, 291, 528, 11, 457, 291, 4659, 380, 8577, 309, 886, 938, 570, 321, 360, 528, 291, 281, 483], "temperature": 0.0, "avg_logprob": -0.13970648447672526, "compression_ratio": 1.9260869565217391, "no_speech_prob": 0.0002593896060716361}, {"id": 13, "seek": 8072, "start": 80.72, "end": 85.56, "text": " underway with what topic you're going to do for your final project. If you are thinking", "tokens": [27534, 365, 437, 4829, 291, 434, 516, 281, 360, 337, 428, 2572, 1716, 13, 759, 291, 366, 1953], "temperature": 0.0, "avg_logprob": -0.13742708045745564, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00010157660290133208}, {"id": 14, "seek": 8072, "start": 85.56, "end": 91.03999999999999, "text": " about final projects you can find some info on the website, but note that the info that's", "tokens": [466, 2572, 4455, 291, 393, 915, 512, 13614, 322, 264, 3144, 11, 457, 3637, 300, 264, 13614, 300, 311], "temperature": 0.0, "avg_logprob": -0.13742708045745564, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00010157660290133208}, {"id": 15, "seek": 8072, "start": 91.03999999999999, "end": 96.2, "text": " there at the moment is still last year's information and it will be being updated over", "tokens": [456, 412, 264, 1623, 307, 920, 1036, 1064, 311, 1589, 293, 309, 486, 312, 885, 10588, 670], "temperature": 0.0, "avg_logprob": -0.13742708045745564, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00010157660290133208}, {"id": 16, "seek": 8072, "start": 96.2, "end": 102.16, "text": " the coming week. We'll also talk about project mentors. If you've got ideas of people who", "tokens": [264, 1348, 1243, 13, 492, 603, 611, 751, 466, 1716, 21798, 13, 759, 291, 600, 658, 3487, 295, 561, 567], "temperature": 0.0, "avg_logprob": -0.13742708045745564, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00010157660290133208}, {"id": 17, "seek": 8072, "start": 102.16, "end": 106.4, "text": " on your own you can line up as a mentor that now would be a good time to ask them about", "tokens": [322, 428, 1065, 291, 393, 1622, 493, 382, 257, 14478, 300, 586, 576, 312, 257, 665, 565, 281, 1029, 552, 466], "temperature": 0.0, "avg_logprob": -0.13742708045745564, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.00010157660290133208}, {"id": 18, "seek": 10640, "start": 106.4, "end": 111.2, "text": " what they're doing and we'll sort of talk about what the alternatives are.", "tokens": [437, 436, 434, 884, 293, 321, 603, 1333, 295, 751, 466, 437, 264, 20478, 366, 13], "temperature": 0.0, "avg_logprob": -0.2919029747087931, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.00019972227164544165}, {"id": 19, "seek": 10640, "start": 111.2, "end": 118.84, "text": " Okay so last lecture I introduced the idea of language models, so probabilistic models", "tokens": [1033, 370, 1036, 7991, 286, 7268, 264, 1558, 295, 2856, 5245, 11, 370, 31959, 3142, 5245], "temperature": 0.0, "avg_logprob": -0.2919029747087931, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.00019972227164544165}, {"id": 20, "seek": 10640, "start": 118.84, "end": 124.24000000000001, "text": " that predict the probability of next words after a word sequence and then we looked at", "tokens": [300, 6069, 264, 8482, 295, 958, 2283, 934, 257, 1349, 8310, 293, 550, 321, 2956, 412], "temperature": 0.0, "avg_logprob": -0.2919029747087931, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.00019972227164544165}, {"id": 21, "seek": 10640, "start": 124.24000000000001, "end": 129.84, "text": " end-gram language models and started into a current neural network models. So today we", "tokens": [917, 12, 1342, 2856, 5245, 293, 1409, 666, 257, 2190, 18161, 3209, 5245, 13, 407, 965, 321], "temperature": 0.0, "avg_logprob": -0.2919029747087931, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.00019972227164544165}, {"id": 22, "seek": 10640, "start": 129.84, "end": 135.56, "text": " go to talk more about the simple RNNs we saw before talking about training RNNs and", "tokens": [352, 281, 751, 544, 466, 264, 2199, 45702, 45, 82, 321, 1866, 949, 1417, 466, 3097, 45702, 45, 82, 293], "temperature": 0.0, "avg_logprob": -0.2919029747087931, "compression_ratio": 1.6827309236947792, "no_speech_prob": 0.00019972227164544165}, {"id": 23, "seek": 13556, "start": 135.56, "end": 142.04, "text": " uses RNNs, but then we'll also look into the problems that occur with RNNs and how we", "tokens": [4960, 45702, 45, 82, 11, 457, 550, 321, 603, 611, 574, 666, 264, 2740, 300, 5160, 365, 45702, 45, 82, 293, 577, 321], "temperature": 0.0, "avg_logprob": -0.1629306877052391, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.00011927384184673429}, {"id": 24, "seek": 13556, "start": 142.04, "end": 148.44, "text": " might fix them. These will motivate a more sophisticated RNN architecture called LSTMs", "tokens": [1062, 3191, 552, 13, 1981, 486, 28497, 257, 544, 16950, 45702, 45, 9482, 1219, 441, 6840, 26386], "temperature": 0.0, "avg_logprob": -0.1629306877052391, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.00011927384184673429}, {"id": 25, "seek": 13556, "start": 148.44, "end": 154.2, "text": " and we'll talk about other more complex RNN options by directional RNNs and multi-layer", "tokens": [293, 321, 603, 751, 466, 661, 544, 3997, 45702, 45, 3956, 538, 42242, 45702, 45, 82, 293, 4825, 12, 8376, 260], "temperature": 0.0, "avg_logprob": -0.1629306877052391, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.00011927384184673429}, {"id": 26, "seek": 13556, "start": 154.2, "end": 163.12, "text": " RNNs. Then next Tuesday we're essentially going to further exploit and build on the RNN", "tokens": [45702, 45, 82, 13, 1396, 958, 10017, 321, 434, 4476, 516, 281, 3052, 25924, 293, 1322, 322, 264, 45702, 45], "temperature": 0.0, "avg_logprob": -0.1629306877052391, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.00011927384184673429}, {"id": 27, "seek": 16312, "start": 163.12, "end": 167.96, "text": " based architectures that we've been looking at to discuss how to build a neural machine", "tokens": [2361, 6331, 1303, 300, 321, 600, 668, 1237, 412, 281, 2248, 577, 281, 1322, 257, 18161, 3479], "temperature": 0.0, "avg_logprob": -0.17862818791316107, "compression_ratio": 1.6906779661016949, "no_speech_prob": 4.82824252685532e-05}, {"id": 28, "seek": 16312, "start": 167.96, "end": 173.96, "text": " translation system with the sequence to sequence and model with attention and effectively", "tokens": [12853, 1185, 365, 264, 8310, 281, 8310, 293, 2316, 365, 3202, 293, 8659], "temperature": 0.0, "avg_logprob": -0.17862818791316107, "compression_ratio": 1.6906779661016949, "no_speech_prob": 4.82824252685532e-05}, {"id": 29, "seek": 16312, "start": 173.96, "end": 179.8, "text": " as that model is what we'll use in assignment 4 but it also means that you'll be using all", "tokens": [382, 300, 2316, 307, 437, 321, 603, 764, 294, 15187, 1017, 457, 309, 611, 1355, 300, 291, 603, 312, 1228, 439], "temperature": 0.0, "avg_logprob": -0.17862818791316107, "compression_ratio": 1.6906779661016949, "no_speech_prob": 4.82824252685532e-05}, {"id": 30, "seek": 16312, "start": 179.8, "end": 182.72, "text": " of the stuff that we're talking about today.", "tokens": [295, 264, 1507, 300, 321, 434, 1417, 466, 965, 13], "temperature": 0.0, "avg_logprob": -0.17862818791316107, "compression_ratio": 1.6906779661016949, "no_speech_prob": 4.82824252685532e-05}, {"id": 31, "seek": 16312, "start": 182.72, "end": 188.96, "text": " Okay so if you remember from last time this was the idea of a simple recurrent neural", "tokens": [1033, 370, 498, 291, 1604, 490, 1036, 565, 341, 390, 264, 1558, 295, 257, 2199, 18680, 1753, 18161], "temperature": 0.0, "avg_logprob": -0.17862818791316107, "compression_ratio": 1.6906779661016949, "no_speech_prob": 4.82824252685532e-05}, {"id": 32, "seek": 18896, "start": 188.96, "end": 195.92000000000002, "text": " network language model. So we had a sequence of words as our context for which we've looked", "tokens": [3209, 2856, 2316, 13, 407, 321, 632, 257, 8310, 295, 2283, 382, 527, 4319, 337, 597, 321, 600, 2956], "temperature": 0.0, "avg_logprob": -0.13368337910349776, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.00013541418593376875}, {"id": 33, "seek": 18896, "start": 195.92000000000002, "end": 203.0, "text": " up word embeddings and then the recurrent neural network model ran this recurrent layer", "tokens": [493, 1349, 12240, 29432, 293, 550, 264, 18680, 1753, 18161, 3209, 2316, 5872, 341, 18680, 1753, 4583], "temperature": 0.0, "avg_logprob": -0.13368337910349776, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.00013541418593376875}, {"id": 34, "seek": 18896, "start": 203.0, "end": 209.52, "text": " where at each point we have a previous hidden state which can just be zero at the beginning", "tokens": [689, 412, 1184, 935, 321, 362, 257, 3894, 7633, 1785, 597, 393, 445, 312, 4018, 412, 264, 2863], "temperature": 0.0, "avg_logprob": -0.13368337910349776, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.00013541418593376875}, {"id": 35, "seek": 18896, "start": 209.52, "end": 216.68, "text": " of a sequence and you have feeding it in to the next hidden state, the previous hidden", "tokens": [295, 257, 8310, 293, 291, 362, 12919, 309, 294, 281, 264, 958, 7633, 1785, 11, 264, 3894, 7633], "temperature": 0.0, "avg_logprob": -0.13368337910349776, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.00013541418593376875}, {"id": 36, "seek": 21668, "start": 216.68, "end": 224.68, "text": " state and encoding and transform the coding of a word using this recurrent neural network", "tokens": [1785, 293, 43430, 293, 4088, 264, 17720, 295, 257, 1349, 1228, 341, 18680, 1753, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.119449875571511, "compression_ratio": 1.7233009708737863, "no_speech_prob": 5.142273221281357e-05}, {"id": 37, "seek": 21668, "start": 224.68, "end": 229.64000000000001, "text": " equation that I have on the left that's very central and based on that you compute a", "tokens": [5367, 300, 286, 362, 322, 264, 1411, 300, 311, 588, 5777, 293, 2361, 322, 300, 291, 14722, 257], "temperature": 0.0, "avg_logprob": -0.119449875571511, "compression_ratio": 1.7233009708737863, "no_speech_prob": 5.142273221281357e-05}, {"id": 38, "seek": 21668, "start": 229.64000000000001, "end": 237.60000000000002, "text": " new hidden representation for the next time step and you can repeat that along for successive", "tokens": [777, 7633, 10290, 337, 264, 958, 565, 1823, 293, 291, 393, 7149, 300, 2051, 337, 48043], "temperature": 0.0, "avg_logprob": -0.119449875571511, "compression_ratio": 1.7233009708737863, "no_speech_prob": 5.142273221281357e-05}, {"id": 39, "seek": 21668, "start": 237.60000000000002, "end": 245.12, "text": " time steps. Now we also usually want our recurrent neural networks to produce outputs.", "tokens": [565, 4439, 13, 823, 321, 611, 2673, 528, 527, 18680, 1753, 18161, 9590, 281, 5258, 23930, 13], "temperature": 0.0, "avg_logprob": -0.119449875571511, "compression_ratio": 1.7233009708737863, "no_speech_prob": 5.142273221281357e-05}, {"id": 40, "seek": 24512, "start": 245.12, "end": 251.56, "text": " So I only show it at the end here but at each time step we're then also going to generate", "tokens": [407, 286, 787, 855, 309, 412, 264, 917, 510, 457, 412, 1184, 565, 1823, 321, 434, 550, 611, 516, 281, 8460], "temperature": 0.0, "avg_logprob": -0.15193289588479436, "compression_ratio": 1.6435185185185186, "no_speech_prob": 5.141882866155356e-05}, {"id": 41, "seek": 24512, "start": 251.56, "end": 258.36, "text": " an output and so to do that we're feeding the hidden layer into a softmax layer so we're", "tokens": [364, 5598, 293, 370, 281, 360, 300, 321, 434, 12919, 264, 7633, 4583, 666, 257, 2787, 41167, 4583, 370, 321, 434], "temperature": 0.0, "avg_logprob": -0.15193289588479436, "compression_ratio": 1.6435185185185186, "no_speech_prob": 5.141882866155356e-05}, {"id": 42, "seek": 24512, "start": 258.36, "end": 263.84000000000003, "text": " doing another matrix multiply add on a bias, put it through the softmax equation and that", "tokens": [884, 1071, 8141, 12972, 909, 322, 257, 12577, 11, 829, 309, 807, 264, 2787, 41167, 5367, 293, 300], "temperature": 0.0, "avg_logprob": -0.15193289588479436, "compression_ratio": 1.6435185185185186, "no_speech_prob": 5.141882866155356e-05}, {"id": 43, "seek": 24512, "start": 263.84000000000003, "end": 269.2, "text": " will then gives the probability distribution over words and we can use that to predict", "tokens": [486, 550, 2709, 264, 8482, 7316, 670, 2283, 293, 321, 393, 764, 300, 281, 6069], "temperature": 0.0, "avg_logprob": -0.15193289588479436, "compression_ratio": 1.6435185185185186, "no_speech_prob": 5.141882866155356e-05}, {"id": 44, "seek": 26920, "start": 269.2, "end": 276.2, "text": " how likely it is that different words are going to occur after the students open there.", "tokens": [577, 3700, 309, 307, 300, 819, 2283, 366, 516, 281, 5160, 934, 264, 1731, 1269, 456, 13], "temperature": 0.0, "avg_logprob": -0.1788581557895826, "compression_ratio": 1.610091743119266, "no_speech_prob": 6.202133954502642e-05}, {"id": 45, "seek": 26920, "start": 276.2, "end": 282.12, "text": " Okay, so I didn't introduce that model but I hadn't really gone through the specifics", "tokens": [1033, 11, 370, 286, 994, 380, 5366, 300, 2316, 457, 286, 8782, 380, 534, 2780, 807, 264, 28454], "temperature": 0.0, "avg_logprob": -0.1788581557895826, "compression_ratio": 1.610091743119266, "no_speech_prob": 6.202133954502642e-05}, {"id": 46, "seek": 26920, "start": 282.12, "end": 290.44, "text": " of how we train this model, how we use it and evaluate it so let me go through this now.", "tokens": [295, 577, 321, 3847, 341, 2316, 11, 577, 321, 764, 309, 293, 13059, 309, 370, 718, 385, 352, 807, 341, 586, 13], "temperature": 0.0, "avg_logprob": -0.1788581557895826, "compression_ratio": 1.610091743119266, "no_speech_prob": 6.202133954502642e-05}, {"id": 47, "seek": 26920, "start": 290.44, "end": 296.28, "text": " So here's how we train an RNN language model, we get a big corpus of text, just a lot of", "tokens": [407, 510, 311, 577, 321, 3847, 364, 45702, 45, 2856, 2316, 11, 321, 483, 257, 955, 1181, 31624, 295, 2487, 11, 445, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.1788581557895826, "compression_ratio": 1.610091743119266, "no_speech_prob": 6.202133954502642e-05}, {"id": 48, "seek": 29628, "start": 296.28, "end": 303.59999999999997, "text": " text and so in regard that is just a long sequence of words x1 to xt and what we're going", "tokens": [2487, 293, 370, 294, 3843, 300, 307, 445, 257, 938, 8310, 295, 2283, 2031, 16, 281, 2031, 83, 293, 437, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.14301940373011998, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.00011946246377192438}, {"id": 49, "seek": 29628, "start": 303.59999999999997, "end": 313.32, "text": " to do is feed it into the RNN LM. So for each we're going to take prefixes of that sequence", "tokens": [281, 360, 307, 3154, 309, 666, 264, 45702, 45, 46529, 13, 407, 337, 1184, 321, 434, 516, 281, 747, 18417, 36005, 295, 300, 8310], "temperature": 0.0, "avg_logprob": -0.14301940373011998, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.00011946246377192438}, {"id": 50, "seek": 29628, "start": 313.32, "end": 319.91999999999996, "text": " and based on each prefix we're going to want to predict the probability distribution", "tokens": [293, 2361, 322, 1184, 46969, 321, 434, 516, 281, 528, 281, 6069, 264, 8482, 7316], "temperature": 0.0, "avg_logprob": -0.14301940373011998, "compression_ratio": 1.6024096385542168, "no_speech_prob": 0.00011946246377192438}, {"id": 51, "seek": 31992, "start": 319.92, "end": 327.88, "text": " for the word that comes next and then we're going to train our model by assessing how", "tokens": [337, 264, 1349, 300, 1487, 958, 293, 550, 321, 434, 516, 281, 3847, 527, 2316, 538, 34348, 577], "temperature": 0.0, "avg_logprob": -0.13047570717043994, "compression_ratio": 1.7075471698113207, "no_speech_prob": 7.228455797303468e-05}, {"id": 52, "seek": 31992, "start": 327.88, "end": 335.08000000000004, "text": " good a job we do about that and so the loss function we use is the loss function normally", "tokens": [665, 257, 1691, 321, 360, 466, 300, 293, 370, 264, 4470, 2445, 321, 764, 307, 264, 4470, 2445, 5646], "temperature": 0.0, "avg_logprob": -0.13047570717043994, "compression_ratio": 1.7075471698113207, "no_speech_prob": 7.228455797303468e-05}, {"id": 53, "seek": 31992, "start": 335.08000000000004, "end": 341.04, "text": " referred to as cross entropy loss in the literature which is this negative log likelihood loss.", "tokens": [10839, 281, 382, 3278, 30867, 4470, 294, 264, 10394, 597, 307, 341, 3671, 3565, 22119, 4470, 13], "temperature": 0.0, "avg_logprob": -0.13047570717043994, "compression_ratio": 1.7075471698113207, "no_speech_prob": 7.228455797303468e-05}, {"id": 54, "seek": 31992, "start": 341.04, "end": 348.12, "text": " So we are going to predict some word to come next. Well we have a probability distribution", "tokens": [407, 321, 366, 516, 281, 6069, 512, 1349, 281, 808, 958, 13, 1042, 321, 362, 257, 8482, 7316], "temperature": 0.0, "avg_logprob": -0.13047570717043994, "compression_ratio": 1.7075471698113207, "no_speech_prob": 7.228455797303468e-05}, {"id": 55, "seek": 34812, "start": 348.12, "end": 353.28000000000003, "text": " over predictions of what word comes next and actually there was an actual next word in", "tokens": [670, 21264, 295, 437, 1349, 1487, 958, 293, 767, 456, 390, 364, 3539, 958, 1349, 294], "temperature": 0.0, "avg_logprob": -0.1349799980237646, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.00011375949543435127}, {"id": 56, "seek": 34812, "start": 353.28000000000003, "end": 358.32, "text": " the text and so we say well what probability did you give to that word and maybe we gave", "tokens": [264, 2487, 293, 370, 321, 584, 731, 437, 8482, 630, 291, 976, 281, 300, 1349, 293, 1310, 321, 2729], "temperature": 0.0, "avg_logprob": -0.1349799980237646, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.00011375949543435127}, {"id": 57, "seek": 34812, "start": 358.32, "end": 363.4, "text": " it a probability estimate of.01. Well it would have been greater if we've given a probability", "tokens": [309, 257, 8482, 12539, 295, 2411, 10607, 13, 1042, 309, 576, 362, 668, 5044, 498, 321, 600, 2212, 257, 8482], "temperature": 0.0, "avg_logprob": -0.1349799980237646, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.00011375949543435127}, {"id": 58, "seek": 34812, "start": 363.4, "end": 369.56, "text": " estimate of almost one because that meant we've almost certain that what did come next", "tokens": [12539, 295, 1920, 472, 570, 300, 4140, 321, 600, 1920, 1629, 300, 437, 630, 808, 958], "temperature": 0.0, "avg_logprob": -0.1349799980237646, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.00011375949543435127}, {"id": 59, "seek": 34812, "start": 369.56, "end": 375.92, "text": " in our model and so we'll take a loss to the extent that we're giving the actual next", "tokens": [294, 527, 2316, 293, 370, 321, 603, 747, 257, 4470, 281, 264, 8396, 300, 321, 434, 2902, 264, 3539, 958], "temperature": 0.0, "avg_logprob": -0.1349799980237646, "compression_ratio": 1.8808510638297873, "no_speech_prob": 0.00011375949543435127}, {"id": 60, "seek": 37592, "start": 375.92, "end": 383.28000000000003, "text": " word a predicted probability of less than one. So then get an idea of how well we're doing", "tokens": [1349, 257, 19147, 8482, 295, 1570, 813, 472, 13, 407, 550, 483, 364, 1558, 295, 577, 731, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.10910768508911133, "compression_ratio": 1.6712962962962963, "no_speech_prob": 5.3848361858399585e-05}, {"id": 61, "seek": 37592, "start": 383.28000000000003, "end": 391.36, "text": " over the entire corpus we work out that loss at each position and then we work out the", "tokens": [670, 264, 2302, 1181, 31624, 321, 589, 484, 300, 4470, 412, 1184, 2535, 293, 550, 321, 589, 484, 264], "temperature": 0.0, "avg_logprob": -0.10910768508911133, "compression_ratio": 1.6712962962962963, "no_speech_prob": 5.3848361858399585e-05}, {"id": 62, "seek": 37592, "start": 391.36, "end": 397.6, "text": " average loss of the entire training set. So let's just go through that again more graphically", "tokens": [4274, 4470, 295, 264, 2302, 3097, 992, 13, 407, 718, 311, 445, 352, 807, 300, 797, 544, 4295, 984], "temperature": 0.0, "avg_logprob": -0.10910768508911133, "compression_ratio": 1.6712962962962963, "no_speech_prob": 5.3848361858399585e-05}, {"id": 63, "seek": 37592, "start": 397.6, "end": 405.0, "text": " in the next couple of slides. So down the bottom here's our corpus of text. We're running", "tokens": [294, 264, 958, 1916, 295, 9788, 13, 407, 760, 264, 2767, 510, 311, 527, 1181, 31624, 295, 2487, 13, 492, 434, 2614], "temperature": 0.0, "avg_logprob": -0.10910768508911133, "compression_ratio": 1.6712962962962963, "no_speech_prob": 5.3848361858399585e-05}, {"id": 64, "seek": 40500, "start": 405.0, "end": 412.48, "text": " it through our simple or current neural network and at each position we've predicting a probability", "tokens": [309, 807, 527, 2199, 420, 2190, 18161, 3209, 293, 412, 1184, 2535, 321, 600, 32884, 257, 8482], "temperature": 0.0, "avg_logprob": -0.13577255748567127, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.00014843700046185404}, {"id": 65, "seek": 40500, "start": 412.48, "end": 421.4, "text": " distribution over words. We then say well actually at each position we know what word is actually", "tokens": [7316, 670, 2283, 13, 492, 550, 584, 731, 767, 412, 1184, 2535, 321, 458, 437, 1349, 307, 767], "temperature": 0.0, "avg_logprob": -0.13577255748567127, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.00014843700046185404}, {"id": 66, "seek": 40500, "start": 421.4, "end": 427.72, "text": " next. So when we're at time step one the actual next word is students because we can see", "tokens": [958, 13, 407, 562, 321, 434, 412, 565, 1823, 472, 264, 3539, 958, 1349, 307, 1731, 570, 321, 393, 536], "temperature": 0.0, "avg_logprob": -0.13577255748567127, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.00014843700046185404}, {"id": 67, "seek": 40500, "start": 427.72, "end": 432.92, "text": " it just to the right of us here and we say what probability estimate did you give to students", "tokens": [309, 445, 281, 264, 558, 295, 505, 510, 293, 321, 584, 437, 8482, 12539, 630, 291, 976, 281, 1731], "temperature": 0.0, "avg_logprob": -0.13577255748567127, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.00014843700046185404}, {"id": 68, "seek": 43292, "start": 432.92, "end": 439.24, "text": " and to the extent that it's not high it's not one we take a loss and then we go on to the", "tokens": [293, 281, 264, 8396, 300, 309, 311, 406, 1090, 309, 311, 406, 472, 321, 747, 257, 4470, 293, 550, 321, 352, 322, 281, 264], "temperature": 0.0, "avg_logprob": -0.13062574646689676, "compression_ratio": 1.8795811518324608, "no_speech_prob": 8.740789780858904e-05}, {"id": 69, "seek": 43292, "start": 439.24, "end": 445.08000000000004, "text": " time step two and we say well at time step two you predict the probability distribution", "tokens": [565, 1823, 732, 293, 321, 584, 731, 412, 565, 1823, 732, 291, 6069, 264, 8482, 7316], "temperature": 0.0, "avg_logprob": -0.13062574646689676, "compression_ratio": 1.8795811518324608, "no_speech_prob": 8.740789780858904e-05}, {"id": 70, "seek": 43292, "start": 445.08000000000004, "end": 450.52000000000004, "text": " over words. The actual next word is opened so to the extent that you haven't given high", "tokens": [670, 2283, 13, 440, 3539, 958, 1349, 307, 5625, 370, 281, 264, 8396, 300, 291, 2378, 380, 2212, 1090], "temperature": 0.0, "avg_logprob": -0.13062574646689676, "compression_ratio": 1.8795811518324608, "no_speech_prob": 8.740789780858904e-05}, {"id": 71, "seek": 43292, "start": 450.52000000000004, "end": 457.48, "text": " probability to open you take a loss and then that repeats in time step three we're hoping the", "tokens": [8482, 281, 1269, 291, 747, 257, 4470, 293, 550, 300, 35038, 294, 565, 1823, 1045, 321, 434, 7159, 264], "temperature": 0.0, "avg_logprob": -0.13062574646689676, "compression_ratio": 1.8795811518324608, "no_speech_prob": 8.740789780858904e-05}, {"id": 72, "seek": 45748, "start": 457.48, "end": 464.28000000000003, "text": " model predict there at time step four we're hoping the model predict exams and then to work out", "tokens": [2316, 6069, 456, 412, 565, 1823, 1451, 321, 434, 7159, 264, 2316, 6069, 20514, 293, 550, 281, 589, 484], "temperature": 0.0, "avg_logprob": -0.10696552781497731, "compression_ratio": 1.6569767441860466, "no_speech_prob": 1.802958286134526e-05}, {"id": 73, "seek": 45748, "start": 464.28000000000003, "end": 473.96000000000004, "text": " our overall loss we're then averaging our per time step loss. So in a way this is a pretty", "tokens": [527, 4787, 4470, 321, 434, 550, 47308, 527, 680, 565, 1823, 4470, 13, 407, 294, 257, 636, 341, 307, 257, 1238], "temperature": 0.0, "avg_logprob": -0.10696552781497731, "compression_ratio": 1.6569767441860466, "no_speech_prob": 1.802958286134526e-05}, {"id": 74, "seek": 45748, "start": 473.96000000000004, "end": 481.48, "text": " obvious thing to do but note that there is a little subtlety here and in particular this algorithm", "tokens": [6322, 551, 281, 360, 457, 3637, 300, 456, 307, 257, 707, 7257, 75, 2210, 510, 293, 294, 1729, 341, 9284], "temperature": 0.0, "avg_logprob": -0.10696552781497731, "compression_ratio": 1.6569767441860466, "no_speech_prob": 1.802958286134526e-05}, {"id": 75, "seek": 48148, "start": 481.48, "end": 487.88, "text": " is referred to in the literature as teacher forcing and so what does that mean? Well you know you", "tokens": [307, 10839, 281, 294, 264, 10394, 382, 5027, 19030, 293, 370, 437, 775, 300, 914, 30, 1042, 291, 458, 291], "temperature": 0.0, "avg_logprob": -0.12162403727686683, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0003043249307665974}, {"id": 76, "seek": 48148, "start": 487.88, "end": 495.24, "text": " can imagine what you can do with a recurrent neural network is say okay just start generating", "tokens": [393, 3811, 437, 291, 393, 360, 365, 257, 18680, 1753, 18161, 3209, 307, 584, 1392, 445, 722, 17746], "temperature": 0.0, "avg_logprob": -0.12162403727686683, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0003043249307665974}, {"id": 77, "seek": 48148, "start": 495.24, "end": 500.28000000000003, "text": " maybe I'll give you a hint as to where to start I'll say the sentence starts thus students", "tokens": [1310, 286, 603, 976, 291, 257, 12075, 382, 281, 689, 281, 722, 286, 603, 584, 264, 8174, 3719, 8807, 1731], "temperature": 0.0, "avg_logprob": -0.12162403727686683, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0003043249307665974}, {"id": 78, "seek": 48148, "start": 500.28000000000003, "end": 507.8, "text": " and then let it run and see what it generates coming next it might start saying the students", "tokens": [293, 550, 718, 309, 1190, 293, 536, 437, 309, 23815, 1348, 958, 309, 1062, 722, 1566, 264, 1731], "temperature": 0.0, "avg_logprob": -0.12162403727686683, "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0003043249307665974}, {"id": 79, "seek": 50780, "start": 507.8, "end": 514.04, "text": " have been locked out in the classroom or whatever it is right and that we could say as well that's", "tokens": [362, 668, 9376, 484, 294, 264, 7419, 420, 2035, 309, 307, 558, 293, 300, 321, 727, 584, 382, 731, 300, 311], "temperature": 0.0, "avg_logprob": -0.06055079067454618, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00026065780548378825}, {"id": 80, "seek": 50780, "start": 514.04, "end": 519.16, "text": " not very close to what the actual text says and somehow we want to learn from that and if you go", "tokens": [406, 588, 1998, 281, 437, 264, 3539, 2487, 1619, 293, 6063, 321, 528, 281, 1466, 490, 300, 293, 498, 291, 352], "temperature": 0.0, "avg_logprob": -0.06055079067454618, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00026065780548378825}, {"id": 81, "seek": 50780, "start": 519.16, "end": 524.6800000000001, "text": " in that direction there's a space of things you can do that leads into more complex algorithms", "tokens": [294, 300, 3513, 456, 311, 257, 1901, 295, 721, 291, 393, 360, 300, 6689, 666, 544, 3997, 14642], "temperature": 0.0, "avg_logprob": -0.06055079067454618, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00026065780548378825}, {"id": 82, "seek": 50780, "start": 524.6800000000001, "end": 532.12, "text": " such as reinforcement learning but from the perspective of training these neural models that's", "tokens": [1270, 382, 29280, 2539, 457, 490, 264, 4585, 295, 3097, 613, 18161, 5245, 300, 311], "temperature": 0.0, "avg_logprob": -0.06055079067454618, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00026065780548378825}, {"id": 83, "seek": 53212, "start": 532.12, "end": 540.76, "text": " unjule complex and unnecessary so we have this very simple way of doing things which is what we do", "tokens": [517, 73, 2271, 3997, 293, 19350, 370, 321, 362, 341, 588, 2199, 636, 295, 884, 721, 597, 307, 437, 321, 360], "temperature": 0.0, "avg_logprob": -0.12839627265930176, "compression_ratio": 1.9211822660098523, "no_speech_prob": 6.099587699281983e-05}, {"id": 84, "seek": 53212, "start": 540.76, "end": 547.88, "text": " is just predict one time step forward so we say we know that the prefix is the students predict a", "tokens": [307, 445, 6069, 472, 565, 1823, 2128, 370, 321, 584, 321, 458, 300, 264, 46969, 307, 264, 1731, 6069, 257], "temperature": 0.0, "avg_logprob": -0.12839627265930176, "compression_ratio": 1.9211822660098523, "no_speech_prob": 6.099587699281983e-05}, {"id": 85, "seek": 53212, "start": 547.88, "end": 553.0, "text": " probability distribution over the next word it's good to the extent that you give probability", "tokens": [8482, 7316, 670, 264, 958, 1349, 309, 311, 665, 281, 264, 8396, 300, 291, 976, 8482], "temperature": 0.0, "avg_logprob": -0.12839627265930176, "compression_ratio": 1.9211822660098523, "no_speech_prob": 6.099587699281983e-05}, {"id": 86, "seek": 53212, "start": 553.0, "end": 559.8, "text": " mass to open okay now the prefix is the students opened predict the a probability distribution over", "tokens": [2758, 281, 1269, 1392, 586, 264, 46969, 307, 264, 1731, 5625, 6069, 264, 257, 8482, 7316, 670], "temperature": 0.0, "avg_logprob": -0.12839627265930176, "compression_ratio": 1.9211822660098523, "no_speech_prob": 6.099587699281983e-05}, {"id": 87, "seek": 55980, "start": 559.8, "end": 567.0799999999999, "text": " the next word it's good to the extent that you give probability mass to there and so effectively", "tokens": [264, 958, 1349, 309, 311, 665, 281, 264, 8396, 300, 291, 976, 8482, 2758, 281, 456, 293, 370, 8659], "temperature": 0.0, "avg_logprob": -0.07304521183391194, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0002254422870464623}, {"id": 88, "seek": 55980, "start": 567.0799999999999, "end": 572.76, "text": " at each step we're resetting to what was actually in the corpus so you know it's possible after the", "tokens": [412, 1184, 1823, 321, 434, 14322, 783, 281, 437, 390, 767, 294, 264, 1181, 31624, 370, 291, 458, 309, 311, 1944, 934, 264], "temperature": 0.0, "avg_logprob": -0.07304521183391194, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0002254422870464623}, {"id": 89, "seek": 55980, "start": 572.76, "end": 580.04, "text": " students opened the model thinks that by far the most probable thing to come next is ah or thus say", "tokens": [1731, 5625, 264, 2316, 7309, 300, 538, 1400, 264, 881, 21759, 551, 281, 808, 958, 307, 3716, 420, 8807, 584], "temperature": 0.0, "avg_logprob": -0.07304521183391194, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0002254422870464623}, {"id": 90, "seek": 55980, "start": 580.04, "end": 585.4799999999999, "text": " I mean we don't actually use what the model suggested we penalize the model for not having", "tokens": [286, 914, 321, 500, 380, 767, 764, 437, 264, 2316, 10945, 321, 13661, 1125, 264, 2316, 337, 406, 1419], "temperature": 0.0, "avg_logprob": -0.07304521183391194, "compression_ratio": 1.7354260089686098, "no_speech_prob": 0.0002254422870464623}, {"id": 91, "seek": 58548, "start": 585.48, "end": 590.84, "text": " suggested there but then we just go with what's actually in the corpus and ask it to predict again", "tokens": [10945, 456, 457, 550, 321, 445, 352, 365, 437, 311, 767, 294, 264, 1181, 31624, 293, 1029, 309, 281, 6069, 797], "temperature": 0.0, "avg_logprob": -0.08385677898631376, "compression_ratio": 1.5921787709497206, "no_speech_prob": 4.8229914682451636e-05}, {"id": 92, "seek": 58548, "start": 595.64, "end": 602.6, "text": " this is just a little side thing but it's an important part to know if you're actually training", "tokens": [341, 307, 445, 257, 707, 1252, 551, 457, 309, 311, 364, 1021, 644, 281, 458, 498, 291, 434, 767, 3097], "temperature": 0.0, "avg_logprob": -0.08385677898631376, "compression_ratio": 1.5921787709497206, "no_speech_prob": 4.8229914682451636e-05}, {"id": 93, "seek": 58548, "start": 602.6, "end": 609.72, "text": " your own neural language model I sort of presented as one huge corpus that we chug through", "tokens": [428, 1065, 18161, 2856, 2316, 286, 1333, 295, 8212, 382, 472, 2603, 1181, 31624, 300, 321, 417, 697, 807], "temperature": 0.0, "avg_logprob": -0.08385677898631376, "compression_ratio": 1.5921787709497206, "no_speech_prob": 4.8229914682451636e-05}, {"id": 94, "seek": 60972, "start": 609.72, "end": 617.96, "text": " but in practice we don't chug through a whole corpus one step at a time what we do is we cut the whole", "tokens": [457, 294, 3124, 321, 500, 380, 417, 697, 807, 257, 1379, 1181, 31624, 472, 1823, 412, 257, 565, 437, 321, 360, 307, 321, 1723, 264, 1379], "temperature": 0.0, "avg_logprob": -0.09628366620353099, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.0600818718085065e-05}, {"id": 95, "seek": 60972, "start": 617.96, "end": 624.6, "text": " corpus into shorter pieces which might commonly be sentences or documents or sometimes they're", "tokens": [1181, 31624, 666, 11639, 3755, 597, 1062, 12719, 312, 16579, 420, 8512, 420, 2171, 436, 434], "temperature": 0.0, "avg_logprob": -0.09628366620353099, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.0600818718085065e-05}, {"id": 96, "seek": 60972, "start": 624.6, "end": 629.72, "text": " literally just pieces that are chopped right so your recall that stochastic gradient send", "tokens": [3736, 445, 3755, 300, 366, 16497, 558, 370, 428, 9901, 300, 342, 8997, 2750, 16235, 2845], "temperature": 0.0, "avg_logprob": -0.09628366620353099, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.0600818718085065e-05}, {"id": 97, "seek": 60972, "start": 629.72, "end": 635.88, "text": " allows us to compute a lost ingredients from a small chunk of data and update so what we do is we", "tokens": [4045, 505, 281, 14722, 257, 2731, 6952, 490, 257, 1359, 16635, 295, 1412, 293, 5623, 370, 437, 321, 360, 307, 321], "temperature": 0.0, "avg_logprob": -0.09628366620353099, "compression_ratio": 1.742081447963801, "no_speech_prob": 3.0600818718085065e-05}, {"id": 98, "seek": 63588, "start": 635.88, "end": 642.76, "text": " take these small pieces compute gradients from those and update weights and repeat and in particular", "tokens": [747, 613, 1359, 3755, 14722, 2771, 2448, 490, 729, 293, 5623, 17443, 293, 7149, 293, 294, 1729], "temperature": 0.0, "avg_logprob": -0.053759903743349276, "compression_ratio": 1.7990867579908676, "no_speech_prob": 3.70089546777308e-05}, {"id": 99, "seek": 63588, "start": 643.56, "end": 650.36, "text": " we get a lot more speed and efficiency in training if we aren't actually doing an update for just", "tokens": [321, 483, 257, 688, 544, 3073, 293, 10493, 294, 3097, 498, 321, 3212, 380, 767, 884, 364, 5623, 337, 445], "temperature": 0.0, "avg_logprob": -0.053759903743349276, "compression_ratio": 1.7990867579908676, "no_speech_prob": 3.70089546777308e-05}, {"id": 100, "seek": 63588, "start": 650.36, "end": 656.76, "text": " one sentence at a time but actually a batch of sentences so typically what we'll actually do", "tokens": [472, 8174, 412, 257, 565, 457, 767, 257, 15245, 295, 16579, 370, 5850, 437, 321, 603, 767, 360], "temperature": 0.0, "avg_logprob": -0.053759903743349276, "compression_ratio": 1.7990867579908676, "no_speech_prob": 3.70089546777308e-05}, {"id": 101, "seek": 63588, "start": 656.76, "end": 663.8, "text": " is we'll feed to the model 32 sentences say of a similar length at the same time compute gradients for", "tokens": [307, 321, 603, 3154, 281, 264, 2316, 8858, 16579, 584, 295, 257, 2531, 4641, 412, 264, 912, 565, 14722, 2771, 2448, 337], "temperature": 0.0, "avg_logprob": -0.053759903743349276, "compression_ratio": 1.7990867579908676, "no_speech_prob": 3.70089546777308e-05}, {"id": 102, "seek": 66380, "start": 663.8, "end": 673.0799999999999, "text": " them update weights and then get another batch of sentences to train on how do we train I haven't", "tokens": [552, 5623, 17443, 293, 550, 483, 1071, 15245, 295, 16579, 281, 3847, 322, 577, 360, 321, 3847, 286, 2378, 380], "temperature": 0.0, "avg_logprob": -0.07749255867891533, "compression_ratio": 1.7168141592920354, "no_speech_prob": 1.5193261788226664e-05}, {"id": 103, "seek": 66380, "start": 673.0799999999999, "end": 679.4, "text": " sort of gone through the details of this I mean in one sense the answer is just like we talked", "tokens": [1333, 295, 2780, 807, 264, 4365, 295, 341, 286, 914, 294, 472, 2020, 264, 1867, 307, 445, 411, 321, 2825], "temperature": 0.0, "avg_logprob": -0.07749255867891533, "compression_ratio": 1.7168141592920354, "no_speech_prob": 1.5193261788226664e-05}, {"id": 104, "seek": 66380, "start": 679.4, "end": 685.7199999999999, "text": " about lecture three we use back propagation to get gradients and update parameters but let's", "tokens": [466, 7991, 1045, 321, 764, 646, 38377, 281, 483, 2771, 2448, 293, 5623, 9834, 457, 718, 311], "temperature": 0.0, "avg_logprob": -0.07749255867891533, "compression_ratio": 1.7168141592920354, "no_speech_prob": 1.5193261788226664e-05}, {"id": 105, "seek": 68572, "start": 685.72, "end": 693.64, "text": " take at least a minute to go through the differences and subtleties of the current neural network case", "tokens": [747, 412, 1935, 257, 3456, 281, 352, 807, 264, 7300, 293, 7257, 2631, 530, 295, 264, 2190, 18161, 3209, 1389], "temperature": 0.0, "avg_logprob": -0.07827370507376534, "compression_ratio": 1.6888888888888889, "no_speech_prob": 4.532278762781061e-05}, {"id": 106, "seek": 68572, "start": 693.64, "end": 700.36, "text": " and the central thing that's a bit you know as before we're going to take our loss and we're going", "tokens": [293, 264, 5777, 551, 300, 311, 257, 857, 291, 458, 382, 949, 321, 434, 516, 281, 747, 527, 4470, 293, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.07827370507376534, "compression_ratio": 1.6888888888888889, "no_speech_prob": 4.532278762781061e-05}, {"id": 107, "seek": 68572, "start": 700.36, "end": 707.5600000000001, "text": " to back propagate it to all of the parameters of the network everything from word embeddings to biases", "tokens": [281, 646, 48256, 309, 281, 439, 295, 264, 9834, 295, 264, 3209, 1203, 490, 1349, 12240, 29432, 281, 32152], "temperature": 0.0, "avg_logprob": -0.07827370507376534, "compression_ratio": 1.6888888888888889, "no_speech_prob": 4.532278762781061e-05}, {"id": 108, "seek": 70756, "start": 707.56, "end": 716.3599999999999, "text": " etc but the central bit that's a little bit different and is more complicated is that we have this", "tokens": [5183, 457, 264, 5777, 857, 300, 311, 257, 707, 857, 819, 293, 307, 544, 6179, 307, 300, 321, 362, 341], "temperature": 0.0, "avg_logprob": -0.12455039237862203, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.110012064804323e-05}, {"id": 109, "seek": 70756, "start": 716.3599999999999, "end": 725.56, "text": " WH matrix that runs along the sequence that we keep on applying to update our hidden state so what's", "tokens": [8183, 8141, 300, 6676, 2051, 264, 8310, 300, 321, 1066, 322, 9275, 281, 5623, 527, 7633, 1785, 370, 437, 311], "temperature": 0.0, "avg_logprob": -0.12455039237862203, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.110012064804323e-05}, {"id": 110, "seek": 70756, "start": 725.56, "end": 735.0799999999999, "text": " the derivative of Jt of theta with respect to the repeated weight matrix WH and well the answer", "tokens": [264, 13760, 295, 508, 83, 295, 9725, 365, 3104, 281, 264, 10477, 3364, 8141, 8183, 293, 731, 264, 1867], "temperature": 0.0, "avg_logprob": -0.12455039237862203, "compression_ratio": 1.5945945945945945, "no_speech_prob": 2.110012064804323e-05}, {"id": 111, "seek": 73508, "start": 735.08, "end": 745.24, "text": " that is that what we do is we look at it in each position and work out what the", "tokens": [300, 307, 300, 437, 321, 360, 307, 321, 574, 412, 309, 294, 1184, 2535, 293, 589, 484, 437, 264], "temperature": 0.0, "avg_logprob": -0.10922899097204208, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.7482147086411715e-05}, {"id": 112, "seek": 73508, "start": 745.24, "end": 754.6800000000001, "text": " particles are of Jt with respect to WH in position one or position two position three position four", "tokens": [10007, 366, 295, 508, 83, 365, 3104, 281, 8183, 294, 2535, 472, 420, 2535, 732, 2535, 1045, 2535, 1451], "temperature": 0.0, "avg_logprob": -0.10922899097204208, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.7482147086411715e-05}, {"id": 113, "seek": 73508, "start": 754.6800000000001, "end": 760.6800000000001, "text": " etc right along the sequence and we just sum up all of those particles and that gives us", "tokens": [5183, 558, 2051, 264, 8310, 293, 321, 445, 2408, 493, 439, 295, 729, 10007, 293, 300, 2709, 505], "temperature": 0.0, "avg_logprob": -0.10922899097204208, "compression_ratio": 1.7179487179487178, "no_speech_prob": 1.7482147086411715e-05}, {"id": 114, "seek": 76068, "start": 760.68, "end": 771.64, "text": " a partial for Jt with respect to WH overall so the answer for a current neural networks is the", "tokens": [257, 14641, 337, 508, 83, 365, 3104, 281, 8183, 4787, 370, 264, 1867, 337, 257, 2190, 18161, 9590, 307, 264], "temperature": 0.0, "avg_logprob": -0.08684599760806921, "compression_ratio": 1.7654320987654322, "no_speech_prob": 2.584930189186707e-05}, {"id": 115, "seek": 76068, "start": 771.64, "end": 777.4, "text": " gradient with respect to a repeated weight in our current network is the sum of the gradient", "tokens": [16235, 365, 3104, 281, 257, 10477, 3364, 294, 527, 2190, 3209, 307, 264, 2408, 295, 264, 16235], "temperature": 0.0, "avg_logprob": -0.08684599760806921, "compression_ratio": 1.7654320987654322, "no_speech_prob": 2.584930189186707e-05}, {"id": 116, "seek": 76068, "start": 777.4, "end": 786.3599999999999, "text": " with respect to each time it appears and let me just then go through a little why that is the case", "tokens": [365, 3104, 281, 1184, 565, 309, 7038, 293, 718, 385, 445, 550, 352, 807, 257, 707, 983, 300, 307, 264, 1389], "temperature": 0.0, "avg_logprob": -0.08684599760806921, "compression_ratio": 1.7654320987654322, "no_speech_prob": 2.584930189186707e-05}, {"id": 117, "seek": 78636, "start": 786.36, "end": 794.84, "text": " but before I do that let me just note one gotcha I mean it's just not the case that this means", "tokens": [457, 949, 286, 360, 300, 718, 385, 445, 3637, 472, 658, 4413, 286, 914, 309, 311, 445, 406, 264, 1389, 300, 341, 1355], "temperature": 0.0, "avg_logprob": -0.06210690074496799, "compression_ratio": 1.6187845303867403, "no_speech_prob": 2.0768316971953027e-05}, {"id": 118, "seek": 78636, "start": 794.84, "end": 805.8000000000001, "text": " it equals t times the partial of Jt with respect to WH because we're using WH here here here here here", "tokens": [309, 6915, 256, 1413, 264, 14641, 295, 508, 83, 365, 3104, 281, 8183, 570, 321, 434, 1228, 8183, 510, 510, 510, 510, 510], "temperature": 0.0, "avg_logprob": -0.06210690074496799, "compression_ratio": 1.6187845303867403, "no_speech_prob": 2.0768316971953027e-05}, {"id": 119, "seek": 78636, "start": 805.8000000000001, "end": 811.72, "text": " through the sequence and for each of the places we use it there's a different upstream gradient", "tokens": [807, 264, 8310, 293, 337, 1184, 295, 264, 3190, 321, 764, 309, 456, 311, 257, 819, 33915, 16235], "temperature": 0.0, "avg_logprob": -0.06210690074496799, "compression_ratio": 1.6187845303867403, "no_speech_prob": 2.0768316971953027e-05}, {"id": 120, "seek": 81172, "start": 811.72, "end": 817.72, "text": " that's being fed into it so each of the values in this sum will be completely different from each other", "tokens": [300, 311, 885, 4636, 666, 309, 370, 1184, 295, 264, 4190, 294, 341, 2408, 486, 312, 2584, 819, 490, 1184, 661], "temperature": 0.0, "avg_logprob": -0.12061610691983934, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.9214641724829562e-05}, {"id": 121, "seek": 81172, "start": 819.8000000000001, "end": 828.0400000000001, "text": " well why we get this answer is essentially a consequence of what we talked about in the third", "tokens": [731, 983, 321, 483, 341, 1867, 307, 4476, 257, 18326, 295, 437, 321, 2825, 466, 294, 264, 2636], "temperature": 0.0, "avg_logprob": -0.12061610691983934, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.9214641724829562e-05}, {"id": 122, "seek": 81172, "start": 828.0400000000001, "end": 836.12, "text": " lecture so to take the simplest case of it right that if you have a multivariable function f of xy", "tokens": [7991, 370, 281, 747, 264, 22811, 1389, 295, 309, 558, 300, 498, 291, 362, 257, 2120, 592, 3504, 712, 2445, 283, 295, 2031, 88], "temperature": 0.0, "avg_logprob": -0.12061610691983934, "compression_ratio": 1.5828877005347595, "no_speech_prob": 1.9214641724829562e-05}, {"id": 123, "seek": 83612, "start": 836.12, "end": 845.96, "text": " and you have two single variable functions x of t and y of t which are fed one input t well then", "tokens": [293, 291, 362, 732, 2167, 7006, 6828, 2031, 295, 256, 293, 288, 295, 256, 597, 366, 4636, 472, 4846, 256, 731, 550], "temperature": 0.0, "avg_logprob": -0.12939958011402802, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.00011400527000660077}, {"id": 124, "seek": 83612, "start": 845.96, "end": 855.5600000000001, "text": " the simple version of working out the derivative of this function is you take the derivative down one", "tokens": [264, 2199, 3037, 295, 1364, 484, 264, 13760, 295, 341, 2445, 307, 291, 747, 264, 13760, 760, 472], "temperature": 0.0, "avg_logprob": -0.12939958011402802, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.00011400527000660077}, {"id": 125, "seek": 83612, "start": 855.5600000000001, "end": 863.8, "text": " path and you take the derivative down the other path and so in the slides in lecture 3 that was", "tokens": [3100, 293, 291, 747, 264, 13760, 760, 264, 661, 3100, 293, 370, 294, 264, 9788, 294, 7991, 805, 300, 390], "temperature": 0.0, "avg_logprob": -0.12939958011402802, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.00011400527000660077}, {"id": 126, "seek": 86380, "start": 863.8, "end": 871.16, "text": " what was summarized on a couple of slides by the slogan gradient sum at outward branches so t", "tokens": [437, 390, 14611, 1602, 322, 257, 1916, 295, 9788, 538, 264, 33052, 16235, 2408, 412, 26914, 14770, 370, 256], "temperature": 0.0, "avg_logprob": -0.0678450247820686, "compression_ratio": 1.6785714285714286, "no_speech_prob": 3.531661059241742e-05}, {"id": 127, "seek": 86380, "start": 871.16, "end": 877.9599999999999, "text": " has outward branches and so you take gradient here on the left gradient on the right and you", "tokens": [575, 26914, 14770, 293, 370, 291, 747, 16235, 510, 322, 264, 1411, 16235, 322, 264, 558, 293, 291], "temperature": 0.0, "avg_logprob": -0.0678450247820686, "compression_ratio": 1.6785714285714286, "no_speech_prob": 3.531661059241742e-05}, {"id": 128, "seek": 86380, "start": 877.9599999999999, "end": 883.9599999999999, "text": " sum them together and so really what's happening with the recurrent neural network is just", "tokens": [2408, 552, 1214, 293, 370, 534, 437, 311, 2737, 365, 264, 18680, 1753, 18161, 3209, 307, 445], "temperature": 0.0, "avg_logprob": -0.0678450247820686, "compression_ratio": 1.6785714285714286, "no_speech_prob": 3.531661059241742e-05}, {"id": 129, "seek": 86380, "start": 884.8399999999999, "end": 892.68, "text": " many pieces generalization of this so we have one WH matrix and we're using it to keep on updating", "tokens": [867, 3755, 2674, 2144, 295, 341, 370, 321, 362, 472, 8183, 8141, 293, 321, 434, 1228, 309, 281, 1066, 322, 25113], "temperature": 0.0, "avg_logprob": -0.0678450247820686, "compression_ratio": 1.6785714285714286, "no_speech_prob": 3.531661059241742e-05}, {"id": 130, "seek": 89268, "start": 892.68, "end": 901.4799999999999, "text": " the hidden state at time one time two time three right through time t and so what we're going to get", "tokens": [264, 7633, 1785, 412, 565, 472, 565, 732, 565, 1045, 558, 807, 565, 256, 293, 370, 437, 321, 434, 516, 281, 483], "temperature": 0.0, "avg_logprob": -0.057144890368824273, "compression_ratio": 1.81875, "no_speech_prob": 6.49298308417201e-05}, {"id": 131, "seek": 89268, "start": 901.4799999999999, "end": 911.88, "text": " is that this has a lot of outward branches and we're going to sum the gradient path at each one", "tokens": [307, 300, 341, 575, 257, 688, 295, 26914, 14770, 293, 321, 434, 516, 281, 2408, 264, 16235, 3100, 412, 1184, 472], "temperature": 0.0, "avg_logprob": -0.057144890368824273, "compression_ratio": 1.81875, "no_speech_prob": 6.49298308417201e-05}, {"id": 132, "seek": 89268, "start": 911.88, "end": 919.8, "text": " of them but what is this gradient path here it kind of goes down here and then goes down there", "tokens": [295, 552, 457, 437, 307, 341, 16235, 3100, 510, 309, 733, 295, 1709, 760, 510, 293, 550, 1709, 760, 456], "temperature": 0.0, "avg_logprob": -0.057144890368824273, "compression_ratio": 1.81875, "no_speech_prob": 6.49298308417201e-05}, {"id": 133, "seek": 91980, "start": 919.8, "end": 927.7199999999999, "text": " but you know actually the bottom part is that we're just using WH at each position so we have the", "tokens": [457, 291, 458, 767, 264, 2767, 644, 307, 300, 321, 434, 445, 1228, 8183, 412, 1184, 2535, 370, 321, 362, 264], "temperature": 0.0, "avg_logprob": -0.10047212328229632, "compression_ratio": 1.722543352601156, "no_speech_prob": 1.669013909122441e-05}, {"id": 134, "seek": 91980, "start": 927.7199999999999, "end": 935.9599999999999, "text": " partial of WH used at position i with respect to the partial of WH which is just our weight matrix", "tokens": [14641, 295, 8183, 1143, 412, 2535, 741, 365, 3104, 281, 264, 14641, 295, 8183, 597, 307, 445, 527, 3364, 8141], "temperature": 0.0, "avg_logprob": -0.10047212328229632, "compression_ratio": 1.722543352601156, "no_speech_prob": 1.669013909122441e-05}, {"id": 135, "seek": 91980, "start": 935.9599999999999, "end": 942.3599999999999, "text": " for our recurrent neural network so that's just one because you know we're just using the same matrix", "tokens": [337, 527, 18680, 1753, 18161, 3209, 370, 300, 311, 445, 472, 570, 291, 458, 321, 434, 445, 1228, 264, 912, 8141], "temperature": 0.0, "avg_logprob": -0.10047212328229632, "compression_ratio": 1.722543352601156, "no_speech_prob": 1.669013909122441e-05}, {"id": 136, "seek": 94236, "start": 942.36, "end": 949.8000000000001, "text": " everywhere and so we are just then summing the partials in each position that we use it", "tokens": [5315, 293, 370, 321, 366, 445, 550, 2408, 2810, 264, 644, 12356, 294, 1184, 2535, 300, 321, 764, 309], "temperature": 0.0, "avg_logprob": -0.10101751071303638, "compression_ratio": 1.5795454545454546, "no_speech_prob": 1.2019923815387301e-05}, {"id": 137, "seek": 94236, "start": 952.52, "end": 960.2, "text": " okay i'm practically what does that mean in terms of how you compute this um well if you're doing", "tokens": [1392, 741, 478, 15667, 437, 775, 300, 914, 294, 2115, 295, 577, 291, 14722, 341, 1105, 731, 498, 291, 434, 884], "temperature": 0.0, "avg_logprob": -0.10101751071303638, "compression_ratio": 1.5795454545454546, "no_speech_prob": 1.2019923815387301e-05}, {"id": 138, "seek": 94236, "start": 960.2, "end": 967.96, "text": " it by hand um what happens is you start at the end just like the general lecture three story", "tokens": [309, 538, 1011, 1105, 437, 2314, 307, 291, 722, 412, 264, 917, 445, 411, 264, 2674, 7991, 1045, 1657], "temperature": 0.0, "avg_logprob": -0.10101751071303638, "compression_ratio": 1.5795454545454546, "no_speech_prob": 1.2019923815387301e-05}, {"id": 139, "seek": 96796, "start": 967.96, "end": 976.9200000000001, "text": " you work out um derivatives um with respect to the hidden layer and then with respect to WH", "tokens": [291, 589, 484, 1105, 33733, 1105, 365, 3104, 281, 264, 7633, 4583, 293, 550, 365, 3104, 281, 8183], "temperature": 0.0, "avg_logprob": -0.059090739030104415, "compression_ratio": 1.6848484848484848, "no_speech_prob": 1.1837008059956133e-05}, {"id": 140, "seek": 96796, "start": 976.9200000000001, "end": 984.6800000000001, "text": " at the last time step and so that gives you one update for WH but then you continue passing the", "tokens": [412, 264, 1036, 565, 1823, 293, 370, 300, 2709, 291, 472, 5623, 337, 8183, 457, 550, 291, 2354, 8437, 264], "temperature": 0.0, "avg_logprob": -0.059090739030104415, "compression_ratio": 1.6848484848484848, "no_speech_prob": 1.1837008059956133e-05}, {"id": 141, "seek": 96796, "start": 984.6800000000001, "end": 991.08, "text": " gradient back to the t minus one time step and after a couple more steps of the chain rule", "tokens": [16235, 646, 281, 264, 256, 3175, 472, 565, 1823, 293, 934, 257, 1916, 544, 4439, 295, 264, 5021, 4978], "temperature": 0.0, "avg_logprob": -0.059090739030104415, "compression_ratio": 1.6848484848484848, "no_speech_prob": 1.1837008059956133e-05}, {"id": 142, "seek": 99108, "start": 991.08, "end": 998.6800000000001, "text": " you get another update for WH and you simply sum that onto your previous update for WH and then you", "tokens": [291, 483, 1071, 5623, 337, 8183, 293, 291, 2935, 2408, 300, 3911, 428, 3894, 5623, 337, 8183, 293, 550, 291], "temperature": 0.0, "avg_logprob": -0.09589359283447266, "compression_ratio": 2.1142857142857143, "no_speech_prob": 9.0661260401248e-06}, {"id": 143, "seek": 99108, "start": 998.6800000000001, "end": 1006.2, "text": " go to ht minus two you get another update for WH and you sum that onto your update for WH and you go", "tokens": [352, 281, 276, 83, 3175, 732, 291, 483, 1071, 5623, 337, 8183, 293, 291, 2408, 300, 3911, 428, 5623, 337, 8183, 293, 291, 352], "temperature": 0.0, "avg_logprob": -0.09589359283447266, "compression_ratio": 2.1142857142857143, "no_speech_prob": 9.0661260401248e-06}, {"id": 144, "seek": 99108, "start": 1006.2, "end": 1013.4000000000001, "text": " back all the way um and you sum up the gradients as you go um and that gives you a total update", "tokens": [646, 439, 264, 636, 1105, 293, 291, 2408, 493, 264, 2771, 2448, 382, 291, 352, 1105, 293, 300, 2709, 291, 257, 3217, 5623], "temperature": 0.0, "avg_logprob": -0.09589359283447266, "compression_ratio": 2.1142857142857143, "no_speech_prob": 9.0661260401248e-06}, {"id": 145, "seek": 101340, "start": 1013.4, "end": 1021.56, "text": " um for WH um and so there's sort of two tricks here and i'll just mention um the two tricks", "tokens": [1105, 337, 8183, 1105, 293, 370, 456, 311, 1333, 295, 732, 11733, 510, 293, 741, 603, 445, 2152, 1105, 264, 732, 11733], "temperature": 0.0, "avg_logprob": -0.08404200800349203, "compression_ratio": 1.737556561085973, "no_speech_prob": 4.3897332943743095e-05}, {"id": 146, "seek": 101340, "start": 1021.56, "end": 1028.04, "text": " you have to kind of separately sum the updates for WH and then once you finished um apply them all", "tokens": [291, 362, 281, 733, 295, 14759, 2408, 264, 9205, 337, 8183, 293, 550, 1564, 291, 4335, 1105, 3079, 552, 439], "temperature": 0.0, "avg_logprob": -0.08404200800349203, "compression_ratio": 1.737556561085973, "no_speech_prob": 4.3897332943743095e-05}, {"id": 147, "seek": 101340, "start": 1028.04, "end": 1034.44, "text": " at once you don't want to actually be changing the WH matrix as you go because that's then invalid", "tokens": [412, 1564, 291, 500, 380, 528, 281, 767, 312, 4473, 264, 8183, 8141, 382, 291, 352, 570, 300, 311, 550, 34702], "temperature": 0.0, "avg_logprob": -0.08404200800349203, "compression_ratio": 1.737556561085973, "no_speech_prob": 4.3897332943743095e-05}, {"id": 148, "seek": 101340, "start": 1034.44, "end": 1041.08, "text": " because the forward calculations were done with the constant WH that you had from the previous", "tokens": [570, 264, 2128, 20448, 645, 1096, 365, 264, 5754, 8183, 300, 291, 632, 490, 264, 3894], "temperature": 0.0, "avg_logprob": -0.08404200800349203, "compression_ratio": 1.737556561085973, "no_speech_prob": 4.3897332943743095e-05}, {"id": 149, "seek": 104108, "start": 1041.08, "end": 1049.8799999999999, "text": " um state all through the network um the second trick is well if you're doing this for sentences", "tokens": [1105, 1785, 439, 807, 264, 3209, 1105, 264, 1150, 4282, 307, 731, 498, 291, 434, 884, 341, 337, 16579], "temperature": 0.0, "avg_logprob": -0.051840142770247025, "compression_ratio": 1.7252252252252251, "no_speech_prob": 7.479250780306756e-05}, {"id": 150, "seek": 104108, "start": 1049.8799999999999, "end": 1055.24, "text": " you can normally just go back to the beginning of the sentence um but if you've got very long", "tokens": [291, 393, 5646, 445, 352, 646, 281, 264, 2863, 295, 264, 8174, 1105, 457, 498, 291, 600, 658, 588, 938], "temperature": 0.0, "avg_logprob": -0.051840142770247025, "compression_ratio": 1.7252252252252251, "no_speech_prob": 7.479250780306756e-05}, {"id": 151, "seek": 104108, "start": 1055.24, "end": 1061.1599999999999, "text": " sequences this can really slow you down if you're having to sort of run this algorithm back for a", "tokens": [22978, 341, 393, 534, 2964, 291, 760, 498, 291, 434, 1419, 281, 1333, 295, 1190, 341, 9284, 646, 337, 257], "temperature": 0.0, "avg_logprob": -0.051840142770247025, "compression_ratio": 1.7252252252252251, "no_speech_prob": 7.479250780306756e-05}, {"id": 152, "seek": 104108, "start": 1061.1599999999999, "end": 1068.52, "text": " huge amount of time so something people commonly do is what's called truncated back propagation", "tokens": [2603, 2372, 295, 565, 370, 746, 561, 12719, 360, 307, 437, 311, 1219, 504, 409, 66, 770, 646, 38377], "temperature": 0.0, "avg_logprob": -0.051840142770247025, "compression_ratio": 1.7252252252252251, "no_speech_prob": 7.479250780306756e-05}, {"id": 153, "seek": 106852, "start": 1068.52, "end": 1074.92, "text": " through time where you choose some constants say 20 and you say well i'm just going to run", "tokens": [807, 565, 689, 291, 2826, 512, 35870, 584, 945, 293, 291, 584, 731, 741, 478, 445, 516, 281, 1190], "temperature": 0.0, "avg_logprob": -0.13853841285183005, "compression_ratio": 1.5473684210526315, "no_speech_prob": 1.7767919416655786e-05}, {"id": 154, "seek": 106852, "start": 1074.92, "end": 1083.0, "text": " this back propagation for 20 time steps some those 20 gradients and then i'm just done that's what i'll", "tokens": [341, 646, 38377, 337, 945, 565, 4439, 512, 729, 945, 2771, 2448, 293, 550, 741, 478, 445, 1096, 300, 311, 437, 741, 603], "temperature": 0.0, "avg_logprob": -0.13853841285183005, "compression_ratio": 1.5473684210526315, "no_speech_prob": 1.7767919416655786e-05}, {"id": 155, "seek": 106852, "start": 1083.0, "end": 1094.28, "text": " update um the WH matrix with um and that works just fine okay so now given a corpus um we can train", "tokens": [5623, 1105, 264, 8183, 8141, 365, 1105, 293, 300, 1985, 445, 2489, 1392, 370, 586, 2212, 257, 1181, 31624, 1105, 321, 393, 3847], "temperature": 0.0, "avg_logprob": -0.13853841285183005, "compression_ratio": 1.5473684210526315, "no_speech_prob": 1.7767919416655786e-05}, {"id": 156, "seek": 109428, "start": 1094.28, "end": 1104.28, "text": " uh uh simple RNN and so that's good progress um but um this is a model that can also generate text", "tokens": [2232, 2232, 2199, 45702, 45, 293, 370, 300, 311, 665, 4205, 1105, 457, 1105, 341, 307, 257, 2316, 300, 393, 611, 8460, 2487], "temperature": 0.0, "avg_logprob": -0.11982833475306415, "compression_ratio": 1.6785714285714286, "no_speech_prob": 9.307615255238488e-05}, {"id": 157, "seek": 109428, "start": 1104.28, "end": 1110.84, "text": " in general so how do we generate text well just like about n-gram language model we're going to", "tokens": [294, 2674, 370, 577, 360, 321, 8460, 2487, 731, 445, 411, 466, 297, 12, 1342, 2856, 2316, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.11982833475306415, "compression_ratio": 1.6785714285714286, "no_speech_prob": 9.307615255238488e-05}, {"id": 158, "seek": 109428, "start": 1110.84, "end": 1117.72, "text": " generate text by repeated sampling so we're going to start off with an initial state um", "tokens": [8460, 2487, 538, 10477, 21179, 370, 321, 434, 516, 281, 722, 766, 365, 364, 5883, 1785, 1105], "temperature": 0.0, "avg_logprob": -0.11982833475306415, "compression_ratio": 1.6785714285714286, "no_speech_prob": 9.307615255238488e-05}, {"id": 159, "seek": 111772, "start": 1117.72, "end": 1129.72, "text": " um and um yeah this slide is imperfect um so the initial state for the hidden state um is is normally", "tokens": [1105, 293, 1105, 1338, 341, 4137, 307, 26714, 1105, 370, 264, 5883, 1785, 337, 264, 7633, 1785, 1105, 307, 307, 5646], "temperature": 0.0, "avg_logprob": -0.09006268927391539, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.9215654901927337e-05}, {"id": 160, "seek": 111772, "start": 1129.72, "end": 1136.28, "text": " just taken as a zero vector and well then we need to have something for a first input and on this", "tokens": [445, 2726, 382, 257, 4018, 8062, 293, 731, 550, 321, 643, 281, 362, 746, 337, 257, 700, 4846, 293, 322, 341], "temperature": 0.0, "avg_logprob": -0.09006268927391539, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.9215654901927337e-05}, {"id": 161, "seek": 111772, "start": 1136.28, "end": 1141.72, "text": " slide the first input is shown as the first word my and if you want to feed a starting point you", "tokens": [4137, 264, 700, 4846, 307, 4898, 382, 264, 700, 1349, 452, 293, 498, 291, 528, 281, 3154, 257, 2891, 935, 291], "temperature": 0.0, "avg_logprob": -0.09006268927391539, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.9215654901927337e-05}, {"id": 162, "seek": 111772, "start": 1141.72, "end": 1147.56, "text": " could feed my but a lot of the time you'd like to generate a sentence from nothing and if you", "tokens": [727, 3154, 452, 457, 257, 688, 295, 264, 565, 291, 1116, 411, 281, 8460, 257, 8174, 490, 1825, 293, 498, 291], "temperature": 0.0, "avg_logprob": -0.09006268927391539, "compression_ratio": 1.8055555555555556, "no_speech_prob": 1.9215654901927337e-05}, {"id": 163, "seek": 114756, "start": 1147.56, "end": 1153.56, "text": " want to do that what's conventional is to additionally have a beginning of sequence token which is a", "tokens": [528, 281, 360, 300, 437, 311, 16011, 307, 281, 43181, 362, 257, 2863, 295, 8310, 14862, 597, 307, 257], "temperature": 0.0, "avg_logprob": -0.07276205129401628, "compression_ratio": 1.8894230769230769, "no_speech_prob": 1.2016909749945626e-05}, {"id": 164, "seek": 114756, "start": 1153.56, "end": 1158.84, "text": " special token so you'll feed in the beginning of sequence token in at the beginning as the first", "tokens": [2121, 14862, 370, 291, 603, 3154, 294, 264, 2863, 295, 8310, 14862, 294, 412, 264, 2863, 382, 264, 700], "temperature": 0.0, "avg_logprob": -0.07276205129401628, "compression_ratio": 1.8894230769230769, "no_speech_prob": 1.2016909749945626e-05}, {"id": 165, "seek": 114756, "start": 1158.84, "end": 1166.9199999999998, "text": " token it has an embedding and then you use the um RNN update and then you generate using the soft", "tokens": [14862, 309, 575, 364, 12240, 3584, 293, 550, 291, 764, 264, 1105, 45702, 45, 5623, 293, 550, 291, 8460, 1228, 264, 2787], "temperature": 0.0, "avg_logprob": -0.07276205129401628, "compression_ratio": 1.8894230769230769, "no_speech_prob": 1.2016909749945626e-05}, {"id": 166, "seek": 114756, "start": 1166.9199999999998, "end": 1174.6, "text": " max and next word and um well you generate a probability probability distribution over next words", "tokens": [11469, 293, 958, 1349, 293, 1105, 731, 291, 8460, 257, 8482, 8482, 7316, 670, 958, 2283], "temperature": 0.0, "avg_logprob": -0.07276205129401628, "compression_ratio": 1.8894230769230769, "no_speech_prob": 1.2016909749945626e-05}, {"id": 167, "seek": 117460, "start": 1174.6, "end": 1181.24, "text": " and then at that point you sample from that and it chooses some word like favorite and so then", "tokens": [293, 550, 412, 300, 935, 291, 6889, 490, 300, 293, 309, 25963, 512, 1349, 411, 2954, 293, 370, 550], "temperature": 0.0, "avg_logprob": -0.07304639606685429, "compression_ratio": 1.8495145631067962, "no_speech_prob": 2.976031282742042e-05}, {"id": 168, "seek": 117460, "start": 1181.24, "end": 1187.48, "text": " the trick is for doing generation that you take this word that you sampled and you copy it back", "tokens": [264, 4282, 307, 337, 884, 5125, 300, 291, 747, 341, 1349, 300, 291, 3247, 15551, 293, 291, 5055, 309, 646], "temperature": 0.0, "avg_logprob": -0.07304639606685429, "compression_ratio": 1.8495145631067962, "no_speech_prob": 2.976031282742042e-05}, {"id": 169, "seek": 117460, "start": 1187.48, "end": 1194.12, "text": " down to the input and then you feed it as an in as an input next step if you are an N sample from", "tokens": [760, 281, 264, 4846, 293, 550, 291, 3154, 309, 382, 364, 294, 382, 364, 4846, 958, 1823, 498, 291, 366, 364, 426, 6889, 490], "temperature": 0.0, "avg_logprob": -0.07304639606685429, "compression_ratio": 1.8495145631067962, "no_speech_prob": 2.976031282742042e-05}, {"id": 170, "seek": 117460, "start": 1194.12, "end": 1200.76, "text": " the soft max get another word and just keep repeating this over and over again and you start", "tokens": [264, 2787, 11469, 483, 1071, 1349, 293, 445, 1066, 18617, 341, 670, 293, 670, 797, 293, 291, 722], "temperature": 0.0, "avg_logprob": -0.07304639606685429, "compression_ratio": 1.8495145631067962, "no_speech_prob": 2.976031282742042e-05}, {"id": 171, "seek": 120076, "start": 1200.76, "end": 1209.16, "text": " generating the text and how you end is as well as having a beginning of sequence um special symbol", "tokens": [17746, 264, 2487, 293, 577, 291, 917, 307, 382, 731, 382, 1419, 257, 2863, 295, 8310, 1105, 2121, 5986], "temperature": 0.0, "avg_logprob": -0.07333042438213642, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.532913979142904e-05}, {"id": 172, "seek": 120076, "start": 1209.16, "end": 1214.76, "text": " you usually have an end of sequence special symbol and at some point um the recurrent neural", "tokens": [291, 2673, 362, 364, 917, 295, 8310, 2121, 5986, 293, 412, 512, 935, 1105, 264, 18680, 1753, 18161], "temperature": 0.0, "avg_logprob": -0.07333042438213642, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.532913979142904e-05}, {"id": 173, "seek": 120076, "start": 1214.76, "end": 1222.2, "text": " network will generate the end of um sequence symbol and then you say okay I'm done I'm finished", "tokens": [3209, 486, 8460, 264, 917, 295, 1105, 8310, 5986, 293, 550, 291, 584, 1392, 286, 478, 1096, 286, 478, 4335], "temperature": 0.0, "avg_logprob": -0.07333042438213642, "compression_ratio": 1.7083333333333333, "no_speech_prob": 3.532913979142904e-05}, {"id": 174, "seek": 122220, "start": 1222.2, "end": 1231.0800000000002, "text": " generating text um so before going on for the um more of the um difficult content of the lecture", "tokens": [17746, 2487, 1105, 370, 949, 516, 322, 337, 264, 1105, 544, 295, 264, 1105, 2252, 2701, 295, 264, 7991], "temperature": 0.0, "avg_logprob": -0.03534011311001248, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.2210276508994866e-05}, {"id": 175, "seek": 122220, "start": 1231.0800000000002, "end": 1237.48, "text": " we can just have a little bit of fun with this and try um training up and generating text with", "tokens": [321, 393, 445, 362, 257, 707, 857, 295, 1019, 365, 341, 293, 853, 1105, 3097, 493, 293, 17746, 2487, 365], "temperature": 0.0, "avg_logprob": -0.03534011311001248, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.2210276508994866e-05}, {"id": 176, "seek": 122220, "start": 1237.48, "end": 1243.8, "text": " a recurrent neural network model so you can generate you can train an RNN on any kind of text", "tokens": [257, 18680, 1753, 18161, 3209, 2316, 370, 291, 393, 8460, 291, 393, 3847, 364, 45702, 45, 322, 604, 733, 295, 2487], "temperature": 0.0, "avg_logprob": -0.03534011311001248, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.2210276508994866e-05}, {"id": 177, "seek": 122220, "start": 1243.8, "end": 1250.04, "text": " and so that means one of the fun things that you can do is generate text um in different styles", "tokens": [293, 370, 300, 1355, 472, 295, 264, 1019, 721, 300, 291, 393, 360, 307, 8460, 2487, 1105, 294, 819, 13273], "temperature": 0.0, "avg_logprob": -0.03534011311001248, "compression_ratio": 1.8317307692307692, "no_speech_prob": 1.2210276508994866e-05}, {"id": 178, "seek": 125004, "start": 1250.04, "end": 1258.84, "text": " based on what you could train it from um so here um Harry Potter as a there it's a fair amount", "tokens": [2361, 322, 437, 291, 727, 3847, 309, 490, 1105, 370, 510, 1105, 9378, 18115, 382, 257, 456, 309, 311, 257, 3143, 2372], "temperature": 0.0, "avg_logprob": -0.18765598965674332, "compression_ratio": 1.6710526315789473, "no_speech_prob": 6.341310654534027e-05}, {"id": 179, "seek": 125004, "start": 1258.84, "end": 1265.0, "text": " of a corpus of text so you can train our NNLM on the Harry Potter box and then say go off and", "tokens": [295, 257, 1181, 31624, 295, 2487, 370, 291, 393, 3847, 527, 426, 45, 43, 44, 322, 264, 9378, 18115, 2424, 293, 550, 584, 352, 766, 293], "temperature": 0.0, "avg_logprob": -0.18765598965674332, "compression_ratio": 1.6710526315789473, "no_speech_prob": 6.341310654534027e-05}, {"id": 180, "seek": 125004, "start": 1265.0, "end": 1271.48, "text": " generate some text and it'll generate text like this sorry how Harry shouted panicking I'll leave", "tokens": [8460, 512, 2487, 293, 309, 603, 8460, 2487, 411, 341, 2597, 577, 9378, 37310, 2462, 10401, 286, 603, 1856], "temperature": 0.0, "avg_logprob": -0.18765598965674332, "compression_ratio": 1.6710526315789473, "no_speech_prob": 6.341310654534027e-05}, {"id": 181, "seek": 125004, "start": 1271.48, "end": 1277.8, "text": " those brooms in London are they no idea said nearly headless Nick casting low close by sadrick", "tokens": [729, 2006, 4785, 294, 7042, 366, 436, 572, 1558, 848, 6217, 1378, 1832, 9449, 17301, 2295, 1998, 538, 4227, 9323], "temperature": 0.0, "avg_logprob": -0.18765598965674332, "compression_ratio": 1.6710526315789473, "no_speech_prob": 6.341310654534027e-05}, {"id": 182, "seek": 127780, "start": 1277.8, "end": 1282.68, "text": " carrying the last bit of treacle charms from Harry's shoulder and to answer him the common room", "tokens": [9792, 264, 1036, 857, 295, 2192, 7041, 41383, 490, 9378, 311, 7948, 293, 281, 1867, 796, 264, 2689, 1808], "temperature": 0.0, "avg_logprob": -0.10898106892903646, "compression_ratio": 1.6885964912280702, "no_speech_prob": 3.0674196750624105e-05}, {"id": 183, "seek": 127780, "start": 1282.68, "end": 1289.0, "text": " pushed upon it four arms held a shining knob from when the spider hadn't felt it seemed he reached", "tokens": [9152, 3564, 309, 1451, 5812, 5167, 257, 18269, 26759, 490, 562, 264, 17614, 8782, 380, 2762, 309, 6576, 415, 6488], "temperature": 0.0, "avg_logprob": -0.10898106892903646, "compression_ratio": 1.6885964912280702, "no_speech_prob": 3.0674196750624105e-05}, {"id": 184, "seek": 127780, "start": 1289.0, "end": 1297.0, "text": " the teams too um well so on the one hand that's still kind of a bit in coherent as a story", "tokens": [264, 5491, 886, 1105, 731, 370, 322, 264, 472, 1011, 300, 311, 920, 733, 295, 257, 857, 294, 36239, 382, 257, 1657], "temperature": 0.0, "avg_logprob": -0.10898106892903646, "compression_ratio": 1.6885964912280702, "no_speech_prob": 3.0674196750624105e-05}, {"id": 185, "seek": 127780, "start": 1297.0, "end": 1302.76, "text": " on the other hand it sort of sounds like Harry Potter and certainly the kind of you know vocabulary", "tokens": [322, 264, 661, 1011, 309, 1333, 295, 3263, 411, 9378, 18115, 293, 3297, 264, 733, 295, 291, 458, 19864], "temperature": 0.0, "avg_logprob": -0.10898106892903646, "compression_ratio": 1.6885964912280702, "no_speech_prob": 3.0674196750624105e-05}, {"id": 186, "seek": 130276, "start": 1302.76, "end": 1309.32, "text": " and constructions it uses and like I think you'd agree that you know even though it gets sort of", "tokens": [293, 7690, 626, 309, 4960, 293, 411, 286, 519, 291, 1116, 3986, 300, 291, 458, 754, 1673, 309, 2170, 1333, 295], "temperature": 0.0, "avg_logprob": -0.10123200217882793, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.184284262009896e-05}, {"id": 187, "seek": 130276, "start": 1309.32, "end": 1316.12, "text": " incoherent it's sort of more coherent than what we got from an in-gram language model when I", "tokens": [834, 78, 511, 317, 309, 311, 1333, 295, 544, 36239, 813, 437, 321, 658, 490, 364, 294, 12, 1342, 2856, 2316, 562, 286], "temperature": 0.0, "avg_logprob": -0.10123200217882793, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.184284262009896e-05}, {"id": 188, "seek": 130276, "start": 1316.12, "end": 1324.84, "text": " showed a generation in the last um lecture um you can choose a very different style of text um so", "tokens": [4712, 257, 5125, 294, 264, 1036, 1105, 7991, 1105, 291, 393, 2826, 257, 588, 819, 3758, 295, 2487, 1105, 370], "temperature": 0.0, "avg_logprob": -0.10123200217882793, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.184284262009896e-05}, {"id": 189, "seek": 130276, "start": 1324.84, "end": 1331.96, "text": " you could instead train the model on a bunch of cookbooks um and if you do that you can then say", "tokens": [291, 727, 2602, 3847, 264, 2316, 322, 257, 3840, 295, 2543, 15170, 1105, 293, 498, 291, 360, 300, 291, 393, 550, 584], "temperature": 0.0, "avg_logprob": -0.10123200217882793, "compression_ratio": 1.6916299559471366, "no_speech_prob": 4.184284262009896e-05}, {"id": 190, "seek": 133196, "start": 1331.96, "end": 1337.96, "text": " generate um based on what you've learned about cookbooks um and it'll just generate a recipe so", "tokens": [8460, 1105, 2361, 322, 437, 291, 600, 3264, 466, 2543, 15170, 1105, 293, 309, 603, 445, 8460, 257, 6782, 370], "temperature": 0.0, "avg_logprob": -0.14926591515541077, "compression_ratio": 1.572972972972973, "no_speech_prob": 9.429550118511543e-05}, {"id": 191, "seek": 133196, "start": 1337.96, "end": 1347.08, "text": " here's a recipe um chocolate ranch barbecue um categories yield six servings two tablespoons of", "tokens": [510, 311, 257, 6782, 1105, 6215, 22883, 21877, 1105, 10479, 11257, 2309, 8148, 82, 732, 21615, 295], "temperature": 0.0, "avg_logprob": -0.14926591515541077, "compression_ratio": 1.572972972972973, "no_speech_prob": 9.429550118511543e-05}, {"id": 192, "seek": 133196, "start": 1347.08, "end": 1354.92, "text": " parmesan cheese chopped a one cup of coconut milk three eggs beaten place each pasta over layers of", "tokens": [971, 43826, 5399, 16497, 257, 472, 4414, 295, 13551, 5392, 1045, 6466, 17909, 1081, 1184, 13296, 670, 7914, 295], "temperature": 0.0, "avg_logprob": -0.14926591515541077, "compression_ratio": 1.572972972972973, "no_speech_prob": 9.429550118511543e-05}, {"id": 193, "seek": 135492, "start": 1354.92, "end": 1362.52, "text": " lumps shaped mixture into the moderate oven and simmer until firm serve hot and bodied fresh mustard", "tokens": [44948, 13475, 9925, 666, 264, 18174, 9090, 293, 29835, 1826, 6174, 4596, 2368, 293, 16737, 1091, 4451, 23659], "temperature": 0.0, "avg_logprob": -0.07420154593207619, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.310375202796422e-05}, {"id": 194, "seek": 135492, "start": 1362.52, "end": 1369.8000000000002, "text": " orange and cheese combine the cheese and salt together the dough in a large skillet add the ingredients", "tokens": [7671, 293, 5399, 10432, 264, 5399, 293, 5139, 1214, 264, 7984, 294, 257, 2416, 5389, 302, 909, 264, 6952], "temperature": 0.0, "avg_logprob": -0.07420154593207619, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.310375202796422e-05}, {"id": 195, "seek": 135492, "start": 1369.8000000000002, "end": 1376.92, "text": " and stir in the chocolate and pepper so you know um this recipe makes um no sense and it's", "tokens": [293, 8946, 294, 264, 6215, 293, 8532, 370, 291, 458, 1105, 341, 6782, 1669, 1105, 572, 2020, 293, 309, 311], "temperature": 0.0, "avg_logprob": -0.07420154593207619, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.310375202796422e-05}, {"id": 196, "seek": 135492, "start": 1376.92, "end": 1382.8400000000001, "text": " sufficiently um incoherent there's actually even no danger that you'll try cooking this at home um", "tokens": [31868, 1105, 834, 78, 511, 317, 456, 311, 767, 754, 572, 4330, 300, 291, 603, 853, 6361, 341, 412, 1280, 1105], "temperature": 0.0, "avg_logprob": -0.07420154593207619, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.310375202796422e-05}, {"id": 197, "seek": 138284, "start": 1382.84, "end": 1387.8, "text": " but you know something that's interesting is although you know this really just", "tokens": [457, 291, 458, 746, 300, 311, 1880, 307, 4878, 291, 458, 341, 534, 445], "temperature": 0.0, "avg_logprob": -0.08434361219406128, "compression_ratio": 1.9109947643979057, "no_speech_prob": 5.720903936889954e-05}, {"id": 198, "seek": 138284, "start": 1388.6, "end": 1396.12, "text": " isn't a recipe and the things that are done in the instructions have no relation um to the", "tokens": [1943, 380, 257, 6782, 293, 264, 721, 300, 366, 1096, 294, 264, 9415, 362, 572, 9721, 1105, 281, 264], "temperature": 0.0, "avg_logprob": -0.08434361219406128, "compression_ratio": 1.9109947643979057, "no_speech_prob": 5.720903936889954e-05}, {"id": 199, "seek": 138284, "start": 1396.12, "end": 1402.28, "text": " ingredients that the thing that's interesting that it has learned as this recurrent your network", "tokens": [6952, 300, 264, 551, 300, 311, 1880, 300, 309, 575, 3264, 382, 341, 18680, 1753, 428, 3209], "temperature": 0.0, "avg_logprob": -0.08434361219406128, "compression_ratio": 1.9109947643979057, "no_speech_prob": 5.720903936889954e-05}, {"id": 200, "seek": 138284, "start": 1402.28, "end": 1408.76, "text": " model is that it's really mastered the overall structure of a recipe it knows that a recipe has a", "tokens": [2316, 307, 300, 309, 311, 534, 38686, 264, 4787, 3877, 295, 257, 6782, 309, 3255, 300, 257, 6782, 575, 257], "temperature": 0.0, "avg_logprob": -0.08434361219406128, "compression_ratio": 1.9109947643979057, "no_speech_prob": 5.720903936889954e-05}, {"id": 201, "seek": 140876, "start": 1408.76, "end": 1415.32, "text": " title it often tells you about how many people it serves at list the ingredients and then it has", "tokens": [4876, 309, 2049, 5112, 291, 466, 577, 867, 561, 309, 13451, 412, 1329, 264, 6952, 293, 550, 309, 575], "temperature": 0.0, "avg_logprob": -0.14359124501546225, "compression_ratio": 1.6830357142857142, "no_speech_prob": 6.596465391339734e-05}, {"id": 202, "seek": 140876, "start": 1415.32, "end": 1421.56, "text": " instructions um to make it so that's sort of fairly impressive in some sense the high level tech", "tokens": [9415, 1105, 281, 652, 309, 370, 300, 311, 1333, 295, 6457, 8992, 294, 512, 2020, 264, 1090, 1496, 7553], "temperature": 0.0, "avg_logprob": -0.14359124501546225, "compression_ratio": 1.6830357142857142, "no_speech_prob": 6.596465391339734e-05}, {"id": 203, "seek": 140876, "start": 1421.56, "end": 1430.04, "text": " structuring um so the one other thing I wanted to mention was um when I say you can train and", "tokens": [6594, 1345, 1105, 370, 264, 472, 661, 551, 286, 1415, 281, 2152, 390, 1105, 562, 286, 584, 291, 393, 3847, 293], "temperature": 0.0, "avg_logprob": -0.14359124501546225, "compression_ratio": 1.6830357142857142, "no_speech_prob": 6.596465391339734e-05}, {"id": 204, "seek": 140876, "start": 1430.04, "end": 1434.52, "text": " our an end language model and any kind of text the other difference from where we were in", "tokens": [527, 364, 917, 2856, 2316, 293, 604, 733, 295, 2487, 264, 661, 2649, 490, 689, 321, 645, 294], "temperature": 0.0, "avg_logprob": -0.14359124501546225, "compression_ratio": 1.6830357142857142, "no_speech_prob": 6.596465391339734e-05}, {"id": 205, "seek": 143452, "start": 1434.52, "end": 1440.2, "text": " in-gram language models was on in-gram language models that just meant counting in grams and meant", "tokens": [294, 12, 1342, 2856, 5245, 390, 322, 294, 12, 1342, 2856, 5245, 300, 445, 4140, 13251, 294, 11899, 293, 4140], "temperature": 0.0, "avg_logprob": -0.12917732924557804, "compression_ratio": 1.6929824561403508, "no_speech_prob": 5.9084461099701e-05}, {"id": 206, "seek": 143452, "start": 1440.2, "end": 1447.08, "text": " it took um two minutes or even on a large corpus with any modern computer training your iron and", "tokens": [309, 1890, 1105, 732, 2077, 420, 754, 322, 257, 2416, 1181, 31624, 365, 604, 4363, 3820, 3097, 428, 6497, 293], "temperature": 0.0, "avg_logprob": -0.12917732924557804, "compression_ratio": 1.6929824561403508, "no_speech_prob": 5.9084461099701e-05}, {"id": 207, "seek": 143452, "start": 1447.08, "end": 1452.84, "text": " L.M. actually can then be a time intensive activity and you can spend hours doing that as you", "tokens": [441, 13, 44, 13, 767, 393, 550, 312, 257, 565, 18957, 5191, 293, 291, 393, 3496, 2496, 884, 300, 382, 291], "temperature": 0.0, "avg_logprob": -0.12917732924557804, "compression_ratio": 1.6929824561403508, "no_speech_prob": 5.9084461099701e-05}, {"id": 208, "seek": 143452, "start": 1452.84, "end": 1461.16, "text": " might find next week when you're training um machine translation models okay um how do we decide", "tokens": [1062, 915, 958, 1243, 562, 291, 434, 3097, 1105, 3479, 12853, 5245, 1392, 1105, 577, 360, 321, 4536], "temperature": 0.0, "avg_logprob": -0.12917732924557804, "compression_ratio": 1.6929824561403508, "no_speech_prob": 5.9084461099701e-05}, {"id": 209, "seek": 146116, "start": 1461.16, "end": 1471.3200000000002, "text": " if our models are good or not um so the standard evaluation metric for language models is what's", "tokens": [498, 527, 5245, 366, 665, 420, 406, 1105, 370, 264, 3832, 13344, 20678, 337, 2856, 5245, 307, 437, 311], "temperature": 0.0, "avg_logprob": -0.0670225970885333, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2999402315472253e-05}, {"id": 210, "seek": 146116, "start": 1471.3200000000002, "end": 1481.72, "text": " called perplexity um and what perplexity is is um kind of like when you were um training your", "tokens": [1219, 680, 18945, 507, 1105, 293, 437, 680, 18945, 507, 307, 307, 1105, 733, 295, 411, 562, 291, 645, 1105, 3097, 428], "temperature": 0.0, "avg_logprob": -0.0670225970885333, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2999402315472253e-05}, {"id": 211, "seek": 146116, "start": 1481.72, "end": 1489.48, "text": " model you use teacher forcing over a piece of text that's a different piece of test text which", "tokens": [2316, 291, 764, 5027, 19030, 670, 257, 2522, 295, 2487, 300, 311, 257, 819, 2522, 295, 1500, 2487, 597], "temperature": 0.0, "avg_logprob": -0.0670225970885333, "compression_ratio": 1.6666666666666667, "no_speech_prob": 1.2999402315472253e-05}, {"id": 212, "seek": 148948, "start": 1489.48, "end": 1497.72, "text": " isn't text that was in the training data and you say well given a sequence of t words um what", "tokens": [1943, 380, 2487, 300, 390, 294, 264, 3097, 1412, 293, 291, 584, 731, 2212, 257, 8310, 295, 256, 2283, 1105, 437], "temperature": 0.0, "avg_logprob": -0.08296421595982142, "compression_ratio": 1.6941176470588235, "no_speech_prob": 3.166075475746766e-05}, {"id": 213, "seek": 148948, "start": 1497.72, "end": 1505.0, "text": " probability do you give to the actual t plus oneth word and you repeat that at each position", "tokens": [8482, 360, 291, 976, 281, 264, 3539, 256, 1804, 472, 392, 1349, 293, 291, 7149, 300, 412, 1184, 2535], "temperature": 0.0, "avg_logprob": -0.08296421595982142, "compression_ratio": 1.6941176470588235, "no_speech_prob": 3.166075475746766e-05}, {"id": 214, "seek": 148948, "start": 1505.0, "end": 1511.96, "text": " and then you take the inverse of that probability and raise it to the one on t for the length of your", "tokens": [293, 550, 291, 747, 264, 17340, 295, 300, 8482, 293, 5300, 309, 281, 264, 472, 322, 256, 337, 264, 4641, 295, 428], "temperature": 0.0, "avg_logprob": -0.08296421595982142, "compression_ratio": 1.6941176470588235, "no_speech_prob": 3.166075475746766e-05}, {"id": 215, "seek": 151196, "start": 1511.96, "end": 1521.48, "text": " test text sample and that number is the perplexity so it's a geometric mean of the inverse probabilities", "tokens": [1500, 2487, 6889, 293, 300, 1230, 307, 264, 680, 18945, 507, 370, 309, 311, 257, 33246, 914, 295, 264, 17340, 33783], "temperature": 0.0, "avg_logprob": -0.16543282689274968, "compression_ratio": 1.5904255319148937, "no_speech_prob": 6.8492608988890424e-06}, {"id": 216, "seek": 151196, "start": 1522.2, "end": 1530.6000000000001, "text": " now um after that explanation perhaps an easier way to think of it is that the perplexity um is", "tokens": [586, 1105, 934, 300, 10835, 4317, 364, 3571, 636, 281, 519, 295, 309, 307, 300, 264, 680, 18945, 507, 1105, 307], "temperature": 0.0, "avg_logprob": -0.16543282689274968, "compression_ratio": 1.5904255319148937, "no_speech_prob": 6.8492608988890424e-06}, {"id": 217, "seek": 151196, "start": 1530.6000000000001, "end": 1540.76, "text": " simply um the cross-entropy loss that I introduced before expenentiated um so um but you know it's", "tokens": [2935, 1105, 264, 3278, 12, 317, 27514, 4470, 300, 286, 7268, 949, 1278, 268, 23012, 770, 1105, 370, 1105, 457, 291, 458, 309, 311], "temperature": 0.0, "avg_logprob": -0.16543282689274968, "compression_ratio": 1.5904255319148937, "no_speech_prob": 6.8492608988890424e-06}, {"id": 218, "seek": 154076, "start": 1540.76, "end": 1547.32, "text": " now the other way around so low perplexity um is better so there's actually an interesting", "tokens": [586, 264, 661, 636, 926, 370, 2295, 680, 18945, 507, 1105, 307, 1101, 370, 456, 311, 767, 364, 1880], "temperature": 0.0, "avg_logprob": -0.10636677060808454, "compression_ratio": 1.6592920353982301, "no_speech_prob": 2.79254909401061e-05}, {"id": 219, "seek": 154076, "start": 1547.32, "end": 1554.44, "text": " story about these perplexities um so a famous figure in the development of um probabilistic", "tokens": [1657, 466, 613, 680, 18945, 1088, 1105, 370, 257, 4618, 2573, 294, 264, 3250, 295, 1105, 31959, 3142], "temperature": 0.0, "avg_logprob": -0.10636677060808454, "compression_ratio": 1.6592920353982301, "no_speech_prob": 2.79254909401061e-05}, {"id": 220, "seek": 154076, "start": 1554.44, "end": 1560.2, "text": " and machine learning approaches to natural language processing is Fred Jeleneck who died a few years", "tokens": [293, 3479, 2539, 11587, 281, 3303, 2856, 9007, 307, 10112, 508, 338, 1450, 547, 567, 4539, 257, 1326, 924], "temperature": 0.0, "avg_logprob": -0.10636677060808454, "compression_ratio": 1.6592920353982301, "no_speech_prob": 2.79254909401061e-05}, {"id": 221, "seek": 154076, "start": 1560.2, "end": 1570.04, "text": " ago um and he was trying to um interest people and the idea of using probability models and", "tokens": [2057, 1105, 293, 415, 390, 1382, 281, 1105, 1179, 561, 293, 264, 1558, 295, 1228, 8482, 5245, 293], "temperature": 0.0, "avg_logprob": -0.10636677060808454, "compression_ratio": 1.6592920353982301, "no_speech_prob": 2.79254909401061e-05}, {"id": 222, "seek": 157004, "start": 1570.04, "end": 1577.6399999999999, "text": " machine learning um for natural language processing at a time i this is the 1970s and early 1980s", "tokens": [3479, 2539, 1105, 337, 3303, 2856, 9007, 412, 257, 565, 741, 341, 307, 264, 14577, 82, 293, 2440, 13626, 82], "temperature": 0.0, "avg_logprob": -0.12330017238855362, "compression_ratio": 1.4791666666666667, "no_speech_prob": 2.1427318642963655e-05}, {"id": 223, "seek": 157004, "start": 1578.52, "end": 1586.36, "text": " when nearly everyone in the field of AI um was still in the thrall of logic-based models and", "tokens": [562, 6217, 1518, 294, 264, 2519, 295, 7318, 1105, 390, 920, 294, 264, 739, 336, 295, 9952, 12, 6032, 5245, 293], "temperature": 0.0, "avg_logprob": -0.12330017238855362, "compression_ratio": 1.4791666666666667, "no_speech_prob": 2.1427318642963655e-05}, {"id": 224, "seek": 157004, "start": 1586.36, "end": 1592.52, "text": " blackboard architectures and things like that for artificial intelligence systems and so Fred", "tokens": [2211, 3787, 6331, 1303, 293, 721, 411, 300, 337, 11677, 7599, 3652, 293, 370, 10112], "temperature": 0.0, "avg_logprob": -0.12330017238855362, "compression_ratio": 1.4791666666666667, "no_speech_prob": 2.1427318642963655e-05}, {"id": 225, "seek": 159252, "start": 1592.52, "end": 1601.0, "text": " Jeleneck was actually an information theorist by background um and who um then got interest in", "tokens": [508, 338, 1450, 547, 390, 767, 364, 1589, 27423, 468, 538, 3678, 1105, 293, 567, 1105, 550, 658, 1179, 294], "temperature": 0.0, "avg_logprob": -0.09189126779744913, "compression_ratio": 1.6059322033898304, "no_speech_prob": 1.8622689822223037e-05}, {"id": 226, "seek": 159252, "start": 1601.0, "end": 1609.08, "text": " working with speech and then language data um so at that time the stuff that's this sort of um", "tokens": [1364, 365, 6218, 293, 550, 2856, 1412, 1105, 370, 412, 300, 565, 264, 1507, 300, 311, 341, 1333, 295, 1105], "temperature": 0.0, "avg_logprob": -0.09189126779744913, "compression_ratio": 1.6059322033898304, "no_speech_prob": 1.8622689822223037e-05}, {"id": 227, "seek": 159252, "start": 1609.08, "end": 1615.6399999999999, "text": " exponential or using cross-entropy losses was completely bread and butter um to Fred Jeleneck", "tokens": [21510, 420, 1228, 3278, 12, 317, 27514, 15352, 390, 2584, 5961, 293, 5517, 1105, 281, 10112, 508, 338, 1450, 547], "temperature": 0.0, "avg_logprob": -0.09189126779744913, "compression_ratio": 1.6059322033898304, "no_speech_prob": 1.8622689822223037e-05}, {"id": 228, "seek": 159252, "start": 1615.6399999999999, "end": 1622.44, "text": " but he'd found that no one in AI could understand the bottom half of the slide and so he wanted", "tokens": [457, 415, 1116, 1352, 300, 572, 472, 294, 7318, 727, 1223, 264, 2767, 1922, 295, 264, 4137, 293, 370, 415, 1415], "temperature": 0.0, "avg_logprob": -0.09189126779744913, "compression_ratio": 1.6059322033898304, "no_speech_prob": 1.8622689822223037e-05}, {"id": 229, "seek": 162244, "start": 1622.44, "end": 1628.44, "text": " to come up with something simple that AI people at that time could understand and perplexity", "tokens": [281, 808, 493, 365, 746, 2199, 300, 7318, 561, 412, 300, 565, 727, 1223, 293, 680, 18945, 507], "temperature": 0.0, "avg_logprob": -0.05113265330974872, "compression_ratio": 1.6104651162790697, "no_speech_prob": 9.071128260984551e-06}, {"id": 230, "seek": 162244, "start": 1628.44, "end": 1638.1200000000001, "text": " has a kind of a simple interpretation you can tell people so if you get a perplexity of 53", "tokens": [575, 257, 733, 295, 257, 2199, 14174, 291, 393, 980, 561, 370, 498, 291, 483, 257, 680, 18945, 507, 295, 21860], "temperature": 0.0, "avg_logprob": -0.05113265330974872, "compression_ratio": 1.6104651162790697, "no_speech_prob": 9.071128260984551e-06}, {"id": 231, "seek": 162244, "start": 1638.1200000000001, "end": 1645.4, "text": " that means how uncertain you are um of the next word is equivalent to the uncertainty of that", "tokens": [300, 1355, 577, 11308, 291, 366, 1105, 295, 264, 958, 1349, 307, 10344, 281, 264, 15697, 295, 300], "temperature": 0.0, "avg_logprob": -0.05113265330974872, "compression_ratio": 1.6104651162790697, "no_speech_prob": 9.071128260984551e-06}, {"id": 232, "seek": 164540, "start": 1645.4, "end": 1654.52, "text": " you're tossing a 53 sided dice and it coming up as a one right so um that was kind of an easy", "tokens": [291, 434, 14432, 278, 257, 21860, 41651, 10313, 293, 309, 1348, 493, 382, 257, 472, 558, 370, 1105, 300, 390, 733, 295, 364, 1858], "temperature": 0.0, "avg_logprob": -0.06736328981924748, "compression_ratio": 1.554945054945055, "no_speech_prob": 2.9700424420298077e-05}, {"id": 233, "seek": 164540, "start": 1654.52, "end": 1663.72, "text": " simple metric and so he introduced um that idea um but you know i guess things stick and to this day", "tokens": [2199, 20678, 293, 370, 415, 7268, 1105, 300, 1558, 1105, 457, 291, 458, 741, 2041, 721, 2897, 293, 281, 341, 786], "temperature": 0.0, "avg_logprob": -0.06736328981924748, "compression_ratio": 1.554945054945055, "no_speech_prob": 2.9700424420298077e-05}, {"id": 234, "seek": 164540, "start": 1664.76, "end": 1669.8000000000002, "text": " everyone evaluates their language models by providing perplexity numbers and so here are", "tokens": [1518, 6133, 1024, 641, 2856, 5245, 538, 6530, 680, 18945, 507, 3547, 293, 370, 510, 366], "temperature": 0.0, "avg_logprob": -0.06736328981924748, "compression_ratio": 1.554945054945055, "no_speech_prob": 2.9700424420298077e-05}, {"id": 235, "seek": 166980, "start": 1669.8, "end": 1678.12, "text": " some perplexity numbers um so traditional n-gram language models commonly had perplexities over 100", "tokens": [512, 680, 18945, 507, 3547, 1105, 370, 5164, 297, 12, 1342, 2856, 5245, 12719, 632, 680, 18945, 1088, 670, 2319], "temperature": 0.0, "avg_logprob": -0.10166471083085615, "compression_ratio": 1.5856573705179282, "no_speech_prob": 3.752786142285913e-05}, {"id": 236, "seek": 166980, "start": 1678.12, "end": 1684.2, "text": " but if you made them really big and really careful you carefully you could get them down into a number", "tokens": [457, 498, 291, 1027, 552, 534, 955, 293, 534, 5026, 291, 7500, 291, 727, 483, 552, 760, 666, 257, 1230], "temperature": 0.0, "avg_logprob": -0.10166471083085615, "compression_ratio": 1.5856573705179282, "no_speech_prob": 3.752786142285913e-05}, {"id": 237, "seek": 166980, "start": 1684.2, "end": 1692.84, "text": " like 67 as people started to build more advanced recurrent neural networks especially as they move", "tokens": [411, 23879, 382, 561, 1409, 281, 1322, 544, 7339, 18680, 1753, 18161, 9590, 2318, 382, 436, 1286], "temperature": 0.0, "avg_logprob": -0.10166471083085615, "compression_ratio": 1.5856573705179282, "no_speech_prob": 3.752786142285913e-05}, {"id": 238, "seek": 166980, "start": 1692.84, "end": 1698.6, "text": " beyond the kind of simple RNNs which has all I've shown you so far which one of is in the second", "tokens": [4399, 264, 733, 295, 2199, 45702, 45, 82, 597, 575, 439, 286, 600, 4898, 291, 370, 1400, 597, 472, 295, 307, 294, 264, 1150], "temperature": 0.0, "avg_logprob": -0.10166471083085615, "compression_ratio": 1.5856573705179282, "no_speech_prob": 3.752786142285913e-05}, {"id": 239, "seek": 169860, "start": 1698.6, "end": 1707.6399999999999, "text": " line of the slide into LSTMs which I talk about later in this course that people started producing", "tokens": [1622, 295, 264, 4137, 666, 441, 6840, 26386, 597, 286, 751, 466, 1780, 294, 341, 1164, 300, 561, 1409, 10501], "temperature": 0.0, "avg_logprob": -0.10915184020996094, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.313655153964646e-05}, {"id": 240, "seek": 169860, "start": 1708.52, "end": 1715.8, "text": " much better perplexities and here we're getting perplexities down to 30 and this is results actually", "tokens": [709, 1101, 680, 18945, 1088, 293, 510, 321, 434, 1242, 680, 18945, 1088, 760, 281, 2217, 293, 341, 307, 3542, 767], "temperature": 0.0, "avg_logprob": -0.10915184020996094, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.313655153964646e-05}, {"id": 241, "seek": 169860, "start": 1715.8, "end": 1723.3999999999999, "text": " from a few years ago so nowadays people get perplexities even lower than 30 you have to be realistic", "tokens": [490, 257, 1326, 924, 2057, 370, 13434, 561, 483, 680, 18945, 1088, 754, 3126, 813, 2217, 291, 362, 281, 312, 12465], "temperature": 0.0, "avg_logprob": -0.10915184020996094, "compression_ratio": 1.5384615384615385, "no_speech_prob": 4.313655153964646e-05}, {"id": 242, "seek": 172340, "start": 1723.4, "end": 1729.4, "text": " and what you can expect right because if you're just generating a text some words are almost", "tokens": [293, 437, 291, 393, 2066, 558, 570, 498, 291, 434, 445, 17746, 257, 2487, 512, 2283, 366, 1920], "temperature": 0.0, "avg_logprob": -0.09653545707784673, "compression_ratio": 1.75, "no_speech_prob": 8.724821964278817e-05}, {"id": 243, "seek": 172340, "start": 1729.4, "end": 1739.0800000000002, "text": " determined um so you know if it's something like um you know sum gave the man a napkin he said thank", "tokens": [9540, 1105, 370, 291, 458, 498, 309, 311, 746, 411, 1105, 291, 458, 2408, 2729, 264, 587, 257, 9296, 5843, 415, 848, 1309], "temperature": 0.0, "avg_logprob": -0.09653545707784673, "compression_ratio": 1.75, "no_speech_prob": 8.724821964278817e-05}, {"id": 244, "seek": 172340, "start": 1739.64, "end": 1745.24, "text": " you know basically 100 percent you should be able to say the word that comes next is you um and", "tokens": [291, 458, 1936, 2319, 3043, 291, 820, 312, 1075, 281, 584, 264, 1349, 300, 1487, 958, 307, 291, 1105, 293], "temperature": 0.0, "avg_logprob": -0.09653545707784673, "compression_ratio": 1.75, "no_speech_prob": 8.724821964278817e-05}, {"id": 245, "seek": 172340, "start": 1745.24, "end": 1751.96, "text": " so that you can predict really well but um you know if it's a lot of other sentences like um he", "tokens": [370, 300, 291, 393, 6069, 534, 731, 457, 1105, 291, 458, 498, 309, 311, 257, 688, 295, 661, 16579, 411, 1105, 415], "temperature": 0.0, "avg_logprob": -0.09653545707784673, "compression_ratio": 1.75, "no_speech_prob": 8.724821964278817e-05}, {"id": 246, "seek": 175196, "start": 1751.96, "end": 1759.08, "text": " looked out the window and saw uh something right no probability in the model or model in the world", "tokens": [2956, 484, 264, 4910, 293, 1866, 2232, 746, 558, 572, 8482, 294, 264, 2316, 420, 2316, 294, 264, 1002], "temperature": 0.0, "avg_logprob": -0.07738873693678114, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.867250052280724e-05}, {"id": 247, "seek": 175196, "start": 1759.08, "end": 1763.88, "text": " can give a very good estimate of what's actually going to be coming next to that point and so that", "tokens": [393, 976, 257, 588, 665, 12539, 295, 437, 311, 767, 516, 281, 312, 1348, 958, 281, 300, 935, 293, 370, 300], "temperature": 0.0, "avg_logprob": -0.07738873693678114, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.867250052280724e-05}, {"id": 248, "seek": 175196, "start": 1763.88, "end": 1770.1200000000001, "text": " gives us the sort of residual um uncertainty that leads to perplexities that on average might be around", "tokens": [2709, 505, 264, 1333, 295, 27980, 1105, 15697, 300, 6689, 281, 680, 18945, 1088, 300, 322, 4274, 1062, 312, 926], "temperature": 0.0, "avg_logprob": -0.07738873693678114, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.867250052280724e-05}, {"id": 249, "seek": 175196, "start": 1770.1200000000001, "end": 1780.44, "text": " 20 or something okay um so we've talked a lot about language models now why should we care about", "tokens": [945, 420, 746, 1392, 1105, 370, 321, 600, 2825, 257, 688, 466, 2856, 5245, 586, 983, 820, 321, 1127, 466], "temperature": 0.0, "avg_logprob": -0.07738873693678114, "compression_ratio": 1.6864406779661016, "no_speech_prob": 3.867250052280724e-05}, {"id": 250, "seek": 178044, "start": 1780.44, "end": 1787.3200000000002, "text": " language modeling you know well there's sort of an intellectual scientific answer that says", "tokens": [2856, 15983, 291, 458, 731, 456, 311, 1333, 295, 364, 12576, 8134, 1867, 300, 1619], "temperature": 0.0, "avg_logprob": -0.044985620281364345, "compression_ratio": 1.766355140186916, "no_speech_prob": 5.810514267068356e-05}, {"id": 251, "seek": 178044, "start": 1787.3200000000002, "end": 1793.4, "text": " this is a benchmark task right if we what we want to do is build machine learning models of", "tokens": [341, 307, 257, 18927, 5633, 558, 498, 321, 437, 321, 528, 281, 360, 307, 1322, 3479, 2539, 5245, 295], "temperature": 0.0, "avg_logprob": -0.044985620281364345, "compression_ratio": 1.766355140186916, "no_speech_prob": 5.810514267068356e-05}, {"id": 252, "seek": 178044, "start": 1793.4, "end": 1799.64, "text": " language and our ability to predict what word will come next in the context that shows how well", "tokens": [2856, 293, 527, 3485, 281, 6069, 437, 1349, 486, 808, 958, 294, 264, 4319, 300, 3110, 577, 731], "temperature": 0.0, "avg_logprob": -0.044985620281364345, "compression_ratio": 1.766355140186916, "no_speech_prob": 5.810514267068356e-05}, {"id": 253, "seek": 178044, "start": 1799.64, "end": 1806.28, "text": " we understand both the structure of language and the structure of the human world that um language", "tokens": [321, 1223, 1293, 264, 3877, 295, 2856, 293, 264, 3877, 295, 264, 1952, 1002, 300, 1105, 2856], "temperature": 0.0, "avg_logprob": -0.044985620281364345, "compression_ratio": 1.766355140186916, "no_speech_prob": 5.810514267068356e-05}, {"id": 254, "seek": 180628, "start": 1806.28, "end": 1813.08, "text": " talks about um but there's a much more practical answer than that um which is you know", "tokens": [6686, 466, 1105, 457, 456, 311, 257, 709, 544, 8496, 1867, 813, 300, 1105, 597, 307, 291, 458], "temperature": 0.0, "avg_logprob": -0.08237640181584145, "compression_ratio": 1.5359116022099448, "no_speech_prob": 3.875504989991896e-05}, {"id": 255, "seek": 180628, "start": 1813.08, "end": 1820.84, "text": " language models are really the secret tool of natural language processing so if you're talking", "tokens": [2856, 5245, 366, 534, 264, 4054, 2290, 295, 3303, 2856, 9007, 370, 498, 291, 434, 1417], "temperature": 0.0, "avg_logprob": -0.08237640181584145, "compression_ratio": 1.5359116022099448, "no_speech_prob": 3.875504989991896e-05}, {"id": 256, "seek": 180628, "start": 1820.84, "end": 1830.92, "text": " to any nlp person and you've got almost any task it's quite likely they'll say oh I bet we could", "tokens": [281, 604, 297, 75, 79, 954, 293, 291, 600, 658, 1920, 604, 5633, 309, 311, 1596, 3700, 436, 603, 584, 1954, 286, 778, 321, 727], "temperature": 0.0, "avg_logprob": -0.08237640181584145, "compression_ratio": 1.5359116022099448, "no_speech_prob": 3.875504989991896e-05}, {"id": 257, "seek": 183092, "start": 1830.92, "end": 1838.52, "text": " use a language model for that and so language models are sort of used as a not the whole solution", "tokens": [764, 257, 2856, 2316, 337, 300, 293, 370, 2856, 5245, 366, 1333, 295, 1143, 382, 257, 406, 264, 1379, 3827], "temperature": 0.0, "avg_logprob": -0.09080936431884766, "compression_ratio": 1.740909090909091, "no_speech_prob": 2.6650151994545013e-05}, {"id": 258, "seek": 183092, "start": 1838.52, "end": 1846.52, "text": " but a part of almost any task any task involves generating or estimating the probability of text", "tokens": [457, 257, 644, 295, 1920, 604, 5633, 604, 5633, 11626, 17746, 420, 8017, 990, 264, 8482, 295, 2487], "temperature": 0.0, "avg_logprob": -0.09080936431884766, "compression_ratio": 1.740909090909091, "no_speech_prob": 2.6650151994545013e-05}, {"id": 259, "seek": 183092, "start": 1846.52, "end": 1853.64, "text": " so you can use it for predictive typing speech recognition grammar correction identifying authors", "tokens": [370, 291, 393, 764, 309, 337, 35521, 18444, 6218, 11150, 22317, 19984, 16696, 16552], "temperature": 0.0, "avg_logprob": -0.09080936431884766, "compression_ratio": 1.740909090909091, "no_speech_prob": 2.6650151994545013e-05}, {"id": 260, "seek": 183092, "start": 1853.64, "end": 1858.6000000000001, "text": " machine translation summarization dialogue just that anything you do with natural language", "tokens": [3479, 12853, 14611, 2144, 10221, 445, 300, 1340, 291, 360, 365, 3303, 2856], "temperature": 0.0, "avg_logprob": -0.09080936431884766, "compression_ratio": 1.740909090909091, "no_speech_prob": 2.6650151994545013e-05}, {"id": 261, "seek": 185860, "start": 1858.6, "end": 1865.56, "text": " involves language models and we'll see examples of that in following classes including next Tuesday", "tokens": [11626, 2856, 5245, 293, 321, 603, 536, 5110, 295, 300, 294, 3480, 5359, 3009, 958, 10017], "temperature": 0.0, "avg_logprob": -0.11852743586555856, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.591121539007872e-05}, {"id": 262, "seek": 185860, "start": 1865.56, "end": 1873.56, "text": " where we're using language models for machine translation okay so a language model is just a system", "tokens": [689, 321, 434, 1228, 2856, 5245, 337, 3479, 12853, 1392, 370, 257, 2856, 2316, 307, 445, 257, 1185], "temperature": 0.0, "avg_logprob": -0.11852743586555856, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.591121539007872e-05}, {"id": 263, "seek": 185860, "start": 1873.56, "end": 1881.6399999999999, "text": " that predicts the next word a recurrent neural network is a family of neural networks which can", "tokens": [300, 6069, 82, 264, 958, 1349, 257, 18680, 1753, 18161, 3209, 307, 257, 1605, 295, 18161, 9590, 597, 393], "temperature": 0.0, "avg_logprob": -0.11852743586555856, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.591121539007872e-05}, {"id": 264, "seek": 188164, "start": 1881.64, "end": 1889.5600000000002, "text": " take sequential input of any length they reuse the same weights to generate a hidden state", "tokens": [747, 42881, 4846, 295, 604, 4641, 436, 26225, 264, 912, 17443, 281, 8460, 257, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.09805411435245129, "compression_ratio": 1.6462882096069869, "no_speech_prob": 4.7504156100330874e-05}, {"id": 265, "seek": 188164, "start": 1889.5600000000002, "end": 1897.4, "text": " and optionally but commonly an output on each step um note that these two things are different um so", "tokens": [293, 3614, 379, 457, 12719, 364, 5598, 322, 1184, 1823, 1105, 3637, 300, 613, 732, 721, 366, 819, 1105, 370], "temperature": 0.0, "avg_logprob": -0.09805411435245129, "compression_ratio": 1.6462882096069869, "no_speech_prob": 4.7504156100330874e-05}, {"id": 266, "seek": 188164, "start": 1898.6000000000001, "end": 1903.64, "text": " we've talked about two ways that you could build language models but one of them is our", "tokens": [321, 600, 2825, 466, 732, 2098, 300, 291, 727, 1322, 2856, 5245, 457, 472, 295, 552, 307, 527], "temperature": 0.0, "avg_logprob": -0.09805411435245129, "compression_ratio": 1.6462882096069869, "no_speech_prob": 4.7504156100330874e-05}, {"id": 267, "seek": 188164, "start": 1903.64, "end": 1909.64, "text": " ns being a great way but our ns can also be used for a lot of other things so let me just quickly", "tokens": [297, 82, 885, 257, 869, 636, 457, 527, 297, 82, 393, 611, 312, 1143, 337, 257, 688, 295, 661, 721, 370, 718, 385, 445, 2661], "temperature": 0.0, "avg_logprob": -0.09805411435245129, "compression_ratio": 1.6462882096069869, "no_speech_prob": 4.7504156100330874e-05}, {"id": 268, "seek": 190964, "start": 1909.64, "end": 1915.72, "text": " preview a few other things you can do with our ns so there are lots of tasks that people want to do", "tokens": [14281, 257, 1326, 661, 721, 291, 393, 360, 365, 527, 297, 82, 370, 456, 366, 3195, 295, 9608, 300, 561, 528, 281, 360], "temperature": 0.0, "avg_logprob": -0.16709567591087104, "compression_ratio": 1.7391304347826086, "no_speech_prob": 3.0214900107239373e-05}, {"id": 269, "seek": 190964, "start": 1915.72, "end": 1924.0400000000002, "text": " an nlp which are referred to as sequence taking tasks where we'd like to take words of text and do", "tokens": [364, 297, 75, 79, 597, 366, 10839, 281, 382, 8310, 1940, 9608, 689, 321, 1116, 411, 281, 747, 2283, 295, 2487, 293, 360], "temperature": 0.0, "avg_logprob": -0.16709567591087104, "compression_ratio": 1.7391304347826086, "no_speech_prob": 3.0214900107239373e-05}, {"id": 270, "seek": 190964, "start": 1924.0400000000002, "end": 1931.16, "text": " some kind of classification along the sequence so one simple common one is to give words parts of", "tokens": [512, 733, 295, 21538, 2051, 264, 8310, 370, 472, 2199, 2689, 472, 307, 281, 976, 2283, 3166, 295], "temperature": 0.0, "avg_logprob": -0.16709567591087104, "compression_ratio": 1.7391304347826086, "no_speech_prob": 3.0214900107239373e-05}, {"id": 271, "seek": 190964, "start": 1931.16, "end": 1938.6000000000001, "text": " speech that is a determinar start orders an adjective cat is a noun not does a verb um and well you can", "tokens": [6218, 300, 307, 257, 3618, 6470, 722, 9470, 364, 44129, 3857, 307, 257, 23307, 406, 775, 257, 9595, 1105, 293, 731, 291, 393], "temperature": 0.0, "avg_logprob": -0.16709567591087104, "compression_ratio": 1.7391304347826086, "no_speech_prob": 3.0214900107239373e-05}, {"id": 272, "seek": 193860, "start": 1938.6, "end": 1946.76, "text": " do this straightforwardly by using a recurrent neural network as a sequential classifier whereas now", "tokens": [360, 341, 15325, 356, 538, 1228, 257, 18680, 1753, 18161, 3209, 382, 257, 42881, 1508, 9902, 9735, 586], "temperature": 0.0, "avg_logprob": -0.08082883151960962, "compression_ratio": 1.7798165137614679, "no_speech_prob": 3.372626088093966e-05}, {"id": 273, "seek": 193860, "start": 1946.76, "end": 1953.8799999999999, "text": " going to generate parts of speech rather than the next word you can use a recurrent neural network", "tokens": [516, 281, 8460, 3166, 295, 6218, 2831, 813, 264, 958, 1349, 291, 393, 764, 257, 18680, 1753, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.08082883151960962, "compression_ratio": 1.7798165137614679, "no_speech_prob": 3.372626088093966e-05}, {"id": 274, "seek": 193860, "start": 1953.8799999999999, "end": 1961.3999999999999, "text": " the sentiment classification well this time we don't actually want to generate um an output at", "tokens": [264, 16149, 21538, 731, 341, 565, 321, 500, 380, 767, 528, 281, 8460, 1105, 364, 5598, 412], "temperature": 0.0, "avg_logprob": -0.08082883151960962, "compression_ratio": 1.7798165137614679, "no_speech_prob": 3.372626088093966e-05}, {"id": 275, "seek": 193860, "start": 1961.3999999999999, "end": 1966.9199999999998, "text": " each word necessarily but we want to know what the overall sentiment looks like so somehow we", "tokens": [1184, 1349, 4725, 457, 321, 528, 281, 458, 437, 264, 4787, 16149, 1542, 411, 370, 6063, 321], "temperature": 0.0, "avg_logprob": -0.08082883151960962, "compression_ratio": 1.7798165137614679, "no_speech_prob": 3.372626088093966e-05}, {"id": 276, "seek": 196692, "start": 1966.92, "end": 1973.24, "text": " want to get out a sentence encoding that we can perhaps put through another neural network layer", "tokens": [528, 281, 483, 484, 257, 8174, 43430, 300, 321, 393, 4317, 829, 807, 1071, 18161, 3209, 4583], "temperature": 0.0, "avg_logprob": -0.06858365710188703, "compression_ratio": 1.7252252252252251, "no_speech_prob": 4.7560082748532295e-05}, {"id": 277, "seek": 196692, "start": 1973.24, "end": 1980.68, "text": " to judge whether the sentence is positive or negative well the simplest way to do that is to think", "tokens": [281, 6995, 1968, 264, 8174, 307, 3353, 420, 3671, 731, 264, 22811, 636, 281, 360, 300, 307, 281, 519], "temperature": 0.0, "avg_logprob": -0.06858365710188703, "compression_ratio": 1.7252252252252251, "no_speech_prob": 4.7560082748532295e-05}, {"id": 278, "seek": 196692, "start": 1980.68, "end": 1991.24, "text": " well after I've run my LSTM through the whole sentence actually this final hidden state it's", "tokens": [731, 934, 286, 600, 1190, 452, 441, 6840, 44, 807, 264, 1379, 8174, 767, 341, 2572, 7633, 1785, 309, 311], "temperature": 0.0, "avg_logprob": -0.06858365710188703, "compression_ratio": 1.7252252252252251, "no_speech_prob": 4.7560082748532295e-05}, {"id": 279, "seek": 196692, "start": 1991.24, "end": 1996.28, "text": " encoded the whole sentence because remember I updated that hidden state based on each previous", "tokens": [2058, 12340, 264, 1379, 8174, 570, 1604, 286, 10588, 300, 7633, 1785, 2361, 322, 1184, 3894], "temperature": 0.0, "avg_logprob": -0.06858365710188703, "compression_ratio": 1.7252252252252251, "no_speech_prob": 4.7560082748532295e-05}, {"id": 280, "seek": 199628, "start": 1996.28, "end": 2001.8799999999999, "text": " word and so you could say that this is the whole meaning of the sentence so let's just say that", "tokens": [1349, 293, 370, 291, 727, 584, 300, 341, 307, 264, 1379, 3620, 295, 264, 8174, 370, 718, 311, 445, 584, 300], "temperature": 0.0, "avg_logprob": -0.0611698410727761, "compression_ratio": 1.8082191780821917, "no_speech_prob": 3.2165582524612546e-05}, {"id": 281, "seek": 199628, "start": 2001.8799999999999, "end": 2009.24, "text": " is the sentence encoding um and then put an extra um classifier layer on that with something like a", "tokens": [307, 264, 8174, 43430, 1105, 293, 550, 829, 364, 2857, 1105, 1508, 9902, 4583, 322, 300, 365, 746, 411, 257], "temperature": 0.0, "avg_logprob": -0.0611698410727761, "compression_ratio": 1.8082191780821917, "no_speech_prob": 3.2165582524612546e-05}, {"id": 282, "seek": 199628, "start": 2009.24, "end": 2015.6399999999999, "text": " softmax classifier um that method has been used and it actually works reasonably well and if you", "tokens": [2787, 41167, 1508, 9902, 1105, 300, 3170, 575, 668, 1143, 293, 309, 767, 1985, 23551, 731, 293, 498, 291], "temperature": 0.0, "avg_logprob": -0.0611698410727761, "compression_ratio": 1.8082191780821917, "no_speech_prob": 3.2165582524612546e-05}, {"id": 283, "seek": 199628, "start": 2015.6399999999999, "end": 2022.92, "text": " sort of train this model end to end well it's actually then motivated to preserve sentiment information", "tokens": [1333, 295, 3847, 341, 2316, 917, 281, 917, 731, 309, 311, 767, 550, 14515, 281, 15665, 16149, 1589], "temperature": 0.0, "avg_logprob": -0.0611698410727761, "compression_ratio": 1.8082191780821917, "no_speech_prob": 3.2165582524612546e-05}, {"id": 284, "seek": 202292, "start": 2022.92, "end": 2027.96, "text": " in the hidden state of the recurrent neural network because that will allow it to better predict", "tokens": [294, 264, 7633, 1785, 295, 264, 18680, 1753, 18161, 3209, 570, 300, 486, 2089, 309, 281, 1101, 6069], "temperature": 0.0, "avg_logprob": -0.05942971796929082, "compression_ratio": 1.8038277511961722, "no_speech_prob": 2.7955466066487134e-05}, {"id": 285, "seek": 202292, "start": 2027.96, "end": 2034.1200000000001, "text": " the sentiment of the whole sentence um which is the final task and hence loss function that we're", "tokens": [264, 16149, 295, 264, 1379, 8174, 1105, 597, 307, 264, 2572, 5633, 293, 16678, 4470, 2445, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.05942971796929082, "compression_ratio": 1.8038277511961722, "no_speech_prob": 2.7955466066487134e-05}, {"id": 286, "seek": 202292, "start": 2034.1200000000001, "end": 2040.04, "text": " giving the network but it turns out that you can commonly do better than that by actually", "tokens": [2902, 264, 3209, 457, 309, 4523, 484, 300, 291, 393, 12719, 360, 1101, 813, 300, 538, 767], "temperature": 0.0, "avg_logprob": -0.05942971796929082, "compression_ratio": 1.8038277511961722, "no_speech_prob": 2.7955466066487134e-05}, {"id": 287, "seek": 202292, "start": 2040.04, "end": 2046.76, "text": " doing things like feeding all hidden states into the sentence encoding perhaps by making the", "tokens": [884, 721, 411, 12919, 439, 7633, 4368, 666, 264, 8174, 43430, 4317, 538, 1455, 264], "temperature": 0.0, "avg_logprob": -0.05942971796929082, "compression_ratio": 1.8038277511961722, "no_speech_prob": 2.7955466066487134e-05}, {"id": 288, "seek": 204676, "start": 2046.76, "end": 2054.76, "text": " sentence encoding an element wise max or an element wise mean of all the hidden states because", "tokens": [8174, 43430, 364, 4478, 10829, 11469, 420, 364, 4478, 10829, 914, 295, 439, 264, 7633, 4368, 570], "temperature": 0.0, "avg_logprob": -0.11125516083280919, "compression_ratio": 1.6335403726708075, "no_speech_prob": 4.633369371731533e-06}, {"id": 289, "seek": 204676, "start": 2054.76, "end": 2060.84, "text": " this then more symmetrically encodes the hidden state over each time step", "tokens": [341, 550, 544, 14232, 27965, 984, 2058, 4789, 264, 7633, 1785, 670, 1184, 565, 1823], "temperature": 0.0, "avg_logprob": -0.11125516083280919, "compression_ratio": 1.6335403726708075, "no_speech_prob": 4.633369371731533e-06}, {"id": 290, "seek": 204676, "start": 2064.68, "end": 2074.04, "text": " another big use of recurrent neural networks is what I'll call language encoder module uses so", "tokens": [1071, 955, 764, 295, 18680, 1753, 18161, 9590, 307, 437, 286, 603, 818, 2856, 2058, 19866, 10088, 4960, 370], "temperature": 0.0, "avg_logprob": -0.11125516083280919, "compression_ratio": 1.6335403726708075, "no_speech_prob": 4.633369371731533e-06}, {"id": 291, "seek": 207404, "start": 2074.04, "end": 2080.68, "text": " anytime you have some text for example here we have a question of what nationality was Beethoven", "tokens": [13038, 291, 362, 512, 2487, 337, 1365, 510, 321, 362, 257, 1168, 295, 437, 4048, 507, 390, 38651], "temperature": 0.0, "avg_logprob": -0.09744682312011718, "compression_ratio": 1.5888888888888888, "no_speech_prob": 1.542034442536533e-05}, {"id": 292, "seek": 207404, "start": 2081.56, "end": 2089.32, "text": " we'd like to construct some kind of neural representation of this so one way to do it is to run", "tokens": [321, 1116, 411, 281, 7690, 512, 733, 295, 18161, 10290, 295, 341, 370, 472, 636, 281, 360, 309, 307, 281, 1190], "temperature": 0.0, "avg_logprob": -0.09744682312011718, "compression_ratio": 1.5888888888888888, "no_speech_prob": 1.542034442536533e-05}, {"id": 293, "seek": 207404, "start": 2090.44, "end": 2096.92, "text": " recurrent neural network over it and then just like last time to either take the final hidden", "tokens": [18680, 1753, 18161, 3209, 670, 309, 293, 550, 445, 411, 1036, 565, 281, 2139, 747, 264, 2572, 7633], "temperature": 0.0, "avg_logprob": -0.09744682312011718, "compression_ratio": 1.5888888888888888, "no_speech_prob": 1.542034442536533e-05}, {"id": 294, "seek": 209692, "start": 2096.92, "end": 2105.08, "text": " state or take some kind of um function of all the hidden states and say that's the sentence representation", "tokens": [1785, 420, 747, 512, 733, 295, 1105, 2445, 295, 439, 264, 7633, 4368, 293, 584, 300, 311, 264, 8174, 10290], "temperature": 0.0, "avg_logprob": -0.08809692628922002, "compression_ratio": 1.83710407239819, "no_speech_prob": 2.1738022041972727e-05}, {"id": 295, "seek": 209692, "start": 2105.08, "end": 2111.48, "text": " and we could do the same thing um for the context so for question answering we're going to build some more", "tokens": [293, 321, 727, 360, 264, 912, 551, 1105, 337, 264, 4319, 370, 337, 1168, 13430, 321, 434, 516, 281, 1322, 512, 544], "temperature": 0.0, "avg_logprob": -0.08809692628922002, "compression_ratio": 1.83710407239819, "no_speech_prob": 2.1738022041972727e-05}, {"id": 296, "seek": 209692, "start": 2111.48, "end": 2117.64, "text": " neural net structure um on top of that and we'll learn more about them a couple of weeks um when we", "tokens": [18161, 2533, 3877, 1105, 322, 1192, 295, 300, 293, 321, 603, 1466, 544, 466, 552, 257, 1916, 295, 3259, 1105, 562, 321], "temperature": 0.0, "avg_logprob": -0.08809692628922002, "compression_ratio": 1.83710407239819, "no_speech_prob": 2.1738022041972727e-05}, {"id": 297, "seek": 209692, "start": 2117.64, "end": 2124.36, "text": " have the question answering lecture but the key thing is what we built so far we used to get", "tokens": [362, 264, 1168, 13430, 7991, 457, 264, 2141, 551, 307, 437, 321, 3094, 370, 1400, 321, 1143, 281, 483], "temperature": 0.0, "avg_logprob": -0.08809692628922002, "compression_ratio": 1.83710407239819, "no_speech_prob": 2.1738022041972727e-05}, {"id": 298, "seek": 212436, "start": 2124.36, "end": 2131.48, "text": " sentence representation so it's a language encoder module so that was the language encoding part", "tokens": [8174, 10290, 370, 309, 311, 257, 2856, 2058, 19866, 10088, 370, 300, 390, 264, 2856, 43430, 644], "temperature": 0.0, "avg_logprob": -0.050915087776622556, "compression_ratio": 1.832535885167464, "no_speech_prob": 7.598048978252336e-05}, {"id": 299, "seek": 212436, "start": 2131.48, "end": 2139.32, "text": " we can also use RNNs to decode into language and that's commonly used in speech recognition", "tokens": [321, 393, 611, 764, 45702, 45, 82, 281, 979, 1429, 666, 2856, 293, 300, 311, 12719, 1143, 294, 6218, 11150], "temperature": 0.0, "avg_logprob": -0.050915087776622556, "compression_ratio": 1.832535885167464, "no_speech_prob": 7.598048978252336e-05}, {"id": 300, "seek": 212436, "start": 2139.32, "end": 2146.44, "text": " machine translation summarization so if we have a speech recognizer the input is an audio signal", "tokens": [3479, 12853, 14611, 2144, 370, 498, 321, 362, 257, 6218, 3068, 6545, 264, 4846, 307, 364, 6278, 6358], "temperature": 0.0, "avg_logprob": -0.050915087776622556, "compression_ratio": 1.832535885167464, "no_speech_prob": 7.598048978252336e-05}, {"id": 301, "seek": 212436, "start": 2146.44, "end": 2153.48, "text": " and what we want to do is decode that into language well what we could do is use some function of", "tokens": [293, 437, 321, 528, 281, 360, 307, 979, 1429, 300, 666, 2856, 731, 437, 321, 727, 360, 307, 764, 512, 2445, 295], "temperature": 0.0, "avg_logprob": -0.050915087776622556, "compression_ratio": 1.832535885167464, "no_speech_prob": 7.598048978252336e-05}, {"id": 302, "seek": 215348, "start": 2153.48, "end": 2162.12, "text": " the input which is probably itself going to be in your net as the initial um hidden state of", "tokens": [264, 4846, 597, 307, 1391, 2564, 516, 281, 312, 294, 428, 2533, 382, 264, 5883, 1105, 7633, 1785, 295], "temperature": 0.0, "avg_logprob": -0.09248014953401354, "compression_ratio": 1.6201117318435754, "no_speech_prob": 4.128396540181711e-05}, {"id": 303, "seek": 215348, "start": 2162.92, "end": 2172.28, "text": " our RNN LM and then we say start generating text based on that and so it should then um we generate", "tokens": [527, 45702, 45, 46529, 293, 550, 321, 584, 722, 17746, 2487, 2361, 322, 300, 293, 370, 309, 820, 550, 1105, 321, 8460], "temperature": 0.0, "avg_logprob": -0.09248014953401354, "compression_ratio": 1.6201117318435754, "no_speech_prob": 4.128396540181711e-05}, {"id": 304, "seek": 215348, "start": 2172.28, "end": 2178.84, "text": " word at a time by the method that we just looked at um we turn the speech into text so this is an", "tokens": [1349, 412, 257, 565, 538, 264, 3170, 300, 321, 445, 2956, 412, 1105, 321, 1261, 264, 6218, 666, 2487, 370, 341, 307, 364], "temperature": 0.0, "avg_logprob": -0.09248014953401354, "compression_ratio": 1.6201117318435754, "no_speech_prob": 4.128396540181711e-05}, {"id": 305, "seek": 217884, "start": 2178.84, "end": 2185.08, "text": " example of a conditional language model because we're now generating text conditioned on the speech", "tokens": [1365, 295, 257, 27708, 2856, 2316, 570, 321, 434, 586, 17746, 2487, 35833, 322, 264, 6218], "temperature": 0.0, "avg_logprob": -0.06746230998509367, "compression_ratio": 1.9154228855721394, "no_speech_prob": 1.5667723346268758e-05}, {"id": 306, "seek": 217884, "start": 2185.08, "end": 2191.8, "text": " signal and a lot of the time you can do interesting more advanced things with the current neural", "tokens": [6358, 293, 257, 688, 295, 264, 565, 291, 393, 360, 1880, 544, 7339, 721, 365, 264, 2190, 18161], "temperature": 0.0, "avg_logprob": -0.06746230998509367, "compression_ratio": 1.9154228855721394, "no_speech_prob": 1.5667723346268758e-05}, {"id": 307, "seek": 217884, "start": 2191.8, "end": 2198.36, "text": " networks by building conditional language models another place you can use conditional language", "tokens": [9590, 538, 2390, 27708, 2856, 5245, 1071, 1081, 291, 393, 764, 27708, 2856], "temperature": 0.0, "avg_logprob": -0.06746230998509367, "compression_ratio": 1.9154228855721394, "no_speech_prob": 1.5667723346268758e-05}, {"id": 308, "seek": 217884, "start": 2198.36, "end": 2205.88, "text": " models is for text classification tasks and including sentiment classification so if you can", "tokens": [5245, 307, 337, 2487, 21538, 9608, 293, 3009, 16149, 21538, 370, 498, 291, 393], "temperature": 0.0, "avg_logprob": -0.06746230998509367, "compression_ratio": 1.9154228855721394, "no_speech_prob": 1.5667723346268758e-05}, {"id": 309, "seek": 220588, "start": 2205.88, "end": 2212.84, "text": " condition um your language model based on a kind of sentiment you can build a kind of classifier", "tokens": [4188, 1105, 428, 2856, 2316, 2361, 322, 257, 733, 295, 16149, 291, 393, 1322, 257, 733, 295, 1508, 9902], "temperature": 0.0, "avg_logprob": -0.07167186159076112, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.1840931620099582e-05}, {"id": 310, "seek": 220588, "start": 2212.84, "end": 2217.6400000000003, "text": " for that and another use that we'll see a lot of next class is for machine translation", "tokens": [337, 300, 293, 1071, 764, 300, 321, 603, 536, 257, 688, 295, 958, 1508, 307, 337, 3479, 12853], "temperature": 0.0, "avg_logprob": -0.07167186159076112, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.1840931620099582e-05}, {"id": 311, "seek": 220588, "start": 2219.32, "end": 2229.08, "text": " okay so that's the end of the intro um to um doing things with um recurrent neural networks and", "tokens": [1392, 370, 300, 311, 264, 917, 295, 264, 12897, 1105, 281, 1105, 884, 721, 365, 1105, 18680, 1753, 18161, 9590, 293], "temperature": 0.0, "avg_logprob": -0.07167186159076112, "compression_ratio": 1.6607142857142858, "no_speech_prob": 1.1840931620099582e-05}, {"id": 312, "seek": 222908, "start": 2229.08, "end": 2237.24, "text": " language models now I want to move on and tell you about the fact that everything is not perfect", "tokens": [2856, 5245, 586, 286, 528, 281, 1286, 322, 293, 980, 291, 466, 264, 1186, 300, 1203, 307, 406, 2176], "temperature": 0.0, "avg_logprob": -0.05409833363124302, "compression_ratio": 1.72, "no_speech_prob": 1.2602686183527112e-05}, {"id": 313, "seek": 222908, "start": 2237.24, "end": 2244.7599999999998, "text": " and these recurrent neural networks tend to have a couple of problems and we'll talk about those", "tokens": [293, 613, 18680, 1753, 18161, 9590, 3928, 281, 362, 257, 1916, 295, 2740, 293, 321, 603, 751, 466, 729], "temperature": 0.0, "avg_logprob": -0.05409833363124302, "compression_ratio": 1.72, "no_speech_prob": 1.2602686183527112e-05}, {"id": 314, "seek": 222908, "start": 2244.7599999999998, "end": 2250.12, "text": " and then in part that'll then motivate coming up with a more advanced recurrent neural network", "tokens": [293, 550, 294, 644, 300, 603, 550, 28497, 1348, 493, 365, 257, 544, 7339, 18680, 1753, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.05409833363124302, "compression_ratio": 1.72, "no_speech_prob": 1.2602686183527112e-05}, {"id": 315, "seek": 225012, "start": 2250.12, "end": 2259.0, "text": " architecture so the first problem to be mentioned is the idea of what's called vanishing gradients", "tokens": [9482, 370, 264, 700, 1154, 281, 312, 2835, 307, 264, 1558, 295, 437, 311, 1219, 3161, 3807, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.0647656286464018, "compression_ratio": 1.6954022988505748, "no_speech_prob": 1.7771693819668144e-05}, {"id": 316, "seek": 225012, "start": 2259.0, "end": 2267.24, "text": " and what does that mean well at the end of our sequence we have some overall um loss that we're", "tokens": [293, 437, 775, 300, 914, 731, 412, 264, 917, 295, 527, 8310, 321, 362, 512, 4787, 1105, 4470, 300, 321, 434], "temperature": 0.0, "avg_logprob": -0.0647656286464018, "compression_ratio": 1.6954022988505748, "no_speech_prob": 1.7771693819668144e-05}, {"id": 317, "seek": 225012, "start": 2267.24, "end": 2275.3199999999997, "text": " calculating and well what we want to do is back propagate that loss um and we want to back propagate", "tokens": [28258, 293, 731, 437, 321, 528, 281, 360, 307, 646, 48256, 300, 4470, 1105, 293, 321, 528, 281, 646, 48256], "temperature": 0.0, "avg_logprob": -0.0647656286464018, "compression_ratio": 1.6954022988505748, "no_speech_prob": 1.7771693819668144e-05}, {"id": 318, "seek": 227532, "start": 2275.32, "end": 2283.56, "text": " it right along the sequence and so we're working out the partials of j4 um with respect to the hidden", "tokens": [309, 558, 2051, 264, 8310, 293, 370, 321, 434, 1364, 484, 264, 644, 12356, 295, 361, 19, 1105, 365, 3104, 281, 264, 7633], "temperature": 0.0, "avg_logprob": -0.07998793376119513, "compression_ratio": 1.9477124183006536, "no_speech_prob": 3.167168324580416e-05}, {"id": 319, "seek": 227532, "start": 2283.56, "end": 2289.48, "text": " state at time one and when we have a longer sequence we'll be working out the partials of j20", "tokens": [1785, 412, 565, 472, 293, 562, 321, 362, 257, 2854, 8310, 321, 603, 312, 1364, 484, 264, 644, 12356, 295, 361, 2009], "temperature": 0.0, "avg_logprob": -0.07998793376119513, "compression_ratio": 1.9477124183006536, "no_speech_prob": 3.167168324580416e-05}, {"id": 320, "seek": 227532, "start": 2289.48, "end": 2298.6000000000004, "text": " with respect to the hidden state at time one and how do we do that well how we do it is by composition", "tokens": [365, 3104, 281, 264, 7633, 1785, 412, 565, 472, 293, 577, 360, 321, 360, 300, 731, 577, 321, 360, 309, 307, 538, 12686], "temperature": 0.0, "avg_logprob": -0.07998793376119513, "compression_ratio": 1.9477124183006536, "no_speech_prob": 3.167168324580416e-05}, {"id": 321, "seek": 229860, "start": 2298.6, "end": 2307.16, "text": " and the chain rule we've got a big long chain rule along the whole sequence um well if we're doing", "tokens": [293, 264, 5021, 4978, 321, 600, 658, 257, 955, 938, 5021, 4978, 2051, 264, 1379, 8310, 1105, 731, 498, 321, 434, 884], "temperature": 0.0, "avg_logprob": -0.07157177243913923, "compression_ratio": 1.6685393258426966, "no_speech_prob": 1.3618032426165882e-05}, {"id": 322, "seek": 229860, "start": 2307.16, "end": 2317.56, "text": " that um you know we're multiplying a ton of things together and so the danger of what tends to happen", "tokens": [300, 1105, 291, 458, 321, 434, 30955, 257, 2952, 295, 721, 1214, 293, 370, 264, 4330, 295, 437, 12258, 281, 1051], "temperature": 0.0, "avg_logprob": -0.07157177243913923, "compression_ratio": 1.6685393258426966, "no_speech_prob": 1.3618032426165882e-05}, {"id": 323, "seek": 229860, "start": 2317.56, "end": 2326.68, "text": " is that as we do these um multiplications a lot of time these partials between successive hidden", "tokens": [307, 300, 382, 321, 360, 613, 1105, 17596, 763, 257, 688, 295, 565, 613, 644, 12356, 1296, 48043, 7633], "temperature": 0.0, "avg_logprob": -0.07157177243913923, "compression_ratio": 1.6685393258426966, "no_speech_prob": 1.3618032426165882e-05}, {"id": 324, "seek": 232668, "start": 2326.68, "end": 2334.8399999999997, "text": " states become small and so what happens is as we go along the gradient gets smaller and smaller", "tokens": [4368, 1813, 1359, 293, 370, 437, 2314, 307, 382, 321, 352, 2051, 264, 16235, 2170, 4356, 293, 4356], "temperature": 0.0, "avg_logprob": -0.08888152819960865, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.4735834156454075e-05}, {"id": 325, "seek": 232668, "start": 2334.8399999999997, "end": 2343.0, "text": " and smaller and starts to peter out and to the extent that appears out um well then we've kind of", "tokens": [293, 4356, 293, 3719, 281, 280, 2398, 484, 293, 281, 264, 8396, 300, 7038, 484, 1105, 731, 550, 321, 600, 733, 295], "temperature": 0.0, "avg_logprob": -0.08888152819960865, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.4735834156454075e-05}, {"id": 326, "seek": 232668, "start": 2343.0, "end": 2350.6, "text": " got no upstream gradient and therefore we won't be changing the parameters at all and that turns out", "tokens": [658, 572, 33915, 16235, 293, 4412, 321, 1582, 380, 312, 4473, 264, 9834, 412, 439, 293, 300, 4523, 484], "temperature": 0.0, "avg_logprob": -0.08888152819960865, "compression_ratio": 1.7818181818181817, "no_speech_prob": 1.4735834156454075e-05}, {"id": 327, "seek": 235060, "start": 2350.6, "end": 2359.64, "text": " to be pretty problematic um so the next couple of slides sort of um say a little bit about the", "tokens": [281, 312, 1238, 19011, 1105, 370, 264, 958, 1916, 295, 9788, 1333, 295, 1105, 584, 257, 707, 857, 466, 264], "temperature": 0.0, "avg_logprob": -0.07695104764855426, "compression_ratio": 1.615819209039548, "no_speech_prob": 1.5681822333135642e-05}, {"id": 328, "seek": 235060, "start": 2359.64, "end": 2371.24, "text": " why and how this happens um what's presented here is a kind of only semi formal wave your hands", "tokens": [983, 293, 577, 341, 2314, 1105, 437, 311, 8212, 510, 307, 257, 733, 295, 787, 12909, 9860, 5772, 428, 2377], "temperature": 0.0, "avg_logprob": -0.07695104764855426, "compression_ratio": 1.615819209039548, "no_speech_prob": 1.5681822333135642e-05}, {"id": 329, "seek": 235060, "start": 2371.24, "end": 2376.7599999999998, "text": " at the kind of problems that you might expect um if you really want to sort of get into all the", "tokens": [412, 264, 733, 295, 2740, 300, 291, 1062, 2066, 1105, 498, 291, 534, 528, 281, 1333, 295, 483, 666, 439, 264], "temperature": 0.0, "avg_logprob": -0.07695104764855426, "compression_ratio": 1.615819209039548, "no_speech_prob": 1.5681822333135642e-05}, {"id": 330, "seek": 237676, "start": 2376.76, "end": 2382.1200000000003, "text": " details of this um you should look at the couple of papers um than I mentioned a small print", "tokens": [4365, 295, 341, 1105, 291, 820, 574, 412, 264, 1916, 295, 10577, 1105, 813, 286, 2835, 257, 1359, 4482], "temperature": 0.0, "avg_logprob": -0.10931591554121538, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.9712011635419913e-05}, {"id": 331, "seek": 237676, "start": 2382.1200000000003, "end": 2387.7200000000003, "text": " at the bottom of the slide but at any rate if you remember that this is our basic um", "tokens": [412, 264, 2767, 295, 264, 4137, 457, 412, 604, 3314, 498, 291, 1604, 300, 341, 307, 527, 3875, 1105], "temperature": 0.0, "avg_logprob": -0.10931591554121538, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.9712011635419913e-05}, {"id": 332, "seek": 237676, "start": 2387.7200000000003, "end": 2394.28, "text": " my current neural network equation well let's consider an easy case suppose we sort of get rid", "tokens": [452, 2190, 18161, 3209, 5367, 731, 718, 311, 1949, 364, 1858, 1389, 7297, 321, 1333, 295, 483, 3973], "temperature": 0.0, "avg_logprob": -0.10931591554121538, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.9712011635419913e-05}, {"id": 333, "seek": 237676, "start": 2394.28, "end": 2402.5200000000004, "text": " of our non-linearity and just assume that it's an identity function okay so then when we're working", "tokens": [295, 527, 2107, 12, 1889, 17409, 293, 445, 6552, 300, 309, 311, 364, 6575, 2445, 1392, 370, 550, 562, 321, 434, 1364], "temperature": 0.0, "avg_logprob": -0.10931591554121538, "compression_ratio": 1.631578947368421, "no_speech_prob": 1.9712011635419913e-05}, {"id": 334, "seek": 240252, "start": 2402.52, "end": 2408.68, "text": " out the partials of the hidden state with respect to the previous hidden state um we can work those", "tokens": [484, 264, 644, 12356, 295, 264, 7633, 1785, 365, 3104, 281, 264, 3894, 7633, 1785, 1105, 321, 393, 589, 729], "temperature": 0.0, "avg_logprob": -0.0553988182183468, "compression_ratio": 1.6900584795321638, "no_speech_prob": 2.0121491616009735e-05}, {"id": 335, "seek": 240252, "start": 2408.68, "end": 2419.32, "text": " out in the usual way according to the chain rule and then if um sigma is um simply the identity", "tokens": [484, 294, 264, 7713, 636, 4650, 281, 264, 5021, 4978, 293, 550, 498, 1105, 12771, 307, 1105, 2935, 264, 6575], "temperature": 0.0, "avg_logprob": -0.0553988182183468, "compression_ratio": 1.6900584795321638, "no_speech_prob": 2.0121491616009735e-05}, {"id": 336, "seek": 240252, "start": 2419.32, "end": 2428.04, "text": " function um well then everything gets really easy for us so only the the sigma just goes away", "tokens": [2445, 1105, 731, 550, 1203, 2170, 534, 1858, 337, 505, 370, 787, 264, 264, 12771, 445, 1709, 1314], "temperature": 0.0, "avg_logprob": -0.0553988182183468, "compression_ratio": 1.6900584795321638, "no_speech_prob": 2.0121491616009735e-05}, {"id": 337, "seek": 242804, "start": 2428.04, "end": 2436.12, "text": " and only the first term involves um h at time t minus 1 so the later terms go away", "tokens": [293, 787, 264, 700, 1433, 11626, 1105, 276, 412, 565, 256, 3175, 502, 370, 264, 1780, 2115, 352, 1314], "temperature": 0.0, "avg_logprob": -0.09356706672244602, "compression_ratio": 1.6369047619047619, "no_speech_prob": 4.155831447860692e-06}, {"id": 338, "seek": 242804, "start": 2436.12, "end": 2445.24, "text": " and so um our gradient ends up as wh well that's doing it for just one time step what happens when", "tokens": [293, 370, 1105, 527, 16235, 5314, 493, 382, 261, 71, 731, 300, 311, 884, 309, 337, 445, 472, 565, 1823, 437, 2314, 562], "temperature": 0.0, "avg_logprob": -0.09356706672244602, "compression_ratio": 1.6369047619047619, "no_speech_prob": 4.155831447860692e-06}, {"id": 339, "seek": 242804, "start": 2445.24, "end": 2451.72, "text": " you want to work out these partials a number of time steps away so we want to work it out the", "tokens": [291, 528, 281, 589, 484, 613, 644, 12356, 257, 1230, 295, 565, 4439, 1314, 370, 321, 528, 281, 589, 309, 484, 264], "temperature": 0.0, "avg_logprob": -0.09356706672244602, "compression_ratio": 1.6369047619047619, "no_speech_prob": 4.155831447860692e-06}, {"id": 340, "seek": 245172, "start": 2451.72, "end": 2463.08, "text": " partial of time step i with respect to j um well what we end up with is a product of the", "tokens": [14641, 295, 565, 1823, 741, 365, 3104, 281, 361, 1105, 731, 437, 321, 917, 493, 365, 307, 257, 1674, 295, 264], "temperature": 0.0, "avg_logprob": -0.07021801948547363, "compression_ratio": 1.5948275862068966, "no_speech_prob": 7.997909051482566e-06}, {"id": 341, "seek": 245172, "start": 2463.08, "end": 2474.2799999999997, "text": " partials of successive time steps um and well each of those um is coming out as wh and so we end", "tokens": [644, 12356, 295, 48043, 565, 4439, 1105, 293, 731, 1184, 295, 729, 1105, 307, 1348, 484, 382, 261, 71, 293, 370, 321, 917], "temperature": 0.0, "avg_logprob": -0.07021801948547363, "compression_ratio": 1.5948275862068966, "no_speech_prob": 7.997909051482566e-06}, {"id": 342, "seek": 247428, "start": 2474.28, "end": 2486.76, "text": " up um getting wh raised to the elf power and well our potential problem is that if wh is small", "tokens": [493, 1105, 1242, 261, 71, 6005, 281, 264, 35565, 1347, 293, 731, 527, 3995, 1154, 307, 300, 498, 261, 71, 307, 1359], "temperature": 0.0, "avg_logprob": -0.08359372615814209, "compression_ratio": 1.687116564417178, "no_speech_prob": 4.491512754611904e-06}, {"id": 343, "seek": 247428, "start": 2486.76, "end": 2493.0800000000004, "text": " in some sense then this term gets exponentially problematic i it becomes vanishingly small", "tokens": [294, 512, 2020, 550, 341, 1433, 2170, 37330, 19011, 741, 309, 3643, 3161, 3807, 356, 1359], "temperature": 0.0, "avg_logprob": -0.08359372615814209, "compression_ratio": 1.687116564417178, "no_speech_prob": 4.491512754611904e-06}, {"id": 344, "seek": 247428, "start": 2493.0800000000004, "end": 2501.88, "text": " as our sequence length becomes long well what can we mean by small well a matrix is small", "tokens": [382, 527, 8310, 4641, 3643, 938, 731, 437, 393, 321, 914, 538, 1359, 731, 257, 8141, 307, 1359], "temperature": 0.0, "avg_logprob": -0.08359372615814209, "compression_ratio": 1.687116564417178, "no_speech_prob": 4.491512754611904e-06}, {"id": 345, "seek": 250188, "start": 2501.88, "end": 2507.96, "text": " if it's eigenvalues are all less than one so we can rewrite what's happening with this", "tokens": [498, 309, 311, 10446, 46033, 366, 439, 1570, 813, 472, 370, 321, 393, 28132, 437, 311, 2737, 365, 341], "temperature": 0.0, "avg_logprob": -0.11132491320029073, "compression_ratio": 1.8775510204081634, "no_speech_prob": 2.5840448870440014e-05}, {"id": 346, "seek": 250188, "start": 2507.96, "end": 2516.12, "text": " successor multiplication using eigenvalues and eigenvectors um and i should say that all eigenvector", "tokens": [31864, 27290, 1228, 10446, 46033, 293, 10446, 303, 5547, 1105, 293, 741, 820, 584, 300, 439, 10446, 303, 1672], "temperature": 0.0, "avg_logprob": -0.11132491320029073, "compression_ratio": 1.8775510204081634, "no_speech_prob": 2.5840448870440014e-05}, {"id": 347, "seek": 250188, "start": 2516.12, "end": 2521.7200000000003, "text": " values less than one is sufficient but not necessary condition for what i'm about to say um", "tokens": [4190, 1570, 813, 472, 307, 11563, 457, 406, 4818, 4188, 337, 437, 741, 478, 466, 281, 584, 1105], "temperature": 0.0, "avg_logprob": -0.11132491320029073, "compression_ratio": 1.8775510204081634, "no_speech_prob": 2.5840448870440014e-05}, {"id": 348, "seek": 250188, "start": 2521.7200000000003, "end": 2530.04, "text": " right so we can rewrite um things using the eigenvectors as a basis and if we do that um", "tokens": [558, 370, 321, 393, 28132, 1105, 721, 1228, 264, 10446, 303, 5547, 382, 257, 5143, 293, 498, 321, 360, 300, 1105], "temperature": 0.0, "avg_logprob": -0.11132491320029073, "compression_ratio": 1.8775510204081634, "no_speech_prob": 2.5840448870440014e-05}, {"id": 349, "seek": 253004, "start": 2530.04, "end": 2540.36, "text": " um we end up getting um the eigenvalues being raised to the elf power and so if all of our eigen", "tokens": [1105, 321, 917, 493, 1242, 1105, 264, 10446, 46033, 885, 6005, 281, 264, 35565, 1347, 293, 370, 498, 439, 295, 527, 10446], "temperature": 0.0, "avg_logprob": -0.07242893904782413, "compression_ratio": 1.7314814814814814, "no_speech_prob": 1.8338221707381308e-05}, {"id": 350, "seek": 253004, "start": 2540.36, "end": 2546.52, "text": " values are less than one if we're taking a number less than one um and raising it to the elf power", "tokens": [4190, 366, 1570, 813, 472, 498, 321, 434, 1940, 257, 1230, 1570, 813, 472, 1105, 293, 11225, 309, 281, 264, 35565, 1347], "temperature": 0.0, "avg_logprob": -0.07242893904782413, "compression_ratio": 1.7314814814814814, "no_speech_prob": 1.8338221707381308e-05}, {"id": 351, "seek": 253004, "start": 2546.52, "end": 2552.68, "text": " that's going to approach zero as the sequence length grows and so the gradient vanishes", "tokens": [300, 311, 516, 281, 3109, 4018, 382, 264, 8310, 4641, 13156, 293, 370, 264, 16235, 3161, 16423], "temperature": 0.0, "avg_logprob": -0.07242893904782413, "compression_ratio": 1.7314814814814814, "no_speech_prob": 1.8338221707381308e-05}, {"id": 352, "seek": 253004, "start": 2553.72, "end": 2559.08, "text": " okay now the reality is more complex than that because actually we always use a non-linear", "tokens": [1392, 586, 264, 4103, 307, 544, 3997, 813, 300, 570, 767, 321, 1009, 764, 257, 2107, 12, 28263], "temperature": 0.0, "avg_logprob": -0.07242893904782413, "compression_ratio": 1.7314814814814814, "no_speech_prob": 1.8338221707381308e-05}, {"id": 353, "seek": 255908, "start": 2559.08, "end": 2565.96, "text": " activation sigma but you know in principle it's sort of the same thing um apart from we have to", "tokens": [24433, 12771, 457, 291, 458, 294, 8665, 309, 311, 1333, 295, 264, 912, 551, 1105, 4936, 490, 321, 362, 281], "temperature": 0.0, "avg_logprob": -0.09981733293675665, "compression_ratio": 1.5901639344262295, "no_speech_prob": 2.543508162489161e-05}, {"id": 354, "seek": 255908, "start": 2565.96, "end": 2574.44, "text": " consider in the effect of the non-linear activation okay so why is this a problem that the gradients", "tokens": [1949, 294, 264, 1802, 295, 264, 2107, 12, 28263, 24433, 1392, 370, 983, 307, 341, 257, 1154, 300, 264, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.09981733293675665, "compression_ratio": 1.5901639344262295, "no_speech_prob": 2.543508162489161e-05}, {"id": 355, "seek": 255908, "start": 2574.44, "end": 2581.88, "text": " disappear well suppose we're wanting to look at the influence of time steps well in the future", "tokens": [11596, 731, 7297, 321, 434, 7935, 281, 574, 412, 264, 6503, 295, 565, 4439, 731, 294, 264, 2027], "temperature": 0.0, "avg_logprob": -0.09981733293675665, "compression_ratio": 1.5901639344262295, "no_speech_prob": 2.543508162489161e-05}, {"id": 356, "seek": 258188, "start": 2581.88, "end": 2591.32, "text": " uh on um the representations we want to have early in the sentence well um what's happening", "tokens": [2232, 322, 1105, 264, 33358, 321, 528, 281, 362, 2440, 294, 264, 8174, 731, 1105, 437, 311, 2737], "temperature": 0.0, "avg_logprob": -0.07893453538417816, "compression_ratio": 1.670731707317073, "no_speech_prob": 4.0674261981621385e-05}, {"id": 357, "seek": 258188, "start": 2591.32, "end": 2596.84, "text": " late in the sentence just isn't going to be giving much information about what we should", "tokens": [3469, 294, 264, 8174, 445, 1943, 380, 516, 281, 312, 2902, 709, 1589, 466, 437, 321, 820], "temperature": 0.0, "avg_logprob": -0.07893453538417816, "compression_ratio": 1.670731707317073, "no_speech_prob": 4.0674261981621385e-05}, {"id": 358, "seek": 258188, "start": 2596.84, "end": 2605.96, "text": " be storing in the h at time one vector whereas on the other hand the loss at time step two is", "tokens": [312, 26085, 294, 264, 276, 412, 565, 472, 8062, 9735, 322, 264, 661, 1011, 264, 4470, 412, 565, 1823, 732, 307], "temperature": 0.0, "avg_logprob": -0.07893453538417816, "compression_ratio": 1.670731707317073, "no_speech_prob": 4.0674261981621385e-05}, {"id": 359, "seek": 260596, "start": 2605.96, "end": 2612.44, "text": " going to be giving a lot of information at what um should be stored in the hidden vector at time step", "tokens": [516, 281, 312, 2902, 257, 688, 295, 1589, 412, 437, 1105, 820, 312, 12187, 294, 264, 7633, 8062, 412, 565, 1823], "temperature": 0.0, "avg_logprob": -0.06738291768466725, "compression_ratio": 1.6514285714285715, "no_speech_prob": 6.4375981310149655e-06}, {"id": 360, "seek": 260596, "start": 2612.44, "end": 2622.6, "text": " one so the end result of that is that what happens is that these simple RNNs are very good at", "tokens": [472, 370, 264, 917, 1874, 295, 300, 307, 300, 437, 2314, 307, 300, 613, 2199, 45702, 45, 82, 366, 588, 665, 412], "temperature": 0.0, "avg_logprob": -0.06738291768466725, "compression_ratio": 1.6514285714285715, "no_speech_prob": 6.4375981310149655e-06}, {"id": 361, "seek": 260596, "start": 2622.6, "end": 2630.2, "text": " modeling nearby effects but they're not good at all at modeling long term effects because the", "tokens": [15983, 11184, 5065, 457, 436, 434, 406, 665, 412, 439, 412, 15983, 938, 1433, 5065, 570, 264], "temperature": 0.0, "avg_logprob": -0.06738291768466725, "compression_ratio": 1.6514285714285715, "no_speech_prob": 6.4375981310149655e-06}, {"id": 362, "seek": 263020, "start": 2630.2, "end": 2638.3599999999997, "text": " gradient signal from far away is just lost too much and therefore the model never effectively gets", "tokens": [16235, 6358, 490, 1400, 1314, 307, 445, 2731, 886, 709, 293, 4412, 264, 2316, 1128, 8659, 2170], "temperature": 0.0, "avg_logprob": -0.06493059978928677, "compression_ratio": 1.6991150442477876, "no_speech_prob": 1.1455435924290214e-05}, {"id": 363, "seek": 263020, "start": 2638.3599999999997, "end": 2646.3599999999997, "text": " to learn um what information from far away it would be useful to preserve into the future so let's", "tokens": [281, 1466, 1105, 437, 1589, 490, 1400, 1314, 309, 576, 312, 4420, 281, 15665, 666, 264, 2027, 370, 718, 311], "temperature": 0.0, "avg_logprob": -0.06493059978928677, "compression_ratio": 1.6991150442477876, "no_speech_prob": 1.1455435924290214e-05}, {"id": 364, "seek": 263020, "start": 2646.3599999999997, "end": 2653.8799999999997, "text": " consider that concretely um for the example of language models that we've worked on so here's", "tokens": [1949, 300, 39481, 736, 1105, 337, 264, 1365, 295, 2856, 5245, 300, 321, 600, 2732, 322, 370, 510, 311], "temperature": 0.0, "avg_logprob": -0.06493059978928677, "compression_ratio": 1.6991150442477876, "no_speech_prob": 1.1455435924290214e-05}, {"id": 365, "seek": 263020, "start": 2653.8799999999997, "end": 2659.56, "text": " a piece of text um when she tried to print her tickets she found that the printer was out of", "tokens": [257, 2522, 295, 2487, 1105, 562, 750, 3031, 281, 4482, 720, 12628, 750, 1352, 300, 264, 16671, 390, 484, 295], "temperature": 0.0, "avg_logprob": -0.06493059978928677, "compression_ratio": 1.6991150442477876, "no_speech_prob": 1.1455435924290214e-05}, {"id": 366, "seek": 265956, "start": 2659.56, "end": 2665.96, "text": " toner she went to the stationery store to buy more toner it was very overpriced after installing", "tokens": [40403, 750, 1437, 281, 264, 5214, 2109, 3531, 281, 2256, 544, 40403, 309, 390, 588, 670, 79, 1341, 292, 934, 20762], "temperature": 0.0, "avg_logprob": -0.0861075442770253, "compression_ratio": 1.719298245614035, "no_speech_prob": 1.3413934539130423e-05}, {"id": 367, "seek": 265956, "start": 2665.96, "end": 2672.6, "text": " the toner into the printer she finally printed her and well you're all smart human beings i trust", "tokens": [264, 40403, 666, 264, 16671, 750, 2721, 13567, 720, 293, 731, 291, 434, 439, 4069, 1952, 8958, 741, 3361], "temperature": 0.0, "avg_logprob": -0.0861075442770253, "compression_ratio": 1.719298245614035, "no_speech_prob": 1.3413934539130423e-05}, {"id": 368, "seek": 265956, "start": 2672.6, "end": 2680.52, "text": " you can all guess what the word that comes next is it should be tickets um but well the problem is", "tokens": [291, 393, 439, 2041, 437, 264, 1349, 300, 1487, 958, 307, 309, 820, 312, 12628, 1105, 457, 731, 264, 1154, 307], "temperature": 0.0, "avg_logprob": -0.0861075442770253, "compression_ratio": 1.719298245614035, "no_speech_prob": 1.3413934539130423e-05}, {"id": 369, "seek": 265956, "start": 2680.52, "end": 2687.96, "text": " that for the RNN start to learn cases like this it would have to carry through in its hidden state", "tokens": [300, 337, 264, 45702, 45, 722, 281, 1466, 3331, 411, 341, 309, 576, 362, 281, 3985, 807, 294, 1080, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.0861075442770253, "compression_ratio": 1.719298245614035, "no_speech_prob": 1.3413934539130423e-05}, {"id": 370, "seek": 268796, "start": 2687.96, "end": 2696.68, "text": " a memory of the word tickets for sort of whatever it is about 30 hidden state updates and well", "tokens": [257, 4675, 295, 264, 1349, 12628, 337, 1333, 295, 2035, 309, 307, 466, 2217, 7633, 1785, 9205, 293, 731], "temperature": 0.0, "avg_logprob": -0.08031923874564793, "compression_ratio": 1.5698924731182795, "no_speech_prob": 8.870264719007537e-05}, {"id": 371, "seek": 268796, "start": 2696.68, "end": 2704.84, "text": " we'll train on this um example and so we'll be wanting it to predict tickets um is the next word", "tokens": [321, 603, 3847, 322, 341, 1105, 1365, 293, 370, 321, 603, 312, 7935, 309, 281, 6069, 12628, 1105, 307, 264, 958, 1349], "temperature": 0.0, "avg_logprob": -0.08031923874564793, "compression_ratio": 1.5698924731182795, "no_speech_prob": 8.870264719007537e-05}, {"id": 372, "seek": 268796, "start": 2704.84, "end": 2712.12, "text": " and so a gradient update will be sent right back through the hidden states of the LSTM corresponding", "tokens": [293, 370, 257, 16235, 5623, 486, 312, 2279, 558, 646, 807, 264, 7633, 4368, 295, 264, 441, 6840, 44, 11760], "temperature": 0.0, "avg_logprob": -0.08031923874564793, "compression_ratio": 1.5698924731182795, "no_speech_prob": 8.870264719007537e-05}, {"id": 373, "seek": 271212, "start": 2712.12, "end": 2720.2, "text": " to this sentence and that should tell the model um is good to preserve information about the word", "tokens": [281, 341, 8174, 293, 300, 820, 980, 264, 2316, 1105, 307, 665, 281, 15665, 1589, 466, 264, 1349], "temperature": 0.0, "avg_logprob": -0.07657065672032973, "compression_ratio": 1.7685185185185186, "no_speech_prob": 2.6976849767379463e-05}, {"id": 374, "seek": 271212, "start": 2720.2, "end": 2725.56, "text": " tickets because that might be useful in the future here it was useful in the future um but the", "tokens": [12628, 570, 300, 1062, 312, 4420, 294, 264, 2027, 510, 309, 390, 4420, 294, 264, 2027, 1105, 457, 264], "temperature": 0.0, "avg_logprob": -0.07657065672032973, "compression_ratio": 1.7685185185185186, "no_speech_prob": 2.6976849767379463e-05}, {"id": 375, "seek": 271212, "start": 2725.56, "end": 2733.16, "text": " problem is that the gradient signal will just become far too weak out after a bunch of words", "tokens": [1154, 307, 300, 264, 16235, 6358, 486, 445, 1813, 1400, 886, 5336, 484, 934, 257, 3840, 295, 2283], "temperature": 0.0, "avg_logprob": -0.07657065672032973, "compression_ratio": 1.7685185185185186, "no_speech_prob": 2.6976849767379463e-05}, {"id": 376, "seek": 271212, "start": 2733.16, "end": 2741.08, "text": " and it just never learns that dependency um and so what we find in practice is the model is just", "tokens": [293, 309, 445, 1128, 27152, 300, 33621, 1105, 293, 370, 437, 321, 915, 294, 3124, 307, 264, 2316, 307, 445], "temperature": 0.0, "avg_logprob": -0.07657065672032973, "compression_ratio": 1.7685185185185186, "no_speech_prob": 2.6976849767379463e-05}, {"id": 377, "seek": 274108, "start": 2741.08, "end": 2748.2, "text": " unable to predict similar long distance dependencies at test time i spent quite a long time on", "tokens": [11299, 281, 6069, 2531, 938, 4560, 36606, 412, 1500, 565, 741, 4418, 1596, 257, 938, 565, 322], "temperature": 0.0, "avg_logprob": -0.1205942199890872, "compression_ratio": 1.8037383177570094, "no_speech_prob": 2.1749341613030992e-05}, {"id": 378, "seek": 274108, "start": 2748.2, "end": 2755.88, "text": " vanishing gradients and and really vanishing gradients are the big problem in practice um with using", "tokens": [3161, 3807, 2771, 2448, 293, 293, 534, 3161, 3807, 2771, 2448, 366, 264, 955, 1154, 294, 3124, 1105, 365, 1228], "temperature": 0.0, "avg_logprob": -0.1205942199890872, "compression_ratio": 1.8037383177570094, "no_speech_prob": 2.1749341613030992e-05}, {"id": 379, "seek": 274108, "start": 2757.7999999999997, "end": 2764.2, "text": " recurrent neural networks over long sequences um but you know i have to do justice to the fact", "tokens": [18680, 1753, 18161, 9590, 670, 938, 22978, 1105, 457, 291, 458, 741, 362, 281, 360, 6118, 281, 264, 1186], "temperature": 0.0, "avg_logprob": -0.1205942199890872, "compression_ratio": 1.8037383177570094, "no_speech_prob": 2.1749341613030992e-05}, {"id": 380, "seek": 274108, "start": 2764.2, "end": 2769.72, "text": " that you could actually also have the opposite problem you can also have exploding gradients so", "tokens": [300, 291, 727, 767, 611, 362, 264, 6182, 1154, 291, 393, 611, 362, 35175, 2771, 2448, 370], "temperature": 0.0, "avg_logprob": -0.1205942199890872, "compression_ratio": 1.8037383177570094, "no_speech_prob": 2.1749341613030992e-05}, {"id": 381, "seek": 276972, "start": 2769.72, "end": 2779.3999999999996, "text": " if a gradient becomes too big that's also a problem and it's a problem because the secastic gradient", "tokens": [498, 257, 16235, 3643, 886, 955, 300, 311, 611, 257, 1154, 293, 309, 311, 257, 1154, 570, 264, 907, 2750, 16235], "temperature": 0.0, "avg_logprob": -0.11334473746163505, "compression_ratio": 1.763975155279503, "no_speech_prob": 1.6434933058917522e-05}, {"id": 382, "seek": 276972, "start": 2780.04, "end": 2788.52, "text": " update step becomes too big right so remember our parameter update is um based on the product", "tokens": [5623, 1823, 3643, 886, 955, 558, 370, 1604, 527, 13075, 5623, 307, 1105, 2361, 322, 264, 1674], "temperature": 0.0, "avg_logprob": -0.11334473746163505, "compression_ratio": 1.763975155279503, "no_speech_prob": 1.6434933058917522e-05}, {"id": 383, "seek": 276972, "start": 2788.52, "end": 2794.12, "text": " of the learning rate and the gradient so if your gradient is huge right you've calculated", "tokens": [295, 264, 2539, 3314, 293, 264, 16235, 370, 498, 428, 16235, 307, 2603, 558, 291, 600, 15598], "temperature": 0.0, "avg_logprob": -0.11334473746163505, "compression_ratio": 1.763975155279503, "no_speech_prob": 1.6434933058917522e-05}, {"id": 384, "seek": 279412, "start": 2794.12, "end": 2802.6, "text": " oh it's got a lot of slope here this has a slope of 10,000 um then your parameter update can be", "tokens": [1954, 309, 311, 658, 257, 688, 295, 13525, 510, 341, 575, 257, 13525, 295, 1266, 11, 1360, 1105, 550, 428, 13075, 5623, 393, 312], "temperature": 0.0, "avg_logprob": -0.0709710172984911, "compression_ratio": 1.709090909090909, "no_speech_prob": 7.886775165388826e-06}, {"id": 385, "seek": 279412, "start": 2802.6, "end": 2809.96, "text": " arbitrarily large and that's potentially problematic that can cause a bad update where you take a", "tokens": [19071, 3289, 2416, 293, 300, 311, 7263, 19011, 300, 393, 3082, 257, 1578, 5623, 689, 291, 747, 257], "temperature": 0.0, "avg_logprob": -0.0709710172984911, "compression_ratio": 1.709090909090909, "no_speech_prob": 7.886775165388826e-06}, {"id": 386, "seek": 279412, "start": 2809.96, "end": 2817.3199999999997, "text": " huge step and you end up at a weird and bad parameter configuration so you sort of think", "tokens": [2603, 1823, 293, 291, 917, 493, 412, 257, 3657, 293, 1578, 13075, 11694, 370, 291, 1333, 295, 519], "temperature": 0.0, "avg_logprob": -0.0709710172984911, "compression_ratio": 1.709090909090909, "no_speech_prob": 7.886775165388826e-06}, {"id": 387, "seek": 279412, "start": 2817.3199999999997, "end": 2823.48, "text": " you're coming up with a to a steep hill to climb and well you want to be climbing the hill to", "tokens": [291, 434, 1348, 493, 365, 257, 281, 257, 16841, 10997, 281, 10724, 293, 731, 291, 528, 281, 312, 14780, 264, 10997, 281], "temperature": 0.0, "avg_logprob": -0.0709710172984911, "compression_ratio": 1.709090909090909, "no_speech_prob": 7.886775165388826e-06}, {"id": 388, "seek": 282348, "start": 2823.48, "end": 2831.8, "text": " high likelihood but actually the gradient is so steep that you make an enormous um update and then", "tokens": [1090, 22119, 457, 767, 264, 16235, 307, 370, 16841, 300, 291, 652, 364, 11322, 1105, 5623, 293, 550], "temperature": 0.0, "avg_logprob": -0.11701679229736328, "compression_ratio": 1.6608695652173913, "no_speech_prob": 3.305310747236945e-05}, {"id": 389, "seek": 282348, "start": 2831.8, "end": 2836.92, "text": " suddenly your parameters are over an IOR and you've lost your hill altogether there's also the", "tokens": [5800, 428, 9834, 366, 670, 364, 286, 2483, 293, 291, 600, 2731, 428, 10997, 19051, 456, 311, 611, 264], "temperature": 0.0, "avg_logprob": -0.11701679229736328, "compression_ratio": 1.6608695652173913, "no_speech_prob": 3.305310747236945e-05}, {"id": 390, "seek": 282348, "start": 2836.92, "end": 2841.88, "text": " practical differently that we only have so much resolution now floating point numbers um so", "tokens": [8496, 7614, 300, 321, 787, 362, 370, 709, 8669, 586, 12607, 935, 3547, 1105, 370], "temperature": 0.0, "avg_logprob": -0.11701679229736328, "compression_ratio": 1.6608695652173913, "no_speech_prob": 3.305310747236945e-05}, {"id": 391, "seek": 282348, "start": 2842.68, "end": 2849.08, "text": " if your gradient gets too steep um you start getting um not a numbers in your calculations which", "tokens": [498, 428, 16235, 2170, 886, 16841, 1105, 291, 722, 1242, 1105, 406, 257, 3547, 294, 428, 20448, 597], "temperature": 0.0, "avg_logprob": -0.11701679229736328, "compression_ratio": 1.6608695652173913, "no_speech_prob": 3.305310747236945e-05}, {"id": 392, "seek": 284908, "start": 2849.08, "end": 2856.68, "text": " ruin all your hard training work um we use a kind of an easy fix to this which is called gradient", "tokens": [15514, 439, 428, 1152, 3097, 589, 1105, 321, 764, 257, 733, 295, 364, 1858, 3191, 281, 341, 597, 307, 1219, 16235], "temperature": 0.0, "avg_logprob": -0.13432427247365317, "compression_ratio": 1.6978723404255318, "no_speech_prob": 3.2130057661561295e-05}, {"id": 393, "seek": 284908, "start": 2856.68, "end": 2864.7599999999998, "text": " clipping um which is we choose some reasonable number and we say we're just not going to deal with", "tokens": [49320, 1105, 597, 307, 321, 2826, 512, 10585, 1230, 293, 321, 584, 321, 434, 445, 406, 516, 281, 2028, 365], "temperature": 0.0, "avg_logprob": -0.13432427247365317, "compression_ratio": 1.6978723404255318, "no_speech_prob": 3.2130057661561295e-05}, {"id": 394, "seek": 284908, "start": 2864.7599999999998, "end": 2870.7599999999998, "text": " gradients that bigger than this number um a commonly used number is 20 you know some the thing that's", "tokens": [2771, 2448, 300, 3801, 813, 341, 1230, 1105, 257, 12719, 1143, 1230, 307, 945, 291, 458, 512, 264, 551, 300, 311], "temperature": 0.0, "avg_logprob": -0.13432427247365317, "compression_ratio": 1.6978723404255318, "no_speech_prob": 3.2130057661561295e-05}, {"id": 395, "seek": 284908, "start": 2870.7599999999998, "end": 2876.92, "text": " got a range of spread but not that high you know you can use 10,000 some where sort of in that range", "tokens": [658, 257, 3613, 295, 3974, 457, 406, 300, 1090, 291, 458, 291, 393, 764, 1266, 11, 1360, 512, 689, 1333, 295, 294, 300, 3613], "temperature": 0.0, "avg_logprob": -0.13432427247365317, "compression_ratio": 1.6978723404255318, "no_speech_prob": 3.2130057661561295e-05}, {"id": 396, "seek": 287692, "start": 2876.92, "end": 2885.16, "text": " um and if the norm of the gradient is greater than that threshold we simply just scale it down", "tokens": [1105, 293, 498, 264, 2026, 295, 264, 16235, 307, 5044, 813, 300, 14678, 321, 2935, 445, 4373, 309, 760], "temperature": 0.0, "avg_logprob": -0.15080855786800385, "compression_ratio": 1.6457142857142857, "no_speech_prob": 2.5048648240044713e-05}, {"id": 397, "seek": 287692, "start": 2885.16, "end": 2892.92, "text": " which means that we then make a smaller gradient update so we're still moving in exactly the same", "tokens": [597, 1355, 300, 321, 550, 652, 257, 4356, 16235, 5623, 370, 321, 434, 920, 2684, 294, 2293, 264, 912], "temperature": 0.0, "avg_logprob": -0.15080855786800385, "compression_ratio": 1.6457142857142857, "no_speech_prob": 2.5048648240044713e-05}, {"id": 398, "seek": 287692, "start": 2892.92, "end": 2900.52, "text": " um direction but we're taking a smaller step um so doing this gradient clipping is important um", "tokens": [1105, 3513, 457, 321, 434, 1940, 257, 4356, 1823, 1105, 370, 884, 341, 16235, 49320, 307, 1021, 1105], "temperature": 0.0, "avg_logprob": -0.15080855786800385, "compression_ratio": 1.6457142857142857, "no_speech_prob": 2.5048648240044713e-05}, {"id": 399, "seek": 290052, "start": 2900.52, "end": 2911.48, "text": " um you know but it's an easy problem to solve okay um so the thing that we've still got left to", "tokens": [1105, 291, 458, 457, 309, 311, 364, 1858, 1154, 281, 5039, 1392, 1105, 370, 264, 551, 300, 321, 600, 920, 658, 1411, 281], "temperature": 0.0, "avg_logprob": -0.07578460029933763, "compression_ratio": 1.6022727272727273, "no_speech_prob": 8.009969860722777e-06}, {"id": 400, "seek": 290052, "start": 2911.48, "end": 2922.04, "text": " solve is how to really solve this problem of vanishing gradients um so the problem is yeah these", "tokens": [5039, 307, 577, 281, 534, 5039, 341, 1154, 295, 3161, 3807, 2771, 2448, 1105, 370, 264, 1154, 307, 1338, 613], "temperature": 0.0, "avg_logprob": -0.07578460029933763, "compression_ratio": 1.6022727272727273, "no_speech_prob": 8.009969860722777e-06}, {"id": 401, "seek": 292204, "start": 2922.04, "end": 2931.96, "text": " RNNs just can't preserve information over many time steps and one way to think about that intuitively", "tokens": [45702, 45, 82, 445, 393, 380, 15665, 1589, 670, 867, 565, 4439, 293, 472, 636, 281, 519, 466, 300, 46506], "temperature": 0.0, "avg_logprob": -0.07077772689588142, "compression_ratio": 1.6783625730994152, "no_speech_prob": 5.011012945033144e-06}, {"id": 402, "seek": 292204, "start": 2931.96, "end": 2941.08, "text": " is at each time step we have a hidden state and the hidden state is being completely changed", "tokens": [307, 412, 1184, 565, 1823, 321, 362, 257, 7633, 1785, 293, 264, 7633, 1785, 307, 885, 2584, 3105], "temperature": 0.0, "avg_logprob": -0.07077772689588142, "compression_ratio": 1.6783625730994152, "no_speech_prob": 5.011012945033144e-06}, {"id": 403, "seek": 292204, "start": 2941.08, "end": 2948.6, "text": " at each time step and it's being changed in a multiplicative manner by multiplying by wh and", "tokens": [412, 1184, 565, 1823, 293, 309, 311, 885, 3105, 294, 257, 17596, 1166, 9060, 538, 30955, 538, 261, 71, 293], "temperature": 0.0, "avg_logprob": -0.07077772689588142, "compression_ratio": 1.6783625730994152, "no_speech_prob": 5.011012945033144e-06}, {"id": 404, "seek": 294860, "start": 2948.6, "end": 2958.68, "text": " then putting it through um and nonlinearity like maybe we can make some more progress um if we", "tokens": [550, 3372, 309, 807, 1105, 293, 2107, 1889, 17409, 411, 1310, 321, 393, 652, 512, 544, 4205, 1105, 498, 321], "temperature": 0.0, "avg_logprob": -0.1049420586947737, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.0122340427187737e-05}, {"id": 405, "seek": 294860, "start": 2958.68, "end": 2966.6, "text": " could more flexibly maintain a memory in our recurrent neural network which we can", "tokens": [727, 544, 5896, 3545, 6909, 257, 4675, 294, 527, 18680, 1753, 18161, 3209, 597, 321, 393], "temperature": 0.0, "avg_logprob": -0.1049420586947737, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.0122340427187737e-05}, {"id": 406, "seek": 294860, "start": 2967.3199999999997, "end": 2974.12, "text": " manipulate in a more flexible manner that allows us to more easily preserve information", "tokens": [20459, 294, 257, 544, 11358, 9060, 300, 4045, 505, 281, 544, 3612, 15665, 1589], "temperature": 0.0, "avg_logprob": -0.1049420586947737, "compression_ratio": 1.606060606060606, "no_speech_prob": 1.0122340427187737e-05}, {"id": 407, "seek": 297412, "start": 2974.12, "end": 2981.08, "text": " and so this was an idea that people started thinking about and actually they started thinking", "tokens": [293, 370, 341, 390, 364, 1558, 300, 561, 1409, 1953, 466, 293, 767, 436, 1409, 1953], "temperature": 0.0, "avg_logprob": -0.19897276825375027, "compression_ratio": 1.5561497326203209, "no_speech_prob": 3.938250301871449e-05}, {"id": 408, "seek": 297412, "start": 2981.08, "end": 2991.64, "text": " about it a long time ago um in the late 1990s um and Huck Rydon Schmitt Hoover came up with this", "tokens": [466, 309, 257, 938, 565, 2057, 1105, 294, 264, 3469, 13384, 82, 1105, 293, 389, 1134, 497, 6655, 266, 2065, 15548, 46382, 1361, 493, 365, 341], "temperature": 0.0, "avg_logprob": -0.19897276825375027, "compression_ratio": 1.5561497326203209, "no_speech_prob": 3.938250301871449e-05}, {"id": 409, "seek": 297412, "start": 2991.64, "end": 3000.52, "text": " idea that got called long short term memory RNNs as a solution to the problem of vanishing gradients", "tokens": [1558, 300, 658, 1219, 938, 2099, 1433, 4675, 45702, 45, 82, 382, 257, 3827, 281, 264, 1154, 295, 3161, 3807, 2771, 2448], "temperature": 0.0, "avg_logprob": -0.19897276825375027, "compression_ratio": 1.5561497326203209, "no_speech_prob": 3.938250301871449e-05}, {"id": 410, "seek": 300052, "start": 3000.52, "end": 3008.7599999999998, "text": " I mean so this 1997 paper is the paper you always see cited for LSTM's but you know actually", "tokens": [286, 914, 370, 341, 22383, 3035, 307, 264, 3035, 291, 1009, 536, 30134, 337, 441, 6840, 44, 311, 457, 291, 458, 767], "temperature": 0.0, "avg_logprob": -0.12164424595079924, "compression_ratio": 1.53475935828877, "no_speech_prob": 6.705090345349163e-05}, {"id": 411, "seek": 300052, "start": 3009.4, "end": 3016.7599999999998, "text": " in terms of what we now understand as an LSTM um it was missing part of it in fact it's missing", "tokens": [294, 2115, 295, 437, 321, 586, 1223, 382, 364, 441, 6840, 44, 1105, 309, 390, 5361, 644, 295, 309, 294, 1186, 309, 311, 5361], "temperature": 0.0, "avg_logprob": -0.12164424595079924, "compression_ratio": 1.53475935828877, "no_speech_prob": 6.705090345349163e-05}, {"id": 412, "seek": 300052, "start": 3016.7599999999998, "end": 3024.6, "text": " what in retrospect has turned out to be the most important part of um the um modern LSTM so really", "tokens": [437, 294, 34997, 575, 3574, 484, 281, 312, 264, 881, 1021, 644, 295, 1105, 264, 1105, 4363, 441, 6840, 44, 370, 534], "temperature": 0.0, "avg_logprob": -0.12164424595079924, "compression_ratio": 1.53475935828877, "no_speech_prob": 6.705090345349163e-05}, {"id": 413, "seek": 302460, "start": 3024.6, "end": 3032.52, "text": " in some sense the real paper that the modern LSTM is due to is this slightly later paper by", "tokens": [294, 512, 2020, 264, 957, 3035, 300, 264, 4363, 441, 6840, 44, 307, 3462, 281, 307, 341, 4748, 1780, 3035, 538], "temperature": 0.0, "avg_logprob": -0.1797980432925017, "compression_ratio": 1.4591836734693877, "no_speech_prob": 2.5351468138978817e-05}, {"id": 414, "seek": 302460, "start": 3032.52, "end": 3038.6, "text": " Gerst, still Schmitt Hoover and Cummins from 2000 which additionally introduces the forget gate", "tokens": [9409, 372, 11, 920, 2065, 15548, 46382, 293, 26337, 76, 1292, 490, 8132, 597, 43181, 31472, 264, 2870, 8539], "temperature": 0.0, "avg_logprob": -0.1797980432925017, "compression_ratio": 1.4591836734693877, "no_speech_prob": 2.5351468138978817e-05}, {"id": 415, "seek": 302460, "start": 3038.6, "end": 3048.2799999999997, "text": " that I'll explain in a minute um yeah so um so this was some very clever stuff that was introduced", "tokens": [300, 286, 603, 2903, 294, 257, 3456, 1105, 1338, 370, 1105, 370, 341, 390, 512, 588, 13494, 1507, 300, 390, 7268], "temperature": 0.0, "avg_logprob": -0.1797980432925017, "compression_ratio": 1.4591836734693877, "no_speech_prob": 2.5351468138978817e-05}, {"id": 416, "seek": 304828, "start": 3048.28, "end": 3056.0400000000004, "text": " and it turned out later to have an enormous impact um if I just diverge from the technical part", "tokens": [293, 309, 3574, 484, 1780, 281, 362, 364, 11322, 2712, 1105, 498, 286, 445, 18558, 432, 490, 264, 6191, 644], "temperature": 0.0, "avg_logprob": -0.07389714752418408, "compression_ratio": 1.6590909090909092, "no_speech_prob": 4.057697515236214e-05}, {"id": 417, "seek": 304828, "start": 3056.0400000000004, "end": 3064.36, "text": " for one more moment um that you know for those of you who these days um think that mastering your", "tokens": [337, 472, 544, 1623, 1105, 300, 291, 458, 337, 729, 295, 291, 567, 613, 1708, 1105, 519, 300, 49382, 428], "temperature": 0.0, "avg_logprob": -0.07389714752418408, "compression_ratio": 1.6590909090909092, "no_speech_prob": 4.057697515236214e-05}, {"id": 418, "seek": 304828, "start": 3064.36, "end": 3071.96, "text": " networks is the path to fame and fortune um the funny thing is you know at the time that this work", "tokens": [9590, 307, 264, 3100, 281, 16874, 293, 16531, 1105, 264, 4074, 551, 307, 291, 458, 412, 264, 565, 300, 341, 589], "temperature": 0.0, "avg_logprob": -0.07389714752418408, "compression_ratio": 1.6590909090909092, "no_speech_prob": 4.057697515236214e-05}, {"id": 419, "seek": 307196, "start": 3071.96, "end": 3080.52, "text": " was done that just was not true right very few people were interested in neural networks and", "tokens": [390, 1096, 300, 445, 390, 406, 2074, 558, 588, 1326, 561, 645, 3102, 294, 18161, 9590, 293], "temperature": 0.0, "avg_logprob": -0.07890456682675845, "compression_ratio": 1.6212765957446809, "no_speech_prob": 5.055776273366064e-05}, {"id": 420, "seek": 307196, "start": 3080.52, "end": 3086.04, "text": " although long short term memories have turned out to be one of the most important successful and", "tokens": [4878, 938, 2099, 1433, 8495, 362, 3574, 484, 281, 312, 472, 295, 264, 881, 1021, 4406, 293], "temperature": 0.0, "avg_logprob": -0.07890456682675845, "compression_ratio": 1.6212765957446809, "no_speech_prob": 5.055776273366064e-05}, {"id": 421, "seek": 307196, "start": 3086.04, "end": 3094.76, "text": " influential ideas in neural networks for the following 25 years um really the original authors", "tokens": [22215, 3487, 294, 18161, 9590, 337, 264, 3480, 3552, 924, 1105, 534, 264, 3380, 16552], "temperature": 0.0, "avg_logprob": -0.07890456682675845, "compression_ratio": 1.6212765957446809, "no_speech_prob": 5.055776273366064e-05}, {"id": 422, "seek": 307196, "start": 3094.76, "end": 3101.7200000000003, "text": " didn't get recognition for that so both of them are now professors at German universities um but", "tokens": [994, 380, 483, 11150, 337, 300, 370, 1293, 295, 552, 366, 586, 15924, 412, 6521, 11779, 1105, 457], "temperature": 0.0, "avg_logprob": -0.07890456682675845, "compression_ratio": 1.6212765957446809, "no_speech_prob": 5.055776273366064e-05}, {"id": 423, "seek": 310172, "start": 3101.72, "end": 3111.24, "text": " Hock Rider um moved over into doing bioinformatics work um to find um something to do and", "tokens": [389, 1560, 40150, 1105, 4259, 670, 666, 884, 12198, 37811, 30292, 589, 1105, 281, 915, 1105, 746, 281, 360, 293], "temperature": 0.0, "avg_logprob": -0.1282943307536922, "compression_ratio": 1.530054644808743, "no_speech_prob": 2.706077430048026e-05}, {"id": 424, "seek": 310172, "start": 3111.24, "end": 3119.16, "text": " Gerst actually is doing kind of multimedia studies um so um that's the fates of history um", "tokens": [9409, 372, 767, 307, 884, 733, 295, 49202, 5313, 1105, 370, 1105, 300, 311, 264, 283, 1024, 295, 2503, 1105], "temperature": 0.0, "avg_logprob": -0.1282943307536922, "compression_ratio": 1.530054644808743, "no_speech_prob": 2.706077430048026e-05}, {"id": 425, "seek": 310172, "start": 3120.4399999999996, "end": 3131.3199999999997, "text": " okay so what is an LSTM so the crucial innovation of an LSTM is to say well rather than just having", "tokens": [1392, 370, 437, 307, 364, 441, 6840, 44, 370, 264, 11462, 8504, 295, 364, 441, 6840, 44, 307, 281, 584, 731, 2831, 813, 445, 1419], "temperature": 0.0, "avg_logprob": -0.1282943307536922, "compression_ratio": 1.530054644808743, "no_speech_prob": 2.706077430048026e-05}, {"id": 426, "seek": 313132, "start": 3131.32, "end": 3142.92, "text": " one hidden vector in the recurrent model we're going to um build a model with two um hidden vectors", "tokens": [472, 7633, 8062, 294, 264, 18680, 1753, 2316, 321, 434, 516, 281, 1105, 1322, 257, 2316, 365, 732, 1105, 7633, 18875], "temperature": 0.0, "avg_logprob": -0.05844642805016559, "compression_ratio": 1.6761363636363635, "no_speech_prob": 3.525537977111526e-05}, {"id": 427, "seek": 313132, "start": 3142.92, "end": 3150.1200000000003, "text": " at each time step one of which is still called the hidden state H and the other of which is called", "tokens": [412, 1184, 565, 1823, 472, 295, 597, 307, 920, 1219, 264, 7633, 1785, 389, 293, 264, 661, 295, 597, 307, 1219], "temperature": 0.0, "avg_logprob": -0.05844642805016559, "compression_ratio": 1.6761363636363635, "no_speech_prob": 3.525537977111526e-05}, {"id": 428, "seek": 313132, "start": 3150.1200000000003, "end": 3157.96, "text": " the cell state um now you know arguably in retrospect these were named wrongly because as you'll", "tokens": [264, 2815, 1785, 1105, 586, 291, 458, 26771, 294, 34997, 613, 645, 4926, 2085, 356, 570, 382, 291, 603], "temperature": 0.0, "avg_logprob": -0.05844642805016559, "compression_ratio": 1.6761363636363635, "no_speech_prob": 3.525537977111526e-05}, {"id": 429, "seek": 315796, "start": 3157.96, "end": 3164.28, "text": " see when we look at in more detail in some sense the cell is more equivalent to the hidden state of", "tokens": [536, 562, 321, 574, 412, 294, 544, 2607, 294, 512, 2020, 264, 2815, 307, 544, 10344, 281, 264, 7633, 1785, 295], "temperature": 0.0, "avg_logprob": -0.1242811161538829, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.3134329239837825e-05}, {"id": 430, "seek": 315796, "start": 3164.28, "end": 3170.76, "text": " the simple RNN than vice versa but we're just going with the names that everybody uses so both", "tokens": [264, 2199, 45702, 45, 813, 11964, 25650, 457, 321, 434, 445, 516, 365, 264, 5288, 300, 2201, 4960, 370, 1293], "temperature": 0.0, "avg_logprob": -0.1242811161538829, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.3134329239837825e-05}, {"id": 431, "seek": 315796, "start": 3170.76, "end": 3177.56, "text": " of these are vectors of length N um and it's going to be the cell that stores long term information", "tokens": [295, 613, 366, 18875, 295, 4641, 426, 1105, 293, 309, 311, 516, 281, 312, 264, 2815, 300, 9512, 938, 1433, 1589], "temperature": 0.0, "avg_logprob": -0.1242811161538829, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.3134329239837825e-05}, {"id": 432, "seek": 315796, "start": 3178.84, "end": 3185.64, "text": " and so we want to have something that's more like memory so the meaning like RAM and the computer", "tokens": [293, 370, 321, 528, 281, 362, 746, 300, 311, 544, 411, 4675, 370, 264, 3620, 411, 14561, 293, 264, 3820], "temperature": 0.0, "avg_logprob": -0.1242811161538829, "compression_ratio": 1.6752136752136753, "no_speech_prob": 2.3134329239837825e-05}, {"id": 433, "seek": 318564, "start": 3185.64, "end": 3193.16, "text": " um so the cell is designed so you can read from it you can erase parts of it and you can write new", "tokens": [1105, 370, 264, 2815, 307, 4761, 370, 291, 393, 1401, 490, 309, 291, 393, 23525, 3166, 295, 309, 293, 291, 393, 2464, 777], "temperature": 0.0, "avg_logprob": -0.06997299865937569, "compression_ratio": 1.7383720930232558, "no_speech_prob": 3.878569259541109e-05}, {"id": 434, "seek": 318564, "start": 3193.16, "end": 3202.04, "text": " information to the cell um and the interesting part of an LSTM is then it's got control structures", "tokens": [1589, 281, 264, 2815, 1105, 293, 264, 1880, 644, 295, 364, 441, 6840, 44, 307, 550, 309, 311, 658, 1969, 9227], "temperature": 0.0, "avg_logprob": -0.06997299865937569, "compression_ratio": 1.7383720930232558, "no_speech_prob": 3.878569259541109e-05}, {"id": 435, "seek": 318564, "start": 3202.04, "end": 3209.56, "text": " to decide how you do that so the selection of which information to erase write and read is controlled", "tokens": [281, 4536, 577, 291, 360, 300, 370, 264, 9450, 295, 597, 1589, 281, 23525, 2464, 293, 1401, 307, 10164], "temperature": 0.0, "avg_logprob": -0.06997299865937569, "compression_ratio": 1.7383720930232558, "no_speech_prob": 3.878569259541109e-05}, {"id": 436, "seek": 320956, "start": 3209.56, "end": 3219.0, "text": " by probabilistic gates so the gates are also vectors of length N and on each time step um we work out", "tokens": [538, 31959, 3142, 19792, 370, 264, 19792, 366, 611, 18875, 295, 4641, 426, 293, 322, 1184, 565, 1823, 1105, 321, 589, 484], "temperature": 0.0, "avg_logprob": -0.07455359805714, "compression_ratio": 1.8, "no_speech_prob": 3.424325768719427e-05}, {"id": 437, "seek": 320956, "start": 3219.0, "end": 3225.08, "text": " a state for the gate vectors so each element of the gate vectors is a probability so they can be", "tokens": [257, 1785, 337, 264, 8539, 18875, 370, 1184, 4478, 295, 264, 8539, 18875, 307, 257, 8482, 370, 436, 393, 312], "temperature": 0.0, "avg_logprob": -0.07455359805714, "compression_ratio": 1.8, "no_speech_prob": 3.424325768719427e-05}, {"id": 438, "seek": 320956, "start": 3225.08, "end": 3232.84, "text": " open probability one close probability zero or somewhere in between and their value will be saying", "tokens": [1269, 8482, 472, 1998, 8482, 4018, 420, 4079, 294, 1296, 293, 641, 2158, 486, 312, 1566], "temperature": 0.0, "avg_logprob": -0.07455359805714, "compression_ratio": 1.8, "no_speech_prob": 3.424325768719427e-05}, {"id": 439, "seek": 323284, "start": 3232.84, "end": 3240.6800000000003, "text": " how much do you erase how much do you write how much do you read and so these are dynamic gates", "tokens": [577, 709, 360, 291, 23525, 577, 709, 360, 291, 2464, 577, 709, 360, 291, 1401, 293, 370, 613, 366, 8546, 19792], "temperature": 0.0, "avg_logprob": -0.07079294358176746, "compression_ratio": 1.6651982378854626, "no_speech_prob": 8.26480936666485e-06}, {"id": 440, "seek": 323284, "start": 3240.6800000000003, "end": 3248.92, "text": " with a value that's computed based on the current context okay so in this next slide we go", "tokens": [365, 257, 2158, 300, 311, 40610, 2361, 322, 264, 2190, 4319, 1392, 370, 294, 341, 958, 4137, 321, 352], "temperature": 0.0, "avg_logprob": -0.07079294358176746, "compression_ratio": 1.6651982378854626, "no_speech_prob": 8.26480936666485e-06}, {"id": 441, "seek": 323284, "start": 3248.92, "end": 3254.52, "text": " through the equations of an LSTM but following this there are some more graphic slides which will", "tokens": [807, 264, 11787, 295, 364, 441, 6840, 44, 457, 3480, 341, 456, 366, 512, 544, 14089, 9788, 597, 486], "temperature": 0.0, "avg_logprob": -0.07079294358176746, "compression_ratio": 1.6651982378854626, "no_speech_prob": 8.26480936666485e-06}, {"id": 442, "seek": 323284, "start": 3254.52, "end": 3261.2400000000002, "text": " probably be easier to absorb right so we again just like before it's a current neural network", "tokens": [1391, 312, 3571, 281, 15631, 558, 370, 321, 797, 445, 411, 949, 309, 311, 257, 2190, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.07079294358176746, "compression_ratio": 1.6651982378854626, "no_speech_prob": 8.26480936666485e-06}, {"id": 443, "seek": 326124, "start": 3261.24, "end": 3269.08, "text": " we have a sequence of inputs x um t and we're going to it each time step compute a cell state", "tokens": [321, 362, 257, 8310, 295, 15743, 2031, 1105, 256, 293, 321, 434, 516, 281, 309, 1184, 565, 1823, 14722, 257, 2815, 1785], "temperature": 0.0, "avg_logprob": -0.10975074768066406, "compression_ratio": 1.7530864197530864, "no_speech_prob": 2.5063474822673015e-05}, {"id": 444, "seek": 326124, "start": 3269.08, "end": 3277.8799999999997, "text": " in the hidden state so how do we do that so firstly we're going to compute values of the three gates", "tokens": [294, 264, 7633, 1785, 370, 577, 360, 321, 360, 300, 370, 27376, 321, 434, 516, 281, 14722, 4190, 295, 264, 1045, 19792], "temperature": 0.0, "avg_logprob": -0.10975074768066406, "compression_ratio": 1.7530864197530864, "no_speech_prob": 2.5063474822673015e-05}, {"id": 445, "seek": 326124, "start": 3277.8799999999997, "end": 3285.08, "text": " and so we're computing the gate values using an equation that's identical to the equation", "tokens": [293, 370, 321, 434, 15866, 264, 8539, 4190, 1228, 364, 5367, 300, 311, 14800, 281, 264, 5367], "temperature": 0.0, "avg_logprob": -0.10975074768066406, "compression_ratio": 1.7530864197530864, "no_speech_prob": 2.5063474822673015e-05}, {"id": 446, "seek": 328508, "start": 3285.08, "end": 3293.72, "text": " um for the simple recurrent neural network um but in particular um oops sorry how does", "tokens": [1105, 337, 264, 2199, 18680, 1753, 18161, 3209, 1105, 457, 294, 1729, 1105, 34166, 2597, 577, 775], "temperature": 0.0, "avg_logprob": -0.12475282296367075, "compression_ratio": 1.7757009345794392, "no_speech_prob": 5.46572009625379e-05}, {"id": 447, "seek": 328508, "start": 3293.72, "end": 3300.2799999999997, "text": " just say what the gates are first so there's a forget gate um which we will control what is kept", "tokens": [445, 584, 437, 264, 19792, 366, 700, 370, 456, 311, 257, 2870, 8539, 1105, 597, 321, 486, 1969, 437, 307, 4305], "temperature": 0.0, "avg_logprob": -0.12475282296367075, "compression_ratio": 1.7757009345794392, "no_speech_prob": 5.46572009625379e-05}, {"id": 448, "seek": 328508, "start": 3300.84, "end": 3307.08, "text": " in the cell at the next time step versus what is forgotten there's an input gate which is going", "tokens": [294, 264, 2815, 412, 264, 958, 565, 1823, 5717, 437, 307, 11832, 456, 311, 364, 4846, 8539, 597, 307, 516], "temperature": 0.0, "avg_logprob": -0.12475282296367075, "compression_ratio": 1.7757009345794392, "no_speech_prob": 5.46572009625379e-05}, {"id": 449, "seek": 328508, "start": 3307.08, "end": 3314.2799999999997, "text": " to determine which parts of a calculator new cell content get written to the cell memory and there's", "tokens": [281, 6997, 597, 3166, 295, 257, 24993, 777, 2815, 2701, 483, 3720, 281, 264, 2815, 4675, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.12475282296367075, "compression_ratio": 1.7757009345794392, "no_speech_prob": 5.46572009625379e-05}, {"id": 450, "seek": 331428, "start": 3314.28, "end": 3320.6000000000004, "text": " an output gate which is going to control what parts of the cell memory are moved over into the hidden", "tokens": [364, 5598, 8539, 597, 307, 516, 281, 1969, 437, 3166, 295, 264, 2815, 4675, 366, 4259, 670, 666, 264, 7633], "temperature": 0.0, "avg_logprob": -0.07113916533333915, "compression_ratio": 1.761467889908257, "no_speech_prob": 3.0213222999009304e-05}, {"id": 451, "seek": 331428, "start": 3320.6000000000004, "end": 3328.76, "text": " state and so each of these is using the logistic function because we want them to be in each", "tokens": [1785, 293, 370, 1184, 295, 613, 307, 1228, 264, 3565, 3142, 2445, 570, 321, 528, 552, 281, 312, 294, 1184], "temperature": 0.0, "avg_logprob": -0.07113916533333915, "compression_ratio": 1.761467889908257, "no_speech_prob": 3.0213222999009304e-05}, {"id": 452, "seek": 331428, "start": 3328.76, "end": 3335.96, "text": " element of this vector a probability which will say whether to fully forget partially forget", "tokens": [4478, 295, 341, 8062, 257, 8482, 597, 486, 584, 1968, 281, 4498, 2870, 18886, 2870], "temperature": 0.0, "avg_logprob": -0.07113916533333915, "compression_ratio": 1.761467889908257, "no_speech_prob": 3.0213222999009304e-05}, {"id": 453, "seek": 331428, "start": 3335.96, "end": 3343.5600000000004, "text": " or fully fully remember yeah and the equation for each of these is exactly like the simple r and", "tokens": [420, 4498, 4498, 1604, 1338, 293, 264, 5367, 337, 1184, 295, 613, 307, 2293, 411, 264, 2199, 367, 293], "temperature": 0.0, "avg_logprob": -0.07113916533333915, "compression_ratio": 1.761467889908257, "no_speech_prob": 3.0213222999009304e-05}, {"id": 454, "seek": 334356, "start": 3343.56, "end": 3349.0, "text": " an equation but note of course that we've got different parameters for each one so we've got", "tokens": [364, 5367, 457, 3637, 295, 1164, 300, 321, 600, 658, 819, 9834, 337, 1184, 472, 370, 321, 600, 658], "temperature": 0.0, "avg_logprob": -0.10318003931353169, "compression_ratio": 1.6094674556213018, "no_speech_prob": 1.1841410014312714e-05}, {"id": 455, "seek": 334356, "start": 3349.7999999999997, "end": 3358.6, "text": " forgetting weight matrix w with a forgetting bias um and a forgetting um multiplier of the input", "tokens": [25428, 3364, 8141, 261, 365, 257, 25428, 12577, 1105, 293, 257, 25428, 1105, 44106, 295, 264, 4846], "temperature": 0.0, "avg_logprob": -0.10318003931353169, "compression_ratio": 1.6094674556213018, "no_speech_prob": 1.1841410014312714e-05}, {"id": 456, "seek": 334356, "start": 3360.92, "end": 3368.52, "text": " okay so then we have the other equations that really are the mechanics of the LSTM", "tokens": [1392, 370, 550, 321, 362, 264, 661, 11787, 300, 534, 366, 264, 12939, 295, 264, 441, 6840, 44], "temperature": 0.0, "avg_logprob": -0.10318003931353169, "compression_ratio": 1.6094674556213018, "no_speech_prob": 1.1841410014312714e-05}, {"id": 457, "seek": 336852, "start": 3368.52, "end": 3376.68, "text": " so we have something that will calculate a new cell content so this is our candidate update", "tokens": [370, 321, 362, 746, 300, 486, 8873, 257, 777, 2815, 2701, 370, 341, 307, 527, 11532, 5623], "temperature": 0.0, "avg_logprob": -0.12985918598790322, "compression_ratio": 1.6432748538011697, "no_speech_prob": 3.2181069400394335e-05}, {"id": 458, "seek": 336852, "start": 3376.68, "end": 3384.36, "text": " and so for calculating the candidate update we're again essentially using exactly the same simple", "tokens": [293, 370, 337, 28258, 264, 11532, 5623, 321, 434, 797, 4476, 1228, 2293, 264, 912, 2199], "temperature": 0.0, "avg_logprob": -0.12985918598790322, "compression_ratio": 1.6432748538011697, "no_speech_prob": 3.2181069400394335e-05}, {"id": 459, "seek": 336852, "start": 3384.36, "end": 3391.24, "text": " r and n equation apart from now it's usual to use 10h so you get something that I discussed", "tokens": [367, 293, 297, 5367, 4936, 490, 586, 309, 311, 7713, 281, 764, 1266, 71, 370, 291, 483, 746, 300, 286, 7152], "temperature": 0.0, "avg_logprob": -0.12985918598790322, "compression_ratio": 1.6432748538011697, "no_speech_prob": 3.2181069400394335e-05}, {"id": 460, "seek": 339124, "start": 3391.24, "end": 3400.6, "text": " last time is balanced around zero okay so then to actually update things we use our gates so", "tokens": [1036, 565, 307, 13902, 926, 4018, 1392, 370, 550, 281, 767, 5623, 721, 321, 764, 527, 19792, 370], "temperature": 0.0, "avg_logprob": -0.040030804547396576, "compression_ratio": 1.6946107784431137, "no_speech_prob": 4.783759322890546e-06}, {"id": 461, "seek": 339124, "start": 3400.6, "end": 3408.8399999999997, "text": " for our new cell content what the idea is is that we want to remember some but probably not all", "tokens": [337, 527, 777, 2815, 2701, 437, 264, 1558, 307, 307, 300, 321, 528, 281, 1604, 512, 457, 1391, 406, 439], "temperature": 0.0, "avg_logprob": -0.040030804547396576, "compression_ratio": 1.6946107784431137, "no_speech_prob": 4.783759322890546e-06}, {"id": 462, "seek": 339124, "start": 3408.8399999999997, "end": 3417.4799999999996, "text": " of what we had in the cell from previous time steps and we want to store some but probably not", "tokens": [295, 437, 321, 632, 294, 264, 2815, 490, 3894, 565, 4439, 293, 321, 528, 281, 3531, 512, 457, 1391, 406], "temperature": 0.0, "avg_logprob": -0.040030804547396576, "compression_ratio": 1.6946107784431137, "no_speech_prob": 4.783759322890546e-06}, {"id": 463, "seek": 341748, "start": 3417.48, "end": 3428.2, "text": " all of the value that we've calculated as the new cell update and so the way we do that is we take", "tokens": [439, 295, 264, 2158, 300, 321, 600, 15598, 382, 264, 777, 2815, 5623, 293, 370, 264, 636, 321, 360, 300, 307, 321, 747], "temperature": 0.0, "avg_logprob": -0.14038882048233695, "compression_ratio": 1.7861635220125787, "no_speech_prob": 2.0437108105397783e-05}, {"id": 464, "seek": 341748, "start": 3428.92, "end": 3436.68, "text": " the previous cell content and then we take its hard-amired product with the forget vector", "tokens": [264, 3894, 2815, 2701, 293, 550, 321, 747, 1080, 1152, 12, 335, 1824, 1674, 365, 264, 2870, 8062], "temperature": 0.0, "avg_logprob": -0.14038882048233695, "compression_ratio": 1.7861635220125787, "no_speech_prob": 2.0437108105397783e-05}, {"id": 465, "seek": 343668, "start": 3436.68, "end": 3446.04, "text": " and then we add to it the hard-amired product of the input gate times the candidate cell update", "tokens": [293, 550, 321, 909, 281, 309, 264, 1152, 12, 335, 1824, 1674, 295, 264, 4846, 8539, 1413, 264, 11532, 2815, 5623], "temperature": 0.0, "avg_logprob": -0.09534951412316525, "compression_ratio": 1.7402597402597402, "no_speech_prob": 2.3533988496637903e-05}, {"id": 466, "seek": 343668, "start": 3450.12, "end": 3457.24, "text": " and then for working out the new hidden state we then work out which parts of the cell", "tokens": [293, 550, 337, 1364, 484, 264, 777, 7633, 1785, 321, 550, 589, 484, 597, 3166, 295, 264, 2815], "temperature": 0.0, "avg_logprob": -0.09534951412316525, "compression_ratio": 1.7402597402597402, "no_speech_prob": 2.3533988496637903e-05}, {"id": 467, "seek": 343668, "start": 3458.04, "end": 3466.44, "text": " to expose in the hidden state and so after taking a 10h transform of the cell we then", "tokens": [281, 19219, 294, 264, 7633, 1785, 293, 370, 934, 1940, 257, 1266, 71, 4088, 295, 264, 2815, 321, 550], "temperature": 0.0, "avg_logprob": -0.09534951412316525, "compression_ratio": 1.7402597402597402, "no_speech_prob": 2.3533988496637903e-05}, {"id": 468, "seek": 346644, "start": 3466.44, "end": 3472.2000000000003, "text": " take the hard-amired product with the output gate and that gives us our hidden representation", "tokens": [747, 264, 1152, 12, 335, 1824, 1674, 365, 264, 5598, 8539, 293, 300, 2709, 505, 527, 7633, 10290], "temperature": 0.0, "avg_logprob": -0.10333670017331145, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.7521694796159863e-05}, {"id": 469, "seek": 346644, "start": 3472.2000000000003, "end": 3478.44, "text": " and if this hidden representation that we then put through a soft softmax layer to generate", "tokens": [293, 498, 341, 7633, 10290, 300, 321, 550, 829, 807, 257, 2787, 2787, 41167, 4583, 281, 8460], "temperature": 0.0, "avg_logprob": -0.10333670017331145, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.7521694796159863e-05}, {"id": 470, "seek": 346644, "start": 3479.08, "end": 3488.76, "text": " our next output of our LSTM or current neural network yeah so the gates and the things that they're", "tokens": [527, 958, 5598, 295, 527, 441, 6840, 44, 420, 2190, 18161, 3209, 1338, 370, 264, 19792, 293, 264, 721, 300, 436, 434], "temperature": 0.0, "avg_logprob": -0.10333670017331145, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.7521694796159863e-05}, {"id": 471, "seek": 348876, "start": 3488.76, "end": 3496.6000000000004, "text": " put with our vectors of size n and what we're doing is we're taking each element of them and", "tokens": [829, 365, 527, 18875, 295, 2744, 297, 293, 437, 321, 434, 884, 307, 321, 434, 1940, 1184, 4478, 295, 552, 293], "temperature": 0.0, "avg_logprob": -0.12485078747352857, "compression_ratio": 1.7674418604651163, "no_speech_prob": 3.8795657019363716e-05}, {"id": 472, "seek": 348876, "start": 3496.6000000000004, "end": 3503.0, "text": " multiplying them element wise to work out a new vector and then we get two vectors and that", "tokens": [30955, 552, 4478, 10829, 281, 589, 484, 257, 777, 8062, 293, 550, 321, 483, 732, 18875, 293, 300], "temperature": 0.0, "avg_logprob": -0.12485078747352857, "compression_ratio": 1.7674418604651163, "no_speech_prob": 3.8795657019363716e-05}, {"id": 473, "seek": 348876, "start": 3503.0, "end": 3510.0400000000004, "text": " we're adding together so this way of doing things element wise you sort of don't really see and", "tokens": [321, 434, 5127, 1214, 370, 341, 636, 295, 884, 721, 4478, 10829, 291, 1333, 295, 500, 380, 534, 536, 293], "temperature": 0.0, "avg_logprob": -0.12485078747352857, "compression_ratio": 1.7674418604651163, "no_speech_prob": 3.8795657019363716e-05}, {"id": 474, "seek": 348876, "start": 3510.0400000000004, "end": 3517.6400000000003, "text": " standard linear algebra course it's referred to as the hard-amired product it's represented by some", "tokens": [3832, 8213, 21989, 1164, 309, 311, 10839, 281, 382, 264, 1152, 12, 335, 1824, 1674, 309, 311, 10379, 538, 512], "temperature": 0.0, "avg_logprob": -0.12485078747352857, "compression_ratio": 1.7674418604651163, "no_speech_prob": 3.8795657019363716e-05}, {"id": 475, "seek": 351764, "start": 3517.64, "end": 3522.8399999999997, "text": " kind of circle I mean actually in more modern work it's been more usual to represent it with", "tokens": [733, 295, 6329, 286, 914, 767, 294, 544, 4363, 589, 309, 311, 668, 544, 7713, 281, 2906, 309, 365], "temperature": 0.0, "avg_logprob": -0.08276311375878075, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.3856241720495746e-05}, {"id": 476, "seek": 351764, "start": 3522.8399999999997, "end": 3529.0, "text": " this slightly bigger circle with the dot at the middle as the hard-amired product symbol and", "tokens": [341, 4748, 3801, 6329, 365, 264, 5893, 412, 264, 2808, 382, 264, 1152, 12, 335, 1824, 1674, 5986, 293], "temperature": 0.0, "avg_logprob": -0.08276311375878075, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.3856241720495746e-05}, {"id": 477, "seek": 351764, "start": 3529.0, "end": 3534.8399999999997, "text": " someday I'll change these slides to be like that but I was lazy and redoing the equations but the", "tokens": [19412, 286, 603, 1319, 613, 9788, 281, 312, 411, 300, 457, 286, 390, 14847, 293, 29956, 278, 264, 11787, 457, 264], "temperature": 0.0, "avg_logprob": -0.08276311375878075, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.3856241720495746e-05}, {"id": 478, "seek": 351764, "start": 3534.8399999999997, "end": 3540.68, "text": " other notation you do see quite often is just using the same little circle that you use for function", "tokens": [661, 24657, 291, 360, 536, 1596, 2049, 307, 445, 1228, 264, 912, 707, 6329, 300, 291, 764, 337, 2445], "temperature": 0.0, "avg_logprob": -0.08276311375878075, "compression_ratio": 1.6551724137931034, "no_speech_prob": 5.3856241720495746e-05}, {"id": 479, "seek": 354068, "start": 3540.68, "end": 3548.8399999999997, "text": " composition to represent hard-amired product okay so all of these things are being done as vectors", "tokens": [12686, 281, 2906, 1152, 12, 335, 1824, 1674, 1392, 370, 439, 295, 613, 721, 366, 885, 1096, 382, 18875], "temperature": 0.0, "avg_logprob": -0.09025090081351143, "compression_ratio": 1.6368715083798884, "no_speech_prob": 5.591803983406862e-06}, {"id": 480, "seek": 354068, "start": 3548.8399999999997, "end": 3556.8399999999997, "text": " of the same length n and the other thing that you might notice is that the candidate update and", "tokens": [295, 264, 912, 4641, 297, 293, 264, 661, 551, 300, 291, 1062, 3449, 307, 300, 264, 11532, 5623, 293], "temperature": 0.0, "avg_logprob": -0.09025090081351143, "compression_ratio": 1.6368715083798884, "no_speech_prob": 5.591803983406862e-06}, {"id": 481, "seek": 354068, "start": 3557.48, "end": 3566.6, "text": " forget import and output gates all have a very similar form the only difference is three logistics", "tokens": [2870, 974, 293, 5598, 19792, 439, 362, 257, 588, 2531, 1254, 264, 787, 2649, 307, 1045, 27420], "temperature": 0.0, "avg_logprob": -0.09025090081351143, "compression_ratio": 1.6368715083798884, "no_speech_prob": 5.591803983406862e-06}, {"id": 482, "seek": 356660, "start": 3566.6, "end": 3574.44, "text": " in one 10-h and none of them depend on each other so all four of those can be calculated parallel", "tokens": [294, 472, 1266, 12, 71, 293, 6022, 295, 552, 5672, 322, 1184, 661, 370, 439, 1451, 295, 729, 393, 312, 15598, 8952], "temperature": 0.0, "avg_logprob": -0.1110148686234669, "compression_ratio": 1.5991735537190082, "no_speech_prob": 6.397263496182859e-05}, {"id": 483, "seek": 356660, "start": 3574.44, "end": 3580.7599999999998, "text": " and if you want to have an efficient LSTM implementation that's what you do okay so here's the more", "tokens": [293, 498, 291, 528, 281, 362, 364, 7148, 441, 6840, 44, 11420, 300, 311, 437, 291, 360, 1392, 370, 510, 311, 264, 544], "temperature": 0.0, "avg_logprob": -0.1110148686234669, "compression_ratio": 1.5991735537190082, "no_speech_prob": 6.397263496182859e-05}, {"id": 484, "seek": 356660, "start": 3580.7599999999998, "end": 3588.44, "text": " graphical presentation of this so these pictures come from Chris Ola and I guess he did such a", "tokens": [35942, 5860, 295, 341, 370, 613, 5242, 808, 490, 6688, 422, 875, 293, 286, 2041, 415, 630, 1270, 257], "temperature": 0.0, "avg_logprob": -0.1110148686234669, "compression_ratio": 1.5991735537190082, "no_speech_prob": 6.397263496182859e-05}, {"id": 485, "seek": 356660, "start": 3588.44, "end": 3596.2799999999997, "text": " nice job at producing pictures for LSTMs that almost everyone uses them these days and so this", "tokens": [1481, 1691, 412, 10501, 5242, 337, 441, 6840, 26386, 300, 1920, 1518, 4960, 552, 613, 1708, 293, 370, 341], "temperature": 0.0, "avg_logprob": -0.1110148686234669, "compression_ratio": 1.5991735537190082, "no_speech_prob": 6.397263496182859e-05}, {"id": 486, "seek": 359628, "start": 3596.28, "end": 3606.52, "text": " sort of pulls apart the computation graph of an LSTM unit so blowing this up you've got from", "tokens": [1333, 295, 16982, 4936, 264, 24903, 4295, 295, 364, 441, 6840, 44, 4985, 370, 15068, 341, 493, 291, 600, 658, 490], "temperature": 0.0, "avg_logprob": -0.06311032266327829, "compression_ratio": 1.6785714285714286, "no_speech_prob": 4.003896901849657e-05}, {"id": 487, "seek": 359628, "start": 3606.52, "end": 3615.88, "text": " the previous time step both your cell and hidden recurrent vectors and so you feed the hidden", "tokens": [264, 3894, 565, 1823, 1293, 428, 2815, 293, 7633, 18680, 1753, 18875, 293, 370, 291, 3154, 264, 7633], "temperature": 0.0, "avg_logprob": -0.06311032266327829, "compression_ratio": 1.6785714285714286, "no_speech_prob": 4.003896901849657e-05}, {"id": 488, "seek": 359628, "start": 3618.36, "end": 3625.7200000000003, "text": " vector from the previous time step and the new input xt into the computation of the gates which", "tokens": [8062, 490, 264, 3894, 565, 1823, 293, 264, 777, 4846, 2031, 83, 666, 264, 24903, 295, 264, 19792, 597], "temperature": 0.0, "avg_logprob": -0.06311032266327829, "compression_ratio": 1.6785714285714286, "no_speech_prob": 4.003896901849657e-05}, {"id": 489, "seek": 362572, "start": 3625.72, "end": 3632.2, "text": " is happening down the bottom so you compute the forget gate and then you use the forget gate in a", "tokens": [307, 2737, 760, 264, 2767, 370, 291, 14722, 264, 2870, 8539, 293, 550, 291, 764, 264, 2870, 8539, 294, 257], "temperature": 0.0, "avg_logprob": -0.10590171813964844, "compression_ratio": 1.7951807228915662, "no_speech_prob": 9.010272333398461e-05}, {"id": 490, "seek": 362572, "start": 3632.2, "end": 3640.2, "text": " hard-amired product here drawn as a actually a time symbol so forget some cell content you work out", "tokens": [1152, 12, 335, 1824, 1674, 510, 10117, 382, 257, 767, 257, 565, 5986, 370, 2870, 512, 2815, 2701, 291, 589, 484], "temperature": 0.0, "avg_logprob": -0.10590171813964844, "compression_ratio": 1.7951807228915662, "no_speech_prob": 9.010272333398461e-05}, {"id": 491, "seek": 362572, "start": 3640.2, "end": 3649.3199999999997, "text": " the input gate and then using the input gate and a regular recurrent neural network like computation", "tokens": [264, 4846, 8539, 293, 550, 1228, 264, 4846, 8539, 293, 257, 3890, 18680, 1753, 18161, 3209, 411, 24903], "temperature": 0.0, "avg_logprob": -0.10590171813964844, "compression_ratio": 1.7951807228915662, "no_speech_prob": 9.010272333398461e-05}, {"id": 492, "seek": 364932, "start": 3649.32, "end": 3660.04, "text": " you can compute candidate new cell content and so then you add those two together to get the new cell", "tokens": [291, 393, 14722, 11532, 777, 2815, 2701, 293, 370, 550, 291, 909, 729, 732, 1214, 281, 483, 264, 777, 2815], "temperature": 0.0, "avg_logprob": -0.07820602024302763, "compression_ratio": 1.908496732026144, "no_speech_prob": 2.75020174740348e-05}, {"id": 493, "seek": 364932, "start": 3660.04, "end": 3668.1200000000003, "text": " content which then heads out as the new cell content at time t but then you also have worked out", "tokens": [2701, 597, 550, 8050, 484, 382, 264, 777, 2815, 2701, 412, 565, 256, 457, 550, 291, 611, 362, 2732, 484], "temperature": 0.0, "avg_logprob": -0.07820602024302763, "compression_ratio": 1.908496732026144, "no_speech_prob": 2.75020174740348e-05}, {"id": 494, "seek": 364932, "start": 3668.1200000000003, "end": 3676.6800000000003, "text": " an output gate and so then you take the cell content put it through another non-linearity and", "tokens": [364, 5598, 8539, 293, 370, 550, 291, 747, 264, 2815, 2701, 829, 309, 807, 1071, 2107, 12, 1889, 17409, 293], "temperature": 0.0, "avg_logprob": -0.07820602024302763, "compression_ratio": 1.908496732026144, "no_speech_prob": 2.75020174740348e-05}, {"id": 495, "seek": 367668, "start": 3676.68, "end": 3684.04, "text": " multi-hard-amired product it with the output gate and that then gives you the new hidden state", "tokens": [4825, 12, 21491, 12, 335, 1824, 1674, 309, 365, 264, 5598, 8539, 293, 300, 550, 2709, 291, 264, 777, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.1547207411597757, "compression_ratio": 1.5956284153005464, "no_speech_prob": 4.905563764623366e-05}, {"id": 496, "seek": 367668, "start": 3686.12, "end": 3694.6, "text": " so this is all kind of complex but as to understanding why something is different as happening here", "tokens": [370, 341, 307, 439, 733, 295, 3997, 457, 382, 281, 3701, 983, 746, 307, 819, 382, 2737, 510], "temperature": 0.0, "avg_logprob": -0.1547207411597757, "compression_ratio": 1.5956284153005464, "no_speech_prob": 4.905563764623366e-05}, {"id": 497, "seek": 367668, "start": 3694.6, "end": 3704.52, "text": " the thing to notice is that the cell state from t minus 1 is passing right through this to be the", "tokens": [264, 551, 281, 3449, 307, 300, 264, 2815, 1785, 490, 256, 3175, 502, 307, 8437, 558, 807, 341, 281, 312, 264], "temperature": 0.0, "avg_logprob": -0.1547207411597757, "compression_ratio": 1.5956284153005464, "no_speech_prob": 4.905563764623366e-05}, {"id": 498, "seek": 370452, "start": 3704.52, "end": 3714.28, "text": " cell state at time t without very much happening to it so some of it is being deleted by the forget gate", "tokens": [2815, 1785, 412, 565, 256, 1553, 588, 709, 2737, 281, 309, 370, 512, 295, 309, 307, 885, 22981, 538, 264, 2870, 8539], "temperature": 0.0, "avg_logprob": -0.06259471812146775, "compression_ratio": 1.53125, "no_speech_prob": 2.6670861188904382e-05}, {"id": 499, "seek": 370452, "start": 3715.16, "end": 3723.88, "text": " and then some new stuff is being written to it as a result of using this candidate new cell", "tokens": [293, 550, 512, 777, 1507, 307, 885, 3720, 281, 309, 382, 257, 1874, 295, 1228, 341, 11532, 777, 2815], "temperature": 0.0, "avg_logprob": -0.06259471812146775, "compression_ratio": 1.53125, "no_speech_prob": 2.6670861188904382e-05}, {"id": 500, "seek": 372388, "start": 3723.88, "end": 3735.8, "text": " content but the real secret of the LSTM is that new stuff is just being added to the cell with an", "tokens": [2701, 457, 264, 957, 4054, 295, 264, 441, 6840, 44, 307, 300, 777, 1507, 307, 445, 885, 3869, 281, 264, 2815, 365, 364], "temperature": 0.0, "avg_logprob": -0.06537142395973206, "compression_ratio": 1.5469613259668509, "no_speech_prob": 3.445948095759377e-06}, {"id": 501, "seek": 372388, "start": 3735.8, "end": 3743.4, "text": " addition right so in the simple RNN at each success of step you are doing a multiplication", "tokens": [4500, 558, 370, 294, 264, 2199, 45702, 45, 412, 1184, 2245, 295, 1823, 291, 366, 884, 257, 27290], "temperature": 0.0, "avg_logprob": -0.06537142395973206, "compression_ratio": 1.5469613259668509, "no_speech_prob": 3.445948095759377e-06}, {"id": 502, "seek": 372388, "start": 3743.4, "end": 3750.92, "text": " and that makes it incredibly difficult to learn to preserve information in the hidden state", "tokens": [293, 300, 1669, 309, 6252, 2252, 281, 1466, 281, 15665, 1589, 294, 264, 7633, 1785], "temperature": 0.0, "avg_logprob": -0.06537142395973206, "compression_ratio": 1.5469613259668509, "no_speech_prob": 3.445948095759377e-06}, {"id": 503, "seek": 375092, "start": 3750.92, "end": 3756.84, "text": " over a long period of time it's not completely impossible but it's a very difficult thing to", "tokens": [670, 257, 938, 2896, 295, 565, 309, 311, 406, 2584, 6243, 457, 309, 311, 257, 588, 2252, 551, 281], "temperature": 0.0, "avg_logprob": -0.08127451491081851, "compression_ratio": 1.6059322033898304, "no_speech_prob": 3.6423312849365175e-05}, {"id": 504, "seek": 375092, "start": 3756.84, "end": 3764.6, "text": " learn whereas with this new LSTM architecture it's trivial to preserve information the cell", "tokens": [1466, 9735, 365, 341, 777, 441, 6840, 44, 9482, 309, 311, 26703, 281, 15665, 1589, 264, 2815], "temperature": 0.0, "avg_logprob": -0.08127451491081851, "compression_ratio": 1.6059322033898304, "no_speech_prob": 3.6423312849365175e-05}, {"id": 505, "seek": 375092, "start": 3764.6, "end": 3771.88, "text": " from one time step to the next you just don't forget it and it'll carry right through with perhaps", "tokens": [490, 472, 565, 1823, 281, 264, 958, 291, 445, 500, 380, 2870, 309, 293, 309, 603, 3985, 558, 807, 365, 4317], "temperature": 0.0, "avg_logprob": -0.08127451491081851, "compression_ratio": 1.6059322033898304, "no_speech_prob": 3.6423312849365175e-05}, {"id": 506, "seek": 375092, "start": 3771.88, "end": 3780.28, "text": " some new stuff added in to also remember and so that's the sense in which the cell behaves much", "tokens": [512, 777, 1507, 3869, 294, 281, 611, 1604, 293, 370, 300, 311, 264, 2020, 294, 597, 264, 2815, 36896, 709], "temperature": 0.0, "avg_logprob": -0.08127451491081851, "compression_ratio": 1.6059322033898304, "no_speech_prob": 3.6423312849365175e-05}, {"id": 507, "seek": 378028, "start": 3780.28, "end": 3786.76, "text": " more like RAM and a conventional computer that storing stuff and extra stuff can be stored", "tokens": [544, 411, 14561, 293, 257, 16011, 3820, 300, 26085, 1507, 293, 2857, 1507, 393, 312, 12187], "temperature": 0.0, "avg_logprob": -0.10697963088750839, "compression_ratio": 1.5026455026455026, "no_speech_prob": 6.480054435087368e-05}, {"id": 508, "seek": 378028, "start": 3786.76, "end": 3795.2400000000002, "text": " into it and other stuff can be deleted from it as you go along. Okay so the LSTM architecture", "tokens": [666, 309, 293, 661, 1507, 393, 312, 22981, 490, 309, 382, 291, 352, 2051, 13, 1033, 370, 264, 441, 6840, 44, 9482], "temperature": 0.0, "avg_logprob": -0.10697963088750839, "compression_ratio": 1.5026455026455026, "no_speech_prob": 6.480054435087368e-05}, {"id": 509, "seek": 378028, "start": 3795.2400000000002, "end": 3804.6800000000003, "text": " makes it much easier to preserve information from many time steps and I just right so in particular", "tokens": [1669, 309, 709, 3571, 281, 15665, 1589, 490, 867, 565, 4439, 293, 286, 445, 558, 370, 294, 1729], "temperature": 0.0, "avg_logprob": -0.10697963088750839, "compression_ratio": 1.5026455026455026, "no_speech_prob": 6.480054435087368e-05}, {"id": 510, "seek": 380468, "start": 3804.68, "end": 3812.52, "text": " standard practice with LSTMs is to initialize the forget gate to a one vector which it's just", "tokens": [3832, 3124, 365, 441, 6840, 26386, 307, 281, 5883, 1125, 264, 2870, 8539, 281, 257, 472, 8062, 597, 309, 311, 445], "temperature": 0.0, "avg_logprob": -0.1440705042036753, "compression_ratio": 1.5689655172413792, "no_speech_prob": 2.8377042326610535e-05}, {"id": 511, "seek": 380468, "start": 3812.52, "end": 3819.72, "text": " so that a starting point is to say preserve everything from previous time steps and then", "tokens": [370, 300, 257, 2891, 935, 307, 281, 584, 15665, 1203, 490, 3894, 565, 4439, 293, 550], "temperature": 0.0, "avg_logprob": -0.1440705042036753, "compression_ratio": 1.5689655172413792, "no_speech_prob": 2.8377042326610535e-05}, {"id": 512, "seek": 380468, "start": 3820.2799999999997, "end": 3827.72, "text": " it is then learning when it's appropriate to forget stuff and contrast is very hard to get", "tokens": [309, 307, 550, 2539, 562, 309, 311, 6854, 281, 2870, 1507, 293, 8712, 307, 588, 1152, 281, 483], "temperature": 0.0, "avg_logprob": -0.1440705042036753, "compression_ratio": 1.5689655172413792, "no_speech_prob": 2.8377042326610535e-05}, {"id": 513, "seek": 382772, "start": 3827.72, "end": 3835.16, "text": " or a simple RNN to preserve stuff for a very long time. I mean what does that actually mean?", "tokens": [420, 257, 2199, 45702, 45, 281, 15665, 1507, 337, 257, 588, 938, 565, 13, 286, 914, 437, 775, 300, 767, 914, 30], "temperature": 0.0, "avg_logprob": -0.1507937654535821, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.09821145189926e-05}, {"id": 514, "seek": 382772, "start": 3836.2799999999997, "end": 3843.08, "text": " Well you know I've put down some numbers here I mean you know how what you get in practice", "tokens": [1042, 291, 458, 286, 600, 829, 760, 512, 3547, 510, 286, 914, 291, 458, 577, 437, 291, 483, 294, 3124], "temperature": 0.0, "avg_logprob": -0.1507937654535821, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.09821145189926e-05}, {"id": 515, "seek": 382772, "start": 3843.08, "end": 3848.12, "text": " you know depends on a million things it depends on the nature of your data and how much data you", "tokens": [291, 458, 5946, 322, 257, 2459, 721, 309, 5946, 322, 264, 3687, 295, 428, 1412, 293, 577, 709, 1412, 291], "temperature": 0.0, "avg_logprob": -0.1507937654535821, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.09821145189926e-05}, {"id": 516, "seek": 382772, "start": 3848.12, "end": 3855.3999999999996, "text": " have and what dimensionality your hidden states are blurdy blurdy blur but just to give you", "tokens": [362, 293, 437, 10139, 1860, 428, 7633, 4368, 366, 888, 374, 3173, 888, 374, 3173, 888, 374, 457, 445, 281, 976, 291], "temperature": 0.0, "avg_logprob": -0.1507937654535821, "compression_ratio": 1.6607142857142858, "no_speech_prob": 6.09821145189926e-05}, {"id": 517, "seek": 385540, "start": 3855.4, "end": 3863.08, "text": " some idea of what's going on is typically if you train a simple recurrent neural network", "tokens": [512, 1558, 295, 437, 311, 516, 322, 307, 5850, 498, 291, 3847, 257, 2199, 18680, 1753, 18161, 3209], "temperature": 0.0, "avg_logprob": -0.05223304033279419, "compression_ratio": 1.6460176991150441, "no_speech_prob": 9.44590792641975e-05}, {"id": 518, "seek": 385540, "start": 3863.08, "end": 3868.2000000000003, "text": " that it's effective memory it's ability to be able to use things in the past to condition the", "tokens": [300, 309, 311, 4942, 4675, 309, 311, 3485, 281, 312, 1075, 281, 764, 721, 294, 264, 1791, 281, 4188, 264], "temperature": 0.0, "avg_logprob": -0.05223304033279419, "compression_ratio": 1.6460176991150441, "no_speech_prob": 9.44590792641975e-05}, {"id": 519, "seek": 385540, "start": 3868.2000000000003, "end": 3874.36, "text": " future goes for about seven time steps you just really can't get it to remember stuff further", "tokens": [2027, 1709, 337, 466, 3407, 565, 4439, 291, 445, 534, 393, 380, 483, 309, 281, 1604, 1507, 3052], "temperature": 0.0, "avg_logprob": -0.05223304033279419, "compression_ratio": 1.6460176991150441, "no_speech_prob": 9.44590792641975e-05}, {"id": 520, "seek": 385540, "start": 3874.36, "end": 3884.12, "text": " back in the past than that whereas for the LSTM it's not complete magic it doesn't work forever", "tokens": [646, 294, 264, 1791, 813, 300, 9735, 337, 264, 441, 6840, 44, 309, 311, 406, 3566, 5585, 309, 1177, 380, 589, 5680], "temperature": 0.0, "avg_logprob": -0.05223304033279419, "compression_ratio": 1.6460176991150441, "no_speech_prob": 9.44590792641975e-05}, {"id": 521, "seek": 388412, "start": 3884.12, "end": 3891.72, "text": " but you know it's effectively able to remember and use things from much much further back so typically", "tokens": [457, 291, 458, 309, 311, 8659, 1075, 281, 1604, 293, 764, 721, 490, 709, 709, 3052, 646, 370, 5850], "temperature": 0.0, "avg_logprob": -0.061020535029722066, "compression_ratio": 1.703056768558952, "no_speech_prob": 2.1081143131596036e-05}, {"id": 522, "seek": 388412, "start": 3891.72, "end": 3898.3599999999997, "text": " you find that with an LSTM you can effectively remember and use things about a hundred time steps", "tokens": [291, 915, 300, 365, 364, 441, 6840, 44, 291, 393, 8659, 1604, 293, 764, 721, 466, 257, 3262, 565, 4439], "temperature": 0.0, "avg_logprob": -0.061020535029722066, "compression_ratio": 1.703056768558952, "no_speech_prob": 2.1081143131596036e-05}, {"id": 523, "seek": 388412, "start": 3898.3599999999997, "end": 3904.2, "text": " back and that's just enormously more useful for a lot of the natural language understanding", "tokens": [646, 293, 300, 311, 445, 39669, 544, 4420, 337, 257, 688, 295, 264, 3303, 2856, 3701], "temperature": 0.0, "avg_logprob": -0.061020535029722066, "compression_ratio": 1.703056768558952, "no_speech_prob": 2.1081143131596036e-05}, {"id": 524, "seek": 388412, "start": 3904.2, "end": 3913.96, "text": " tasks that we want to do and so that was precisely what the LSTM was designed to do and I mean so", "tokens": [9608, 300, 321, 528, 281, 360, 293, 370, 300, 390, 13402, 437, 264, 441, 6840, 44, 390, 4761, 281, 360, 293, 286, 914, 370], "temperature": 0.0, "avg_logprob": -0.061020535029722066, "compression_ratio": 1.703056768558952, "no_speech_prob": 2.1081143131596036e-05}, {"id": 525, "seek": 391396, "start": 3913.96, "end": 3920.6, "text": " in particular just going back to its name quite a few people miss paths its name the idea of", "tokens": [294, 1729, 445, 516, 646, 281, 1080, 1315, 1596, 257, 1326, 561, 1713, 14518, 1080, 1315, 264, 1558, 295], "temperature": 0.0, "avg_logprob": -0.10293969340708065, "compression_ratio": 1.7442922374429224, "no_speech_prob": 9.85735168796964e-05}, {"id": 526, "seek": 391396, "start": 3920.6, "end": 3926.6, "text": " its name was there's a concept of short term memory which comes from psychology and it'd been", "tokens": [1080, 1315, 390, 456, 311, 257, 3410, 295, 2099, 1433, 4675, 597, 1487, 490, 15105, 293, 309, 1116, 668], "temperature": 0.0, "avg_logprob": -0.10293969340708065, "compression_ratio": 1.7442922374429224, "no_speech_prob": 9.85735168796964e-05}, {"id": 527, "seek": 391396, "start": 3926.6, "end": 3934.44, "text": " suggested for simple RNNs that the hidden state of the RNN could be a model of human short term", "tokens": [10945, 337, 2199, 45702, 45, 82, 300, 264, 7633, 1785, 295, 264, 45702, 45, 727, 312, 257, 2316, 295, 1952, 2099, 1433], "temperature": 0.0, "avg_logprob": -0.10293969340708065, "compression_ratio": 1.7442922374429224, "no_speech_prob": 9.85735168796964e-05}, {"id": 528, "seek": 391396, "start": 3934.44, "end": 3941.16, "text": " memory and then there would be something somewhere else that would deal with human long term memory", "tokens": [4675, 293, 550, 456, 576, 312, 746, 4079, 1646, 300, 576, 2028, 365, 1952, 938, 1433, 4675], "temperature": 0.0, "avg_logprob": -0.10293969340708065, "compression_ratio": 1.7442922374429224, "no_speech_prob": 9.85735168796964e-05}, {"id": 529, "seek": 394116, "start": 3941.16, "end": 3947.64, "text": " but while people had found that this only gave you a very short short term memory so what", "tokens": [457, 1339, 561, 632, 1352, 300, 341, 787, 2729, 291, 257, 588, 2099, 2099, 1433, 4675, 370, 437], "temperature": 0.0, "avg_logprob": -0.16941177143770106, "compression_ratio": 1.599078341013825, "no_speech_prob": 4.574549529934302e-05}, {"id": 530, "seek": 394116, "start": 3948.6, "end": 3952.7599999999998, "text": " Hock-Rider and Schmidt who were interested in was how we could give", "tokens": [389, 1560, 12, 49, 1438, 293, 42621, 567, 645, 3102, 294, 390, 577, 321, 727, 976], "temperature": 0.0, "avg_logprob": -0.16941177143770106, "compression_ratio": 1.599078341013825, "no_speech_prob": 4.574549529934302e-05}, {"id": 531, "seek": 394116, "start": 3953.96, "end": 3960.8399999999997, "text": " construct models with a long short term memory and so that then gave us this name of LSTM.", "tokens": [7690, 5245, 365, 257, 938, 2099, 1433, 4675, 293, 370, 300, 550, 2729, 505, 341, 1315, 295, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.16941177143770106, "compression_ratio": 1.599078341013825, "no_speech_prob": 4.574549529934302e-05}, {"id": 532, "seek": 394116, "start": 3963.0, "end": 3970.2, "text": " LSTMs don't guarantee that there are no vanishing exploding gradients but in practice they provide", "tokens": [441, 6840, 26386, 500, 380, 10815, 300, 456, 366, 572, 3161, 3807, 35175, 2771, 2448, 457, 294, 3124, 436, 2893], "temperature": 0.0, "avg_logprob": -0.16941177143770106, "compression_ratio": 1.599078341013825, "no_speech_prob": 4.574549529934302e-05}, {"id": 533, "seek": 397020, "start": 3970.2, "end": 3977.48, "text": " they they don't tend to explode nearly the same way again that plus sign is crucial rather than a", "tokens": [436, 436, 500, 380, 3928, 281, 21411, 6217, 264, 912, 636, 797, 300, 1804, 1465, 307, 11462, 2831, 813, 257], "temperature": 0.0, "avg_logprob": -0.12545226170466497, "compression_ratio": 1.4577114427860696, "no_speech_prob": 0.00023972828057594597}, {"id": 534, "seek": 397020, "start": 3977.48, "end": 3983.16, "text": " multiplication and so they're a much more effective way of learning long distance dependencies.", "tokens": [27290, 293, 370, 436, 434, 257, 709, 544, 4942, 636, 295, 2539, 938, 4560, 36606, 13], "temperature": 0.0, "avg_logprob": -0.12545226170466497, "compression_ratio": 1.4577114427860696, "no_speech_prob": 0.00023972828057594597}, {"id": 535, "seek": 397020, "start": 3985.0, "end": 3996.3599999999997, "text": " Okay so despite the fact that LSTMs were developed around 1997 2000 it was really only in the early", "tokens": [1033, 370, 7228, 264, 1186, 300, 441, 6840, 26386, 645, 4743, 926, 22383, 8132, 309, 390, 534, 787, 294, 264, 2440], "temperature": 0.0, "avg_logprob": -0.12545226170466497, "compression_ratio": 1.4577114427860696, "no_speech_prob": 0.00023972828057594597}, {"id": 536, "seek": 399636, "start": 3996.36, "end": 4005.0, "text": " 2010s that the world woke up to them and how successful they were so it was really around 2013 to", "tokens": [9657, 82, 300, 264, 1002, 12852, 493, 281, 552, 293, 577, 4406, 436, 645, 370, 309, 390, 534, 926, 9012, 281], "temperature": 0.0, "avg_logprob": -0.10514921216822382, "compression_ratio": 1.5, "no_speech_prob": 8.059730316745117e-05}, {"id": 537, "seek": 399636, "start": 4005.0, "end": 4013.7200000000003, "text": " 2015 that LSTMs sort of hit the world achieving state-of-the-art results on all kinds of problems.", "tokens": [7546, 300, 441, 6840, 26386, 1333, 295, 2045, 264, 1002, 19626, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 322, 439, 3685, 295, 2740, 13], "temperature": 0.0, "avg_logprob": -0.10514921216822382, "compression_ratio": 1.5, "no_speech_prob": 8.059730316745117e-05}, {"id": 538, "seek": 399636, "start": 4013.7200000000003, "end": 4019.6400000000003, "text": " One of the first big demonstrations was for handwriting recognition then speech recognition", "tokens": [1485, 295, 264, 700, 955, 34714, 390, 337, 39179, 11150, 550, 6218, 11150], "temperature": 0.0, "avg_logprob": -0.10514921216822382, "compression_ratio": 1.5, "no_speech_prob": 8.059730316745117e-05}, {"id": 539, "seek": 401964, "start": 4019.64, "end": 4026.7599999999998, "text": " and then going on to a lot of natural language tasks including machine translation, parsing,", "tokens": [293, 550, 516, 322, 281, 257, 688, 295, 3303, 2856, 9608, 3009, 3479, 12853, 11, 21156, 278, 11], "temperature": 0.0, "avg_logprob": -0.26697768587054627, "compression_ratio": 1.5351351351351352, "no_speech_prob": 8.206076745409518e-05}, {"id": 540, "seek": 401964, "start": 4027.48, "end": 4033.16, "text": " vision and language tasks like minskapshening as well of course using them for language models", "tokens": [5201, 293, 2856, 9608, 411, 923, 5161, 569, 2716, 4559, 382, 731, 295, 1164, 1228, 552, 337, 2856, 5245], "temperature": 0.0, "avg_logprob": -0.26697768587054627, "compression_ratio": 1.5351351351351352, "no_speech_prob": 8.206076745409518e-05}, {"id": 541, "seek": 401964, "start": 4033.16, "end": 4040.7599999999998, "text": " and around these years LSTMs became the dominant approach for most NLP tasks. The easiest way to", "tokens": [293, 926, 613, 924, 441, 6840, 26386, 3062, 264, 15657, 3109, 337, 881, 426, 45196, 9608, 13, 440, 12889, 636, 281], "temperature": 0.0, "avg_logprob": -0.26697768587054627, "compression_ratio": 1.5351351351351352, "no_speech_prob": 8.206076745409518e-05}, {"id": 542, "seek": 404076, "start": 4040.76, "end": 4050.2000000000003, "text": " build a good strong model was to approach the problem with an LSTM. So now in 2021 actually LSTMs", "tokens": [1322, 257, 665, 2068, 2316, 390, 281, 3109, 264, 1154, 365, 364, 441, 6840, 44, 13, 407, 586, 294, 7201, 767, 441, 6840, 26386], "temperature": 0.0, "avg_logprob": -0.10025067025042594, "compression_ratio": 1.5810276679841897, "no_speech_prob": 4.389639070723206e-05}, {"id": 543, "seek": 404076, "start": 4050.2000000000003, "end": 4056.36, "text": " are starting to be supplanted or have been supplanted by other approaches particularly transformer models", "tokens": [366, 2891, 281, 312, 9386, 15587, 420, 362, 668, 9386, 15587, 538, 661, 11587, 4098, 31782, 5245], "temperature": 0.0, "avg_logprob": -0.10025067025042594, "compression_ratio": 1.5810276679841897, "no_speech_prob": 4.389639070723206e-05}, {"id": 544, "seek": 404076, "start": 4057.4, "end": 4062.84, "text": " which we'll get to in the class in a couple of weeks time. So this is the sort of picture you can see.", "tokens": [597, 321, 603, 483, 281, 294, 264, 1508, 294, 257, 1916, 295, 3259, 565, 13, 407, 341, 307, 264, 1333, 295, 3036, 291, 393, 536, 13], "temperature": 0.0, "avg_logprob": -0.10025067025042594, "compression_ratio": 1.5810276679841897, "no_speech_prob": 4.389639070723206e-05}, {"id": 545, "seek": 404076, "start": 4062.84, "end": 4069.0, "text": " So for many years there's been a machine translation conference and so a Bake Off competition", "tokens": [407, 337, 867, 924, 456, 311, 668, 257, 3479, 12853, 7586, 293, 370, 257, 42597, 6318, 6211], "temperature": 0.0, "avg_logprob": -0.10025067025042594, "compression_ratio": 1.5810276679841897, "no_speech_prob": 4.389639070723206e-05}, {"id": 546, "seek": 406900, "start": 4069.0, "end": 4077.8, "text": " called WMT workshop on machine translation. So if you look at the history of that in WMT 2014", "tokens": [1219, 343, 44, 51, 13541, 322, 3479, 12853, 13, 407, 498, 291, 574, 412, 264, 2503, 295, 300, 294, 343, 44, 51, 8227], "temperature": 0.0, "avg_logprob": -0.0964299663901329, "compression_ratio": 1.622093023255814, "no_speech_prob": 0.00018588631064631045}, {"id": 547, "seek": 406900, "start": 4078.84, "end": 4085.72, "text": " there was zero neural machine translation systems in the competition. 2014 was actually the first", "tokens": [456, 390, 4018, 18161, 3479, 12853, 3652, 294, 264, 6211, 13, 8227, 390, 767, 264, 700], "temperature": 0.0, "avg_logprob": -0.0964299663901329, "compression_ratio": 1.622093023255814, "no_speech_prob": 0.00018588631064631045}, {"id": 548, "seek": 406900, "start": 4085.72, "end": 4094.84, "text": " year that the success of LSTMs for machine translation was proven in a conference paper", "tokens": [1064, 300, 264, 2245, 295, 441, 6840, 26386, 337, 3479, 12853, 390, 12785, 294, 257, 7586, 3035], "temperature": 0.0, "avg_logprob": -0.0964299663901329, "compression_ratio": 1.622093023255814, "no_speech_prob": 0.00018588631064631045}, {"id": 549, "seek": 409484, "start": 4094.84, "end": 4105.88, "text": " but nothing occurred in this competition. By 2016 everyone had jumped on LSTMs as working great", "tokens": [457, 1825, 11068, 294, 341, 6211, 13, 3146, 6549, 1518, 632, 13864, 322, 441, 6840, 26386, 382, 1364, 869], "temperature": 0.0, "avg_logprob": -0.10374332919265285, "compression_ratio": 1.4450261780104712, "no_speech_prob": 0.0003310300235170871}, {"id": 550, "seek": 409484, "start": 4106.68, "end": 4111.56, "text": " and lots of people including the winner of the competition was using an LSTM model.", "tokens": [293, 3195, 295, 561, 3009, 264, 8507, 295, 264, 6211, 390, 1228, 364, 441, 6840, 44, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10374332919265285, "compression_ratio": 1.4450261780104712, "no_speech_prob": 0.0003310300235170871}, {"id": 551, "seek": 409484, "start": 4112.52, "end": 4121.64, "text": " If you then jump ahead to 2019 then there's relatively little use of LSTMs and the vast majority", "tokens": [759, 291, 550, 3012, 2286, 281, 6071, 550, 456, 311, 7226, 707, 764, 295, 441, 6840, 26386, 293, 264, 8369, 6286], "temperature": 0.0, "avg_logprob": -0.10374332919265285, "compression_ratio": 1.4450261780104712, "no_speech_prob": 0.0003310300235170871}, {"id": 552, "seek": 412164, "start": 4121.64, "end": 4127.8, "text": " of people are now using transformers. So things change quickly in your network land and I keep", "tokens": [295, 561, 366, 586, 1228, 4088, 433, 13, 407, 721, 1319, 2661, 294, 428, 3209, 2117, 293, 286, 1066], "temperature": 0.0, "avg_logprob": -0.11286284706809303, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.00014170406211633235}, {"id": 553, "seek": 412164, "start": 4127.8, "end": 4136.84, "text": " on having to rewrite these lectures. So quick further note on vanishing and exploding gradients.", "tokens": [322, 1419, 281, 28132, 613, 16564, 13, 407, 1702, 3052, 3637, 322, 3161, 3807, 293, 35175, 2771, 2448, 13], "temperature": 0.0, "avg_logprob": -0.11286284706809303, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.00014170406211633235}, {"id": 554, "seek": 412164, "start": 4136.84, "end": 4143.400000000001, "text": " Is it only a problem with recurrent neural networks? It's not. It's actually a problem that also", "tokens": [1119, 309, 787, 257, 1154, 365, 18680, 1753, 18161, 9590, 30, 467, 311, 406, 13, 467, 311, 767, 257, 1154, 300, 611], "temperature": 0.0, "avg_logprob": -0.11286284706809303, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.00014170406211633235}, {"id": 555, "seek": 412164, "start": 4143.400000000001, "end": 4149.64, "text": " occurs anywhere where you have a lot of depth including feed forward and convolutional neural networks.", "tokens": [11843, 4992, 689, 291, 362, 257, 688, 295, 7161, 3009, 3154, 2128, 293, 45216, 304, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.11286284706809303, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.00014170406211633235}, {"id": 556, "seek": 414964, "start": 4149.64, "end": 4157.72, "text": " As any time when you've got long sequences of chain rules which give you multiplications the", "tokens": [1018, 604, 565, 562, 291, 600, 658, 938, 22978, 295, 5021, 4474, 597, 976, 291, 17596, 763, 264], "temperature": 0.0, "avg_logprob": -0.16934743134871774, "compression_ratio": 1.5, "no_speech_prob": 0.0002486874582245946}, {"id": 557, "seek": 414964, "start": 4157.72, "end": 4166.200000000001, "text": " gradient can become vanishingly small as it back propagates. And so generally sort of lower layers", "tokens": [16235, 393, 1813, 3161, 3807, 356, 1359, 382, 309, 646, 12425, 1024, 13, 400, 370, 5101, 1333, 295, 3126, 7914], "temperature": 0.0, "avg_logprob": -0.16934743134871774, "compression_ratio": 1.5, "no_speech_prob": 0.0002486874582245946}, {"id": 558, "seek": 414964, "start": 4166.200000000001, "end": 4172.84, "text": " are learned very slowly in a hard to train. So there's been a lot of effort in other places as well", "tokens": [366, 3264, 588, 5692, 294, 257, 1152, 281, 3847, 13, 407, 456, 311, 668, 257, 688, 295, 4630, 294, 661, 3190, 382, 731], "temperature": 0.0, "avg_logprob": -0.16934743134871774, "compression_ratio": 1.5, "no_speech_prob": 0.0002486874582245946}, {"id": 559, "seek": 417284, "start": 4172.84, "end": 4181.8, "text": " to come up with different architectures that let you learn more efficiently in deep network.", "tokens": [281, 808, 493, 365, 819, 6331, 1303, 300, 718, 291, 1466, 544, 19621, 294, 2452, 3209, 13], "temperature": 0.0, "avg_logprob": -0.18360300625071807, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.00011944224388571456}, {"id": 560, "seek": 417284, "start": 4181.8, "end": 4188.68, "text": " And the commonest way to do that is to add more direct connections that allow the gradient to flow.", "tokens": [400, 264, 2689, 377, 636, 281, 360, 300, 307, 281, 909, 544, 2047, 9271, 300, 2089, 264, 16235, 281, 3095, 13], "temperature": 0.0, "avg_logprob": -0.18360300625071807, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.00011944224388571456}, {"id": 561, "seek": 417284, "start": 4188.68, "end": 4196.04, "text": " So the big thing in vision in the last few years has been resnets where the res stands for residual", "tokens": [407, 264, 955, 551, 294, 5201, 294, 264, 1036, 1326, 924, 575, 668, 725, 77, 1385, 689, 264, 725, 7382, 337, 27980], "temperature": 0.0, "avg_logprob": -0.18360300625071807, "compression_ratio": 1.5698924731182795, "no_speech_prob": 0.00011944224388571456}, {"id": 562, "seek": 419604, "start": 4196.04, "end": 4204.84, "text": " connections. And so the way they made this picture is upside down so the input is at the top is that you", "tokens": [9271, 13, 400, 370, 264, 636, 436, 1027, 341, 3036, 307, 14119, 760, 370, 264, 4846, 307, 412, 264, 1192, 307, 300, 291], "temperature": 0.0, "avg_logprob": -0.13589244418674046, "compression_ratio": 1.6282722513089005, "no_speech_prob": 0.00010378572187619284}, {"id": 563, "seek": 419604, "start": 4205.48, "end": 4212.04, "text": " have these sort of two paths that are summed together. One path is just an identity path and the other", "tokens": [362, 613, 1333, 295, 732, 14518, 300, 366, 2408, 1912, 1214, 13, 1485, 3100, 307, 445, 364, 6575, 3100, 293, 264, 661], "temperature": 0.0, "avg_logprob": -0.13589244418674046, "compression_ratio": 1.6282722513089005, "no_speech_prob": 0.00010378572187619284}, {"id": 564, "seek": 419604, "start": 4212.04, "end": 4218.6, "text": " one goes through some neural network layers. And so therefore it's default behavior is just to preserve", "tokens": [472, 1709, 807, 512, 18161, 3209, 7914, 13, 400, 370, 4412, 309, 311, 7576, 5223, 307, 445, 281, 15665], "temperature": 0.0, "avg_logprob": -0.13589244418674046, "compression_ratio": 1.6282722513089005, "no_speech_prob": 0.00010378572187619284}, {"id": 565, "seek": 421860, "start": 4218.6, "end": 4226.6, "text": " the input which might sound a little bit like what we just saw for LSTMs. There are other methods", "tokens": [264, 4846, 597, 1062, 1626, 257, 707, 857, 411, 437, 321, 445, 1866, 337, 441, 6840, 26386, 13, 821, 366, 661, 7150], "temperature": 0.0, "avg_logprob": -0.16116437315940857, "compression_ratio": 1.4764397905759161, "no_speech_prob": 5.351341314963065e-05}, {"id": 566, "seek": 421860, "start": 4226.6, "end": 4231.88, "text": " that there have been dense nets where you add skip connections forward to every layer.", "tokens": [300, 456, 362, 668, 18011, 36170, 689, 291, 909, 10023, 9271, 2128, 281, 633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.16116437315940857, "compression_ratio": 1.4764397905759161, "no_speech_prob": 5.351341314963065e-05}, {"id": 567, "seek": 421860, "start": 4233.240000000001, "end": 4239.72, "text": " Highway nets were also actually developed by Schmitt Hoover and sort of a reminiscent of what was", "tokens": [30911, 36170, 645, 611, 767, 4743, 538, 2065, 15548, 46382, 293, 1333, 295, 257, 44304, 295, 437, 390], "temperature": 0.0, "avg_logprob": -0.16116437315940857, "compression_ratio": 1.4764397905759161, "no_speech_prob": 5.351341314963065e-05}, {"id": 568, "seek": 423972, "start": 4239.72, "end": 4248.52, "text": " done with LSTMs. So rather than just having an identity connection as a resnet has, it introduces an extra", "tokens": [1096, 365, 441, 6840, 26386, 13, 407, 2831, 813, 445, 1419, 364, 6575, 4984, 382, 257, 725, 7129, 575, 11, 309, 31472, 364, 2857], "temperature": 0.0, "avg_logprob": -0.09159756359988697, "compression_ratio": 1.572192513368984, "no_speech_prob": 1.773910116753541e-05}, {"id": 569, "seek": 423972, "start": 4248.52, "end": 4255.16, "text": " gate. So it looks more like an LSTM which says how much to send the input through the highway", "tokens": [8539, 13, 407, 309, 1542, 544, 411, 364, 441, 6840, 44, 597, 1619, 577, 709, 281, 2845, 264, 4846, 807, 264, 17205], "temperature": 0.0, "avg_logprob": -0.09159756359988697, "compression_ratio": 1.572192513368984, "no_speech_prob": 1.773910116753541e-05}, {"id": 570, "seek": 423972, "start": 4256.360000000001, "end": 4262.76, "text": " versus how much to put it through a neural net layer and those two are then combined into the", "tokens": [5717, 577, 709, 281, 829, 309, 807, 257, 18161, 2533, 4583, 293, 729, 732, 366, 550, 9354, 666, 264], "temperature": 0.0, "avg_logprob": -0.09159756359988697, "compression_ratio": 1.572192513368984, "no_speech_prob": 1.773910116753541e-05}, {"id": 571, "seek": 426276, "start": 4262.76, "end": 4274.12, "text": " output. So essentially this problem occurs anywhere when you have a lot of depth in your layers of", "tokens": [5598, 13, 407, 4476, 341, 1154, 11843, 4992, 562, 291, 362, 257, 688, 295, 7161, 294, 428, 7914, 295], "temperature": 0.0, "avg_logprob": -0.08928108215332031, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.8364998797769658e-05}, {"id": 572, "seek": 426276, "start": 4274.12, "end": 4282.360000000001, "text": " neural network. But it first arose and turns out to be especially problematic with recurrent", "tokens": [18161, 3209, 13, 583, 309, 700, 37192, 293, 4523, 484, 281, 312, 2318, 19011, 365, 18680, 1753], "temperature": 0.0, "avg_logprob": -0.08928108215332031, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.8364998797769658e-05}, {"id": 573, "seek": 426276, "start": 4282.360000000001, "end": 4288.76, "text": " neural networks. They're particularly unstable because of the fact that you've got this one", "tokens": [18161, 9590, 13, 814, 434, 4098, 23742, 570, 295, 264, 1186, 300, 291, 600, 658, 341, 472], "temperature": 0.0, "avg_logprob": -0.08928108215332031, "compression_ratio": 1.5297297297297296, "no_speech_prob": 2.8364998797769658e-05}, {"id": 574, "seek": 428876, "start": 4288.76, "end": 4293.56, "text": " weight matrix that you're repeatedly using through the time sequence.", "tokens": [3364, 8141, 300, 291, 434, 18227, 1228, 807, 264, 565, 8310, 13], "temperature": 0.0, "avg_logprob": -0.1843878210407414, "compression_ratio": 1.3711340206185567, "no_speech_prob": 2.07715856959112e-05}, {"id": 575, "seek": 428876, "start": 4295.64, "end": 4296.04, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1843878210407414, "compression_ratio": 1.3711340206185567, "no_speech_prob": 2.07715856959112e-05}, {"id": 576, "seek": 428876, "start": 4300.12, "end": 4306.280000000001, "text": " So Chris, we've got a couple of questions more or less about whether you would ever want to use", "tokens": [407, 6688, 11, 321, 600, 658, 257, 1916, 295, 1651, 544, 420, 1570, 466, 1968, 291, 576, 1562, 528, 281, 764], "temperature": 0.0, "avg_logprob": -0.1843878210407414, "compression_ratio": 1.3711340206185567, "no_speech_prob": 2.07715856959112e-05}, {"id": 577, "seek": 428876, "start": 4306.280000000001, "end": 4312.76, "text": " an RN like a simple RNN instead of an LSTM. How does the LSTM learn what to do with its gates?", "tokens": [364, 45702, 411, 257, 2199, 45702, 45, 2602, 295, 364, 441, 6840, 44, 13, 1012, 775, 264, 441, 6840, 44, 1466, 437, 281, 360, 365, 1080, 19792, 30], "temperature": 0.0, "avg_logprob": -0.1843878210407414, "compression_ratio": 1.3711340206185567, "no_speech_prob": 2.07715856959112e-05}, {"id": 578, "seek": 431276, "start": 4312.76, "end": 4322.12, "text": " How can you apply in on those things? Sure. So I think basically the answer is you should never", "tokens": [1012, 393, 291, 3079, 294, 322, 729, 721, 30, 4894, 13, 407, 286, 519, 1936, 264, 1867, 307, 291, 820, 1128], "temperature": 0.0, "avg_logprob": -0.12525767784614067, "compression_ratio": 1.4874371859296482, "no_speech_prob": 5.219121521804482e-05}, {"id": 579, "seek": 431276, "start": 4322.12, "end": 4329.24, "text": " use a simple RNN these days. You should always use an LSTM. I mean, you know, obviously that depends", "tokens": [764, 257, 2199, 45702, 45, 613, 1708, 13, 509, 820, 1009, 764, 364, 441, 6840, 44, 13, 286, 914, 11, 291, 458, 11, 2745, 300, 5946], "temperature": 0.0, "avg_logprob": -0.12525767784614067, "compression_ratio": 1.4874371859296482, "no_speech_prob": 5.219121521804482e-05}, {"id": 580, "seek": 431276, "start": 4329.24, "end": 4334.76, "text": " on what you're doing. If you're wanting to do some kind of analytical paper or something, you might", "tokens": [322, 437, 291, 434, 884, 13, 759, 291, 434, 7935, 281, 360, 512, 733, 295, 29579, 3035, 420, 746, 11, 291, 1062], "temperature": 0.0, "avg_logprob": -0.12525767784614067, "compression_ratio": 1.4874371859296482, "no_speech_prob": 5.219121521804482e-05}, {"id": 581, "seek": 433476, "start": 4334.76, "end": 4343.96, "text": " prefer a simple RNN. And it is the case that you can actually get decent results with simple RNNs", "tokens": [4382, 257, 2199, 45702, 45, 13, 400, 309, 307, 264, 1389, 300, 291, 393, 767, 483, 8681, 3542, 365, 2199, 45702, 45, 82], "temperature": 0.0, "avg_logprob": -0.08559194031883688, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.00012701799278147519}, {"id": 582, "seek": 433476, "start": 4343.96, "end": 4349.96, "text": " providing you're very careful to make sure that things aren't exploding nor vanishing.", "tokens": [6530, 291, 434, 588, 5026, 281, 652, 988, 300, 721, 3212, 380, 35175, 6051, 3161, 3807, 13], "temperature": 0.0, "avg_logprob": -0.08559194031883688, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.00012701799278147519}, {"id": 583, "seek": 433476, "start": 4352.92, "end": 4359.64, "text": " But, you know, in practice, getting simple RNNs to work and preserve long contexts is", "tokens": [583, 11, 291, 458, 11, 294, 3124, 11, 1242, 2199, 45702, 45, 82, 281, 589, 293, 15665, 938, 30628, 307], "temperature": 0.0, "avg_logprob": -0.08559194031883688, "compression_ratio": 1.4917127071823204, "no_speech_prob": 0.00012701799278147519}, {"id": 584, "seek": 435964, "start": 4359.64, "end": 4365.96, "text": " incredibly difficult where you can train LSTMs and they will just work. So really, you should", "tokens": [6252, 2252, 689, 291, 393, 3847, 441, 6840, 26386, 293, 436, 486, 445, 589, 13, 407, 534, 11, 291, 820], "temperature": 0.0, "avg_logprob": -0.1621723175048828, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.26016267738305e-05}, {"id": 585, "seek": 435964, "start": 4365.96, "end": 4374.4400000000005, "text": " always just use an LSTM. Now wait, the second question was... I think there's a bit of confusion", "tokens": [1009, 445, 764, 364, 441, 6840, 44, 13, 823, 1699, 11, 264, 1150, 1168, 390, 485, 286, 519, 456, 311, 257, 857, 295, 15075], "temperature": 0.0, "avg_logprob": -0.1621723175048828, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.26016267738305e-05}, {"id": 586, "seek": 435964, "start": 4374.4400000000005, "end": 4384.52, "text": " about like whether the gates are learning differently. Yeah. So the gates are also just learned. So", "tokens": [466, 411, 1968, 264, 19792, 366, 2539, 7614, 13, 865, 13, 407, 264, 19792, 366, 611, 445, 3264, 13, 407], "temperature": 0.0, "avg_logprob": -0.1621723175048828, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.26016267738305e-05}, {"id": 587, "seek": 438452, "start": 4384.52, "end": 4393.0, "text": " if we go back to these equations, you know, this is the complete model. And when we're training the", "tokens": [498, 321, 352, 646, 281, 613, 11787, 11, 291, 458, 11, 341, 307, 264, 3566, 2316, 13, 400, 562, 321, 434, 3097, 264], "temperature": 0.0, "avg_logprob": -0.13953884333780367, "compression_ratio": 1.5078534031413613, "no_speech_prob": 7.017399912001565e-05}, {"id": 588, "seek": 438452, "start": 4393.0, "end": 4402.92, "text": " model, every one of these parameters, so all of these WU and B's, everything is simultaneously", "tokens": [2316, 11, 633, 472, 295, 613, 9834, 11, 370, 439, 295, 613, 343, 52, 293, 363, 311, 11, 1203, 307, 16561], "temperature": 0.0, "avg_logprob": -0.13953884333780367, "compression_ratio": 1.5078534031413613, "no_speech_prob": 7.017399912001565e-05}, {"id": 589, "seek": 438452, "start": 4402.92, "end": 4412.76, "text": " being trained by BackProp. So that what you hope and indeed it works is the model is learning", "tokens": [885, 8895, 538, 5833, 47, 1513, 13, 407, 300, 437, 291, 1454, 293, 6451, 309, 1985, 307, 264, 2316, 307, 2539], "temperature": 0.0, "avg_logprob": -0.13953884333780367, "compression_ratio": 1.5078534031413613, "no_speech_prob": 7.017399912001565e-05}, {"id": 590, "seek": 441276, "start": 4412.76, "end": 4418.84, "text": " what stuff should I remember for a long time versus what stuff should I forget, what things in", "tokens": [437, 1507, 820, 286, 1604, 337, 257, 938, 565, 5717, 437, 1507, 820, 286, 2870, 11, 437, 721, 294], "temperature": 0.0, "avg_logprob": -0.1515840000576443, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.00034817896084859967}, {"id": 591, "seek": 441276, "start": 4418.84, "end": 4424.92, "text": " the input are important versus what things in the input don't really matter. So it can learn things", "tokens": [264, 4846, 366, 1021, 5717, 437, 721, 294, 264, 4846, 500, 380, 534, 1871, 13, 407, 309, 393, 1466, 721], "temperature": 0.0, "avg_logprob": -0.1515840000576443, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.00034817896084859967}, {"id": 592, "seek": 441276, "start": 4424.92, "end": 4430.92, "text": " like function words like A and D, don't really matter even though everyone uses them in English.", "tokens": [411, 2445, 2283, 411, 316, 293, 413, 11, 500, 380, 534, 1871, 754, 1673, 1518, 4960, 552, 294, 3669, 13], "temperature": 0.0, "avg_logprob": -0.1515840000576443, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.00034817896084859967}, {"id": 593, "seek": 441276, "start": 4431.4800000000005, "end": 4437.64, "text": " So you can just not worry about those. So all of this is learned. And the models do actually", "tokens": [407, 291, 393, 445, 406, 3292, 466, 729, 13, 407, 439, 295, 341, 307, 3264, 13, 400, 264, 5245, 360, 767], "temperature": 0.0, "avg_logprob": -0.1515840000576443, "compression_ratio": 1.761467889908257, "no_speech_prob": 0.00034817896084859967}, {"id": 594, "seek": 443764, "start": 4437.64, "end": 4444.360000000001, "text": " successfully learn gate values about what information is useful to preserve long term versus what", "tokens": [10727, 1466, 8539, 4190, 466, 437, 1589, 307, 4420, 281, 15665, 938, 1433, 5717, 437], "temperature": 0.0, "avg_logprob": -0.13304163534430008, "compression_ratio": 1.6293103448275863, "no_speech_prob": 8.206350321415812e-05}, {"id": 595, "seek": 443764, "start": 4444.360000000001, "end": 4450.04, "text": " information is really only useful short term for predicting the next one or two words.", "tokens": [1589, 307, 534, 787, 4420, 2099, 1433, 337, 32884, 264, 958, 472, 420, 732, 2283, 13], "temperature": 0.0, "avg_logprob": -0.13304163534430008, "compression_ratio": 1.6293103448275863, "no_speech_prob": 8.206350321415812e-05}, {"id": 596, "seek": 443764, "start": 4451.72, "end": 4459.320000000001, "text": " Finally, the gradient improvements due to the... So you said that the addition is really important", "tokens": [6288, 11, 264, 16235, 13797, 3462, 281, 264, 485, 407, 291, 848, 300, 264, 4500, 307, 534, 1021], "temperature": 0.0, "avg_logprob": -0.13304163534430008, "compression_ratio": 1.6293103448275863, "no_speech_prob": 8.206350321415812e-05}, {"id": 597, "seek": 443764, "start": 4459.320000000001, "end": 4463.400000000001, "text": " between the New Cell candidate and the Cell State. I don't think at least a couple of students", "tokens": [1296, 264, 1873, 28859, 11532, 293, 264, 28859, 4533, 13, 286, 500, 380, 519, 412, 1935, 257, 1916, 295, 1731], "temperature": 0.0, "avg_logprob": -0.13304163534430008, "compression_ratio": 1.6293103448275863, "no_speech_prob": 8.206350321415812e-05}, {"id": 598, "seek": 446340, "start": 4463.4, "end": 4469.48, "text": " have sort of questioned that. So if you want to go over that again, then maybe useful. Sure.", "tokens": [362, 1333, 295, 28146, 300, 13, 407, 498, 291, 528, 281, 352, 670, 300, 797, 11, 550, 1310, 4420, 13, 4894, 13], "temperature": 0.0, "avg_logprob": -0.16046056613116197, "compression_ratio": 1.453551912568306, "no_speech_prob": 4.466920654522255e-05}, {"id": 599, "seek": 446340, "start": 4472.679999999999, "end": 4482.04, "text": " So what we would like is an easy way for memory to be preserved long term. And", "tokens": [407, 437, 321, 576, 411, 307, 364, 1858, 636, 337, 4675, 281, 312, 22242, 938, 1433, 13, 400], "temperature": 0.0, "avg_logprob": -0.16046056613116197, "compression_ratio": 1.453551912568306, "no_speech_prob": 4.466920654522255e-05}, {"id": 600, "seek": 446340, "start": 4483.24, "end": 4489.719999999999, "text": " you know, one way, which is what ResNet's use is just to sort of completely have a direct path", "tokens": [291, 458, 11, 472, 636, 11, 597, 307, 437, 5015, 31890, 311, 764, 307, 445, 281, 1333, 295, 2584, 362, 257, 2047, 3100], "temperature": 0.0, "avg_logprob": -0.16046056613116197, "compression_ratio": 1.453551912568306, "no_speech_prob": 4.466920654522255e-05}, {"id": 601, "seek": 448972, "start": 4489.72, "end": 4496.92, "text": " from CT minus one to CT and will preserve entirely the history. So there's kind of... There's", "tokens": [490, 19529, 3175, 472, 281, 19529, 293, 486, 15665, 7696, 264, 2503, 13, 407, 456, 311, 733, 295, 485, 821, 311], "temperature": 0.0, "avg_logprob": -0.1350731440952846, "compression_ratio": 1.518716577540107, "no_speech_prob": 9.600383054930717e-05}, {"id": 602, "seek": 448972, "start": 4496.92, "end": 4506.6, "text": " the fault action of preserving information about the past long term. LSTMs don't quite do that,", "tokens": [264, 7441, 3069, 295, 33173, 1589, 466, 264, 1791, 938, 1433, 13, 441, 6840, 26386, 500, 380, 1596, 360, 300, 11], "temperature": 0.0, "avg_logprob": -0.1350731440952846, "compression_ratio": 1.518716577540107, "no_speech_prob": 9.600383054930717e-05}, {"id": 603, "seek": 448972, "start": 4506.6, "end": 4514.280000000001, "text": " but they allow that function to be easy. So you start off with the previous Cell State and you", "tokens": [457, 436, 2089, 300, 2445, 281, 312, 1858, 13, 407, 291, 722, 766, 365, 264, 3894, 28859, 4533, 293, 291], "temperature": 0.0, "avg_logprob": -0.1350731440952846, "compression_ratio": 1.518716577540107, "no_speech_prob": 9.600383054930717e-05}, {"id": 604, "seek": 451428, "start": 4514.28, "end": 4519.5599999999995, "text": " can forget some of it by the Forget Gate, so you can delete stuff out of your memory that's used for", "tokens": [393, 2870, 512, 295, 309, 538, 264, 18675, 21913, 11, 370, 291, 393, 12097, 1507, 484, 295, 428, 4675, 300, 311, 1143, 337], "temperature": 0.0, "avg_logprob": -0.1841060236880654, "compression_ratio": 1.6638297872340426, "no_speech_prob": 3.8114954804768786e-05}, {"id": 605, "seek": 451428, "start": 4519.5599999999995, "end": 4526.04, "text": " operation. And then while you're going to be able to update the content of the Cell with this,", "tokens": [6916, 13, 400, 550, 1339, 291, 434, 516, 281, 312, 1075, 281, 5623, 264, 2701, 295, 264, 28859, 365, 341, 11], "temperature": 0.0, "avg_logprob": -0.1841060236880654, "compression_ratio": 1.6638297872340426, "no_speech_prob": 3.8114954804768786e-05}, {"id": 606, "seek": 451428, "start": 4526.04, "end": 4533.96, "text": " the right operation that occurs in the plus where depending on the input gate, some parts of what's", "tokens": [264, 558, 6916, 300, 11843, 294, 264, 1804, 689, 5413, 322, 264, 4846, 8539, 11, 512, 3166, 295, 437, 311], "temperature": 0.0, "avg_logprob": -0.1841060236880654, "compression_ratio": 1.6638297872340426, "no_speech_prob": 3.8114954804768786e-05}, {"id": 607, "seek": 451428, "start": 4533.96, "end": 4541.48, "text": " in the Cell will be added to. But you can think of that adding as overlaying extra information.", "tokens": [294, 264, 28859, 486, 312, 3869, 281, 13, 583, 291, 393, 519, 295, 300, 5127, 382, 31741, 278, 2857, 1589, 13], "temperature": 0.0, "avg_logprob": -0.1841060236880654, "compression_ratio": 1.6638297872340426, "no_speech_prob": 3.8114954804768786e-05}, {"id": 608, "seek": 454148, "start": 4541.48, "end": 4547.879999999999, "text": " Everything that was in the Cell that wasn't forgotten is still continuing on to the next time step.", "tokens": [5471, 300, 390, 294, 264, 28859, 300, 2067, 380, 11832, 307, 920, 9289, 322, 281, 264, 958, 565, 1823, 13], "temperature": 0.0, "avg_logprob": -0.15583043457359397, "compression_ratio": 1.6008230452674896, "no_speech_prob": 7.697531691519544e-05}, {"id": 609, "seek": 454148, "start": 4549.959999999999, "end": 4556.12, "text": " And in particular, when you're doing the back propagation through time, that there isn't...", "tokens": [400, 294, 1729, 11, 562, 291, 434, 884, 264, 646, 38377, 807, 565, 11, 300, 456, 1943, 380, 485], "temperature": 0.0, "avg_logprob": -0.15583043457359397, "compression_ratio": 1.6008230452674896, "no_speech_prob": 7.697531691519544e-05}, {"id": 610, "seek": 454148, "start": 4558.28, "end": 4564.5199999999995, "text": " I want to say there isn't a multiplication between CT and CT minus one. And there's this unfortunate", "tokens": [286, 528, 281, 584, 456, 1943, 380, 257, 27290, 1296, 19529, 293, 19529, 3175, 472, 13, 400, 456, 311, 341, 17843], "temperature": 0.0, "avg_logprob": -0.15583043457359397, "compression_ratio": 1.6008230452674896, "no_speech_prob": 7.697531691519544e-05}, {"id": 611, "seek": 454148, "start": 4565.4, "end": 4570.5199999999995, "text": " time symbol here, but remember that's the Hadamard product, which is zeroing out part of it with", "tokens": [565, 5986, 510, 11, 457, 1604, 300, 311, 264, 12298, 335, 515, 1674, 11, 597, 307, 4018, 278, 484, 644, 295, 309, 365], "temperature": 0.0, "avg_logprob": -0.15583043457359397, "compression_ratio": 1.6008230452674896, "no_speech_prob": 7.697531691519544e-05}, {"id": 612, "seek": 457052, "start": 4570.52, "end": 4576.200000000001, "text": " the Forget Gate. It's not a multiplication by a matrix like in the simple RNN.", "tokens": [264, 18675, 21913, 13, 467, 311, 406, 257, 27290, 538, 257, 8141, 411, 294, 264, 2199, 45702, 45, 13], "temperature": 0.0, "avg_logprob": -0.1387014901766213, "compression_ratio": 1.5642201834862386, "no_speech_prob": 5.5435066315112635e-05}, {"id": 613, "seek": 457052, "start": 4582.52, "end": 4587.72, "text": " I hope that's good. Okay, so there are a couple of other things that I", "tokens": [286, 1454, 300, 311, 665, 13, 1033, 11, 370, 456, 366, 257, 1916, 295, 661, 721, 300, 286], "temperature": 0.0, "avg_logprob": -0.1387014901766213, "compression_ratio": 1.5642201834862386, "no_speech_prob": 5.5435066315112635e-05}, {"id": 614, "seek": 457052, "start": 4588.6, "end": 4592.6, "text": " wanted to get through before the end. I guess I'm not going to have time to do both of them,", "tokens": [1415, 281, 483, 807, 949, 264, 917, 13, 286, 2041, 286, 478, 406, 516, 281, 362, 565, 281, 360, 1293, 295, 552, 11], "temperature": 0.0, "avg_logprob": -0.1387014901766213, "compression_ratio": 1.5642201834862386, "no_speech_prob": 5.5435066315112635e-05}, {"id": 615, "seek": 457052, "start": 4592.6, "end": 4598.4400000000005, "text": " I think, so I'll do the last one probably next time. So these are actually simple and easy things,", "tokens": [286, 519, 11, 370, 286, 603, 360, 264, 1036, 472, 1391, 958, 565, 13, 407, 613, 366, 767, 2199, 293, 1858, 721, 11], "temperature": 0.0, "avg_logprob": -0.1387014901766213, "compression_ratio": 1.5642201834862386, "no_speech_prob": 5.5435066315112635e-05}, {"id": 616, "seek": 459844, "start": 4598.44, "end": 4607.719999999999, "text": " but they complete our picture. So I sort of briefly alluded to this example of sentiment classification", "tokens": [457, 436, 3566, 527, 3036, 13, 407, 286, 1333, 295, 10515, 33919, 281, 341, 1365, 295, 16149, 21538], "temperature": 0.0, "avg_logprob": -0.17531252578950265, "compression_ratio": 1.5797872340425532, "no_speech_prob": 0.00023373823205474764}, {"id": 617, "seek": 459844, "start": 4607.719999999999, "end": 4618.04, "text": " where what we could do is run an RNN, maybe an LSTM over a sentence, call this our representation", "tokens": [689, 437, 321, 727, 360, 307, 1190, 364, 45702, 45, 11, 1310, 364, 441, 6840, 44, 670, 257, 8174, 11, 818, 341, 527, 10290], "temperature": 0.0, "avg_logprob": -0.17531252578950265, "compression_ratio": 1.5797872340425532, "no_speech_prob": 0.00023373823205474764}, {"id": 618, "seek": 459844, "start": 4618.04, "end": 4628.04, "text": " of the sentence and you feed it into a softmax classifier to classify for sentiment. So what we", "tokens": [295, 264, 8174, 293, 291, 3154, 309, 666, 257, 2787, 41167, 1508, 9902, 281, 33872, 337, 16149, 13, 407, 437, 321], "temperature": 0.0, "avg_logprob": -0.17531252578950265, "compression_ratio": 1.5797872340425532, "no_speech_prob": 0.00023373823205474764}, {"id": 619, "seek": 462804, "start": 4628.04, "end": 4635.96, "text": " are actually saying there is that we can regard the hidden state as a representation of a word in", "tokens": [366, 767, 1566, 456, 307, 300, 321, 393, 3843, 264, 7633, 1785, 382, 257, 10290, 295, 257, 1349, 294], "temperature": 0.0, "avg_logprob": -0.11753554905162138, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00010057460167445242}, {"id": 620, "seek": 462804, "start": 4635.96, "end": 4645.8, "text": " context, that below that we have just a word vector for terribly, but we then looked at our context", "tokens": [4319, 11, 300, 2507, 300, 321, 362, 445, 257, 1349, 8062, 337, 22903, 11, 457, 321, 550, 2956, 412, 527, 4319], "temperature": 0.0, "avg_logprob": -0.11753554905162138, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00010057460167445242}, {"id": 621, "seek": 462804, "start": 4645.8, "end": 4654.04, "text": " and say, okay, we've now created a hidden state representation for the word terribly in the context", "tokens": [293, 584, 11, 1392, 11, 321, 600, 586, 2942, 257, 7633, 1785, 10290, 337, 264, 1349, 22903, 294, 264, 4319], "temperature": 0.0, "avg_logprob": -0.11753554905162138, "compression_ratio": 1.736842105263158, "no_speech_prob": 0.00010057460167445242}, {"id": 622, "seek": 465404, "start": 4654.04, "end": 4661.4, "text": " of the movie was and that proves to be a really useful idea because words have different meanings", "tokens": [295, 264, 3169, 390, 293, 300, 25019, 281, 312, 257, 534, 4420, 1558, 570, 2283, 362, 819, 28138], "temperature": 0.0, "avg_logprob": -0.13437612732844567, "compression_ratio": 1.5863874345549738, "no_speech_prob": 7.953316526254639e-05}, {"id": 623, "seek": 465404, "start": 4661.4, "end": 4669.0, "text": " in different contexts, but it seems like there's a defect of what we've done here because our context", "tokens": [294, 819, 30628, 11, 457, 309, 2544, 411, 456, 311, 257, 16445, 295, 437, 321, 600, 1096, 510, 570, 527, 4319], "temperature": 0.0, "avg_logprob": -0.13437612732844567, "compression_ratio": 1.5863874345549738, "no_speech_prob": 7.953316526254639e-05}, {"id": 624, "seek": 465404, "start": 4669.0, "end": 4676.6, "text": " only contains information from the left. What about right context? Surely it also be useful to have the", "tokens": [787, 8306, 1589, 490, 264, 1411, 13, 708, 466, 558, 4319, 30, 29803, 309, 611, 312, 4420, 281, 362, 264], "temperature": 0.0, "avg_logprob": -0.13437612732844567, "compression_ratio": 1.5863874345549738, "no_speech_prob": 7.953316526254639e-05}, {"id": 625, "seek": 467660, "start": 4676.6, "end": 4684.4400000000005, "text": " meaning of terribly depend on exciting because often words mean different things based on what follows", "tokens": [3620, 295, 22903, 5672, 322, 4670, 570, 2049, 2283, 914, 819, 721, 2361, 322, 437, 10002], "temperature": 0.0, "avg_logprob": -0.1140229966905382, "compression_ratio": 1.6096256684491979, "no_speech_prob": 3.472793468972668e-05}, {"id": 626, "seek": 467660, "start": 4684.4400000000005, "end": 4692.280000000001, "text": " them. So if you have something like red wine, it means something quite different from a red light.", "tokens": [552, 13, 407, 498, 291, 362, 746, 411, 2182, 7209, 11, 309, 1355, 746, 1596, 819, 490, 257, 2182, 1442, 13], "temperature": 0.0, "avg_logprob": -0.1140229966905382, "compression_ratio": 1.6096256684491979, "no_speech_prob": 3.472793468972668e-05}, {"id": 627, "seek": 467660, "start": 4694.200000000001, "end": 4700.6, "text": " So how could we deal with that? Well, an easy way to deal with that would be to say, well, if we're", "tokens": [407, 577, 727, 321, 2028, 365, 300, 30, 1042, 11, 364, 1858, 636, 281, 2028, 365, 300, 576, 312, 281, 584, 11, 731, 11, 498, 321, 434], "temperature": 0.0, "avg_logprob": -0.1140229966905382, "compression_ratio": 1.6096256684491979, "no_speech_prob": 3.472793468972668e-05}, {"id": 628, "seek": 470060, "start": 4700.6, "end": 4707.0, "text": " just going to come up with a neural encoding of a sentence, we could have a second RNN with", "tokens": [445, 516, 281, 808, 493, 365, 257, 18161, 43430, 295, 257, 8174, 11, 321, 727, 362, 257, 1150, 45702, 45, 365], "temperature": 0.0, "avg_logprob": -0.0740722885614709, "compression_ratio": 1.7718446601941749, "no_speech_prob": 2.8383428798406385e-05}, {"id": 629, "seek": 470060, "start": 4707.0, "end": 4712.84, "text": " completely separate parameters learned and we could run it backwards through the sentence", "tokens": [2584, 4994, 9834, 3264, 293, 321, 727, 1190, 309, 12204, 807, 264, 8174], "temperature": 0.0, "avg_logprob": -0.0740722885614709, "compression_ratio": 1.7718446601941749, "no_speech_prob": 2.8383428798406385e-05}, {"id": 630, "seek": 470060, "start": 4713.4800000000005, "end": 4719.56, "text": " to get a backward representation of each word and then we can get an overall representation of", "tokens": [281, 483, 257, 23897, 10290, 295, 1184, 1349, 293, 550, 321, 393, 483, 364, 4787, 10290, 295], "temperature": 0.0, "avg_logprob": -0.0740722885614709, "compression_ratio": 1.7718446601941749, "no_speech_prob": 2.8383428798406385e-05}, {"id": 631, "seek": 470060, "start": 4719.56, "end": 4725.72, "text": " each word in context by just concatenating those two representations and now we've got a", "tokens": [1184, 1349, 294, 4319, 538, 445, 1588, 7186, 990, 729, 732, 33358, 293, 586, 321, 600, 658, 257], "temperature": 0.0, "avg_logprob": -0.0740722885614709, "compression_ratio": 1.7718446601941749, "no_speech_prob": 2.8383428798406385e-05}, {"id": 632, "seek": 472572, "start": 4725.72, "end": 4735.64, "text": " representation of terribly that has both left and right context. So we're simply running a forward", "tokens": [10290, 295, 22903, 300, 575, 1293, 1411, 293, 558, 4319, 13, 407, 321, 434, 2935, 2614, 257, 2128], "temperature": 0.0, "avg_logprob": -0.11933623569112428, "compression_ratio": 1.4615384615384615, "no_speech_prob": 1.8310538507648744e-05}, {"id": 633, "seek": 472572, "start": 4735.64, "end": 4742.04, "text": " RNN and when I say RNN here, that just means any kind of recurrent neural network so commonly", "tokens": [45702, 45, 293, 562, 286, 584, 45702, 45, 510, 11, 300, 445, 1355, 604, 733, 295, 18680, 1753, 18161, 3209, 370, 12719], "temperature": 0.0, "avg_logprob": -0.11933623569112428, "compression_ratio": 1.4615384615384615, "no_speech_prob": 1.8310538507648744e-05}, {"id": 634, "seek": 472572, "start": 4742.04, "end": 4749.56, "text": " it'll be an LSTM and the backward one and then at each time step we just concatenating their", "tokens": [309, 603, 312, 364, 441, 6840, 44, 293, 264, 23897, 472, 293, 550, 412, 1184, 565, 1823, 321, 445, 1588, 7186, 990, 641], "temperature": 0.0, "avg_logprob": -0.11933623569112428, "compression_ratio": 1.4615384615384615, "no_speech_prob": 1.8310538507648744e-05}, {"id": 635, "seek": 474956, "start": 4749.56, "end": 4757.56, "text": " representations with each of these having separate weights. And so then we regard this concatenated", "tokens": [33358, 365, 1184, 295, 613, 1419, 4994, 17443, 13, 400, 370, 550, 321, 3843, 341, 1588, 7186, 770], "temperature": 0.0, "avg_logprob": -0.09049471341646635, "compression_ratio": 1.5677083333333333, "no_speech_prob": 2.4159859094652347e-05}, {"id": 636, "seek": 474956, "start": 4757.56, "end": 4764.6, "text": " thing as the hidden state, the contextual representation of a token at a particular time that we pass", "tokens": [551, 382, 264, 7633, 1785, 11, 264, 35526, 10290, 295, 257, 14862, 412, 257, 1729, 565, 300, 321, 1320], "temperature": 0.0, "avg_logprob": -0.09049471341646635, "compression_ratio": 1.5677083333333333, "no_speech_prob": 2.4159859094652347e-05}, {"id": 637, "seek": 474956, "start": 4764.6, "end": 4773.8, "text": " forward. This is so common that people use a shortcut to denote that and now just draw this picture", "tokens": [2128, 13, 639, 307, 370, 2689, 300, 561, 764, 257, 24822, 281, 45708, 300, 293, 586, 445, 2642, 341, 3036], "temperature": 0.0, "avg_logprob": -0.09049471341646635, "compression_ratio": 1.5677083333333333, "no_speech_prob": 2.4159859094652347e-05}, {"id": 638, "seek": 477380, "start": 4773.8, "end": 4780.6, "text": " with two sided arrows and when you see that picture with two sided arrows it means that you're", "tokens": [365, 732, 41651, 19669, 293, 562, 291, 536, 300, 3036, 365, 732, 41651, 19669, 309, 1355, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.0943625141197527, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7501433578436263e-05}, {"id": 639, "seek": 477380, "start": 4780.6, "end": 4790.12, "text": " running two RNNs one in each direction and then concatenating their results at each time step", "tokens": [2614, 732, 45702, 45, 82, 472, 294, 1184, 3513, 293, 550, 1588, 7186, 990, 641, 3542, 412, 1184, 565, 1823], "temperature": 0.0, "avg_logprob": -0.0943625141197527, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7501433578436263e-05}, {"id": 640, "seek": 477380, "start": 4790.12, "end": 4799.96, "text": " and that's what you're going to use later in the model. Okay, but so if you're doing an encoding", "tokens": [293, 300, 311, 437, 291, 434, 516, 281, 764, 1780, 294, 264, 2316, 13, 1033, 11, 457, 370, 498, 291, 434, 884, 364, 43430], "temperature": 0.0, "avg_logprob": -0.0943625141197527, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.7501433578436263e-05}, {"id": 641, "seek": 479996, "start": 4799.96, "end": 4808.12, "text": " problem like for sentiment classification or question answering using bidirectional RNNs is a", "tokens": [1154, 411, 337, 16149, 21538, 420, 1168, 13430, 1228, 12957, 621, 41048, 45702, 45, 82, 307, 257], "temperature": 0.0, "avg_logprob": -0.10273654286454363, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.8832324460381642e-05}, {"id": 642, "seek": 479996, "start": 4808.12, "end": 4814.84, "text": " great thing to do but they're only applicable if you have access to the entire input sequence.", "tokens": [869, 551, 281, 360, 457, 436, 434, 787, 21142, 498, 291, 362, 2105, 281, 264, 2302, 4846, 8310, 13], "temperature": 0.0, "avg_logprob": -0.10273654286454363, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.8832324460381642e-05}, {"id": 643, "seek": 479996, "start": 4815.72, "end": 4822.28, "text": " They're not applicable to language modeling because in a language model necessarily you have to", "tokens": [814, 434, 406, 21142, 281, 2856, 15983, 570, 294, 257, 2856, 2316, 4725, 291, 362, 281], "temperature": 0.0, "avg_logprob": -0.10273654286454363, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.8832324460381642e-05}, {"id": 644, "seek": 479996, "start": 4822.28, "end": 4829.4, "text": " generate the next word based on only the preceding context. But if you do have the entire input", "tokens": [8460, 264, 958, 1349, 2361, 322, 787, 264, 16969, 278, 4319, 13, 583, 498, 291, 360, 362, 264, 2302, 4846], "temperature": 0.0, "avg_logprob": -0.10273654286454363, "compression_ratio": 1.6964285714285714, "no_speech_prob": 2.8832324460381642e-05}, {"id": 645, "seek": 482940, "start": 4829.4, "end": 4836.599999999999, "text": " sequence that bidirectionality gives you greater power and indeed that's been an idea that people", "tokens": [8310, 300, 12957, 621, 882, 1860, 2709, 291, 5044, 1347, 293, 6451, 300, 311, 668, 364, 1558, 300, 561], "temperature": 0.0, "avg_logprob": -0.09550161221448113, "compression_ratio": 1.5257731958762886, "no_speech_prob": 6.908177601872012e-05}, {"id": 646, "seek": 482940, "start": 4836.599999999999, "end": 4844.12, "text": " have built on in subsequent work. So when we get to transformers in a couple of weeks we'll spend", "tokens": [362, 3094, 322, 294, 19962, 589, 13, 407, 562, 321, 483, 281, 4088, 433, 294, 257, 1916, 295, 3259, 321, 603, 3496], "temperature": 0.0, "avg_logprob": -0.09550161221448113, "compression_ratio": 1.5257731958762886, "no_speech_prob": 6.908177601872012e-05}, {"id": 647, "seek": 482940, "start": 4844.12, "end": 4851.639999999999, "text": " plenty of time on the BERT model where that acronym stands for bidirectional encoder representations", "tokens": [7140, 295, 565, 322, 264, 363, 31479, 2316, 689, 300, 39195, 7382, 337, 12957, 621, 41048, 2058, 19866, 33358], "temperature": 0.0, "avg_logprob": -0.09550161221448113, "compression_ratio": 1.5257731958762886, "no_speech_prob": 6.908177601872012e-05}, {"id": 648, "seek": 485164, "start": 4851.64, "end": 4860.200000000001, "text": " from transformers. So part of what's important in that model is the transformer but really a central", "tokens": [490, 4088, 433, 13, 407, 644, 295, 437, 311, 1021, 294, 300, 2316, 307, 264, 31782, 457, 534, 257, 5777], "temperature": 0.0, "avg_logprob": -0.07581172762690364, "compression_ratio": 1.52, "no_speech_prob": 3.938172812922858e-05}, {"id": 649, "seek": 485164, "start": 4860.200000000001, "end": 4867.240000000001, "text": " point of the paper was to say that you could build more powerful models using transformers by again", "tokens": [935, 295, 264, 3035, 390, 281, 584, 300, 291, 727, 1322, 544, 4005, 5245, 1228, 4088, 433, 538, 797], "temperature": 0.0, "avg_logprob": -0.07581172762690364, "compression_ratio": 1.52, "no_speech_prob": 3.938172812922858e-05}, {"id": 650, "seek": 485164, "start": 4868.200000000001, "end": 4877.4800000000005, "text": " exploiting bidirectionality. Okay, there's one teeny bit left on RNNs but I'll sneak it into next class", "tokens": [12382, 1748, 12957, 621, 882, 1860, 13, 1033, 11, 456, 311, 472, 48232, 857, 1411, 322, 45702, 45, 82, 457, 286, 603, 13164, 309, 666, 958, 1508], "temperature": 0.0, "avg_logprob": -0.07581172762690364, "compression_ratio": 1.52, "no_speech_prob": 3.938172812922858e-05}, {"id": 651, "seek": 487748, "start": 4877.48, "end": 4883.5599999999995, "text": " and I'll call it the end for today and if there are other things you'd like to ask questions about", "tokens": [293, 286, 603, 818, 309, 264, 917, 337, 965, 293, 498, 456, 366, 661, 721, 291, 1116, 411, 281, 1029, 1651, 466], "temperature": 0.0, "avg_logprob": -0.1916347391465131, "compression_ratio": 1.4014598540145986, "no_speech_prob": 6.346546433633193e-05}, {"id": 652, "seek": 488356, "start": 4883.56, "end": 4913.400000000001, "text": " you can find me on NOX again in just in just a minute. Okay, so see you again next Tuesday.", "tokens": [50364, 291, 393, 915, 385, 322, 9146, 55, 797, 294, 445, 294, 445, 257, 3456, 13, 1033, 11, 370, 536, 291, 797, 958, 10017, 13, 51856], "temperature": 0.0, "avg_logprob": -0.2837762479428892, "compression_ratio": 1.1097560975609757, "no_speech_prob": 0.00043258097139187157}], "language": "en"}