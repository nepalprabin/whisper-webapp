{"text": " Welcome to CS224N, lecture 17. Model analysis and explanation. Okay, look at us. We're here. Start with some course logistics. We have updated the policy on the guest lecture reactions. They're all due Friday. All at 11.59 pm. You can't use late days for this. So please get them in. Watch the lectures. They're awesome lectures. They're awesome guests. And you get something like half a point for each of them. And yeah, all three can be submitted up through Friday. Okay, so final project. Remember that the due date is Tuesday. It's Tuesday at 4.30 pm, March 16th. And let me emphasize that there's a hard deadline on the three days from then Friday. We won't be accepting for additional points off assignments, sorry, final projects that are submitted after the 4.30 deadline on Friday. We need to get these graded and get grades in. So it's the end stretch. Week 9. Week 10 is really the lectures are us giving you help on the final projects. So this is really the last week of lectures. Thanks for all your hard work. And for asking awesome questions and lecture and in office hours and on the ed. And let's get right into it. So today we get to talk about one of my favorite subjects in natural language processing. It's model analysis and explanation. So first we're going to do what I love doing, which is motivating why we want to talk about the topic at all. We'll talk about how we can look at a model at different levels of abstraction to perform different kinds of analysis on it. We'll talk about out of domain evaluation sets. So we'll feel familiar to the robust QA folks. Then we'll talk about sort of trying to figure out for a given example, why did it make the decision that it made? Had some input, it produced some output. And we come up with some sort of interpretable explanation for it. And then we'll look at actually the representations of the models. So these are the sort of hidden states, the vectors that are being built throughout the processing of the model, try to figure out if we can understand some of the representations and mechanisms that the model is performing. And then we'll actually come back to sort of one of the kind of default states that we've been in in this course, which is trying to look at model improvements, removing things from models, seeing how it performs, and relate that to the analysis that we're doing in this lecture, show how it's not all that different. Okay. So if you haven't seen this XKCD, now you have, and it's one of my favorites, I'm going to say all the words. So person A says this is your machine learning system, person B says yep, you pour the data into this big pile of linear algebra, and then collect the answers on the other side, person A, what if the answers are wrong, and person B, just stir the pile until they start looking right. And I feel like at its worst, deep learning can feel like this from time to time. You have a model, maybe it works for some things, maybe it doesn't work for other things, you're not sure why it works for some things and doesn't work for others. And the changes that we make to our models, they're based on intuition, but frequently, what are the TAs told? Everyone in office hours, sometimes you just have to try it and see if it's going to work out because it's very hard to tell. It's very, very difficult to understand our models at any level. And so today we'll go through a number of ways for trying to carve out little bits of understanding here and there. So beyond it being important because it's in the next KCD comic, why should we care about what our models, about understanding our models? One, is that we want to know what our models are doing. So here you have a black box, black box functions, or this idea that you can't look into them and interpret what they're doing. You have an input sentence, say, and then some output prediction. Maybe this black box is actually your final project model and it gets some accuracy. Now we summarize our models and in your final projects you'll summarize your model with sort of one or a handful of summary metrics of accuracy or f1 score or blue score or something. But there's a lot of model to explain with just a small number of metrics. So what do they learn? Why do they succeed and why do they fail? What's another motivation? We want to know what our models are doing. But maybe that's because we want to be able to make tomorrow's model. So today, when you're building models in this class at a company, you start out with some kind of recipe that is known to work either at the company or because you have experience from this class and it's not perfect right in makes mistakes to look at the errors. And then over time, you take what works and then you find what needs changing. So it seems like maybe adding another layer to the model helped. And maybe that's a nice tweak and the model performance gets better, et cetera. And incremental progress doesn't always feel exciting, but I want to pitch to you that it is actually very important for us to understand how much incremental progress can kind of get us towards some of our goals so that we can have a better job of evaluating when we need big leaps, when we need major changes because there are problems that we're attacking with our incremental sort of progress and we're not getting very far. OK, so we want to make tomorrow's model. The thing that's very related to both a part of and bigger than this field of analysis is model biases. So let's say you take your word to that analogy's solver from glove or word to that is from assignment one and you give it the analogy managed to computer programmer as woman is to and it gives you the output home maker. This is a real example from the paper below. You should be like, wow, well, I'm glad I know that now and of course you saw the lecture from Julia. It's what kind of last week you said, wow, I'm glad I know that now and that's a huge problem. What did the model use in its decision? What biases is it learning from data and possibly making even worse? So that's the kind of thing you can also do with model analysis beyond is making models better according to some sort of summary metric as well. And then another thing, we don't just want to make tomorrow's model and this is something that I think is super important. We don't just want to look at that time scale. We want to say, what about 10, 15, 25 years from now, what kinds of things will we be doing? What are the limits? What can be learned by language model pre-training? What's the model that will replace the transformer? What's the model that will replace that model? What does deep learning struggle to do? What are we sort of attacking over and over again and failing to make significant progress on? What do neural models tell us about language potentially? There's some people who are primarily interested in understanding language better using neural networks. Cool. How are our models affecting people, transferring power between groups of people, governments, et cetera? That's an excellent type of analysis. That can't be learned via language model pre-training. That's sort of the complementary question there. If you sort of come to the edge of what you can learn via language model pre-training, is there stuff that we need total paradigm shifts in order to do well? All of this falls under some category of trying to really deeply understand our models and their capabilities. There's a lot of different methods here that will go over today. One thing that I want you to take away from it is that they're all going to tell us some aspect of the model elucidates, some kind of intuition or something, but none of them are we going to say, aha, I really understand 100% about what this model is doing now. They're going to provide some clarity, but never total clarity. One way, if you're trying to decide how you want to understand your model more, I think you should start out by thinking about what level of abstraction do I want to be looking at my model. The very high level abstraction, let's say you've trained a QA model to estimate the probabilities of start and end indices in a reading comprehension problem or you've trained a language model that assigns probabilities to words in context. You can just look at the model as that object. It's just a probability distribution defined by your model. You are not looking into it any further than the fact that you can sort of give it inputs and see what outputs it provides. That's not even who even cares if it's a neural network. It could be anything, but it's a way to understand its behavior. Another level of abstraction that you can look at, you can dig a little deeper. You can say, well, I know that my network is a bunch of layers that are kind of stacked on top of each other, you've got sort of maybe your transformer encoder with your one layer, two layer, three layer, you can try to see what it's doing as it goes deeper in the layers. Maybe your neural model is the sequence of these vector representations. A third option of sort of specificity is to look as much detail as you can. You've got these parameters in there. You've got the connections in the computation graph. Now you're sort of trying to remove all of the abstraction that you can and look at as many details as possible. All three of these ways of looking at your model and performing analysis are going to be useful and will actually sort of travel slowly from one to two to three as we go through this lecture. Okay. We haven't actually talked about any analyses yet. We're going to get started on that now. We're starting with the sort of testing our model's behaviors. So would we want to see, well, my model perform well. I mean, the natural thing to ask is, how does it behave on some sort of test set? And so we don't really care about mechanisms yet. Why is it performing this? By what method is it making its decision? Instead, we're just interested in sort of the more higher level abstraction of like, does it perform the way I wanted to perform? So let's like, take our model evaluation that we are already doing and sort of recast it in the framework of analysis. So you've trained your model on some samples from some distribution. So you've got input, output pairs of some kind. So how does the model behave on samples from the same distribution? It's a simple question and it's sort of, you know, it's known as, you know, in domain accuracy or you can say that the samples are IID and that's what you're testing on. And this is just what we've been doing this whole time. It's your test set accuracy or F1 or blue score. And you know, so you've got some model with some accuracy and maybe it's better than some model with some other accuracy on this test set, right? So this is what you're doing as you're iterating on your models and your final project as well. You say, well, you know, on my test set, which is what I've decided to care about for now, model A does better. They both seem pretty good. And so maybe I'll choose model A to keep working on. Maybe I'll choose it if you were putting something into production. But remember back to, you know, this idea that it's just one number to summarize a very complex system. It's not going to be sufficient to tell you how it's going to perform in a wide variety of settings. Okay, so we've been doing this. This is model evaluation as model analysis. Now we're going to say what if we are not testing on exactly the same type of data that we trained on. So now we're asking, did the model learn something such that it's able to sort of extrapolate or perform how I want it to on data that looks a little bit different from what it was trained on? And we're going to take the example of natural language inference. So to recall the task of natural language inference, and this is through the multi-analye data set that we're just pulling our definition, you have a premise. He turned and saw John sleeping in his half tent, and you have a hypothesis. He saw John was asleep. And then you give them both two of model. And this is the model that we had before that gets some good accuracy. And the model is supposed to tell whether the hypothesis is sort of implied by the premise or contradicting. So you could be contradicting. Maybe if the hypothesis is, you know, John was awake. For example, or he saw John was awake. Maybe that would be contradiction. Or if sort of both could be true at the same time, so to speak. And then in this case, you know, it seems like they're saying that the premise implies the hypothesis. And so, you know, you would say probably this is likely to get the right answer since the accuracy of the model is 95%. And if I percent of the time, we get the right answer. And we're going to dig deeper into that. What if the model is not doing what we think we want it to be doing in order to perform natural language inference? So in a data set like multi-nLI, the authors who gathered the data set will have asked humans to perform the task and, you know, gotten the accuracy that the humans achieved. And models nowadays are achieving accuracies that are around where humans are achieving, which sounds great at first. But as we'll see, it's not the same as actually performing the task more broadly in the right way. So what if the model is not doing something smart effectively? We're going to use a diagnostic test set of carefully constructed examples that seem like things the model should be able to do to test for a specific skill or capacity. In this case, we'll use Hans. So Hans is the heuristic analysis for analyzed systems data set. And it's intended to take systems that do natural language inference and test whether they're using some simple syntactic heuristics. What we'll have in each of these cases, we'll have some heuristic. We'll talk through the definition. We'll get an example. So the first thing is lexical overlap. So the model might do this thing where it assumes that a premise entails all hypotheses constructed from words in the premise. So in this example, you have the premise the doctor was paid by the actor. And then the hypothesis is the doctor paid the actor. And you'll notice that in bold here, get the doctor, and then paid, and then the actor. And so if you use this heuristic, you will think that the doctor was paid by the actor, implies the doctor paid the actor that does not imply it, of course. And so you could expect a model you want the model to be able to do this. It's somewhat simple. But if it's using this heuristic, it won't get this example right. Next is a sub-sequence heuristics. So here, if the model assumes that the premise entails all of its contiguous sub-sequences, it will get this one wrong as well. So this example is the doctor near the actor danced. That's the premise. The hypothesis is the actor danced. Now this is a simple syntactic thing. The doctor is doing the dancing near the actor. Is this prepositional phrase? And so the model sort of uses this heuristic, oh, look, the actor danced. That's a sub-sequence entailed. Awesome. And it'll get this one wrong as well. And here's another one that's a lot like sub-sequence. So if the premise, if the model thinks that the premise entails all complete sub-trees, so this is like sort of fully formed phrases. So the artist slept here is a fully formed sort of, is that sub-tree, if the artist slept, the actor ran, and then that's the premise. Does it entail the hypothesis? The actor slept, no, sorry, the artist slept. That does not entail it because this is in that conditional. Okay, let me pause here for some questions before I move on to see how these models do. Anyone unclear about how this sort of evaluation is being set up? Cool. Okay. Okay, so how do models perform? That's sort of the question of the hour. What we'll do is, we'll look at these results from the same paper that really released the data set. So they took four strong multi-nl i models with the following accuracy. So the accuracy is here are something between 60 and 80 something 80 percent burnt over here is doing the best. And in domain, in that first sort of setting that we talked about, you get these reasonable accuracies. And that is sort of what we said before about it, like looking pretty good. And when we evaluate on Hans, in this setting here, we have examples where the heuristics we talked about actually work. So if the model is using the heuristic, it will get this right. And it gets very high accuracies. And then if we evaluate the model in the settings where if it uses the heuristic, it gets the examples wrong. You know, maybe birds doing like epsilon better than some of the other stuff here, but it's a very different story. Okay. And you saw those examples. They're not complex in our sort of own idea of complexity. And so this is why it sort of feels like a clear failure of the system. Now you can say though that well, maybe the training data sort of wasn't, didn't have any of those sort of phenomena. So the model couldn't have learned not to do that. And that's sort of a reasonable argument except, well, you know, Bert is pre-trained on a bunch of language texts. So you might hope, you might expect, you might hope that it does better. Okay. So we saw that example of models performing well on examples that are like those that it was trained on. And then performing not very well at all on examples that seem reasonable, but are sort of a little bit tricky. Now we're going to take this idea of having a test set that we've carefully crafted and go in a slightly different direction. So we're going to have, what does it mean to try to understand the linguistic properties of our models? Does it? So that's some tactic heuristics question was one thing for natural language inference, but can we sort of test how the models, whether they think certain things are sort of right or wrong as language models? And the first way that we'll do this is we'll ask, well, how do we think about sort of what humans think of as good language? How do we evaluate their sort of preferences about language? And one answer is minimal pairs. And the idea of a minimal pair is that you've got one sentence that sounds okay to a speaker. So this sentence is the chef who made the pizzas is here. It's called, it's an acceptable sentence, at least to me. And then with a small change, a minimal change, the sentence is no longer okay to the speaker. So the chef who made the pizzas are here. And this, whoops, this should be, present tense verbs. In English, present tense verbs agree in number with their subject when they are third person. So chef pizzas, okay. And this is sort of a pretty general thing. Most people don't like this. It's a misconjugated verb. And so the syntax here looks like you have the chef who made the pizzas. And then this arc of agreement in number is requiring the word is here to be singular is instead of plural R. Despite the fact that there's this noun pizzas, which is plural, closer linearly, comes back to dependency parsing. Or back, okay. And what this looks like in the tree structure, right, is well, chef and is are attached in the tree. Chef is the subject of is, pizza is down here in the subtree. And so that subject verb relationship has this sort of agreement thing. So this is a pretty sort of basic and interesting property of language that also reflects the syntactic sort of hierarchical structure of language. So we've been training these language models sampling from them, seeing that they get interesting things. And they tend to seem to generate syntactic content. But does it really understand or does it behave as if it understands this idea of agreement more broadly and does it sort of get the syntax right so that it matches the subjects and the verbs. But language models can't tell us exactly whether they think that a sentence is good or bad, they just tell us the probability of a sentence. So before we had acceptable and unacceptable, that's what we get from humans. And the language models analog is just, does it assign higher probability to the acceptable sentence in the minimal pair, right? So you have the probability under the model of the chef who made the pizzas is here. And then you have the probability under the model of the chef who made the pizzas are here. And you want this probability here to be higher. And if it is, that's sort of like a simple way to test whether the model got it right effectively. And just like in Huns, we can develop a test set with very carefully chosen properties, right? So most sentences in English don't have terribly complex subject verb agreement structure or with a lot of words in the middle like pizzas that are going to make it difficult. So if I say, you know, the dog runs sort of no way to get it wrong because there's no syntax is very simple. So we can create, well, we can look for sentences that have these things called attractors in the sentence. So pizzas is an attractor because the model might be attracted to the plurality here and get the conjugation wrong. So this is our question. Can language models sort of very generally handle these examples with attractors? So we can take examples with zero attractors, see whether the model gets the minimal pairs evaluation right. We can take examples with one attractor, two attractors. You can see how people would still reasonably understand the sentences, right? Chef who made the pizzas and prep the ingredients is. It's still the chef who is. And then on and on and on, it gets rarer, obviously, but you can have more and more attractors. And so now we've created this test set that's intended to evaluate this very specific linguistic phenomenon. So in this paper here, I concur at all, trained an LSTM language model on a subset of Wikipedia back in 2018. And they evaluate it sort of in these buckets that are specified by the paper that sort of introduced subject verb agreement to the NLP field, or more recently at least, and they evaluate it in buckets based on the number of attractors. And so in this table here that you're about to see, the numbers are sort of the percentive times that you get this assign higher probability to the correct sentence in the minimal pair. So if you were just to do random or majority class, you get these errors, oh, sorry, it's the percent of times that you get it wrong. Sorry about that. So lower is better. And so with no attractors, you get very low error rates. So this is 1.3 error rate with a 350-dimensional LSTM. And with one attractor, your error rate is higher, but actually humans start to get errors with more attractors too. So zero attractors is easy. The larger the LSTM, it looks like in general the better you're doing, right? So the smaller models doing worse, OK? And then even on sort of very difficult examples with four attractors, which I try to think of an example in your head like the chef made the pizzas and took out the trash and sort of has to be this long sentence, the error rate is definitely higher, so it gets more difficult, but it's still relatively low. And so even on these very hard examples, models are actually performing subject verb number agreement relatively well. Very cool. OK. Here are some examples that a model got wrong. This is actually a worse model than the ones from the paper that was just there, but I think actually the errors are quite interesting. So here's a sentence, the ship that the player drives has a very high speed. Now this model thought that was less probable than the ship that the player drives have a very high speed. My hypothesis, right, is that it sort of misanalyzes drives as a plural noun, for example, sort of a difficult construction there. I think it's pretty interesting. Likewise here, this one is fun. The lead is also rather long. Five paragraphs is pretty lengthy. So here five paragraphs is a singular noun together, it gets it's like a unit of length, I guess. But the model thought that it was more likely to say five paragraphs are pretty lengthy, because it's referring to this sort of five paragraphs as the five actual paragraphs themselves as opposed to a single unit of length describing the lead. Fascinating. OK. Maybe questions again? So I guess there are a couple. Can we do the similar heuristic analysis for other tasks such as Q and A classification? Yes. So yes, I think that it's easier to do this kind of analysis for the Huns style analysis with question answering and other sorts of tasks, because you can construct examples that similarly have these heuristics and then have the answer depend on the syntax or not. The actual probability of one sentence is higher than the other, of course, is sort of a language model dependent thing. But the idea that you can sort of develop kind of bespoke test sets for various tasks, I think is very, very general. And something that I think is actually quite interesting. Yes. So I won't go on further, but I think the answer is just yes. So there's another one. How do you know where to find these failure cases? Maybe that's the right time to advertise linguistics classes. Sorry. You're still very quiet over here. How do you find what? How do you know where to find these failure cases? Oh, interesting. Yes. How do we know where to find the failure cases? That's a good question. I mean, I think I agree with Chris that actually thinking about what is interesting about things in language is one way to do it. I mean, the heuristics that we saw in our language model, sorry, in our NLI models with hans, you can kind of imagine that they, if the model was sort of ignoring facts about language and sort of just doing this sort of rough bag of words with some extra magic, then it would do well about as bad as it is doing here. And these sorts of ideas about understanding that this statement, if the artist slept the actor ran does not imply the artist slept, is the kind of thing that maybe you'd think up on your own, but also you'd spend time sort of pondering about and thinking broad thoughts about in linguistics curricula as well. Anything else, Chris? So there's also, well, I guess someone was also saying, I think it's about the sort of intervening verbs example, or intervening nouns, sorry, example, but the data set itself probably includes mistakes with higher attractors. Yeah, yeah, that's a good point. Yeah, because humans make more and more mistakes as the number of attractors gets larger. On the other hand, I think that the mistakes are fewer in written text than in spoken. Maybe I'm just making that up, but that's what I think. But yeah, it would be interesting to actually go through that test set and see how many of the errors the really strong model makes are actually due to the sort of observed form being incorrect. I'd be super curious. Okay, should I move on? Yep, great. Okay, so what does it feel like we're doing when we are kind of constructing these sort of bespoke, small, careful test sets for various phenomena? Well, it sort of feels like unit testing. And in fact, this sort of idea has been brought to the fore, you might say an NLP, unit tests but for these NLP neural networks. And in particular, the paper here that I'm citing at the bottom suggests this minimum functionality test. You want a small test set that targets a specific behavior that should sound like some of the things that we were, that we've already talked about. But in this case, we're going to get even more specific. So here's a single test case. We're going to have an expected label, what was actually predicted, whether the model passed this unit test. And the labels are going to be sentiment analysis here. So negative label, positive label, or neutral, or the three options. And the unit test is going to consist simply of sentences that follow this template. I, the navigation, the positive verb, and then the thing. So if you negation positive verb, means you negative verb, right? And so here's an example. I can't say I recommend the food. The expected label is negative. The answer that the model provided, and this is, I think, a commercial sentiment analysis system. I was positive. So it's pretty positive. And then I didn't love the flight. The expected label was negative, and then the predicted answer was neutral. And this commercial sentiment analysis system gets a lot of, well, you could imagine are pretty reasonably simple examples, wrong. And so what your bureau had all 2020 showed is that they could actually provide a system that sort of had this framework of building test cases for NLP models, two ML engineers working on these products. And given that interface, and they would actually find bugs, you know, bugs being categories of high error, right? Find bugs in their models that they could then kind of try to go and fix. And that this was kind of an efficient way of trying to find things that were simple and still wrong with what should be pretty sophisticated neural systems. So I really like this, and it's sort of a nice way of thinking more specifically about what are the capabilities in sort of precise terms of our models. And altogether, now you've seen problems in natural language inference. You've seen language models actually perform pretty well at the language modeling objective. But then you see, you just saw an example of a commercial sentiment analysis system that sort of should do better and doesn't. And this comes with really, I think, broad and important takeaway, which is if you get high accuracy on the end domain test set, you are not guaranteed high accuracy on even what you might consider to be reasonable out of domain evaluations. And life is always out of domain. And if you're building a system that we have given to users, it's immediately out of domain that the very least because it's trained on text that's now older than the things that the users are now saying. So it's a really, really important takeaway that your sort of benchmark accuracy is a single number that does not guarantee good performance on a wide variety of things. And from a, what are our neural networks doing perspective? One way to think about it is that models seem to be learning the data set, fitting sort of the fine-grained, sort of heuristics and statistics that help it fit this one data set as opposed to learning the task. So humans can perform natural language inference if you give them examples from whatever data set. You know, once you've told them how to do the task, they'll be very generally strong at it. But you take your MNLI model and you test it on Hans and it got, you know, whatever that was, below chance accuracy. That's not the kind of thing that you want to see. So it definitely learns the data set well because the accuracy in domain is high. But our models are seemingly not frequently learning, sort of the mechanisms that we would like them to be learning. Last week, we heard about language models and sort of the implicit knowledge that they encode about the world through pre-training. And one of the ways that we sought to interact with language models was providing them with a prompt like Dante was born in mask and then seeing if it puts high probability on the correct continuation, which requires you to access knowledge about where Dante was born. And we didn't frame it this way last week, but this fits into the set of behavioral studies that we've done so far. This is a specific kind of input. You could ask this for multiple types of multiple people. You could swap out Dante for other people. You could swap out born in for, I don't know, died in or something. And then you can, there are like test suites again. And so it's all connected. OK, so I won't go too deep into sort of the knowledge of language models in terms of world knowledge because we've gone over it some. But when you're thinking about ways of interacting with your models, this sort of behavioral study can be very, very general. Even though, remember, we're at still this highest level of abstraction where we're just looking at the probability distributions that are defined. All right. So now we'll go into, so we've sort of looked at understanding in fine grain areas what our model is actually doing. What about sort of why for an individual input is it getting the answer right or wrong? And then are there changes to the inputs that look fine to humans, but actually make the models do a bad job? So one study that I love to reference that really draws back into our original motivation of using LSTM networks instead of simple recurrent neural networks was that they could use long context. But like how long is your long short term memory? And the idea of Kendall well at all 2018 was shuffle or remove contexts that are farther than some k words away, changing k. And if the accuracy, if the predictive ability of your language model, the perplexity, right, doesn't change once you do that, it means the model wasn't actually using that context. I think this is so cool. So on the x-axis, we've got how far away from the word that you're trying to predict, are you actually sort of corrupting, shuffling, or moving stuff from the sequence. And then on the y-axis is the increase in loss. So if the increase in loss is zero, it means that the model was not using the thing that you just removed because if it was using it, it would now do worse without it, right? And so if you shuffle in the blue line here, if you shuffle the history that's farther away from 50 words, the model does not even notice. I think that's really interesting. One it says, everything passed 50 words of this LSTM language model, you could have given it in random order and it wouldn't have noticed. And then two it says that if you're closer than that, it actually is making use of the word order. That's a pretty long memory, okay, that's really interesting. And then if you actually remove the words entirely, you can kind of notice that the words are missing up to 200 words away. So you don't know the order that you don't care about the order they're in, but you care whether they're there or not. And so this is an evaluation of, well, do LSTMs have long term memory? Well, this one at least has effectively no longer than 200 words of memory, but also no less. So very cool. So that's like a general study for a single model. It talks about, it's sort of average behavior over a wide range of examples, but we want to talk about individual predictions on individual inputs. So let's talk about that. So one way of interpreting why did my model make this decision, that's very popular, is for a single example, what parts of the input actually led to the decision? And this is where we come in with saliency maps. So saliency map provides a score for each word indicating its importance to the model's prediction. So you've got something like Bert here. You've got Bert. Bert is making a prediction for this mask. And a mask rush to the emergency room to see her patient. And the predictions that the model is making is, thanks with 47%, it's going to be nurse that's here in the mask instead, or maybe woman, or doctor, or mother, or girl. And then the saliency map is being visualized here in orange. According to this method of saliency called simple gradients, which we'll get into, emergency her and the septokin, it's not worried about the septokin for now, but the emergency and her are the important words apparently. And the septokin shows up in every sentence, so I'm not going to, yeah. And so these two together are, according to this method, what's important for the model to make this prediction to mask. And you can see maybe some statistics, biases, etc., that is picked up in the predictions and then have it mapped out onto the sentence. And this is, well, it seems like it's really helping interpretability. And yeah, I think that this is sort of a very useful tool. And actually, this is part of a demo from Alan and LP that allows you to do this yourself for any sentence that you want. So what's this way of making saliency maps? We're not going to go, there's so many ways to do it. We're going to take a very simple one and work through why it sort of makes sense. So the sort of issue is how do you define importance? What does it mean to be important to the model's prediction? And here's one way of thinking about it. It's called the simple gradient method. It's got a little formula. You got words x1 to xn. Okay? And then you got a model score for a given output class. So maybe you've got, in the birth example, each output class was each output word that you could possibly predict. And then you take the norm of the gradient of the score with respect to each word. Okay, so what we're saying here is the score is sort of the unnormalized probability for that class. Okay, so you got a single class. You're taking the scores like how likely it is not yet normalized by how likely everything else is sort of. So gradient, how much is it going to change if I move it a little bit in one direction or another, and then you take the norm to get a scalar from a vector. So it looks like this. So salience of word i, you have the norm bars on the outside, gradient with respect to xi. So that's if I change a little bit locally xi, how much does my score change? So the idea is that a high gradient norm means that if I were to change it locally, I'd affect the score a lot. That means it was very important to the decision. Let's visualize this a little bit. So here on the y axis we've got loss. Just the loss of the model, sorry, this should be score. It should be score. And on the x axis you've got word space. The word space is like sort of a flattening of the ability to move your word embedding in thousand dimensional space. So I've just plotted it here in one dimension. Now a high saliency thing, you can see that the relationship between what should be score and moving the word in word space, you move it a little bit on the x axis and the score changes a lot. That's that derivative, that's the gradient, awesome, love it. Low saliency, you move the word around locally and the score doesn't change. So that's an interpretation is. That means that the actual identity of this word wasn't that important to the prediction because I could have changed it and the score wouldn't have changed. Now why are there more methods than this? Because honestly reading that sounds awesome, that sounds great. There are sort of a lot of issues with this kind of method in lots of ways of getting around them. Here's one issue. It's not perfect because well maybe your linear approximation that the gradient gives you holds only very, very locally. So here the gradient is zero. So this is a low saliency word because at the bottom of this parabola, but if I were to move even a little bit in either direction, the score would shoot up. Is this not an important word? It seems important to be right there as opposed to anywhere else even sort of nearby in order for the score not to go up. The simple gradients method won't capture this because it just looks at the gradient which is that zero right there. Okay. But if you want to look into more, there's a bunch of different methods that are sort of applied in these papers. And I think that there's a good tool for the toolbox. Okay. So that is one way of explaining a prediction. And it has some issues like why are individual words being scored as opposed to phrases or something like that. But for now, we're going to move on to another type of explanation. And I'm going to check the time. Okay. Cool. Actually, yeah, let me pause for a second. Any questions about this? I mean, the earlier on, they were a couple of questions. One of them was, what are your thoughts on whether looking at attention weights is a methodologically rigorous way of determining the importance of the model places on certain tokens? It seems like there's some back and forth in the literature. That is a great question. And I probably won't engage with that question as much as I could if we had like a second lecture on this. I actually will provide some attention analyses and tell you they're interesting. And then I'll say a little bit about why they can be interesting without being sort of maybe sort of the end all of analysis of where information is flowing in a transformer, for example. I think the debate is something that we would have to get into in a much longer period of time. Look at the slides that I show about attention and the caveats that I provide and let me know if that answers your question first because we have quite a number of slides on it. And if not, please, please ask again and we can chat more about it. And maybe you can go on. Great. Okay. So, I think this is a really fascinating question which also gets at what was important about the input but in actually kind of an even more direct way, which is, could I just keep some minimal part of the input and get the same answer. So, here's an example from Squad. You have this passage in 1899, John Jacob Astor IV invested $100,000 for Tesla. Okay. And then the answer that is being predicted by the model is going to always be in blue in these examples, Colorado Springs experiments. So, you got this passage. And the question is what did Tesla spend Astor's money on? That's why the prediction is Colorado Springs experiments. The model gets the answer right, which is nice. And we would like to think it's because it's doing some kind of reading comprehension. But here's the issue. It turns out, based on this fascinating paper, that if you just reduced the question to did, you actually get exactly the same, you actually get exactly the same answer. And in fact, with the original question, the model had sort of a.78 confidence, you know, probability in that answer. And with the reduced question did, you get even higher confidence. And that, if you give a human this, they would not be able to know really what you're trying to ask about. So, it seems like something is going really wonky here. Here's another. So, here's sort of like a very high level overview of the method. In fact, it actually references our input saline's theme methods. Nice, it's connected. So, you iteratively remove non-salient or unimportant words. So here's a passage again talking about football. I think. Yeah. And, oh, nice. Okay. So, the question is, where did the Broncos practice with a super bowl as the prediction of Stanford University? And that is correct. So again, seems nice. And now, we're not actually going to get the model to be incorrect. We're just going to say, how can I change this question such that I still look at the answer right? So, I'm going to remove the word that was least important according to a saliency method. So, now, it's where did the practice for the super bowl? Already, this is sort of unanswerable because you've got two teams practicing. You don't even know which one you're asking about. So, why the model still thinks it's so confident in Stanford University makes no sense. But you can just sort of keep going. And now, I think, here, the model stops being confident in the answer Stanford University. But I think this is really interesting just to show that if the model is able to do this with very high confidence, it's not reflecting the uncertainty that really should be there because you can't know what you're even asking about. Okay. So, what was important to make this answer? Well, at least these parts were important because you could keep just those parts and get the same answer, fascinating. All right. So, that's sort of the end of the admittedly brief section on thinking about input saliency methods and similar things. Now, we're going to talk about actually breaking models and understanding models by breaking them. Okay. Cool. So, if we have a passage here, Peyton Manning came the first quarterback, something Super Bowl, age 39, past record, held by John L. Wei. Again, we're doing question answering. We got this question. What was the name of the quarterback who was 38 in the Super Bowl? The prediction is correct. Looks good. Now, we're not going to change the question to try to sort of make the question nonsensical while keeping the same answer, instead we're going to change the passage by adding the sentence at the end, which really shouldn't distract anyone. This is quarterback, well known quarterback, Jeff Dean, you know, had jersey number 37 in champ bull. So, this just doesn't, it's really not even related. But now the prediction is Jeff Dean for our nice QA model. And so, this shows as well that it seems like maybe there's this like end of the passage by as to what the answer should be, for example. And so, that's an adversarial example where we flipped the prediction by adding something that is innocuous to humans. And so, sort of like the higher level takeaway is like, oh, it seems like the QA model that we had that seemed good. It's not actually performing QA how we want it to, even though it's in domain accuracy it was good. And here's another example. So, you've got this paragraph with a question, what has been the result of this publicity? The answer is increased scrutiny on teacher misconduct. Now instead of changing the paragraph, we're going to change the question in really, really seemingly in significant ways to change the model's prediction. So first, what HA, and I've got this typo L, then the result of this publicity, the answer changes to teacher misconduct, likely a human would sort of ignore this typo or something and answer the right answer. And then this is really nuts. Instead of asking what has been the result of this publicity, if you ask what's been the result of this publicity, the answer also changes. And this is the author's call, this is semantically equivalent adversary. This is pretty rough. But in general, swapping what for what in this QA model breaks it pretty frequently. And so again, when you go back and sort of re-tinker how to build your model, you're going to be thinking about these things, not just the sort of average accuracy. So that's sort of talking about noise. Our models are bus to noise and their inputs. Our humans are bus to noise. And so this is another question we can ask. And so you can kind of go to this popular sort of meme passed around the internet from time to time where you have all the letters in these words scrambled, you say, according to research or Cambridge University, it doesn't matter in what order the letters in a word are. And so it seems like, I think I did a pretty good job there. And we can be robust as humans to reading and processing the language without actually all that much of a difficulty. So that's maybe something that we might want our models to also be robust to. And it's very practical as well. Noise is a part of all NLP systems inputs at all times. There's just no such thing, effectively, as having users, for example, and not having any noise. And so there's a study that was performed on some popular machine translation models where you train machine translation models in French, German and Czech, I think all to English. And you get blue scores. These blue scores will look a lot better than the ones in your Simon Four because much, much more training data. The idea is these are actually pretty strong machine translation systems. And that's an in domain clean text. Now if you add character swaps like the ones we saw in that sentence about Cambridge, the blue scores take a pretty harsh dive. Not very good. And even if you take somewhat more natural typo noise distribution here, you'll see that you're still getting 20-ish drops in blue score through simply natural noise. And so maybe you'll go back and retrain the model on more types of noise. And then you ask, oh, I do that. Is it robust to even different kinds of noise? These are the questions that are going to be really important. And it's important to know that you're able to break your model really easily so that you can then go and try to make it more robust. OK, now, let's see, 20 minutes. Some. Now we're going to, I guess, yeah. So now we're going to look at the representations of our neural networks. We've talked about sort of their behavior and then whether we could sort of change or observe reasons behind their behavior. Now we'll go into less abstraction, like more at the actual vector representations that are being built by models. And we can answer a different kind of question at the very least than with the other studies. The first thing is related to the question I was asked about attention, which is that some modeling components lend themselves to inspection. Now this is a sentence that I chose somewhat carefully actually because in part of this debate, are they interpretable components? We'll see. But they lend themselves to inspection in the following way. You can visualize them well and you can correlate them easily with various properties. So let's say you have attention heads in Burt. This is from a really nice study that was done here where you look at attention heads of Burt and you say, on most sentences, this attention head had one one seems to do this very sort of global aggregation. Simple kind of operation does this pretty consistently. That's cool. Is it interpretable? Well, maybe, right? So it's the first layer, which means that this word found is sort of uncontextualized. And then, you know, but in deeper layers, the problem is that like once you do some rounds of attention, you've had information mixing and flowing between words. And how do you know exactly what information you're combining, what you're attending to, even, the little hard to tell. And saliency methods more directly sort of evaluate the importance of models. But it's still interesting to see at sort of a local mechanistic point of view what kinds of things are being attended to. So let's take another example. Some attention heads seem to perform simple operations. So you have the global aggregation here that we saw already. Others seem to attend pretty robustly to the next token. Cool. Next token is a great signal. Some heads attend to the CEP token. So here you have attending to CEP. And then maybe some attend to periods. Maybe that's sort of a splitting sentences together and things like that. Not things that are hard to do. But things that some attention had seemed to pretty robustly perform. Again now though, deep in the network, what's actually represented at this period at layer 11? Little unclear. Little unclear. Okay. So some heads though are correlated with really interesting linguistic properties. So this head is actually attending to noun modifiers. So you got this the complicated language in the huge new law. That's pretty fascinating. Even if the model is not like doing this as a causal mechanism to do syntax necessarily, the fact that these things so strongly correlate is actually pretty, pretty cool. And so what we have in all of these studies is we've got sort of an approximate return partition and quantitative analysis relating, like allowing us to reason about very complicated model behavior. They're all approximations, but they're definitely interesting. One other example is that of co-reference. So we saw some work on co-reference. And it seems like this head does a pretty okay job of actually matching up co-referent entities. These are in red. Talks, negotiations, she, her. And that's not obvious how to do that. This is a difficult task. And so it does so with some percentage of the time. And again, it's sort of connecting very complex model behavior to these sort of interpretable summaries of correlating properties. Other cases you can have individual hidden units that lend themselves to interpretation. So here you've got a character level LSTM language model. Which row here is a sentence, if you can't read it, it's totally okay. The interpretation that you should take is that as we walk along the sentence, this single unit is going from I think very negative to very positive or very positive to very negative. I don't really remember. But it's tracking the position in the line. So it's just a linear position unit and pretty robustly doing so across all of these sentences. So this is from a nice visualization study way back in 2016, way back. Here's another cell from that same LSTM language model that seems to sort of turn on inside quotes. So here's a quote and then it turns on. Okay, so I guess that's positive in the blue. End quote here. And then it's negative. Here you start with no quote, negative in the red. See a quote and then blue. Again, very interpretable, also potentially a very useful feature to keep in mind. And this is just an individual unit in the LSTM that you can just look at and see that it does this. Very, very interesting. Even farther on this, and this is actually a study by some AI and neuroscience researchers, we saw the LSTMs were good at subject for a number agreement. Can we figure out the mechanisms by which the LSTM is solving the task? We actually get some insight into that. And so we have a word level language model. The word level language model is going to be a little small, but you have a sentence, the boy, gently, and kindly greets the. And this cell that's being tracked here, so it's an individual hidden unit, one dimension, right, is actually after it sees boy, it sort of starts to go higher. And then it goes down to something very small once it sees greets. And this cell seems to correlate with the scope of a subject for number agreement instance effectively. So here, the boy that watches the dog, that watches the cat greets, you got that cell, again, staying high, maintaining the scope of subject until greets, at which point it stops. What allows it to do that? Probably some complex other dynamics in the network, but it's still a fascinating, I think, insight. And yeah, this is just neuron, 1,150 in this LSTM. Now, so those are sort of all observational studies that you could do by picking out individual components of the model that you can sort of just take each one of and correlating them with some behavior. Now we'll look at a general class of methods called probing by which we still sort of use supervised knowledge, like the knowledge of the type of co-reference that we're looking for. But instead of thinking if it correlates with something that's immediately interpretable, like a attention head, we're going to look into the vector representations of the model and see if these properties can be read out by some simple function. To say, oh, maybe this property was made very easily accessible by my neural network. So let's dig into this. So the general paradigm is that you've got language data that goes into some big pre-trained transformer with fine tuning. And you get state-of-the-art results. So that means state-of-the-art. And so the question for the probing sort of methodology is like, if it's providing these general purpose language representations, what does it actually encode about language? Like, can we quantify this? Can we figure out what kinds of things is learning about language that we seemingly now don't have to tell it? And so you might have something like a sentence, like I record the record. That's an interesting sentence. And you put it into your transformer model with its word embeddings at the beginning, maybe some layers of self-attention and stuff, and you make some predictions. And now our objects of study are going to be these intermediate layers. Right? So it's a vector per word or sub word for every layer. And the question is, like, can we use these linguistic properties like the dependency parsing that we had way back in the early part of the course to understand correlations between properties in the vectors and these things that we can interpret. We can interpret dependency parses. So there are a couple of things that we might want to look for here. You might want to look for semantics. So here, in the sentence, I record the record. I am an agent. That's a semantics thing. Record is a patient. It's the thing I'm recording. You might have syntax. So you might have the syntax tree that you're interested in. That's the dependency parse tree. Maybe you're interested in part of speech, right? Because you have record and record. And the first one's a verb, the second one's a noun. They're identical strings. That's the model encode that one is one and the other is the other. So how do we do this kind of study? So we're going to decide on a layer that we want to analyze. And we're going to freeze Bert. So we're not going to fine tune Bert. All the parameters are frozen. So we decide on layer two of Bert. We're going to pass it some sentences. We decide on what's called a probe family. The question I'm asking is, can I use a model for my family, say linear, to decode a property that I'm interested in really well from this layer? So it's indicating that this property is easily accessible to linear models effectively. So maybe I get a train a model, a train a linear classifier on top of Bert. And I get a really high accuracy. That's sort of interesting already because you know from prior work in part of speech tagging that if you run a linear classifier on simpler features that aren't Bert, you probably don't get as high an accuracy. So that's an interesting sort of takeaway. But then you can also take like a baseline. So I want to compare two layers now. So I've got layer one here. I want to compare it to layer two. I train a probe on it as well. Maybe the accuracy isn't as good. Now I can say, oh wow, look, by layer two, part of speech is more easily accessible to linear functions than it was at layer one. So what did that? Well, the self-attention and feed-forward stuff made it more easily accessible. That's interesting because it's a statement about sort of the information processing of the model. Okay. Okay, so that's, we're going to analyze these layers. Just take a second more to think about it, and you just really give me just a second. So if you have the model representations, h1 to ht, and you have a function family f, that's the subset linear models, or maybe you have like a feed-forward neural network, some fixed set of hyper parameters, freeze the model, train the probe. So you get some predictions for part of speech tagging or whatever. That's just the probe applied to the hidden state of the model. The probe was a member of the probe family, and then the extent that we can predict why is a measure of accessibility. So that's just kind of written out not as pictorially. Okay. So I'm not going to stay on this for too much longer. And it may help in the search for causal mechanisms, but it sort of just gives us a rough understanding of sort of processing of the model and what things are accessible at what layer. So what are some results here? So one result is that BERT, if you run linear probes on it, does really, really well on things that require syntax in part of speech named NETA recognition. Actually in some cases, approximately as well as just doing the very best thing you could possibly do without BERT. So it just makes easily accessible, amazingly strong features for these properties. And that's an interesting sort of emergent quality of BERT, you might say. It seems like as well that the layers of BERT have this property where, so if you look at the columns of this plot here, each column is a task, you've got input words at the sort of layer zero of BERT here, layer 24 is the last layer of BERT large, lower performance is yellow, higher performance is blue, and I, the resolution isn't perfect, but consistently the best place to read out these properties is somewhere a bit past the middle of the model, which is a very consistent rule, which is fascinating. And then it seems as well like if you look at this function of increasingly abstract or increasingly difficult to compute linguistic properties on this axis, an increasing depth in the network on that axis, so the deeper you go in the network, it seems like the more easily you can access more and more abstract linguistic properties, suggesting that that accessibility is being constructed over time by the layers of processing of BERT, so it's building more and more abstract features. Which I think is again, sort of really interesting result. And now I think, yeah, one thing that I think comes to mind that really brings us back right today one is we built intuitions around word to veck. We were asking like what does each dimension of word to veck mean? And the answer was not really anything, but we could build intuitions about it and think about properties of it through sort of these connections between simple mathematical properties of word to veck and linguistic properties that we could sort of understand. So we had this approximation, which is not 100% true, but it's an approximation that says cosine similarity is effectively correlated with semantic similarity. Think about even if all we're going to do at the end of the day is fine tune these word embeddings anyway. Likewise we had this sort of idea about the analogies being encoded by linear offsets. So some relationships are linear in space and they didn't have to be, that's fascinating. This is emergent property that we've now been able to study since we discovered this. Why is that the case in word to veck? And in general, even though you can't interpret the individual dimensions of word to veck, these sort of emergent, interpretable connections between approximately linguistic ideas and sort of simple math on these objects is fascinating. And so one piece of work that sort of extends this idea comes back to dependency parse trees. So they describe the syntax of sentences. And in a paper that I did with Chris, we showed that actually birds and models like it make dependency parse tree structure emergent sort of more easily accessible than one might imagine in its vector space. So if you've got a tree right here, the chef who ran to the store was out of food. So what you can sort of do is think about the tree in terms of distances between words. So you've got the number of edges in the tree between two words is their path distance. So you've got sort of that the distance between chef and was is one. And we're going to use this interpretation of a tree as a distance to make a connection with birds embedding space. And what we were able to show is that under a single linear transformation, the squared Euclidean distance between bird vectors for the same sentence actually correlates well if you choose the B matrix right with the distances in the tree. So here in this Euclidean space that we've transformed, the approximate distance between chef and was is also one. Likewise the difference between was and store is four in the tree. And in my simple sort of transformation of bird space, the distance between store and was is also approximately four. And this is true across a wide range of sentences. And this is like to me a fascinating example of again emergent approximate structure in these very nonlinear models that don't necessarily need to encode things so simply. Okay. All right. Great. So probing studies and correlation studies are I think interesting and pointless in directions to build intuitions about models. But they're not arguments that the model is actually using the thing that you're finding to make a decision. Not causal studies. This is for probing and correlation studies. So and some work that I did around the same time, we showed actually that certain conditions on probes allow you to achieve high accuracy on a task. It's effectively just fitting random labels. And so there's a difficulty of interpreting what the model could or could not be doing with this thing that is somehow easily accessible. It's interesting that this property is easily accessible, but the model might not be doing anything with it, for example, because it's totally random. Likewise, another paper showed that you can achieve high accuracy with a probe, even if the model is trained to know that thing that you're probing for is not useful. And there's causal studies that sort of try to extend this work. It's much more difficult to read this paper than it's a fascinating line of future work. Now in my last two minutes, I want to talk about recasting model tweaks and ablations as analysis. So we had this improvement process where we had a network that was going to work, okay. And we would see whether we could tweak it in simple ways to improve it. And then you could see whether you could remove anything and how it still be okay. And that's kind of like analysis. Like I have my network. Do I want it to like, is it going to be better if it's more complicated, if it's going to be better, if it's simpler, can I get away with it being simpler? And so one example of some folks who did this is they took this idea of multi-headed attention and said, so many heads, all the heads important. And what they showed is that if you train a system with multi-headed attention and then just remove the heads at test time and not use them at all, you can actually do pretty well on the original task, not retraining at all without some of the attention heads, showing that they weren't important. You could just get rid of them after training. And likewise, you can do the same thing for, this is on machine translation, this is on multi-analye, you can actually get away without a large, large percentage of your attention heads. Let's see. Yeah, so another thing that you could think about is questioning sort of the basics of the models that we're building. So we have transformer models that are sort of self-attention, feed-forward, self-attention, feed-forward, but like why in that order, with some of the things emitted here, and this paper asked this question and said, if this is my transformer, self-attention, feed-forward, self-attention, feed-forward, et cetera, et cetera, et cetera. And if I just reordered it so that I had a bunch of self-attention at the head and a bunch of feed-forward at the back, and they tried a bunch of these orderings, and this one actually does better. So this achieves a lower perplexity on a benchmark. And this is a way of analyzing what's important about the architectures that I'm building, and how can they be changed in order to perform better. So neural models are very complex, and they're difficult to characterize and impossible to characterize with a single sort of statistic, I think, for your test set accuracy, especially in domain. And we want to find intuitive descriptions of model behaviors, but we should look at multiple levels of abstraction, and none of them are going to be complete. And someone tells you that their neural network is interpretable. I encourage you to engage critically with that. It's not necessarily false, but like the levels of interpretability and what you can interpret, these are the questions that you should be asking, because it's going to be opaque in some ways, almost definitely. And then bring this lens to your model building as you try to think about how to build better models, even if you're not going to be doing analysis as one of your main driving goals. And with that, good luck on your final projects. I realize we're at time. The teaching staff is really appreciative of your efforts over this difficult quarter. And yeah, I hope there's a lecture left on Thursday, but yeah, this is my last one. So thanks, everyone.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.72, "text": " Welcome to CS224N, lecture 17.", "tokens": [4027, 281, 9460, 7490, 19, 45, 11, 7991, 3282, 13], "temperature": 0.0, "avg_logprob": -0.41220706701278687, "compression_ratio": 1.2822085889570551, "no_speech_prob": 0.10707531124353409}, {"id": 1, "seek": 0, "start": 12.72, "end": 14.92, "text": " Model analysis and explanation.", "tokens": [17105, 5215, 293, 10835, 13], "temperature": 0.0, "avg_logprob": -0.41220706701278687, "compression_ratio": 1.2822085889570551, "no_speech_prob": 0.10707531124353409}, {"id": 2, "seek": 0, "start": 14.92, "end": 16.68, "text": " Okay, look at us.", "tokens": [1033, 11, 574, 412, 505, 13], "temperature": 0.0, "avg_logprob": -0.41220706701278687, "compression_ratio": 1.2822085889570551, "no_speech_prob": 0.10707531124353409}, {"id": 3, "seek": 0, "start": 16.68, "end": 19.16, "text": " We're here.", "tokens": [492, 434, 510, 13], "temperature": 0.0, "avg_logprob": -0.41220706701278687, "compression_ratio": 1.2822085889570551, "no_speech_prob": 0.10707531124353409}, {"id": 4, "seek": 0, "start": 19.16, "end": 21.8, "text": " Start with some course logistics.", "tokens": [6481, 365, 512, 1164, 27420, 13], "temperature": 0.0, "avg_logprob": -0.41220706701278687, "compression_ratio": 1.2822085889570551, "no_speech_prob": 0.10707531124353409}, {"id": 5, "seek": 0, "start": 21.8, "end": 26.52, "text": " We have updated the policy on the guest lecture reactions.", "tokens": [492, 362, 10588, 264, 3897, 322, 264, 8341, 7991, 12215, 13], "temperature": 0.0, "avg_logprob": -0.41220706701278687, "compression_ratio": 1.2822085889570551, "no_speech_prob": 0.10707531124353409}, {"id": 6, "seek": 0, "start": 26.52, "end": 28.84, "text": " They're all due Friday.", "tokens": [814, 434, 439, 3462, 6984, 13], "temperature": 0.0, "avg_logprob": -0.41220706701278687, "compression_ratio": 1.2822085889570551, "no_speech_prob": 0.10707531124353409}, {"id": 7, "seek": 2884, "start": 28.84, "end": 30.72, "text": " All at 11.59 pm.", "tokens": [1057, 412, 2975, 13, 19600, 23023, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 8, "seek": 2884, "start": 30.72, "end": 33.24, "text": " You can't use late days for this.", "tokens": [509, 393, 380, 764, 3469, 1708, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 9, "seek": 2884, "start": 33.24, "end": 35.68, "text": " So please get them in.", "tokens": [407, 1767, 483, 552, 294, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 10, "seek": 2884, "start": 35.68, "end": 36.68, "text": " Watch the lectures.", "tokens": [7277, 264, 16564, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 11, "seek": 2884, "start": 36.68, "end": 37.68, "text": " They're awesome lectures.", "tokens": [814, 434, 3476, 16564, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 12, "seek": 2884, "start": 37.68, "end": 39.92, "text": " They're awesome guests.", "tokens": [814, 434, 3476, 9804, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 13, "seek": 2884, "start": 39.92, "end": 42.6, "text": " And you get something like half a point for each of them.", "tokens": [400, 291, 483, 746, 411, 1922, 257, 935, 337, 1184, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 14, "seek": 2884, "start": 42.6, "end": 46.32, "text": " And yeah, all three can be submitted up through Friday.", "tokens": [400, 1338, 11, 439, 1045, 393, 312, 14405, 493, 807, 6984, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 15, "seek": 2884, "start": 46.32, "end": 48.519999999999996, "text": " Okay, so final project.", "tokens": [1033, 11, 370, 2572, 1716, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 16, "seek": 2884, "start": 48.519999999999996, "end": 51.0, "text": " Remember that the due date is Tuesday.", "tokens": [5459, 300, 264, 3462, 4002, 307, 10017, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 17, "seek": 2884, "start": 51.0, "end": 55.480000000000004, "text": " It's Tuesday at 4.30 pm, March 16th.", "tokens": [467, 311, 10017, 412, 1017, 13, 3446, 23023, 11, 6129, 3165, 392, 13], "temperature": 0.0, "avg_logprob": -0.23423106329781668, "compression_ratio": 1.5191489361702128, "no_speech_prob": 7.248674955917522e-05}, {"id": 18, "seek": 5548, "start": 55.48, "end": 65.08, "text": " And let me emphasize that there's a hard deadline on the three days from then Friday.", "tokens": [400, 718, 385, 16078, 300, 456, 311, 257, 1152, 20615, 322, 264, 1045, 1708, 490, 550, 6984, 13], "temperature": 0.0, "avg_logprob": -0.18557559119330513, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.00024900908465497196}, {"id": 19, "seek": 5548, "start": 65.08, "end": 70.88, "text": " We won't be accepting for additional points off assignments, sorry, final projects that", "tokens": [492, 1582, 380, 312, 17391, 337, 4497, 2793, 766, 22546, 11, 2597, 11, 2572, 4455, 300], "temperature": 0.0, "avg_logprob": -0.18557559119330513, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.00024900908465497196}, {"id": 20, "seek": 5548, "start": 70.88, "end": 75.67999999999999, "text": " are submitted after the 4.30 deadline on Friday.", "tokens": [366, 14405, 934, 264, 1017, 13, 3446, 20615, 322, 6984, 13], "temperature": 0.0, "avg_logprob": -0.18557559119330513, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.00024900908465497196}, {"id": 21, "seek": 5548, "start": 75.67999999999999, "end": 78.08, "text": " We need to get these graded and get grades in.", "tokens": [492, 643, 281, 483, 613, 2771, 292, 293, 483, 18041, 294, 13], "temperature": 0.0, "avg_logprob": -0.18557559119330513, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.00024900908465497196}, {"id": 22, "seek": 5548, "start": 78.08, "end": 80.88, "text": " So it's the end stretch.", "tokens": [407, 309, 311, 264, 917, 5985, 13], "temperature": 0.0, "avg_logprob": -0.18557559119330513, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.00024900908465497196}, {"id": 23, "seek": 5548, "start": 80.88, "end": 81.88, "text": " Week 9.", "tokens": [12615, 1722, 13], "temperature": 0.0, "avg_logprob": -0.18557559119330513, "compression_ratio": 1.532994923857868, "no_speech_prob": 0.00024900908465497196}, {"id": 24, "seek": 8188, "start": 81.88, "end": 87.19999999999999, "text": " Week 10 is really the lectures are us giving you help on the final projects.", "tokens": [12615, 1266, 307, 534, 264, 16564, 366, 505, 2902, 291, 854, 322, 264, 2572, 4455, 13], "temperature": 0.0, "avg_logprob": -0.1536266803741455, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0002529530320316553}, {"id": 25, "seek": 8188, "start": 87.19999999999999, "end": 89.36, "text": " So this is really the last week of lectures.", "tokens": [407, 341, 307, 534, 264, 1036, 1243, 295, 16564, 13], "temperature": 0.0, "avg_logprob": -0.1536266803741455, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0002529530320316553}, {"id": 26, "seek": 8188, "start": 89.36, "end": 91.72, "text": " Thanks for all your hard work.", "tokens": [2561, 337, 439, 428, 1152, 589, 13], "temperature": 0.0, "avg_logprob": -0.1536266803741455, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0002529530320316553}, {"id": 27, "seek": 8188, "start": 91.72, "end": 96.0, "text": " And for asking awesome questions and lecture and in office hours and on the ed.", "tokens": [400, 337, 3365, 3476, 1651, 293, 7991, 293, 294, 3398, 2496, 293, 322, 264, 1257, 13], "temperature": 0.0, "avg_logprob": -0.1536266803741455, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0002529530320316553}, {"id": 28, "seek": 8188, "start": 96.0, "end": 97.72, "text": " And let's get right into it.", "tokens": [400, 718, 311, 483, 558, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.1536266803741455, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0002529530320316553}, {"id": 29, "seek": 8188, "start": 97.72, "end": 104.0, "text": " So today we get to talk about one of my favorite subjects in natural language processing.", "tokens": [407, 965, 321, 483, 281, 751, 466, 472, 295, 452, 2954, 13066, 294, 3303, 2856, 9007, 13], "temperature": 0.0, "avg_logprob": -0.1536266803741455, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0002529530320316553}, {"id": 30, "seek": 8188, "start": 104.0, "end": 107.16, "text": " It's model analysis and explanation.", "tokens": [467, 311, 2316, 5215, 293, 10835, 13], "temperature": 0.0, "avg_logprob": -0.1536266803741455, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0002529530320316553}, {"id": 31, "seek": 8188, "start": 107.16, "end": 111.64, "text": " So first we're going to do what I love doing, which is motivating why we want to talk about", "tokens": [407, 700, 321, 434, 516, 281, 360, 437, 286, 959, 884, 11, 597, 307, 41066, 983, 321, 528, 281, 751, 466], "temperature": 0.0, "avg_logprob": -0.1536266803741455, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.0002529530320316553}, {"id": 32, "seek": 11164, "start": 111.64, "end": 114.36, "text": " the topic at all.", "tokens": [264, 4829, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.17525694710867745, "compression_ratio": 1.6375, "no_speech_prob": 3.4253924241056666e-05}, {"id": 33, "seek": 11164, "start": 114.36, "end": 119.48, "text": " We'll talk about how we can look at a model at different levels of abstraction to perform", "tokens": [492, 603, 751, 466, 577, 321, 393, 574, 412, 257, 2316, 412, 819, 4358, 295, 37765, 281, 2042], "temperature": 0.0, "avg_logprob": -0.17525694710867745, "compression_ratio": 1.6375, "no_speech_prob": 3.4253924241056666e-05}, {"id": 34, "seek": 11164, "start": 119.48, "end": 122.08, "text": " different kinds of analysis on it.", "tokens": [819, 3685, 295, 5215, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.17525694710867745, "compression_ratio": 1.6375, "no_speech_prob": 3.4253924241056666e-05}, {"id": 35, "seek": 11164, "start": 122.08, "end": 125.24, "text": " We'll talk about out of domain evaluation sets.", "tokens": [492, 603, 751, 466, 484, 295, 9274, 13344, 6352, 13], "temperature": 0.0, "avg_logprob": -0.17525694710867745, "compression_ratio": 1.6375, "no_speech_prob": 3.4253924241056666e-05}, {"id": 36, "seek": 11164, "start": 125.24, "end": 130.64, "text": " So we'll feel familiar to the robust QA folks.", "tokens": [407, 321, 603, 841, 4963, 281, 264, 13956, 1249, 32, 4024, 13], "temperature": 0.0, "avg_logprob": -0.17525694710867745, "compression_ratio": 1.6375, "no_speech_prob": 3.4253924241056666e-05}, {"id": 37, "seek": 11164, "start": 130.64, "end": 135.72, "text": " Then we'll talk about sort of trying to figure out for a given example, why did it make", "tokens": [1396, 321, 603, 751, 466, 1333, 295, 1382, 281, 2573, 484, 337, 257, 2212, 1365, 11, 983, 630, 309, 652], "temperature": 0.0, "avg_logprob": -0.17525694710867745, "compression_ratio": 1.6375, "no_speech_prob": 3.4253924241056666e-05}, {"id": 38, "seek": 11164, "start": 135.72, "end": 137.32, "text": " the decision that it made?", "tokens": [264, 3537, 300, 309, 1027, 30], "temperature": 0.0, "avg_logprob": -0.17525694710867745, "compression_ratio": 1.6375, "no_speech_prob": 3.4253924241056666e-05}, {"id": 39, "seek": 11164, "start": 137.32, "end": 139.6, "text": " Had some input, it produced some output.", "tokens": [12298, 512, 4846, 11, 309, 7126, 512, 5598, 13], "temperature": 0.0, "avg_logprob": -0.17525694710867745, "compression_ratio": 1.6375, "no_speech_prob": 3.4253924241056666e-05}, {"id": 40, "seek": 13960, "start": 139.6, "end": 143.79999999999998, "text": " And we come up with some sort of interpretable explanation for it.", "tokens": [400, 321, 808, 493, 365, 512, 1333, 295, 7302, 712, 10835, 337, 309, 13], "temperature": 0.0, "avg_logprob": -0.10195469620204207, "compression_ratio": 1.8416666666666666, "no_speech_prob": 5.559883720707148e-05}, {"id": 41, "seek": 13960, "start": 143.79999999999998, "end": 149.24, "text": " And then we'll look at actually the representations of the models.", "tokens": [400, 550, 321, 603, 574, 412, 767, 264, 33358, 295, 264, 5245, 13], "temperature": 0.0, "avg_logprob": -0.10195469620204207, "compression_ratio": 1.8416666666666666, "no_speech_prob": 5.559883720707148e-05}, {"id": 42, "seek": 13960, "start": 149.24, "end": 154.07999999999998, "text": " So these are the sort of hidden states, the vectors that are being built throughout the processing", "tokens": [407, 613, 366, 264, 1333, 295, 7633, 4368, 11, 264, 18875, 300, 366, 885, 3094, 3710, 264, 9007], "temperature": 0.0, "avg_logprob": -0.10195469620204207, "compression_ratio": 1.8416666666666666, "no_speech_prob": 5.559883720707148e-05}, {"id": 43, "seek": 13960, "start": 154.07999999999998, "end": 158.79999999999998, "text": " of the model, try to figure out if we can understand some of the representations and", "tokens": [295, 264, 2316, 11, 853, 281, 2573, 484, 498, 321, 393, 1223, 512, 295, 264, 33358, 293], "temperature": 0.0, "avg_logprob": -0.10195469620204207, "compression_ratio": 1.8416666666666666, "no_speech_prob": 5.559883720707148e-05}, {"id": 44, "seek": 13960, "start": 158.79999999999998, "end": 161.51999999999998, "text": " mechanisms that the model is performing.", "tokens": [15902, 300, 264, 2316, 307, 10205, 13], "temperature": 0.0, "avg_logprob": -0.10195469620204207, "compression_ratio": 1.8416666666666666, "no_speech_prob": 5.559883720707148e-05}, {"id": 45, "seek": 13960, "start": 161.51999999999998, "end": 165.6, "text": " And then we'll actually come back to sort of one of the kind of default states that", "tokens": [400, 550, 321, 603, 767, 808, 646, 281, 1333, 295, 472, 295, 264, 733, 295, 7576, 4368, 300], "temperature": 0.0, "avg_logprob": -0.10195469620204207, "compression_ratio": 1.8416666666666666, "no_speech_prob": 5.559883720707148e-05}, {"id": 46, "seek": 16560, "start": 165.6, "end": 170.76, "text": " we've been in in this course, which is trying to look at model improvements, removing things", "tokens": [321, 600, 668, 294, 294, 341, 1164, 11, 597, 307, 1382, 281, 574, 412, 2316, 13797, 11, 12720, 721], "temperature": 0.0, "avg_logprob": -0.18444952672841597, "compression_ratio": 1.572072072072072, "no_speech_prob": 4.538297071121633e-05}, {"id": 47, "seek": 16560, "start": 170.76, "end": 175.6, "text": " from models, seeing how it performs, and relate that to the analysis that we're doing in this", "tokens": [490, 5245, 11, 2577, 577, 309, 26213, 11, 293, 10961, 300, 281, 264, 5215, 300, 321, 434, 884, 294, 341], "temperature": 0.0, "avg_logprob": -0.18444952672841597, "compression_ratio": 1.572072072072072, "no_speech_prob": 4.538297071121633e-05}, {"id": 48, "seek": 16560, "start": 175.6, "end": 179.28, "text": " lecture, show how it's not all that different.", "tokens": [7991, 11, 855, 577, 309, 311, 406, 439, 300, 819, 13], "temperature": 0.0, "avg_logprob": -0.18444952672841597, "compression_ratio": 1.572072072072072, "no_speech_prob": 4.538297071121633e-05}, {"id": 49, "seek": 16560, "start": 179.28, "end": 181.28, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18444952672841597, "compression_ratio": 1.572072072072072, "no_speech_prob": 4.538297071121633e-05}, {"id": 50, "seek": 16560, "start": 181.28, "end": 188.12, "text": " So if you haven't seen this XKCD, now you have, and it's one of my favorites, I'm going", "tokens": [407, 498, 291, 2378, 380, 1612, 341, 1783, 42, 16508, 11, 586, 291, 362, 11, 293, 309, 311, 472, 295, 452, 16907, 11, 286, 478, 516], "temperature": 0.0, "avg_logprob": -0.18444952672841597, "compression_ratio": 1.572072072072072, "no_speech_prob": 4.538297071121633e-05}, {"id": 51, "seek": 16560, "start": 188.12, "end": 189.68, "text": " to say all the words.", "tokens": [281, 584, 439, 264, 2283, 13], "temperature": 0.0, "avg_logprob": -0.18444952672841597, "compression_ratio": 1.572072072072072, "no_speech_prob": 4.538297071121633e-05}, {"id": 52, "seek": 18968, "start": 189.68, "end": 196.04000000000002, "text": " So person A says this is your machine learning system, person B says yep, you pour the", "tokens": [407, 954, 316, 1619, 341, 307, 428, 3479, 2539, 1185, 11, 954, 363, 1619, 18633, 11, 291, 2016, 264], "temperature": 0.0, "avg_logprob": -0.18046579689815126, "compression_ratio": 1.7635658914728682, "no_speech_prob": 3.372703940840438e-05}, {"id": 53, "seek": 18968, "start": 196.04000000000002, "end": 201.8, "text": " data into this big pile of linear algebra, and then collect the answers on the other side,", "tokens": [1412, 666, 341, 955, 14375, 295, 8213, 21989, 11, 293, 550, 2500, 264, 6338, 322, 264, 661, 1252, 11], "temperature": 0.0, "avg_logprob": -0.18046579689815126, "compression_ratio": 1.7635658914728682, "no_speech_prob": 3.372703940840438e-05}, {"id": 54, "seek": 18968, "start": 201.8, "end": 206.36, "text": " person A, what if the answers are wrong, and person B, just stir the pile until they", "tokens": [954, 316, 11, 437, 498, 264, 6338, 366, 2085, 11, 293, 954, 363, 11, 445, 8946, 264, 14375, 1826, 436], "temperature": 0.0, "avg_logprob": -0.18046579689815126, "compression_ratio": 1.7635658914728682, "no_speech_prob": 3.372703940840438e-05}, {"id": 55, "seek": 18968, "start": 206.36, "end": 208.60000000000002, "text": " start looking right.", "tokens": [722, 1237, 558, 13], "temperature": 0.0, "avg_logprob": -0.18046579689815126, "compression_ratio": 1.7635658914728682, "no_speech_prob": 3.372703940840438e-05}, {"id": 56, "seek": 18968, "start": 208.60000000000002, "end": 212.88, "text": " And I feel like at its worst, deep learning can feel like this from time to time.", "tokens": [400, 286, 841, 411, 412, 1080, 5855, 11, 2452, 2539, 393, 841, 411, 341, 490, 565, 281, 565, 13], "temperature": 0.0, "avg_logprob": -0.18046579689815126, "compression_ratio": 1.7635658914728682, "no_speech_prob": 3.372703940840438e-05}, {"id": 57, "seek": 18968, "start": 212.88, "end": 217.8, "text": " You have a model, maybe it works for some things, maybe it doesn't work for other things,", "tokens": [509, 362, 257, 2316, 11, 1310, 309, 1985, 337, 512, 721, 11, 1310, 309, 1177, 380, 589, 337, 661, 721, 11], "temperature": 0.0, "avg_logprob": -0.18046579689815126, "compression_ratio": 1.7635658914728682, "no_speech_prob": 3.372703940840438e-05}, {"id": 58, "seek": 21780, "start": 217.8, "end": 221.28, "text": " you're not sure why it works for some things and doesn't work for others.", "tokens": [291, 434, 406, 988, 983, 309, 1985, 337, 512, 721, 293, 1177, 380, 589, 337, 2357, 13], "temperature": 0.0, "avg_logprob": -0.21155138313770294, "compression_ratio": 1.6872852233676976, "no_speech_prob": 7.365116471191868e-05}, {"id": 59, "seek": 21780, "start": 221.28, "end": 227.60000000000002, "text": " And the changes that we make to our models, they're based on intuition, but frequently,", "tokens": [400, 264, 2962, 300, 321, 652, 281, 527, 5245, 11, 436, 434, 2361, 322, 24002, 11, 457, 10374, 11], "temperature": 0.0, "avg_logprob": -0.21155138313770294, "compression_ratio": 1.6872852233676976, "no_speech_prob": 7.365116471191868e-05}, {"id": 60, "seek": 21780, "start": 227.60000000000002, "end": 229.32000000000002, "text": " what are the TAs told?", "tokens": [437, 366, 264, 314, 10884, 1907, 30], "temperature": 0.0, "avg_logprob": -0.21155138313770294, "compression_ratio": 1.6872852233676976, "no_speech_prob": 7.365116471191868e-05}, {"id": 61, "seek": 21780, "start": 229.32000000000002, "end": 232.48000000000002, "text": " Everyone in office hours, sometimes you just have to try it and see if it's going to work", "tokens": [5198, 294, 3398, 2496, 11, 2171, 291, 445, 362, 281, 853, 309, 293, 536, 498, 309, 311, 516, 281, 589], "temperature": 0.0, "avg_logprob": -0.21155138313770294, "compression_ratio": 1.6872852233676976, "no_speech_prob": 7.365116471191868e-05}, {"id": 62, "seek": 21780, "start": 232.48000000000002, "end": 235.24, "text": " out because it's very hard to tell.", "tokens": [484, 570, 309, 311, 588, 1152, 281, 980, 13], "temperature": 0.0, "avg_logprob": -0.21155138313770294, "compression_ratio": 1.6872852233676976, "no_speech_prob": 7.365116471191868e-05}, {"id": 63, "seek": 21780, "start": 235.24, "end": 241.48000000000002, "text": " It's very, very difficult to understand our models at any level.", "tokens": [467, 311, 588, 11, 588, 2252, 281, 1223, 527, 5245, 412, 604, 1496, 13], "temperature": 0.0, "avg_logprob": -0.21155138313770294, "compression_ratio": 1.6872852233676976, "no_speech_prob": 7.365116471191868e-05}, {"id": 64, "seek": 21780, "start": 241.48000000000002, "end": 246.08, "text": " And so today we'll go through a number of ways for trying to carve out little bits of understanding", "tokens": [400, 370, 965, 321, 603, 352, 807, 257, 1230, 295, 2098, 337, 1382, 281, 33832, 484, 707, 9239, 295, 3701], "temperature": 0.0, "avg_logprob": -0.21155138313770294, "compression_ratio": 1.6872852233676976, "no_speech_prob": 7.365116471191868e-05}, {"id": 65, "seek": 21780, "start": 246.08, "end": 247.76000000000002, "text": " here and there.", "tokens": [510, 293, 456, 13], "temperature": 0.0, "avg_logprob": -0.21155138313770294, "compression_ratio": 1.6872852233676976, "no_speech_prob": 7.365116471191868e-05}, {"id": 66, "seek": 24776, "start": 247.76, "end": 256.03999999999996, "text": " So beyond it being important because it's in the next KCD comic, why should we care about", "tokens": [407, 4399, 309, 885, 1021, 570, 309, 311, 294, 264, 958, 591, 16508, 13900, 11, 983, 820, 321, 1127, 466], "temperature": 0.0, "avg_logprob": -0.2097949527558826, "compression_ratio": 1.5862068965517242, "no_speech_prob": 4.906582034891471e-05}, {"id": 67, "seek": 24776, "start": 256.03999999999996, "end": 258.48, "text": " what our models, about understanding our models?", "tokens": [437, 527, 5245, 11, 466, 3701, 527, 5245, 30], "temperature": 0.0, "avg_logprob": -0.2097949527558826, "compression_ratio": 1.5862068965517242, "no_speech_prob": 4.906582034891471e-05}, {"id": 68, "seek": 24776, "start": 258.48, "end": 263.76, "text": " One, is that we want to know what our models are doing.", "tokens": [1485, 11, 307, 300, 321, 528, 281, 458, 437, 527, 5245, 366, 884, 13], "temperature": 0.0, "avg_logprob": -0.2097949527558826, "compression_ratio": 1.5862068965517242, "no_speech_prob": 4.906582034891471e-05}, {"id": 69, "seek": 24776, "start": 263.76, "end": 270.56, "text": " So here you have a black box, black box functions, or this idea that you can't look into", "tokens": [407, 510, 291, 362, 257, 2211, 2424, 11, 2211, 2424, 6828, 11, 420, 341, 1558, 300, 291, 393, 380, 574, 666], "temperature": 0.0, "avg_logprob": -0.2097949527558826, "compression_ratio": 1.5862068965517242, "no_speech_prob": 4.906582034891471e-05}, {"id": 70, "seek": 24776, "start": 270.56, "end": 273.48, "text": " them and interpret what they're doing.", "tokens": [552, 293, 7302, 437, 436, 434, 884, 13], "temperature": 0.0, "avg_logprob": -0.2097949527558826, "compression_ratio": 1.5862068965517242, "no_speech_prob": 4.906582034891471e-05}, {"id": 71, "seek": 27348, "start": 273.48, "end": 277.84000000000003, "text": " You have an input sentence, say, and then some output prediction.", "tokens": [509, 362, 364, 4846, 8174, 11, 584, 11, 293, 550, 512, 5598, 17630, 13], "temperature": 0.0, "avg_logprob": -0.18210097204280806, "compression_ratio": 1.6616915422885572, "no_speech_prob": 3.0710496503161266e-05}, {"id": 72, "seek": 27348, "start": 277.84000000000003, "end": 286.04, "text": " Maybe this black box is actually your final project model and it gets some accuracy.", "tokens": [2704, 341, 2211, 2424, 307, 767, 428, 2572, 1716, 2316, 293, 309, 2170, 512, 14170, 13], "temperature": 0.0, "avg_logprob": -0.18210097204280806, "compression_ratio": 1.6616915422885572, "no_speech_prob": 3.0710496503161266e-05}, {"id": 73, "seek": 27348, "start": 286.04, "end": 291.12, "text": " Now we summarize our models and in your final projects you'll summarize your model with", "tokens": [823, 321, 20858, 527, 5245, 293, 294, 428, 2572, 4455, 291, 603, 20858, 428, 2316, 365], "temperature": 0.0, "avg_logprob": -0.18210097204280806, "compression_ratio": 1.6616915422885572, "no_speech_prob": 3.0710496503161266e-05}, {"id": 74, "seek": 27348, "start": 291.12, "end": 297.52000000000004, "text": " sort of one or a handful of summary metrics of accuracy or f1 score or blue score or", "tokens": [1333, 295, 472, 420, 257, 16458, 295, 12691, 16367, 295, 14170, 420, 283, 16, 6175, 420, 3344, 6175, 420], "temperature": 0.0, "avg_logprob": -0.18210097204280806, "compression_ratio": 1.6616915422885572, "no_speech_prob": 3.0710496503161266e-05}, {"id": 75, "seek": 27348, "start": 297.52000000000004, "end": 299.0, "text": " something.", "tokens": [746, 13], "temperature": 0.0, "avg_logprob": -0.18210097204280806, "compression_ratio": 1.6616915422885572, "no_speech_prob": 3.0710496503161266e-05}, {"id": 76, "seek": 29900, "start": 299.0, "end": 303.6, "text": " But there's a lot of model to explain with just a small number of metrics.", "tokens": [583, 456, 311, 257, 688, 295, 2316, 281, 2903, 365, 445, 257, 1359, 1230, 295, 16367, 13], "temperature": 0.0, "avg_logprob": -0.15101562628225118, "compression_ratio": 1.7, "no_speech_prob": 0.00017667721840552986}, {"id": 77, "seek": 29900, "start": 303.6, "end": 305.12, "text": " So what do they learn?", "tokens": [407, 437, 360, 436, 1466, 30], "temperature": 0.0, "avg_logprob": -0.15101562628225118, "compression_ratio": 1.7, "no_speech_prob": 0.00017667721840552986}, {"id": 78, "seek": 29900, "start": 305.12, "end": 308.56, "text": " Why do they succeed and why do they fail?", "tokens": [1545, 360, 436, 7754, 293, 983, 360, 436, 3061, 30], "temperature": 0.0, "avg_logprob": -0.15101562628225118, "compression_ratio": 1.7, "no_speech_prob": 0.00017667721840552986}, {"id": 79, "seek": 29900, "start": 308.56, "end": 309.56, "text": " What's another motivation?", "tokens": [708, 311, 1071, 12335, 30], "temperature": 0.0, "avg_logprob": -0.15101562628225118, "compression_ratio": 1.7, "no_speech_prob": 0.00017667721840552986}, {"id": 80, "seek": 29900, "start": 309.56, "end": 312.96, "text": " We want to know what our models are doing.", "tokens": [492, 528, 281, 458, 437, 527, 5245, 366, 884, 13], "temperature": 0.0, "avg_logprob": -0.15101562628225118, "compression_ratio": 1.7, "no_speech_prob": 0.00017667721840552986}, {"id": 81, "seek": 29900, "start": 312.96, "end": 317.2, "text": " But maybe that's because we want to be able to make tomorrow's model.", "tokens": [583, 1310, 300, 311, 570, 321, 528, 281, 312, 1075, 281, 652, 4153, 311, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15101562628225118, "compression_ratio": 1.7, "no_speech_prob": 0.00017667721840552986}, {"id": 82, "seek": 29900, "start": 317.2, "end": 323.6, "text": " So today, when you're building models in this class at a company, you start out with some", "tokens": [407, 965, 11, 562, 291, 434, 2390, 5245, 294, 341, 1508, 412, 257, 2237, 11, 291, 722, 484, 365, 512], "temperature": 0.0, "avg_logprob": -0.15101562628225118, "compression_ratio": 1.7, "no_speech_prob": 0.00017667721840552986}, {"id": 83, "seek": 29900, "start": 323.6, "end": 328.6, "text": " kind of recipe that is known to work either at the company or because you have experience", "tokens": [733, 295, 6782, 300, 307, 2570, 281, 589, 2139, 412, 264, 2237, 420, 570, 291, 362, 1752], "temperature": 0.0, "avg_logprob": -0.15101562628225118, "compression_ratio": 1.7, "no_speech_prob": 0.00017667721840552986}, {"id": 84, "seek": 32860, "start": 328.6, "end": 333.6, "text": " from this class and it's not perfect right in makes mistakes to look at the errors.", "tokens": [490, 341, 1508, 293, 309, 311, 406, 2176, 558, 294, 1669, 8038, 281, 574, 412, 264, 13603, 13], "temperature": 0.0, "avg_logprob": -0.17953292359697057, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.0001292511005885899}, {"id": 85, "seek": 32860, "start": 333.6, "end": 339.56, "text": " And then over time, you take what works and then you find what needs changing.", "tokens": [400, 550, 670, 565, 11, 291, 747, 437, 1985, 293, 550, 291, 915, 437, 2203, 4473, 13], "temperature": 0.0, "avg_logprob": -0.17953292359697057, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.0001292511005885899}, {"id": 86, "seek": 32860, "start": 339.56, "end": 343.84000000000003, "text": " So it seems like maybe adding another layer to the model helped.", "tokens": [407, 309, 2544, 411, 1310, 5127, 1071, 4583, 281, 264, 2316, 4254, 13], "temperature": 0.0, "avg_logprob": -0.17953292359697057, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.0001292511005885899}, {"id": 87, "seek": 32860, "start": 343.84000000000003, "end": 349.12, "text": " And maybe that's a nice tweak and the model performance gets better, et cetera.", "tokens": [400, 1310, 300, 311, 257, 1481, 29879, 293, 264, 2316, 3389, 2170, 1101, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.17953292359697057, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.0001292511005885899}, {"id": 88, "seek": 32860, "start": 349.12, "end": 355.04, "text": " And incremental progress doesn't always feel exciting, but I want to pitch to you that", "tokens": [400, 35759, 4205, 1177, 380, 1009, 841, 4670, 11, 457, 286, 528, 281, 7293, 281, 291, 300], "temperature": 0.0, "avg_logprob": -0.17953292359697057, "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.0001292511005885899}, {"id": 89, "seek": 35504, "start": 355.04, "end": 361.16, "text": " it is actually very important for us to understand how much incremental progress can kind of get", "tokens": [309, 307, 767, 588, 1021, 337, 505, 281, 1223, 577, 709, 35759, 4205, 393, 733, 295, 483], "temperature": 0.0, "avg_logprob": -0.14789425532023112, "compression_ratio": 1.646551724137931, "no_speech_prob": 3.704530172399245e-05}, {"id": 90, "seek": 35504, "start": 361.16, "end": 368.28000000000003, "text": " us towards some of our goals so that we can have a better job of evaluating when we need", "tokens": [505, 3030, 512, 295, 527, 5493, 370, 300, 321, 393, 362, 257, 1101, 1691, 295, 27479, 562, 321, 643], "temperature": 0.0, "avg_logprob": -0.14789425532023112, "compression_ratio": 1.646551724137931, "no_speech_prob": 3.704530172399245e-05}, {"id": 91, "seek": 35504, "start": 368.28000000000003, "end": 372.6, "text": " big leaps, when we need major changes because there are problems that we're attacking with", "tokens": [955, 476, 2382, 11, 562, 321, 643, 2563, 2962, 570, 456, 366, 2740, 300, 321, 434, 15010, 365], "temperature": 0.0, "avg_logprob": -0.14789425532023112, "compression_ratio": 1.646551724137931, "no_speech_prob": 3.704530172399245e-05}, {"id": 92, "seek": 35504, "start": 372.6, "end": 376.0, "text": " our incremental sort of progress and we're not getting very far.", "tokens": [527, 35759, 1333, 295, 4205, 293, 321, 434, 406, 1242, 588, 1400, 13], "temperature": 0.0, "avg_logprob": -0.14789425532023112, "compression_ratio": 1.646551724137931, "no_speech_prob": 3.704530172399245e-05}, {"id": 93, "seek": 35504, "start": 376.0, "end": 380.56, "text": " OK, so we want to make tomorrow's model.", "tokens": [2264, 11, 370, 321, 528, 281, 652, 4153, 311, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14789425532023112, "compression_ratio": 1.646551724137931, "no_speech_prob": 3.704530172399245e-05}, {"id": 94, "seek": 38056, "start": 380.56, "end": 387.64, "text": " The thing that's very related to both a part of and bigger than this field of analysis", "tokens": [440, 551, 300, 311, 588, 4077, 281, 1293, 257, 644, 295, 293, 3801, 813, 341, 2519, 295, 5215], "temperature": 0.0, "avg_logprob": -0.30007341172960067, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.32929809903726e-05}, {"id": 95, "seek": 38056, "start": 387.64, "end": 389.64, "text": " is model biases.", "tokens": [307, 2316, 32152, 13], "temperature": 0.0, "avg_logprob": -0.30007341172960067, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.32929809903726e-05}, {"id": 96, "seek": 38056, "start": 389.64, "end": 398.48, "text": " So let's say you take your word to that analogy's solver from glove or word to that is from", "tokens": [407, 718, 311, 584, 291, 747, 428, 1349, 281, 300, 21663, 311, 1404, 331, 490, 26928, 420, 1349, 281, 300, 307, 490], "temperature": 0.0, "avg_logprob": -0.30007341172960067, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.32929809903726e-05}, {"id": 97, "seek": 38056, "start": 398.48, "end": 403.72, "text": " assignment one and you give it the analogy managed to computer programmer as woman is", "tokens": [15187, 472, 293, 291, 976, 309, 264, 21663, 6453, 281, 3820, 32116, 382, 3059, 307], "temperature": 0.0, "avg_logprob": -0.30007341172960067, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.32929809903726e-05}, {"id": 98, "seek": 38056, "start": 403.72, "end": 406.56, "text": " to and it gives you the output home maker.", "tokens": [281, 293, 309, 2709, 291, 264, 5598, 1280, 17127, 13], "temperature": 0.0, "avg_logprob": -0.30007341172960067, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.32929809903726e-05}, {"id": 99, "seek": 40656, "start": 406.56, "end": 410.92, "text": " This is a real example from the paper below.", "tokens": [639, 307, 257, 957, 1365, 490, 264, 3035, 2507, 13], "temperature": 0.0, "avg_logprob": -0.24477462445275258, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.304297883412801e-05}, {"id": 100, "seek": 40656, "start": 410.92, "end": 416.96, "text": " You should be like, wow, well, I'm glad I know that now and of course you saw the lecture", "tokens": [509, 820, 312, 411, 11, 6076, 11, 731, 11, 286, 478, 5404, 286, 458, 300, 586, 293, 295, 1164, 291, 1866, 264, 7991], "temperature": 0.0, "avg_logprob": -0.24477462445275258, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.304297883412801e-05}, {"id": 101, "seek": 40656, "start": 416.96, "end": 418.44, "text": " from Julia.", "tokens": [490, 18551, 13], "temperature": 0.0, "avg_logprob": -0.24477462445275258, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.304297883412801e-05}, {"id": 102, "seek": 40656, "start": 418.44, "end": 424.92, "text": " It's what kind of last week you said, wow, I'm glad I know that now and that's a huge problem.", "tokens": [467, 311, 437, 733, 295, 1036, 1243, 291, 848, 11, 6076, 11, 286, 478, 5404, 286, 458, 300, 586, 293, 300, 311, 257, 2603, 1154, 13], "temperature": 0.0, "avg_logprob": -0.24477462445275258, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.304297883412801e-05}, {"id": 103, "seek": 40656, "start": 424.92, "end": 426.72, "text": " What did the model use in its decision?", "tokens": [708, 630, 264, 2316, 764, 294, 1080, 3537, 30], "temperature": 0.0, "avg_logprob": -0.24477462445275258, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.304297883412801e-05}, {"id": 104, "seek": 40656, "start": 426.72, "end": 430.52, "text": " What biases is it learning from data and possibly making even worse?", "tokens": [708, 32152, 307, 309, 2539, 490, 1412, 293, 6264, 1455, 754, 5324, 30], "temperature": 0.0, "avg_logprob": -0.24477462445275258, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.304297883412801e-05}, {"id": 105, "seek": 40656, "start": 430.52, "end": 435.0, "text": " So that's the kind of thing you can also do with model analysis beyond is making models", "tokens": [407, 300, 311, 264, 733, 295, 551, 291, 393, 611, 360, 365, 2316, 5215, 4399, 307, 1455, 5245], "temperature": 0.0, "avg_logprob": -0.24477462445275258, "compression_ratio": 1.7176470588235293, "no_speech_prob": 5.304297883412801e-05}, {"id": 106, "seek": 43500, "start": 435.0, "end": 439.6, "text": " better according to some sort of summary metric as well.", "tokens": [1101, 4650, 281, 512, 1333, 295, 12691, 20678, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.185508359380129, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.00016082088404800743}, {"id": 107, "seek": 43500, "start": 439.6, "end": 443.28, "text": " And then another thing, we don't just want to make tomorrow's model and this is something", "tokens": [400, 550, 1071, 551, 11, 321, 500, 380, 445, 528, 281, 652, 4153, 311, 2316, 293, 341, 307, 746], "temperature": 0.0, "avg_logprob": -0.185508359380129, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.00016082088404800743}, {"id": 108, "seek": 43500, "start": 443.28, "end": 448.12, "text": " that I think is super important.", "tokens": [300, 286, 519, 307, 1687, 1021, 13], "temperature": 0.0, "avg_logprob": -0.185508359380129, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.00016082088404800743}, {"id": 109, "seek": 43500, "start": 448.12, "end": 450.2, "text": " We don't just want to look at that time scale.", "tokens": [492, 500, 380, 445, 528, 281, 574, 412, 300, 565, 4373, 13], "temperature": 0.0, "avg_logprob": -0.185508359380129, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.00016082088404800743}, {"id": 110, "seek": 43500, "start": 450.2, "end": 456.48, "text": " We want to say, what about 10, 15, 25 years from now, what kinds of things will we be doing?", "tokens": [492, 528, 281, 584, 11, 437, 466, 1266, 11, 2119, 11, 3552, 924, 490, 586, 11, 437, 3685, 295, 721, 486, 321, 312, 884, 30], "temperature": 0.0, "avg_logprob": -0.185508359380129, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.00016082088404800743}, {"id": 111, "seek": 43500, "start": 456.48, "end": 457.72, "text": " What are the limits?", "tokens": [708, 366, 264, 10406, 30], "temperature": 0.0, "avg_logprob": -0.185508359380129, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.00016082088404800743}, {"id": 112, "seek": 43500, "start": 457.72, "end": 461.48, "text": " What can be learned by language model pre-training?", "tokens": [708, 393, 312, 3264, 538, 2856, 2316, 659, 12, 17227, 1760, 30], "temperature": 0.0, "avg_logprob": -0.185508359380129, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.00016082088404800743}, {"id": 113, "seek": 43500, "start": 461.48, "end": 464.0, "text": " What's the model that will replace the transformer?", "tokens": [708, 311, 264, 2316, 300, 486, 7406, 264, 31782, 30], "temperature": 0.0, "avg_logprob": -0.185508359380129, "compression_ratio": 1.688212927756654, "no_speech_prob": 0.00016082088404800743}, {"id": 114, "seek": 46400, "start": 464.0, "end": 466.44, "text": " What's the model that will replace that model?", "tokens": [708, 311, 264, 2316, 300, 486, 7406, 300, 2316, 30], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 115, "seek": 46400, "start": 466.44, "end": 468.12, "text": " What does deep learning struggle to do?", "tokens": [708, 775, 2452, 2539, 7799, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 116, "seek": 46400, "start": 468.12, "end": 472.2, "text": " What are we sort of attacking over and over again and failing to make significant progress", "tokens": [708, 366, 321, 1333, 295, 15010, 670, 293, 670, 797, 293, 18223, 281, 652, 4776, 4205], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 117, "seek": 46400, "start": 472.2, "end": 473.2, "text": " on?", "tokens": [322, 30], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 118, "seek": 46400, "start": 473.2, "end": 475.36, "text": " What do neural models tell us about language potentially?", "tokens": [708, 360, 18161, 5245, 980, 505, 466, 2856, 7263, 30], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 119, "seek": 46400, "start": 475.36, "end": 480.24, "text": " There's some people who are primarily interested in understanding language better using neural", "tokens": [821, 311, 512, 561, 567, 366, 10029, 3102, 294, 3701, 2856, 1101, 1228, 18161], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 120, "seek": 46400, "start": 480.24, "end": 481.24, "text": " networks.", "tokens": [9590, 13], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 121, "seek": 46400, "start": 481.24, "end": 483.08, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 122, "seek": 46400, "start": 483.08, "end": 490.04, "text": " How are our models affecting people, transferring power between groups of people, governments,", "tokens": [1012, 366, 527, 5245, 17476, 561, 11, 31437, 1347, 1296, 3935, 295, 561, 11, 11280, 11], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 123, "seek": 46400, "start": 490.04, "end": 491.04, "text": " et cetera?", "tokens": [1030, 11458, 30], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 124, "seek": 46400, "start": 491.04, "end": 492.84, "text": " That's an excellent type of analysis.", "tokens": [663, 311, 364, 7103, 2010, 295, 5215, 13], "temperature": 0.0, "avg_logprob": -0.1658850794253142, "compression_ratio": 1.6917808219178083, "no_speech_prob": 0.00010065106471301988}, {"id": 125, "seek": 49284, "start": 492.84, "end": 495.4, "text": " That can't be learned via language model pre-training.", "tokens": [663, 393, 380, 312, 3264, 5766, 2856, 2316, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.17403445734041872, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.0001333457912551239}, {"id": 126, "seek": 49284, "start": 495.4, "end": 497.71999999999997, "text": " That's sort of the complementary question there.", "tokens": [663, 311, 1333, 295, 264, 40705, 1168, 456, 13], "temperature": 0.0, "avg_logprob": -0.17403445734041872, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.0001333457912551239}, {"id": 127, "seek": 49284, "start": 497.71999999999997, "end": 502.28, "text": " If you sort of come to the edge of what you can learn via language model pre-training,", "tokens": [759, 291, 1333, 295, 808, 281, 264, 4691, 295, 437, 291, 393, 1466, 5766, 2856, 2316, 659, 12, 17227, 1760, 11], "temperature": 0.0, "avg_logprob": -0.17403445734041872, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.0001333457912551239}, {"id": 128, "seek": 49284, "start": 502.28, "end": 508.64, "text": " is there stuff that we need total paradigm shifts in order to do well?", "tokens": [307, 456, 1507, 300, 321, 643, 3217, 24709, 19201, 294, 1668, 281, 360, 731, 30], "temperature": 0.0, "avg_logprob": -0.17403445734041872, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.0001333457912551239}, {"id": 129, "seek": 49284, "start": 508.64, "end": 514.16, "text": " All of this falls under some category of trying to really deeply understand our models", "tokens": [1057, 295, 341, 8804, 833, 512, 7719, 295, 1382, 281, 534, 8760, 1223, 527, 5245], "temperature": 0.0, "avg_logprob": -0.17403445734041872, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.0001333457912551239}, {"id": 130, "seek": 49284, "start": 514.16, "end": 517.4399999999999, "text": " and their capabilities.", "tokens": [293, 641, 10862, 13], "temperature": 0.0, "avg_logprob": -0.17403445734041872, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.0001333457912551239}, {"id": 131, "seek": 49284, "start": 517.4399999999999, "end": 520.8399999999999, "text": " There's a lot of different methods here that will go over today.", "tokens": [821, 311, 257, 688, 295, 819, 7150, 510, 300, 486, 352, 670, 965, 13], "temperature": 0.0, "avg_logprob": -0.17403445734041872, "compression_ratio": 1.720472440944882, "no_speech_prob": 0.0001333457912551239}, {"id": 132, "seek": 52084, "start": 520.84, "end": 527.72, "text": " One thing that I want you to take away from it is that they're all going to tell us", "tokens": [1485, 551, 300, 286, 528, 291, 281, 747, 1314, 490, 309, 307, 300, 436, 434, 439, 516, 281, 980, 505], "temperature": 0.0, "avg_logprob": -0.17241351403922678, "compression_ratio": 1.7136929460580912, "no_speech_prob": 6.706520798616111e-05}, {"id": 133, "seek": 52084, "start": 527.72, "end": 532.2800000000001, "text": " some aspect of the model elucidates, some kind of intuition or something, but none of them", "tokens": [512, 4171, 295, 264, 2316, 806, 1311, 327, 1024, 11, 512, 733, 295, 24002, 420, 746, 11, 457, 6022, 295, 552], "temperature": 0.0, "avg_logprob": -0.17241351403922678, "compression_ratio": 1.7136929460580912, "no_speech_prob": 6.706520798616111e-05}, {"id": 134, "seek": 52084, "start": 532.2800000000001, "end": 538.08, "text": " are we going to say, aha, I really understand 100% about what this model is doing now.", "tokens": [366, 321, 516, 281, 584, 11, 47340, 11, 286, 534, 1223, 2319, 4, 466, 437, 341, 2316, 307, 884, 586, 13], "temperature": 0.0, "avg_logprob": -0.17241351403922678, "compression_ratio": 1.7136929460580912, "no_speech_prob": 6.706520798616111e-05}, {"id": 135, "seek": 52084, "start": 538.08, "end": 541.96, "text": " They're going to provide some clarity, but never total clarity.", "tokens": [814, 434, 516, 281, 2893, 512, 16992, 11, 457, 1128, 3217, 16992, 13], "temperature": 0.0, "avg_logprob": -0.17241351403922678, "compression_ratio": 1.7136929460580912, "no_speech_prob": 6.706520798616111e-05}, {"id": 136, "seek": 52084, "start": 541.96, "end": 547.2, "text": " One way, if you're trying to decide how you want to understand your model more, I think", "tokens": [1485, 636, 11, 498, 291, 434, 1382, 281, 4536, 577, 291, 528, 281, 1223, 428, 2316, 544, 11, 286, 519], "temperature": 0.0, "avg_logprob": -0.17241351403922678, "compression_ratio": 1.7136929460580912, "no_speech_prob": 6.706520798616111e-05}, {"id": 137, "seek": 54720, "start": 547.2, "end": 552.0400000000001, "text": " you should start out by thinking about what level of abstraction do I want to be looking", "tokens": [291, 820, 722, 484, 538, 1953, 466, 437, 1496, 295, 37765, 360, 286, 528, 281, 312, 1237], "temperature": 0.0, "avg_logprob": -0.16880610714788022, "compression_ratio": 1.6810344827586208, "no_speech_prob": 6.50081128696911e-05}, {"id": 138, "seek": 54720, "start": 552.0400000000001, "end": 554.96, "text": " at my model.", "tokens": [412, 452, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16880610714788022, "compression_ratio": 1.6810344827586208, "no_speech_prob": 6.50081128696911e-05}, {"id": 139, "seek": 54720, "start": 554.96, "end": 562.2, "text": " The very high level abstraction, let's say you've trained a QA model to estimate the probabilities", "tokens": [440, 588, 1090, 1496, 37765, 11, 718, 311, 584, 291, 600, 8895, 257, 1249, 32, 2316, 281, 12539, 264, 33783], "temperature": 0.0, "avg_logprob": -0.16880610714788022, "compression_ratio": 1.6810344827586208, "no_speech_prob": 6.50081128696911e-05}, {"id": 140, "seek": 54720, "start": 562.2, "end": 567.6400000000001, "text": " of start and end indices in a reading comprehension problem or you've trained a language model", "tokens": [295, 722, 293, 917, 43840, 294, 257, 3760, 44991, 1154, 420, 291, 600, 8895, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.16880610714788022, "compression_ratio": 1.6810344827586208, "no_speech_prob": 6.50081128696911e-05}, {"id": 141, "seek": 54720, "start": 567.6400000000001, "end": 570.5200000000001, "text": " that assigns probabilities to words in context.", "tokens": [300, 6269, 82, 33783, 281, 2283, 294, 4319, 13], "temperature": 0.0, "avg_logprob": -0.16880610714788022, "compression_ratio": 1.6810344827586208, "no_speech_prob": 6.50081128696911e-05}, {"id": 142, "seek": 54720, "start": 570.5200000000001, "end": 573.6, "text": " You can just look at the model as that object.", "tokens": [509, 393, 445, 574, 412, 264, 2316, 382, 300, 2657, 13], "temperature": 0.0, "avg_logprob": -0.16880610714788022, "compression_ratio": 1.6810344827586208, "no_speech_prob": 6.50081128696911e-05}, {"id": 143, "seek": 57360, "start": 573.6, "end": 577.72, "text": " It's just a probability distribution defined by your model.", "tokens": [467, 311, 445, 257, 8482, 7316, 7642, 538, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14537286758422852, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.0135814338573255e-05}, {"id": 144, "seek": 57360, "start": 577.72, "end": 581.64, "text": " You are not looking into it any further than the fact that you can sort of give it inputs", "tokens": [509, 366, 406, 1237, 666, 309, 604, 3052, 813, 264, 1186, 300, 291, 393, 1333, 295, 976, 309, 15743], "temperature": 0.0, "avg_logprob": -0.14537286758422852, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.0135814338573255e-05}, {"id": 145, "seek": 57360, "start": 581.64, "end": 585.28, "text": " and see what outputs it provides.", "tokens": [293, 536, 437, 23930, 309, 6417, 13], "temperature": 0.0, "avg_logprob": -0.14537286758422852, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.0135814338573255e-05}, {"id": 146, "seek": 57360, "start": 585.28, "end": 589.52, "text": " That's not even who even cares if it's a neural network.", "tokens": [663, 311, 406, 754, 567, 754, 12310, 498, 309, 311, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.14537286758422852, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.0135814338573255e-05}, {"id": 147, "seek": 57360, "start": 589.52, "end": 593.5600000000001, "text": " It could be anything, but it's a way to understand its behavior.", "tokens": [467, 727, 312, 1340, 11, 457, 309, 311, 257, 636, 281, 1223, 1080, 5223, 13], "temperature": 0.0, "avg_logprob": -0.14537286758422852, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.0135814338573255e-05}, {"id": 148, "seek": 57360, "start": 593.5600000000001, "end": 596.48, "text": " Another level of abstraction that you can look at, you can dig a little deeper.", "tokens": [3996, 1496, 295, 37765, 300, 291, 393, 574, 412, 11, 291, 393, 2528, 257, 707, 7731, 13], "temperature": 0.0, "avg_logprob": -0.14537286758422852, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.0135814338573255e-05}, {"id": 149, "seek": 57360, "start": 596.48, "end": 601.0400000000001, "text": " You can say, well, I know that my network is a bunch of layers that are kind of stacked", "tokens": [509, 393, 584, 11, 731, 11, 286, 458, 300, 452, 3209, 307, 257, 3840, 295, 7914, 300, 366, 733, 295, 28867], "temperature": 0.0, "avg_logprob": -0.14537286758422852, "compression_ratio": 1.6538461538461537, "no_speech_prob": 2.0135814338573255e-05}, {"id": 150, "seek": 60104, "start": 601.04, "end": 606.24, "text": " on top of each other, you've got sort of maybe your transformer encoder with your one", "tokens": [322, 1192, 295, 1184, 661, 11, 291, 600, 658, 1333, 295, 1310, 428, 31782, 2058, 19866, 365, 428, 472], "temperature": 0.0, "avg_logprob": -0.17280082349424009, "compression_ratio": 1.7666666666666666, "no_speech_prob": 3.64726765837986e-05}, {"id": 151, "seek": 60104, "start": 606.24, "end": 609.64, "text": " layer, two layer, three layer, you can try to see what it's doing as it goes deeper", "tokens": [4583, 11, 732, 4583, 11, 1045, 4583, 11, 291, 393, 853, 281, 536, 437, 309, 311, 884, 382, 309, 1709, 7731], "temperature": 0.0, "avg_logprob": -0.17280082349424009, "compression_ratio": 1.7666666666666666, "no_speech_prob": 3.64726765837986e-05}, {"id": 152, "seek": 60104, "start": 609.64, "end": 612.36, "text": " in the layers.", "tokens": [294, 264, 7914, 13], "temperature": 0.0, "avg_logprob": -0.17280082349424009, "compression_ratio": 1.7666666666666666, "no_speech_prob": 3.64726765837986e-05}, {"id": 153, "seek": 60104, "start": 612.36, "end": 615.1999999999999, "text": " Maybe your neural model is the sequence of these vector representations.", "tokens": [2704, 428, 18161, 2316, 307, 264, 8310, 295, 613, 8062, 33358, 13], "temperature": 0.0, "avg_logprob": -0.17280082349424009, "compression_ratio": 1.7666666666666666, "no_speech_prob": 3.64726765837986e-05}, {"id": 154, "seek": 60104, "start": 615.1999999999999, "end": 622.04, "text": " A third option of sort of specificity is to look as much detail as you can.", "tokens": [316, 2636, 3614, 295, 1333, 295, 2685, 507, 307, 281, 574, 382, 709, 2607, 382, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.17280082349424009, "compression_ratio": 1.7666666666666666, "no_speech_prob": 3.64726765837986e-05}, {"id": 155, "seek": 60104, "start": 622.04, "end": 623.36, "text": " You've got these parameters in there.", "tokens": [509, 600, 658, 613, 9834, 294, 456, 13], "temperature": 0.0, "avg_logprob": -0.17280082349424009, "compression_ratio": 1.7666666666666666, "no_speech_prob": 3.64726765837986e-05}, {"id": 156, "seek": 60104, "start": 623.36, "end": 626.36, "text": " You've got the connections in the computation graph.", "tokens": [509, 600, 658, 264, 9271, 294, 264, 24903, 4295, 13], "temperature": 0.0, "avg_logprob": -0.17280082349424009, "compression_ratio": 1.7666666666666666, "no_speech_prob": 3.64726765837986e-05}, {"id": 157, "seek": 62636, "start": 626.36, "end": 630.88, "text": " Now you're sort of trying to remove all of the abstraction that you can and look at as", "tokens": [823, 291, 434, 1333, 295, 1382, 281, 4159, 439, 295, 264, 37765, 300, 291, 393, 293, 574, 412, 382], "temperature": 0.0, "avg_logprob": -0.21987170799105776, "compression_ratio": 1.7063829787234042, "no_speech_prob": 2.668567503860686e-05}, {"id": 158, "seek": 62636, "start": 630.88, "end": 632.4, "text": " many details as possible.", "tokens": [867, 4365, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.21987170799105776, "compression_ratio": 1.7063829787234042, "no_speech_prob": 2.668567503860686e-05}, {"id": 159, "seek": 62636, "start": 632.4, "end": 636.6800000000001, "text": " All three of these ways of looking at your model and performing analysis are going to", "tokens": [1057, 1045, 295, 613, 2098, 295, 1237, 412, 428, 2316, 293, 10205, 5215, 366, 516, 281], "temperature": 0.0, "avg_logprob": -0.21987170799105776, "compression_ratio": 1.7063829787234042, "no_speech_prob": 2.668567503860686e-05}, {"id": 160, "seek": 62636, "start": 636.6800000000001, "end": 642.8000000000001, "text": " be useful and will actually sort of travel slowly from one to two to three as we go through", "tokens": [312, 4420, 293, 486, 767, 1333, 295, 3147, 5692, 490, 472, 281, 732, 281, 1045, 382, 321, 352, 807], "temperature": 0.0, "avg_logprob": -0.21987170799105776, "compression_ratio": 1.7063829787234042, "no_speech_prob": 2.668567503860686e-05}, {"id": 161, "seek": 62636, "start": 642.8000000000001, "end": 645.16, "text": " this lecture.", "tokens": [341, 7991, 13], "temperature": 0.0, "avg_logprob": -0.21987170799105776, "compression_ratio": 1.7063829787234042, "no_speech_prob": 2.668567503860686e-05}, {"id": 162, "seek": 62636, "start": 645.16, "end": 647.28, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21987170799105776, "compression_ratio": 1.7063829787234042, "no_speech_prob": 2.668567503860686e-05}, {"id": 163, "seek": 62636, "start": 647.28, "end": 651.32, "text": " We haven't actually talked about any analyses yet.", "tokens": [492, 2378, 380, 767, 2825, 466, 604, 37560, 1939, 13], "temperature": 0.0, "avg_logprob": -0.21987170799105776, "compression_ratio": 1.7063829787234042, "no_speech_prob": 2.668567503860686e-05}, {"id": 164, "seek": 62636, "start": 651.32, "end": 656.04, "text": " We're going to get started on that now.", "tokens": [492, 434, 516, 281, 483, 1409, 322, 300, 586, 13], "temperature": 0.0, "avg_logprob": -0.21987170799105776, "compression_ratio": 1.7063829787234042, "no_speech_prob": 2.668567503860686e-05}, {"id": 165, "seek": 65604, "start": 656.04, "end": 659.5999999999999, "text": " We're starting with the sort of testing our model's behaviors.", "tokens": [492, 434, 2891, 365, 264, 1333, 295, 4997, 527, 2316, 311, 15501, 13], "temperature": 0.0, "avg_logprob": -0.21054428164698497, "compression_ratio": 1.6679104477611941, "no_speech_prob": 5.3898122132522985e-05}, {"id": 166, "seek": 65604, "start": 659.5999999999999, "end": 662.8, "text": " So would we want to see, well, my model perform well.", "tokens": [407, 576, 321, 528, 281, 536, 11, 731, 11, 452, 2316, 2042, 731, 13], "temperature": 0.0, "avg_logprob": -0.21054428164698497, "compression_ratio": 1.6679104477611941, "no_speech_prob": 5.3898122132522985e-05}, {"id": 167, "seek": 65604, "start": 662.8, "end": 670.0799999999999, "text": " I mean, the natural thing to ask is, how does it behave on some sort of test set?", "tokens": [286, 914, 11, 264, 3303, 551, 281, 1029, 307, 11, 577, 775, 309, 15158, 322, 512, 1333, 295, 1500, 992, 30], "temperature": 0.0, "avg_logprob": -0.21054428164698497, "compression_ratio": 1.6679104477611941, "no_speech_prob": 5.3898122132522985e-05}, {"id": 168, "seek": 65604, "start": 670.0799999999999, "end": 673.28, "text": " And so we don't really care about mechanisms yet.", "tokens": [400, 370, 321, 500, 380, 534, 1127, 466, 15902, 1939, 13], "temperature": 0.0, "avg_logprob": -0.21054428164698497, "compression_ratio": 1.6679104477611941, "no_speech_prob": 5.3898122132522985e-05}, {"id": 169, "seek": 65604, "start": 673.28, "end": 674.92, "text": " Why is it performing this?", "tokens": [1545, 307, 309, 10205, 341, 30], "temperature": 0.0, "avg_logprob": -0.21054428164698497, "compression_ratio": 1.6679104477611941, "no_speech_prob": 5.3898122132522985e-05}, {"id": 170, "seek": 65604, "start": 674.92, "end": 677.48, "text": " By what method is it making its decision?", "tokens": [3146, 437, 3170, 307, 309, 1455, 1080, 3537, 30], "temperature": 0.0, "avg_logprob": -0.21054428164698497, "compression_ratio": 1.6679104477611941, "no_speech_prob": 5.3898122132522985e-05}, {"id": 171, "seek": 65604, "start": 677.48, "end": 682.52, "text": " Instead, we're just interested in sort of the more higher level abstraction of like, does", "tokens": [7156, 11, 321, 434, 445, 3102, 294, 1333, 295, 264, 544, 2946, 1496, 37765, 295, 411, 11, 775], "temperature": 0.0, "avg_logprob": -0.21054428164698497, "compression_ratio": 1.6679104477611941, "no_speech_prob": 5.3898122132522985e-05}, {"id": 172, "seek": 65604, "start": 682.52, "end": 684.8399999999999, "text": " it perform the way I wanted to perform?", "tokens": [309, 2042, 264, 636, 286, 1415, 281, 2042, 30], "temperature": 0.0, "avg_logprob": -0.21054428164698497, "compression_ratio": 1.6679104477611941, "no_speech_prob": 5.3898122132522985e-05}, {"id": 173, "seek": 68484, "start": 684.84, "end": 691.36, "text": " So let's like, take our model evaluation that we are already doing and sort of recast", "tokens": [407, 718, 311, 411, 11, 747, 527, 2316, 13344, 300, 321, 366, 1217, 884, 293, 1333, 295, 850, 525], "temperature": 0.0, "avg_logprob": -0.1754991469844695, "compression_ratio": 1.7984790874524714, "no_speech_prob": 2.7529125873115845e-05}, {"id": 174, "seek": 68484, "start": 691.36, "end": 693.76, "text": " it in the framework of analysis.", "tokens": [309, 294, 264, 8388, 295, 5215, 13], "temperature": 0.0, "avg_logprob": -0.1754991469844695, "compression_ratio": 1.7984790874524714, "no_speech_prob": 2.7529125873115845e-05}, {"id": 175, "seek": 68484, "start": 693.76, "end": 697.4, "text": " So you've trained your model on some samples from some distribution.", "tokens": [407, 291, 600, 8895, 428, 2316, 322, 512, 10938, 490, 512, 7316, 13], "temperature": 0.0, "avg_logprob": -0.1754991469844695, "compression_ratio": 1.7984790874524714, "no_speech_prob": 2.7529125873115845e-05}, {"id": 176, "seek": 68484, "start": 697.4, "end": 700.5600000000001, "text": " So you've got input, output pairs of some kind.", "tokens": [407, 291, 600, 658, 4846, 11, 5598, 15494, 295, 512, 733, 13], "temperature": 0.0, "avg_logprob": -0.1754991469844695, "compression_ratio": 1.7984790874524714, "no_speech_prob": 2.7529125873115845e-05}, {"id": 177, "seek": 68484, "start": 700.5600000000001, "end": 703.5600000000001, "text": " So how does the model behave on samples from the same distribution?", "tokens": [407, 577, 775, 264, 2316, 15158, 322, 10938, 490, 264, 912, 7316, 30], "temperature": 0.0, "avg_logprob": -0.1754991469844695, "compression_ratio": 1.7984790874524714, "no_speech_prob": 2.7529125873115845e-05}, {"id": 178, "seek": 68484, "start": 703.5600000000001, "end": 708.76, "text": " It's a simple question and it's sort of, you know, it's known as, you know, in domain", "tokens": [467, 311, 257, 2199, 1168, 293, 309, 311, 1333, 295, 11, 291, 458, 11, 309, 311, 2570, 382, 11, 291, 458, 11, 294, 9274], "temperature": 0.0, "avg_logprob": -0.1754991469844695, "compression_ratio": 1.7984790874524714, "no_speech_prob": 2.7529125873115845e-05}, {"id": 179, "seek": 68484, "start": 708.76, "end": 713.88, "text": " accuracy or you can say that the samples are IID and that's what you're testing on.", "tokens": [14170, 420, 291, 393, 584, 300, 264, 10938, 366, 286, 2777, 293, 300, 311, 437, 291, 434, 4997, 322, 13], "temperature": 0.0, "avg_logprob": -0.1754991469844695, "compression_ratio": 1.7984790874524714, "no_speech_prob": 2.7529125873115845e-05}, {"id": 180, "seek": 71388, "start": 713.88, "end": 716.36, "text": " And this is just what we've been doing this whole time.", "tokens": [400, 341, 307, 445, 437, 321, 600, 668, 884, 341, 1379, 565, 13], "temperature": 0.0, "avg_logprob": -0.13908723574965748, "compression_ratio": 1.8, "no_speech_prob": 1.9828650692943484e-05}, {"id": 181, "seek": 71388, "start": 716.36, "end": 720.2, "text": " It's your test set accuracy or F1 or blue score.", "tokens": [467, 311, 428, 1500, 992, 14170, 420, 479, 16, 420, 3344, 6175, 13], "temperature": 0.0, "avg_logprob": -0.13908723574965748, "compression_ratio": 1.8, "no_speech_prob": 1.9828650692943484e-05}, {"id": 182, "seek": 71388, "start": 720.2, "end": 726.64, "text": " And you know, so you've got some model with some accuracy and maybe it's better than some", "tokens": [400, 291, 458, 11, 370, 291, 600, 658, 512, 2316, 365, 512, 14170, 293, 1310, 309, 311, 1101, 813, 512], "temperature": 0.0, "avg_logprob": -0.13908723574965748, "compression_ratio": 1.8, "no_speech_prob": 1.9828650692943484e-05}, {"id": 183, "seek": 71388, "start": 726.64, "end": 729.12, "text": " model with some other accuracy on this test set, right?", "tokens": [2316, 365, 512, 661, 14170, 322, 341, 1500, 992, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13908723574965748, "compression_ratio": 1.8, "no_speech_prob": 1.9828650692943484e-05}, {"id": 184, "seek": 71388, "start": 729.12, "end": 734.56, "text": " So this is what you're doing as you're iterating on your models and your final project as well.", "tokens": [407, 341, 307, 437, 291, 434, 884, 382, 291, 434, 17138, 990, 322, 428, 5245, 293, 428, 2572, 1716, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13908723574965748, "compression_ratio": 1.8, "no_speech_prob": 1.9828650692943484e-05}, {"id": 185, "seek": 71388, "start": 734.56, "end": 738.12, "text": " You say, well, you know, on my test set, which is what I've decided to care about for", "tokens": [509, 584, 11, 731, 11, 291, 458, 11, 322, 452, 1500, 992, 11, 597, 307, 437, 286, 600, 3047, 281, 1127, 466, 337], "temperature": 0.0, "avg_logprob": -0.13908723574965748, "compression_ratio": 1.8, "no_speech_prob": 1.9828650692943484e-05}, {"id": 186, "seek": 71388, "start": 738.12, "end": 739.8, "text": " now, model A does better.", "tokens": [586, 11, 2316, 316, 775, 1101, 13], "temperature": 0.0, "avg_logprob": -0.13908723574965748, "compression_ratio": 1.8, "no_speech_prob": 1.9828650692943484e-05}, {"id": 187, "seek": 71388, "start": 739.8, "end": 742.16, "text": " They both seem pretty good.", "tokens": [814, 1293, 1643, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.13908723574965748, "compression_ratio": 1.8, "no_speech_prob": 1.9828650692943484e-05}, {"id": 188, "seek": 74216, "start": 742.16, "end": 744.92, "text": " And so maybe I'll choose model A to keep working on.", "tokens": [400, 370, 1310, 286, 603, 2826, 2316, 316, 281, 1066, 1364, 322, 13], "temperature": 0.0, "avg_logprob": -0.15466675802926036, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.961733561183792e-06}, {"id": 189, "seek": 74216, "start": 744.92, "end": 748.7199999999999, "text": " Maybe I'll choose it if you were putting something into production.", "tokens": [2704, 286, 603, 2826, 309, 498, 291, 645, 3372, 746, 666, 4265, 13], "temperature": 0.0, "avg_logprob": -0.15466675802926036, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.961733561183792e-06}, {"id": 190, "seek": 74216, "start": 748.7199999999999, "end": 753.9599999999999, "text": " But remember back to, you know, this idea that it's just one number to summarize a very", "tokens": [583, 1604, 646, 281, 11, 291, 458, 11, 341, 1558, 300, 309, 311, 445, 472, 1230, 281, 20858, 257, 588], "temperature": 0.0, "avg_logprob": -0.15466675802926036, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.961733561183792e-06}, {"id": 191, "seek": 74216, "start": 753.9599999999999, "end": 756.28, "text": " complex system.", "tokens": [3997, 1185, 13], "temperature": 0.0, "avg_logprob": -0.15466675802926036, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.961733561183792e-06}, {"id": 192, "seek": 74216, "start": 756.28, "end": 760.0799999999999, "text": " It's not going to be sufficient to tell you how it's going to perform in a wide variety", "tokens": [467, 311, 406, 516, 281, 312, 11563, 281, 980, 291, 577, 309, 311, 516, 281, 2042, 294, 257, 4874, 5673], "temperature": 0.0, "avg_logprob": -0.15466675802926036, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.961733561183792e-06}, {"id": 193, "seek": 74216, "start": 760.0799999999999, "end": 761.4, "text": " of settings.", "tokens": [295, 6257, 13], "temperature": 0.0, "avg_logprob": -0.15466675802926036, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.961733561183792e-06}, {"id": 194, "seek": 74216, "start": 761.4, "end": 764.16, "text": " Okay, so we've been doing this.", "tokens": [1033, 11, 370, 321, 600, 668, 884, 341, 13], "temperature": 0.0, "avg_logprob": -0.15466675802926036, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.961733561183792e-06}, {"id": 195, "seek": 74216, "start": 764.16, "end": 768.52, "text": " This is model evaluation as model analysis.", "tokens": [639, 307, 2316, 13344, 382, 2316, 5215, 13], "temperature": 0.0, "avg_logprob": -0.15466675802926036, "compression_ratio": 1.6104417670682731, "no_speech_prob": 6.961733561183792e-06}, {"id": 196, "seek": 76852, "start": 768.52, "end": 775.0799999999999, "text": " Now we're going to say what if we are not testing on exactly the same type of data that we", "tokens": [823, 321, 434, 516, 281, 584, 437, 498, 321, 366, 406, 4997, 322, 2293, 264, 912, 2010, 295, 1412, 300, 321], "temperature": 0.0, "avg_logprob": -0.15043679150668057, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00010069822747027501}, {"id": 197, "seek": 76852, "start": 775.0799999999999, "end": 776.4, "text": " trained on.", "tokens": [8895, 322, 13], "temperature": 0.0, "avg_logprob": -0.15043679150668057, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00010069822747027501}, {"id": 198, "seek": 76852, "start": 776.4, "end": 781.12, "text": " So now we're asking, did the model learn something such that it's able to sort of extrapolate", "tokens": [407, 586, 321, 434, 3365, 11, 630, 264, 2316, 1466, 746, 1270, 300, 309, 311, 1075, 281, 1333, 295, 48224, 473], "temperature": 0.0, "avg_logprob": -0.15043679150668057, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00010069822747027501}, {"id": 199, "seek": 76852, "start": 781.12, "end": 785.3199999999999, "text": " or perform how I want it to on data that looks a little bit different from what it was", "tokens": [420, 2042, 577, 286, 528, 309, 281, 322, 1412, 300, 1542, 257, 707, 857, 819, 490, 437, 309, 390], "temperature": 0.0, "avg_logprob": -0.15043679150668057, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00010069822747027501}, {"id": 200, "seek": 76852, "start": 785.3199999999999, "end": 786.3199999999999, "text": " trained on?", "tokens": [8895, 322, 30], "temperature": 0.0, "avg_logprob": -0.15043679150668057, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00010069822747027501}, {"id": 201, "seek": 76852, "start": 786.3199999999999, "end": 789.0, "text": " And we're going to take the example of natural language inference.", "tokens": [400, 321, 434, 516, 281, 747, 264, 1365, 295, 3303, 2856, 38253, 13], "temperature": 0.0, "avg_logprob": -0.15043679150668057, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00010069822747027501}, {"id": 202, "seek": 76852, "start": 789.0, "end": 792.84, "text": " So to recall the task of natural language inference, and this is through the multi-analye", "tokens": [407, 281, 9901, 264, 5633, 295, 3303, 2856, 38253, 11, 293, 341, 307, 807, 264, 4825, 12, 29702, 1200], "temperature": 0.0, "avg_logprob": -0.15043679150668057, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00010069822747027501}, {"id": 203, "seek": 76852, "start": 792.84, "end": 797.0, "text": " data set that we're just pulling our definition, you have a premise.", "tokens": [1412, 992, 300, 321, 434, 445, 8407, 527, 7123, 11, 291, 362, 257, 22045, 13], "temperature": 0.0, "avg_logprob": -0.15043679150668057, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.00010069822747027501}, {"id": 204, "seek": 79700, "start": 797.0, "end": 801.8, "text": " He turned and saw John sleeping in his half tent, and you have a hypothesis.", "tokens": [634, 3574, 293, 1866, 2619, 8296, 294, 702, 1922, 7054, 11, 293, 291, 362, 257, 17291, 13], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 205, "seek": 79700, "start": 801.8, "end": 804.08, "text": " He saw John was asleep.", "tokens": [634, 1866, 2619, 390, 11039, 13], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 206, "seek": 79700, "start": 804.08, "end": 806.08, "text": " And then you give them both two of model.", "tokens": [400, 550, 291, 976, 552, 1293, 732, 295, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 207, "seek": 79700, "start": 806.08, "end": 809.96, "text": " And this is the model that we had before that gets some good accuracy.", "tokens": [400, 341, 307, 264, 2316, 300, 321, 632, 949, 300, 2170, 512, 665, 14170, 13], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 208, "seek": 79700, "start": 809.96, "end": 815.84, "text": " And the model is supposed to tell whether the hypothesis is sort of implied by the premise", "tokens": [400, 264, 2316, 307, 3442, 281, 980, 1968, 264, 17291, 307, 1333, 295, 32614, 538, 264, 22045], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 209, "seek": 79700, "start": 815.84, "end": 817.88, "text": " or contradicting.", "tokens": [420, 15858, 21490, 13], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 210, "seek": 79700, "start": 817.88, "end": 819.24, "text": " So you could be contradicting.", "tokens": [407, 291, 727, 312, 15858, 21490, 13], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 211, "seek": 79700, "start": 819.24, "end": 822.76, "text": " Maybe if the hypothesis is, you know, John was awake.", "tokens": [2704, 498, 264, 17291, 307, 11, 291, 458, 11, 2619, 390, 15994, 13], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 212, "seek": 79700, "start": 822.76, "end": 824.52, "text": " For example, or he saw John was awake.", "tokens": [1171, 1365, 11, 420, 415, 1866, 2619, 390, 15994, 13], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 213, "seek": 79700, "start": 824.52, "end": 826.2, "text": " Maybe that would be contradiction.", "tokens": [2704, 300, 576, 312, 34937, 13], "temperature": 0.0, "avg_logprob": -0.20618114321250616, "compression_ratio": 1.8862745098039215, "no_speech_prob": 2.7959222279605456e-05}, {"id": 214, "seek": 82620, "start": 826.2, "end": 830.36, "text": " Or if sort of both could be true at the same time, so to speak.", "tokens": [1610, 498, 1333, 295, 1293, 727, 312, 2074, 412, 264, 912, 565, 11, 370, 281, 1710, 13], "temperature": 0.0, "avg_logprob": -0.217302605509758, "compression_ratio": 1.7803030303030303, "no_speech_prob": 2.796569788188208e-05}, {"id": 215, "seek": 82620, "start": 830.36, "end": 834.2800000000001, "text": " And then in this case, you know, it seems like they're saying that the premise implies", "tokens": [400, 550, 294, 341, 1389, 11, 291, 458, 11, 309, 2544, 411, 436, 434, 1566, 300, 264, 22045, 18779], "temperature": 0.0, "avg_logprob": -0.217302605509758, "compression_ratio": 1.7803030303030303, "no_speech_prob": 2.796569788188208e-05}, {"id": 216, "seek": 82620, "start": 834.2800000000001, "end": 836.0, "text": " the hypothesis.", "tokens": [264, 17291, 13], "temperature": 0.0, "avg_logprob": -0.217302605509758, "compression_ratio": 1.7803030303030303, "no_speech_prob": 2.796569788188208e-05}, {"id": 217, "seek": 82620, "start": 836.0, "end": 840.0400000000001, "text": " And so, you know, you would say probably this is likely to get the right answer since", "tokens": [400, 370, 11, 291, 458, 11, 291, 576, 584, 1391, 341, 307, 3700, 281, 483, 264, 558, 1867, 1670], "temperature": 0.0, "avg_logprob": -0.217302605509758, "compression_ratio": 1.7803030303030303, "no_speech_prob": 2.796569788188208e-05}, {"id": 218, "seek": 82620, "start": 840.0400000000001, "end": 841.84, "text": " the accuracy of the model is 95%.", "tokens": [264, 14170, 295, 264, 2316, 307, 13420, 6856], "temperature": 0.0, "avg_logprob": -0.217302605509758, "compression_ratio": 1.7803030303030303, "no_speech_prob": 2.796569788188208e-05}, {"id": 219, "seek": 82620, "start": 841.84, "end": 846.6, "text": " And if I percent of the time, we get the right answer.", "tokens": [400, 498, 286, 3043, 295, 264, 565, 11, 321, 483, 264, 558, 1867, 13], "temperature": 0.0, "avg_logprob": -0.217302605509758, "compression_ratio": 1.7803030303030303, "no_speech_prob": 2.796569788188208e-05}, {"id": 220, "seek": 82620, "start": 846.6, "end": 849.8000000000001, "text": " And we're going to dig deeper into that.", "tokens": [400, 321, 434, 516, 281, 2528, 7731, 666, 300, 13], "temperature": 0.0, "avg_logprob": -0.217302605509758, "compression_ratio": 1.7803030303030303, "no_speech_prob": 2.796569788188208e-05}, {"id": 221, "seek": 82620, "start": 849.8000000000001, "end": 855.0, "text": " What if the model is not doing what we think we want it to be doing in order to perform", "tokens": [708, 498, 264, 2316, 307, 406, 884, 437, 321, 519, 321, 528, 309, 281, 312, 884, 294, 1668, 281, 2042], "temperature": 0.0, "avg_logprob": -0.217302605509758, "compression_ratio": 1.7803030303030303, "no_speech_prob": 2.796569788188208e-05}, {"id": 222, "seek": 85500, "start": 855.0, "end": 856.8, "text": " natural language inference?", "tokens": [3303, 2856, 38253, 30], "temperature": 0.0, "avg_logprob": -0.1882411879722518, "compression_ratio": 1.7037037037037037, "no_speech_prob": 4.682921644416638e-05}, {"id": 223, "seek": 85500, "start": 856.8, "end": 862.08, "text": " So in a data set like multi-nLI, the authors who gathered the data set will have asked", "tokens": [407, 294, 257, 1412, 992, 411, 4825, 12, 77, 48718, 11, 264, 16552, 567, 13032, 264, 1412, 992, 486, 362, 2351], "temperature": 0.0, "avg_logprob": -0.1882411879722518, "compression_ratio": 1.7037037037037037, "no_speech_prob": 4.682921644416638e-05}, {"id": 224, "seek": 85500, "start": 862.08, "end": 867.56, "text": " humans to perform the task and, you know, gotten the accuracy that the humans achieved.", "tokens": [6255, 281, 2042, 264, 5633, 293, 11, 291, 458, 11, 5768, 264, 14170, 300, 264, 6255, 11042, 13], "temperature": 0.0, "avg_logprob": -0.1882411879722518, "compression_ratio": 1.7037037037037037, "no_speech_prob": 4.682921644416638e-05}, {"id": 225, "seek": 85500, "start": 867.56, "end": 874.04, "text": " And models nowadays are achieving accuracies that are around where humans are achieving,", "tokens": [400, 5245, 13434, 366, 19626, 5771, 20330, 300, 366, 926, 689, 6255, 366, 19626, 11], "temperature": 0.0, "avg_logprob": -0.1882411879722518, "compression_ratio": 1.7037037037037037, "no_speech_prob": 4.682921644416638e-05}, {"id": 226, "seek": 85500, "start": 874.04, "end": 876.76, "text": " which sounds great at first.", "tokens": [597, 3263, 869, 412, 700, 13], "temperature": 0.0, "avg_logprob": -0.1882411879722518, "compression_ratio": 1.7037037037037037, "no_speech_prob": 4.682921644416638e-05}, {"id": 227, "seek": 85500, "start": 876.76, "end": 883.4, "text": " But as we'll see, it's not the same as actually performing the task more broadly in the right", "tokens": [583, 382, 321, 603, 536, 11, 309, 311, 406, 264, 912, 382, 767, 10205, 264, 5633, 544, 19511, 294, 264, 558], "temperature": 0.0, "avg_logprob": -0.1882411879722518, "compression_ratio": 1.7037037037037037, "no_speech_prob": 4.682921644416638e-05}, {"id": 228, "seek": 88340, "start": 883.4, "end": 885.4, "text": " way.", "tokens": [636, 13], "temperature": 0.0, "avg_logprob": -0.13834523682547087, "compression_ratio": 1.6015325670498084, "no_speech_prob": 2.5067205569939688e-05}, {"id": 229, "seek": 88340, "start": 885.4, "end": 889.3199999999999, "text": " So what if the model is not doing something smart effectively?", "tokens": [407, 437, 498, 264, 2316, 307, 406, 884, 746, 4069, 8659, 30], "temperature": 0.0, "avg_logprob": -0.13834523682547087, "compression_ratio": 1.6015325670498084, "no_speech_prob": 2.5067205569939688e-05}, {"id": 230, "seek": 88340, "start": 889.3199999999999, "end": 894.92, "text": " We're going to use a diagnostic test set of carefully constructed examples that seem", "tokens": [492, 434, 516, 281, 764, 257, 27897, 1500, 992, 295, 7500, 17083, 5110, 300, 1643], "temperature": 0.0, "avg_logprob": -0.13834523682547087, "compression_ratio": 1.6015325670498084, "no_speech_prob": 2.5067205569939688e-05}, {"id": 231, "seek": 88340, "start": 894.92, "end": 901.4, "text": " like things the model should be able to do to test for a specific skill or capacity.", "tokens": [411, 721, 264, 2316, 820, 312, 1075, 281, 360, 281, 1500, 337, 257, 2685, 5389, 420, 6042, 13], "temperature": 0.0, "avg_logprob": -0.13834523682547087, "compression_ratio": 1.6015325670498084, "no_speech_prob": 2.5067205569939688e-05}, {"id": 232, "seek": 88340, "start": 901.4, "end": 903.1999999999999, "text": " In this case, we'll use Hans.", "tokens": [682, 341, 1389, 11, 321, 603, 764, 17926, 13], "temperature": 0.0, "avg_logprob": -0.13834523682547087, "compression_ratio": 1.6015325670498084, "no_speech_prob": 2.5067205569939688e-05}, {"id": 233, "seek": 88340, "start": 903.1999999999999, "end": 907.64, "text": " So Hans is the heuristic analysis for analyzed systems data set.", "tokens": [407, 17926, 307, 264, 415, 374, 3142, 5215, 337, 28181, 3652, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.13834523682547087, "compression_ratio": 1.6015325670498084, "no_speech_prob": 2.5067205569939688e-05}, {"id": 234, "seek": 88340, "start": 907.64, "end": 912.84, "text": " And it's intended to take systems that do natural language inference and test whether", "tokens": [400, 309, 311, 10226, 281, 747, 3652, 300, 360, 3303, 2856, 38253, 293, 1500, 1968], "temperature": 0.0, "avg_logprob": -0.13834523682547087, "compression_ratio": 1.6015325670498084, "no_speech_prob": 2.5067205569939688e-05}, {"id": 235, "seek": 91284, "start": 912.84, "end": 916.1600000000001, "text": " they're using some simple syntactic heuristics.", "tokens": [436, 434, 1228, 512, 2199, 23980, 19892, 415, 374, 6006, 13], "temperature": 0.0, "avg_logprob": -0.1676796479658647, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.5862456823233515e-05}, {"id": 236, "seek": 91284, "start": 916.1600000000001, "end": 919.1600000000001, "text": " What we'll have in each of these cases, we'll have some heuristic.", "tokens": [708, 321, 603, 362, 294, 1184, 295, 613, 3331, 11, 321, 603, 362, 512, 415, 374, 3142, 13], "temperature": 0.0, "avg_logprob": -0.1676796479658647, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.5862456823233515e-05}, {"id": 237, "seek": 91284, "start": 919.1600000000001, "end": 921.2, "text": " We'll talk through the definition.", "tokens": [492, 603, 751, 807, 264, 7123, 13], "temperature": 0.0, "avg_logprob": -0.1676796479658647, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.5862456823233515e-05}, {"id": 238, "seek": 91284, "start": 921.2, "end": 922.2, "text": " We'll get an example.", "tokens": [492, 603, 483, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1676796479658647, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.5862456823233515e-05}, {"id": 239, "seek": 91284, "start": 922.2, "end": 924.4, "text": " So the first thing is lexical overlap.", "tokens": [407, 264, 700, 551, 307, 476, 87, 804, 19959, 13], "temperature": 0.0, "avg_logprob": -0.1676796479658647, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.5862456823233515e-05}, {"id": 240, "seek": 91284, "start": 924.4, "end": 931.0, "text": " So the model might do this thing where it assumes that a premise entails all hypotheses", "tokens": [407, 264, 2316, 1062, 360, 341, 551, 689, 309, 37808, 300, 257, 22045, 50133, 439, 49969], "temperature": 0.0, "avg_logprob": -0.1676796479658647, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.5862456823233515e-05}, {"id": 241, "seek": 91284, "start": 931.0, "end": 932.84, "text": " constructed from words in the premise.", "tokens": [17083, 490, 2283, 294, 264, 22045, 13], "temperature": 0.0, "avg_logprob": -0.1676796479658647, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.5862456823233515e-05}, {"id": 242, "seek": 91284, "start": 932.84, "end": 940.88, "text": " So in this example, you have the premise the doctor was paid by the actor.", "tokens": [407, 294, 341, 1365, 11, 291, 362, 264, 22045, 264, 4631, 390, 4835, 538, 264, 8747, 13], "temperature": 0.0, "avg_logprob": -0.1676796479658647, "compression_ratio": 1.7457627118644068, "no_speech_prob": 2.5862456823233515e-05}, {"id": 243, "seek": 94088, "start": 940.88, "end": 943.76, "text": " And then the hypothesis is the doctor paid the actor.", "tokens": [400, 550, 264, 17291, 307, 264, 4631, 4835, 264, 8747, 13], "temperature": 0.0, "avg_logprob": -0.19543863024030414, "compression_ratio": 1.9375, "no_speech_prob": 1.7775466403691098e-05}, {"id": 244, "seek": 94088, "start": 943.76, "end": 949.88, "text": " And you'll notice that in bold here, get the doctor, and then paid, and then the actor.", "tokens": [400, 291, 603, 3449, 300, 294, 11928, 510, 11, 483, 264, 4631, 11, 293, 550, 4835, 11, 293, 550, 264, 8747, 13], "temperature": 0.0, "avg_logprob": -0.19543863024030414, "compression_ratio": 1.9375, "no_speech_prob": 1.7775466403691098e-05}, {"id": 245, "seek": 94088, "start": 949.88, "end": 954.8, "text": " And so if you use this heuristic, you will think that the doctor was paid by the actor,", "tokens": [400, 370, 498, 291, 764, 341, 415, 374, 3142, 11, 291, 486, 519, 300, 264, 4631, 390, 4835, 538, 264, 8747, 11], "temperature": 0.0, "avg_logprob": -0.19543863024030414, "compression_ratio": 1.9375, "no_speech_prob": 1.7775466403691098e-05}, {"id": 246, "seek": 94088, "start": 954.8, "end": 958.72, "text": " implies the doctor paid the actor that does not imply it, of course.", "tokens": [18779, 264, 4631, 4835, 264, 8747, 300, 775, 406, 33616, 309, 11, 295, 1164, 13], "temperature": 0.0, "avg_logprob": -0.19543863024030414, "compression_ratio": 1.9375, "no_speech_prob": 1.7775466403691098e-05}, {"id": 247, "seek": 94088, "start": 958.72, "end": 962.04, "text": " And so you could expect a model you want the model to be able to do this.", "tokens": [400, 370, 291, 727, 2066, 257, 2316, 291, 528, 264, 2316, 281, 312, 1075, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.19543863024030414, "compression_ratio": 1.9375, "no_speech_prob": 1.7775466403691098e-05}, {"id": 248, "seek": 94088, "start": 962.04, "end": 963.76, "text": " It's somewhat simple.", "tokens": [467, 311, 8344, 2199, 13], "temperature": 0.0, "avg_logprob": -0.19543863024030414, "compression_ratio": 1.9375, "no_speech_prob": 1.7775466403691098e-05}, {"id": 249, "seek": 94088, "start": 963.76, "end": 968.16, "text": " But if it's using this heuristic, it won't get this example right.", "tokens": [583, 498, 309, 311, 1228, 341, 415, 374, 3142, 11, 309, 1582, 380, 483, 341, 1365, 558, 13], "temperature": 0.0, "avg_logprob": -0.19543863024030414, "compression_ratio": 1.9375, "no_speech_prob": 1.7775466403691098e-05}, {"id": 250, "seek": 94088, "start": 968.16, "end": 970.68, "text": " Next is a sub-sequence heuristics.", "tokens": [3087, 307, 257, 1422, 12, 11834, 655, 415, 374, 6006, 13], "temperature": 0.0, "avg_logprob": -0.19543863024030414, "compression_ratio": 1.9375, "no_speech_prob": 1.7775466403691098e-05}, {"id": 251, "seek": 97068, "start": 970.68, "end": 977.68, "text": " So here, if the model assumes that the premise entails all of its contiguous sub-sequences,", "tokens": [407, 510, 11, 498, 264, 2316, 37808, 300, 264, 22045, 50133, 439, 295, 1080, 660, 30525, 1422, 12, 11834, 2667, 11], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 252, "seek": 97068, "start": 977.68, "end": 979.12, "text": " it will get this one wrong as well.", "tokens": [309, 486, 483, 341, 472, 2085, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 253, "seek": 97068, "start": 979.12, "end": 983.4, "text": " So this example is the doctor near the actor danced.", "tokens": [407, 341, 1365, 307, 264, 4631, 2651, 264, 8747, 32909, 13], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 254, "seek": 97068, "start": 983.4, "end": 984.4, "text": " That's the premise.", "tokens": [663, 311, 264, 22045, 13], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 255, "seek": 97068, "start": 984.4, "end": 986.7199999999999, "text": " The hypothesis is the actor danced.", "tokens": [440, 17291, 307, 264, 8747, 32909, 13], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 256, "seek": 97068, "start": 986.7199999999999, "end": 988.1999999999999, "text": " Now this is a simple syntactic thing.", "tokens": [823, 341, 307, 257, 2199, 23980, 19892, 551, 13], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 257, "seek": 97068, "start": 988.1999999999999, "end": 991.4399999999999, "text": " The doctor is doing the dancing near the actor.", "tokens": [440, 4631, 307, 884, 264, 8898, 2651, 264, 8747, 13], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 258, "seek": 97068, "start": 991.4399999999999, "end": 993.88, "text": " Is this prepositional phrase?", "tokens": [1119, 341, 2666, 329, 2628, 9535, 30], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 259, "seek": 97068, "start": 993.88, "end": 997.24, "text": " And so the model sort of uses this heuristic, oh, look, the actor danced.", "tokens": [400, 370, 264, 2316, 1333, 295, 4960, 341, 415, 374, 3142, 11, 1954, 11, 574, 11, 264, 8747, 32909, 13], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 260, "seek": 97068, "start": 997.24, "end": 998.7199999999999, "text": " That's a sub-sequence entailed.", "tokens": [663, 311, 257, 1422, 12, 11834, 655, 948, 24731, 13], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 261, "seek": 97068, "start": 998.7199999999999, "end": 999.7199999999999, "text": " Awesome.", "tokens": [10391, 13], "temperature": 0.0, "avg_logprob": -0.1787001433195891, "compression_ratio": 1.8531746031746033, "no_speech_prob": 2.5065712179639377e-05}, {"id": 262, "seek": 99972, "start": 999.72, "end": 1002.64, "text": " And it'll get this one wrong as well.", "tokens": [400, 309, 603, 483, 341, 472, 2085, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.20856924816570452, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.5462264602538198e-05}, {"id": 263, "seek": 99972, "start": 1002.64, "end": 1006.6, "text": " And here's another one that's a lot like sub-sequence.", "tokens": [400, 510, 311, 1071, 472, 300, 311, 257, 688, 411, 1422, 12, 11834, 655, 13], "temperature": 0.0, "avg_logprob": -0.20856924816570452, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.5462264602538198e-05}, {"id": 264, "seek": 99972, "start": 1006.6, "end": 1012.6800000000001, "text": " So if the premise, if the model thinks that the premise entails all complete sub-trees,", "tokens": [407, 498, 264, 22045, 11, 498, 264, 2316, 7309, 300, 264, 22045, 50133, 439, 3566, 1422, 12, 3599, 279, 11], "temperature": 0.0, "avg_logprob": -0.20856924816570452, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.5462264602538198e-05}, {"id": 265, "seek": 99972, "start": 1012.6800000000001, "end": 1015.76, "text": " so this is like sort of fully formed phrases.", "tokens": [370, 341, 307, 411, 1333, 295, 4498, 8693, 20312, 13], "temperature": 0.0, "avg_logprob": -0.20856924816570452, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.5462264602538198e-05}, {"id": 266, "seek": 99972, "start": 1015.76, "end": 1021.24, "text": " So the artist slept here is a fully formed sort of, is that sub-tree, if the artist slept,", "tokens": [407, 264, 5748, 17400, 510, 307, 257, 4498, 8693, 1333, 295, 11, 307, 300, 1422, 12, 83, 701, 11, 498, 264, 5748, 17400, 11], "temperature": 0.0, "avg_logprob": -0.20856924816570452, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.5462264602538198e-05}, {"id": 267, "seek": 99972, "start": 1021.24, "end": 1024.04, "text": " the actor ran, and then that's the premise.", "tokens": [264, 8747, 5872, 11, 293, 550, 300, 311, 264, 22045, 13], "temperature": 0.0, "avg_logprob": -0.20856924816570452, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.5462264602538198e-05}, {"id": 268, "seek": 99972, "start": 1024.04, "end": 1025.72, "text": " Does it entail the hypothesis?", "tokens": [4402, 309, 948, 864, 264, 17291, 30], "temperature": 0.0, "avg_logprob": -0.20856924816570452, "compression_ratio": 1.798165137614679, "no_speech_prob": 2.5462264602538198e-05}, {"id": 269, "seek": 102572, "start": 1025.72, "end": 1031.04, "text": " The actor slept, no, sorry, the artist slept.", "tokens": [440, 8747, 17400, 11, 572, 11, 2597, 11, 264, 5748, 17400, 13], "temperature": 0.0, "avg_logprob": -0.26861462873571057, "compression_ratio": 1.452513966480447, "no_speech_prob": 9.51481251831865e-06}, {"id": 270, "seek": 102572, "start": 1031.04, "end": 1033.56, "text": " That does not entail it because this is in that conditional.", "tokens": [663, 775, 406, 948, 864, 309, 570, 341, 307, 294, 300, 27708, 13], "temperature": 0.0, "avg_logprob": -0.26861462873571057, "compression_ratio": 1.452513966480447, "no_speech_prob": 9.51481251831865e-06}, {"id": 271, "seek": 102572, "start": 1033.56, "end": 1040.56, "text": " Okay, let me pause here for some questions before I move on to see how these models do.", "tokens": [1033, 11, 718, 385, 10465, 510, 337, 512, 1651, 949, 286, 1286, 322, 281, 536, 577, 613, 5245, 360, 13], "temperature": 0.0, "avg_logprob": -0.26861462873571057, "compression_ratio": 1.452513966480447, "no_speech_prob": 9.51481251831865e-06}, {"id": 272, "seek": 102572, "start": 1040.56, "end": 1048.28, "text": " Anyone unclear about how this sort of evaluation is being set up?", "tokens": [14643, 25636, 466, 577, 341, 1333, 295, 13344, 307, 885, 992, 493, 30], "temperature": 0.0, "avg_logprob": -0.26861462873571057, "compression_ratio": 1.452513966480447, "no_speech_prob": 9.51481251831865e-06}, {"id": 273, "seek": 104828, "start": 1048.28, "end": 1057.84, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.3316071250221946, "compression_ratio": 1.4602272727272727, "no_speech_prob": 7.841473416192457e-05}, {"id": 274, "seek": 104828, "start": 1057.84, "end": 1059.16, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3316071250221946, "compression_ratio": 1.4602272727272727, "no_speech_prob": 7.841473416192457e-05}, {"id": 275, "seek": 104828, "start": 1059.16, "end": 1062.92, "text": " Okay, so how do models perform?", "tokens": [1033, 11, 370, 577, 360, 5245, 2042, 30], "temperature": 0.0, "avg_logprob": -0.3316071250221946, "compression_ratio": 1.4602272727272727, "no_speech_prob": 7.841473416192457e-05}, {"id": 276, "seek": 104828, "start": 1062.92, "end": 1066.32, "text": " That's sort of the question of the hour.", "tokens": [663, 311, 1333, 295, 264, 1168, 295, 264, 1773, 13], "temperature": 0.0, "avg_logprob": -0.3316071250221946, "compression_ratio": 1.4602272727272727, "no_speech_prob": 7.841473416192457e-05}, {"id": 277, "seek": 104828, "start": 1066.32, "end": 1071.04, "text": " What we'll do is, we'll look at these results from the same paper that really released the", "tokens": [708, 321, 603, 360, 307, 11, 321, 603, 574, 412, 613, 3542, 490, 264, 912, 3035, 300, 534, 4736, 264], "temperature": 0.0, "avg_logprob": -0.3316071250221946, "compression_ratio": 1.4602272727272727, "no_speech_prob": 7.841473416192457e-05}, {"id": 278, "seek": 104828, "start": 1071.04, "end": 1072.04, "text": " data set.", "tokens": [1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.3316071250221946, "compression_ratio": 1.4602272727272727, "no_speech_prob": 7.841473416192457e-05}, {"id": 279, "seek": 104828, "start": 1072.04, "end": 1077.76, "text": " So they took four strong multi-nl i models with the following accuracy.", "tokens": [407, 436, 1890, 1451, 2068, 4825, 12, 77, 75, 741, 5245, 365, 264, 3480, 14170, 13], "temperature": 0.0, "avg_logprob": -0.3316071250221946, "compression_ratio": 1.4602272727272727, "no_speech_prob": 7.841473416192457e-05}, {"id": 280, "seek": 107776, "start": 1077.76, "end": 1082.28, "text": " So the accuracy is here are something between 60 and 80 something 80 percent burnt over", "tokens": [407, 264, 14170, 307, 510, 366, 746, 1296, 4060, 293, 4688, 746, 4688, 3043, 18901, 670], "temperature": 0.0, "avg_logprob": -0.2191409028094748, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.372964420123026e-05}, {"id": 281, "seek": 107776, "start": 1082.28, "end": 1085.16, "text": " here is doing the best.", "tokens": [510, 307, 884, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.2191409028094748, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.372964420123026e-05}, {"id": 282, "seek": 107776, "start": 1085.16, "end": 1092.12, "text": " And in domain, in that first sort of setting that we talked about, you get these reasonable", "tokens": [400, 294, 9274, 11, 294, 300, 700, 1333, 295, 3287, 300, 321, 2825, 466, 11, 291, 483, 613, 10585], "temperature": 0.0, "avg_logprob": -0.2191409028094748, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.372964420123026e-05}, {"id": 283, "seek": 107776, "start": 1092.12, "end": 1094.2, "text": " accuracies.", "tokens": [5771, 20330, 13], "temperature": 0.0, "avg_logprob": -0.2191409028094748, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.372964420123026e-05}, {"id": 284, "seek": 107776, "start": 1094.2, "end": 1100.08, "text": " And that is sort of what we said before about it, like looking pretty good.", "tokens": [400, 300, 307, 1333, 295, 437, 321, 848, 949, 466, 309, 11, 411, 1237, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.2191409028094748, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.372964420123026e-05}, {"id": 285, "seek": 107776, "start": 1100.08, "end": 1107.68, "text": " And when we evaluate on Hans, in this setting here, we have examples where the", "tokens": [400, 562, 321, 13059, 322, 17926, 11, 294, 341, 3287, 510, 11, 321, 362, 5110, 689, 264], "temperature": 0.0, "avg_logprob": -0.2191409028094748, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.372964420123026e-05}, {"id": 286, "seek": 110768, "start": 1107.68, "end": 1110.52, "text": " heuristics we talked about actually work.", "tokens": [415, 374, 6006, 321, 2825, 466, 767, 589, 13], "temperature": 0.0, "avg_logprob": -0.22509092944008963, "compression_ratio": 1.6487603305785123, "no_speech_prob": 2.8852748073404655e-05}, {"id": 287, "seek": 110768, "start": 1110.52, "end": 1114.1200000000001, "text": " So if the model is using the heuristic, it will get this right.", "tokens": [407, 498, 264, 2316, 307, 1228, 264, 415, 374, 3142, 11, 309, 486, 483, 341, 558, 13], "temperature": 0.0, "avg_logprob": -0.22509092944008963, "compression_ratio": 1.6487603305785123, "no_speech_prob": 2.8852748073404655e-05}, {"id": 288, "seek": 110768, "start": 1114.1200000000001, "end": 1117.1200000000001, "text": " And it gets very high accuracies.", "tokens": [400, 309, 2170, 588, 1090, 5771, 20330, 13], "temperature": 0.0, "avg_logprob": -0.22509092944008963, "compression_ratio": 1.6487603305785123, "no_speech_prob": 2.8852748073404655e-05}, {"id": 289, "seek": 110768, "start": 1117.1200000000001, "end": 1122.3200000000002, "text": " And then if we evaluate the model in the settings where if it uses the heuristic, it gets the", "tokens": [400, 550, 498, 321, 13059, 264, 2316, 294, 264, 6257, 689, 498, 309, 4960, 264, 415, 374, 3142, 11, 309, 2170, 264], "temperature": 0.0, "avg_logprob": -0.22509092944008963, "compression_ratio": 1.6487603305785123, "no_speech_prob": 2.8852748073404655e-05}, {"id": 290, "seek": 110768, "start": 1122.3200000000002, "end": 1124.92, "text": " examples wrong.", "tokens": [5110, 2085, 13], "temperature": 0.0, "avg_logprob": -0.22509092944008963, "compression_ratio": 1.6487603305785123, "no_speech_prob": 2.8852748073404655e-05}, {"id": 291, "seek": 110768, "start": 1124.92, "end": 1131.52, "text": " You know, maybe birds doing like epsilon better than some of the other stuff here, but it's", "tokens": [509, 458, 11, 1310, 9009, 884, 411, 17889, 1101, 813, 512, 295, 264, 661, 1507, 510, 11, 457, 309, 311], "temperature": 0.0, "avg_logprob": -0.22509092944008963, "compression_ratio": 1.6487603305785123, "no_speech_prob": 2.8852748073404655e-05}, {"id": 292, "seek": 110768, "start": 1131.52, "end": 1133.52, "text": " a very different story.", "tokens": [257, 588, 819, 1657, 13], "temperature": 0.0, "avg_logprob": -0.22509092944008963, "compression_ratio": 1.6487603305785123, "no_speech_prob": 2.8852748073404655e-05}, {"id": 293, "seek": 110768, "start": 1133.52, "end": 1134.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.22509092944008963, "compression_ratio": 1.6487603305785123, "no_speech_prob": 2.8852748073404655e-05}, {"id": 294, "seek": 110768, "start": 1134.52, "end": 1135.6000000000001, "text": " And you saw those examples.", "tokens": [400, 291, 1866, 729, 5110, 13], "temperature": 0.0, "avg_logprob": -0.22509092944008963, "compression_ratio": 1.6487603305785123, "no_speech_prob": 2.8852748073404655e-05}, {"id": 295, "seek": 113560, "start": 1135.6, "end": 1143.56, "text": " They're not complex in our sort of own idea of complexity.", "tokens": [814, 434, 406, 3997, 294, 527, 1333, 295, 1065, 1558, 295, 14024, 13], "temperature": 0.0, "avg_logprob": -0.14036175213028898, "compression_ratio": 1.656, "no_speech_prob": 9.024889732245356e-05}, {"id": 296, "seek": 113560, "start": 1143.56, "end": 1148.48, "text": " And so this is why it sort of feels like a clear failure of the system.", "tokens": [400, 370, 341, 307, 983, 309, 1333, 295, 3417, 411, 257, 1850, 7763, 295, 264, 1185, 13], "temperature": 0.0, "avg_logprob": -0.14036175213028898, "compression_ratio": 1.656, "no_speech_prob": 9.024889732245356e-05}, {"id": 297, "seek": 113560, "start": 1148.48, "end": 1153.36, "text": " Now you can say though that well, maybe the training data sort of wasn't, didn't have", "tokens": [823, 291, 393, 584, 1673, 300, 731, 11, 1310, 264, 3097, 1412, 1333, 295, 2067, 380, 11, 994, 380, 362], "temperature": 0.0, "avg_logprob": -0.14036175213028898, "compression_ratio": 1.656, "no_speech_prob": 9.024889732245356e-05}, {"id": 298, "seek": 113560, "start": 1153.36, "end": 1154.8, "text": " any of those sort of phenomena.", "tokens": [604, 295, 729, 1333, 295, 22004, 13], "temperature": 0.0, "avg_logprob": -0.14036175213028898, "compression_ratio": 1.656, "no_speech_prob": 9.024889732245356e-05}, {"id": 299, "seek": 113560, "start": 1154.8, "end": 1158.12, "text": " So the model couldn't have learned not to do that.", "tokens": [407, 264, 2316, 2809, 380, 362, 3264, 406, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.14036175213028898, "compression_ratio": 1.656, "no_speech_prob": 9.024889732245356e-05}, {"id": 300, "seek": 113560, "start": 1158.12, "end": 1162.28, "text": " And that's sort of a reasonable argument except, well, you know, Bert is pre-trained on", "tokens": [400, 300, 311, 1333, 295, 257, 10585, 6770, 3993, 11, 731, 11, 291, 458, 11, 29594, 307, 659, 12, 17227, 2001, 322], "temperature": 0.0, "avg_logprob": -0.14036175213028898, "compression_ratio": 1.656, "no_speech_prob": 9.024889732245356e-05}, {"id": 301, "seek": 113560, "start": 1162.28, "end": 1163.6399999999999, "text": " a bunch of language texts.", "tokens": [257, 3840, 295, 2856, 15765, 13], "temperature": 0.0, "avg_logprob": -0.14036175213028898, "compression_ratio": 1.656, "no_speech_prob": 9.024889732245356e-05}, {"id": 302, "seek": 116364, "start": 1163.64, "end": 1167.8000000000002, "text": " So you might hope, you might expect, you might hope that it does better.", "tokens": [407, 291, 1062, 1454, 11, 291, 1062, 2066, 11, 291, 1062, 1454, 300, 309, 775, 1101, 13], "temperature": 0.0, "avg_logprob": -0.18555023143817853, "compression_ratio": 1.7251461988304093, "no_speech_prob": 1.4061012734600808e-05}, {"id": 303, "seek": 116364, "start": 1167.8000000000002, "end": 1169.5600000000002, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18555023143817853, "compression_ratio": 1.7251461988304093, "no_speech_prob": 1.4061012734600808e-05}, {"id": 304, "seek": 116364, "start": 1169.5600000000002, "end": 1179.3200000000002, "text": " So we saw that example of models performing well on examples that are like those that", "tokens": [407, 321, 1866, 300, 1365, 295, 5245, 10205, 731, 322, 5110, 300, 366, 411, 729, 300], "temperature": 0.0, "avg_logprob": -0.18555023143817853, "compression_ratio": 1.7251461988304093, "no_speech_prob": 1.4061012734600808e-05}, {"id": 305, "seek": 116364, "start": 1179.3200000000002, "end": 1180.48, "text": " it was trained on.", "tokens": [309, 390, 8895, 322, 13], "temperature": 0.0, "avg_logprob": -0.18555023143817853, "compression_ratio": 1.7251461988304093, "no_speech_prob": 1.4061012734600808e-05}, {"id": 306, "seek": 116364, "start": 1180.48, "end": 1186.68, "text": " And then performing not very well at all on examples that seem reasonable, but are sort", "tokens": [400, 550, 10205, 406, 588, 731, 412, 439, 322, 5110, 300, 1643, 10585, 11, 457, 366, 1333], "temperature": 0.0, "avg_logprob": -0.18555023143817853, "compression_ratio": 1.7251461988304093, "no_speech_prob": 1.4061012734600808e-05}, {"id": 307, "seek": 116364, "start": 1186.68, "end": 1189.48, "text": " of a little bit tricky.", "tokens": [295, 257, 707, 857, 12414, 13], "temperature": 0.0, "avg_logprob": -0.18555023143817853, "compression_ratio": 1.7251461988304093, "no_speech_prob": 1.4061012734600808e-05}, {"id": 308, "seek": 118948, "start": 1189.48, "end": 1193.88, "text": " Now we're going to take this idea of having a test set that we've carefully crafted and", "tokens": [823, 321, 434, 516, 281, 747, 341, 1558, 295, 1419, 257, 1500, 992, 300, 321, 600, 7500, 36213, 293], "temperature": 0.0, "avg_logprob": -0.13918113708496094, "compression_ratio": 1.7641196013289036, "no_speech_prob": 4.9840749852592126e-05}, {"id": 309, "seek": 118948, "start": 1193.88, "end": 1195.68, "text": " go in a slightly different direction.", "tokens": [352, 294, 257, 4748, 819, 3513, 13], "temperature": 0.0, "avg_logprob": -0.13918113708496094, "compression_ratio": 1.7641196013289036, "no_speech_prob": 4.9840749852592126e-05}, {"id": 310, "seek": 118948, "start": 1195.68, "end": 1199.92, "text": " So we're going to have, what does it mean to try to understand the linguistic properties", "tokens": [407, 321, 434, 516, 281, 362, 11, 437, 775, 309, 914, 281, 853, 281, 1223, 264, 43002, 7221], "temperature": 0.0, "avg_logprob": -0.13918113708496094, "compression_ratio": 1.7641196013289036, "no_speech_prob": 4.9840749852592126e-05}, {"id": 311, "seek": 118948, "start": 1199.92, "end": 1200.92, "text": " of our models?", "tokens": [295, 527, 5245, 30], "temperature": 0.0, "avg_logprob": -0.13918113708496094, "compression_ratio": 1.7641196013289036, "no_speech_prob": 4.9840749852592126e-05}, {"id": 312, "seek": 118948, "start": 1200.92, "end": 1201.92, "text": " Does it?", "tokens": [4402, 309, 30], "temperature": 0.0, "avg_logprob": -0.13918113708496094, "compression_ratio": 1.7641196013289036, "no_speech_prob": 4.9840749852592126e-05}, {"id": 313, "seek": 118948, "start": 1201.92, "end": 1205.32, "text": " So that's some tactic heuristics question was one thing for natural language inference,", "tokens": [407, 300, 311, 512, 31012, 415, 374, 6006, 1168, 390, 472, 551, 337, 3303, 2856, 38253, 11], "temperature": 0.0, "avg_logprob": -0.13918113708496094, "compression_ratio": 1.7641196013289036, "no_speech_prob": 4.9840749852592126e-05}, {"id": 314, "seek": 118948, "start": 1205.32, "end": 1210.64, "text": " but can we sort of test how the models, whether they think certain things are sort of right", "tokens": [457, 393, 321, 1333, 295, 1500, 577, 264, 5245, 11, 1968, 436, 519, 1629, 721, 366, 1333, 295, 558], "temperature": 0.0, "avg_logprob": -0.13918113708496094, "compression_ratio": 1.7641196013289036, "no_speech_prob": 4.9840749852592126e-05}, {"id": 315, "seek": 118948, "start": 1210.64, "end": 1214.48, "text": " or wrong as language models?", "tokens": [420, 2085, 382, 2856, 5245, 30], "temperature": 0.0, "avg_logprob": -0.13918113708496094, "compression_ratio": 1.7641196013289036, "no_speech_prob": 4.9840749852592126e-05}, {"id": 316, "seek": 118948, "start": 1214.48, "end": 1218.16, "text": " And the first way that we'll do this is we'll ask, well, how do we think about sort", "tokens": [400, 264, 700, 636, 300, 321, 603, 360, 341, 307, 321, 603, 1029, 11, 731, 11, 577, 360, 321, 519, 466, 1333], "temperature": 0.0, "avg_logprob": -0.13918113708496094, "compression_ratio": 1.7641196013289036, "no_speech_prob": 4.9840749852592126e-05}, {"id": 317, "seek": 121816, "start": 1218.16, "end": 1221.3200000000002, "text": " of what humans think of as good language?", "tokens": [295, 437, 6255, 519, 295, 382, 665, 2856, 30], "temperature": 0.0, "avg_logprob": -0.1585327042473687, "compression_ratio": 1.619718309859155, "no_speech_prob": 4.126476778765209e-05}, {"id": 318, "seek": 121816, "start": 1221.3200000000002, "end": 1226.8400000000001, "text": " How do we evaluate their sort of preferences about language?", "tokens": [1012, 360, 321, 13059, 641, 1333, 295, 21910, 466, 2856, 30], "temperature": 0.0, "avg_logprob": -0.1585327042473687, "compression_ratio": 1.619718309859155, "no_speech_prob": 4.126476778765209e-05}, {"id": 319, "seek": 121816, "start": 1226.8400000000001, "end": 1229.0800000000002, "text": " And one answer is minimal pairs.", "tokens": [400, 472, 1867, 307, 13206, 15494, 13], "temperature": 0.0, "avg_logprob": -0.1585327042473687, "compression_ratio": 1.619718309859155, "no_speech_prob": 4.126476778765209e-05}, {"id": 320, "seek": 121816, "start": 1229.0800000000002, "end": 1234.8000000000002, "text": " And the idea of a minimal pair is that you've got one sentence that sounds okay to a speaker.", "tokens": [400, 264, 1558, 295, 257, 13206, 6119, 307, 300, 291, 600, 658, 472, 8174, 300, 3263, 1392, 281, 257, 8145, 13], "temperature": 0.0, "avg_logprob": -0.1585327042473687, "compression_ratio": 1.619718309859155, "no_speech_prob": 4.126476778765209e-05}, {"id": 321, "seek": 121816, "start": 1234.8000000000002, "end": 1238.76, "text": " So this sentence is the chef who made the pizzas is here.", "tokens": [407, 341, 8174, 307, 264, 10530, 567, 1027, 264, 44037, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.1585327042473687, "compression_ratio": 1.619718309859155, "no_speech_prob": 4.126476778765209e-05}, {"id": 322, "seek": 121816, "start": 1238.76, "end": 1243.8400000000001, "text": " It's called, it's an acceptable sentence, at least to me.", "tokens": [467, 311, 1219, 11, 309, 311, 364, 15513, 8174, 11, 412, 1935, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.1585327042473687, "compression_ratio": 1.619718309859155, "no_speech_prob": 4.126476778765209e-05}, {"id": 323, "seek": 124384, "start": 1243.84, "end": 1250.12, "text": " And then with a small change, a minimal change, the sentence is no longer okay to the speaker.", "tokens": [400, 550, 365, 257, 1359, 1319, 11, 257, 13206, 1319, 11, 264, 8174, 307, 572, 2854, 1392, 281, 264, 8145, 13], "temperature": 0.0, "avg_logprob": -0.2325974889548428, "compression_ratio": 1.6210526315789473, "no_speech_prob": 1.2024928764731158e-05}, {"id": 324, "seek": 124384, "start": 1250.12, "end": 1253.6399999999999, "text": " So the chef who made the pizzas are here.", "tokens": [407, 264, 10530, 567, 1027, 264, 44037, 366, 510, 13], "temperature": 0.0, "avg_logprob": -0.2325974889548428, "compression_ratio": 1.6210526315789473, "no_speech_prob": 1.2024928764731158e-05}, {"id": 325, "seek": 124384, "start": 1253.6399999999999, "end": 1261.3999999999999, "text": " And this, whoops, this should be, present tense verbs.", "tokens": [400, 341, 11, 567, 3370, 11, 341, 820, 312, 11, 1974, 18760, 30051, 13], "temperature": 0.0, "avg_logprob": -0.2325974889548428, "compression_ratio": 1.6210526315789473, "no_speech_prob": 1.2024928764731158e-05}, {"id": 326, "seek": 124384, "start": 1261.3999999999999, "end": 1265.76, "text": " In English, present tense verbs agree in number with their subject when they are third", "tokens": [682, 3669, 11, 1974, 18760, 30051, 3986, 294, 1230, 365, 641, 3983, 562, 436, 366, 2636], "temperature": 0.0, "avg_logprob": -0.2325974889548428, "compression_ratio": 1.6210526315789473, "no_speech_prob": 1.2024928764731158e-05}, {"id": 327, "seek": 124384, "start": 1265.76, "end": 1267.1599999999999, "text": " person.", "tokens": [954, 13], "temperature": 0.0, "avg_logprob": -0.2325974889548428, "compression_ratio": 1.6210526315789473, "no_speech_prob": 1.2024928764731158e-05}, {"id": 328, "seek": 124384, "start": 1267.1599999999999, "end": 1270.84, "text": " So chef pizzas, okay.", "tokens": [407, 10530, 44037, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.2325974889548428, "compression_ratio": 1.6210526315789473, "no_speech_prob": 1.2024928764731158e-05}, {"id": 329, "seek": 127084, "start": 1270.84, "end": 1274.9199999999998, "text": " And this is sort of a pretty general thing.", "tokens": [400, 341, 307, 1333, 295, 257, 1238, 2674, 551, 13], "temperature": 0.0, "avg_logprob": -0.15402490847578673, "compression_ratio": 1.5793650793650793, "no_speech_prob": 4.984572660760023e-05}, {"id": 330, "seek": 127084, "start": 1274.9199999999998, "end": 1276.0, "text": " Most people don't like this.", "tokens": [4534, 561, 500, 380, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.15402490847578673, "compression_ratio": 1.5793650793650793, "no_speech_prob": 4.984572660760023e-05}, {"id": 331, "seek": 127084, "start": 1276.0, "end": 1278.9199999999998, "text": " It's a misconjugated verb.", "tokens": [467, 311, 257, 3346, 1671, 42068, 770, 9595, 13], "temperature": 0.0, "avg_logprob": -0.15402490847578673, "compression_ratio": 1.5793650793650793, "no_speech_prob": 4.984572660760023e-05}, {"id": 332, "seek": 127084, "start": 1278.9199999999998, "end": 1283.36, "text": " And so the syntax here looks like you have the chef who made the pizzas.", "tokens": [400, 370, 264, 28431, 510, 1542, 411, 291, 362, 264, 10530, 567, 1027, 264, 44037, 13], "temperature": 0.0, "avg_logprob": -0.15402490847578673, "compression_ratio": 1.5793650793650793, "no_speech_prob": 4.984572660760023e-05}, {"id": 333, "seek": 127084, "start": 1283.36, "end": 1290.1999999999998, "text": " And then this arc of agreement in number is requiring the word is here to be singular", "tokens": [400, 550, 341, 10346, 295, 8106, 294, 1230, 307, 24165, 264, 1349, 307, 510, 281, 312, 20010], "temperature": 0.0, "avg_logprob": -0.15402490847578673, "compression_ratio": 1.5793650793650793, "no_speech_prob": 4.984572660760023e-05}, {"id": 334, "seek": 127084, "start": 1290.1999999999998, "end": 1292.4399999999998, "text": " is instead of plural R.", "tokens": [307, 2602, 295, 25377, 497, 13], "temperature": 0.0, "avg_logprob": -0.15402490847578673, "compression_ratio": 1.5793650793650793, "no_speech_prob": 4.984572660760023e-05}, {"id": 335, "seek": 127084, "start": 1292.4399999999998, "end": 1298.36, "text": " Despite the fact that there's this noun pizzas, which is plural, closer linearly, comes", "tokens": [11334, 264, 1186, 300, 456, 311, 341, 23307, 44037, 11, 597, 307, 25377, 11, 4966, 43586, 11, 1487], "temperature": 0.0, "avg_logprob": -0.15402490847578673, "compression_ratio": 1.5793650793650793, "no_speech_prob": 4.984572660760023e-05}, {"id": 336, "seek": 127084, "start": 1298.36, "end": 1300.0, "text": " back to dependency parsing.", "tokens": [646, 281, 33621, 21156, 278, 13], "temperature": 0.0, "avg_logprob": -0.15402490847578673, "compression_ratio": 1.5793650793650793, "no_speech_prob": 4.984572660760023e-05}, {"id": 337, "seek": 130000, "start": 1300.0, "end": 1302.32, "text": " Or back, okay.", "tokens": [1610, 646, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.22278841425863544, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.0237826649681665e-05}, {"id": 338, "seek": 130000, "start": 1302.32, "end": 1309.56, "text": " And what this looks like in the tree structure, right, is well, chef and is are attached in", "tokens": [400, 437, 341, 1542, 411, 294, 264, 4230, 3877, 11, 558, 11, 307, 731, 11, 10530, 293, 307, 366, 8570, 294], "temperature": 0.0, "avg_logprob": -0.22278841425863544, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.0237826649681665e-05}, {"id": 339, "seek": 130000, "start": 1309.56, "end": 1312.4, "text": " the tree.", "tokens": [264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.22278841425863544, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.0237826649681665e-05}, {"id": 340, "seek": 130000, "start": 1312.4, "end": 1317.16, "text": " Chef is the subject of is, pizza is down here in the subtree.", "tokens": [14447, 307, 264, 3983, 295, 307, 11, 8298, 307, 760, 510, 294, 264, 7257, 701, 13], "temperature": 0.0, "avg_logprob": -0.22278841425863544, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.0237826649681665e-05}, {"id": 341, "seek": 130000, "start": 1317.16, "end": 1322.64, "text": " And so that subject verb relationship has this sort of agreement thing.", "tokens": [400, 370, 300, 3983, 9595, 2480, 575, 341, 1333, 295, 8106, 551, 13], "temperature": 0.0, "avg_logprob": -0.22278841425863544, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.0237826649681665e-05}, {"id": 342, "seek": 130000, "start": 1322.64, "end": 1328.16, "text": " So this is a pretty sort of basic and interesting property of language that also reflects the", "tokens": [407, 341, 307, 257, 1238, 1333, 295, 3875, 293, 1880, 4707, 295, 2856, 300, 611, 18926, 264], "temperature": 0.0, "avg_logprob": -0.22278841425863544, "compression_ratio": 1.638095238095238, "no_speech_prob": 3.0237826649681665e-05}, {"id": 343, "seek": 132816, "start": 1328.16, "end": 1331.3200000000002, "text": " syntactic sort of hierarchical structure of language.", "tokens": [23980, 19892, 1333, 295, 35250, 804, 3877, 295, 2856, 13], "temperature": 0.0, "avg_logprob": -0.17567119767180586, "compression_ratio": 1.8191881918819188, "no_speech_prob": 5.7374913012608886e-05}, {"id": 344, "seek": 132816, "start": 1331.3200000000002, "end": 1334.4, "text": " So we've been training these language models sampling from them, seeing that they get", "tokens": [407, 321, 600, 668, 3097, 613, 2856, 5245, 21179, 490, 552, 11, 2577, 300, 436, 483], "temperature": 0.0, "avg_logprob": -0.17567119767180586, "compression_ratio": 1.8191881918819188, "no_speech_prob": 5.7374913012608886e-05}, {"id": 345, "seek": 132816, "start": 1334.4, "end": 1335.96, "text": " interesting things.", "tokens": [1880, 721, 13], "temperature": 0.0, "avg_logprob": -0.17567119767180586, "compression_ratio": 1.8191881918819188, "no_speech_prob": 5.7374913012608886e-05}, {"id": 346, "seek": 132816, "start": 1335.96, "end": 1339.44, "text": " And they tend to seem to generate syntactic content.", "tokens": [400, 436, 3928, 281, 1643, 281, 8460, 23980, 19892, 2701, 13], "temperature": 0.0, "avg_logprob": -0.17567119767180586, "compression_ratio": 1.8191881918819188, "no_speech_prob": 5.7374913012608886e-05}, {"id": 347, "seek": 132816, "start": 1339.44, "end": 1345.3600000000001, "text": " But does it really understand or does it behave as if it understands this idea of agreement", "tokens": [583, 775, 309, 534, 1223, 420, 775, 309, 15158, 382, 498, 309, 15146, 341, 1558, 295, 8106], "temperature": 0.0, "avg_logprob": -0.17567119767180586, "compression_ratio": 1.8191881918819188, "no_speech_prob": 5.7374913012608886e-05}, {"id": 348, "seek": 132816, "start": 1345.3600000000001, "end": 1349.76, "text": " more broadly and does it sort of get the syntax right so that it matches the subjects and", "tokens": [544, 19511, 293, 775, 309, 1333, 295, 483, 264, 28431, 558, 370, 300, 309, 10676, 264, 13066, 293], "temperature": 0.0, "avg_logprob": -0.17567119767180586, "compression_ratio": 1.8191881918819188, "no_speech_prob": 5.7374913012608886e-05}, {"id": 349, "seek": 132816, "start": 1349.76, "end": 1351.8000000000002, "text": " the verbs.", "tokens": [264, 30051, 13], "temperature": 0.0, "avg_logprob": -0.17567119767180586, "compression_ratio": 1.8191881918819188, "no_speech_prob": 5.7374913012608886e-05}, {"id": 350, "seek": 132816, "start": 1351.8000000000002, "end": 1356.3200000000002, "text": " But language models can't tell us exactly whether they think that a sentence is good or", "tokens": [583, 2856, 5245, 393, 380, 980, 505, 2293, 1968, 436, 519, 300, 257, 8174, 307, 665, 420], "temperature": 0.0, "avg_logprob": -0.17567119767180586, "compression_ratio": 1.8191881918819188, "no_speech_prob": 5.7374913012608886e-05}, {"id": 351, "seek": 135632, "start": 1356.32, "end": 1360.4399999999998, "text": " bad, they just tell us the probability of a sentence.", "tokens": [1578, 11, 436, 445, 980, 505, 264, 8482, 295, 257, 8174, 13], "temperature": 0.0, "avg_logprob": -0.12607195717947825, "compression_ratio": 2.0045871559633026, "no_speech_prob": 3.821271093329415e-05}, {"id": 352, "seek": 135632, "start": 1360.4399999999998, "end": 1365.8799999999999, "text": " So before we had acceptable and unacceptable, that's what we get from humans.", "tokens": [407, 949, 321, 632, 15513, 293, 31812, 11, 300, 311, 437, 321, 483, 490, 6255, 13], "temperature": 0.0, "avg_logprob": -0.12607195717947825, "compression_ratio": 2.0045871559633026, "no_speech_prob": 3.821271093329415e-05}, {"id": 353, "seek": 135632, "start": 1365.8799999999999, "end": 1370.6399999999999, "text": " And the language models analog is just, does it assign higher probability to the acceptable", "tokens": [400, 264, 2856, 5245, 16660, 307, 445, 11, 775, 309, 6269, 2946, 8482, 281, 264, 15513], "temperature": 0.0, "avg_logprob": -0.12607195717947825, "compression_ratio": 2.0045871559633026, "no_speech_prob": 3.821271093329415e-05}, {"id": 354, "seek": 135632, "start": 1370.6399999999999, "end": 1372.24, "text": " sentence in the minimal pair, right?", "tokens": [8174, 294, 264, 13206, 6119, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12607195717947825, "compression_ratio": 2.0045871559633026, "no_speech_prob": 3.821271093329415e-05}, {"id": 355, "seek": 135632, "start": 1372.24, "end": 1378.28, "text": " So you have the probability under the model of the chef who made the pizzas is here.", "tokens": [407, 291, 362, 264, 8482, 833, 264, 2316, 295, 264, 10530, 567, 1027, 264, 44037, 307, 510, 13], "temperature": 0.0, "avg_logprob": -0.12607195717947825, "compression_ratio": 2.0045871559633026, "no_speech_prob": 3.821271093329415e-05}, {"id": 356, "seek": 135632, "start": 1378.28, "end": 1382.8799999999999, "text": " And then you have the probability under the model of the chef who made the pizzas are", "tokens": [400, 550, 291, 362, 264, 8482, 833, 264, 2316, 295, 264, 10530, 567, 1027, 264, 44037, 366], "temperature": 0.0, "avg_logprob": -0.12607195717947825, "compression_ratio": 2.0045871559633026, "no_speech_prob": 3.821271093329415e-05}, {"id": 357, "seek": 135632, "start": 1382.8799999999999, "end": 1383.8799999999999, "text": " here.", "tokens": [510, 13], "temperature": 0.0, "avg_logprob": -0.12607195717947825, "compression_ratio": 2.0045871559633026, "no_speech_prob": 3.821271093329415e-05}, {"id": 358, "seek": 138388, "start": 1383.88, "end": 1388.1200000000001, "text": " And you want this probability here to be higher.", "tokens": [400, 291, 528, 341, 8482, 510, 281, 312, 2946, 13], "temperature": 0.0, "avg_logprob": -0.15529440641403197, "compression_ratio": 1.5159817351598173, "no_speech_prob": 9.664601748227142e-06}, {"id": 359, "seek": 138388, "start": 1388.1200000000001, "end": 1395.88, "text": " And if it is, that's sort of like a simple way to test whether the model got it right effectively.", "tokens": [400, 498, 309, 307, 11, 300, 311, 1333, 295, 411, 257, 2199, 636, 281, 1500, 1968, 264, 2316, 658, 309, 558, 8659, 13], "temperature": 0.0, "avg_logprob": -0.15529440641403197, "compression_ratio": 1.5159817351598173, "no_speech_prob": 9.664601748227142e-06}, {"id": 360, "seek": 138388, "start": 1395.88, "end": 1402.16, "text": " And just like in Huns, we can develop a test set with very carefully chosen properties,", "tokens": [400, 445, 411, 294, 11648, 82, 11, 321, 393, 1499, 257, 1500, 992, 365, 588, 7500, 8614, 7221, 11], "temperature": 0.0, "avg_logprob": -0.15529440641403197, "compression_ratio": 1.5159817351598173, "no_speech_prob": 9.664601748227142e-06}, {"id": 361, "seek": 138388, "start": 1402.16, "end": 1403.16, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.15529440641403197, "compression_ratio": 1.5159817351598173, "no_speech_prob": 9.664601748227142e-06}, {"id": 362, "seek": 138388, "start": 1403.16, "end": 1409.7600000000002, "text": " So most sentences in English don't have terribly complex subject verb agreement structure", "tokens": [407, 881, 16579, 294, 3669, 500, 380, 362, 22903, 3997, 3983, 9595, 8106, 3877], "temperature": 0.0, "avg_logprob": -0.15529440641403197, "compression_ratio": 1.5159817351598173, "no_speech_prob": 9.664601748227142e-06}, {"id": 363, "seek": 140976, "start": 1409.76, "end": 1414.2, "text": " or with a lot of words in the middle like pizzas that are going to make it difficult.", "tokens": [420, 365, 257, 688, 295, 2283, 294, 264, 2808, 411, 44037, 300, 366, 516, 281, 652, 309, 2252, 13], "temperature": 0.0, "avg_logprob": -0.16727640114578546, "compression_ratio": 1.7, "no_speech_prob": 5.223744665272534e-05}, {"id": 364, "seek": 140976, "start": 1414.2, "end": 1422.8, "text": " So if I say, you know, the dog runs sort of no way to get it wrong because there's no", "tokens": [407, 498, 286, 584, 11, 291, 458, 11, 264, 3000, 6676, 1333, 295, 572, 636, 281, 483, 309, 2085, 570, 456, 311, 572], "temperature": 0.0, "avg_logprob": -0.16727640114578546, "compression_ratio": 1.7, "no_speech_prob": 5.223744665272534e-05}, {"id": 365, "seek": 140976, "start": 1422.8, "end": 1424.96, "text": " syntax is very simple.", "tokens": [28431, 307, 588, 2199, 13], "temperature": 0.0, "avg_logprob": -0.16727640114578546, "compression_ratio": 1.7, "no_speech_prob": 5.223744665272534e-05}, {"id": 366, "seek": 140976, "start": 1424.96, "end": 1433.08, "text": " So we can create, well, we can look for sentences that have these things called attractors in", "tokens": [407, 321, 393, 1884, 11, 731, 11, 321, 393, 574, 337, 16579, 300, 362, 613, 721, 1219, 5049, 830, 294], "temperature": 0.0, "avg_logprob": -0.16727640114578546, "compression_ratio": 1.7, "no_speech_prob": 5.223744665272534e-05}, {"id": 367, "seek": 140976, "start": 1433.08, "end": 1434.08, "text": " the sentence.", "tokens": [264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.16727640114578546, "compression_ratio": 1.7, "no_speech_prob": 5.223744665272534e-05}, {"id": 368, "seek": 140976, "start": 1434.08, "end": 1439.68, "text": " So pizzas is an attractor because the model might be attracted to the plurality here and", "tokens": [407, 44037, 307, 364, 5049, 284, 570, 264, 2316, 1062, 312, 15912, 281, 264, 25377, 507, 510, 293], "temperature": 0.0, "avg_logprob": -0.16727640114578546, "compression_ratio": 1.7, "no_speech_prob": 5.223744665272534e-05}, {"id": 369, "seek": 143968, "start": 1439.68, "end": 1443.0800000000002, "text": " get the conjugation wrong.", "tokens": [483, 264, 29456, 399, 2085, 13], "temperature": 0.0, "avg_logprob": -0.16353028221467955, "compression_ratio": 1.7424242424242424, "no_speech_prob": 2.014193159993738e-05}, {"id": 370, "seek": 143968, "start": 1443.0800000000002, "end": 1444.0800000000002, "text": " So this is our question.", "tokens": [407, 341, 307, 527, 1168, 13], "temperature": 0.0, "avg_logprob": -0.16353028221467955, "compression_ratio": 1.7424242424242424, "no_speech_prob": 2.014193159993738e-05}, {"id": 371, "seek": 143968, "start": 1444.0800000000002, "end": 1448.6000000000001, "text": " Can language models sort of very generally handle these examples with attractors?", "tokens": [1664, 2856, 5245, 1333, 295, 588, 5101, 4813, 613, 5110, 365, 5049, 830, 30], "temperature": 0.0, "avg_logprob": -0.16353028221467955, "compression_ratio": 1.7424242424242424, "no_speech_prob": 2.014193159993738e-05}, {"id": 372, "seek": 143968, "start": 1448.6000000000001, "end": 1453.16, "text": " So we can take examples with zero attractors, see whether the model gets the minimal pairs", "tokens": [407, 321, 393, 747, 5110, 365, 4018, 5049, 830, 11, 536, 1968, 264, 2316, 2170, 264, 13206, 15494], "temperature": 0.0, "avg_logprob": -0.16353028221467955, "compression_ratio": 1.7424242424242424, "no_speech_prob": 2.014193159993738e-05}, {"id": 373, "seek": 143968, "start": 1453.16, "end": 1454.16, "text": " evaluation right.", "tokens": [13344, 558, 13], "temperature": 0.0, "avg_logprob": -0.16353028221467955, "compression_ratio": 1.7424242424242424, "no_speech_prob": 2.014193159993738e-05}, {"id": 374, "seek": 143968, "start": 1454.16, "end": 1458.48, "text": " We can take examples with one attractor, two attractors.", "tokens": [492, 393, 747, 5110, 365, 472, 5049, 284, 11, 732, 5049, 830, 13], "temperature": 0.0, "avg_logprob": -0.16353028221467955, "compression_ratio": 1.7424242424242424, "no_speech_prob": 2.014193159993738e-05}, {"id": 375, "seek": 143968, "start": 1458.48, "end": 1462.0, "text": " You can see how people would still reasonably understand the sentences, right?", "tokens": [509, 393, 536, 577, 561, 576, 920, 23551, 1223, 264, 16579, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16353028221467955, "compression_ratio": 1.7424242424242424, "no_speech_prob": 2.014193159993738e-05}, {"id": 376, "seek": 143968, "start": 1462.0, "end": 1464.76, "text": " Chef who made the pizzas and prep the ingredients is.", "tokens": [14447, 567, 1027, 264, 44037, 293, 2666, 264, 6952, 307, 13], "temperature": 0.0, "avg_logprob": -0.16353028221467955, "compression_ratio": 1.7424242424242424, "no_speech_prob": 2.014193159993738e-05}, {"id": 377, "seek": 143968, "start": 1464.76, "end": 1466.68, "text": " It's still the chef who is.", "tokens": [467, 311, 920, 264, 10530, 567, 307, 13], "temperature": 0.0, "avg_logprob": -0.16353028221467955, "compression_ratio": 1.7424242424242424, "no_speech_prob": 2.014193159993738e-05}, {"id": 378, "seek": 146668, "start": 1466.68, "end": 1472.76, "text": " And then on and on and on, it gets rarer, obviously, but you can have more and more attractors.", "tokens": [400, 550, 322, 293, 322, 293, 322, 11, 309, 2170, 367, 289, 260, 11, 2745, 11, 457, 291, 393, 362, 544, 293, 544, 5049, 830, 13], "temperature": 0.0, "avg_logprob": -0.16778063774108887, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.9306109354365617e-05}, {"id": 379, "seek": 146668, "start": 1472.76, "end": 1476.8, "text": " And so now we've created this test set that's intended to evaluate this very specific linguistic", "tokens": [400, 370, 586, 321, 600, 2942, 341, 1500, 992, 300, 311, 10226, 281, 13059, 341, 588, 2685, 43002], "temperature": 0.0, "avg_logprob": -0.16778063774108887, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.9306109354365617e-05}, {"id": 380, "seek": 146668, "start": 1476.8, "end": 1479.28, "text": " phenomenon.", "tokens": [14029, 13], "temperature": 0.0, "avg_logprob": -0.16778063774108887, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.9306109354365617e-05}, {"id": 381, "seek": 146668, "start": 1479.28, "end": 1486.6000000000001, "text": " So in this paper here, I concur at all, trained an LSTM language model on a subset of Wikipedia", "tokens": [407, 294, 341, 3035, 510, 11, 286, 23702, 412, 439, 11, 8895, 364, 441, 6840, 44, 2856, 2316, 322, 257, 25993, 295, 28999], "temperature": 0.0, "avg_logprob": -0.16778063774108887, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.9306109354365617e-05}, {"id": 382, "seek": 146668, "start": 1486.6000000000001, "end": 1488.24, "text": " back in 2018.", "tokens": [646, 294, 6096, 13], "temperature": 0.0, "avg_logprob": -0.16778063774108887, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.9306109354365617e-05}, {"id": 383, "seek": 146668, "start": 1488.24, "end": 1494.0, "text": " And they evaluate it sort of in these buckets that are specified by the paper that sort", "tokens": [400, 436, 13059, 309, 1333, 295, 294, 613, 32191, 300, 366, 22206, 538, 264, 3035, 300, 1333], "temperature": 0.0, "avg_logprob": -0.16778063774108887, "compression_ratio": 1.5889328063241106, "no_speech_prob": 2.9306109354365617e-05}, {"id": 384, "seek": 149400, "start": 1494.0, "end": 1502.8, "text": " of introduced subject verb agreement to the NLP field, or more recently at least, and", "tokens": [295, 7268, 3983, 9595, 8106, 281, 264, 426, 45196, 2519, 11, 420, 544, 3938, 412, 1935, 11, 293], "temperature": 0.0, "avg_logprob": -0.14427690322582537, "compression_ratio": 1.6307692307692307, "no_speech_prob": 2.2468553652288392e-05}, {"id": 385, "seek": 149400, "start": 1502.8, "end": 1506.28, "text": " they evaluate it in buckets based on the number of attractors.", "tokens": [436, 13059, 309, 294, 32191, 2361, 322, 264, 1230, 295, 5049, 830, 13], "temperature": 0.0, "avg_logprob": -0.14427690322582537, "compression_ratio": 1.6307692307692307, "no_speech_prob": 2.2468553652288392e-05}, {"id": 386, "seek": 149400, "start": 1506.28, "end": 1512.08, "text": " And so in this table here that you're about to see, the numbers are sort of the percentive", "tokens": [400, 370, 294, 341, 3199, 510, 300, 291, 434, 466, 281, 536, 11, 264, 3547, 366, 1333, 295, 264, 3043, 488], "temperature": 0.0, "avg_logprob": -0.14427690322582537, "compression_ratio": 1.6307692307692307, "no_speech_prob": 2.2468553652288392e-05}, {"id": 387, "seek": 149400, "start": 1512.08, "end": 1519.76, "text": " times that you get this assign higher probability to the correct sentence in the minimal pair.", "tokens": [1413, 300, 291, 483, 341, 6269, 2946, 8482, 281, 264, 3006, 8174, 294, 264, 13206, 6119, 13], "temperature": 0.0, "avg_logprob": -0.14427690322582537, "compression_ratio": 1.6307692307692307, "no_speech_prob": 2.2468553652288392e-05}, {"id": 388, "seek": 149400, "start": 1519.76, "end": 1523.72, "text": " So if you were just to do random or majority class, you get these errors, oh, sorry, it's", "tokens": [407, 498, 291, 645, 445, 281, 360, 4974, 420, 6286, 1508, 11, 291, 483, 613, 13603, 11, 1954, 11, 2597, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.14427690322582537, "compression_ratio": 1.6307692307692307, "no_speech_prob": 2.2468553652288392e-05}, {"id": 389, "seek": 152372, "start": 1523.72, "end": 1526.4, "text": " the percent of times that you get it wrong.", "tokens": [264, 3043, 295, 1413, 300, 291, 483, 309, 2085, 13], "temperature": 0.0, "avg_logprob": -0.18648652561375353, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.1657815775834024e-05}, {"id": 390, "seek": 152372, "start": 1526.4, "end": 1527.4, "text": " Sorry about that.", "tokens": [4919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.18648652561375353, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.1657815775834024e-05}, {"id": 391, "seek": 152372, "start": 1527.4, "end": 1530.08, "text": " So lower is better.", "tokens": [407, 3126, 307, 1101, 13], "temperature": 0.0, "avg_logprob": -0.18648652561375353, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.1657815775834024e-05}, {"id": 392, "seek": 152372, "start": 1530.08, "end": 1533.44, "text": " And so with no attractors, you get very low error rates.", "tokens": [400, 370, 365, 572, 5049, 830, 11, 291, 483, 588, 2295, 6713, 6846, 13], "temperature": 0.0, "avg_logprob": -0.18648652561375353, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.1657815775834024e-05}, {"id": 393, "seek": 152372, "start": 1533.44, "end": 1539.3600000000001, "text": " So this is 1.3 error rate with a 350-dimensional LSTM.", "tokens": [407, 341, 307, 502, 13, 18, 6713, 3314, 365, 257, 18065, 12, 18759, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.18648652561375353, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.1657815775834024e-05}, {"id": 394, "seek": 152372, "start": 1539.3600000000001, "end": 1545.44, "text": " And with one attractor, your error rate is higher, but actually humans start to get errors", "tokens": [400, 365, 472, 5049, 284, 11, 428, 6713, 3314, 307, 2946, 11, 457, 767, 6255, 722, 281, 483, 13603], "temperature": 0.0, "avg_logprob": -0.18648652561375353, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.1657815775834024e-05}, {"id": 395, "seek": 152372, "start": 1545.44, "end": 1547.3600000000001, "text": " with more attractors too.", "tokens": [365, 544, 5049, 830, 886, 13], "temperature": 0.0, "avg_logprob": -0.18648652561375353, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.1657815775834024e-05}, {"id": 396, "seek": 152372, "start": 1547.3600000000001, "end": 1550.28, "text": " So zero attractors is easy.", "tokens": [407, 4018, 5049, 830, 307, 1858, 13], "temperature": 0.0, "avg_logprob": -0.18648652561375353, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.1657815775834024e-05}, {"id": 397, "seek": 152372, "start": 1550.28, "end": 1553.64, "text": " The larger the LSTM, it looks like in general the better you're doing, right?", "tokens": [440, 4833, 264, 441, 6840, 44, 11, 309, 1542, 411, 294, 2674, 264, 1101, 291, 434, 884, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18648652561375353, "compression_ratio": 1.6910569105691058, "no_speech_prob": 1.1657815775834024e-05}, {"id": 398, "seek": 155364, "start": 1553.64, "end": 1556.5600000000002, "text": " So the smaller models doing worse, OK?", "tokens": [407, 264, 4356, 5245, 884, 5324, 11, 2264, 30], "temperature": 0.0, "avg_logprob": -0.21161737527933205, "compression_ratio": 1.646643109540636, "no_speech_prob": 5.5069822337827645e-06}, {"id": 399, "seek": 155364, "start": 1556.5600000000002, "end": 1561.5200000000002, "text": " And then even on sort of very difficult examples with four attractors, which I try to think", "tokens": [400, 550, 754, 322, 1333, 295, 588, 2252, 5110, 365, 1451, 5049, 830, 11, 597, 286, 853, 281, 519], "temperature": 0.0, "avg_logprob": -0.21161737527933205, "compression_ratio": 1.646643109540636, "no_speech_prob": 5.5069822337827645e-06}, {"id": 400, "seek": 155364, "start": 1561.5200000000002, "end": 1567.0400000000002, "text": " of an example in your head like the chef made the pizzas and took out the trash and sort", "tokens": [295, 364, 1365, 294, 428, 1378, 411, 264, 10530, 1027, 264, 44037, 293, 1890, 484, 264, 11321, 293, 1333], "temperature": 0.0, "avg_logprob": -0.21161737527933205, "compression_ratio": 1.646643109540636, "no_speech_prob": 5.5069822337827645e-06}, {"id": 401, "seek": 155364, "start": 1567.0400000000002, "end": 1572.92, "text": " of has to be this long sentence, the error rate is definitely higher, so it gets more difficult,", "tokens": [295, 575, 281, 312, 341, 938, 8174, 11, 264, 6713, 3314, 307, 2138, 2946, 11, 370, 309, 2170, 544, 2252, 11], "temperature": 0.0, "avg_logprob": -0.21161737527933205, "compression_ratio": 1.646643109540636, "no_speech_prob": 5.5069822337827645e-06}, {"id": 402, "seek": 155364, "start": 1572.92, "end": 1575.44, "text": " but it's still relatively low.", "tokens": [457, 309, 311, 920, 7226, 2295, 13], "temperature": 0.0, "avg_logprob": -0.21161737527933205, "compression_ratio": 1.646643109540636, "no_speech_prob": 5.5069822337827645e-06}, {"id": 403, "seek": 155364, "start": 1575.44, "end": 1578.8400000000001, "text": " And so even on these very hard examples, models are actually performing subject verb number", "tokens": [400, 370, 754, 322, 613, 588, 1152, 5110, 11, 5245, 366, 767, 10205, 3983, 9595, 1230], "temperature": 0.0, "avg_logprob": -0.21161737527933205, "compression_ratio": 1.646643109540636, "no_speech_prob": 5.5069822337827645e-06}, {"id": 404, "seek": 155364, "start": 1578.8400000000001, "end": 1581.44, "text": " agreement relatively well.", "tokens": [8106, 7226, 731, 13], "temperature": 0.0, "avg_logprob": -0.21161737527933205, "compression_ratio": 1.646643109540636, "no_speech_prob": 5.5069822337827645e-06}, {"id": 405, "seek": 158144, "start": 1581.44, "end": 1584.0, "text": " Very cool.", "tokens": [4372, 1627, 13], "temperature": 0.0, "avg_logprob": -0.1674113225455236, "compression_ratio": 1.7720930232558139, "no_speech_prob": 3.1198174838209525e-05}, {"id": 406, "seek": 158144, "start": 1584.0, "end": 1585.6000000000001, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.1674113225455236, "compression_ratio": 1.7720930232558139, "no_speech_prob": 3.1198174838209525e-05}, {"id": 407, "seek": 158144, "start": 1585.6000000000001, "end": 1588.76, "text": " Here are some examples that a model got wrong.", "tokens": [1692, 366, 512, 5110, 300, 257, 2316, 658, 2085, 13], "temperature": 0.0, "avg_logprob": -0.1674113225455236, "compression_ratio": 1.7720930232558139, "no_speech_prob": 3.1198174838209525e-05}, {"id": 408, "seek": 158144, "start": 1588.76, "end": 1592.2, "text": " This is actually a worse model than the ones from the paper that was just there, but I", "tokens": [639, 307, 767, 257, 5324, 2316, 813, 264, 2306, 490, 264, 3035, 300, 390, 445, 456, 11, 457, 286], "temperature": 0.0, "avg_logprob": -0.1674113225455236, "compression_ratio": 1.7720930232558139, "no_speech_prob": 3.1198174838209525e-05}, {"id": 409, "seek": 158144, "start": 1592.2, "end": 1595.0800000000002, "text": " think actually the errors are quite interesting.", "tokens": [519, 767, 264, 13603, 366, 1596, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1674113225455236, "compression_ratio": 1.7720930232558139, "no_speech_prob": 3.1198174838209525e-05}, {"id": 410, "seek": 158144, "start": 1595.0800000000002, "end": 1601.4, "text": " So here's a sentence, the ship that the player drives has a very high speed.", "tokens": [407, 510, 311, 257, 8174, 11, 264, 5374, 300, 264, 4256, 11754, 575, 257, 588, 1090, 3073, 13], "temperature": 0.0, "avg_logprob": -0.1674113225455236, "compression_ratio": 1.7720930232558139, "no_speech_prob": 3.1198174838209525e-05}, {"id": 411, "seek": 158144, "start": 1601.4, "end": 1607.68, "text": " Now this model thought that was less probable than the ship that the player drives have a", "tokens": [823, 341, 2316, 1194, 300, 390, 1570, 21759, 813, 264, 5374, 300, 264, 4256, 11754, 362, 257], "temperature": 0.0, "avg_logprob": -0.1674113225455236, "compression_ratio": 1.7720930232558139, "no_speech_prob": 3.1198174838209525e-05}, {"id": 412, "seek": 158144, "start": 1607.68, "end": 1611.0800000000002, "text": " very high speed.", "tokens": [588, 1090, 3073, 13], "temperature": 0.0, "avg_logprob": -0.1674113225455236, "compression_ratio": 1.7720930232558139, "no_speech_prob": 3.1198174838209525e-05}, {"id": 413, "seek": 161108, "start": 1611.08, "end": 1620.08, "text": " My hypothesis, right, is that it sort of misanalyzes drives as a plural noun, for example,", "tokens": [1222, 17291, 11, 558, 11, 307, 300, 309, 1333, 295, 3346, 282, 5222, 12214, 11754, 382, 257, 25377, 23307, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.27259165831286497, "compression_ratio": 1.584070796460177, "no_speech_prob": 5.22331683896482e-05}, {"id": 414, "seek": 161108, "start": 1620.08, "end": 1621.76, "text": " sort of a difficult construction there.", "tokens": [1333, 295, 257, 2252, 6435, 456, 13], "temperature": 0.0, "avg_logprob": -0.27259165831286497, "compression_ratio": 1.584070796460177, "no_speech_prob": 5.22331683896482e-05}, {"id": 415, "seek": 161108, "start": 1621.76, "end": 1624.6799999999998, "text": " I think it's pretty interesting.", "tokens": [286, 519, 309, 311, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.27259165831286497, "compression_ratio": 1.584070796460177, "no_speech_prob": 5.22331683896482e-05}, {"id": 416, "seek": 161108, "start": 1624.6799999999998, "end": 1627.1599999999999, "text": " Likewise here, this one is fun.", "tokens": [30269, 510, 11, 341, 472, 307, 1019, 13], "temperature": 0.0, "avg_logprob": -0.27259165831286497, "compression_ratio": 1.584070796460177, "no_speech_prob": 5.22331683896482e-05}, {"id": 417, "seek": 161108, "start": 1627.1599999999999, "end": 1629.4399999999998, "text": " The lead is also rather long.", "tokens": [440, 1477, 307, 611, 2831, 938, 13], "temperature": 0.0, "avg_logprob": -0.27259165831286497, "compression_ratio": 1.584070796460177, "no_speech_prob": 5.22331683896482e-05}, {"id": 418, "seek": 161108, "start": 1629.4399999999998, "end": 1632.6799999999998, "text": " Five paragraphs is pretty lengthy.", "tokens": [9436, 48910, 307, 1238, 35374, 13], "temperature": 0.0, "avg_logprob": -0.27259165831286497, "compression_ratio": 1.584070796460177, "no_speech_prob": 5.22331683896482e-05}, {"id": 419, "seek": 161108, "start": 1632.6799999999998, "end": 1638.76, "text": " So here five paragraphs is a singular noun together, it gets it's like a unit of length,", "tokens": [407, 510, 1732, 48910, 307, 257, 20010, 23307, 1214, 11, 309, 2170, 309, 311, 411, 257, 4985, 295, 4641, 11], "temperature": 0.0, "avg_logprob": -0.27259165831286497, "compression_ratio": 1.584070796460177, "no_speech_prob": 5.22331683896482e-05}, {"id": 420, "seek": 161108, "start": 1638.76, "end": 1639.76, "text": " I guess.", "tokens": [286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.27259165831286497, "compression_ratio": 1.584070796460177, "no_speech_prob": 5.22331683896482e-05}, {"id": 421, "seek": 163976, "start": 1639.76, "end": 1646.48, "text": " But the model thought that it was more likely to say five paragraphs are pretty lengthy,", "tokens": [583, 264, 2316, 1194, 300, 309, 390, 544, 3700, 281, 584, 1732, 48910, 366, 1238, 35374, 11], "temperature": 0.0, "avg_logprob": -0.2788967932424238, "compression_ratio": 1.5476190476190477, "no_speech_prob": 2.8849035516032018e-05}, {"id": 422, "seek": 163976, "start": 1646.48, "end": 1652.8, "text": " because it's referring to this sort of five paragraphs as the five actual paragraphs", "tokens": [570, 309, 311, 13761, 281, 341, 1333, 295, 1732, 48910, 382, 264, 1732, 3539, 48910], "temperature": 0.0, "avg_logprob": -0.2788967932424238, "compression_ratio": 1.5476190476190477, "no_speech_prob": 2.8849035516032018e-05}, {"id": 423, "seek": 163976, "start": 1652.8, "end": 1657.8, "text": " themselves as opposed to a single unit of length describing the lead.", "tokens": [2969, 382, 8851, 281, 257, 2167, 4985, 295, 4641, 16141, 264, 1477, 13], "temperature": 0.0, "avg_logprob": -0.2788967932424238, "compression_ratio": 1.5476190476190477, "no_speech_prob": 2.8849035516032018e-05}, {"id": 424, "seek": 163976, "start": 1657.8, "end": 1659.8, "text": " Fascinating.", "tokens": [49098, 8205, 13], "temperature": 0.0, "avg_logprob": -0.2788967932424238, "compression_ratio": 1.5476190476190477, "no_speech_prob": 2.8849035516032018e-05}, {"id": 425, "seek": 163976, "start": 1659.8, "end": 1662.8, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.2788967932424238, "compression_ratio": 1.5476190476190477, "no_speech_prob": 2.8849035516032018e-05}, {"id": 426, "seek": 166280, "start": 1662.8, "end": 1673.6, "text": " Maybe questions again?", "tokens": [2704, 1651, 797, 30], "temperature": 0.0, "avg_logprob": -0.31536286376243416, "compression_ratio": 1.2148760330578512, "no_speech_prob": 6.481375021394342e-05}, {"id": 427, "seek": 166280, "start": 1673.6, "end": 1676.24, "text": " So I guess there are a couple.", "tokens": [407, 286, 2041, 456, 366, 257, 1916, 13], "temperature": 0.0, "avg_logprob": -0.31536286376243416, "compression_ratio": 1.2148760330578512, "no_speech_prob": 6.481375021394342e-05}, {"id": 428, "seek": 166280, "start": 1676.24, "end": 1684.6399999999999, "text": " Can we do the similar heuristic analysis for other tasks such as Q and A classification?", "tokens": [1664, 321, 360, 264, 2531, 415, 374, 3142, 5215, 337, 661, 9608, 1270, 382, 1249, 293, 316, 21538, 30], "temperature": 0.0, "avg_logprob": -0.31536286376243416, "compression_ratio": 1.2148760330578512, "no_speech_prob": 6.481375021394342e-05}, {"id": 429, "seek": 166280, "start": 1684.6399999999999, "end": 1687.68, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.31536286376243416, "compression_ratio": 1.2148760330578512, "no_speech_prob": 6.481375021394342e-05}, {"id": 430, "seek": 168768, "start": 1687.68, "end": 1694.68, "text": " So yes, I think that it's easier to do this kind of analysis for the Huns style analysis", "tokens": [407, 2086, 11, 286, 519, 300, 309, 311, 3571, 281, 360, 341, 733, 295, 5215, 337, 264, 11648, 82, 3758, 5215], "temperature": 0.0, "avg_logprob": -0.18156036734580994, "compression_ratio": 1.5085714285714287, "no_speech_prob": 2.5456167350057513e-05}, {"id": 431, "seek": 168768, "start": 1694.68, "end": 1703.0, "text": " with question answering and other sorts of tasks, because you can construct examples", "tokens": [365, 1168, 13430, 293, 661, 7527, 295, 9608, 11, 570, 291, 393, 7690, 5110], "temperature": 0.0, "avg_logprob": -0.18156036734580994, "compression_ratio": 1.5085714285714287, "no_speech_prob": 2.5456167350057513e-05}, {"id": 432, "seek": 168768, "start": 1703.0, "end": 1715.16, "text": " that similarly have these heuristics and then have the answer depend on the syntax or", "tokens": [300, 14138, 362, 613, 415, 374, 6006, 293, 550, 362, 264, 1867, 5672, 322, 264, 28431, 420], "temperature": 0.0, "avg_logprob": -0.18156036734580994, "compression_ratio": 1.5085714285714287, "no_speech_prob": 2.5456167350057513e-05}, {"id": 433, "seek": 168768, "start": 1715.16, "end": 1716.16, "text": " not.", "tokens": [406, 13], "temperature": 0.0, "avg_logprob": -0.18156036734580994, "compression_ratio": 1.5085714285714287, "no_speech_prob": 2.5456167350057513e-05}, {"id": 434, "seek": 171616, "start": 1716.16, "end": 1721.72, "text": " The actual probability of one sentence is higher than the other, of course, is sort of a language", "tokens": [440, 3539, 8482, 295, 472, 8174, 307, 2946, 813, 264, 661, 11, 295, 1164, 11, 307, 1333, 295, 257, 2856], "temperature": 0.0, "avg_logprob": -0.23533926407496134, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00013329196372069418}, {"id": 435, "seek": 171616, "start": 1721.72, "end": 1723.52, "text": " model dependent thing.", "tokens": [2316, 12334, 551, 13], "temperature": 0.0, "avg_logprob": -0.23533926407496134, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00013329196372069418}, {"id": 436, "seek": 171616, "start": 1723.52, "end": 1732.28, "text": " But the idea that you can sort of develop kind of bespoke test sets for various tasks,", "tokens": [583, 264, 1558, 300, 291, 393, 1333, 295, 1499, 733, 295, 4097, 48776, 1500, 6352, 337, 3683, 9608, 11], "temperature": 0.0, "avg_logprob": -0.23533926407496134, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00013329196372069418}, {"id": 437, "seek": 171616, "start": 1732.28, "end": 1734.3600000000001, "text": " I think is very, very general.", "tokens": [286, 519, 307, 588, 11, 588, 2674, 13], "temperature": 0.0, "avg_logprob": -0.23533926407496134, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00013329196372069418}, {"id": 438, "seek": 171616, "start": 1734.3600000000001, "end": 1739.2, "text": " And something that I think is actually quite interesting.", "tokens": [400, 746, 300, 286, 519, 307, 767, 1596, 1880, 13], "temperature": 0.0, "avg_logprob": -0.23533926407496134, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00013329196372069418}, {"id": 439, "seek": 171616, "start": 1739.2, "end": 1740.2, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.23533926407496134, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00013329196372069418}, {"id": 440, "seek": 171616, "start": 1740.2, "end": 1745.0400000000002, "text": " So I won't go on further, but I think the answer is just yes.", "tokens": [407, 286, 1582, 380, 352, 322, 3052, 11, 457, 286, 519, 264, 1867, 307, 445, 2086, 13], "temperature": 0.0, "avg_logprob": -0.23533926407496134, "compression_ratio": 1.5782608695652174, "no_speech_prob": 0.00013329196372069418}, {"id": 441, "seek": 174504, "start": 1745.04, "end": 1747.44, "text": " So there's another one.", "tokens": [407, 456, 311, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 442, "seek": 174504, "start": 1747.44, "end": 1750.24, "text": " How do you know where to find these failure cases?", "tokens": [1012, 360, 291, 458, 689, 281, 915, 613, 7763, 3331, 30], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 443, "seek": 174504, "start": 1750.24, "end": 1753.8799999999999, "text": " Maybe that's the right time to advertise linguistics classes.", "tokens": [2704, 300, 311, 264, 558, 565, 281, 35379, 21766, 6006, 5359, 13], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 444, "seek": 174504, "start": 1753.8799999999999, "end": 1754.8799999999999, "text": " Sorry.", "tokens": [4919, 13], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 445, "seek": 174504, "start": 1754.8799999999999, "end": 1757.52, "text": " You're still very quiet over here.", "tokens": [509, 434, 920, 588, 5677, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 446, "seek": 174504, "start": 1757.52, "end": 1759.8, "text": " How do you find what?", "tokens": [1012, 360, 291, 915, 437, 30], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 447, "seek": 174504, "start": 1759.8, "end": 1763.08, "text": " How do you know where to find these failure cases?", "tokens": [1012, 360, 291, 458, 689, 281, 915, 613, 7763, 3331, 30], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 448, "seek": 174504, "start": 1763.08, "end": 1764.08, "text": " Oh, interesting.", "tokens": [876, 11, 1880, 13], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 449, "seek": 174504, "start": 1764.08, "end": 1765.08, "text": " Yes.", "tokens": [1079, 13], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 450, "seek": 174504, "start": 1765.08, "end": 1767.2, "text": " How do we know where to find the failure cases?", "tokens": [1012, 360, 321, 458, 689, 281, 915, 264, 7763, 3331, 30], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 451, "seek": 174504, "start": 1767.2, "end": 1768.44, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.25559706826811857, "compression_ratio": 1.8852459016393444, "no_speech_prob": 3.112273770966567e-05}, {"id": 452, "seek": 176844, "start": 1768.44, "end": 1776.88, "text": " I mean, I think I agree with Chris that actually thinking about what is interesting about things", "tokens": [286, 914, 11, 286, 519, 286, 3986, 365, 6688, 300, 767, 1953, 466, 437, 307, 1880, 466, 721], "temperature": 0.0, "avg_logprob": -0.22324013710021973, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.246387884952128e-05}, {"id": 453, "seek": 176844, "start": 1776.88, "end": 1779.4, "text": " in language is one way to do it.", "tokens": [294, 2856, 307, 472, 636, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.22324013710021973, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.246387884952128e-05}, {"id": 454, "seek": 176844, "start": 1779.4, "end": 1787.16, "text": " I mean, the heuristics that we saw in our language model, sorry, in our NLI models with", "tokens": [286, 914, 11, 264, 415, 374, 6006, 300, 321, 1866, 294, 527, 2856, 2316, 11, 2597, 11, 294, 527, 426, 48718, 5245, 365], "temperature": 0.0, "avg_logprob": -0.22324013710021973, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.246387884952128e-05}, {"id": 455, "seek": 176844, "start": 1787.16, "end": 1796.0800000000002, "text": " hans, you can kind of imagine that they, if the model was sort of ignoring facts about", "tokens": [276, 599, 11, 291, 393, 733, 295, 3811, 300, 436, 11, 498, 264, 2316, 390, 1333, 295, 26258, 9130, 466], "temperature": 0.0, "avg_logprob": -0.22324013710021973, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.246387884952128e-05}, {"id": 456, "seek": 179608, "start": 1796.08, "end": 1801.1599999999999, "text": " language and sort of just doing this sort of rough bag of words with some extra magic,", "tokens": [2856, 293, 1333, 295, 445, 884, 341, 1333, 295, 5903, 3411, 295, 2283, 365, 512, 2857, 5585, 11], "temperature": 0.0, "avg_logprob": -0.15200595466458067, "compression_ratio": 1.7719298245614035, "no_speech_prob": 7.13710323907435e-05}, {"id": 457, "seek": 179608, "start": 1801.1599999999999, "end": 1805.52, "text": " then it would do well about as bad as it is doing here.", "tokens": [550, 309, 576, 360, 731, 466, 382, 1578, 382, 309, 307, 884, 510, 13], "temperature": 0.0, "avg_logprob": -0.15200595466458067, "compression_ratio": 1.7719298245614035, "no_speech_prob": 7.13710323907435e-05}, {"id": 458, "seek": 179608, "start": 1805.52, "end": 1812.4399999999998, "text": " And these sorts of ideas about understanding that this statement, if the artist slept", "tokens": [400, 613, 7527, 295, 3487, 466, 3701, 300, 341, 5629, 11, 498, 264, 5748, 17400], "temperature": 0.0, "avg_logprob": -0.15200595466458067, "compression_ratio": 1.7719298245614035, "no_speech_prob": 7.13710323907435e-05}, {"id": 459, "seek": 179608, "start": 1812.4399999999998, "end": 1817.6399999999999, "text": " the actor ran does not imply the artist slept, is the kind of thing that maybe you'd think", "tokens": [264, 8747, 5872, 775, 406, 33616, 264, 5748, 17400, 11, 307, 264, 733, 295, 551, 300, 1310, 291, 1116, 519], "temperature": 0.0, "avg_logprob": -0.15200595466458067, "compression_ratio": 1.7719298245614035, "no_speech_prob": 7.13710323907435e-05}, {"id": 460, "seek": 179608, "start": 1817.6399999999999, "end": 1822.96, "text": " up on your own, but also you'd spend time sort of pondering about and thinking broad", "tokens": [493, 322, 428, 1065, 11, 457, 611, 291, 1116, 3496, 565, 1333, 295, 17384, 1794, 466, 293, 1953, 4152], "temperature": 0.0, "avg_logprob": -0.15200595466458067, "compression_ratio": 1.7719298245614035, "no_speech_prob": 7.13710323907435e-05}, {"id": 461, "seek": 182296, "start": 1822.96, "end": 1829.96, "text": " thoughts about in linguistics curricula as well.", "tokens": [4598, 466, 294, 21766, 6006, 13179, 3780, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.2939881411465732, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0001883556105894968}, {"id": 462, "seek": 182296, "start": 1829.96, "end": 1836.08, "text": " Anything else, Chris?", "tokens": [11998, 1646, 11, 6688, 30], "temperature": 0.0, "avg_logprob": -0.2939881411465732, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0001883556105894968}, {"id": 463, "seek": 182296, "start": 1836.08, "end": 1842.16, "text": " So there's also, well, I guess someone was also saying, I think it's about the sort of", "tokens": [407, 456, 311, 611, 11, 731, 11, 286, 2041, 1580, 390, 611, 1566, 11, 286, 519, 309, 311, 466, 264, 1333, 295], "temperature": 0.0, "avg_logprob": -0.2939881411465732, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0001883556105894968}, {"id": 464, "seek": 182296, "start": 1842.16, "end": 1848.8, "text": " intervening verbs example, or intervening nouns, sorry, example, but the data set itself", "tokens": [17104, 278, 30051, 1365, 11, 420, 17104, 278, 48184, 11, 2597, 11, 1365, 11, 457, 264, 1412, 992, 2564], "temperature": 0.0, "avg_logprob": -0.2939881411465732, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0001883556105894968}, {"id": 465, "seek": 184880, "start": 1848.8, "end": 1853.28, "text": " probably includes mistakes with higher attractors.", "tokens": [1391, 5974, 8038, 365, 2946, 5049, 830, 13], "temperature": 0.0, "avg_logprob": -0.1911044353392066, "compression_ratio": 1.6040609137055837, "no_speech_prob": 2.0135192244197242e-05}, {"id": 466, "seek": 184880, "start": 1853.28, "end": 1855.68, "text": " Yeah, yeah, that's a good point.", "tokens": [865, 11, 1338, 11, 300, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.1911044353392066, "compression_ratio": 1.6040609137055837, "no_speech_prob": 2.0135192244197242e-05}, {"id": 467, "seek": 184880, "start": 1855.68, "end": 1863.9199999999998, "text": " Yeah, because humans make more and more mistakes as the number of attractors gets larger.", "tokens": [865, 11, 570, 6255, 652, 544, 293, 544, 8038, 382, 264, 1230, 295, 5049, 830, 2170, 4833, 13], "temperature": 0.0, "avg_logprob": -0.1911044353392066, "compression_ratio": 1.6040609137055837, "no_speech_prob": 2.0135192244197242e-05}, {"id": 468, "seek": 184880, "start": 1863.9199999999998, "end": 1870.44, "text": " On the other hand, I think that the mistakes are fewer in written text than in spoken.", "tokens": [1282, 264, 661, 1011, 11, 286, 519, 300, 264, 8038, 366, 13366, 294, 3720, 2487, 813, 294, 10759, 13], "temperature": 0.0, "avg_logprob": -0.1911044353392066, "compression_ratio": 1.6040609137055837, "no_speech_prob": 2.0135192244197242e-05}, {"id": 469, "seek": 184880, "start": 1870.44, "end": 1874.1599999999999, "text": " Maybe I'm just making that up, but that's what I think.", "tokens": [2704, 286, 478, 445, 1455, 300, 493, 11, 457, 300, 311, 437, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.1911044353392066, "compression_ratio": 1.6040609137055837, "no_speech_prob": 2.0135192244197242e-05}, {"id": 470, "seek": 187416, "start": 1874.16, "end": 1879.64, "text": " But yeah, it would be interesting to actually go through that test set and see how many", "tokens": [583, 1338, 11, 309, 576, 312, 1880, 281, 767, 352, 807, 300, 1500, 992, 293, 536, 577, 867], "temperature": 0.0, "avg_logprob": -0.25719651721772696, "compression_ratio": 1.437125748502994, "no_speech_prob": 2.0451207092264667e-05}, {"id": 471, "seek": 187416, "start": 1879.64, "end": 1884.8000000000002, "text": " of the errors the really strong model makes are actually due to the sort of observed form", "tokens": [295, 264, 13603, 264, 534, 2068, 2316, 1669, 366, 767, 3462, 281, 264, 1333, 295, 13095, 1254], "temperature": 0.0, "avg_logprob": -0.25719651721772696, "compression_ratio": 1.437125748502994, "no_speech_prob": 2.0451207092264667e-05}, {"id": 472, "seek": 187416, "start": 1884.8000000000002, "end": 1885.8000000000002, "text": " being incorrect.", "tokens": [885, 18424, 13], "temperature": 0.0, "avg_logprob": -0.25719651721772696, "compression_ratio": 1.437125748502994, "no_speech_prob": 2.0451207092264667e-05}, {"id": 473, "seek": 187416, "start": 1885.8000000000002, "end": 1892.8000000000002, "text": " I'd be super curious.", "tokens": [286, 1116, 312, 1687, 6369, 13], "temperature": 0.0, "avg_logprob": -0.25719651721772696, "compression_ratio": 1.437125748502994, "no_speech_prob": 2.0451207092264667e-05}, {"id": 474, "seek": 187416, "start": 1892.8000000000002, "end": 1896.0, "text": " Okay, should I move on?", "tokens": [1033, 11, 820, 286, 1286, 322, 30], "temperature": 0.0, "avg_logprob": -0.25719651721772696, "compression_ratio": 1.437125748502994, "no_speech_prob": 2.0451207092264667e-05}, {"id": 475, "seek": 189600, "start": 1896.0, "end": 1907.8, "text": " Yep, great.", "tokens": [7010, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.2385979209627424, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.7102836611447856e-05}, {"id": 476, "seek": 189600, "start": 1907.8, "end": 1915.8, "text": " Okay, so what does it feel like we're doing when we are kind of constructing these sort", "tokens": [1033, 11, 370, 437, 775, 309, 841, 411, 321, 434, 884, 562, 321, 366, 733, 295, 39969, 613, 1333], "temperature": 0.0, "avg_logprob": -0.2385979209627424, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.7102836611447856e-05}, {"id": 477, "seek": 189600, "start": 1915.8, "end": 1919.84, "text": " of bespoke, small, careful test sets for various phenomena?", "tokens": [295, 4097, 48776, 11, 1359, 11, 5026, 1500, 6352, 337, 3683, 22004, 30], "temperature": 0.0, "avg_logprob": -0.2385979209627424, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.7102836611447856e-05}, {"id": 478, "seek": 189600, "start": 1919.84, "end": 1923.76, "text": " Well, it sort of feels like unit testing.", "tokens": [1042, 11, 309, 1333, 295, 3417, 411, 4985, 4997, 13], "temperature": 0.0, "avg_logprob": -0.2385979209627424, "compression_ratio": 1.3862068965517242, "no_speech_prob": 2.7102836611447856e-05}, {"id": 479, "seek": 192376, "start": 1923.76, "end": 1933.52, "text": " And in fact, this sort of idea has been brought to the fore, you might say an NLP, unit tests", "tokens": [400, 294, 1186, 11, 341, 1333, 295, 1558, 575, 668, 3038, 281, 264, 2091, 11, 291, 1062, 584, 364, 426, 45196, 11, 4985, 6921], "temperature": 0.0, "avg_logprob": -0.17911758025487265, "compression_ratio": 1.5528455284552845, "no_speech_prob": 2.045886321866419e-05}, {"id": 480, "seek": 192376, "start": 1933.52, "end": 1935.28, "text": " but for these NLP neural networks.", "tokens": [457, 337, 613, 426, 45196, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.17911758025487265, "compression_ratio": 1.5528455284552845, "no_speech_prob": 2.045886321866419e-05}, {"id": 481, "seek": 192376, "start": 1935.28, "end": 1941.8799999999999, "text": " And in particular, the paper here that I'm citing at the bottom suggests this minimum", "tokens": [400, 294, 1729, 11, 264, 3035, 510, 300, 286, 478, 48749, 412, 264, 2767, 13409, 341, 7285], "temperature": 0.0, "avg_logprob": -0.17911758025487265, "compression_ratio": 1.5528455284552845, "no_speech_prob": 2.045886321866419e-05}, {"id": 482, "seek": 192376, "start": 1941.8799999999999, "end": 1942.8799999999999, "text": " functionality test.", "tokens": [14980, 1500, 13], "temperature": 0.0, "avg_logprob": -0.17911758025487265, "compression_ratio": 1.5528455284552845, "no_speech_prob": 2.045886321866419e-05}, {"id": 483, "seek": 192376, "start": 1942.8799999999999, "end": 1948.36, "text": " You want a small test set that targets a specific behavior that should sound like some of the", "tokens": [509, 528, 257, 1359, 1500, 992, 300, 12911, 257, 2685, 5223, 300, 820, 1626, 411, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.17911758025487265, "compression_ratio": 1.5528455284552845, "no_speech_prob": 2.045886321866419e-05}, {"id": 484, "seek": 192376, "start": 1948.36, "end": 1951.0, "text": " things that we were, that we've already talked about.", "tokens": [721, 300, 321, 645, 11, 300, 321, 600, 1217, 2825, 466, 13], "temperature": 0.0, "avg_logprob": -0.17911758025487265, "compression_ratio": 1.5528455284552845, "no_speech_prob": 2.045886321866419e-05}, {"id": 485, "seek": 195100, "start": 1951.0, "end": 1954.8, "text": " But in this case, we're going to get even more specific.", "tokens": [583, 294, 341, 1389, 11, 321, 434, 516, 281, 483, 754, 544, 2685, 13], "temperature": 0.0, "avg_logprob": -0.17946766872032016, "compression_ratio": 1.7608695652173914, "no_speech_prob": 1.805710962798912e-05}, {"id": 486, "seek": 195100, "start": 1954.8, "end": 1956.88, "text": " So here's a single test case.", "tokens": [407, 510, 311, 257, 2167, 1500, 1389, 13], "temperature": 0.0, "avg_logprob": -0.17946766872032016, "compression_ratio": 1.7608695652173914, "no_speech_prob": 1.805710962798912e-05}, {"id": 487, "seek": 195100, "start": 1956.88, "end": 1962.28, "text": " We're going to have an expected label, what was actually predicted, whether the model passed", "tokens": [492, 434, 516, 281, 362, 364, 5176, 7645, 11, 437, 390, 767, 19147, 11, 1968, 264, 2316, 4678], "temperature": 0.0, "avg_logprob": -0.17946766872032016, "compression_ratio": 1.7608695652173914, "no_speech_prob": 1.805710962798912e-05}, {"id": 488, "seek": 195100, "start": 1962.28, "end": 1964.04, "text": " this unit test.", "tokens": [341, 4985, 1500, 13], "temperature": 0.0, "avg_logprob": -0.17946766872032016, "compression_ratio": 1.7608695652173914, "no_speech_prob": 1.805710962798912e-05}, {"id": 489, "seek": 195100, "start": 1964.04, "end": 1967.8, "text": " And the labels are going to be sentiment analysis here.", "tokens": [400, 264, 16949, 366, 516, 281, 312, 16149, 5215, 510, 13], "temperature": 0.0, "avg_logprob": -0.17946766872032016, "compression_ratio": 1.7608695652173914, "no_speech_prob": 1.805710962798912e-05}, {"id": 490, "seek": 195100, "start": 1967.8, "end": 1972.44, "text": " So negative label, positive label, or neutral, or the three options.", "tokens": [407, 3671, 7645, 11, 3353, 7645, 11, 420, 10598, 11, 420, 264, 1045, 3956, 13], "temperature": 0.0, "avg_logprob": -0.17946766872032016, "compression_ratio": 1.7608695652173914, "no_speech_prob": 1.805710962798912e-05}, {"id": 491, "seek": 195100, "start": 1972.44, "end": 1977.76, "text": " And the unit test is going to consist simply of sentences that follow this template.", "tokens": [400, 264, 4985, 1500, 307, 516, 281, 4603, 2935, 295, 16579, 300, 1524, 341, 12379, 13], "temperature": 0.0, "avg_logprob": -0.17946766872032016, "compression_ratio": 1.7608695652173914, "no_speech_prob": 1.805710962798912e-05}, {"id": 492, "seek": 197776, "start": 1977.76, "end": 1982.8, "text": " I, the navigation, the positive verb, and then the thing.", "tokens": [286, 11, 264, 17346, 11, 264, 3353, 9595, 11, 293, 550, 264, 551, 13], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 493, "seek": 197776, "start": 1982.8, "end": 1987.96, "text": " So if you negation positive verb, means you negative verb, right?", "tokens": [407, 498, 291, 2485, 399, 3353, 9595, 11, 1355, 291, 3671, 9595, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 494, "seek": 197776, "start": 1987.96, "end": 1988.96, "text": " And so here's an example.", "tokens": [400, 370, 510, 311, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 495, "seek": 197776, "start": 1988.96, "end": 1991.36, "text": " I can't say I recommend the food.", "tokens": [286, 393, 380, 584, 286, 2748, 264, 1755, 13], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 496, "seek": 197776, "start": 1991.36, "end": 1993.12, "text": " The expected label is negative.", "tokens": [440, 5176, 7645, 307, 3671, 13], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 497, "seek": 197776, "start": 1993.12, "end": 1997.76, "text": " The answer that the model provided, and this is, I think, a commercial sentiment analysis", "tokens": [440, 1867, 300, 264, 2316, 5649, 11, 293, 341, 307, 11, 286, 519, 11, 257, 6841, 16149, 5215], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 498, "seek": 197776, "start": 1997.76, "end": 1998.76, "text": " system.", "tokens": [1185, 13], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 499, "seek": 197776, "start": 1998.76, "end": 1999.76, "text": " I was positive.", "tokens": [286, 390, 3353, 13], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 500, "seek": 197776, "start": 1999.76, "end": 2001.08, "text": " So it's pretty positive.", "tokens": [407, 309, 311, 1238, 3353, 13], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 501, "seek": 197776, "start": 2001.08, "end": 2004.56, "text": " And then I didn't love the flight.", "tokens": [400, 550, 286, 994, 380, 959, 264, 7018, 13], "temperature": 0.0, "avg_logprob": -0.23676296642848424, "compression_ratio": 1.7443946188340806, "no_speech_prob": 3.0710296414326876e-05}, {"id": 502, "seek": 200456, "start": 2004.56, "end": 2010.04, "text": " The expected label was negative, and then the predicted answer was neutral.", "tokens": [440, 5176, 7645, 390, 3671, 11, 293, 550, 264, 19147, 1867, 390, 10598, 13], "temperature": 0.0, "avg_logprob": -0.2208615593288256, "compression_ratio": 1.5891472868217054, "no_speech_prob": 5.5613807489862666e-05}, {"id": 503, "seek": 200456, "start": 2010.04, "end": 2015.84, "text": " And this commercial sentiment analysis system gets a lot of, well, you could imagine are", "tokens": [400, 341, 6841, 16149, 5215, 1185, 2170, 257, 688, 295, 11, 731, 11, 291, 727, 3811, 366], "temperature": 0.0, "avg_logprob": -0.2208615593288256, "compression_ratio": 1.5891472868217054, "no_speech_prob": 5.5613807489862666e-05}, {"id": 504, "seek": 200456, "start": 2015.84, "end": 2018.56, "text": " pretty reasonably simple examples, wrong.", "tokens": [1238, 23551, 2199, 5110, 11, 2085, 13], "temperature": 0.0, "avg_logprob": -0.2208615593288256, "compression_ratio": 1.5891472868217054, "no_speech_prob": 5.5613807489862666e-05}, {"id": 505, "seek": 200456, "start": 2018.56, "end": 2024.56, "text": " And so what your bureau had all 2020 showed is that they could actually provide a system", "tokens": [400, 370, 437, 428, 35343, 632, 439, 4808, 4712, 307, 300, 436, 727, 767, 2893, 257, 1185], "temperature": 0.0, "avg_logprob": -0.2208615593288256, "compression_ratio": 1.5891472868217054, "no_speech_prob": 5.5613807489862666e-05}, {"id": 506, "seek": 200456, "start": 2024.56, "end": 2030.44, "text": " that sort of had this framework of building test cases for NLP models, two ML engineers", "tokens": [300, 1333, 295, 632, 341, 8388, 295, 2390, 1500, 3331, 337, 426, 45196, 5245, 11, 732, 21601, 11955], "temperature": 0.0, "avg_logprob": -0.2208615593288256, "compression_ratio": 1.5891472868217054, "no_speech_prob": 5.5613807489862666e-05}, {"id": 507, "seek": 200456, "start": 2030.44, "end": 2033.28, "text": " working on these products.", "tokens": [1364, 322, 613, 3383, 13], "temperature": 0.0, "avg_logprob": -0.2208615593288256, "compression_ratio": 1.5891472868217054, "no_speech_prob": 5.5613807489862666e-05}, {"id": 508, "seek": 203328, "start": 2033.28, "end": 2040.84, "text": " And given that interface, and they would actually find bugs, you know, bugs being categories", "tokens": [400, 2212, 300, 9226, 11, 293, 436, 576, 767, 915, 15120, 11, 291, 458, 11, 15120, 885, 10479], "temperature": 0.0, "avg_logprob": -0.12423287549065155, "compression_ratio": 1.6718146718146718, "no_speech_prob": 3.320726682431996e-05}, {"id": 509, "seek": 203328, "start": 2040.84, "end": 2042.16, "text": " of high error, right?", "tokens": [295, 1090, 6713, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12423287549065155, "compression_ratio": 1.6718146718146718, "no_speech_prob": 3.320726682431996e-05}, {"id": 510, "seek": 203328, "start": 2042.16, "end": 2046.48, "text": " Find bugs in their models that they could then kind of try to go and fix.", "tokens": [11809, 15120, 294, 641, 5245, 300, 436, 727, 550, 733, 295, 853, 281, 352, 293, 3191, 13], "temperature": 0.0, "avg_logprob": -0.12423287549065155, "compression_ratio": 1.6718146718146718, "no_speech_prob": 3.320726682431996e-05}, {"id": 511, "seek": 203328, "start": 2046.48, "end": 2050.24, "text": " And that this was kind of an efficient way of trying to find things that were simple", "tokens": [400, 300, 341, 390, 733, 295, 364, 7148, 636, 295, 1382, 281, 915, 721, 300, 645, 2199], "temperature": 0.0, "avg_logprob": -0.12423287549065155, "compression_ratio": 1.6718146718146718, "no_speech_prob": 3.320726682431996e-05}, {"id": 512, "seek": 203328, "start": 2050.24, "end": 2056.44, "text": " and still wrong with what should be pretty sophisticated neural systems.", "tokens": [293, 920, 2085, 365, 437, 820, 312, 1238, 16950, 18161, 3652, 13], "temperature": 0.0, "avg_logprob": -0.12423287549065155, "compression_ratio": 1.6718146718146718, "no_speech_prob": 3.320726682431996e-05}, {"id": 513, "seek": 203328, "start": 2056.44, "end": 2061.2799999999997, "text": " So I really like this, and it's sort of a nice way of thinking more specifically about", "tokens": [407, 286, 534, 411, 341, 11, 293, 309, 311, 1333, 295, 257, 1481, 636, 295, 1953, 544, 4682, 466], "temperature": 0.0, "avg_logprob": -0.12423287549065155, "compression_ratio": 1.6718146718146718, "no_speech_prob": 3.320726682431996e-05}, {"id": 514, "seek": 206128, "start": 2061.28, "end": 2067.4, "text": " what are the capabilities in sort of precise terms of our models.", "tokens": [437, 366, 264, 10862, 294, 1333, 295, 13600, 2115, 295, 527, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15167622626582278, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.6267303837812506e-05}, {"id": 515, "seek": 206128, "start": 2067.4, "end": 2073.4, "text": " And altogether, now you've seen problems in natural language inference.", "tokens": [400, 19051, 11, 586, 291, 600, 1612, 2740, 294, 3303, 2856, 38253, 13], "temperature": 0.0, "avg_logprob": -0.15167622626582278, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.6267303837812506e-05}, {"id": 516, "seek": 206128, "start": 2073.4, "end": 2077.48, "text": " You've seen language models actually perform pretty well at the language modeling objective.", "tokens": [509, 600, 1612, 2856, 5245, 767, 2042, 1238, 731, 412, 264, 2856, 15983, 10024, 13], "temperature": 0.0, "avg_logprob": -0.15167622626582278, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.6267303837812506e-05}, {"id": 517, "seek": 206128, "start": 2077.48, "end": 2082.44, "text": " But then you see, you just saw an example of a commercial sentiment analysis system", "tokens": [583, 550, 291, 536, 11, 291, 445, 1866, 364, 1365, 295, 257, 6841, 16149, 5215, 1185], "temperature": 0.0, "avg_logprob": -0.15167622626582278, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.6267303837812506e-05}, {"id": 518, "seek": 206128, "start": 2082.44, "end": 2085.28, "text": " that sort of should do better and doesn't.", "tokens": [300, 1333, 295, 820, 360, 1101, 293, 1177, 380, 13], "temperature": 0.0, "avg_logprob": -0.15167622626582278, "compression_ratio": 1.6153846153846154, "no_speech_prob": 2.6267303837812506e-05}, {"id": 519, "seek": 208528, "start": 2085.28, "end": 2092.4, "text": " And this comes with really, I think, broad and important takeaway, which is if you get", "tokens": [400, 341, 1487, 365, 534, 11, 286, 519, 11, 4152, 293, 1021, 30681, 11, 597, 307, 498, 291, 483], "temperature": 0.0, "avg_logprob": -0.15554253808383284, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.590724372770637e-05}, {"id": 520, "seek": 208528, "start": 2092.4, "end": 2098.92, "text": " high accuracy on the end domain test set, you are not guaranteed high accuracy on even", "tokens": [1090, 14170, 322, 264, 917, 9274, 1500, 992, 11, 291, 366, 406, 18031, 1090, 14170, 322, 754], "temperature": 0.0, "avg_logprob": -0.15554253808383284, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.590724372770637e-05}, {"id": 521, "seek": 208528, "start": 2098.92, "end": 2105.1200000000003, "text": " what you might consider to be reasonable out of domain evaluations.", "tokens": [437, 291, 1062, 1949, 281, 312, 10585, 484, 295, 9274, 43085, 13], "temperature": 0.0, "avg_logprob": -0.15554253808383284, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.590724372770637e-05}, {"id": 522, "seek": 208528, "start": 2105.1200000000003, "end": 2108.28, "text": " And life is always out of domain.", "tokens": [400, 993, 307, 1009, 484, 295, 9274, 13], "temperature": 0.0, "avg_logprob": -0.15554253808383284, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.590724372770637e-05}, {"id": 523, "seek": 208528, "start": 2108.28, "end": 2112.88, "text": " And if you're building a system that we have given to users, it's immediately out of", "tokens": [400, 498, 291, 434, 2390, 257, 1185, 300, 321, 362, 2212, 281, 5022, 11, 309, 311, 4258, 484, 295], "temperature": 0.0, "avg_logprob": -0.15554253808383284, "compression_ratio": 1.6363636363636365, "no_speech_prob": 3.590724372770637e-05}, {"id": 524, "seek": 211288, "start": 2112.88, "end": 2117.0, "text": " domain that the very least because it's trained on text that's now older than the things", "tokens": [9274, 300, 264, 588, 1935, 570, 309, 311, 8895, 322, 2487, 300, 311, 586, 4906, 813, 264, 721], "temperature": 0.0, "avg_logprob": -0.14359136962890626, "compression_ratio": 1.7317880794701987, "no_speech_prob": 2.7529191356734373e-05}, {"id": 525, "seek": 211288, "start": 2117.0, "end": 2118.32, "text": " that the users are now saying.", "tokens": [300, 264, 5022, 366, 586, 1566, 13], "temperature": 0.0, "avg_logprob": -0.14359136962890626, "compression_ratio": 1.7317880794701987, "no_speech_prob": 2.7529191356734373e-05}, {"id": 526, "seek": 211288, "start": 2118.32, "end": 2123.36, "text": " So it's a really, really important takeaway that your sort of benchmark accuracy is a", "tokens": [407, 309, 311, 257, 534, 11, 534, 1021, 30681, 300, 428, 1333, 295, 18927, 14170, 307, 257], "temperature": 0.0, "avg_logprob": -0.14359136962890626, "compression_ratio": 1.7317880794701987, "no_speech_prob": 2.7529191356734373e-05}, {"id": 527, "seek": 211288, "start": 2123.36, "end": 2128.2000000000003, "text": " single number that does not guarantee good performance on a wide variety of things.", "tokens": [2167, 1230, 300, 775, 406, 10815, 665, 3389, 322, 257, 4874, 5673, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.14359136962890626, "compression_ratio": 1.7317880794701987, "no_speech_prob": 2.7529191356734373e-05}, {"id": 528, "seek": 211288, "start": 2128.2000000000003, "end": 2132.2000000000003, "text": " And from a, what are our neural networks doing perspective?", "tokens": [400, 490, 257, 11, 437, 366, 527, 18161, 9590, 884, 4585, 30], "temperature": 0.0, "avg_logprob": -0.14359136962890626, "compression_ratio": 1.7317880794701987, "no_speech_prob": 2.7529191356734373e-05}, {"id": 529, "seek": 211288, "start": 2132.2000000000003, "end": 2136.48, "text": " One way to think about it is that models seem to be learning the data set, fitting sort", "tokens": [1485, 636, 281, 519, 466, 309, 307, 300, 5245, 1643, 281, 312, 2539, 264, 1412, 992, 11, 15669, 1333], "temperature": 0.0, "avg_logprob": -0.14359136962890626, "compression_ratio": 1.7317880794701987, "no_speech_prob": 2.7529191356734373e-05}, {"id": 530, "seek": 211288, "start": 2136.48, "end": 2142.1600000000003, "text": " of the fine-grained, sort of heuristics and statistics that help it fit this one data", "tokens": [295, 264, 2489, 12, 20735, 2001, 11, 1333, 295, 415, 374, 6006, 293, 12523, 300, 854, 309, 3318, 341, 472, 1412], "temperature": 0.0, "avg_logprob": -0.14359136962890626, "compression_ratio": 1.7317880794701987, "no_speech_prob": 2.7529191356734373e-05}, {"id": 531, "seek": 214216, "start": 2142.16, "end": 2144.6, "text": " set as opposed to learning the task.", "tokens": [992, 382, 8851, 281, 2539, 264, 5633, 13], "temperature": 0.0, "avg_logprob": -0.18124227675180588, "compression_ratio": 1.6620209059233448, "no_speech_prob": 4.5381213567452505e-05}, {"id": 532, "seek": 214216, "start": 2144.6, "end": 2148.52, "text": " So humans can perform natural language inference if you give them examples from whatever data", "tokens": [407, 6255, 393, 2042, 3303, 2856, 38253, 498, 291, 976, 552, 5110, 490, 2035, 1412], "temperature": 0.0, "avg_logprob": -0.18124227675180588, "compression_ratio": 1.6620209059233448, "no_speech_prob": 4.5381213567452505e-05}, {"id": 533, "seek": 214216, "start": 2148.52, "end": 2149.52, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.18124227675180588, "compression_ratio": 1.6620209059233448, "no_speech_prob": 4.5381213567452505e-05}, {"id": 534, "seek": 214216, "start": 2149.52, "end": 2154.2799999999997, "text": " You know, once you've told them how to do the task, they'll be very generally strong at", "tokens": [509, 458, 11, 1564, 291, 600, 1907, 552, 577, 281, 360, 264, 5633, 11, 436, 603, 312, 588, 5101, 2068, 412], "temperature": 0.0, "avg_logprob": -0.18124227675180588, "compression_ratio": 1.6620209059233448, "no_speech_prob": 4.5381213567452505e-05}, {"id": 535, "seek": 214216, "start": 2154.2799999999997, "end": 2155.2799999999997, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.18124227675180588, "compression_ratio": 1.6620209059233448, "no_speech_prob": 4.5381213567452505e-05}, {"id": 536, "seek": 214216, "start": 2155.2799999999997, "end": 2161.6, "text": " But you take your MNLI model and you test it on Hans and it got, you know, whatever that", "tokens": [583, 291, 747, 428, 376, 45, 48718, 2316, 293, 291, 1500, 309, 322, 17926, 293, 309, 658, 11, 291, 458, 11, 2035, 300], "temperature": 0.0, "avg_logprob": -0.18124227675180588, "compression_ratio": 1.6620209059233448, "no_speech_prob": 4.5381213567452505e-05}, {"id": 537, "seek": 214216, "start": 2161.6, "end": 2163.24, "text": " was, below chance accuracy.", "tokens": [390, 11, 2507, 2931, 14170, 13], "temperature": 0.0, "avg_logprob": -0.18124227675180588, "compression_ratio": 1.6620209059233448, "no_speech_prob": 4.5381213567452505e-05}, {"id": 538, "seek": 214216, "start": 2163.24, "end": 2165.04, "text": " That's not the kind of thing that you want to see.", "tokens": [663, 311, 406, 264, 733, 295, 551, 300, 291, 528, 281, 536, 13], "temperature": 0.0, "avg_logprob": -0.18124227675180588, "compression_ratio": 1.6620209059233448, "no_speech_prob": 4.5381213567452505e-05}, {"id": 539, "seek": 214216, "start": 2165.04, "end": 2170.8799999999997, "text": " So it definitely learns the data set well because the accuracy in domain is high.", "tokens": [407, 309, 2138, 27152, 264, 1412, 992, 731, 570, 264, 14170, 294, 9274, 307, 1090, 13], "temperature": 0.0, "avg_logprob": -0.18124227675180588, "compression_ratio": 1.6620209059233448, "no_speech_prob": 4.5381213567452505e-05}, {"id": 540, "seek": 217088, "start": 2170.88, "end": 2177.92, "text": " But our models are seemingly not frequently learning, sort of the mechanisms that we would", "tokens": [583, 527, 5245, 366, 18709, 406, 10374, 2539, 11, 1333, 295, 264, 15902, 300, 321, 576], "temperature": 0.0, "avg_logprob": -0.15407804566986708, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0001484474923927337}, {"id": 541, "seek": 217088, "start": 2177.92, "end": 2179.7200000000003, "text": " like them to be learning.", "tokens": [411, 552, 281, 312, 2539, 13], "temperature": 0.0, "avg_logprob": -0.15407804566986708, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0001484474923927337}, {"id": 542, "seek": 217088, "start": 2179.7200000000003, "end": 2183.84, "text": " Last week, we heard about language models and sort of the implicit knowledge that they", "tokens": [5264, 1243, 11, 321, 2198, 466, 2856, 5245, 293, 1333, 295, 264, 26947, 3601, 300, 436], "temperature": 0.0, "avg_logprob": -0.15407804566986708, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0001484474923927337}, {"id": 543, "seek": 217088, "start": 2183.84, "end": 2186.88, "text": " encode about the world through pre-training.", "tokens": [2058, 1429, 466, 264, 1002, 807, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.15407804566986708, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0001484474923927337}, {"id": 544, "seek": 217088, "start": 2186.88, "end": 2190.56, "text": " And one of the ways that we sought to interact with language models was providing them with", "tokens": [400, 472, 295, 264, 2098, 300, 321, 17532, 281, 4648, 365, 2856, 5245, 390, 6530, 552, 365], "temperature": 0.0, "avg_logprob": -0.15407804566986708, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0001484474923927337}, {"id": 545, "seek": 217088, "start": 2190.56, "end": 2196.96, "text": " a prompt like Dante was born in mask and then seeing if it puts high probability on the", "tokens": [257, 12391, 411, 35602, 390, 4232, 294, 6094, 293, 550, 2577, 498, 309, 8137, 1090, 8482, 322, 264], "temperature": 0.0, "avg_logprob": -0.15407804566986708, "compression_ratio": 1.7540983606557377, "no_speech_prob": 0.0001484474923927337}, {"id": 546, "seek": 219696, "start": 2196.96, "end": 2202.64, "text": " correct continuation, which requires you to access knowledge about where Dante was", "tokens": [3006, 29357, 11, 597, 7029, 291, 281, 2105, 3601, 466, 689, 35602, 390], "temperature": 0.0, "avg_logprob": -0.19348150836534736, "compression_ratio": 1.6813186813186813, "no_speech_prob": 5.1412844186415896e-05}, {"id": 547, "seek": 219696, "start": 2202.64, "end": 2203.64, "text": " born.", "tokens": [4232, 13], "temperature": 0.0, "avg_logprob": -0.19348150836534736, "compression_ratio": 1.6813186813186813, "no_speech_prob": 5.1412844186415896e-05}, {"id": 548, "seek": 219696, "start": 2203.64, "end": 2207.88, "text": " And we didn't frame it this way last week, but this fits into the set of behavioral studies", "tokens": [400, 321, 994, 380, 3920, 309, 341, 636, 1036, 1243, 11, 457, 341, 9001, 666, 264, 992, 295, 19124, 5313], "temperature": 0.0, "avg_logprob": -0.19348150836534736, "compression_ratio": 1.6813186813186813, "no_speech_prob": 5.1412844186415896e-05}, {"id": 549, "seek": 219696, "start": 2207.88, "end": 2209.52, "text": " that we've done so far.", "tokens": [300, 321, 600, 1096, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.19348150836534736, "compression_ratio": 1.6813186813186813, "no_speech_prob": 5.1412844186415896e-05}, {"id": 550, "seek": 219696, "start": 2209.52, "end": 2211.84, "text": " This is a specific kind of input.", "tokens": [639, 307, 257, 2685, 733, 295, 4846, 13], "temperature": 0.0, "avg_logprob": -0.19348150836534736, "compression_ratio": 1.6813186813186813, "no_speech_prob": 5.1412844186415896e-05}, {"id": 551, "seek": 219696, "start": 2211.84, "end": 2214.88, "text": " You could ask this for multiple types of multiple people.", "tokens": [509, 727, 1029, 341, 337, 3866, 3467, 295, 3866, 561, 13], "temperature": 0.0, "avg_logprob": -0.19348150836534736, "compression_ratio": 1.6813186813186813, "no_speech_prob": 5.1412844186415896e-05}, {"id": 552, "seek": 219696, "start": 2214.88, "end": 2216.84, "text": " You could swap out Dante for other people.", "tokens": [509, 727, 18135, 484, 35602, 337, 661, 561, 13], "temperature": 0.0, "avg_logprob": -0.19348150836534736, "compression_ratio": 1.6813186813186813, "no_speech_prob": 5.1412844186415896e-05}, {"id": 553, "seek": 219696, "start": 2216.84, "end": 2222.04, "text": " You could swap out born in for, I don't know, died in or something.", "tokens": [509, 727, 18135, 484, 4232, 294, 337, 11, 286, 500, 380, 458, 11, 4539, 294, 420, 746, 13], "temperature": 0.0, "avg_logprob": -0.19348150836534736, "compression_ratio": 1.6813186813186813, "no_speech_prob": 5.1412844186415896e-05}, {"id": 554, "seek": 219696, "start": 2222.04, "end": 2224.92, "text": " And then you can, there are like test suites again.", "tokens": [400, 550, 291, 393, 11, 456, 366, 411, 1500, 459, 3324, 797, 13], "temperature": 0.0, "avg_logprob": -0.19348150836534736, "compression_ratio": 1.6813186813186813, "no_speech_prob": 5.1412844186415896e-05}, {"id": 555, "seek": 222492, "start": 2224.92, "end": 2227.4, "text": " And so it's all connected.", "tokens": [400, 370, 309, 311, 439, 4582, 13], "temperature": 0.0, "avg_logprob": -0.1967902963811701, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.4737069250259083e-05}, {"id": 556, "seek": 222492, "start": 2227.4, "end": 2231.64, "text": " OK, so I won't go too deep into sort of the knowledge of language models in terms of", "tokens": [2264, 11, 370, 286, 1582, 380, 352, 886, 2452, 666, 1333, 295, 264, 3601, 295, 2856, 5245, 294, 2115, 295], "temperature": 0.0, "avg_logprob": -0.1967902963811701, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.4737069250259083e-05}, {"id": 557, "seek": 222492, "start": 2231.64, "end": 2234.84, "text": " world knowledge because we've gone over it some.", "tokens": [1002, 3601, 570, 321, 600, 2780, 670, 309, 512, 13], "temperature": 0.0, "avg_logprob": -0.1967902963811701, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.4737069250259083e-05}, {"id": 558, "seek": 222492, "start": 2234.84, "end": 2240.76, "text": " But when you're thinking about ways of interacting with your models, this sort of behavioral study", "tokens": [583, 562, 291, 434, 1953, 466, 2098, 295, 18017, 365, 428, 5245, 11, 341, 1333, 295, 19124, 2979], "temperature": 0.0, "avg_logprob": -0.1967902963811701, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.4737069250259083e-05}, {"id": 559, "seek": 222492, "start": 2240.76, "end": 2242.8, "text": " can be very, very general.", "tokens": [393, 312, 588, 11, 588, 2674, 13], "temperature": 0.0, "avg_logprob": -0.1967902963811701, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.4737069250259083e-05}, {"id": 560, "seek": 222492, "start": 2242.8, "end": 2247.6800000000003, "text": " Even though, remember, we're at still this highest level of abstraction where we're just", "tokens": [2754, 1673, 11, 1604, 11, 321, 434, 412, 920, 341, 6343, 1496, 295, 37765, 689, 321, 434, 445], "temperature": 0.0, "avg_logprob": -0.1967902963811701, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.4737069250259083e-05}, {"id": 561, "seek": 222492, "start": 2247.6800000000003, "end": 2250.7200000000003, "text": " looking at the probability distributions that are defined.", "tokens": [1237, 412, 264, 8482, 37870, 300, 366, 7642, 13], "temperature": 0.0, "avg_logprob": -0.1967902963811701, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.4737069250259083e-05}, {"id": 562, "seek": 222492, "start": 2250.7200000000003, "end": 2253.48, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1967902963811701, "compression_ratio": 1.6181818181818182, "no_speech_prob": 1.4737069250259083e-05}, {"id": 563, "seek": 225348, "start": 2253.48, "end": 2258.68, "text": " So now we'll go into, so we've sort of looked at understanding in fine grain areas what", "tokens": [407, 586, 321, 603, 352, 666, 11, 370, 321, 600, 1333, 295, 2956, 412, 3701, 294, 2489, 12837, 3179, 437], "temperature": 0.0, "avg_logprob": -0.14696161114439674, "compression_ratio": 1.6422764227642277, "no_speech_prob": 2.3914763005450368e-05}, {"id": 564, "seek": 225348, "start": 2258.68, "end": 2261.92, "text": " our model is actually doing.", "tokens": [527, 2316, 307, 767, 884, 13], "temperature": 0.0, "avg_logprob": -0.14696161114439674, "compression_ratio": 1.6422764227642277, "no_speech_prob": 2.3914763005450368e-05}, {"id": 565, "seek": 225348, "start": 2261.92, "end": 2268.12, "text": " What about sort of why for an individual input is it getting the answer right or wrong?", "tokens": [708, 466, 1333, 295, 983, 337, 364, 2609, 4846, 307, 309, 1242, 264, 1867, 558, 420, 2085, 30], "temperature": 0.0, "avg_logprob": -0.14696161114439674, "compression_ratio": 1.6422764227642277, "no_speech_prob": 2.3914763005450368e-05}, {"id": 566, "seek": 225348, "start": 2268.12, "end": 2272.92, "text": " And then are there changes to the inputs that look fine to humans, but actually make the", "tokens": [400, 550, 366, 456, 2962, 281, 264, 15743, 300, 574, 2489, 281, 6255, 11, 457, 767, 652, 264], "temperature": 0.0, "avg_logprob": -0.14696161114439674, "compression_ratio": 1.6422764227642277, "no_speech_prob": 2.3914763005450368e-05}, {"id": 567, "seek": 225348, "start": 2272.92, "end": 2275.96, "text": " models do a bad job?", "tokens": [5245, 360, 257, 1578, 1691, 30], "temperature": 0.0, "avg_logprob": -0.14696161114439674, "compression_ratio": 1.6422764227642277, "no_speech_prob": 2.3914763005450368e-05}, {"id": 568, "seek": 225348, "start": 2275.96, "end": 2282.28, "text": " So one study that I love to reference that really draws back into our original motivation", "tokens": [407, 472, 2979, 300, 286, 959, 281, 6408, 300, 534, 20045, 646, 666, 527, 3380, 12335], "temperature": 0.0, "avg_logprob": -0.14696161114439674, "compression_ratio": 1.6422764227642277, "no_speech_prob": 2.3914763005450368e-05}, {"id": 569, "seek": 228228, "start": 2282.28, "end": 2287.48, "text": " of using LSTM networks instead of simple recurrent neural networks was that they could use", "tokens": [295, 1228, 441, 6840, 44, 9590, 2602, 295, 2199, 18680, 1753, 18161, 9590, 390, 300, 436, 727, 764], "temperature": 0.0, "avg_logprob": -0.20989631925310406, "compression_ratio": 1.4455958549222798, "no_speech_prob": 2.626876266731415e-05}, {"id": 570, "seek": 228228, "start": 2287.48, "end": 2290.6000000000004, "text": " long context.", "tokens": [938, 4319, 13], "temperature": 0.0, "avg_logprob": -0.20989631925310406, "compression_ratio": 1.4455958549222798, "no_speech_prob": 2.626876266731415e-05}, {"id": 571, "seek": 228228, "start": 2290.6000000000004, "end": 2295.28, "text": " But like how long is your long short term memory?", "tokens": [583, 411, 577, 938, 307, 428, 938, 2099, 1433, 4675, 30], "temperature": 0.0, "avg_logprob": -0.20989631925310406, "compression_ratio": 1.4455958549222798, "no_speech_prob": 2.626876266731415e-05}, {"id": 572, "seek": 228228, "start": 2295.28, "end": 2303.96, "text": " And the idea of Kendall well at all 2018 was shuffle or remove contexts that are farther", "tokens": [400, 264, 1558, 295, 38794, 731, 412, 439, 6096, 390, 39426, 420, 4159, 30628, 300, 366, 20344], "temperature": 0.0, "avg_logprob": -0.20989631925310406, "compression_ratio": 1.4455958549222798, "no_speech_prob": 2.626876266731415e-05}, {"id": 573, "seek": 228228, "start": 2303.96, "end": 2309.52, "text": " than some k words away, changing k.", "tokens": [813, 512, 350, 2283, 1314, 11, 4473, 350, 13], "temperature": 0.0, "avg_logprob": -0.20989631925310406, "compression_ratio": 1.4455958549222798, "no_speech_prob": 2.626876266731415e-05}, {"id": 574, "seek": 230952, "start": 2309.52, "end": 2315.4, "text": " And if the accuracy, if the predictive ability of your language model, the perplexity,", "tokens": [400, 498, 264, 14170, 11, 498, 264, 35521, 3485, 295, 428, 2856, 2316, 11, 264, 680, 18945, 507, 11], "temperature": 0.0, "avg_logprob": -0.17533286143157442, "compression_ratio": 1.6653543307086613, "no_speech_prob": 4.263997107045725e-05}, {"id": 575, "seek": 230952, "start": 2315.4, "end": 2319.72, "text": " right, doesn't change once you do that, it means the model wasn't actually using that", "tokens": [558, 11, 1177, 380, 1319, 1564, 291, 360, 300, 11, 309, 1355, 264, 2316, 2067, 380, 767, 1228, 300], "temperature": 0.0, "avg_logprob": -0.17533286143157442, "compression_ratio": 1.6653543307086613, "no_speech_prob": 4.263997107045725e-05}, {"id": 576, "seek": 230952, "start": 2319.72, "end": 2320.72, "text": " context.", "tokens": [4319, 13], "temperature": 0.0, "avg_logprob": -0.17533286143157442, "compression_ratio": 1.6653543307086613, "no_speech_prob": 4.263997107045725e-05}, {"id": 577, "seek": 230952, "start": 2320.72, "end": 2322.24, "text": " I think this is so cool.", "tokens": [286, 519, 341, 307, 370, 1627, 13], "temperature": 0.0, "avg_logprob": -0.17533286143157442, "compression_ratio": 1.6653543307086613, "no_speech_prob": 4.263997107045725e-05}, {"id": 578, "seek": 230952, "start": 2322.24, "end": 2328.32, "text": " So on the x-axis, we've got how far away from the word that you're trying to predict,", "tokens": [407, 322, 264, 2031, 12, 24633, 11, 321, 600, 658, 577, 1400, 1314, 490, 264, 1349, 300, 291, 434, 1382, 281, 6069, 11], "temperature": 0.0, "avg_logprob": -0.17533286143157442, "compression_ratio": 1.6653543307086613, "no_speech_prob": 4.263997107045725e-05}, {"id": 579, "seek": 230952, "start": 2328.32, "end": 2334.24, "text": " are you actually sort of corrupting, shuffling, or moving stuff from the sequence.", "tokens": [366, 291, 767, 1333, 295, 17366, 278, 11, 402, 1245, 1688, 11, 420, 2684, 1507, 490, 264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.17533286143157442, "compression_ratio": 1.6653543307086613, "no_speech_prob": 4.263997107045725e-05}, {"id": 580, "seek": 230952, "start": 2334.24, "end": 2337.36, "text": " And then on the y-axis is the increase in loss.", "tokens": [400, 550, 322, 264, 288, 12, 24633, 307, 264, 3488, 294, 4470, 13], "temperature": 0.0, "avg_logprob": -0.17533286143157442, "compression_ratio": 1.6653543307086613, "no_speech_prob": 4.263997107045725e-05}, {"id": 581, "seek": 233736, "start": 2337.36, "end": 2343.44, "text": " So if the increase in loss is zero, it means that the model was not using the thing that", "tokens": [407, 498, 264, 3488, 294, 4470, 307, 4018, 11, 309, 1355, 300, 264, 2316, 390, 406, 1228, 264, 551, 300], "temperature": 0.0, "avg_logprob": -0.1277991703578404, "compression_ratio": 1.6717557251908397, "no_speech_prob": 1.6184032574528828e-05}, {"id": 582, "seek": 233736, "start": 2343.44, "end": 2348.28, "text": " you just removed because if it was using it, it would now do worse without it, right?", "tokens": [291, 445, 7261, 570, 498, 309, 390, 1228, 309, 11, 309, 576, 586, 360, 5324, 1553, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1277991703578404, "compression_ratio": 1.6717557251908397, "no_speech_prob": 1.6184032574528828e-05}, {"id": 583, "seek": 233736, "start": 2348.28, "end": 2353.76, "text": " And so if you shuffle in the blue line here, if you shuffle the history that's farther", "tokens": [400, 370, 498, 291, 39426, 294, 264, 3344, 1622, 510, 11, 498, 291, 39426, 264, 2503, 300, 311, 20344], "temperature": 0.0, "avg_logprob": -0.1277991703578404, "compression_ratio": 1.6717557251908397, "no_speech_prob": 1.6184032574528828e-05}, {"id": 584, "seek": 233736, "start": 2353.76, "end": 2358.7200000000003, "text": " away from 50 words, the model does not even notice.", "tokens": [1314, 490, 2625, 2283, 11, 264, 2316, 775, 406, 754, 3449, 13], "temperature": 0.0, "avg_logprob": -0.1277991703578404, "compression_ratio": 1.6717557251908397, "no_speech_prob": 1.6184032574528828e-05}, {"id": 585, "seek": 233736, "start": 2358.7200000000003, "end": 2360.2000000000003, "text": " I think that's really interesting.", "tokens": [286, 519, 300, 311, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1277991703578404, "compression_ratio": 1.6717557251908397, "no_speech_prob": 1.6184032574528828e-05}, {"id": 586, "seek": 233736, "start": 2360.2000000000003, "end": 2365.2400000000002, "text": " One it says, everything passed 50 words of this LSTM language model, you could have given", "tokens": [1485, 309, 1619, 11, 1203, 4678, 2625, 2283, 295, 341, 441, 6840, 44, 2856, 2316, 11, 291, 727, 362, 2212], "temperature": 0.0, "avg_logprob": -0.1277991703578404, "compression_ratio": 1.6717557251908397, "no_speech_prob": 1.6184032574528828e-05}, {"id": 587, "seek": 236524, "start": 2365.24, "end": 2368.64, "text": " it in random order and it wouldn't have noticed.", "tokens": [309, 294, 4974, 1668, 293, 309, 2759, 380, 362, 5694, 13], "temperature": 0.0, "avg_logprob": -0.11666291196581344, "compression_ratio": 1.8159722222222223, "no_speech_prob": 3.3207023079739884e-05}, {"id": 588, "seek": 236524, "start": 2368.64, "end": 2372.4399999999996, "text": " And then two it says that if you're closer than that, it actually is making use of the", "tokens": [400, 550, 732, 309, 1619, 300, 498, 291, 434, 4966, 813, 300, 11, 309, 767, 307, 1455, 764, 295, 264], "temperature": 0.0, "avg_logprob": -0.11666291196581344, "compression_ratio": 1.8159722222222223, "no_speech_prob": 3.3207023079739884e-05}, {"id": 589, "seek": 236524, "start": 2372.4399999999996, "end": 2373.4399999999996, "text": " word order.", "tokens": [1349, 1668, 13], "temperature": 0.0, "avg_logprob": -0.11666291196581344, "compression_ratio": 1.8159722222222223, "no_speech_prob": 3.3207023079739884e-05}, {"id": 590, "seek": 236524, "start": 2373.4399999999996, "end": 2376.8399999999997, "text": " That's a pretty long memory, okay, that's really interesting.", "tokens": [663, 311, 257, 1238, 938, 4675, 11, 1392, 11, 300, 311, 534, 1880, 13], "temperature": 0.0, "avg_logprob": -0.11666291196581344, "compression_ratio": 1.8159722222222223, "no_speech_prob": 3.3207023079739884e-05}, {"id": 591, "seek": 236524, "start": 2376.8399999999997, "end": 2382.4799999999996, "text": " And then if you actually remove the words entirely, you can kind of notice that the words", "tokens": [400, 550, 498, 291, 767, 4159, 264, 2283, 7696, 11, 291, 393, 733, 295, 3449, 300, 264, 2283], "temperature": 0.0, "avg_logprob": -0.11666291196581344, "compression_ratio": 1.8159722222222223, "no_speech_prob": 3.3207023079739884e-05}, {"id": 592, "seek": 236524, "start": 2382.4799999999996, "end": 2385.7599999999998, "text": " are missing up to 200 words away.", "tokens": [366, 5361, 493, 281, 2331, 2283, 1314, 13], "temperature": 0.0, "avg_logprob": -0.11666291196581344, "compression_ratio": 1.8159722222222223, "no_speech_prob": 3.3207023079739884e-05}, {"id": 593, "seek": 236524, "start": 2385.7599999999998, "end": 2388.6, "text": " So you don't know the order that you don't care about the order they're in, but you", "tokens": [407, 291, 500, 380, 458, 264, 1668, 300, 291, 500, 380, 1127, 466, 264, 1668, 436, 434, 294, 11, 457, 291], "temperature": 0.0, "avg_logprob": -0.11666291196581344, "compression_ratio": 1.8159722222222223, "no_speech_prob": 3.3207023079739884e-05}, {"id": 594, "seek": 236524, "start": 2388.6, "end": 2390.64, "text": " care whether they're there or not.", "tokens": [1127, 1968, 436, 434, 456, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.11666291196581344, "compression_ratio": 1.8159722222222223, "no_speech_prob": 3.3207023079739884e-05}, {"id": 595, "seek": 236524, "start": 2390.64, "end": 2394.8799999999997, "text": " And so this is an evaluation of, well, do LSTMs have long term memory?", "tokens": [400, 370, 341, 307, 364, 13344, 295, 11, 731, 11, 360, 441, 6840, 26386, 362, 938, 1433, 4675, 30], "temperature": 0.0, "avg_logprob": -0.11666291196581344, "compression_ratio": 1.8159722222222223, "no_speech_prob": 3.3207023079739884e-05}, {"id": 596, "seek": 239488, "start": 2394.88, "end": 2401.08, "text": " Well, this one at least has effectively no longer than 200 words of memory, but also", "tokens": [1042, 11, 341, 472, 412, 1935, 575, 8659, 572, 2854, 813, 2331, 2283, 295, 4675, 11, 457, 611], "temperature": 0.0, "avg_logprob": -0.14036816157651752, "compression_ratio": 1.5660377358490567, "no_speech_prob": 6.853400918771513e-06}, {"id": 597, "seek": 239488, "start": 2401.08, "end": 2402.28, "text": " no less.", "tokens": [572, 1570, 13], "temperature": 0.0, "avg_logprob": -0.14036816157651752, "compression_ratio": 1.5660377358490567, "no_speech_prob": 6.853400918771513e-06}, {"id": 598, "seek": 239488, "start": 2402.28, "end": 2407.0, "text": " So very cool.", "tokens": [407, 588, 1627, 13], "temperature": 0.0, "avg_logprob": -0.14036816157651752, "compression_ratio": 1.5660377358490567, "no_speech_prob": 6.853400918771513e-06}, {"id": 599, "seek": 239488, "start": 2407.0, "end": 2409.48, "text": " So that's like a general study for a single model.", "tokens": [407, 300, 311, 411, 257, 2674, 2979, 337, 257, 2167, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14036816157651752, "compression_ratio": 1.5660377358490567, "no_speech_prob": 6.853400918771513e-06}, {"id": 600, "seek": 239488, "start": 2409.48, "end": 2415.12, "text": " It talks about, it's sort of average behavior over a wide range of examples, but we want", "tokens": [467, 6686, 466, 11, 309, 311, 1333, 295, 4274, 5223, 670, 257, 4874, 3613, 295, 5110, 11, 457, 321, 528], "temperature": 0.0, "avg_logprob": -0.14036816157651752, "compression_ratio": 1.5660377358490567, "no_speech_prob": 6.853400918771513e-06}, {"id": 601, "seek": 239488, "start": 2415.12, "end": 2418.08, "text": " to talk about individual predictions on individual inputs.", "tokens": [281, 751, 466, 2609, 21264, 322, 2609, 15743, 13], "temperature": 0.0, "avg_logprob": -0.14036816157651752, "compression_ratio": 1.5660377358490567, "no_speech_prob": 6.853400918771513e-06}, {"id": 602, "seek": 239488, "start": 2418.08, "end": 2419.48, "text": " So let's talk about that.", "tokens": [407, 718, 311, 751, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.14036816157651752, "compression_ratio": 1.5660377358490567, "no_speech_prob": 6.853400918771513e-06}, {"id": 603, "seek": 241948, "start": 2419.48, "end": 2426.68, "text": " So one way of interpreting why did my model make this decision, that's very popular, is", "tokens": [407, 472, 636, 295, 37395, 983, 630, 452, 2316, 652, 341, 3537, 11, 300, 311, 588, 3743, 11, 307], "temperature": 0.0, "avg_logprob": -0.18666522302360178, "compression_ratio": 1.6546184738955823, "no_speech_prob": 1.5935107512632385e-05}, {"id": 604, "seek": 241948, "start": 2426.68, "end": 2431.56, "text": " for a single example, what parts of the input actually led to the decision?", "tokens": [337, 257, 2167, 1365, 11, 437, 3166, 295, 264, 4846, 767, 4684, 281, 264, 3537, 30], "temperature": 0.0, "avg_logprob": -0.18666522302360178, "compression_ratio": 1.6546184738955823, "no_speech_prob": 1.5935107512632385e-05}, {"id": 605, "seek": 241948, "start": 2431.56, "end": 2434.48, "text": " And this is where we come in with saliency maps.", "tokens": [400, 341, 307, 689, 321, 808, 294, 365, 1845, 7848, 11317, 13], "temperature": 0.0, "avg_logprob": -0.18666522302360178, "compression_ratio": 1.6546184738955823, "no_speech_prob": 1.5935107512632385e-05}, {"id": 606, "seek": 241948, "start": 2434.48, "end": 2440.12, "text": " So saliency map provides a score for each word indicating its importance to the model's", "tokens": [407, 1845, 7848, 4471, 6417, 257, 6175, 337, 1184, 1349, 25604, 1080, 7379, 281, 264, 2316, 311], "temperature": 0.0, "avg_logprob": -0.18666522302360178, "compression_ratio": 1.6546184738955823, "no_speech_prob": 1.5935107512632385e-05}, {"id": 607, "seek": 241948, "start": 2440.12, "end": 2441.12, "text": " prediction.", "tokens": [17630, 13], "temperature": 0.0, "avg_logprob": -0.18666522302360178, "compression_ratio": 1.6546184738955823, "no_speech_prob": 1.5935107512632385e-05}, {"id": 608, "seek": 241948, "start": 2441.12, "end": 2444.16, "text": " So you've got something like Bert here.", "tokens": [407, 291, 600, 658, 746, 411, 29594, 510, 13], "temperature": 0.0, "avg_logprob": -0.18666522302360178, "compression_ratio": 1.6546184738955823, "no_speech_prob": 1.5935107512632385e-05}, {"id": 609, "seek": 241948, "start": 2444.16, "end": 2445.16, "text": " You've got Bert.", "tokens": [509, 600, 658, 29594, 13], "temperature": 0.0, "avg_logprob": -0.18666522302360178, "compression_ratio": 1.6546184738955823, "no_speech_prob": 1.5935107512632385e-05}, {"id": 610, "seek": 241948, "start": 2445.16, "end": 2447.64, "text": " Bert is making a prediction for this mask.", "tokens": [29594, 307, 1455, 257, 17630, 337, 341, 6094, 13], "temperature": 0.0, "avg_logprob": -0.18666522302360178, "compression_ratio": 1.6546184738955823, "no_speech_prob": 1.5935107512632385e-05}, {"id": 611, "seek": 244764, "start": 2447.64, "end": 2452.48, "text": " And a mask rush to the emergency room to see her patient.", "tokens": [400, 257, 6094, 9300, 281, 264, 7473, 1808, 281, 536, 720, 4537, 13], "temperature": 0.0, "avg_logprob": -0.22380449314310094, "compression_ratio": 1.6754385964912282, "no_speech_prob": 7.180753527791239e-06}, {"id": 612, "seek": 244764, "start": 2452.48, "end": 2458.2, "text": " And the predictions that the model is making is, thanks with 47%, it's going to be nurse", "tokens": [400, 264, 21264, 300, 264, 2316, 307, 1455, 307, 11, 3231, 365, 16953, 8923, 309, 311, 516, 281, 312, 14012], "temperature": 0.0, "avg_logprob": -0.22380449314310094, "compression_ratio": 1.6754385964912282, "no_speech_prob": 7.180753527791239e-06}, {"id": 613, "seek": 244764, "start": 2458.2, "end": 2464.68, "text": " that's here in the mask instead, or maybe woman, or doctor, or mother, or girl.", "tokens": [300, 311, 510, 294, 264, 6094, 2602, 11, 420, 1310, 3059, 11, 420, 4631, 11, 420, 2895, 11, 420, 2013, 13], "temperature": 0.0, "avg_logprob": -0.22380449314310094, "compression_ratio": 1.6754385964912282, "no_speech_prob": 7.180753527791239e-06}, {"id": 614, "seek": 244764, "start": 2464.68, "end": 2468.08, "text": " And then the saliency map is being visualized here in orange.", "tokens": [400, 550, 264, 1845, 7848, 4471, 307, 885, 5056, 1602, 510, 294, 7671, 13], "temperature": 0.0, "avg_logprob": -0.22380449314310094, "compression_ratio": 1.6754385964912282, "no_speech_prob": 7.180753527791239e-06}, {"id": 615, "seek": 244764, "start": 2468.08, "end": 2473.12, "text": " According to this method of saliency called simple gradients, which we'll get into, emergency", "tokens": [7328, 281, 341, 3170, 295, 1845, 7848, 1219, 2199, 2771, 2448, 11, 597, 321, 603, 483, 666, 11, 7473], "temperature": 0.0, "avg_logprob": -0.22380449314310094, "compression_ratio": 1.6754385964912282, "no_speech_prob": 7.180753527791239e-06}, {"id": 616, "seek": 247312, "start": 2473.12, "end": 2478.7599999999998, "text": " her and the septokin, it's not worried about the septokin for now, but the emergency and", "tokens": [720, 293, 264, 23891, 453, 259, 11, 309, 311, 406, 5804, 466, 264, 23891, 453, 259, 337, 586, 11, 457, 264, 7473, 293], "temperature": 0.0, "avg_logprob": -0.2101362705230713, "compression_ratio": 1.7890625, "no_speech_prob": 2.8402486350387335e-05}, {"id": 617, "seek": 247312, "start": 2478.7599999999998, "end": 2481.88, "text": " her are the important words apparently.", "tokens": [720, 366, 264, 1021, 2283, 7970, 13], "temperature": 0.0, "avg_logprob": -0.2101362705230713, "compression_ratio": 1.7890625, "no_speech_prob": 2.8402486350387335e-05}, {"id": 618, "seek": 247312, "start": 2481.88, "end": 2485.88, "text": " And the septokin shows up in every sentence, so I'm not going to, yeah.", "tokens": [400, 264, 23891, 453, 259, 3110, 493, 294, 633, 8174, 11, 370, 286, 478, 406, 516, 281, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2101362705230713, "compression_ratio": 1.7890625, "no_speech_prob": 2.8402486350387335e-05}, {"id": 619, "seek": 247312, "start": 2485.88, "end": 2490.48, "text": " And so these two together are, according to this method, what's important for the model", "tokens": [400, 370, 613, 732, 1214, 366, 11, 4650, 281, 341, 3170, 11, 437, 311, 1021, 337, 264, 2316], "temperature": 0.0, "avg_logprob": -0.2101362705230713, "compression_ratio": 1.7890625, "no_speech_prob": 2.8402486350387335e-05}, {"id": 620, "seek": 247312, "start": 2490.48, "end": 2493.48, "text": " to make this prediction to mask.", "tokens": [281, 652, 341, 17630, 281, 6094, 13], "temperature": 0.0, "avg_logprob": -0.2101362705230713, "compression_ratio": 1.7890625, "no_speech_prob": 2.8402486350387335e-05}, {"id": 621, "seek": 247312, "start": 2493.48, "end": 2498.92, "text": " And you can see maybe some statistics, biases, etc., that is picked up in the predictions", "tokens": [400, 291, 393, 536, 1310, 512, 12523, 11, 32152, 11, 5183, 7933, 300, 307, 6183, 493, 294, 264, 21264], "temperature": 0.0, "avg_logprob": -0.2101362705230713, "compression_ratio": 1.7890625, "no_speech_prob": 2.8402486350387335e-05}, {"id": 622, "seek": 247312, "start": 2498.92, "end": 2501.92, "text": " and then have it mapped out onto the sentence.", "tokens": [293, 550, 362, 309, 33318, 484, 3911, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.2101362705230713, "compression_ratio": 1.7890625, "no_speech_prob": 2.8402486350387335e-05}, {"id": 623, "seek": 250192, "start": 2501.92, "end": 2507.36, "text": " And this is, well, it seems like it's really helping interpretability.", "tokens": [400, 341, 307, 11, 731, 11, 309, 2544, 411, 309, 311, 534, 4315, 7302, 2310, 13], "temperature": 0.0, "avg_logprob": -0.19096702036231455, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5442483345395885e-05}, {"id": 624, "seek": 250192, "start": 2507.36, "end": 2512.16, "text": " And yeah, I think that this is sort of a very useful tool.", "tokens": [400, 1338, 11, 286, 519, 300, 341, 307, 1333, 295, 257, 588, 4420, 2290, 13], "temperature": 0.0, "avg_logprob": -0.19096702036231455, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5442483345395885e-05}, {"id": 625, "seek": 250192, "start": 2512.16, "end": 2520.0, "text": " And actually, this is part of a demo from Alan and LP that allows you to do this yourself", "tokens": [400, 767, 11, 341, 307, 644, 295, 257, 10723, 490, 16442, 293, 38095, 300, 4045, 291, 281, 360, 341, 1803], "temperature": 0.0, "avg_logprob": -0.19096702036231455, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5442483345395885e-05}, {"id": 626, "seek": 250192, "start": 2520.0, "end": 2522.7200000000003, "text": " for any sentence that you want.", "tokens": [337, 604, 8174, 300, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.19096702036231455, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5442483345395885e-05}, {"id": 627, "seek": 250192, "start": 2522.7200000000003, "end": 2525.6800000000003, "text": " So what's this way of making saliency maps?", "tokens": [407, 437, 311, 341, 636, 295, 1455, 1845, 7848, 11317, 30], "temperature": 0.0, "avg_logprob": -0.19096702036231455, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5442483345395885e-05}, {"id": 628, "seek": 250192, "start": 2525.6800000000003, "end": 2527.76, "text": " We're not going to go, there's so many ways to do it.", "tokens": [492, 434, 406, 516, 281, 352, 11, 456, 311, 370, 867, 2098, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.19096702036231455, "compression_ratio": 1.579185520361991, "no_speech_prob": 1.5442483345395885e-05}, {"id": 629, "seek": 252776, "start": 2527.76, "end": 2532.76, "text": " We're going to take a very simple one and work through why it sort of makes sense.", "tokens": [492, 434, 516, 281, 747, 257, 588, 2199, 472, 293, 589, 807, 983, 309, 1333, 295, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 630, "seek": 252776, "start": 2532.76, "end": 2537.48, "text": " So the sort of issue is how do you define importance?", "tokens": [407, 264, 1333, 295, 2734, 307, 577, 360, 291, 6964, 7379, 30], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 631, "seek": 252776, "start": 2537.48, "end": 2540.84, "text": " What does it mean to be important to the model's prediction?", "tokens": [708, 775, 309, 914, 281, 312, 1021, 281, 264, 2316, 311, 17630, 30], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 632, "seek": 252776, "start": 2540.84, "end": 2542.36, "text": " And here's one way of thinking about it.", "tokens": [400, 510, 311, 472, 636, 295, 1953, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 633, "seek": 252776, "start": 2542.36, "end": 2543.84, "text": " It's called the simple gradient method.", "tokens": [467, 311, 1219, 264, 2199, 16235, 3170, 13], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 634, "seek": 252776, "start": 2543.84, "end": 2545.32, "text": " It's got a little formula.", "tokens": [467, 311, 658, 257, 707, 8513, 13], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 635, "seek": 252776, "start": 2545.32, "end": 2547.36, "text": " You got words x1 to xn.", "tokens": [509, 658, 2283, 2031, 16, 281, 2031, 77, 13], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 636, "seek": 252776, "start": 2547.36, "end": 2548.36, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 637, "seek": 252776, "start": 2548.36, "end": 2551.0, "text": " And then you got a model score for a given output class.", "tokens": [400, 550, 291, 658, 257, 2316, 6175, 337, 257, 2212, 5598, 1508, 13], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 638, "seek": 252776, "start": 2551.0, "end": 2556.0, "text": " So maybe you've got, in the birth example, each output class was each output word that", "tokens": [407, 1310, 291, 600, 658, 11, 294, 264, 3965, 1365, 11, 1184, 5598, 1508, 390, 1184, 5598, 1349, 300], "temperature": 0.0, "avg_logprob": -0.21200713826649226, "compression_ratio": 1.6631944444444444, "no_speech_prob": 3.1687741284258664e-05}, {"id": 639, "seek": 255600, "start": 2556.0, "end": 2558.76, "text": " you could possibly predict.", "tokens": [291, 727, 6264, 6069, 13], "temperature": 0.0, "avg_logprob": -0.24618411320512013, "compression_ratio": 1.7425742574257426, "no_speech_prob": 2.54622136708349e-05}, {"id": 640, "seek": 255600, "start": 2558.76, "end": 2563.76, "text": " And then you take the norm of the gradient of the score with respect to each word.", "tokens": [400, 550, 291, 747, 264, 2026, 295, 264, 16235, 295, 264, 6175, 365, 3104, 281, 1184, 1349, 13], "temperature": 0.0, "avg_logprob": -0.24618411320512013, "compression_ratio": 1.7425742574257426, "no_speech_prob": 2.54622136708349e-05}, {"id": 641, "seek": 255600, "start": 2563.76, "end": 2573.0, "text": " Okay, so what we're saying here is the score is sort of the unnormalized probability for", "tokens": [1033, 11, 370, 437, 321, 434, 1566, 510, 307, 264, 6175, 307, 1333, 295, 264, 517, 23157, 1602, 8482, 337], "temperature": 0.0, "avg_logprob": -0.24618411320512013, "compression_ratio": 1.7425742574257426, "no_speech_prob": 2.54622136708349e-05}, {"id": 642, "seek": 255600, "start": 2573.0, "end": 2575.2, "text": " that class.", "tokens": [300, 1508, 13], "temperature": 0.0, "avg_logprob": -0.24618411320512013, "compression_ratio": 1.7425742574257426, "no_speech_prob": 2.54622136708349e-05}, {"id": 643, "seek": 255600, "start": 2575.2, "end": 2576.6, "text": " Okay, so you got a single class.", "tokens": [1033, 11, 370, 291, 658, 257, 2167, 1508, 13], "temperature": 0.0, "avg_logprob": -0.24618411320512013, "compression_ratio": 1.7425742574257426, "no_speech_prob": 2.54622136708349e-05}, {"id": 644, "seek": 255600, "start": 2576.6, "end": 2580.88, "text": " You're taking the scores like how likely it is not yet normalized by how likely everything", "tokens": [509, 434, 1940, 264, 13444, 411, 577, 3700, 309, 307, 406, 1939, 48704, 538, 577, 3700, 1203], "temperature": 0.0, "avg_logprob": -0.24618411320512013, "compression_ratio": 1.7425742574257426, "no_speech_prob": 2.54622136708349e-05}, {"id": 645, "seek": 255600, "start": 2580.88, "end": 2582.68, "text": " else is sort of.", "tokens": [1646, 307, 1333, 295, 13], "temperature": 0.0, "avg_logprob": -0.24618411320512013, "compression_ratio": 1.7425742574257426, "no_speech_prob": 2.54622136708349e-05}, {"id": 646, "seek": 258268, "start": 2582.68, "end": 2587.44, "text": " So gradient, how much is it going to change if I move it a little bit in one direction", "tokens": [407, 16235, 11, 577, 709, 307, 309, 516, 281, 1319, 498, 286, 1286, 309, 257, 707, 857, 294, 472, 3513], "temperature": 0.0, "avg_logprob": -0.16572404817770456, "compression_ratio": 1.811023622047244, "no_speech_prob": 2.840691558958497e-05}, {"id": 647, "seek": 258268, "start": 2587.44, "end": 2591.0, "text": " or another, and then you take the norm to get a scalar from a vector.", "tokens": [420, 1071, 11, 293, 550, 291, 747, 264, 2026, 281, 483, 257, 39684, 490, 257, 8062, 13], "temperature": 0.0, "avg_logprob": -0.16572404817770456, "compression_ratio": 1.811023622047244, "no_speech_prob": 2.840691558958497e-05}, {"id": 648, "seek": 258268, "start": 2591.0, "end": 2592.0, "text": " So it looks like this.", "tokens": [407, 309, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.16572404817770456, "compression_ratio": 1.811023622047244, "no_speech_prob": 2.840691558958497e-05}, {"id": 649, "seek": 258268, "start": 2592.0, "end": 2597.68, "text": " So salience of word i, you have the norm bars on the outside, gradient with respect to", "tokens": [407, 1845, 1182, 295, 1349, 741, 11, 291, 362, 264, 2026, 10228, 322, 264, 2380, 11, 16235, 365, 3104, 281], "temperature": 0.0, "avg_logprob": -0.16572404817770456, "compression_ratio": 1.811023622047244, "no_speech_prob": 2.840691558958497e-05}, {"id": 650, "seek": 258268, "start": 2597.68, "end": 2599.0, "text": " xi.", "tokens": [2031, 72, 13], "temperature": 0.0, "avg_logprob": -0.16572404817770456, "compression_ratio": 1.811023622047244, "no_speech_prob": 2.840691558958497e-05}, {"id": 651, "seek": 258268, "start": 2599.0, "end": 2605.64, "text": " So that's if I change a little bit locally xi, how much does my score change?", "tokens": [407, 300, 311, 498, 286, 1319, 257, 707, 857, 16143, 2031, 72, 11, 577, 709, 775, 452, 6175, 1319, 30], "temperature": 0.0, "avg_logprob": -0.16572404817770456, "compression_ratio": 1.811023622047244, "no_speech_prob": 2.840691558958497e-05}, {"id": 652, "seek": 258268, "start": 2605.64, "end": 2610.52, "text": " So the idea is that a high gradient norm means that if I were to change it locally, I'd", "tokens": [407, 264, 1558, 307, 300, 257, 1090, 16235, 2026, 1355, 300, 498, 286, 645, 281, 1319, 309, 16143, 11, 286, 1116], "temperature": 0.0, "avg_logprob": -0.16572404817770456, "compression_ratio": 1.811023622047244, "no_speech_prob": 2.840691558958497e-05}, {"id": 653, "seek": 258268, "start": 2610.52, "end": 2612.2799999999997, "text": " affect the score a lot.", "tokens": [3345, 264, 6175, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.16572404817770456, "compression_ratio": 1.811023622047244, "no_speech_prob": 2.840691558958497e-05}, {"id": 654, "seek": 261228, "start": 2612.28, "end": 2614.4, "text": " That means it was very important to the decision.", "tokens": [663, 1355, 309, 390, 588, 1021, 281, 264, 3537, 13], "temperature": 0.0, "avg_logprob": -0.18020619844135485, "compression_ratio": 1.7016806722689075, "no_speech_prob": 8.267425073427148e-06}, {"id": 655, "seek": 261228, "start": 2614.4, "end": 2615.92, "text": " Let's visualize this a little bit.", "tokens": [961, 311, 23273, 341, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.18020619844135485, "compression_ratio": 1.7016806722689075, "no_speech_prob": 8.267425073427148e-06}, {"id": 656, "seek": 261228, "start": 2615.92, "end": 2619.6800000000003, "text": " So here on the y axis we've got loss.", "tokens": [407, 510, 322, 264, 288, 10298, 321, 600, 658, 4470, 13], "temperature": 0.0, "avg_logprob": -0.18020619844135485, "compression_ratio": 1.7016806722689075, "no_speech_prob": 8.267425073427148e-06}, {"id": 657, "seek": 261228, "start": 2619.6800000000003, "end": 2623.36, "text": " Just the loss of the model, sorry, this should be score.", "tokens": [1449, 264, 4470, 295, 264, 2316, 11, 2597, 11, 341, 820, 312, 6175, 13], "temperature": 0.0, "avg_logprob": -0.18020619844135485, "compression_ratio": 1.7016806722689075, "no_speech_prob": 8.267425073427148e-06}, {"id": 658, "seek": 261228, "start": 2623.36, "end": 2624.36, "text": " It should be score.", "tokens": [467, 820, 312, 6175, 13], "temperature": 0.0, "avg_logprob": -0.18020619844135485, "compression_ratio": 1.7016806722689075, "no_speech_prob": 8.267425073427148e-06}, {"id": 659, "seek": 261228, "start": 2624.36, "end": 2627.0400000000004, "text": " And on the x axis you've got word space.", "tokens": [400, 322, 264, 2031, 10298, 291, 600, 658, 1349, 1901, 13], "temperature": 0.0, "avg_logprob": -0.18020619844135485, "compression_ratio": 1.7016806722689075, "no_speech_prob": 8.267425073427148e-06}, {"id": 660, "seek": 261228, "start": 2627.0400000000004, "end": 2633.32, "text": " The word space is like sort of a flattening of the ability to move your word embedding", "tokens": [440, 1349, 1901, 307, 411, 1333, 295, 257, 24183, 278, 295, 264, 3485, 281, 1286, 428, 1349, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.18020619844135485, "compression_ratio": 1.7016806722689075, "no_speech_prob": 8.267425073427148e-06}, {"id": 661, "seek": 261228, "start": 2633.32, "end": 2634.8, "text": " in thousand dimensional space.", "tokens": [294, 4714, 18795, 1901, 13], "temperature": 0.0, "avg_logprob": -0.18020619844135485, "compression_ratio": 1.7016806722689075, "no_speech_prob": 8.267425073427148e-06}, {"id": 662, "seek": 261228, "start": 2634.8, "end": 2638.92, "text": " So I've just plotted it here in one dimension.", "tokens": [407, 286, 600, 445, 43288, 309, 510, 294, 472, 10139, 13], "temperature": 0.0, "avg_logprob": -0.18020619844135485, "compression_ratio": 1.7016806722689075, "no_speech_prob": 8.267425073427148e-06}, {"id": 663, "seek": 263892, "start": 2638.92, "end": 2644.36, "text": " Now a high saliency thing, you can see that the relationship between what should be score", "tokens": [823, 257, 1090, 1845, 7848, 551, 11, 291, 393, 536, 300, 264, 2480, 1296, 437, 820, 312, 6175], "temperature": 0.0, "avg_logprob": -0.17120475099797836, "compression_ratio": 1.790513833992095, "no_speech_prob": 8.529606020601932e-06}, {"id": 664, "seek": 263892, "start": 2644.36, "end": 2649.76, "text": " and moving the word in word space, you move it a little bit on the x axis and the score", "tokens": [293, 2684, 264, 1349, 294, 1349, 1901, 11, 291, 1286, 309, 257, 707, 857, 322, 264, 2031, 10298, 293, 264, 6175], "temperature": 0.0, "avg_logprob": -0.17120475099797836, "compression_ratio": 1.790513833992095, "no_speech_prob": 8.529606020601932e-06}, {"id": 665, "seek": 263892, "start": 2649.76, "end": 2650.76, "text": " changes a lot.", "tokens": [2962, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.17120475099797836, "compression_ratio": 1.790513833992095, "no_speech_prob": 8.529606020601932e-06}, {"id": 666, "seek": 263892, "start": 2650.76, "end": 2653.6800000000003, "text": " That's that derivative, that's the gradient, awesome, love it.", "tokens": [663, 311, 300, 13760, 11, 300, 311, 264, 16235, 11, 3476, 11, 959, 309, 13], "temperature": 0.0, "avg_logprob": -0.17120475099797836, "compression_ratio": 1.790513833992095, "no_speech_prob": 8.529606020601932e-06}, {"id": 667, "seek": 263892, "start": 2653.6800000000003, "end": 2660.44, "text": " Low saliency, you move the word around locally and the score doesn't change.", "tokens": [17078, 1845, 7848, 11, 291, 1286, 264, 1349, 926, 16143, 293, 264, 6175, 1177, 380, 1319, 13], "temperature": 0.0, "avg_logprob": -0.17120475099797836, "compression_ratio": 1.790513833992095, "no_speech_prob": 8.529606020601932e-06}, {"id": 668, "seek": 263892, "start": 2660.44, "end": 2663.4, "text": " So that's an interpretation is.", "tokens": [407, 300, 311, 364, 14174, 307, 13], "temperature": 0.0, "avg_logprob": -0.17120475099797836, "compression_ratio": 1.790513833992095, "no_speech_prob": 8.529606020601932e-06}, {"id": 669, "seek": 263892, "start": 2663.4, "end": 2667.6, "text": " That means that the actual identity of this word wasn't that important to the prediction", "tokens": [663, 1355, 300, 264, 3539, 6575, 295, 341, 1349, 2067, 380, 300, 1021, 281, 264, 17630], "temperature": 0.0, "avg_logprob": -0.17120475099797836, "compression_ratio": 1.790513833992095, "no_speech_prob": 8.529606020601932e-06}, {"id": 670, "seek": 266760, "start": 2667.6, "end": 2671.44, "text": " because I could have changed it and the score wouldn't have changed.", "tokens": [570, 286, 727, 362, 3105, 309, 293, 264, 6175, 2759, 380, 362, 3105, 13], "temperature": 0.0, "avg_logprob": -0.1866746468119102, "compression_ratio": 1.6454183266932272, "no_speech_prob": 1.8340178939979523e-05}, {"id": 671, "seek": 266760, "start": 2671.44, "end": 2674.12, "text": " Now why are there more methods than this?", "tokens": [823, 983, 366, 456, 544, 7150, 813, 341, 30], "temperature": 0.0, "avg_logprob": -0.1866746468119102, "compression_ratio": 1.6454183266932272, "no_speech_prob": 1.8340178939979523e-05}, {"id": 672, "seek": 266760, "start": 2674.12, "end": 2678.68, "text": " Because honestly reading that sounds awesome, that sounds great.", "tokens": [1436, 6095, 3760, 300, 3263, 3476, 11, 300, 3263, 869, 13], "temperature": 0.0, "avg_logprob": -0.1866746468119102, "compression_ratio": 1.6454183266932272, "no_speech_prob": 1.8340178939979523e-05}, {"id": 673, "seek": 266760, "start": 2678.68, "end": 2685.0, "text": " There are sort of a lot of issues with this kind of method in lots of ways of getting around", "tokens": [821, 366, 1333, 295, 257, 688, 295, 2663, 365, 341, 733, 295, 3170, 294, 3195, 295, 2098, 295, 1242, 926], "temperature": 0.0, "avg_logprob": -0.1866746468119102, "compression_ratio": 1.6454183266932272, "no_speech_prob": 1.8340178939979523e-05}, {"id": 674, "seek": 266760, "start": 2685.0, "end": 2686.0, "text": " them.", "tokens": [552, 13], "temperature": 0.0, "avg_logprob": -0.1866746468119102, "compression_ratio": 1.6454183266932272, "no_speech_prob": 1.8340178939979523e-05}, {"id": 675, "seek": 266760, "start": 2686.0, "end": 2687.0, "text": " Here's one issue.", "tokens": [1692, 311, 472, 2734, 13], "temperature": 0.0, "avg_logprob": -0.1866746468119102, "compression_ratio": 1.6454183266932272, "no_speech_prob": 1.8340178939979523e-05}, {"id": 676, "seek": 266760, "start": 2687.0, "end": 2692.68, "text": " It's not perfect because well maybe your linear approximation that the gradient gives", "tokens": [467, 311, 406, 2176, 570, 731, 1310, 428, 8213, 28023, 300, 264, 16235, 2709], "temperature": 0.0, "avg_logprob": -0.1866746468119102, "compression_ratio": 1.6454183266932272, "no_speech_prob": 1.8340178939979523e-05}, {"id": 677, "seek": 266760, "start": 2692.68, "end": 2696.8399999999997, "text": " you holds only very, very locally.", "tokens": [291, 9190, 787, 588, 11, 588, 16143, 13], "temperature": 0.0, "avg_logprob": -0.1866746468119102, "compression_ratio": 1.6454183266932272, "no_speech_prob": 1.8340178939979523e-05}, {"id": 678, "seek": 269684, "start": 2696.84, "end": 2700.2000000000003, "text": " So here the gradient is zero.", "tokens": [407, 510, 264, 16235, 307, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1943457257616651, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.14431929634884e-05}, {"id": 679, "seek": 269684, "start": 2700.2000000000003, "end": 2705.04, "text": " So this is a low saliency word because at the bottom of this parabola, but if I were to", "tokens": [407, 341, 307, 257, 2295, 1845, 7848, 1349, 570, 412, 264, 2767, 295, 341, 45729, 4711, 11, 457, 498, 286, 645, 281], "temperature": 0.0, "avg_logprob": -0.1943457257616651, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.14431929634884e-05}, {"id": 680, "seek": 269684, "start": 2705.04, "end": 2710.2400000000002, "text": " move even a little bit in either direction, the score would shoot up.", "tokens": [1286, 754, 257, 707, 857, 294, 2139, 3513, 11, 264, 6175, 576, 3076, 493, 13], "temperature": 0.0, "avg_logprob": -0.1943457257616651, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.14431929634884e-05}, {"id": 681, "seek": 269684, "start": 2710.2400000000002, "end": 2711.8, "text": " Is this not an important word?", "tokens": [1119, 341, 406, 364, 1021, 1349, 30], "temperature": 0.0, "avg_logprob": -0.1943457257616651, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.14431929634884e-05}, {"id": 682, "seek": 269684, "start": 2711.8, "end": 2719.52, "text": " It seems important to be right there as opposed to anywhere else even sort of nearby in", "tokens": [467, 2544, 1021, 281, 312, 558, 456, 382, 8851, 281, 4992, 1646, 754, 1333, 295, 11184, 294], "temperature": 0.0, "avg_logprob": -0.1943457257616651, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.14431929634884e-05}, {"id": 683, "seek": 269684, "start": 2719.52, "end": 2722.88, "text": " order for the score not to go up.", "tokens": [1668, 337, 264, 6175, 406, 281, 352, 493, 13], "temperature": 0.0, "avg_logprob": -0.1943457257616651, "compression_ratio": 1.6113744075829384, "no_speech_prob": 2.14431929634884e-05}, {"id": 684, "seek": 272288, "start": 2722.88, "end": 2726.96, "text": " The simple gradients method won't capture this because it just looks at the gradient which", "tokens": [440, 2199, 2771, 2448, 3170, 1582, 380, 7983, 341, 570, 309, 445, 1542, 412, 264, 16235, 597], "temperature": 0.0, "avg_logprob": -0.27533808062153475, "compression_ratio": 1.5662100456621004, "no_speech_prob": 2.976733594550751e-05}, {"id": 685, "seek": 272288, "start": 2726.96, "end": 2729.88, "text": " is that zero right there.", "tokens": [307, 300, 4018, 558, 456, 13], "temperature": 0.0, "avg_logprob": -0.27533808062153475, "compression_ratio": 1.5662100456621004, "no_speech_prob": 2.976733594550751e-05}, {"id": 686, "seek": 272288, "start": 2729.88, "end": 2730.88, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.27533808062153475, "compression_ratio": 1.5662100456621004, "no_speech_prob": 2.976733594550751e-05}, {"id": 687, "seek": 272288, "start": 2730.88, "end": 2735.04, "text": " But if you want to look into more, there's a bunch of different methods that are sort of", "tokens": [583, 498, 291, 528, 281, 574, 666, 544, 11, 456, 311, 257, 3840, 295, 819, 7150, 300, 366, 1333, 295], "temperature": 0.0, "avg_logprob": -0.27533808062153475, "compression_ratio": 1.5662100456621004, "no_speech_prob": 2.976733594550751e-05}, {"id": 688, "seek": 272288, "start": 2735.04, "end": 2737.44, "text": " applied in these papers.", "tokens": [6456, 294, 613, 10577, 13], "temperature": 0.0, "avg_logprob": -0.27533808062153475, "compression_ratio": 1.5662100456621004, "no_speech_prob": 2.976733594550751e-05}, {"id": 689, "seek": 272288, "start": 2737.44, "end": 2742.52, "text": " And I think that there's a good tool for the toolbox.", "tokens": [400, 286, 519, 300, 456, 311, 257, 665, 2290, 337, 264, 44593, 13], "temperature": 0.0, "avg_logprob": -0.27533808062153475, "compression_ratio": 1.5662100456621004, "no_speech_prob": 2.976733594550751e-05}, {"id": 690, "seek": 272288, "start": 2742.52, "end": 2743.52, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.27533808062153475, "compression_ratio": 1.5662100456621004, "no_speech_prob": 2.976733594550751e-05}, {"id": 691, "seek": 272288, "start": 2743.52, "end": 2747.48, "text": " So that is one way of explaining a prediction.", "tokens": [407, 300, 307, 472, 636, 295, 13468, 257, 17630, 13], "temperature": 0.0, "avg_logprob": -0.27533808062153475, "compression_ratio": 1.5662100456621004, "no_speech_prob": 2.976733594550751e-05}, {"id": 692, "seek": 274748, "start": 2747.48, "end": 2755.0, "text": " And it has some issues like why are individual words being scored as opposed to phrases or", "tokens": [400, 309, 575, 512, 2663, 411, 983, 366, 2609, 2283, 885, 18139, 382, 8851, 281, 20312, 420], "temperature": 0.0, "avg_logprob": -0.25319976806640626, "compression_ratio": 1.5283842794759825, "no_speech_prob": 1.6697027604095638e-05}, {"id": 693, "seek": 274748, "start": 2755.0, "end": 2757.08, "text": " something like that.", "tokens": [746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.25319976806640626, "compression_ratio": 1.5283842794759825, "no_speech_prob": 1.6697027604095638e-05}, {"id": 694, "seek": 274748, "start": 2757.08, "end": 2759.96, "text": " But for now, we're going to move on to another type of explanation.", "tokens": [583, 337, 586, 11, 321, 434, 516, 281, 1286, 322, 281, 1071, 2010, 295, 10835, 13], "temperature": 0.0, "avg_logprob": -0.25319976806640626, "compression_ratio": 1.5283842794759825, "no_speech_prob": 1.6697027604095638e-05}, {"id": 695, "seek": 274748, "start": 2759.96, "end": 2762.48, "text": " And I'm going to check the time.", "tokens": [400, 286, 478, 516, 281, 1520, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.25319976806640626, "compression_ratio": 1.5283842794759825, "no_speech_prob": 1.6697027604095638e-05}, {"id": 696, "seek": 274748, "start": 2762.48, "end": 2763.48, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.25319976806640626, "compression_ratio": 1.5283842794759825, "no_speech_prob": 1.6697027604095638e-05}, {"id": 697, "seek": 274748, "start": 2763.48, "end": 2764.48, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.25319976806640626, "compression_ratio": 1.5283842794759825, "no_speech_prob": 1.6697027604095638e-05}, {"id": 698, "seek": 274748, "start": 2764.48, "end": 2766.56, "text": " Actually, yeah, let me pause for a second.", "tokens": [5135, 11, 1338, 11, 718, 385, 10465, 337, 257, 1150, 13], "temperature": 0.0, "avg_logprob": -0.25319976806640626, "compression_ratio": 1.5283842794759825, "no_speech_prob": 1.6697027604095638e-05}, {"id": 699, "seek": 274748, "start": 2766.56, "end": 2770.68, "text": " Any questions about this?", "tokens": [2639, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.25319976806640626, "compression_ratio": 1.5283842794759825, "no_speech_prob": 1.6697027604095638e-05}, {"id": 700, "seek": 274748, "start": 2770.68, "end": 2776.2, "text": " I mean, the earlier on, they were a couple of questions.", "tokens": [286, 914, 11, 264, 3071, 322, 11, 436, 645, 257, 1916, 295, 1651, 13], "temperature": 0.0, "avg_logprob": -0.25319976806640626, "compression_ratio": 1.5283842794759825, "no_speech_prob": 1.6697027604095638e-05}, {"id": 701, "seek": 277620, "start": 2776.2, "end": 2782.52, "text": " One of them was, what are your thoughts on whether looking at attention weights is a methodologically", "tokens": [1485, 295, 552, 390, 11, 437, 366, 428, 4598, 322, 1968, 1237, 412, 3202, 17443, 307, 257, 3170, 17157], "temperature": 0.0, "avg_logprob": -0.13076006691410857, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.00040905267815105617}, {"id": 702, "seek": 277620, "start": 2782.52, "end": 2788.0, "text": " rigorous way of determining the importance of the model places on certain tokens?", "tokens": [29882, 636, 295, 23751, 264, 7379, 295, 264, 2316, 3190, 322, 1629, 22667, 30], "temperature": 0.0, "avg_logprob": -0.13076006691410857, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.00040905267815105617}, {"id": 703, "seek": 277620, "start": 2788.0, "end": 2792.08, "text": " It seems like there's some back and forth in the literature.", "tokens": [467, 2544, 411, 456, 311, 512, 646, 293, 5220, 294, 264, 10394, 13], "temperature": 0.0, "avg_logprob": -0.13076006691410857, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.00040905267815105617}, {"id": 704, "seek": 277620, "start": 2792.08, "end": 2794.8399999999997, "text": " That is a great question.", "tokens": [663, 307, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.13076006691410857, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.00040905267815105617}, {"id": 705, "seek": 277620, "start": 2794.8399999999997, "end": 2799.56, "text": " And I probably won't engage with that question as much as I could if we had like a second", "tokens": [400, 286, 1391, 1582, 380, 4683, 365, 300, 1168, 382, 709, 382, 286, 727, 498, 321, 632, 411, 257, 1150], "temperature": 0.0, "avg_logprob": -0.13076006691410857, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.00040905267815105617}, {"id": 706, "seek": 277620, "start": 2799.56, "end": 2800.56, "text": " lecture on this.", "tokens": [7991, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.13076006691410857, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.00040905267815105617}, {"id": 707, "seek": 277620, "start": 2800.56, "end": 2805.16, "text": " I actually will provide some attention analyses and tell you they're interesting.", "tokens": [286, 767, 486, 2893, 512, 3202, 37560, 293, 980, 291, 436, 434, 1880, 13], "temperature": 0.0, "avg_logprob": -0.13076006691410857, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.00040905267815105617}, {"id": 708, "seek": 280516, "start": 2805.16, "end": 2814.7999999999997, "text": " And then I'll say a little bit about why they can be interesting without being sort of", "tokens": [400, 550, 286, 603, 584, 257, 707, 857, 466, 983, 436, 393, 312, 1880, 1553, 885, 1333, 295], "temperature": 0.0, "avg_logprob": -0.16774851839307328, "compression_ratio": 1.5133689839572193, "no_speech_prob": 1.643917494220659e-05}, {"id": 709, "seek": 280516, "start": 2814.7999999999997, "end": 2824.56, "text": " maybe sort of the end all of analysis of where information is flowing in a transformer,", "tokens": [1310, 1333, 295, 264, 917, 439, 295, 5215, 295, 689, 1589, 307, 13974, 294, 257, 31782, 11], "temperature": 0.0, "avg_logprob": -0.16774851839307328, "compression_ratio": 1.5133689839572193, "no_speech_prob": 1.643917494220659e-05}, {"id": 710, "seek": 280516, "start": 2824.56, "end": 2825.56, "text": " for example.", "tokens": [337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.16774851839307328, "compression_ratio": 1.5133689839572193, "no_speech_prob": 1.643917494220659e-05}, {"id": 711, "seek": 280516, "start": 2825.56, "end": 2831.12, "text": " I think the debate is something that we would have to get into in a much longer period of", "tokens": [286, 519, 264, 7958, 307, 746, 300, 321, 576, 362, 281, 483, 666, 294, 257, 709, 2854, 2896, 295], "temperature": 0.0, "avg_logprob": -0.16774851839307328, "compression_ratio": 1.5133689839572193, "no_speech_prob": 1.643917494220659e-05}, {"id": 712, "seek": 280516, "start": 2831.12, "end": 2832.12, "text": " time.", "tokens": [565, 13], "temperature": 0.0, "avg_logprob": -0.16774851839307328, "compression_ratio": 1.5133689839572193, "no_speech_prob": 1.643917494220659e-05}, {"id": 713, "seek": 283212, "start": 2832.12, "end": 2836.2, "text": " Look at the slides that I show about attention and the caveats that I provide and let me", "tokens": [2053, 412, 264, 9788, 300, 286, 855, 466, 3202, 293, 264, 11730, 1720, 300, 286, 2893, 293, 718, 385], "temperature": 0.0, "avg_logprob": -0.18097702662150064, "compression_ratio": 1.6690909090909092, "no_speech_prob": 8.347265247721225e-05}, {"id": 714, "seek": 283212, "start": 2836.2, "end": 2839.8399999999997, "text": " know if that answers your question first because we have quite a number of slides on it.", "tokens": [458, 498, 300, 6338, 428, 1168, 700, 570, 321, 362, 1596, 257, 1230, 295, 9788, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.18097702662150064, "compression_ratio": 1.6690909090909092, "no_speech_prob": 8.347265247721225e-05}, {"id": 715, "seek": 283212, "start": 2839.8399999999997, "end": 2844.12, "text": " And if not, please, please ask again and we can chat more about it.", "tokens": [400, 498, 406, 11, 1767, 11, 1767, 1029, 797, 293, 321, 393, 5081, 544, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.18097702662150064, "compression_ratio": 1.6690909090909092, "no_speech_prob": 8.347265247721225e-05}, {"id": 716, "seek": 283212, "start": 2844.12, "end": 2847.12, "text": " And maybe you can go on.", "tokens": [400, 1310, 291, 393, 352, 322, 13], "temperature": 0.0, "avg_logprob": -0.18097702662150064, "compression_ratio": 1.6690909090909092, "no_speech_prob": 8.347265247721225e-05}, {"id": 717, "seek": 283212, "start": 2847.12, "end": 2848.12, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.18097702662150064, "compression_ratio": 1.6690909090909092, "no_speech_prob": 8.347265247721225e-05}, {"id": 718, "seek": 283212, "start": 2848.12, "end": 2849.12, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18097702662150064, "compression_ratio": 1.6690909090909092, "no_speech_prob": 8.347265247721225e-05}, {"id": 719, "seek": 283212, "start": 2849.12, "end": 2853.3199999999997, "text": " So, I think this is a really fascinating question which also gets at what was important", "tokens": [407, 11, 286, 519, 341, 307, 257, 534, 10343, 1168, 597, 611, 2170, 412, 437, 390, 1021], "temperature": 0.0, "avg_logprob": -0.18097702662150064, "compression_ratio": 1.6690909090909092, "no_speech_prob": 8.347265247721225e-05}, {"id": 720, "seek": 283212, "start": 2853.3199999999997, "end": 2859.44, "text": " about the input but in actually kind of an even more direct way, which is, could I just", "tokens": [466, 264, 4846, 457, 294, 767, 733, 295, 364, 754, 544, 2047, 636, 11, 597, 307, 11, 727, 286, 445], "temperature": 0.0, "avg_logprob": -0.18097702662150064, "compression_ratio": 1.6690909090909092, "no_speech_prob": 8.347265247721225e-05}, {"id": 721, "seek": 285944, "start": 2859.44, "end": 2862.76, "text": " keep some minimal part of the input and get the same answer.", "tokens": [1066, 512, 13206, 644, 295, 264, 4846, 293, 483, 264, 912, 1867, 13], "temperature": 0.0, "avg_logprob": -0.22813409273741675, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.1973933548433706e-05}, {"id": 722, "seek": 285944, "start": 2862.76, "end": 2864.88, "text": " So, here's an example from Squad.", "tokens": [407, 11, 510, 311, 364, 1365, 490, 26596, 13], "temperature": 0.0, "avg_logprob": -0.22813409273741675, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.1973933548433706e-05}, {"id": 723, "seek": 285944, "start": 2864.88, "end": 2871.08, "text": " You have this passage in 1899, John Jacob Astor IV invested $100,000 for Tesla.", "tokens": [509, 362, 341, 11497, 294, 2443, 8494, 11, 2619, 14117, 12884, 284, 15967, 13104, 1848, 6879, 11, 1360, 337, 13666, 13], "temperature": 0.0, "avg_logprob": -0.22813409273741675, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.1973933548433706e-05}, {"id": 724, "seek": 285944, "start": 2871.08, "end": 2872.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.22813409273741675, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.1973933548433706e-05}, {"id": 725, "seek": 285944, "start": 2872.08, "end": 2875.2000000000003, "text": " And then the answer that is being predicted by the model is going to always be in blue", "tokens": [400, 550, 264, 1867, 300, 307, 885, 19147, 538, 264, 2316, 307, 516, 281, 1009, 312, 294, 3344], "temperature": 0.0, "avg_logprob": -0.22813409273741675, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.1973933548433706e-05}, {"id": 726, "seek": 285944, "start": 2875.2000000000003, "end": 2877.2000000000003, "text": " in these examples, Colorado Springs experiments.", "tokens": [294, 613, 5110, 11, 15786, 33065, 12050, 13], "temperature": 0.0, "avg_logprob": -0.22813409273741675, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.1973933548433706e-05}, {"id": 727, "seek": 285944, "start": 2877.2000000000003, "end": 2879.92, "text": " So, you got this passage.", "tokens": [407, 11, 291, 658, 341, 11497, 13], "temperature": 0.0, "avg_logprob": -0.22813409273741675, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.1973933548433706e-05}, {"id": 728, "seek": 285944, "start": 2879.92, "end": 2883.7200000000003, "text": " And the question is what did Tesla spend Astor's money on?", "tokens": [400, 264, 1168, 307, 437, 630, 13666, 3496, 12884, 284, 311, 1460, 322, 30], "temperature": 0.0, "avg_logprob": -0.22813409273741675, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.1973933548433706e-05}, {"id": 729, "seek": 285944, "start": 2883.7200000000003, "end": 2886.0, "text": " That's why the prediction is Colorado Springs experiments.", "tokens": [663, 311, 983, 264, 17630, 307, 15786, 33065, 12050, 13], "temperature": 0.0, "avg_logprob": -0.22813409273741675, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.1973933548433706e-05}, {"id": 730, "seek": 288600, "start": 2886.0, "end": 2890.52, "text": " The model gets the answer right, which is nice.", "tokens": [440, 2316, 2170, 264, 1867, 558, 11, 597, 307, 1481, 13], "temperature": 0.0, "avg_logprob": -0.18140061308697955, "compression_ratio": 1.63, "no_speech_prob": 2.3921855245134793e-05}, {"id": 731, "seek": 288600, "start": 2890.52, "end": 2894.96, "text": " And we would like to think it's because it's doing some kind of reading comprehension.", "tokens": [400, 321, 576, 411, 281, 519, 309, 311, 570, 309, 311, 884, 512, 733, 295, 3760, 44991, 13], "temperature": 0.0, "avg_logprob": -0.18140061308697955, "compression_ratio": 1.63, "no_speech_prob": 2.3921855245134793e-05}, {"id": 732, "seek": 288600, "start": 2894.96, "end": 2896.56, "text": " But here's the issue.", "tokens": [583, 510, 311, 264, 2734, 13], "temperature": 0.0, "avg_logprob": -0.18140061308697955, "compression_ratio": 1.63, "no_speech_prob": 2.3921855245134793e-05}, {"id": 733, "seek": 288600, "start": 2896.56, "end": 2903.2, "text": " It turns out, based on this fascinating paper, that if you just reduced the question to", "tokens": [467, 4523, 484, 11, 2361, 322, 341, 10343, 3035, 11, 300, 498, 291, 445, 9212, 264, 1168, 281], "temperature": 0.0, "avg_logprob": -0.18140061308697955, "compression_ratio": 1.63, "no_speech_prob": 2.3921855245134793e-05}, {"id": 734, "seek": 288600, "start": 2903.2, "end": 2910.32, "text": " did, you actually get exactly the same, you actually get exactly the same answer.", "tokens": [630, 11, 291, 767, 483, 2293, 264, 912, 11, 291, 767, 483, 2293, 264, 912, 1867, 13], "temperature": 0.0, "avg_logprob": -0.18140061308697955, "compression_ratio": 1.63, "no_speech_prob": 2.3921855245134793e-05}, {"id": 735, "seek": 291032, "start": 2910.32, "end": 2916.04, "text": " And in fact, with the original question, the model had sort of a.78 confidence, you", "tokens": [400, 294, 1186, 11, 365, 264, 3380, 1168, 11, 264, 2316, 632, 1333, 295, 257, 2411, 30693, 6687, 11, 291], "temperature": 0.0, "avg_logprob": -0.170407466399364, "compression_ratio": 1.67578125, "no_speech_prob": 1.0450979971210472e-05}, {"id": 736, "seek": 291032, "start": 2916.04, "end": 2918.04, "text": " know, probability in that answer.", "tokens": [458, 11, 8482, 294, 300, 1867, 13], "temperature": 0.0, "avg_logprob": -0.170407466399364, "compression_ratio": 1.67578125, "no_speech_prob": 1.0450979971210472e-05}, {"id": 737, "seek": 291032, "start": 2918.04, "end": 2924.0, "text": " And with the reduced question did, you get even higher confidence.", "tokens": [400, 365, 264, 9212, 1168, 630, 11, 291, 483, 754, 2946, 6687, 13], "temperature": 0.0, "avg_logprob": -0.170407466399364, "compression_ratio": 1.67578125, "no_speech_prob": 1.0450979971210472e-05}, {"id": 738, "seek": 291032, "start": 2924.0, "end": 2928.28, "text": " And that, if you give a human this, they would not be able to know really what you're", "tokens": [400, 300, 11, 498, 291, 976, 257, 1952, 341, 11, 436, 576, 406, 312, 1075, 281, 458, 534, 437, 291, 434], "temperature": 0.0, "avg_logprob": -0.170407466399364, "compression_ratio": 1.67578125, "no_speech_prob": 1.0450979971210472e-05}, {"id": 739, "seek": 291032, "start": 2928.28, "end": 2929.28, "text": " trying to ask about.", "tokens": [1382, 281, 1029, 466, 13], "temperature": 0.0, "avg_logprob": -0.170407466399364, "compression_ratio": 1.67578125, "no_speech_prob": 1.0450979971210472e-05}, {"id": 740, "seek": 291032, "start": 2929.28, "end": 2933.4, "text": " So, it seems like something is going really wonky here.", "tokens": [407, 11, 309, 2544, 411, 746, 307, 516, 534, 1582, 4133, 510, 13], "temperature": 0.0, "avg_logprob": -0.170407466399364, "compression_ratio": 1.67578125, "no_speech_prob": 1.0450979971210472e-05}, {"id": 741, "seek": 291032, "start": 2933.4, "end": 2934.4, "text": " Here's another.", "tokens": [1692, 311, 1071, 13], "temperature": 0.0, "avg_logprob": -0.170407466399364, "compression_ratio": 1.67578125, "no_speech_prob": 1.0450979971210472e-05}, {"id": 742, "seek": 291032, "start": 2934.4, "end": 2938.32, "text": " So, here's sort of like a very high level overview of the method.", "tokens": [407, 11, 510, 311, 1333, 295, 411, 257, 588, 1090, 1496, 12492, 295, 264, 3170, 13], "temperature": 0.0, "avg_logprob": -0.170407466399364, "compression_ratio": 1.67578125, "no_speech_prob": 1.0450979971210472e-05}, {"id": 743, "seek": 293832, "start": 2938.32, "end": 2941.56, "text": " In fact, it actually references our input saline's theme methods.", "tokens": [682, 1186, 11, 309, 767, 15400, 527, 4846, 1845, 533, 311, 6314, 7150, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 744, "seek": 293832, "start": 2941.56, "end": 2943.04, "text": " Nice, it's connected.", "tokens": [5490, 11, 309, 311, 4582, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 745, "seek": 293832, "start": 2943.04, "end": 2949.04, "text": " So, you iteratively remove non-salient or unimportant words.", "tokens": [407, 11, 291, 17138, 19020, 4159, 2107, 12, 15142, 1196, 420, 517, 41654, 2283, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 746, "seek": 293832, "start": 2949.04, "end": 2952.44, "text": " So here's a passage again talking about football.", "tokens": [407, 510, 311, 257, 11497, 797, 1417, 466, 7346, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 747, "seek": 293832, "start": 2952.44, "end": 2953.6400000000003, "text": " I think.", "tokens": [286, 519, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 748, "seek": 293832, "start": 2953.6400000000003, "end": 2954.6400000000003, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 749, "seek": 293832, "start": 2954.6400000000003, "end": 2956.44, "text": " And, oh, nice.", "tokens": [400, 11, 1954, 11, 1481, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 750, "seek": 293832, "start": 2956.44, "end": 2957.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 751, "seek": 293832, "start": 2957.44, "end": 2960.8, "text": " So, the question is, where did the Broncos practice with a super bowl as the prediction", "tokens": [407, 11, 264, 1168, 307, 11, 689, 630, 264, 19544, 6877, 3124, 365, 257, 1687, 6571, 382, 264, 17630], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 752, "seek": 293832, "start": 2960.8, "end": 2964.2400000000002, "text": " of Stanford University?", "tokens": [295, 20374, 3535, 30], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 753, "seek": 293832, "start": 2964.2400000000002, "end": 2965.44, "text": " And that is correct.", "tokens": [400, 300, 307, 3006, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 754, "seek": 293832, "start": 2965.44, "end": 2967.2000000000003, "text": " So again, seems nice.", "tokens": [407, 797, 11, 2544, 1481, 13], "temperature": 0.0, "avg_logprob": -0.3317468610860534, "compression_ratio": 1.501930501930502, "no_speech_prob": 2.6684278054744937e-05}, {"id": 755, "seek": 296720, "start": 2967.2, "end": 2971.2799999999997, "text": " And now, we're not actually going to get the model to be incorrect.", "tokens": [400, 586, 11, 321, 434, 406, 767, 516, 281, 483, 264, 2316, 281, 312, 18424, 13], "temperature": 0.0, "avg_logprob": -0.1610725732158414, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.177847818529699e-05}, {"id": 756, "seek": 296720, "start": 2971.2799999999997, "end": 2976.56, "text": " We're just going to say, how can I change this question such that I still look at the", "tokens": [492, 434, 445, 516, 281, 584, 11, 577, 393, 286, 1319, 341, 1168, 1270, 300, 286, 920, 574, 412, 264], "temperature": 0.0, "avg_logprob": -0.1610725732158414, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.177847818529699e-05}, {"id": 757, "seek": 296720, "start": 2976.56, "end": 2977.56, "text": " answer right?", "tokens": [1867, 558, 30], "temperature": 0.0, "avg_logprob": -0.1610725732158414, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.177847818529699e-05}, {"id": 758, "seek": 296720, "start": 2977.56, "end": 2981.56, "text": " So, I'm going to remove the word that was least important according to a saliency method.", "tokens": [407, 11, 286, 478, 516, 281, 4159, 264, 1349, 300, 390, 1935, 1021, 4650, 281, 257, 1845, 7848, 3170, 13], "temperature": 0.0, "avg_logprob": -0.1610725732158414, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.177847818529699e-05}, {"id": 759, "seek": 296720, "start": 2981.56, "end": 2985.16, "text": " So, now, it's where did the practice for the super bowl?", "tokens": [407, 11, 586, 11, 309, 311, 689, 630, 264, 3124, 337, 264, 1687, 6571, 30], "temperature": 0.0, "avg_logprob": -0.1610725732158414, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.177847818529699e-05}, {"id": 760, "seek": 296720, "start": 2985.16, "end": 2988.72, "text": " Already, this is sort of unanswerable because you've got two teams practicing.", "tokens": [23741, 11, 341, 307, 1333, 295, 517, 43904, 712, 570, 291, 600, 658, 732, 5491, 11350, 13], "temperature": 0.0, "avg_logprob": -0.1610725732158414, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.177847818529699e-05}, {"id": 761, "seek": 296720, "start": 2988.72, "end": 2990.7599999999998, "text": " You don't even know which one you're asking about.", "tokens": [509, 500, 380, 754, 458, 597, 472, 291, 434, 3365, 466, 13], "temperature": 0.0, "avg_logprob": -0.1610725732158414, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.177847818529699e-05}, {"id": 762, "seek": 296720, "start": 2990.7599999999998, "end": 2995.3599999999997, "text": " So, why the model still thinks it's so confident in Stanford University makes no sense.", "tokens": [407, 11, 983, 264, 2316, 920, 7309, 309, 311, 370, 6679, 294, 20374, 3535, 1669, 572, 2020, 13], "temperature": 0.0, "avg_logprob": -0.1610725732158414, "compression_ratio": 1.6521739130434783, "no_speech_prob": 2.177847818529699e-05}, {"id": 763, "seek": 299536, "start": 2995.36, "end": 2999.2000000000003, "text": " But you can just sort of keep going.", "tokens": [583, 291, 393, 445, 1333, 295, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.1726854030902569, "compression_ratio": 1.5683760683760684, "no_speech_prob": 2.318415317859035e-05}, {"id": 764, "seek": 299536, "start": 2999.2000000000003, "end": 3007.44, "text": " And now, I think, here, the model stops being confident in the answer Stanford University.", "tokens": [400, 586, 11, 286, 519, 11, 510, 11, 264, 2316, 10094, 885, 6679, 294, 264, 1867, 20374, 3535, 13], "temperature": 0.0, "avg_logprob": -0.1726854030902569, "compression_ratio": 1.5683760683760684, "no_speech_prob": 2.318415317859035e-05}, {"id": 765, "seek": 299536, "start": 3007.44, "end": 3013.1600000000003, "text": " But I think this is really interesting just to show that if the model is able to do this", "tokens": [583, 286, 519, 341, 307, 534, 1880, 445, 281, 855, 300, 498, 264, 2316, 307, 1075, 281, 360, 341], "temperature": 0.0, "avg_logprob": -0.1726854030902569, "compression_ratio": 1.5683760683760684, "no_speech_prob": 2.318415317859035e-05}, {"id": 766, "seek": 299536, "start": 3013.1600000000003, "end": 3019.6, "text": " with very high confidence, it's not reflecting the uncertainty that really should be there", "tokens": [365, 588, 1090, 6687, 11, 309, 311, 406, 23543, 264, 15697, 300, 534, 820, 312, 456], "temperature": 0.0, "avg_logprob": -0.1726854030902569, "compression_ratio": 1.5683760683760684, "no_speech_prob": 2.318415317859035e-05}, {"id": 767, "seek": 299536, "start": 3019.6, "end": 3022.84, "text": " because you can't know what you're even asking about.", "tokens": [570, 291, 393, 380, 458, 437, 291, 434, 754, 3365, 466, 13], "temperature": 0.0, "avg_logprob": -0.1726854030902569, "compression_ratio": 1.5683760683760684, "no_speech_prob": 2.318415317859035e-05}, {"id": 768, "seek": 299536, "start": 3022.84, "end": 3023.84, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1726854030902569, "compression_ratio": 1.5683760683760684, "no_speech_prob": 2.318415317859035e-05}, {"id": 769, "seek": 302384, "start": 3023.84, "end": 3026.1600000000003, "text": " So, what was important to make this answer?", "tokens": [407, 11, 437, 390, 1021, 281, 652, 341, 1867, 30], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 770, "seek": 302384, "start": 3026.1600000000003, "end": 3032.0, "text": " Well, at least these parts were important because you could keep just those parts and", "tokens": [1042, 11, 412, 1935, 613, 3166, 645, 1021, 570, 291, 727, 1066, 445, 729, 3166, 293], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 771, "seek": 302384, "start": 3032.0, "end": 3034.92, "text": " get the same answer, fascinating.", "tokens": [483, 264, 912, 1867, 11, 10343, 13], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 772, "seek": 302384, "start": 3034.92, "end": 3036.4, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 773, "seek": 302384, "start": 3036.4, "end": 3044.1600000000003, "text": " So, that's sort of the end of the admittedly brief section on thinking about input saliency", "tokens": [407, 11, 300, 311, 1333, 295, 264, 917, 295, 264, 14920, 356, 5353, 3541, 322, 1953, 466, 4846, 1845, 7848], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 774, "seek": 302384, "start": 3044.1600000000003, "end": 3045.1600000000003, "text": " methods and similar things.", "tokens": [7150, 293, 2531, 721, 13], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 775, "seek": 302384, "start": 3045.1600000000003, "end": 3048.92, "text": " Now, we're going to talk about actually breaking models and understanding models by breaking", "tokens": [823, 11, 321, 434, 516, 281, 751, 466, 767, 7697, 5245, 293, 3701, 5245, 538, 7697], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 776, "seek": 302384, "start": 3048.92, "end": 3049.92, "text": " them.", "tokens": [552, 13], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 777, "seek": 302384, "start": 3049.92, "end": 3050.92, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 778, "seek": 302384, "start": 3050.92, "end": 3051.92, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.21603653317406063, "compression_ratio": 1.653061224489796, "no_speech_prob": 4.1981504182331264e-05}, {"id": 779, "seek": 305192, "start": 3051.92, "end": 3058.6800000000003, "text": " So, if we have a passage here, Peyton Manning came the first quarterback, something Super", "tokens": [407, 11, 498, 321, 362, 257, 11497, 510, 11, 36206, 1756, 2458, 773, 1361, 264, 700, 31952, 11, 746, 4548], "temperature": 0.0, "avg_logprob": -0.24593485583056202, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.1329745727125555e-05}, {"id": 780, "seek": 305192, "start": 3058.6800000000003, "end": 3062.16, "text": " Bowl, age 39, past record, held by John L. Wei.", "tokens": [25044, 11, 3205, 15238, 11, 1791, 2136, 11, 5167, 538, 2619, 441, 13, 21174, 13], "temperature": 0.0, "avg_logprob": -0.24593485583056202, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.1329745727125555e-05}, {"id": 781, "seek": 305192, "start": 3062.16, "end": 3063.76, "text": " Again, we're doing question answering.", "tokens": [3764, 11, 321, 434, 884, 1168, 13430, 13], "temperature": 0.0, "avg_logprob": -0.24593485583056202, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.1329745727125555e-05}, {"id": 782, "seek": 305192, "start": 3063.76, "end": 3065.32, "text": " We got this question.", "tokens": [492, 658, 341, 1168, 13], "temperature": 0.0, "avg_logprob": -0.24593485583056202, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.1329745727125555e-05}, {"id": 783, "seek": 305192, "start": 3065.32, "end": 3068.56, "text": " What was the name of the quarterback who was 38 in the Super Bowl?", "tokens": [708, 390, 264, 1315, 295, 264, 31952, 567, 390, 12843, 294, 264, 4548, 25044, 30], "temperature": 0.0, "avg_logprob": -0.24593485583056202, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.1329745727125555e-05}, {"id": 784, "seek": 305192, "start": 3068.56, "end": 3070.56, "text": " The prediction is correct.", "tokens": [440, 17630, 307, 3006, 13], "temperature": 0.0, "avg_logprob": -0.24593485583056202, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.1329745727125555e-05}, {"id": 785, "seek": 305192, "start": 3070.56, "end": 3071.56, "text": " Looks good.", "tokens": [10027, 665, 13], "temperature": 0.0, "avg_logprob": -0.24593485583056202, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.1329745727125555e-05}, {"id": 786, "seek": 305192, "start": 3071.56, "end": 3076.88, "text": " Now, we're not going to change the question to try to sort of make the question nonsensical", "tokens": [823, 11, 321, 434, 406, 516, 281, 1319, 264, 1168, 281, 853, 281, 1333, 295, 652, 264, 1168, 297, 892, 694, 804], "temperature": 0.0, "avg_logprob": -0.24593485583056202, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.1329745727125555e-05}, {"id": 787, "seek": 307688, "start": 3076.88, "end": 3083.12, "text": " while keeping the same answer, instead we're going to change the passage by adding the", "tokens": [1339, 5145, 264, 912, 1867, 11, 2602, 321, 434, 516, 281, 1319, 264, 11497, 538, 5127, 264], "temperature": 0.0, "avg_logprob": -0.1767606320588485, "compression_ratio": 1.618705035971223, "no_speech_prob": 1.2605413758137729e-05}, {"id": 788, "seek": 307688, "start": 3083.12, "end": 3085.56, "text": " sentence at the end, which really shouldn't distract anyone.", "tokens": [8174, 412, 264, 917, 11, 597, 534, 4659, 380, 9945, 2878, 13], "temperature": 0.0, "avg_logprob": -0.1767606320588485, "compression_ratio": 1.618705035971223, "no_speech_prob": 1.2605413758137729e-05}, {"id": 789, "seek": 307688, "start": 3085.56, "end": 3090.7200000000003, "text": " This is quarterback, well known quarterback, Jeff Dean, you know, had jersey number 37", "tokens": [639, 307, 31952, 11, 731, 2570, 31952, 11, 7506, 13324, 11, 291, 458, 11, 632, 40700, 1230, 13435], "temperature": 0.0, "avg_logprob": -0.1767606320588485, "compression_ratio": 1.618705035971223, "no_speech_prob": 1.2605413758137729e-05}, {"id": 790, "seek": 307688, "start": 3090.7200000000003, "end": 3091.7200000000003, "text": " in champ bull.", "tokens": [294, 5921, 4693, 13], "temperature": 0.0, "avg_logprob": -0.1767606320588485, "compression_ratio": 1.618705035971223, "no_speech_prob": 1.2605413758137729e-05}, {"id": 791, "seek": 307688, "start": 3091.7200000000003, "end": 3094.84, "text": " So, this just doesn't, it's really not even related.", "tokens": [407, 11, 341, 445, 1177, 380, 11, 309, 311, 534, 406, 754, 4077, 13], "temperature": 0.0, "avg_logprob": -0.1767606320588485, "compression_ratio": 1.618705035971223, "no_speech_prob": 1.2605413758137729e-05}, {"id": 792, "seek": 307688, "start": 3094.84, "end": 3100.52, "text": " But now the prediction is Jeff Dean for our nice QA model.", "tokens": [583, 586, 264, 17630, 307, 7506, 13324, 337, 527, 1481, 1249, 32, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1767606320588485, "compression_ratio": 1.618705035971223, "no_speech_prob": 1.2605413758137729e-05}, {"id": 793, "seek": 307688, "start": 3100.52, "end": 3106.6800000000003, "text": " And so, this shows as well that it seems like maybe there's this like end of the passage", "tokens": [400, 370, 11, 341, 3110, 382, 731, 300, 309, 2544, 411, 1310, 456, 311, 341, 411, 917, 295, 264, 11497], "temperature": 0.0, "avg_logprob": -0.1767606320588485, "compression_ratio": 1.618705035971223, "no_speech_prob": 1.2605413758137729e-05}, {"id": 794, "seek": 310668, "start": 3106.68, "end": 3110.0, "text": " by as to what the answer should be, for example.", "tokens": [538, 382, 281, 437, 264, 1867, 820, 312, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.22189538668742223, "compression_ratio": 1.648, "no_speech_prob": 7.595684292027727e-05}, {"id": 795, "seek": 310668, "start": 3110.0, "end": 3115.12, "text": " And so, that's an adversarial example where we flipped the prediction by adding something", "tokens": [400, 370, 11, 300, 311, 364, 17641, 44745, 1365, 689, 321, 26273, 264, 17630, 538, 5127, 746], "temperature": 0.0, "avg_logprob": -0.22189538668742223, "compression_ratio": 1.648, "no_speech_prob": 7.595684292027727e-05}, {"id": 796, "seek": 310668, "start": 3115.12, "end": 3117.3599999999997, "text": " that is innocuous to humans.", "tokens": [300, 307, 10843, 12549, 281, 6255, 13], "temperature": 0.0, "avg_logprob": -0.22189538668742223, "compression_ratio": 1.648, "no_speech_prob": 7.595684292027727e-05}, {"id": 797, "seek": 310668, "start": 3117.3599999999997, "end": 3121.7599999999998, "text": " And so, sort of like the higher level takeaway is like, oh, it seems like the QA model that", "tokens": [400, 370, 11, 1333, 295, 411, 264, 2946, 1496, 30681, 307, 411, 11, 1954, 11, 309, 2544, 411, 264, 1249, 32, 2316, 300], "temperature": 0.0, "avg_logprob": -0.22189538668742223, "compression_ratio": 1.648, "no_speech_prob": 7.595684292027727e-05}, {"id": 798, "seek": 310668, "start": 3121.7599999999998, "end": 3122.7599999999998, "text": " we had that seemed good.", "tokens": [321, 632, 300, 6576, 665, 13], "temperature": 0.0, "avg_logprob": -0.22189538668742223, "compression_ratio": 1.648, "no_speech_prob": 7.595684292027727e-05}, {"id": 799, "seek": 310668, "start": 3122.7599999999998, "end": 3127.3599999999997, "text": " It's not actually performing QA how we want it to, even though it's in domain accuracy", "tokens": [467, 311, 406, 767, 10205, 1249, 32, 577, 321, 528, 309, 281, 11, 754, 1673, 309, 311, 294, 9274, 14170], "temperature": 0.0, "avg_logprob": -0.22189538668742223, "compression_ratio": 1.648, "no_speech_prob": 7.595684292027727e-05}, {"id": 800, "seek": 310668, "start": 3127.3599999999997, "end": 3130.2, "text": " it was good.", "tokens": [309, 390, 665, 13], "temperature": 0.0, "avg_logprob": -0.22189538668742223, "compression_ratio": 1.648, "no_speech_prob": 7.595684292027727e-05}, {"id": 801, "seek": 310668, "start": 3130.2, "end": 3132.3999999999996, "text": " And here's another example.", "tokens": [400, 510, 311, 1071, 1365, 13], "temperature": 0.0, "avg_logprob": -0.22189538668742223, "compression_ratio": 1.648, "no_speech_prob": 7.595684292027727e-05}, {"id": 802, "seek": 313240, "start": 3132.4, "end": 3139.6800000000003, "text": " So, you've got this paragraph with a question, what has been the result of this publicity?", "tokens": [407, 11, 291, 600, 658, 341, 18865, 365, 257, 1168, 11, 437, 575, 668, 264, 1874, 295, 341, 37264, 30], "temperature": 0.0, "avg_logprob": -0.1880816398782933, "compression_ratio": 1.7079646017699115, "no_speech_prob": 5.5603264627279714e-05}, {"id": 803, "seek": 313240, "start": 3139.6800000000003, "end": 3142.92, "text": " The answer is increased scrutiny on teacher misconduct.", "tokens": [440, 1867, 307, 6505, 38615, 322, 5027, 3346, 38150, 13], "temperature": 0.0, "avg_logprob": -0.1880816398782933, "compression_ratio": 1.7079646017699115, "no_speech_prob": 5.5603264627279714e-05}, {"id": 804, "seek": 313240, "start": 3142.92, "end": 3148.2400000000002, "text": " Now instead of changing the paragraph, we're going to change the question in really, really", "tokens": [823, 2602, 295, 4473, 264, 18865, 11, 321, 434, 516, 281, 1319, 264, 1168, 294, 534, 11, 534], "temperature": 0.0, "avg_logprob": -0.1880816398782933, "compression_ratio": 1.7079646017699115, "no_speech_prob": 5.5603264627279714e-05}, {"id": 805, "seek": 313240, "start": 3148.2400000000002, "end": 3152.88, "text": " seemingly in significant ways to change the model's prediction.", "tokens": [18709, 294, 4776, 2098, 281, 1319, 264, 2316, 311, 17630, 13], "temperature": 0.0, "avg_logprob": -0.1880816398782933, "compression_ratio": 1.7079646017699115, "no_speech_prob": 5.5603264627279714e-05}, {"id": 806, "seek": 313240, "start": 3152.88, "end": 3159.48, "text": " So first, what HA, and I've got this typo L, then the result of this publicity, the", "tokens": [407, 700, 11, 437, 11979, 11, 293, 286, 600, 658, 341, 2125, 78, 441, 11, 550, 264, 1874, 295, 341, 37264, 11, 264], "temperature": 0.0, "avg_logprob": -0.1880816398782933, "compression_ratio": 1.7079646017699115, "no_speech_prob": 5.5603264627279714e-05}, {"id": 807, "seek": 315948, "start": 3159.48, "end": 3165.56, "text": " answer changes to teacher misconduct, likely a human would sort of ignore this typo or", "tokens": [1867, 2962, 281, 5027, 3346, 38150, 11, 3700, 257, 1952, 576, 1333, 295, 11200, 341, 2125, 78, 420], "temperature": 0.0, "avg_logprob": -0.21084472813557104, "compression_ratio": 1.7692307692307692, "no_speech_prob": 7.76585511630401e-06}, {"id": 808, "seek": 315948, "start": 3165.56, "end": 3167.64, "text": " something and answer the right answer.", "tokens": [746, 293, 1867, 264, 558, 1867, 13], "temperature": 0.0, "avg_logprob": -0.21084472813557104, "compression_ratio": 1.7692307692307692, "no_speech_prob": 7.76585511630401e-06}, {"id": 809, "seek": 315948, "start": 3167.64, "end": 3169.76, "text": " And then this is really nuts.", "tokens": [400, 550, 341, 307, 534, 10483, 13], "temperature": 0.0, "avg_logprob": -0.21084472813557104, "compression_ratio": 1.7692307692307692, "no_speech_prob": 7.76585511630401e-06}, {"id": 810, "seek": 315948, "start": 3169.76, "end": 3174.4, "text": " Instead of asking what has been the result of this publicity, if you ask what's been", "tokens": [7156, 295, 3365, 437, 575, 668, 264, 1874, 295, 341, 37264, 11, 498, 291, 1029, 437, 311, 668], "temperature": 0.0, "avg_logprob": -0.21084472813557104, "compression_ratio": 1.7692307692307692, "no_speech_prob": 7.76585511630401e-06}, {"id": 811, "seek": 315948, "start": 3174.4, "end": 3179.48, "text": " the result of this publicity, the answer also changes.", "tokens": [264, 1874, 295, 341, 37264, 11, 264, 1867, 611, 2962, 13], "temperature": 0.0, "avg_logprob": -0.21084472813557104, "compression_ratio": 1.7692307692307692, "no_speech_prob": 7.76585511630401e-06}, {"id": 812, "seek": 315948, "start": 3179.48, "end": 3183.76, "text": " And this is the author's call, this is semantically equivalent adversary.", "tokens": [400, 341, 307, 264, 3793, 311, 818, 11, 341, 307, 4361, 49505, 10344, 48222, 13], "temperature": 0.0, "avg_logprob": -0.21084472813557104, "compression_ratio": 1.7692307692307692, "no_speech_prob": 7.76585511630401e-06}, {"id": 813, "seek": 315948, "start": 3183.76, "end": 3185.84, "text": " This is pretty rough.", "tokens": [639, 307, 1238, 5903, 13], "temperature": 0.0, "avg_logprob": -0.21084472813557104, "compression_ratio": 1.7692307692307692, "no_speech_prob": 7.76585511630401e-06}, {"id": 814, "seek": 318584, "start": 3185.84, "end": 3193.04, "text": " But in general, swapping what for what in this QA model breaks it pretty frequently.", "tokens": [583, 294, 2674, 11, 1693, 10534, 437, 337, 437, 294, 341, 1249, 32, 2316, 9857, 309, 1238, 10374, 13], "temperature": 0.0, "avg_logprob": -0.18070176182960979, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.8628594261826947e-05}, {"id": 815, "seek": 318584, "start": 3193.04, "end": 3199.1600000000003, "text": " And so again, when you go back and sort of re-tinker how to build your model, you're going", "tokens": [400, 370, 797, 11, 562, 291, 352, 646, 293, 1333, 295, 319, 12, 83, 40467, 577, 281, 1322, 428, 2316, 11, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.18070176182960979, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.8628594261826947e-05}, {"id": 816, "seek": 318584, "start": 3199.1600000000003, "end": 3203.92, "text": " to be thinking about these things, not just the sort of average accuracy.", "tokens": [281, 312, 1953, 466, 613, 721, 11, 406, 445, 264, 1333, 295, 4274, 14170, 13], "temperature": 0.0, "avg_logprob": -0.18070176182960979, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.8628594261826947e-05}, {"id": 817, "seek": 318584, "start": 3203.92, "end": 3208.1200000000003, "text": " So that's sort of talking about noise.", "tokens": [407, 300, 311, 1333, 295, 1417, 466, 5658, 13], "temperature": 0.0, "avg_logprob": -0.18070176182960979, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.8628594261826947e-05}, {"id": 818, "seek": 318584, "start": 3208.1200000000003, "end": 3211.1200000000003, "text": " Our models are bus to noise and their inputs.", "tokens": [2621, 5245, 366, 1255, 281, 5658, 293, 641, 15743, 13], "temperature": 0.0, "avg_logprob": -0.18070176182960979, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.8628594261826947e-05}, {"id": 819, "seek": 318584, "start": 3211.1200000000003, "end": 3212.36, "text": " Our humans are bus to noise.", "tokens": [2621, 6255, 366, 1255, 281, 5658, 13], "temperature": 0.0, "avg_logprob": -0.18070176182960979, "compression_ratio": 1.6575342465753424, "no_speech_prob": 1.8628594261826947e-05}, {"id": 820, "seek": 321236, "start": 3212.36, "end": 3216.44, "text": " And so this is another question we can ask.", "tokens": [400, 370, 341, 307, 1071, 1168, 321, 393, 1029, 13], "temperature": 0.0, "avg_logprob": -0.2995551824569702, "compression_ratio": 1.613733905579399, "no_speech_prob": 8.217462163884193e-05}, {"id": 821, "seek": 321236, "start": 3216.44, "end": 3223.8, "text": " And so you can kind of go to this popular sort of meme passed around the internet from time", "tokens": [400, 370, 291, 393, 733, 295, 352, 281, 341, 3743, 1333, 295, 21701, 4678, 926, 264, 4705, 490, 565], "temperature": 0.0, "avg_logprob": -0.2995551824569702, "compression_ratio": 1.613733905579399, "no_speech_prob": 8.217462163884193e-05}, {"id": 822, "seek": 321236, "start": 3223.8, "end": 3229.4, "text": " to time where you have all the letters in these words scrambled, you say, according to", "tokens": [281, 565, 689, 291, 362, 439, 264, 7825, 294, 613, 2283, 49127, 11, 291, 584, 11, 4650, 281], "temperature": 0.0, "avg_logprob": -0.2995551824569702, "compression_ratio": 1.613733905579399, "no_speech_prob": 8.217462163884193e-05}, {"id": 823, "seek": 321236, "start": 3229.4, "end": 3234.92, "text": " research or Cambridge University, it doesn't matter in what order the letters in a word", "tokens": [2132, 420, 24876, 3535, 11, 309, 1177, 380, 1871, 294, 437, 1668, 264, 7825, 294, 257, 1349], "temperature": 0.0, "avg_logprob": -0.2995551824569702, "compression_ratio": 1.613733905579399, "no_speech_prob": 8.217462163884193e-05}, {"id": 824, "seek": 321236, "start": 3234.92, "end": 3235.92, "text": " are.", "tokens": [366, 13], "temperature": 0.0, "avg_logprob": -0.2995551824569702, "compression_ratio": 1.613733905579399, "no_speech_prob": 8.217462163884193e-05}, {"id": 825, "seek": 321236, "start": 3235.92, "end": 3240.4, "text": " And so it seems like, I think I did a pretty good job there.", "tokens": [400, 370, 309, 2544, 411, 11, 286, 519, 286, 630, 257, 1238, 665, 1691, 456, 13], "temperature": 0.0, "avg_logprob": -0.2995551824569702, "compression_ratio": 1.613733905579399, "no_speech_prob": 8.217462163884193e-05}, {"id": 826, "seek": 324040, "start": 3240.4, "end": 3247.28, "text": " And we can be robust as humans to reading and processing the language without actually", "tokens": [400, 321, 393, 312, 13956, 382, 6255, 281, 3760, 293, 9007, 264, 2856, 1553, 767], "temperature": 0.0, "avg_logprob": -0.180484487655315, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.133932376746088e-05}, {"id": 827, "seek": 324040, "start": 3247.28, "end": 3250.2400000000002, "text": " all that much of a difficulty.", "tokens": [439, 300, 709, 295, 257, 10360, 13], "temperature": 0.0, "avg_logprob": -0.180484487655315, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.133932376746088e-05}, {"id": 828, "seek": 324040, "start": 3250.2400000000002, "end": 3255.6, "text": " So that's maybe something that we might want our models to also be robust to.", "tokens": [407, 300, 311, 1310, 746, 300, 321, 1062, 528, 527, 5245, 281, 611, 312, 13956, 281, 13], "temperature": 0.0, "avg_logprob": -0.180484487655315, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.133932376746088e-05}, {"id": 829, "seek": 324040, "start": 3255.6, "end": 3259.28, "text": " And it's very practical as well.", "tokens": [400, 309, 311, 588, 8496, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.180484487655315, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.133932376746088e-05}, {"id": 830, "seek": 324040, "start": 3259.28, "end": 3263.7200000000003, "text": " Noise is a part of all NLP systems inputs at all times.", "tokens": [44821, 307, 257, 644, 295, 439, 426, 45196, 3652, 15743, 412, 439, 1413, 13], "temperature": 0.0, "avg_logprob": -0.180484487655315, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.133932376746088e-05}, {"id": 831, "seek": 324040, "start": 3263.7200000000003, "end": 3268.48, "text": " There's just no such thing, effectively, as having users, for example, and not having", "tokens": [821, 311, 445, 572, 1270, 551, 11, 8659, 11, 382, 1419, 5022, 11, 337, 1365, 11, 293, 406, 1419], "temperature": 0.0, "avg_logprob": -0.180484487655315, "compression_ratio": 1.5416666666666667, "no_speech_prob": 7.133932376746088e-05}, {"id": 832, "seek": 326848, "start": 3268.48, "end": 3270.72, "text": " any noise.", "tokens": [604, 5658, 13], "temperature": 0.0, "avg_logprob": -0.20736409141903833, "compression_ratio": 1.7215686274509805, "no_speech_prob": 3.425922113819979e-05}, {"id": 833, "seek": 326848, "start": 3270.72, "end": 3276.2, "text": " And so there's a study that was performed on some popular machine translation models where", "tokens": [400, 370, 456, 311, 257, 2979, 300, 390, 10332, 322, 512, 3743, 3479, 12853, 5245, 689], "temperature": 0.0, "avg_logprob": -0.20736409141903833, "compression_ratio": 1.7215686274509805, "no_speech_prob": 3.425922113819979e-05}, {"id": 834, "seek": 326848, "start": 3276.2, "end": 3282.32, "text": " you train machine translation models in French, German and Czech, I think all to English.", "tokens": [291, 3847, 3479, 12853, 5245, 294, 5522, 11, 6521, 293, 25227, 11, 286, 519, 439, 281, 3669, 13], "temperature": 0.0, "avg_logprob": -0.20736409141903833, "compression_ratio": 1.7215686274509805, "no_speech_prob": 3.425922113819979e-05}, {"id": 835, "seek": 326848, "start": 3282.32, "end": 3283.72, "text": " And you get blue scores.", "tokens": [400, 291, 483, 3344, 13444, 13], "temperature": 0.0, "avg_logprob": -0.20736409141903833, "compression_ratio": 1.7215686274509805, "no_speech_prob": 3.425922113819979e-05}, {"id": 836, "seek": 326848, "start": 3283.72, "end": 3287.2400000000002, "text": " These blue scores will look a lot better than the ones in your Simon Four because much,", "tokens": [1981, 3344, 13444, 486, 574, 257, 688, 1101, 813, 264, 2306, 294, 428, 13193, 7451, 570, 709, 11], "temperature": 0.0, "avg_logprob": -0.20736409141903833, "compression_ratio": 1.7215686274509805, "no_speech_prob": 3.425922113819979e-05}, {"id": 837, "seek": 326848, "start": 3287.2400000000002, "end": 3288.4, "text": " much more training data.", "tokens": [709, 544, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.20736409141903833, "compression_ratio": 1.7215686274509805, "no_speech_prob": 3.425922113819979e-05}, {"id": 838, "seek": 326848, "start": 3288.4, "end": 3293.16, "text": " The idea is these are actually pretty strong machine translation systems.", "tokens": [440, 1558, 307, 613, 366, 767, 1238, 2068, 3479, 12853, 3652, 13], "temperature": 0.0, "avg_logprob": -0.20736409141903833, "compression_ratio": 1.7215686274509805, "no_speech_prob": 3.425922113819979e-05}, {"id": 839, "seek": 326848, "start": 3293.16, "end": 3296.2, "text": " And that's an in domain clean text.", "tokens": [400, 300, 311, 364, 294, 9274, 2541, 2487, 13], "temperature": 0.0, "avg_logprob": -0.20736409141903833, "compression_ratio": 1.7215686274509805, "no_speech_prob": 3.425922113819979e-05}, {"id": 840, "seek": 329620, "start": 3296.2, "end": 3303.7999999999997, "text": " Now if you add character swaps like the ones we saw in that sentence about Cambridge,", "tokens": [823, 498, 291, 909, 2517, 1693, 2382, 411, 264, 2306, 321, 1866, 294, 300, 8174, 466, 24876, 11], "temperature": 0.0, "avg_logprob": -0.15210480525575834, "compression_ratio": 1.3950617283950617, "no_speech_prob": 2.4294555259984918e-05}, {"id": 841, "seek": 329620, "start": 3303.7999999999997, "end": 3307.7999999999997, "text": " the blue scores take a pretty harsh dive.", "tokens": [264, 3344, 13444, 747, 257, 1238, 14897, 9192, 13], "temperature": 0.0, "avg_logprob": -0.15210480525575834, "compression_ratio": 1.3950617283950617, "no_speech_prob": 2.4294555259984918e-05}, {"id": 842, "seek": 329620, "start": 3307.7999999999997, "end": 3309.7599999999998, "text": " Not very good.", "tokens": [1726, 588, 665, 13], "temperature": 0.0, "avg_logprob": -0.15210480525575834, "compression_ratio": 1.3950617283950617, "no_speech_prob": 2.4294555259984918e-05}, {"id": 843, "seek": 329620, "start": 3309.7599999999998, "end": 3317.3599999999997, "text": " And even if you take somewhat more natural typo noise distribution here, you'll see", "tokens": [400, 754, 498, 291, 747, 8344, 544, 3303, 2125, 78, 5658, 7316, 510, 11, 291, 603, 536], "temperature": 0.0, "avg_logprob": -0.15210480525575834, "compression_ratio": 1.3950617283950617, "no_speech_prob": 2.4294555259984918e-05}, {"id": 844, "seek": 331736, "start": 3317.36, "end": 3327.96, "text": " that you're still getting 20-ish drops in blue score through simply natural noise.", "tokens": [300, 291, 434, 920, 1242, 945, 12, 742, 11438, 294, 3344, 6175, 807, 2935, 3303, 5658, 13], "temperature": 0.0, "avg_logprob": -0.1686661870856034, "compression_ratio": 1.7114624505928853, "no_speech_prob": 2.1107452994328924e-05}, {"id": 845, "seek": 331736, "start": 3327.96, "end": 3330.96, "text": " And so maybe you'll go back and retrain the model on more types of noise.", "tokens": [400, 370, 1310, 291, 603, 352, 646, 293, 1533, 7146, 264, 2316, 322, 544, 3467, 295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.1686661870856034, "compression_ratio": 1.7114624505928853, "no_speech_prob": 2.1107452994328924e-05}, {"id": 846, "seek": 331736, "start": 3330.96, "end": 3332.6400000000003, "text": " And then you ask, oh, I do that.", "tokens": [400, 550, 291, 1029, 11, 1954, 11, 286, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1686661870856034, "compression_ratio": 1.7114624505928853, "no_speech_prob": 2.1107452994328924e-05}, {"id": 847, "seek": 331736, "start": 3332.6400000000003, "end": 3335.2400000000002, "text": " Is it robust to even different kinds of noise?", "tokens": [1119, 309, 13956, 281, 754, 819, 3685, 295, 5658, 30], "temperature": 0.0, "avg_logprob": -0.1686661870856034, "compression_ratio": 1.7114624505928853, "no_speech_prob": 2.1107452994328924e-05}, {"id": 848, "seek": 331736, "start": 3335.2400000000002, "end": 3337.6400000000003, "text": " These are the questions that are going to be really important.", "tokens": [1981, 366, 264, 1651, 300, 366, 516, 281, 312, 534, 1021, 13], "temperature": 0.0, "avg_logprob": -0.1686661870856034, "compression_ratio": 1.7114624505928853, "no_speech_prob": 2.1107452994328924e-05}, {"id": 849, "seek": 331736, "start": 3337.6400000000003, "end": 3341.44, "text": " And it's important to know that you're able to break your model really easily so that", "tokens": [400, 309, 311, 1021, 281, 458, 300, 291, 434, 1075, 281, 1821, 428, 2316, 534, 3612, 370, 300], "temperature": 0.0, "avg_logprob": -0.1686661870856034, "compression_ratio": 1.7114624505928853, "no_speech_prob": 2.1107452994328924e-05}, {"id": 850, "seek": 331736, "start": 3341.44, "end": 3345.4, "text": " you can then go and try to make it more robust.", "tokens": [291, 393, 550, 352, 293, 853, 281, 652, 309, 544, 13956, 13], "temperature": 0.0, "avg_logprob": -0.1686661870856034, "compression_ratio": 1.7114624505928853, "no_speech_prob": 2.1107452994328924e-05}, {"id": 851, "seek": 334540, "start": 3345.4, "end": 3351.28, "text": " OK, now, let's see, 20 minutes.", "tokens": [2264, 11, 586, 11, 718, 311, 536, 11, 945, 2077, 13], "temperature": 0.0, "avg_logprob": -0.27236060092323705, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.482284854631871e-05}, {"id": 852, "seek": 334540, "start": 3351.28, "end": 3353.52, "text": " Some.", "tokens": [2188, 13], "temperature": 0.0, "avg_logprob": -0.27236060092323705, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.482284854631871e-05}, {"id": 853, "seek": 334540, "start": 3353.52, "end": 3357.64, "text": " Now we're going to, I guess, yeah.", "tokens": [823, 321, 434, 516, 281, 11, 286, 2041, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.27236060092323705, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.482284854631871e-05}, {"id": 854, "seek": 334540, "start": 3357.64, "end": 3361.64, "text": " So now we're going to look at the representations of our neural networks.", "tokens": [407, 586, 321, 434, 516, 281, 574, 412, 264, 33358, 295, 527, 18161, 9590, 13], "temperature": 0.0, "avg_logprob": -0.27236060092323705, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.482284854631871e-05}, {"id": 855, "seek": 334540, "start": 3361.64, "end": 3366.36, "text": " We've talked about sort of their behavior and then whether we could sort of change or", "tokens": [492, 600, 2825, 466, 1333, 295, 641, 5223, 293, 550, 1968, 321, 727, 1333, 295, 1319, 420], "temperature": 0.0, "avg_logprob": -0.27236060092323705, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.482284854631871e-05}, {"id": 856, "seek": 334540, "start": 3366.36, "end": 3369.12, "text": " observe reasons behind their behavior.", "tokens": [11441, 4112, 2261, 641, 5223, 13], "temperature": 0.0, "avg_logprob": -0.27236060092323705, "compression_ratio": 1.5055555555555555, "no_speech_prob": 7.482284854631871e-05}, {"id": 857, "seek": 336912, "start": 3369.12, "end": 3375.8399999999997, "text": " Now we'll go into less abstraction, like more at the actual vector representations that", "tokens": [823, 321, 603, 352, 666, 1570, 37765, 11, 411, 544, 412, 264, 3539, 8062, 33358, 300], "temperature": 0.0, "avg_logprob": -0.16102118293444315, "compression_ratio": 1.6282527881040891, "no_speech_prob": 2.045543988060672e-05}, {"id": 858, "seek": 336912, "start": 3375.8399999999997, "end": 3377.48, "text": " are being built by models.", "tokens": [366, 885, 3094, 538, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16102118293444315, "compression_ratio": 1.6282527881040891, "no_speech_prob": 2.045543988060672e-05}, {"id": 859, "seek": 336912, "start": 3377.48, "end": 3384.2, "text": " And we can answer a different kind of question at the very least than with the other studies.", "tokens": [400, 321, 393, 1867, 257, 819, 733, 295, 1168, 412, 264, 588, 1935, 813, 365, 264, 661, 5313, 13], "temperature": 0.0, "avg_logprob": -0.16102118293444315, "compression_ratio": 1.6282527881040891, "no_speech_prob": 2.045543988060672e-05}, {"id": 860, "seek": 336912, "start": 3384.2, "end": 3390.2, "text": " The first thing is related to the question I was asked about attention, which is that", "tokens": [440, 700, 551, 307, 4077, 281, 264, 1168, 286, 390, 2351, 466, 3202, 11, 597, 307, 300], "temperature": 0.0, "avg_logprob": -0.16102118293444315, "compression_ratio": 1.6282527881040891, "no_speech_prob": 2.045543988060672e-05}, {"id": 861, "seek": 336912, "start": 3390.2, "end": 3393.72, "text": " some modeling components lend themselves to inspection.", "tokens": [512, 15983, 6677, 21774, 2969, 281, 22085, 13], "temperature": 0.0, "avg_logprob": -0.16102118293444315, "compression_ratio": 1.6282527881040891, "no_speech_prob": 2.045543988060672e-05}, {"id": 862, "seek": 336912, "start": 3393.72, "end": 3397.68, "text": " Now this is a sentence that I chose somewhat carefully actually because in part of this", "tokens": [823, 341, 307, 257, 8174, 300, 286, 5111, 8344, 7500, 767, 570, 294, 644, 295, 341], "temperature": 0.0, "avg_logprob": -0.16102118293444315, "compression_ratio": 1.6282527881040891, "no_speech_prob": 2.045543988060672e-05}, {"id": 863, "seek": 339768, "start": 3397.68, "end": 3402.12, "text": " debate, are they interpretable components?", "tokens": [7958, 11, 366, 436, 7302, 712, 6677, 30], "temperature": 0.0, "avg_logprob": -0.1974015235900879, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.4674860469531268e-05}, {"id": 864, "seek": 339768, "start": 3402.12, "end": 3403.3599999999997, "text": " We'll see.", "tokens": [492, 603, 536, 13], "temperature": 0.0, "avg_logprob": -0.1974015235900879, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.4674860469531268e-05}, {"id": 865, "seek": 339768, "start": 3403.3599999999997, "end": 3406.64, "text": " But they lend themselves to inspection in the following way.", "tokens": [583, 436, 21774, 2969, 281, 22085, 294, 264, 3480, 636, 13], "temperature": 0.0, "avg_logprob": -0.1974015235900879, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.4674860469531268e-05}, {"id": 866, "seek": 339768, "start": 3406.64, "end": 3411.7599999999998, "text": " You can visualize them well and you can correlate them easily with various properties.", "tokens": [509, 393, 23273, 552, 731, 293, 291, 393, 48742, 552, 3612, 365, 3683, 7221, 13], "temperature": 0.0, "avg_logprob": -0.1974015235900879, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.4674860469531268e-05}, {"id": 867, "seek": 339768, "start": 3411.7599999999998, "end": 3413.8799999999997, "text": " So let's say you have attention heads in Burt.", "tokens": [407, 718, 311, 584, 291, 362, 3202, 8050, 294, 363, 6224, 13], "temperature": 0.0, "avg_logprob": -0.1974015235900879, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.4674860469531268e-05}, {"id": 868, "seek": 339768, "start": 3413.8799999999997, "end": 3420.16, "text": " This is from a really nice study that was done here where you look at attention heads", "tokens": [639, 307, 490, 257, 534, 1481, 2979, 300, 390, 1096, 510, 689, 291, 574, 412, 3202, 8050], "temperature": 0.0, "avg_logprob": -0.1974015235900879, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.4674860469531268e-05}, {"id": 869, "seek": 339768, "start": 3420.16, "end": 3425.9199999999996, "text": " of Burt and you say, on most sentences, this attention head had one one seems to do this", "tokens": [295, 363, 6224, 293, 291, 584, 11, 322, 881, 16579, 11, 341, 3202, 1378, 632, 472, 472, 2544, 281, 360, 341], "temperature": 0.0, "avg_logprob": -0.1974015235900879, "compression_ratio": 1.6459143968871595, "no_speech_prob": 2.4674860469531268e-05}, {"id": 870, "seek": 342592, "start": 3425.92, "end": 3428.6800000000003, "text": " very sort of global aggregation.", "tokens": [588, 1333, 295, 4338, 16743, 399, 13], "temperature": 0.0, "avg_logprob": -0.18136343089017, "compression_ratio": 1.4694835680751173, "no_speech_prob": 2.4291623049066402e-05}, {"id": 871, "seek": 342592, "start": 3428.6800000000003, "end": 3431.96, "text": " Simple kind of operation does this pretty consistently.", "tokens": [21532, 733, 295, 6916, 775, 341, 1238, 14961, 13], "temperature": 0.0, "avg_logprob": -0.18136343089017, "compression_ratio": 1.4694835680751173, "no_speech_prob": 2.4291623049066402e-05}, {"id": 872, "seek": 342592, "start": 3431.96, "end": 3433.84, "text": " That's cool.", "tokens": [663, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.18136343089017, "compression_ratio": 1.4694835680751173, "no_speech_prob": 2.4291623049066402e-05}, {"id": 873, "seek": 342592, "start": 3433.84, "end": 3436.04, "text": " Is it interpretable?", "tokens": [1119, 309, 7302, 712, 30], "temperature": 0.0, "avg_logprob": -0.18136343089017, "compression_ratio": 1.4694835680751173, "no_speech_prob": 2.4291623049066402e-05}, {"id": 874, "seek": 342592, "start": 3436.04, "end": 3438.56, "text": " Well, maybe, right?", "tokens": [1042, 11, 1310, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18136343089017, "compression_ratio": 1.4694835680751173, "no_speech_prob": 2.4291623049066402e-05}, {"id": 875, "seek": 342592, "start": 3438.56, "end": 3445.08, "text": " So it's the first layer, which means that this word found is sort of uncontextualized.", "tokens": [407, 309, 311, 264, 700, 4583, 11, 597, 1355, 300, 341, 1349, 1352, 307, 1333, 295, 36019, 3828, 901, 1602, 13], "temperature": 0.0, "avg_logprob": -0.18136343089017, "compression_ratio": 1.4694835680751173, "no_speech_prob": 2.4291623049066402e-05}, {"id": 876, "seek": 342592, "start": 3445.08, "end": 3451.44, "text": " And then, you know, but in deeper layers, the problem is that like once you do some", "tokens": [400, 550, 11, 291, 458, 11, 457, 294, 7731, 7914, 11, 264, 1154, 307, 300, 411, 1564, 291, 360, 512], "temperature": 0.0, "avg_logprob": -0.18136343089017, "compression_ratio": 1.4694835680751173, "no_speech_prob": 2.4291623049066402e-05}, {"id": 877, "seek": 345144, "start": 3451.44, "end": 3457.08, "text": " rounds of attention, you've had information mixing and flowing between words.", "tokens": [13757, 295, 3202, 11, 291, 600, 632, 1589, 11983, 293, 13974, 1296, 2283, 13], "temperature": 0.0, "avg_logprob": -0.14138292759022814, "compression_ratio": 1.6722689075630253, "no_speech_prob": 3.168065450154245e-05}, {"id": 878, "seek": 345144, "start": 3457.08, "end": 3460.68, "text": " And how do you know exactly what information you're combining, what you're attending", "tokens": [400, 577, 360, 291, 458, 2293, 437, 1589, 291, 434, 21928, 11, 437, 291, 434, 15862], "temperature": 0.0, "avg_logprob": -0.14138292759022814, "compression_ratio": 1.6722689075630253, "no_speech_prob": 3.168065450154245e-05}, {"id": 879, "seek": 345144, "start": 3460.68, "end": 3464.88, "text": " to, even, the little hard to tell.", "tokens": [281, 11, 754, 11, 264, 707, 1152, 281, 980, 13], "temperature": 0.0, "avg_logprob": -0.14138292759022814, "compression_ratio": 1.6722689075630253, "no_speech_prob": 3.168065450154245e-05}, {"id": 880, "seek": 345144, "start": 3464.88, "end": 3470.2400000000002, "text": " And saliency methods more directly sort of evaluate the importance of models.", "tokens": [400, 1845, 7848, 7150, 544, 3838, 1333, 295, 13059, 264, 7379, 295, 5245, 13], "temperature": 0.0, "avg_logprob": -0.14138292759022814, "compression_ratio": 1.6722689075630253, "no_speech_prob": 3.168065450154245e-05}, {"id": 881, "seek": 345144, "start": 3470.2400000000002, "end": 3474.56, "text": " But it's still interesting to see at sort of a local mechanistic point of view what", "tokens": [583, 309, 311, 920, 1880, 281, 536, 412, 1333, 295, 257, 2654, 4236, 3142, 935, 295, 1910, 437], "temperature": 0.0, "avg_logprob": -0.14138292759022814, "compression_ratio": 1.6722689075630253, "no_speech_prob": 3.168065450154245e-05}, {"id": 882, "seek": 345144, "start": 3474.56, "end": 3477.2000000000003, "text": " kinds of things are being attended to.", "tokens": [3685, 295, 721, 366, 885, 15990, 281, 13], "temperature": 0.0, "avg_logprob": -0.14138292759022814, "compression_ratio": 1.6722689075630253, "no_speech_prob": 3.168065450154245e-05}, {"id": 883, "seek": 347720, "start": 3477.2, "end": 3481.7999999999997, "text": " So let's take another example.", "tokens": [407, 718, 311, 747, 1071, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 884, "seek": 347720, "start": 3481.7999999999997, "end": 3482.56, "text": " Some attention heads seem to perform simple operations.", "tokens": [2188, 3202, 8050, 1643, 281, 2042, 2199, 7705, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 885, "seek": 347720, "start": 3482.56, "end": 3485.68, "text": " So you have the global aggregation here that we saw already.", "tokens": [407, 291, 362, 264, 4338, 16743, 399, 510, 300, 321, 1866, 1217, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 886, "seek": 347720, "start": 3485.68, "end": 3489.48, "text": " Others seem to attend pretty robustly to the next token.", "tokens": [20277, 1643, 281, 6888, 1238, 13956, 356, 281, 264, 958, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 887, "seek": 347720, "start": 3489.48, "end": 3490.48, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 888, "seek": 347720, "start": 3490.48, "end": 3492.0, "text": " Next token is a great signal.", "tokens": [3087, 14862, 307, 257, 869, 6358, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 889, "seek": 347720, "start": 3492.0, "end": 3494.96, "text": " Some heads attend to the CEP token.", "tokens": [2188, 8050, 6888, 281, 264, 383, 8929, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 890, "seek": 347720, "start": 3494.96, "end": 3497.0, "text": " So here you have attending to CEP.", "tokens": [407, 510, 291, 362, 15862, 281, 383, 8929, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 891, "seek": 347720, "start": 3497.0, "end": 3498.9199999999996, "text": " And then maybe some attend to periods.", "tokens": [400, 550, 1310, 512, 6888, 281, 13804, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 892, "seek": 347720, "start": 3498.9199999999996, "end": 3503.3999999999996, "text": " Maybe that's sort of a splitting sentences together and things like that.", "tokens": [2704, 300, 311, 1333, 295, 257, 30348, 16579, 1214, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 893, "seek": 347720, "start": 3503.3999999999996, "end": 3505.0, "text": " Not things that are hard to do.", "tokens": [1726, 721, 300, 366, 1152, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1834851092979556, "compression_ratio": 1.7743190661478598, "no_speech_prob": 1.5685487596783787e-05}, {"id": 894, "seek": 350500, "start": 3505.0, "end": 3510.68, "text": " But things that some attention had seemed to pretty robustly perform.", "tokens": [583, 721, 300, 512, 3202, 632, 6576, 281, 1238, 13956, 356, 2042, 13], "temperature": 0.0, "avg_logprob": -0.24382228386111376, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.110459899995476e-05}, {"id": 895, "seek": 350500, "start": 3510.68, "end": 3515.84, "text": " Again now though, deep in the network, what's actually represented at this period at layer", "tokens": [3764, 586, 1673, 11, 2452, 294, 264, 3209, 11, 437, 311, 767, 10379, 412, 341, 2896, 412, 4583], "temperature": 0.0, "avg_logprob": -0.24382228386111376, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.110459899995476e-05}, {"id": 896, "seek": 350500, "start": 3515.84, "end": 3517.88, "text": " 11?", "tokens": [2975, 30], "temperature": 0.0, "avg_logprob": -0.24382228386111376, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.110459899995476e-05}, {"id": 897, "seek": 350500, "start": 3517.88, "end": 3518.88, "text": " Little unclear.", "tokens": [8022, 25636, 13], "temperature": 0.0, "avg_logprob": -0.24382228386111376, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.110459899995476e-05}, {"id": 898, "seek": 350500, "start": 3518.88, "end": 3519.88, "text": " Little unclear.", "tokens": [8022, 25636, 13], "temperature": 0.0, "avg_logprob": -0.24382228386111376, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.110459899995476e-05}, {"id": 899, "seek": 350500, "start": 3519.88, "end": 3521.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.24382228386111376, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.110459899995476e-05}, {"id": 900, "seek": 350500, "start": 3521.4, "end": 3526.08, "text": " So some heads though are correlated with really interesting linguistic properties.", "tokens": [407, 512, 8050, 1673, 366, 38574, 365, 534, 1880, 43002, 7221, 13], "temperature": 0.0, "avg_logprob": -0.24382228386111376, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.110459899995476e-05}, {"id": 901, "seek": 350500, "start": 3526.08, "end": 3529.92, "text": " So this head is actually attending to noun modifiers.", "tokens": [407, 341, 1378, 307, 767, 15862, 281, 23307, 1072, 23463, 13], "temperature": 0.0, "avg_logprob": -0.24382228386111376, "compression_ratio": 1.5622119815668203, "no_speech_prob": 2.110459899995476e-05}, {"id": 902, "seek": 352992, "start": 3529.92, "end": 3537.6800000000003, "text": " So you got this the complicated language in the huge new law.", "tokens": [407, 291, 658, 341, 264, 6179, 2856, 294, 264, 2603, 777, 2101, 13], "temperature": 0.0, "avg_logprob": -0.15324056884388865, "compression_ratio": 1.555045871559633, "no_speech_prob": 2.318303450010717e-05}, {"id": 903, "seek": 352992, "start": 3537.6800000000003, "end": 3540.2000000000003, "text": " That's pretty fascinating.", "tokens": [663, 311, 1238, 10343, 13], "temperature": 0.0, "avg_logprob": -0.15324056884388865, "compression_ratio": 1.555045871559633, "no_speech_prob": 2.318303450010717e-05}, {"id": 904, "seek": 352992, "start": 3540.2000000000003, "end": 3545.96, "text": " Even if the model is not like doing this as a causal mechanism to do syntax necessarily,", "tokens": [2754, 498, 264, 2316, 307, 406, 411, 884, 341, 382, 257, 38755, 7513, 281, 360, 28431, 4725, 11], "temperature": 0.0, "avg_logprob": -0.15324056884388865, "compression_ratio": 1.555045871559633, "no_speech_prob": 2.318303450010717e-05}, {"id": 905, "seek": 352992, "start": 3545.96, "end": 3550.04, "text": " the fact that these things so strongly correlate is actually pretty, pretty cool.", "tokens": [264, 1186, 300, 613, 721, 370, 10613, 48742, 307, 767, 1238, 11, 1238, 1627, 13], "temperature": 0.0, "avg_logprob": -0.15324056884388865, "compression_ratio": 1.555045871559633, "no_speech_prob": 2.318303450010717e-05}, {"id": 906, "seek": 352992, "start": 3550.04, "end": 3553.26, "text": " And so what we have in all of these studies is we've got sort of an approximate", "tokens": [400, 370, 437, 321, 362, 294, 439, 295, 613, 5313, 307, 321, 600, 658, 1333, 295, 364, 30874], "temperature": 0.0, "avg_logprob": -0.15324056884388865, "compression_ratio": 1.555045871559633, "no_speech_prob": 2.318303450010717e-05}, {"id": 907, "seek": 355326, "start": 3553.26, "end": 3559.96, "text": " return partition and quantitative analysis relating, like allowing us to reason about very", "tokens": [2736, 24808, 293, 27778, 5215, 23968, 11, 411, 8293, 505, 281, 1778, 466, 588], "temperature": 0.0, "avg_logprob": -0.21806218889024523, "compression_ratio": 1.6, "no_speech_prob": 2.392039095866494e-05}, {"id": 908, "seek": 355326, "start": 3559.96, "end": 3561.5200000000004, "text": " complicated model behavior.", "tokens": [6179, 2316, 5223, 13], "temperature": 0.0, "avg_logprob": -0.21806218889024523, "compression_ratio": 1.6, "no_speech_prob": 2.392039095866494e-05}, {"id": 909, "seek": 355326, "start": 3561.5200000000004, "end": 3564.88, "text": " They're all approximations, but they're definitely interesting.", "tokens": [814, 434, 439, 8542, 763, 11, 457, 436, 434, 2138, 1880, 13], "temperature": 0.0, "avg_logprob": -0.21806218889024523, "compression_ratio": 1.6, "no_speech_prob": 2.392039095866494e-05}, {"id": 910, "seek": 355326, "start": 3564.88, "end": 3566.7200000000003, "text": " One other example is that of co-reference.", "tokens": [1485, 661, 1365, 307, 300, 295, 598, 12, 265, 5158, 13], "temperature": 0.0, "avg_logprob": -0.21806218889024523, "compression_ratio": 1.6, "no_speech_prob": 2.392039095866494e-05}, {"id": 911, "seek": 355326, "start": 3566.7200000000003, "end": 3569.92, "text": " So we saw some work on co-reference.", "tokens": [407, 321, 1866, 512, 589, 322, 598, 12, 265, 5158, 13], "temperature": 0.0, "avg_logprob": -0.21806218889024523, "compression_ratio": 1.6, "no_speech_prob": 2.392039095866494e-05}, {"id": 912, "seek": 355326, "start": 3569.92, "end": 3576.6800000000003, "text": " And it seems like this head does a pretty okay job of actually matching up co-referent", "tokens": [400, 309, 2544, 411, 341, 1378, 775, 257, 1238, 1392, 1691, 295, 767, 14324, 493, 598, 12, 265, 612, 317], "temperature": 0.0, "avg_logprob": -0.21806218889024523, "compression_ratio": 1.6, "no_speech_prob": 2.392039095866494e-05}, {"id": 913, "seek": 355326, "start": 3576.6800000000003, "end": 3577.6800000000003, "text": " entities.", "tokens": [16667, 13], "temperature": 0.0, "avg_logprob": -0.21806218889024523, "compression_ratio": 1.6, "no_speech_prob": 2.392039095866494e-05}, {"id": 914, "seek": 355326, "start": 3577.6800000000003, "end": 3579.2000000000003, "text": " These are in red.", "tokens": [1981, 366, 294, 2182, 13], "temperature": 0.0, "avg_logprob": -0.21806218889024523, "compression_ratio": 1.6, "no_speech_prob": 2.392039095866494e-05}, {"id": 915, "seek": 355326, "start": 3579.2000000000003, "end": 3582.1200000000003, "text": " Talks, negotiations, she, her.", "tokens": [8780, 82, 11, 20476, 11, 750, 11, 720, 13], "temperature": 0.0, "avg_logprob": -0.21806218889024523, "compression_ratio": 1.6, "no_speech_prob": 2.392039095866494e-05}, {"id": 916, "seek": 358212, "start": 3582.12, "end": 3583.88, "text": " And that's not obvious how to do that.", "tokens": [400, 300, 311, 406, 6322, 577, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.13286997608302795, "compression_ratio": 1.5634920634920635, "no_speech_prob": 5.0144162742071785e-06}, {"id": 917, "seek": 358212, "start": 3583.88, "end": 3585.68, "text": " This is a difficult task.", "tokens": [639, 307, 257, 2252, 5633, 13], "temperature": 0.0, "avg_logprob": -0.13286997608302795, "compression_ratio": 1.5634920634920635, "no_speech_prob": 5.0144162742071785e-06}, {"id": 918, "seek": 358212, "start": 3585.68, "end": 3590.08, "text": " And so it does so with some percentage of the time.", "tokens": [400, 370, 309, 775, 370, 365, 512, 9668, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.13286997608302795, "compression_ratio": 1.5634920634920635, "no_speech_prob": 5.0144162742071785e-06}, {"id": 919, "seek": 358212, "start": 3590.08, "end": 3596.48, "text": " And again, it's sort of connecting very complex model behavior to these sort of interpretable", "tokens": [400, 797, 11, 309, 311, 1333, 295, 11015, 588, 3997, 2316, 5223, 281, 613, 1333, 295, 7302, 712], "temperature": 0.0, "avg_logprob": -0.13286997608302795, "compression_ratio": 1.5634920634920635, "no_speech_prob": 5.0144162742071785e-06}, {"id": 920, "seek": 358212, "start": 3596.48, "end": 3600.4, "text": " summaries of correlating properties.", "tokens": [8367, 4889, 295, 13983, 990, 7221, 13], "temperature": 0.0, "avg_logprob": -0.13286997608302795, "compression_ratio": 1.5634920634920635, "no_speech_prob": 5.0144162742071785e-06}, {"id": 921, "seek": 358212, "start": 3600.4, "end": 3604.64, "text": " Other cases you can have individual hidden units that lend themselves to interpretation.", "tokens": [5358, 3331, 291, 393, 362, 2609, 7633, 6815, 300, 21774, 2969, 281, 14174, 13], "temperature": 0.0, "avg_logprob": -0.13286997608302795, "compression_ratio": 1.5634920634920635, "no_speech_prob": 5.0144162742071785e-06}, {"id": 922, "seek": 358212, "start": 3604.64, "end": 3610.44, "text": " So here you've got a character level LSTM language model.", "tokens": [407, 510, 291, 600, 658, 257, 2517, 1496, 441, 6840, 44, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13286997608302795, "compression_ratio": 1.5634920634920635, "no_speech_prob": 5.0144162742071785e-06}, {"id": 923, "seek": 361044, "start": 3610.44, "end": 3614.2000000000003, "text": " Which row here is a sentence, if you can't read it, it's totally okay.", "tokens": [3013, 5386, 510, 307, 257, 8174, 11, 498, 291, 393, 380, 1401, 309, 11, 309, 311, 3879, 1392, 13], "temperature": 0.0, "avg_logprob": -0.15814957795319734, "compression_ratio": 1.7754237288135593, "no_speech_prob": 5.063643038738519e-05}, {"id": 924, "seek": 361044, "start": 3614.2000000000003, "end": 3618.04, "text": " The interpretation that you should take is that as we walk along the sentence, this single", "tokens": [440, 14174, 300, 291, 820, 747, 307, 300, 382, 321, 1792, 2051, 264, 8174, 11, 341, 2167], "temperature": 0.0, "avg_logprob": -0.15814957795319734, "compression_ratio": 1.7754237288135593, "no_speech_prob": 5.063643038738519e-05}, {"id": 925, "seek": 361044, "start": 3618.04, "end": 3623.28, "text": " unit is going from I think very negative to very positive or very positive to very", "tokens": [4985, 307, 516, 490, 286, 519, 588, 3671, 281, 588, 3353, 420, 588, 3353, 281, 588], "temperature": 0.0, "avg_logprob": -0.15814957795319734, "compression_ratio": 1.7754237288135593, "no_speech_prob": 5.063643038738519e-05}, {"id": 926, "seek": 361044, "start": 3623.28, "end": 3624.28, "text": " negative.", "tokens": [3671, 13], "temperature": 0.0, "avg_logprob": -0.15814957795319734, "compression_ratio": 1.7754237288135593, "no_speech_prob": 5.063643038738519e-05}, {"id": 927, "seek": 361044, "start": 3624.28, "end": 3626.6, "text": " I don't really remember.", "tokens": [286, 500, 380, 534, 1604, 13], "temperature": 0.0, "avg_logprob": -0.15814957795319734, "compression_ratio": 1.7754237288135593, "no_speech_prob": 5.063643038738519e-05}, {"id": 928, "seek": 361044, "start": 3626.6, "end": 3630.2000000000003, "text": " But it's tracking the position in the line.", "tokens": [583, 309, 311, 11603, 264, 2535, 294, 264, 1622, 13], "temperature": 0.0, "avg_logprob": -0.15814957795319734, "compression_ratio": 1.7754237288135593, "no_speech_prob": 5.063643038738519e-05}, {"id": 929, "seek": 361044, "start": 3630.2000000000003, "end": 3634.88, "text": " So it's just a linear position unit and pretty robustly doing so across all of these", "tokens": [407, 309, 311, 445, 257, 8213, 2535, 4985, 293, 1238, 13956, 356, 884, 370, 2108, 439, 295, 613], "temperature": 0.0, "avg_logprob": -0.15814957795319734, "compression_ratio": 1.7754237288135593, "no_speech_prob": 5.063643038738519e-05}, {"id": 930, "seek": 361044, "start": 3634.88, "end": 3636.64, "text": " sentences.", "tokens": [16579, 13], "temperature": 0.0, "avg_logprob": -0.15814957795319734, "compression_ratio": 1.7754237288135593, "no_speech_prob": 5.063643038738519e-05}, {"id": 931, "seek": 363664, "start": 3636.64, "end": 3642.04, "text": " So this is from a nice visualization study way back in 2016, way back.", "tokens": [407, 341, 307, 490, 257, 1481, 25801, 2979, 636, 646, 294, 6549, 11, 636, 646, 13], "temperature": 0.0, "avg_logprob": -0.1940292011607777, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.341831375611946e-05}, {"id": 932, "seek": 363664, "start": 3642.04, "end": 3647.92, "text": " Here's another cell from that same LSTM language model that seems to sort of turn on inside", "tokens": [1692, 311, 1071, 2815, 490, 300, 912, 441, 6840, 44, 2856, 2316, 300, 2544, 281, 1333, 295, 1261, 322, 1854], "temperature": 0.0, "avg_logprob": -0.1940292011607777, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.341831375611946e-05}, {"id": 933, "seek": 363664, "start": 3647.92, "end": 3648.92, "text": " quotes.", "tokens": [19963, 13], "temperature": 0.0, "avg_logprob": -0.1940292011607777, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.341831375611946e-05}, {"id": 934, "seek": 363664, "start": 3648.92, "end": 3650.8799999999997, "text": " So here's a quote and then it turns on.", "tokens": [407, 510, 311, 257, 6513, 293, 550, 309, 4523, 322, 13], "temperature": 0.0, "avg_logprob": -0.1940292011607777, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.341831375611946e-05}, {"id": 935, "seek": 363664, "start": 3650.8799999999997, "end": 3653.2799999999997, "text": " Okay, so I guess that's positive in the blue.", "tokens": [1033, 11, 370, 286, 2041, 300, 311, 3353, 294, 264, 3344, 13], "temperature": 0.0, "avg_logprob": -0.1940292011607777, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.341831375611946e-05}, {"id": 936, "seek": 363664, "start": 3653.2799999999997, "end": 3655.68, "text": " End quote here.", "tokens": [6967, 6513, 510, 13], "temperature": 0.0, "avg_logprob": -0.1940292011607777, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.341831375611946e-05}, {"id": 937, "seek": 363664, "start": 3655.68, "end": 3657.3199999999997, "text": " And then it's negative.", "tokens": [400, 550, 309, 311, 3671, 13], "temperature": 0.0, "avg_logprob": -0.1940292011607777, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.341831375611946e-05}, {"id": 938, "seek": 363664, "start": 3657.3199999999997, "end": 3661.0, "text": " Here you start with no quote, negative in the red.", "tokens": [1692, 291, 722, 365, 572, 6513, 11, 3671, 294, 264, 2182, 13], "temperature": 0.0, "avg_logprob": -0.1940292011607777, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.341831375611946e-05}, {"id": 939, "seek": 363664, "start": 3661.0, "end": 3663.96, "text": " See a quote and then blue.", "tokens": [3008, 257, 6513, 293, 550, 3344, 13], "temperature": 0.0, "avg_logprob": -0.1940292011607777, "compression_ratio": 1.6331877729257642, "no_speech_prob": 1.341831375611946e-05}, {"id": 940, "seek": 366396, "start": 3663.96, "end": 3668.08, "text": " Again, very interpretable, also potentially a very useful feature to keep in mind.", "tokens": [3764, 11, 588, 7302, 712, 11, 611, 7263, 257, 588, 4420, 4111, 281, 1066, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.19027654259605745, "compression_ratio": 1.6037037037037036, "no_speech_prob": 5.3061470680404454e-05}, {"id": 941, "seek": 366396, "start": 3668.08, "end": 3671.84, "text": " And this is just an individual unit in the LSTM that you can just look at and see that", "tokens": [400, 341, 307, 445, 364, 2609, 4985, 294, 264, 441, 6840, 44, 300, 291, 393, 445, 574, 412, 293, 536, 300], "temperature": 0.0, "avg_logprob": -0.19027654259605745, "compression_ratio": 1.6037037037037036, "no_speech_prob": 5.3061470680404454e-05}, {"id": 942, "seek": 366396, "start": 3671.84, "end": 3673.0, "text": " it does this.", "tokens": [309, 775, 341, 13], "temperature": 0.0, "avg_logprob": -0.19027654259605745, "compression_ratio": 1.6037037037037036, "no_speech_prob": 5.3061470680404454e-05}, {"id": 943, "seek": 366396, "start": 3673.0, "end": 3677.8, "text": " Very, very interesting.", "tokens": [4372, 11, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.19027654259605745, "compression_ratio": 1.6037037037037036, "no_speech_prob": 5.3061470680404454e-05}, {"id": 944, "seek": 366396, "start": 3677.8, "end": 3686.2, "text": " Even farther on this, and this is actually a study by some AI and neuroscience researchers,", "tokens": [2754, 20344, 322, 341, 11, 293, 341, 307, 767, 257, 2979, 538, 512, 7318, 293, 42762, 10309, 11], "temperature": 0.0, "avg_logprob": -0.19027654259605745, "compression_ratio": 1.6037037037037036, "no_speech_prob": 5.3061470680404454e-05}, {"id": 945, "seek": 366396, "start": 3686.2, "end": 3689.68, "text": " we saw the LSTMs were good at subject for a number agreement.", "tokens": [321, 1866, 264, 441, 6840, 26386, 645, 665, 412, 3983, 337, 257, 1230, 8106, 13], "temperature": 0.0, "avg_logprob": -0.19027654259605745, "compression_ratio": 1.6037037037037036, "no_speech_prob": 5.3061470680404454e-05}, {"id": 946, "seek": 366396, "start": 3689.68, "end": 3693.0, "text": " Can we figure out the mechanisms by which the LSTM is solving the task?", "tokens": [1664, 321, 2573, 484, 264, 15902, 538, 597, 264, 441, 6840, 44, 307, 12606, 264, 5633, 30], "temperature": 0.0, "avg_logprob": -0.19027654259605745, "compression_ratio": 1.6037037037037036, "no_speech_prob": 5.3061470680404454e-05}, {"id": 947, "seek": 369300, "start": 3693.0, "end": 3695.04, "text": " We actually get some insight into that.", "tokens": [492, 767, 483, 512, 11269, 666, 300, 13], "temperature": 0.0, "avg_logprob": -0.17513424774696087, "compression_ratio": 1.7341269841269842, "no_speech_prob": 9.972354746423662e-06}, {"id": 948, "seek": 369300, "start": 3695.04, "end": 3697.76, "text": " And so we have a word level language model.", "tokens": [400, 370, 321, 362, 257, 1349, 1496, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17513424774696087, "compression_ratio": 1.7341269841269842, "no_speech_prob": 9.972354746423662e-06}, {"id": 949, "seek": 369300, "start": 3697.76, "end": 3701.4, "text": " The word level language model is going to be a little small, but you have a sentence,", "tokens": [440, 1349, 1496, 2856, 2316, 307, 516, 281, 312, 257, 707, 1359, 11, 457, 291, 362, 257, 8174, 11], "temperature": 0.0, "avg_logprob": -0.17513424774696087, "compression_ratio": 1.7341269841269842, "no_speech_prob": 9.972354746423662e-06}, {"id": 950, "seek": 369300, "start": 3701.4, "end": 3705.64, "text": " the boy, gently, and kindly greets the.", "tokens": [264, 3237, 11, 13073, 11, 293, 29736, 6066, 1385, 264, 13], "temperature": 0.0, "avg_logprob": -0.17513424774696087, "compression_ratio": 1.7341269841269842, "no_speech_prob": 9.972354746423662e-06}, {"id": 951, "seek": 369300, "start": 3705.64, "end": 3711.64, "text": " And this cell that's being tracked here, so it's an individual hidden unit, one dimension,", "tokens": [400, 341, 2815, 300, 311, 885, 31703, 510, 11, 370, 309, 311, 364, 2609, 7633, 4985, 11, 472, 10139, 11], "temperature": 0.0, "avg_logprob": -0.17513424774696087, "compression_ratio": 1.7341269841269842, "no_speech_prob": 9.972354746423662e-06}, {"id": 952, "seek": 369300, "start": 3711.64, "end": 3717.84, "text": " right, is actually after it sees boy, it sort of starts to go higher.", "tokens": [558, 11, 307, 767, 934, 309, 8194, 3237, 11, 309, 1333, 295, 3719, 281, 352, 2946, 13], "temperature": 0.0, "avg_logprob": -0.17513424774696087, "compression_ratio": 1.7341269841269842, "no_speech_prob": 9.972354746423662e-06}, {"id": 953, "seek": 369300, "start": 3717.84, "end": 3722.4, "text": " And then it goes down to something very small once it sees greets.", "tokens": [400, 550, 309, 1709, 760, 281, 746, 588, 1359, 1564, 309, 8194, 6066, 1385, 13], "temperature": 0.0, "avg_logprob": -0.17513424774696087, "compression_ratio": 1.7341269841269842, "no_speech_prob": 9.972354746423662e-06}, {"id": 954, "seek": 372240, "start": 3722.4, "end": 3728.96, "text": " And this cell seems to correlate with the scope of a subject for number agreement instance", "tokens": [400, 341, 2815, 2544, 281, 48742, 365, 264, 11923, 295, 257, 3983, 337, 1230, 8106, 5197], "temperature": 0.0, "avg_logprob": -0.1987479047955207, "compression_ratio": 1.6181102362204725, "no_speech_prob": 2.0457982827792875e-05}, {"id": 955, "seek": 372240, "start": 3728.96, "end": 3729.96, "text": " effectively.", "tokens": [8659, 13], "temperature": 0.0, "avg_logprob": -0.1987479047955207, "compression_ratio": 1.6181102362204725, "no_speech_prob": 2.0457982827792875e-05}, {"id": 956, "seek": 372240, "start": 3729.96, "end": 3734.84, "text": " So here, the boy that watches the dog, that watches the cat greets, you got that cell,", "tokens": [407, 510, 11, 264, 3237, 300, 17062, 264, 3000, 11, 300, 17062, 264, 3857, 6066, 1385, 11, 291, 658, 300, 2815, 11], "temperature": 0.0, "avg_logprob": -0.1987479047955207, "compression_ratio": 1.6181102362204725, "no_speech_prob": 2.0457982827792875e-05}, {"id": 957, "seek": 372240, "start": 3734.84, "end": 3740.6800000000003, "text": " again, staying high, maintaining the scope of subject until greets, at which point it", "tokens": [797, 11, 7939, 1090, 11, 14916, 264, 11923, 295, 3983, 1826, 6066, 1385, 11, 412, 597, 935, 309], "temperature": 0.0, "avg_logprob": -0.1987479047955207, "compression_ratio": 1.6181102362204725, "no_speech_prob": 2.0457982827792875e-05}, {"id": 958, "seek": 372240, "start": 3740.6800000000003, "end": 3742.0, "text": " stops.", "tokens": [10094, 13], "temperature": 0.0, "avg_logprob": -0.1987479047955207, "compression_ratio": 1.6181102362204725, "no_speech_prob": 2.0457982827792875e-05}, {"id": 959, "seek": 372240, "start": 3742.0, "end": 3743.8, "text": " What allows it to do that?", "tokens": [708, 4045, 309, 281, 360, 300, 30], "temperature": 0.0, "avg_logprob": -0.1987479047955207, "compression_ratio": 1.6181102362204725, "no_speech_prob": 2.0457982827792875e-05}, {"id": 960, "seek": 372240, "start": 3743.8, "end": 3748.6800000000003, "text": " Probably some complex other dynamics in the network, but it's still a fascinating, I", "tokens": [9210, 512, 3997, 661, 15679, 294, 264, 3209, 11, 457, 309, 311, 920, 257, 10343, 11, 286], "temperature": 0.0, "avg_logprob": -0.1987479047955207, "compression_ratio": 1.6181102362204725, "no_speech_prob": 2.0457982827792875e-05}, {"id": 961, "seek": 372240, "start": 3748.6800000000003, "end": 3750.08, "text": " think, insight.", "tokens": [519, 11, 11269, 13], "temperature": 0.0, "avg_logprob": -0.1987479047955207, "compression_ratio": 1.6181102362204725, "no_speech_prob": 2.0457982827792875e-05}, {"id": 962, "seek": 375008, "start": 3750.08, "end": 3757.68, "text": " And yeah, this is just neuron, 1,150 in this LSTM.", "tokens": [400, 1338, 11, 341, 307, 445, 34090, 11, 502, 11, 20120, 294, 341, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.20854360917035272, "compression_ratio": 1.3956043956043955, "no_speech_prob": 2.1110256056999788e-05}, {"id": 963, "seek": 375008, "start": 3757.68, "end": 3766.52, "text": " Now, so those are sort of all observational studies that you could do by picking out individual", "tokens": [823, 11, 370, 729, 366, 1333, 295, 439, 9951, 1478, 5313, 300, 291, 727, 360, 538, 8867, 484, 2609], "temperature": 0.0, "avg_logprob": -0.20854360917035272, "compression_ratio": 1.3956043956043955, "no_speech_prob": 2.1110256056999788e-05}, {"id": 964, "seek": 375008, "start": 3766.52, "end": 3771.96, "text": " components of the model that you can sort of just take each one of and correlating them", "tokens": [6677, 295, 264, 2316, 300, 291, 393, 1333, 295, 445, 747, 1184, 472, 295, 293, 13983, 990, 552], "temperature": 0.0, "avg_logprob": -0.20854360917035272, "compression_ratio": 1.3956043956043955, "no_speech_prob": 2.1110256056999788e-05}, {"id": 965, "seek": 375008, "start": 3771.96, "end": 3773.36, "text": " with some behavior.", "tokens": [365, 512, 5223, 13], "temperature": 0.0, "avg_logprob": -0.20854360917035272, "compression_ratio": 1.3956043956043955, "no_speech_prob": 2.1110256056999788e-05}, {"id": 966, "seek": 377336, "start": 3773.36, "end": 3780.48, "text": " Now we'll look at a general class of methods called probing by which we still sort of use", "tokens": [823, 321, 603, 574, 412, 257, 2674, 1508, 295, 7150, 1219, 1239, 278, 538, 597, 321, 920, 1333, 295, 764], "temperature": 0.0, "avg_logprob": -0.15872018644125155, "compression_ratio": 1.6627906976744187, "no_speech_prob": 4.198156966594979e-05}, {"id": 967, "seek": 377336, "start": 3780.48, "end": 3786.6400000000003, "text": " supervised knowledge, like the knowledge of the type of co-reference that we're looking", "tokens": [46533, 3601, 11, 411, 264, 3601, 295, 264, 2010, 295, 598, 12, 265, 5158, 300, 321, 434, 1237], "temperature": 0.0, "avg_logprob": -0.15872018644125155, "compression_ratio": 1.6627906976744187, "no_speech_prob": 4.198156966594979e-05}, {"id": 968, "seek": 377336, "start": 3786.6400000000003, "end": 3787.6400000000003, "text": " for.", "tokens": [337, 13], "temperature": 0.0, "avg_logprob": -0.15872018644125155, "compression_ratio": 1.6627906976744187, "no_speech_prob": 4.198156966594979e-05}, {"id": 969, "seek": 377336, "start": 3787.6400000000003, "end": 3790.6400000000003, "text": " But instead of thinking if it correlates with something that's immediately interpretable,", "tokens": [583, 2602, 295, 1953, 498, 309, 13983, 1024, 365, 746, 300, 311, 4258, 7302, 712, 11], "temperature": 0.0, "avg_logprob": -0.15872018644125155, "compression_ratio": 1.6627906976744187, "no_speech_prob": 4.198156966594979e-05}, {"id": 970, "seek": 377336, "start": 3790.6400000000003, "end": 3796.6400000000003, "text": " like a attention head, we're going to look into the vector representations of the model", "tokens": [411, 257, 3202, 1378, 11, 321, 434, 516, 281, 574, 666, 264, 8062, 33358, 295, 264, 2316], "temperature": 0.0, "avg_logprob": -0.15872018644125155, "compression_ratio": 1.6627906976744187, "no_speech_prob": 4.198156966594979e-05}, {"id": 971, "seek": 377336, "start": 3796.6400000000003, "end": 3801.28, "text": " and see if these properties can be read out by some simple function.", "tokens": [293, 536, 498, 613, 7221, 393, 312, 1401, 484, 538, 512, 2199, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15872018644125155, "compression_ratio": 1.6627906976744187, "no_speech_prob": 4.198156966594979e-05}, {"id": 972, "seek": 380128, "start": 3801.28, "end": 3806.88, "text": " To say, oh, maybe this property was made very easily accessible by my neural network.", "tokens": [1407, 584, 11, 1954, 11, 1310, 341, 4707, 390, 1027, 588, 3612, 9515, 538, 452, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.19250399066555884, "compression_ratio": 1.6830985915492958, "no_speech_prob": 2.3920352759887464e-05}, {"id": 973, "seek": 380128, "start": 3806.88, "end": 3808.84, "text": " So let's dig into this.", "tokens": [407, 718, 311, 2528, 666, 341, 13], "temperature": 0.0, "avg_logprob": -0.19250399066555884, "compression_ratio": 1.6830985915492958, "no_speech_prob": 2.3920352759887464e-05}, {"id": 974, "seek": 380128, "start": 3808.84, "end": 3814.6000000000004, "text": " So the general paradigm is that you've got language data that goes into some big pre-trained", "tokens": [407, 264, 2674, 24709, 307, 300, 291, 600, 658, 2856, 1412, 300, 1709, 666, 512, 955, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.19250399066555884, "compression_ratio": 1.6830985915492958, "no_speech_prob": 2.3920352759887464e-05}, {"id": 975, "seek": 380128, "start": 3814.6000000000004, "end": 3816.6000000000004, "text": " transformer with fine tuning.", "tokens": [31782, 365, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.19250399066555884, "compression_ratio": 1.6830985915492958, "no_speech_prob": 2.3920352759887464e-05}, {"id": 976, "seek": 380128, "start": 3816.6000000000004, "end": 3819.0400000000004, "text": " And you get state-of-the-art results.", "tokens": [400, 291, 483, 1785, 12, 2670, 12, 3322, 12, 446, 3542, 13], "temperature": 0.0, "avg_logprob": -0.19250399066555884, "compression_ratio": 1.6830985915492958, "no_speech_prob": 2.3920352759887464e-05}, {"id": 977, "seek": 380128, "start": 3819.0400000000004, "end": 3821.0, "text": " So that means state-of-the-art.", "tokens": [407, 300, 1355, 1785, 12, 2670, 12, 3322, 12, 446, 13], "temperature": 0.0, "avg_logprob": -0.19250399066555884, "compression_ratio": 1.6830985915492958, "no_speech_prob": 2.3920352759887464e-05}, {"id": 978, "seek": 380128, "start": 3821.0, "end": 3826.2000000000003, "text": " And so the question for the probing sort of methodology is like, if it's providing these", "tokens": [400, 370, 264, 1168, 337, 264, 1239, 278, 1333, 295, 24850, 307, 411, 11, 498, 309, 311, 6530, 613], "temperature": 0.0, "avg_logprob": -0.19250399066555884, "compression_ratio": 1.6830985915492958, "no_speech_prob": 2.3920352759887464e-05}, {"id": 979, "seek": 380128, "start": 3826.2000000000003, "end": 3831.2400000000002, "text": " general purpose language representations, what does it actually encode about language?", "tokens": [2674, 4334, 2856, 33358, 11, 437, 775, 309, 767, 2058, 1429, 466, 2856, 30], "temperature": 0.0, "avg_logprob": -0.19250399066555884, "compression_ratio": 1.6830985915492958, "no_speech_prob": 2.3920352759887464e-05}, {"id": 980, "seek": 383124, "start": 3831.24, "end": 3834.7599999999998, "text": " Like, can we quantify this?", "tokens": [1743, 11, 393, 321, 40421, 341, 30], "temperature": 0.0, "avg_logprob": -0.17143544963761873, "compression_ratio": 1.610236220472441, "no_speech_prob": 6.201757059898227e-05}, {"id": 981, "seek": 383124, "start": 3834.7599999999998, "end": 3837.9199999999996, "text": " Can we figure out what kinds of things is learning about language that we seemingly", "tokens": [1664, 321, 2573, 484, 437, 3685, 295, 721, 307, 2539, 466, 2856, 300, 321, 18709], "temperature": 0.0, "avg_logprob": -0.17143544963761873, "compression_ratio": 1.610236220472441, "no_speech_prob": 6.201757059898227e-05}, {"id": 982, "seek": 383124, "start": 3837.9199999999996, "end": 3840.72, "text": " now don't have to tell it?", "tokens": [586, 500, 380, 362, 281, 980, 309, 30], "temperature": 0.0, "avg_logprob": -0.17143544963761873, "compression_ratio": 1.610236220472441, "no_speech_prob": 6.201757059898227e-05}, {"id": 983, "seek": 383124, "start": 3840.72, "end": 3846.56, "text": " And so you might have something like a sentence, like I record the record.", "tokens": [400, 370, 291, 1062, 362, 746, 411, 257, 8174, 11, 411, 286, 2136, 264, 2136, 13], "temperature": 0.0, "avg_logprob": -0.17143544963761873, "compression_ratio": 1.610236220472441, "no_speech_prob": 6.201757059898227e-05}, {"id": 984, "seek": 383124, "start": 3846.56, "end": 3848.08, "text": " That's an interesting sentence.", "tokens": [663, 311, 364, 1880, 8174, 13], "temperature": 0.0, "avg_logprob": -0.17143544963761873, "compression_ratio": 1.610236220472441, "no_speech_prob": 6.201757059898227e-05}, {"id": 985, "seek": 383124, "start": 3848.08, "end": 3853.9599999999996, "text": " And you put it into your transformer model with its word embeddings at the beginning,", "tokens": [400, 291, 829, 309, 666, 428, 31782, 2316, 365, 1080, 1349, 12240, 29432, 412, 264, 2863, 11], "temperature": 0.0, "avg_logprob": -0.17143544963761873, "compression_ratio": 1.610236220472441, "no_speech_prob": 6.201757059898227e-05}, {"id": 986, "seek": 383124, "start": 3853.9599999999996, "end": 3857.8799999999997, "text": " maybe some layers of self-attention and stuff, and you make some predictions.", "tokens": [1310, 512, 7914, 295, 2698, 12, 1591, 1251, 293, 1507, 11, 293, 291, 652, 512, 21264, 13], "temperature": 0.0, "avg_logprob": -0.17143544963761873, "compression_ratio": 1.610236220472441, "no_speech_prob": 6.201757059898227e-05}, {"id": 987, "seek": 385788, "start": 3857.88, "end": 3861.6, "text": " And now our objects of study are going to be these intermediate layers.", "tokens": [400, 586, 527, 6565, 295, 2979, 366, 516, 281, 312, 613, 19376, 7914, 13], "temperature": 0.0, "avg_logprob": -0.17668298721313477, "compression_ratio": 1.7302904564315353, "no_speech_prob": 4.028909188491525e-06}, {"id": 988, "seek": 385788, "start": 3861.6, "end": 3862.6, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.17668298721313477, "compression_ratio": 1.7302904564315353, "no_speech_prob": 4.028909188491525e-06}, {"id": 989, "seek": 385788, "start": 3862.6, "end": 3867.2000000000003, "text": " So it's a vector per word or sub word for every layer.", "tokens": [407, 309, 311, 257, 8062, 680, 1349, 420, 1422, 1349, 337, 633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.17668298721313477, "compression_ratio": 1.7302904564315353, "no_speech_prob": 4.028909188491525e-06}, {"id": 990, "seek": 385788, "start": 3867.2000000000003, "end": 3871.4, "text": " And the question is, like, can we use these linguistic properties like the dependency", "tokens": [400, 264, 1168, 307, 11, 411, 11, 393, 321, 764, 613, 43002, 7221, 411, 264, 33621], "temperature": 0.0, "avg_logprob": -0.17668298721313477, "compression_ratio": 1.7302904564315353, "no_speech_prob": 4.028909188491525e-06}, {"id": 991, "seek": 385788, "start": 3871.4, "end": 3878.44, "text": " parsing that we had way back in the early part of the course to understand correlations", "tokens": [21156, 278, 300, 321, 632, 636, 646, 294, 264, 2440, 644, 295, 264, 1164, 281, 1223, 13983, 763], "temperature": 0.0, "avg_logprob": -0.17668298721313477, "compression_ratio": 1.7302904564315353, "no_speech_prob": 4.028909188491525e-06}, {"id": 992, "seek": 385788, "start": 3878.44, "end": 3884.1600000000003, "text": " between properties in the vectors and these things that we can interpret.", "tokens": [1296, 7221, 294, 264, 18875, 293, 613, 721, 300, 321, 393, 7302, 13], "temperature": 0.0, "avg_logprob": -0.17668298721313477, "compression_ratio": 1.7302904564315353, "no_speech_prob": 4.028909188491525e-06}, {"id": 993, "seek": 385788, "start": 3884.1600000000003, "end": 3886.96, "text": " We can interpret dependency parses.", "tokens": [492, 393, 7302, 33621, 21156, 279, 13], "temperature": 0.0, "avg_logprob": -0.17668298721313477, "compression_ratio": 1.7302904564315353, "no_speech_prob": 4.028909188491525e-06}, {"id": 994, "seek": 388696, "start": 3886.96, "end": 3891.7200000000003, "text": " So there are a couple of things that we might want to look for here.", "tokens": [407, 456, 366, 257, 1916, 295, 721, 300, 321, 1062, 528, 281, 574, 337, 510, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 995, "seek": 388696, "start": 3891.7200000000003, "end": 3893.64, "text": " You might want to look for semantics.", "tokens": [509, 1062, 528, 281, 574, 337, 4361, 45298, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 996, "seek": 388696, "start": 3893.64, "end": 3896.52, "text": " So here, in the sentence, I record the record.", "tokens": [407, 510, 11, 294, 264, 8174, 11, 286, 2136, 264, 2136, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 997, "seek": 388696, "start": 3896.52, "end": 3898.56, "text": " I am an agent.", "tokens": [286, 669, 364, 9461, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 998, "seek": 388696, "start": 3898.56, "end": 3901.2400000000002, "text": " That's a semantics thing.", "tokens": [663, 311, 257, 4361, 45298, 551, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 999, "seek": 388696, "start": 3901.2400000000002, "end": 3902.2400000000002, "text": " Record is a patient.", "tokens": [27401, 307, 257, 4537, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 1000, "seek": 388696, "start": 3902.2400000000002, "end": 3904.2, "text": " It's the thing I'm recording.", "tokens": [467, 311, 264, 551, 286, 478, 6613, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 1001, "seek": 388696, "start": 3904.2, "end": 3905.2, "text": " You might have syntax.", "tokens": [509, 1062, 362, 28431, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 1002, "seek": 388696, "start": 3905.2, "end": 3907.48, "text": " So you might have the syntax tree that you're interested in.", "tokens": [407, 291, 1062, 362, 264, 28431, 4230, 300, 291, 434, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 1003, "seek": 388696, "start": 3907.48, "end": 3909.44, "text": " That's the dependency parse tree.", "tokens": [663, 311, 264, 33621, 48377, 4230, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 1004, "seek": 388696, "start": 3909.44, "end": 3911.08, "text": " Maybe you're interested in part of speech, right?", "tokens": [2704, 291, 434, 3102, 294, 644, 295, 6218, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 1005, "seek": 388696, "start": 3911.08, "end": 3914.96, "text": " Because you have record and record.", "tokens": [1436, 291, 362, 2136, 293, 2136, 13], "temperature": 0.0, "avg_logprob": -0.18175130946035603, "compression_ratio": 1.8865546218487395, "no_speech_prob": 4.262928632670082e-05}, {"id": 1006, "seek": 391496, "start": 3914.96, "end": 3917.44, "text": " And the first one's a verb, the second one's a noun.", "tokens": [400, 264, 700, 472, 311, 257, 9595, 11, 264, 1150, 472, 311, 257, 23307, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1007, "seek": 391496, "start": 3917.44, "end": 3919.08, "text": " They're identical strings.", "tokens": [814, 434, 14800, 13985, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1008, "seek": 391496, "start": 3919.08, "end": 3923.8, "text": " That's the model encode that one is one and the other is the other.", "tokens": [663, 311, 264, 2316, 2058, 1429, 300, 472, 307, 472, 293, 264, 661, 307, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1009, "seek": 391496, "start": 3923.8, "end": 3926.2400000000002, "text": " So how do we do this kind of study?", "tokens": [407, 577, 360, 321, 360, 341, 733, 295, 2979, 30], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1010, "seek": 391496, "start": 3926.2400000000002, "end": 3929.48, "text": " So we're going to decide on a layer that we want to analyze.", "tokens": [407, 321, 434, 516, 281, 4536, 322, 257, 4583, 300, 321, 528, 281, 12477, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1011, "seek": 391496, "start": 3929.48, "end": 3931.12, "text": " And we're going to freeze Bert.", "tokens": [400, 321, 434, 516, 281, 15959, 29594, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1012, "seek": 391496, "start": 3931.12, "end": 3932.68, "text": " So we're not going to fine tune Bert.", "tokens": [407, 321, 434, 406, 516, 281, 2489, 10864, 29594, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1013, "seek": 391496, "start": 3932.68, "end": 3934.8, "text": " All the parameters are frozen.", "tokens": [1057, 264, 9834, 366, 12496, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1014, "seek": 391496, "start": 3934.8, "end": 3936.7200000000003, "text": " So we decide on layer two of Bert.", "tokens": [407, 321, 4536, 322, 4583, 732, 295, 29594, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1015, "seek": 391496, "start": 3936.7200000000003, "end": 3938.48, "text": " We're going to pass it some sentences.", "tokens": [492, 434, 516, 281, 1320, 309, 512, 16579, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1016, "seek": 391496, "start": 3938.48, "end": 3942.04, "text": " We decide on what's called a probe family.", "tokens": [492, 4536, 322, 437, 311, 1219, 257, 22715, 1605, 13], "temperature": 0.0, "avg_logprob": -0.21689037654710852, "compression_ratio": 1.826086956521739, "no_speech_prob": 1.7500025933259167e-05}, {"id": 1017, "seek": 394204, "start": 3942.04, "end": 3949.84, "text": " The question I'm asking is, can I use a model for my family, say linear, to decode a property", "tokens": [440, 1168, 286, 478, 3365, 307, 11, 393, 286, 764, 257, 2316, 337, 452, 1605, 11, 584, 8213, 11, 281, 979, 1429, 257, 4707], "temperature": 0.0, "avg_logprob": -0.1876691606309679, "compression_ratio": 1.6, "no_speech_prob": 1.6184123523999006e-05}, {"id": 1018, "seek": 394204, "start": 3949.84, "end": 3953.84, "text": " that I'm interested in really well from this layer?", "tokens": [300, 286, 478, 3102, 294, 534, 731, 490, 341, 4583, 30], "temperature": 0.0, "avg_logprob": -0.1876691606309679, "compression_ratio": 1.6, "no_speech_prob": 1.6184123523999006e-05}, {"id": 1019, "seek": 394204, "start": 3953.84, "end": 3960.04, "text": " So it's indicating that this property is easily accessible to linear models effectively.", "tokens": [407, 309, 311, 25604, 300, 341, 4707, 307, 3612, 9515, 281, 8213, 5245, 8659, 13], "temperature": 0.0, "avg_logprob": -0.1876691606309679, "compression_ratio": 1.6, "no_speech_prob": 1.6184123523999006e-05}, {"id": 1020, "seek": 394204, "start": 3960.04, "end": 3966.04, "text": " So maybe I get a train a model, a train a linear classifier on top of Bert.", "tokens": [407, 1310, 286, 483, 257, 3847, 257, 2316, 11, 257, 3847, 257, 8213, 1508, 9902, 322, 1192, 295, 29594, 13], "temperature": 0.0, "avg_logprob": -0.1876691606309679, "compression_ratio": 1.6, "no_speech_prob": 1.6184123523999006e-05}, {"id": 1021, "seek": 394204, "start": 3966.04, "end": 3969.0, "text": " And I get a really high accuracy.", "tokens": [400, 286, 483, 257, 534, 1090, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1876691606309679, "compression_ratio": 1.6, "no_speech_prob": 1.6184123523999006e-05}, {"id": 1022, "seek": 396900, "start": 3969.0, "end": 3973.44, "text": " That's sort of interesting already because you know from prior work in part of speech", "tokens": [663, 311, 1333, 295, 1880, 1217, 570, 291, 458, 490, 4059, 589, 294, 644, 295, 6218], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1023, "seek": 396900, "start": 3973.44, "end": 3978.52, "text": " tagging that if you run a linear classifier on simpler features that aren't Bert, you", "tokens": [6162, 3249, 300, 498, 291, 1190, 257, 8213, 1508, 9902, 322, 18587, 4122, 300, 3212, 380, 29594, 11, 291], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1024, "seek": 396900, "start": 3978.52, "end": 3980.2, "text": " probably don't get as high an accuracy.", "tokens": [1391, 500, 380, 483, 382, 1090, 364, 14170, 13], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1025, "seek": 396900, "start": 3980.2, "end": 3982.32, "text": " So that's an interesting sort of takeaway.", "tokens": [407, 300, 311, 364, 1880, 1333, 295, 30681, 13], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1026, "seek": 396900, "start": 3982.32, "end": 3984.4, "text": " But then you can also take like a baseline.", "tokens": [583, 550, 291, 393, 611, 747, 411, 257, 20518, 13], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1027, "seek": 396900, "start": 3984.4, "end": 3986.04, "text": " So I want to compare two layers now.", "tokens": [407, 286, 528, 281, 6794, 732, 7914, 586, 13], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1028, "seek": 396900, "start": 3986.04, "end": 3987.44, "text": " So I've got layer one here.", "tokens": [407, 286, 600, 658, 4583, 472, 510, 13], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1029, "seek": 396900, "start": 3987.44, "end": 3989.6, "text": " I want to compare it to layer two.", "tokens": [286, 528, 281, 6794, 309, 281, 4583, 732, 13], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1030, "seek": 396900, "start": 3989.6, "end": 3992.44, "text": " I train a probe on it as well.", "tokens": [286, 3847, 257, 22715, 322, 309, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1031, "seek": 396900, "start": 3992.44, "end": 3994.48, "text": " Maybe the accuracy isn't as good.", "tokens": [2704, 264, 14170, 1943, 380, 382, 665, 13], "temperature": 0.0, "avg_logprob": -0.16933442652225494, "compression_ratio": 1.721189591078067, "no_speech_prob": 2.6684545446187258e-05}, {"id": 1032, "seek": 399448, "start": 3994.48, "end": 4000.88, "text": " Now I can say, oh wow, look, by layer two, part of speech is more easily accessible to linear", "tokens": [823, 286, 393, 584, 11, 1954, 6076, 11, 574, 11, 538, 4583, 732, 11, 644, 295, 6218, 307, 544, 3612, 9515, 281, 8213], "temperature": 0.0, "avg_logprob": -0.21055023625211897, "compression_ratio": 1.5884773662551441, "no_speech_prob": 1.061540206137579e-05}, {"id": 1033, "seek": 399448, "start": 4000.88, "end": 4004.48, "text": " functions than it was at layer one.", "tokens": [6828, 813, 309, 390, 412, 4583, 472, 13], "temperature": 0.0, "avg_logprob": -0.21055023625211897, "compression_ratio": 1.5884773662551441, "no_speech_prob": 1.061540206137579e-05}, {"id": 1034, "seek": 399448, "start": 4004.48, "end": 4005.48, "text": " So what did that?", "tokens": [407, 437, 630, 300, 30], "temperature": 0.0, "avg_logprob": -0.21055023625211897, "compression_ratio": 1.5884773662551441, "no_speech_prob": 1.061540206137579e-05}, {"id": 1035, "seek": 399448, "start": 4005.48, "end": 4009.48, "text": " Well, the self-attention and feed-forward stuff made it more easily accessible.", "tokens": [1042, 11, 264, 2698, 12, 1591, 1251, 293, 3154, 12, 13305, 1507, 1027, 309, 544, 3612, 9515, 13], "temperature": 0.0, "avg_logprob": -0.21055023625211897, "compression_ratio": 1.5884773662551441, "no_speech_prob": 1.061540206137579e-05}, {"id": 1036, "seek": 399448, "start": 4009.48, "end": 4013.0, "text": " That's interesting because it's a statement about sort of the information processing of", "tokens": [663, 311, 1880, 570, 309, 311, 257, 5629, 466, 1333, 295, 264, 1589, 9007, 295], "temperature": 0.0, "avg_logprob": -0.21055023625211897, "compression_ratio": 1.5884773662551441, "no_speech_prob": 1.061540206137579e-05}, {"id": 1037, "seek": 399448, "start": 4013.0, "end": 4015.2, "text": " the model.", "tokens": [264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.21055023625211897, "compression_ratio": 1.5884773662551441, "no_speech_prob": 1.061540206137579e-05}, {"id": 1038, "seek": 399448, "start": 4015.2, "end": 4016.2, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.21055023625211897, "compression_ratio": 1.5884773662551441, "no_speech_prob": 1.061540206137579e-05}, {"id": 1039, "seek": 399448, "start": 4016.2, "end": 4020.28, "text": " Okay, so that's, we're going to analyze these layers.", "tokens": [1033, 11, 370, 300, 311, 11, 321, 434, 516, 281, 12477, 613, 7914, 13], "temperature": 0.0, "avg_logprob": -0.21055023625211897, "compression_ratio": 1.5884773662551441, "no_speech_prob": 1.061540206137579e-05}, {"id": 1040, "seek": 402028, "start": 4020.28, "end": 4026.0400000000004, "text": " Just take a second more to think about it, and you just really give me just a second.", "tokens": [1449, 747, 257, 1150, 544, 281, 519, 466, 309, 11, 293, 291, 445, 534, 976, 385, 445, 257, 1150, 13], "temperature": 0.0, "avg_logprob": -0.19521402908583818, "compression_ratio": 1.7201492537313432, "no_speech_prob": 3.0715102184331045e-05}, {"id": 1041, "seek": 402028, "start": 4026.0400000000004, "end": 4032.28, "text": " So if you have the model representations, h1 to ht, and you have a function family f,", "tokens": [407, 498, 291, 362, 264, 2316, 33358, 11, 276, 16, 281, 276, 83, 11, 293, 291, 362, 257, 2445, 1605, 283, 11], "temperature": 0.0, "avg_logprob": -0.19521402908583818, "compression_ratio": 1.7201492537313432, "no_speech_prob": 3.0715102184331045e-05}, {"id": 1042, "seek": 402028, "start": 4032.28, "end": 4036.76, "text": " that's the subset linear models, or maybe you have like a feed-forward neural network, some", "tokens": [300, 311, 264, 25993, 8213, 5245, 11, 420, 1310, 291, 362, 411, 257, 3154, 12, 13305, 18161, 3209, 11, 512], "temperature": 0.0, "avg_logprob": -0.19521402908583818, "compression_ratio": 1.7201492537313432, "no_speech_prob": 3.0715102184331045e-05}, {"id": 1043, "seek": 402028, "start": 4036.76, "end": 4041.6000000000004, "text": " fixed set of hyper parameters, freeze the model, train the probe.", "tokens": [6806, 992, 295, 9848, 9834, 11, 15959, 264, 2316, 11, 3847, 264, 22715, 13], "temperature": 0.0, "avg_logprob": -0.19521402908583818, "compression_ratio": 1.7201492537313432, "no_speech_prob": 3.0715102184331045e-05}, {"id": 1044, "seek": 402028, "start": 4041.6000000000004, "end": 4045.0800000000004, "text": " So you get some predictions for part of speech tagging or whatever.", "tokens": [407, 291, 483, 512, 21264, 337, 644, 295, 6218, 6162, 3249, 420, 2035, 13], "temperature": 0.0, "avg_logprob": -0.19521402908583818, "compression_ratio": 1.7201492537313432, "no_speech_prob": 3.0715102184331045e-05}, {"id": 1045, "seek": 402028, "start": 4045.0800000000004, "end": 4048.5600000000004, "text": " That's just the probe applied to the hidden state of the model.", "tokens": [663, 311, 445, 264, 22715, 6456, 281, 264, 7633, 1785, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19521402908583818, "compression_ratio": 1.7201492537313432, "no_speech_prob": 3.0715102184331045e-05}, {"id": 1046, "seek": 404856, "start": 4048.56, "end": 4053.0, "text": " The probe was a member of the probe family, and then the extent that we can predict why", "tokens": [440, 22715, 390, 257, 4006, 295, 264, 22715, 1605, 11, 293, 550, 264, 8396, 300, 321, 393, 6069, 983], "temperature": 0.0, "avg_logprob": -0.19885891826212906, "compression_ratio": 1.6568265682656826, "no_speech_prob": 1.3844544810126536e-05}, {"id": 1047, "seek": 404856, "start": 4053.0, "end": 4054.72, "text": " is a measure of accessibility.", "tokens": [307, 257, 3481, 295, 15002, 13], "temperature": 0.0, "avg_logprob": -0.19885891826212906, "compression_ratio": 1.6568265682656826, "no_speech_prob": 1.3844544810126536e-05}, {"id": 1048, "seek": 404856, "start": 4054.72, "end": 4057.36, "text": " So that's just kind of written out not as pictorially.", "tokens": [407, 300, 311, 445, 733, 295, 3720, 484, 406, 382, 2317, 284, 2270, 13], "temperature": 0.0, "avg_logprob": -0.19885891826212906, "compression_ratio": 1.6568265682656826, "no_speech_prob": 1.3844544810126536e-05}, {"id": 1049, "seek": 404856, "start": 4057.36, "end": 4058.36, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19885891826212906, "compression_ratio": 1.6568265682656826, "no_speech_prob": 1.3844544810126536e-05}, {"id": 1050, "seek": 404856, "start": 4058.36, "end": 4064.48, "text": " So I'm not going to stay on this for too much longer.", "tokens": [407, 286, 478, 406, 516, 281, 1754, 322, 341, 337, 886, 709, 2854, 13], "temperature": 0.0, "avg_logprob": -0.19885891826212906, "compression_ratio": 1.6568265682656826, "no_speech_prob": 1.3844544810126536e-05}, {"id": 1051, "seek": 404856, "start": 4064.48, "end": 4069.7999999999997, "text": " And it may help in the search for causal mechanisms, but it sort of just gives us a rough", "tokens": [400, 309, 815, 854, 294, 264, 3164, 337, 38755, 15902, 11, 457, 309, 1333, 295, 445, 2709, 505, 257, 5903], "temperature": 0.0, "avg_logprob": -0.19885891826212906, "compression_ratio": 1.6568265682656826, "no_speech_prob": 1.3844544810126536e-05}, {"id": 1052, "seek": 404856, "start": 4069.7999999999997, "end": 4074.2, "text": " understanding of sort of processing of the model and what things are accessible at what", "tokens": [3701, 295, 1333, 295, 9007, 295, 264, 2316, 293, 437, 721, 366, 9515, 412, 437], "temperature": 0.0, "avg_logprob": -0.19885891826212906, "compression_ratio": 1.6568265682656826, "no_speech_prob": 1.3844544810126536e-05}, {"id": 1053, "seek": 404856, "start": 4074.2, "end": 4075.52, "text": " layer.", "tokens": [4583, 13], "temperature": 0.0, "avg_logprob": -0.19885891826212906, "compression_ratio": 1.6568265682656826, "no_speech_prob": 1.3844544810126536e-05}, {"id": 1054, "seek": 404856, "start": 4075.52, "end": 4077.0, "text": " So what are some results here?", "tokens": [407, 437, 366, 512, 3542, 510, 30], "temperature": 0.0, "avg_logprob": -0.19885891826212906, "compression_ratio": 1.6568265682656826, "no_speech_prob": 1.3844544810126536e-05}, {"id": 1055, "seek": 407700, "start": 4077.0, "end": 4083.68, "text": " So one result is that BERT, if you run linear probes on it, does really, really well on things", "tokens": [407, 472, 1874, 307, 300, 363, 31479, 11, 498, 291, 1190, 8213, 1239, 279, 322, 309, 11, 775, 534, 11, 534, 731, 322, 721], "temperature": 0.0, "avg_logprob": -0.22247778694584686, "compression_ratio": 1.5904059040590406, "no_speech_prob": 9.222379048878793e-06}, {"id": 1056, "seek": 407700, "start": 4083.68, "end": 4087.88, "text": " that require syntax in part of speech named NETA recognition.", "tokens": [300, 3651, 28431, 294, 644, 295, 6218, 4926, 426, 4850, 32, 11150, 13], "temperature": 0.0, "avg_logprob": -0.22247778694584686, "compression_ratio": 1.5904059040590406, "no_speech_prob": 9.222379048878793e-06}, {"id": 1057, "seek": 407700, "start": 4087.88, "end": 4092.4, "text": " Actually in some cases, approximately as well as just doing the very best thing you could", "tokens": [5135, 294, 512, 3331, 11, 10447, 382, 731, 382, 445, 884, 264, 588, 1151, 551, 291, 727], "temperature": 0.0, "avg_logprob": -0.22247778694584686, "compression_ratio": 1.5904059040590406, "no_speech_prob": 9.222379048878793e-06}, {"id": 1058, "seek": 407700, "start": 4092.4, "end": 4095.48, "text": " possibly do without BERT.", "tokens": [6264, 360, 1553, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.22247778694584686, "compression_ratio": 1.5904059040590406, "no_speech_prob": 9.222379048878793e-06}, {"id": 1059, "seek": 407700, "start": 4095.48, "end": 4099.92, "text": " So it just makes easily accessible, amazingly strong features for these properties.", "tokens": [407, 309, 445, 1669, 3612, 9515, 11, 31762, 2068, 4122, 337, 613, 7221, 13], "temperature": 0.0, "avg_logprob": -0.22247778694584686, "compression_ratio": 1.5904059040590406, "no_speech_prob": 9.222379048878793e-06}, {"id": 1060, "seek": 407700, "start": 4099.92, "end": 4106.0, "text": " And that's an interesting sort of emergent quality of BERT, you might say.", "tokens": [400, 300, 311, 364, 1880, 1333, 295, 4345, 6930, 3125, 295, 363, 31479, 11, 291, 1062, 584, 13], "temperature": 0.0, "avg_logprob": -0.22247778694584686, "compression_ratio": 1.5904059040590406, "no_speech_prob": 9.222379048878793e-06}, {"id": 1061, "seek": 410600, "start": 4106.0, "end": 4111.48, "text": " It seems like as well that the layers of BERT have this property where, so if you look", "tokens": [467, 2544, 411, 382, 731, 300, 264, 7914, 295, 363, 31479, 362, 341, 4707, 689, 11, 370, 498, 291, 574], "temperature": 0.0, "avg_logprob": -0.1504397692981067, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.5906876291846856e-05}, {"id": 1062, "seek": 410600, "start": 4111.48, "end": 4119.4, "text": " at the columns of this plot here, each column is a task, you've got input words at the sort", "tokens": [412, 264, 13766, 295, 341, 7542, 510, 11, 1184, 7738, 307, 257, 5633, 11, 291, 600, 658, 4846, 2283, 412, 264, 1333], "temperature": 0.0, "avg_logprob": -0.1504397692981067, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.5906876291846856e-05}, {"id": 1063, "seek": 410600, "start": 4119.4, "end": 4124.72, "text": " of layer zero of BERT here, layer 24 is the last layer of BERT large, lower performance", "tokens": [295, 4583, 4018, 295, 363, 31479, 510, 11, 4583, 4022, 307, 264, 1036, 4583, 295, 363, 31479, 2416, 11, 3126, 3389], "temperature": 0.0, "avg_logprob": -0.1504397692981067, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.5906876291846856e-05}, {"id": 1064, "seek": 410600, "start": 4124.72, "end": 4131.84, "text": " is yellow, higher performance is blue, and I, the resolution isn't perfect, but consistently", "tokens": [307, 5566, 11, 2946, 3389, 307, 3344, 11, 293, 286, 11, 264, 8669, 1943, 380, 2176, 11, 457, 14961], "temperature": 0.0, "avg_logprob": -0.1504397692981067, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.5906876291846856e-05}, {"id": 1065, "seek": 410600, "start": 4131.84, "end": 4135.4, "text": " the best place to read out these properties is somewhere a bit past the middle of the", "tokens": [264, 1151, 1081, 281, 1401, 484, 613, 7221, 307, 4079, 257, 857, 1791, 264, 2808, 295, 264], "temperature": 0.0, "avg_logprob": -0.1504397692981067, "compression_ratio": 1.7450980392156863, "no_speech_prob": 3.5906876291846856e-05}, {"id": 1066, "seek": 413540, "start": 4135.4, "end": 4141.28, "text": " model, which is a very consistent rule, which is fascinating.", "tokens": [2316, 11, 597, 307, 257, 588, 8398, 4978, 11, 597, 307, 10343, 13], "temperature": 0.0, "avg_logprob": -0.16223820773037997, "compression_ratio": 1.8669724770642202, "no_speech_prob": 9.166524978354573e-05}, {"id": 1067, "seek": 413540, "start": 4141.28, "end": 4147.28, "text": " And then it seems as well like if you look at this function of increasingly abstract or", "tokens": [400, 550, 309, 2544, 382, 731, 411, 498, 291, 574, 412, 341, 2445, 295, 12980, 12649, 420], "temperature": 0.0, "avg_logprob": -0.16223820773037997, "compression_ratio": 1.8669724770642202, "no_speech_prob": 9.166524978354573e-05}, {"id": 1068, "seek": 413540, "start": 4147.28, "end": 4151.839999999999, "text": " increasingly difficult to compute linguistic properties on this axis, an increasing", "tokens": [12980, 2252, 281, 14722, 43002, 7221, 322, 341, 10298, 11, 364, 5662], "temperature": 0.0, "avg_logprob": -0.16223820773037997, "compression_ratio": 1.8669724770642202, "no_speech_prob": 9.166524978354573e-05}, {"id": 1069, "seek": 413540, "start": 4151.839999999999, "end": 4157.599999999999, "text": " depth in the network on that axis, so the deeper you go in the network, it seems like", "tokens": [7161, 294, 264, 3209, 322, 300, 10298, 11, 370, 264, 7731, 291, 352, 294, 264, 3209, 11, 309, 2544, 411], "temperature": 0.0, "avg_logprob": -0.16223820773037997, "compression_ratio": 1.8669724770642202, "no_speech_prob": 9.166524978354573e-05}, {"id": 1070, "seek": 413540, "start": 4157.599999999999, "end": 4164.32, "text": " the more easily you can access more and more abstract linguistic properties, suggesting", "tokens": [264, 544, 3612, 291, 393, 2105, 544, 293, 544, 12649, 43002, 7221, 11, 18094], "temperature": 0.0, "avg_logprob": -0.16223820773037997, "compression_ratio": 1.8669724770642202, "no_speech_prob": 9.166524978354573e-05}, {"id": 1071, "seek": 416432, "start": 4164.32, "end": 4169.799999999999, "text": " that that accessibility is being constructed over time by the layers of processing of BERT,", "tokens": [300, 300, 15002, 307, 885, 17083, 670, 565, 538, 264, 7914, 295, 9007, 295, 363, 31479, 11], "temperature": 0.0, "avg_logprob": -0.2667246051863128, "compression_ratio": 1.6171875, "no_speech_prob": 5.8235051255906e-05}, {"id": 1072, "seek": 416432, "start": 4169.799999999999, "end": 4172.0, "text": " so it's building more and more abstract features.", "tokens": [370, 309, 311, 2390, 544, 293, 544, 12649, 4122, 13], "temperature": 0.0, "avg_logprob": -0.2667246051863128, "compression_ratio": 1.6171875, "no_speech_prob": 5.8235051255906e-05}, {"id": 1073, "seek": 416432, "start": 4172.0, "end": 4177.44, "text": " Which I think is again, sort of really interesting result.", "tokens": [3013, 286, 519, 307, 797, 11, 1333, 295, 534, 1880, 1874, 13], "temperature": 0.0, "avg_logprob": -0.2667246051863128, "compression_ratio": 1.6171875, "no_speech_prob": 5.8235051255906e-05}, {"id": 1074, "seek": 416432, "start": 4177.44, "end": 4183.679999999999, "text": " And now I think, yeah, one thing that I think comes to mind that really brings us back", "tokens": [400, 586, 286, 519, 11, 1338, 11, 472, 551, 300, 286, 519, 1487, 281, 1575, 300, 534, 5607, 505, 646], "temperature": 0.0, "avg_logprob": -0.2667246051863128, "compression_ratio": 1.6171875, "no_speech_prob": 5.8235051255906e-05}, {"id": 1075, "seek": 416432, "start": 4183.679999999999, "end": 4188.84, "text": " right today one is we built intuitions around word to veck.", "tokens": [558, 965, 472, 307, 321, 3094, 16224, 626, 926, 1349, 281, 1241, 547, 13], "temperature": 0.0, "avg_logprob": -0.2667246051863128, "compression_ratio": 1.6171875, "no_speech_prob": 5.8235051255906e-05}, {"id": 1076, "seek": 416432, "start": 4188.84, "end": 4191.48, "text": " We were asking like what does each dimension of word to veck mean?", "tokens": [492, 645, 3365, 411, 437, 775, 1184, 10139, 295, 1349, 281, 1241, 547, 914, 30], "temperature": 0.0, "avg_logprob": -0.2667246051863128, "compression_ratio": 1.6171875, "no_speech_prob": 5.8235051255906e-05}, {"id": 1077, "seek": 419148, "start": 4191.48, "end": 4197.08, "text": " And the answer was not really anything, but we could build intuitions about it and", "tokens": [400, 264, 1867, 390, 406, 534, 1340, 11, 457, 321, 727, 1322, 16224, 626, 466, 309, 293], "temperature": 0.0, "avg_logprob": -0.20305540395337482, "compression_ratio": 1.7427385892116183, "no_speech_prob": 1.4366009963850956e-06}, {"id": 1078, "seek": 419148, "start": 4197.08, "end": 4201.799999999999, "text": " think about properties of it through sort of these connections between simple mathematical", "tokens": [519, 466, 7221, 295, 309, 807, 1333, 295, 613, 9271, 1296, 2199, 18894], "temperature": 0.0, "avg_logprob": -0.20305540395337482, "compression_ratio": 1.7427385892116183, "no_speech_prob": 1.4366009963850956e-06}, {"id": 1079, "seek": 419148, "start": 4201.799999999999, "end": 4208.08, "text": " properties of word to veck and linguistic properties that we could sort of understand.", "tokens": [7221, 295, 1349, 281, 1241, 547, 293, 43002, 7221, 300, 321, 727, 1333, 295, 1223, 13], "temperature": 0.0, "avg_logprob": -0.20305540395337482, "compression_ratio": 1.7427385892116183, "no_speech_prob": 1.4366009963850956e-06}, {"id": 1080, "seek": 419148, "start": 4208.08, "end": 4212.24, "text": " So we had this approximation, which is not 100% true, but it's an approximation that", "tokens": [407, 321, 632, 341, 28023, 11, 597, 307, 406, 2319, 4, 2074, 11, 457, 309, 311, 364, 28023, 300], "temperature": 0.0, "avg_logprob": -0.20305540395337482, "compression_ratio": 1.7427385892116183, "no_speech_prob": 1.4366009963850956e-06}, {"id": 1081, "seek": 421224, "start": 4212.24, "end": 4222.5199999999995, "text": " says cosine similarity is effectively correlated with semantic similarity.", "tokens": [1619, 23565, 32194, 307, 8659, 38574, 365, 47982, 32194, 13], "temperature": 0.0, "avg_logprob": -0.1578866061042337, "compression_ratio": 1.577092511013216, "no_speech_prob": 5.593606601905776e-06}, {"id": 1082, "seek": 421224, "start": 4222.5199999999995, "end": 4225.84, "text": " Think about even if all we're going to do at the end of the day is fine tune these word", "tokens": [6557, 466, 754, 498, 439, 321, 434, 516, 281, 360, 412, 264, 917, 295, 264, 786, 307, 2489, 10864, 613, 1349], "temperature": 0.0, "avg_logprob": -0.1578866061042337, "compression_ratio": 1.577092511013216, "no_speech_prob": 5.593606601905776e-06}, {"id": 1083, "seek": 421224, "start": 4225.84, "end": 4227.88, "text": " embeddings anyway.", "tokens": [12240, 29432, 4033, 13], "temperature": 0.0, "avg_logprob": -0.1578866061042337, "compression_ratio": 1.577092511013216, "no_speech_prob": 5.593606601905776e-06}, {"id": 1084, "seek": 421224, "start": 4227.88, "end": 4232.28, "text": " Likewise we had this sort of idea about the analogies being encoded by linear offsets.", "tokens": [30269, 321, 632, 341, 1333, 295, 1558, 466, 264, 16660, 530, 885, 2058, 12340, 538, 8213, 39457, 1385, 13], "temperature": 0.0, "avg_logprob": -0.1578866061042337, "compression_ratio": 1.577092511013216, "no_speech_prob": 5.593606601905776e-06}, {"id": 1085, "seek": 421224, "start": 4232.28, "end": 4239.08, "text": " So some relationships are linear in space and they didn't have to be, that's fascinating.", "tokens": [407, 512, 6159, 366, 8213, 294, 1901, 293, 436, 994, 380, 362, 281, 312, 11, 300, 311, 10343, 13], "temperature": 0.0, "avg_logprob": -0.1578866061042337, "compression_ratio": 1.577092511013216, "no_speech_prob": 5.593606601905776e-06}, {"id": 1086, "seek": 423908, "start": 4239.08, "end": 4243.36, "text": " This is emergent property that we've now been able to study since we discovered this.", "tokens": [639, 307, 4345, 6930, 4707, 300, 321, 600, 586, 668, 1075, 281, 2979, 1670, 321, 6941, 341, 13], "temperature": 0.0, "avg_logprob": -0.13753826178393316, "compression_ratio": 1.6867924528301887, "no_speech_prob": 1.0450179615872912e-05}, {"id": 1087, "seek": 423908, "start": 4243.36, "end": 4245.6, "text": " Why is that the case in word to veck?", "tokens": [1545, 307, 300, 264, 1389, 294, 1349, 281, 1241, 547, 30], "temperature": 0.0, "avg_logprob": -0.13753826178393316, "compression_ratio": 1.6867924528301887, "no_speech_prob": 1.0450179615872912e-05}, {"id": 1088, "seek": 423908, "start": 4245.6, "end": 4251.0, "text": " And in general, even though you can't interpret the individual dimensions of word to veck,", "tokens": [400, 294, 2674, 11, 754, 1673, 291, 393, 380, 7302, 264, 2609, 12819, 295, 1349, 281, 1241, 547, 11], "temperature": 0.0, "avg_logprob": -0.13753826178393316, "compression_ratio": 1.6867924528301887, "no_speech_prob": 1.0450179615872912e-05}, {"id": 1089, "seek": 423908, "start": 4251.0, "end": 4257.2, "text": " these sort of emergent, interpretable connections between approximately linguistic ideas and sort", "tokens": [613, 1333, 295, 4345, 6930, 11, 7302, 712, 9271, 1296, 10447, 43002, 3487, 293, 1333], "temperature": 0.0, "avg_logprob": -0.13753826178393316, "compression_ratio": 1.6867924528301887, "no_speech_prob": 1.0450179615872912e-05}, {"id": 1090, "seek": 423908, "start": 4257.2, "end": 4260.64, "text": " of simple math on these objects is fascinating.", "tokens": [295, 2199, 5221, 322, 613, 6565, 307, 10343, 13], "temperature": 0.0, "avg_logprob": -0.13753826178393316, "compression_ratio": 1.6867924528301887, "no_speech_prob": 1.0450179615872912e-05}, {"id": 1091, "seek": 423908, "start": 4260.64, "end": 4266.04, "text": " And so one piece of work that sort of extends this idea comes back to dependency parse", "tokens": [400, 370, 472, 2522, 295, 589, 300, 1333, 295, 26448, 341, 1558, 1487, 646, 281, 33621, 48377], "temperature": 0.0, "avg_logprob": -0.13753826178393316, "compression_ratio": 1.6867924528301887, "no_speech_prob": 1.0450179615872912e-05}, {"id": 1092, "seek": 426604, "start": 4266.04, "end": 4269.56, "text": " trees. So they describe the syntax of sentences.", "tokens": [5852, 13, 407, 436, 6786, 264, 28431, 295, 16579, 13], "temperature": 0.0, "avg_logprob": -0.18776889074416386, "compression_ratio": 1.5339366515837105, "no_speech_prob": 3.023244244104717e-05}, {"id": 1093, "seek": 426604, "start": 4269.56, "end": 4278.0, "text": " And in a paper that I did with Chris, we showed that actually birds and models like it make", "tokens": [400, 294, 257, 3035, 300, 286, 630, 365, 6688, 11, 321, 4712, 300, 767, 9009, 293, 5245, 411, 309, 652], "temperature": 0.0, "avg_logprob": -0.18776889074416386, "compression_ratio": 1.5339366515837105, "no_speech_prob": 3.023244244104717e-05}, {"id": 1094, "seek": 426604, "start": 4278.0, "end": 4284.5199999999995, "text": " dependency parse tree structure emergent sort of more easily accessible than one might", "tokens": [33621, 48377, 4230, 3877, 4345, 6930, 1333, 295, 544, 3612, 9515, 813, 472, 1062], "temperature": 0.0, "avg_logprob": -0.18776889074416386, "compression_ratio": 1.5339366515837105, "no_speech_prob": 3.023244244104717e-05}, {"id": 1095, "seek": 426604, "start": 4284.5199999999995, "end": 4286.72, "text": " imagine in its vector space.", "tokens": [3811, 294, 1080, 8062, 1901, 13], "temperature": 0.0, "avg_logprob": -0.18776889074416386, "compression_ratio": 1.5339366515837105, "no_speech_prob": 3.023244244104717e-05}, {"id": 1096, "seek": 426604, "start": 4286.72, "end": 4292.84, "text": " So if you've got a tree right here, the chef who ran to the store was out of food.", "tokens": [407, 498, 291, 600, 658, 257, 4230, 558, 510, 11, 264, 10530, 567, 5872, 281, 264, 3531, 390, 484, 295, 1755, 13], "temperature": 0.0, "avg_logprob": -0.18776889074416386, "compression_ratio": 1.5339366515837105, "no_speech_prob": 3.023244244104717e-05}, {"id": 1097, "seek": 429284, "start": 4292.84, "end": 4299.0, "text": " So what you can sort of do is think about the tree in terms of distances between words.", "tokens": [407, 437, 291, 393, 1333, 295, 360, 307, 519, 466, 264, 4230, 294, 2115, 295, 22182, 1296, 2283, 13], "temperature": 0.0, "avg_logprob": -0.12400871241858247, "compression_ratio": 1.8360655737704918, "no_speech_prob": 1.0287838449585252e-05}, {"id": 1098, "seek": 429284, "start": 4299.0, "end": 4304.24, "text": " So you've got the number of edges in the tree between two words is their path distance.", "tokens": [407, 291, 600, 658, 264, 1230, 295, 8819, 294, 264, 4230, 1296, 732, 2283, 307, 641, 3100, 4560, 13], "temperature": 0.0, "avg_logprob": -0.12400871241858247, "compression_ratio": 1.8360655737704918, "no_speech_prob": 1.0287838449585252e-05}, {"id": 1099, "seek": 429284, "start": 4304.24, "end": 4308.32, "text": " So you've got sort of that the distance between chef and was is one.", "tokens": [407, 291, 600, 658, 1333, 295, 300, 264, 4560, 1296, 10530, 293, 390, 307, 472, 13], "temperature": 0.0, "avg_logprob": -0.12400871241858247, "compression_ratio": 1.8360655737704918, "no_speech_prob": 1.0287838449585252e-05}, {"id": 1100, "seek": 429284, "start": 4308.32, "end": 4312.6, "text": " And we're going to use this interpretation of a tree as a distance to make a connection", "tokens": [400, 321, 434, 516, 281, 764, 341, 14174, 295, 257, 4230, 382, 257, 4560, 281, 652, 257, 4984], "temperature": 0.0, "avg_logprob": -0.12400871241858247, "compression_ratio": 1.8360655737704918, "no_speech_prob": 1.0287838449585252e-05}, {"id": 1101, "seek": 429284, "start": 4312.6, "end": 4314.92, "text": " with birds embedding space.", "tokens": [365, 9009, 12240, 3584, 1901, 13], "temperature": 0.0, "avg_logprob": -0.12400871241858247, "compression_ratio": 1.8360655737704918, "no_speech_prob": 1.0287838449585252e-05}, {"id": 1102, "seek": 429284, "start": 4314.92, "end": 4320.24, "text": " And what we were able to show is that under a single linear transformation, the squared", "tokens": [400, 437, 321, 645, 1075, 281, 855, 307, 300, 833, 257, 2167, 8213, 9887, 11, 264, 8889], "temperature": 0.0, "avg_logprob": -0.12400871241858247, "compression_ratio": 1.8360655737704918, "no_speech_prob": 1.0287838449585252e-05}, {"id": 1103, "seek": 432024, "start": 4320.24, "end": 4327.24, "text": " Euclidean distance between bird vectors for the same sentence actually correlates well", "tokens": [462, 1311, 31264, 282, 4560, 1296, 5255, 18875, 337, 264, 912, 8174, 767, 13983, 1024, 731], "temperature": 0.0, "avg_logprob": -0.14255515336990357, "compression_ratio": 1.665, "no_speech_prob": 4.935936431138543e-06}, {"id": 1104, "seek": 432024, "start": 4327.24, "end": 4332.4, "text": " if you choose the B matrix right with the distances in the tree.", "tokens": [498, 291, 2826, 264, 363, 8141, 558, 365, 264, 22182, 294, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.14255515336990357, "compression_ratio": 1.665, "no_speech_prob": 4.935936431138543e-06}, {"id": 1105, "seek": 432024, "start": 4332.4, "end": 4337.639999999999, "text": " So here in this Euclidean space that we've transformed, the approximate distance between", "tokens": [407, 510, 294, 341, 462, 1311, 31264, 282, 1901, 300, 321, 600, 16894, 11, 264, 30874, 4560, 1296], "temperature": 0.0, "avg_logprob": -0.14255515336990357, "compression_ratio": 1.665, "no_speech_prob": 4.935936431138543e-06}, {"id": 1106, "seek": 432024, "start": 4337.639999999999, "end": 4341.12, "text": " chef and was is also one.", "tokens": [10530, 293, 390, 307, 611, 472, 13], "temperature": 0.0, "avg_logprob": -0.14255515336990357, "compression_ratio": 1.665, "no_speech_prob": 4.935936431138543e-06}, {"id": 1107, "seek": 432024, "start": 4341.12, "end": 4346.08, "text": " Likewise the difference between was and store is four in the tree.", "tokens": [30269, 264, 2649, 1296, 390, 293, 3531, 307, 1451, 294, 264, 4230, 13], "temperature": 0.0, "avg_logprob": -0.14255515336990357, "compression_ratio": 1.665, "no_speech_prob": 4.935936431138543e-06}, {"id": 1108, "seek": 434608, "start": 4346.08, "end": 4351.64, "text": " And in my simple sort of transformation of bird space, the distance between store and", "tokens": [400, 294, 452, 2199, 1333, 295, 9887, 295, 5255, 1901, 11, 264, 4560, 1296, 3531, 293], "temperature": 0.0, "avg_logprob": -0.2486698444073017, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.784532393387053e-06}, {"id": 1109, "seek": 434608, "start": 4351.64, "end": 4353.48, "text": " was is also approximately four.", "tokens": [390, 307, 611, 10447, 1451, 13], "temperature": 0.0, "avg_logprob": -0.2486698444073017, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.784532393387053e-06}, {"id": 1110, "seek": 434608, "start": 4353.48, "end": 4356.64, "text": " And this is true across a wide range of sentences.", "tokens": [400, 341, 307, 2074, 2108, 257, 4874, 3613, 295, 16579, 13], "temperature": 0.0, "avg_logprob": -0.2486698444073017, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.784532393387053e-06}, {"id": 1111, "seek": 434608, "start": 4356.64, "end": 4362.64, "text": " And this is like to me a fascinating example of again emergent approximate structure in", "tokens": [400, 341, 307, 411, 281, 385, 257, 10343, 1365, 295, 797, 4345, 6930, 30874, 3877, 294], "temperature": 0.0, "avg_logprob": -0.2486698444073017, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.784532393387053e-06}, {"id": 1112, "seek": 434608, "start": 4362.64, "end": 4369.08, "text": " these very nonlinear models that don't necessarily need to encode things so simply.", "tokens": [613, 588, 2107, 28263, 5245, 300, 500, 380, 4725, 643, 281, 2058, 1429, 721, 370, 2935, 13], "temperature": 0.0, "avg_logprob": -0.2486698444073017, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.784532393387053e-06}, {"id": 1113, "seek": 434608, "start": 4369.08, "end": 4371.08, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2486698444073017, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.784532393387053e-06}, {"id": 1114, "seek": 434608, "start": 4371.08, "end": 4372.72, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.2486698444073017, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.784532393387053e-06}, {"id": 1115, "seek": 434608, "start": 4372.72, "end": 4373.72, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.2486698444073017, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.784532393387053e-06}, {"id": 1116, "seek": 437372, "start": 4373.72, "end": 4379.240000000001, "text": " So probing studies and correlation studies are I think interesting and pointless in directions", "tokens": [407, 1239, 278, 5313, 293, 20009, 5313, 366, 286, 519, 1880, 293, 935, 1832, 294, 11095], "temperature": 0.0, "avg_logprob": -0.17537685643846743, "compression_ratio": 1.748062015503876, "no_speech_prob": 7.252830255310982e-05}, {"id": 1117, "seek": 437372, "start": 4379.240000000001, "end": 4381.84, "text": " to build intuitions about models.", "tokens": [281, 1322, 16224, 626, 466, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17537685643846743, "compression_ratio": 1.748062015503876, "no_speech_prob": 7.252830255310982e-05}, {"id": 1118, "seek": 437372, "start": 4381.84, "end": 4385.280000000001, "text": " But they're not arguments that the model is actually using the thing that you're finding", "tokens": [583, 436, 434, 406, 12869, 300, 264, 2316, 307, 767, 1228, 264, 551, 300, 291, 434, 5006], "temperature": 0.0, "avg_logprob": -0.17537685643846743, "compression_ratio": 1.748062015503876, "no_speech_prob": 7.252830255310982e-05}, {"id": 1119, "seek": 437372, "start": 4385.280000000001, "end": 4387.360000000001, "text": " to make a decision.", "tokens": [281, 652, 257, 3537, 13], "temperature": 0.0, "avg_logprob": -0.17537685643846743, "compression_ratio": 1.748062015503876, "no_speech_prob": 7.252830255310982e-05}, {"id": 1120, "seek": 437372, "start": 4387.360000000001, "end": 4390.12, "text": " Not causal studies.", "tokens": [1726, 38755, 5313, 13], "temperature": 0.0, "avg_logprob": -0.17537685643846743, "compression_ratio": 1.748062015503876, "no_speech_prob": 7.252830255310982e-05}, {"id": 1121, "seek": 437372, "start": 4390.12, "end": 4392.12, "text": " This is for probing and correlation studies.", "tokens": [639, 307, 337, 1239, 278, 293, 20009, 5313, 13], "temperature": 0.0, "avg_logprob": -0.17537685643846743, "compression_ratio": 1.748062015503876, "no_speech_prob": 7.252830255310982e-05}, {"id": 1122, "seek": 437372, "start": 4392.12, "end": 4398.320000000001, "text": " So and some work that I did around the same time, we showed actually that certain conditions", "tokens": [407, 293, 512, 589, 300, 286, 630, 926, 264, 912, 565, 11, 321, 4712, 767, 300, 1629, 4487], "temperature": 0.0, "avg_logprob": -0.17537685643846743, "compression_ratio": 1.748062015503876, "no_speech_prob": 7.252830255310982e-05}, {"id": 1123, "seek": 437372, "start": 4398.320000000001, "end": 4402.4400000000005, "text": " on probes allow you to achieve high accuracy on a task.", "tokens": [322, 1239, 279, 2089, 291, 281, 4584, 1090, 14170, 322, 257, 5633, 13], "temperature": 0.0, "avg_logprob": -0.17537685643846743, "compression_ratio": 1.748062015503876, "no_speech_prob": 7.252830255310982e-05}, {"id": 1124, "seek": 440244, "start": 4402.44, "end": 4405.04, "text": " It's effectively just fitting random labels.", "tokens": [467, 311, 8659, 445, 15669, 4974, 16949, 13], "temperature": 0.0, "avg_logprob": -0.17043673738520196, "compression_ratio": 1.7201646090534979, "no_speech_prob": 1.7777092580217868e-05}, {"id": 1125, "seek": 440244, "start": 4405.04, "end": 4411.48, "text": " And so there's a difficulty of interpreting what the model could or could not be doing", "tokens": [400, 370, 456, 311, 257, 10360, 295, 37395, 437, 264, 2316, 727, 420, 727, 406, 312, 884], "temperature": 0.0, "avg_logprob": -0.17043673738520196, "compression_ratio": 1.7201646090534979, "no_speech_prob": 1.7777092580217868e-05}, {"id": 1126, "seek": 440244, "start": 4411.48, "end": 4414.639999999999, "text": " with this thing that is somehow easily accessible.", "tokens": [365, 341, 551, 300, 307, 6063, 3612, 9515, 13], "temperature": 0.0, "avg_logprob": -0.17043673738520196, "compression_ratio": 1.7201646090534979, "no_speech_prob": 1.7777092580217868e-05}, {"id": 1127, "seek": 440244, "start": 4414.639999999999, "end": 4418.639999999999, "text": " It's interesting that this property is easily accessible, but the model might not be doing", "tokens": [467, 311, 1880, 300, 341, 4707, 307, 3612, 9515, 11, 457, 264, 2316, 1062, 406, 312, 884], "temperature": 0.0, "avg_logprob": -0.17043673738520196, "compression_ratio": 1.7201646090534979, "no_speech_prob": 1.7777092580217868e-05}, {"id": 1128, "seek": 440244, "start": 4418.639999999999, "end": 4422.44, "text": " anything with it, for example, because it's totally random.", "tokens": [1340, 365, 309, 11, 337, 1365, 11, 570, 309, 311, 3879, 4974, 13], "temperature": 0.0, "avg_logprob": -0.17043673738520196, "compression_ratio": 1.7201646090534979, "no_speech_prob": 1.7777092580217868e-05}, {"id": 1129, "seek": 440244, "start": 4422.44, "end": 4427.08, "text": " Likewise, another paper showed that you can achieve high accuracy with a probe, even", "tokens": [30269, 11, 1071, 3035, 4712, 300, 291, 393, 4584, 1090, 14170, 365, 257, 22715, 11, 754], "temperature": 0.0, "avg_logprob": -0.17043673738520196, "compression_ratio": 1.7201646090534979, "no_speech_prob": 1.7777092580217868e-05}, {"id": 1130, "seek": 442708, "start": 4427.08, "end": 4432.4, "text": " if the model is trained to know that thing that you're probing for is not useful.", "tokens": [498, 264, 2316, 307, 8895, 281, 458, 300, 551, 300, 291, 434, 1239, 278, 337, 307, 406, 4420, 13], "temperature": 0.0, "avg_logprob": -0.11986634466383192, "compression_ratio": 1.6293436293436294, "no_speech_prob": 2.3184213205240667e-05}, {"id": 1131, "seek": 442708, "start": 4432.4, "end": 4436.08, "text": " And there's causal studies that sort of try to extend this work.", "tokens": [400, 456, 311, 38755, 5313, 300, 1333, 295, 853, 281, 10101, 341, 589, 13], "temperature": 0.0, "avg_logprob": -0.11986634466383192, "compression_ratio": 1.6293436293436294, "no_speech_prob": 2.3184213205240667e-05}, {"id": 1132, "seek": 442708, "start": 4436.08, "end": 4441.5199999999995, "text": " It's much more difficult to read this paper than it's a fascinating line of future work.", "tokens": [467, 311, 709, 544, 2252, 281, 1401, 341, 3035, 813, 309, 311, 257, 10343, 1622, 295, 2027, 589, 13], "temperature": 0.0, "avg_logprob": -0.11986634466383192, "compression_ratio": 1.6293436293436294, "no_speech_prob": 2.3184213205240667e-05}, {"id": 1133, "seek": 442708, "start": 4441.5199999999995, "end": 4447.44, "text": " Now in my last two minutes, I want to talk about recasting model tweaks and ablations", "tokens": [823, 294, 452, 1036, 732, 2077, 11, 286, 528, 281, 751, 466, 850, 30587, 2316, 46664, 293, 410, 75, 763], "temperature": 0.0, "avg_logprob": -0.11986634466383192, "compression_ratio": 1.6293436293436294, "no_speech_prob": 2.3184213205240667e-05}, {"id": 1134, "seek": 442708, "start": 4447.44, "end": 4449.64, "text": " as analysis.", "tokens": [382, 5215, 13], "temperature": 0.0, "avg_logprob": -0.11986634466383192, "compression_ratio": 1.6293436293436294, "no_speech_prob": 2.3184213205240667e-05}, {"id": 1135, "seek": 442708, "start": 4449.64, "end": 4454.48, "text": " So we had this improvement process where we had a network that was going to work, okay.", "tokens": [407, 321, 632, 341, 10444, 1399, 689, 321, 632, 257, 3209, 300, 390, 516, 281, 589, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.11986634466383192, "compression_ratio": 1.6293436293436294, "no_speech_prob": 2.3184213205240667e-05}, {"id": 1136, "seek": 445448, "start": 4454.48, "end": 4457.879999999999, "text": " And we would see whether we could tweak it in simple ways to improve it.", "tokens": [400, 321, 576, 536, 1968, 321, 727, 29879, 309, 294, 2199, 2098, 281, 3470, 309, 13], "temperature": 0.0, "avg_logprob": -0.15802974977355072, "compression_ratio": 1.7909407665505226, "no_speech_prob": 7.962330710142851e-05}, {"id": 1137, "seek": 445448, "start": 4457.879999999999, "end": 4461.12, "text": " And then you could see whether you could remove anything and how it still be okay.", "tokens": [400, 550, 291, 727, 536, 1968, 291, 727, 4159, 1340, 293, 577, 309, 920, 312, 1392, 13], "temperature": 0.0, "avg_logprob": -0.15802974977355072, "compression_ratio": 1.7909407665505226, "no_speech_prob": 7.962330710142851e-05}, {"id": 1138, "seek": 445448, "start": 4461.12, "end": 4462.12, "text": " And that's kind of like analysis.", "tokens": [400, 300, 311, 733, 295, 411, 5215, 13], "temperature": 0.0, "avg_logprob": -0.15802974977355072, "compression_ratio": 1.7909407665505226, "no_speech_prob": 7.962330710142851e-05}, {"id": 1139, "seek": 445448, "start": 4462.12, "end": 4463.48, "text": " Like I have my network.", "tokens": [1743, 286, 362, 452, 3209, 13], "temperature": 0.0, "avg_logprob": -0.15802974977355072, "compression_ratio": 1.7909407665505226, "no_speech_prob": 7.962330710142851e-05}, {"id": 1140, "seek": 445448, "start": 4463.48, "end": 4466.959999999999, "text": " Do I want it to like, is it going to be better if it's more complicated, if it's going", "tokens": [1144, 286, 528, 309, 281, 411, 11, 307, 309, 516, 281, 312, 1101, 498, 309, 311, 544, 6179, 11, 498, 309, 311, 516], "temperature": 0.0, "avg_logprob": -0.15802974977355072, "compression_ratio": 1.7909407665505226, "no_speech_prob": 7.962330710142851e-05}, {"id": 1141, "seek": 445448, "start": 4466.959999999999, "end": 4470.4, "text": " to be better, if it's simpler, can I get away with it being simpler?", "tokens": [281, 312, 1101, 11, 498, 309, 311, 18587, 11, 393, 286, 483, 1314, 365, 309, 885, 18587, 30], "temperature": 0.0, "avg_logprob": -0.15802974977355072, "compression_ratio": 1.7909407665505226, "no_speech_prob": 7.962330710142851e-05}, {"id": 1142, "seek": 445448, "start": 4470.4, "end": 4475.28, "text": " And so one example of some folks who did this is they took this idea of multi-headed", "tokens": [400, 370, 472, 1365, 295, 512, 4024, 567, 630, 341, 307, 436, 1890, 341, 1558, 295, 4825, 12, 28409], "temperature": 0.0, "avg_logprob": -0.15802974977355072, "compression_ratio": 1.7909407665505226, "no_speech_prob": 7.962330710142851e-05}, {"id": 1143, "seek": 445448, "start": 4475.28, "end": 4480.04, "text": " attention and said, so many heads, all the heads important.", "tokens": [3202, 293, 848, 11, 370, 867, 8050, 11, 439, 264, 8050, 1021, 13], "temperature": 0.0, "avg_logprob": -0.15802974977355072, "compression_ratio": 1.7909407665505226, "no_speech_prob": 7.962330710142851e-05}, {"id": 1144, "seek": 448004, "start": 4480.04, "end": 4484.72, "text": " And what they showed is that if you train a system with multi-headed attention and then", "tokens": [400, 437, 436, 4712, 307, 300, 498, 291, 3847, 257, 1185, 365, 4825, 12, 28409, 3202, 293, 550], "temperature": 0.0, "avg_logprob": -0.16613556956516878, "compression_ratio": 1.8964285714285714, "no_speech_prob": 1.7229202057933435e-05}, {"id": 1145, "seek": 448004, "start": 4484.72, "end": 4489.32, "text": " just remove the heads at test time and not use them at all, you can actually do pretty", "tokens": [445, 4159, 264, 8050, 412, 1500, 565, 293, 406, 764, 552, 412, 439, 11, 291, 393, 767, 360, 1238], "temperature": 0.0, "avg_logprob": -0.16613556956516878, "compression_ratio": 1.8964285714285714, "no_speech_prob": 1.7229202057933435e-05}, {"id": 1146, "seek": 448004, "start": 4489.32, "end": 4494.44, "text": " well on the original task, not retraining at all without some of the attention heads,", "tokens": [731, 322, 264, 3380, 5633, 11, 406, 49356, 1760, 412, 439, 1553, 512, 295, 264, 3202, 8050, 11], "temperature": 0.0, "avg_logprob": -0.16613556956516878, "compression_ratio": 1.8964285714285714, "no_speech_prob": 1.7229202057933435e-05}, {"id": 1147, "seek": 448004, "start": 4494.44, "end": 4496.08, "text": " showing that they weren't important.", "tokens": [4099, 300, 436, 4999, 380, 1021, 13], "temperature": 0.0, "avg_logprob": -0.16613556956516878, "compression_ratio": 1.8964285714285714, "no_speech_prob": 1.7229202057933435e-05}, {"id": 1148, "seek": 448004, "start": 4496.08, "end": 4498.56, "text": " You could just get rid of them after training.", "tokens": [509, 727, 445, 483, 3973, 295, 552, 934, 3097, 13], "temperature": 0.0, "avg_logprob": -0.16613556956516878, "compression_ratio": 1.8964285714285714, "no_speech_prob": 1.7229202057933435e-05}, {"id": 1149, "seek": 448004, "start": 4498.56, "end": 4502.0, "text": " And likewise, you can do the same thing for, this is on machine translation, this is on", "tokens": [400, 32407, 11, 291, 393, 360, 264, 912, 551, 337, 11, 341, 307, 322, 3479, 12853, 11, 341, 307, 322], "temperature": 0.0, "avg_logprob": -0.16613556956516878, "compression_ratio": 1.8964285714285714, "no_speech_prob": 1.7229202057933435e-05}, {"id": 1150, "seek": 448004, "start": 4502.0, "end": 4506.36, "text": " multi-analye, you can actually get away without a large, large percentage of your attention", "tokens": [4825, 12, 29702, 1200, 11, 291, 393, 767, 483, 1314, 1553, 257, 2416, 11, 2416, 9668, 295, 428, 3202], "temperature": 0.0, "avg_logprob": -0.16613556956516878, "compression_ratio": 1.8964285714285714, "no_speech_prob": 1.7229202057933435e-05}, {"id": 1151, "seek": 448004, "start": 4506.36, "end": 4507.36, "text": " heads.", "tokens": [8050, 13], "temperature": 0.0, "avg_logprob": -0.16613556956516878, "compression_ratio": 1.8964285714285714, "no_speech_prob": 1.7229202057933435e-05}, {"id": 1152, "seek": 450736, "start": 4507.36, "end": 4511.2, "text": " Let's see.", "tokens": [961, 311, 536, 13], "temperature": 0.0, "avg_logprob": -0.18787065148353577, "compression_ratio": 2.0848214285714284, "no_speech_prob": 4.682928556576371e-05}, {"id": 1153, "seek": 450736, "start": 4511.2, "end": 4517.28, "text": " Yeah, so another thing that you could think about is questioning sort of the basics of", "tokens": [865, 11, 370, 1071, 551, 300, 291, 727, 519, 466, 307, 21257, 1333, 295, 264, 14688, 295], "temperature": 0.0, "avg_logprob": -0.18787065148353577, "compression_ratio": 2.0848214285714284, "no_speech_prob": 4.682928556576371e-05}, {"id": 1154, "seek": 450736, "start": 4517.28, "end": 4519.0, "text": " the models that we're building.", "tokens": [264, 5245, 300, 321, 434, 2390, 13], "temperature": 0.0, "avg_logprob": -0.18787065148353577, "compression_ratio": 2.0848214285714284, "no_speech_prob": 4.682928556576371e-05}, {"id": 1155, "seek": 450736, "start": 4519.0, "end": 4522.92, "text": " So we have transformer models that are sort of self-attention, feed-forward, self-attention,", "tokens": [407, 321, 362, 31782, 5245, 300, 366, 1333, 295, 2698, 12, 1591, 1251, 11, 3154, 12, 13305, 11, 2698, 12, 1591, 1251, 11], "temperature": 0.0, "avg_logprob": -0.18787065148353577, "compression_ratio": 2.0848214285714284, "no_speech_prob": 4.682928556576371e-05}, {"id": 1156, "seek": 450736, "start": 4522.92, "end": 4528.5199999999995, "text": " feed-forward, but like why in that order, with some of the things emitted here, and this", "tokens": [3154, 12, 13305, 11, 457, 411, 983, 294, 300, 1668, 11, 365, 512, 295, 264, 721, 44897, 510, 11, 293, 341], "temperature": 0.0, "avg_logprob": -0.18787065148353577, "compression_ratio": 2.0848214285714284, "no_speech_prob": 4.682928556576371e-05}, {"id": 1157, "seek": 450736, "start": 4528.5199999999995, "end": 4533.839999999999, "text": " paper asked this question and said, if this is my transformer, self-attention, feed-forward,", "tokens": [3035, 2351, 341, 1168, 293, 848, 11, 498, 341, 307, 452, 31782, 11, 2698, 12, 1591, 1251, 11, 3154, 12, 13305, 11], "temperature": 0.0, "avg_logprob": -0.18787065148353577, "compression_ratio": 2.0848214285714284, "no_speech_prob": 4.682928556576371e-05}, {"id": 1158, "seek": 450736, "start": 4533.839999999999, "end": 4536.599999999999, "text": " self-attention, feed-forward, et cetera, et cetera, et cetera.", "tokens": [2698, 12, 1591, 1251, 11, 3154, 12, 13305, 11, 1030, 11458, 11, 1030, 11458, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.18787065148353577, "compression_ratio": 2.0848214285714284, "no_speech_prob": 4.682928556576371e-05}, {"id": 1159, "seek": 453660, "start": 4536.6, "end": 4540.08, "text": " And if I just reordered it so that I had a bunch of self-attention at the head and a bunch", "tokens": [400, 498, 286, 445, 319, 765, 4073, 309, 370, 300, 286, 632, 257, 3840, 295, 2698, 12, 1591, 1251, 412, 264, 1378, 293, 257, 3840], "temperature": 0.0, "avg_logprob": -0.14421798758310814, "compression_ratio": 1.7767584097859328, "no_speech_prob": 3.3203905331902206e-05}, {"id": 1160, "seek": 453660, "start": 4540.08, "end": 4544.04, "text": " of feed-forward at the back, and they tried a bunch of these orderings, and this one actually", "tokens": [295, 3154, 12, 13305, 412, 264, 646, 11, 293, 436, 3031, 257, 3840, 295, 613, 1668, 1109, 11, 293, 341, 472, 767], "temperature": 0.0, "avg_logprob": -0.14421798758310814, "compression_ratio": 1.7767584097859328, "no_speech_prob": 3.3203905331902206e-05}, {"id": 1161, "seek": 453660, "start": 4544.04, "end": 4545.84, "text": " does better.", "tokens": [775, 1101, 13], "temperature": 0.0, "avg_logprob": -0.14421798758310814, "compression_ratio": 1.7767584097859328, "no_speech_prob": 3.3203905331902206e-05}, {"id": 1162, "seek": 453660, "start": 4545.84, "end": 4548.52, "text": " So this achieves a lower perplexity on a benchmark.", "tokens": [407, 341, 3538, 977, 257, 3126, 680, 18945, 507, 322, 257, 18927, 13], "temperature": 0.0, "avg_logprob": -0.14421798758310814, "compression_ratio": 1.7767584097859328, "no_speech_prob": 3.3203905331902206e-05}, {"id": 1163, "seek": 453660, "start": 4548.52, "end": 4553.400000000001, "text": " And this is a way of analyzing what's important about the architectures that I'm building,", "tokens": [400, 341, 307, 257, 636, 295, 23663, 437, 311, 1021, 466, 264, 6331, 1303, 300, 286, 478, 2390, 11], "temperature": 0.0, "avg_logprob": -0.14421798758310814, "compression_ratio": 1.7767584097859328, "no_speech_prob": 3.3203905331902206e-05}, {"id": 1164, "seek": 453660, "start": 4553.400000000001, "end": 4556.320000000001, "text": " and how can they be changed in order to perform better.", "tokens": [293, 577, 393, 436, 312, 3105, 294, 1668, 281, 2042, 1101, 13], "temperature": 0.0, "avg_logprob": -0.14421798758310814, "compression_ratio": 1.7767584097859328, "no_speech_prob": 3.3203905331902206e-05}, {"id": 1165, "seek": 453660, "start": 4556.320000000001, "end": 4560.84, "text": " So neural models are very complex, and they're difficult to characterize and impossible to", "tokens": [407, 18161, 5245, 366, 588, 3997, 11, 293, 436, 434, 2252, 281, 38463, 293, 6243, 281], "temperature": 0.0, "avg_logprob": -0.14421798758310814, "compression_ratio": 1.7767584097859328, "no_speech_prob": 3.3203905331902206e-05}, {"id": 1166, "seek": 453660, "start": 4560.84, "end": 4565.6, "text": " characterize with a single sort of statistic, I think, for your test set accuracy, especially", "tokens": [38463, 365, 257, 2167, 1333, 295, 29588, 11, 286, 519, 11, 337, 428, 1500, 992, 14170, 11, 2318], "temperature": 0.0, "avg_logprob": -0.14421798758310814, "compression_ratio": 1.7767584097859328, "no_speech_prob": 3.3203905331902206e-05}, {"id": 1167, "seek": 456560, "start": 4565.6, "end": 4567.52, "text": " in domain.", "tokens": [294, 9274, 13], "temperature": 0.0, "avg_logprob": -0.12704947899127828, "compression_ratio": 1.7588652482269505, "no_speech_prob": 4.3300951801938936e-05}, {"id": 1168, "seek": 456560, "start": 4567.52, "end": 4572.08, "text": " And we want to find intuitive descriptions of model behaviors, but we should look at", "tokens": [400, 321, 528, 281, 915, 21769, 24406, 295, 2316, 15501, 11, 457, 321, 820, 574, 412], "temperature": 0.0, "avg_logprob": -0.12704947899127828, "compression_ratio": 1.7588652482269505, "no_speech_prob": 4.3300951801938936e-05}, {"id": 1169, "seek": 456560, "start": 4572.08, "end": 4576.88, "text": " multiple levels of abstraction, and none of them are going to be complete.", "tokens": [3866, 4358, 295, 37765, 11, 293, 6022, 295, 552, 366, 516, 281, 312, 3566, 13], "temperature": 0.0, "avg_logprob": -0.12704947899127828, "compression_ratio": 1.7588652482269505, "no_speech_prob": 4.3300951801938936e-05}, {"id": 1170, "seek": 456560, "start": 4576.88, "end": 4580.240000000001, "text": " And someone tells you that their neural network is interpretable.", "tokens": [400, 1580, 5112, 291, 300, 641, 18161, 3209, 307, 7302, 712, 13], "temperature": 0.0, "avg_logprob": -0.12704947899127828, "compression_ratio": 1.7588652482269505, "no_speech_prob": 4.3300951801938936e-05}, {"id": 1171, "seek": 456560, "start": 4580.240000000001, "end": 4583.96, "text": " I encourage you to engage critically with that.", "tokens": [286, 5373, 291, 281, 4683, 22797, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.12704947899127828, "compression_ratio": 1.7588652482269505, "no_speech_prob": 4.3300951801938936e-05}, {"id": 1172, "seek": 456560, "start": 4583.96, "end": 4588.92, "text": " It's not necessarily false, but like the levels of interpretability and what you can interpret,", "tokens": [467, 311, 406, 4725, 7908, 11, 457, 411, 264, 4358, 295, 7302, 2310, 293, 437, 291, 393, 7302, 11], "temperature": 0.0, "avg_logprob": -0.12704947899127828, "compression_ratio": 1.7588652482269505, "no_speech_prob": 4.3300951801938936e-05}, {"id": 1173, "seek": 456560, "start": 4588.92, "end": 4592.08, "text": " these are the questions that you should be asking, because it's going to be opaque in", "tokens": [613, 366, 264, 1651, 300, 291, 820, 312, 3365, 11, 570, 309, 311, 516, 281, 312, 42687, 294], "temperature": 0.0, "avg_logprob": -0.12704947899127828, "compression_ratio": 1.7588652482269505, "no_speech_prob": 4.3300951801938936e-05}, {"id": 1174, "seek": 456560, "start": 4592.08, "end": 4595.280000000001, "text": " some ways, almost definitely.", "tokens": [512, 2098, 11, 1920, 2138, 13], "temperature": 0.0, "avg_logprob": -0.12704947899127828, "compression_ratio": 1.7588652482269505, "no_speech_prob": 4.3300951801938936e-05}, {"id": 1175, "seek": 459528, "start": 4595.28, "end": 4601.2, "text": " And then bring this lens to your model building as you try to think about how to build better", "tokens": [400, 550, 1565, 341, 6765, 281, 428, 2316, 2390, 382, 291, 853, 281, 519, 466, 577, 281, 1322, 1101], "temperature": 0.0, "avg_logprob": -0.1859920359103479, "compression_ratio": 1.6174242424242424, "no_speech_prob": 5.9143963881069794e-05}, {"id": 1176, "seek": 459528, "start": 4601.2, "end": 4606.96, "text": " models, even if you're not going to be doing analysis as one of your main driving goals.", "tokens": [5245, 11, 754, 498, 291, 434, 406, 516, 281, 312, 884, 5215, 382, 472, 295, 428, 2135, 4840, 5493, 13], "temperature": 0.0, "avg_logprob": -0.1859920359103479, "compression_ratio": 1.6174242424242424, "no_speech_prob": 5.9143963881069794e-05}, {"id": 1177, "seek": 459528, "start": 4606.96, "end": 4610.32, "text": " And with that, good luck on your final projects.", "tokens": [400, 365, 300, 11, 665, 3668, 322, 428, 2572, 4455, 13], "temperature": 0.0, "avg_logprob": -0.1859920359103479, "compression_ratio": 1.6174242424242424, "no_speech_prob": 5.9143963881069794e-05}, {"id": 1178, "seek": 459528, "start": 4610.32, "end": 4612.16, "text": " I realize we're at time.", "tokens": [286, 4325, 321, 434, 412, 565, 13], "temperature": 0.0, "avg_logprob": -0.1859920359103479, "compression_ratio": 1.6174242424242424, "no_speech_prob": 5.9143963881069794e-05}, {"id": 1179, "seek": 459528, "start": 4612.16, "end": 4617.5599999999995, "text": " The teaching staff is really appreciative of your efforts over this difficult quarter.", "tokens": [440, 4571, 3525, 307, 534, 43239, 295, 428, 6484, 670, 341, 2252, 6555, 13], "temperature": 0.0, "avg_logprob": -0.1859920359103479, "compression_ratio": 1.6174242424242424, "no_speech_prob": 5.9143963881069794e-05}, {"id": 1180, "seek": 459528, "start": 4617.5599999999995, "end": 4624.32, "text": " And yeah, I hope there's a lecture left on Thursday, but yeah, this is my last one.", "tokens": [400, 1338, 11, 286, 1454, 456, 311, 257, 7991, 1411, 322, 10383, 11, 457, 1338, 11, 341, 307, 452, 1036, 472, 13], "temperature": 0.0, "avg_logprob": -0.1859920359103479, "compression_ratio": 1.6174242424242424, "no_speech_prob": 5.9143963881069794e-05}, {"id": 1181, "seek": 462432, "start": 4624.32, "end": 4625.32, "text": " So thanks, everyone.", "tokens": [50364, 407, 3231, 11, 1518, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3697783946990967, "compression_ratio": 0.7142857142857143, "no_speech_prob": 0.0020455222111195326}], "language": "en"}