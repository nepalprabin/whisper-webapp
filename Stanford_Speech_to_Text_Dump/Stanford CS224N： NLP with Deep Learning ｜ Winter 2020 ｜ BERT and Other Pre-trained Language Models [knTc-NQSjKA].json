{"text": " Okay, so I'm going to talk about BERT and also some kind of precursor work and then some follow-up work that's happened in the last year or not, we'll not follow up, but more recent advancements that's happened since then. So first we're going to talk about history and background. So everyone knows and loves word embeddings in NLP, right? They're kind of the basis for why neural networks work for NLP. Because neural networks work in continuous space vectors and matrices and obviously text is discrete space and so there needed to be something to bridge the gap and it turns out that the thing to bridge the gap, it's actually pretty simple, it's just a look-up table from each, from a set of discrete vocabulary to a vector that's learned discriminatively end to end, right? So originally these were just learned like in the original Benjiah 2003, neural language on a paper, these were just trained discriminatively end to end and these were actually, and so then people would train language models and then use these pre-trained, use the embedding layer as pre-trained representations for other tasks. But they wouldn't use the rest of the language model, they would just use the embedding layer. And then word devalc and glove and stuff came along where then people found a much cheaper, much more scalable way to train where you can just use the statistics of a corpus where it's just a linear model so you don't have to compute these expensive feed-forward layers that you're going to throw out anyways and so you can scale up to like billions of tokens on a single CPU, right? So the problem though is that these word embeddings are applied in the context-free manner, right? So for like a kind of a simple, to example, the word bank, if you say open a bank account and on a river bank, it's going to be the same embedding. So people try to do stuff like word sense embeddings where it's not just a single word, it's a full word sense, but this kind of bank example, it's a little bit of a toy example, right? Almost any word has a different meaning depending on the context. It's very, so even like open the bank account and I went to the bank, those are still in a semi-different senses of the word bank. Kind of them is a, I mean they have different parts of each text kind of, well I guess not really, but like they're kind of using different senses, right? And so, yes, so we really need a contextual representation, right? So we want something where it's a representation of a word after it's been put into the context of the sense that we've seen it in, right? Which would be like at the bottom here. So kind of for history of contextual representations, the first big paper for this type of contextual representation was a paper from Google in 2015 called semi-supervised sequence learning from Andrew Dianne Cochle. And so in this one, it was actually very similar to papers that came after it, it didn't get as much attention for various reasons. So but basically they had some classification task like sentiment classification on movie reviews, and they had a big corpus on movie reviews. And so then they said what happens if we just take our existing LSTM model and instead of just using pre-trained embeddings, which everyone has already been doing since like at least for the like actually probably since 2003, people had been using pre-trained embeddings, but they said let's actually pre-trained the entire model as a language model and then let's fine tune it for our classification task. And they got pretty good results but not like stellar results. And so now we know that the reason why they didn't get stellar results is they didn't train on enough data and they, because they basically train on the same corpus that they were training on and they trained the same size model that they were training on. Which we now know needs to be bigger. But that's kind of, this was already kind of a little bit ahead of its time partially because like stuff wasn't, like we didn't have a bit of a computer back then even those only five years ago. And it would have been more expensive. So and then in 2017, Elmok came out, which was from the University of Washington in AI2. And so this one, they did something pretty clever where they took, you train a language model on a big corpus, so they trained it on a billion word corpus and they trained a big model, LSTM with 4,000 hidden dimensions which is quite expensive. And they trained a bi-directional model. But it was kind of weekly bi-directional where they trained a left-right model and then a left-right model and then they concatenated the two. And they called these contextual pre-trained embeddings. And so the idea behind Elmok is that this doesn't actually change your existing model architecture. You kind of take whatever task specific model architecture that you have, which could be for question answering, it might be some sort of fancy model where you do a LSTM over the source and over the question and over the answer, then you tend to one another and whatever kind of architecture you have. And wherever you would have put in glove embeddings before, now you put in Elmok embeddings. And so this got set of the art on everything at the time, question answering, semantic parsing, syntactic parsing, because it was, and so if you just took any existing kind of state-of-the-art model, you could fit in, put in Elmok embeddings and get state-of-the-art, right? But they weren't, but these were kind of, the models were kind of fixed. And so then after that, opening AI published, improving language understanding with generative pre-training, which is called GPT-1. And so in this, they took a similar large corpus, that of billion words, and they trained a very large language model. So a 12-layer language model, which at the time was maybe, I don't know whether it was actually a large language model that had been trained at the time, certainly it was the largest language model that had been trained on that much data for a kind of open-source model. And when I first read it, I actually thought that it was too big, not that it was worse, but that they were kind of just showing off by showing how big of a model they could train. But now we know that actually this depth that they had was actually kind of the crucial element. So they did something that was like fairly simple, right? They just trained a language model, a very large one, and then they just fine-tuned it by taking the last token and then fine-tuning it for a classification task, right? So is this positive or negative? And they got basically state-of-the-art on lots of different classification tasks. But, and so I'm going to actually take a kind of a side here before I go into BERT, which is about transformer. So that was the other kind of big thing, like the big precursor that allowed BERT and GPT to work well, right? So BERT and GPT both use the transformer, which I'm sure you guys have learned about. And so I don't need to necessarily go into all the details about it. But, so it has multi-headed attention, feed-forward layers, lay-in-arm. I won't go into all the details because I think you guys already learned about it. But the big thing about why this kind of took over is, there's really two advantages versus the LSTM. One is that there's no locality bias. And so, longest since context has an equal to the opportunity to short distance context, which is important. So for like normal language understanding, that's the locality bias of LSTM's is generally considered to be a good thing. Because local context is more relevant than longest since context. But the way that GPT and BERT and other models work is that they actually can catenate context. And so if you have a model that says does sentence one entail sentence two, the way that it was done historically, meaning like before GPT, was that you would like encode them both, let's say with an LSTM, then you would do attention from one to the other. With a transformer, you can just put them into the same sequence and give them separate sequence of adding to it at a separated token. And it will learn how to, and then it can, things can attend to its own sentence locally. But it can also attend all the way to the other sentence for almost, for no, it's just as easy for it to attend all the way to the other sentence. And so when you do this, kind of you can just pack everything into a single sequence and then everything will be learned. Rather than having to do this as part of the model architecture, which ends up being a pretty important thing about simplifying these models. And so the other thing is that having a, with transformers, with LSTMs, let's say this is a batch and these are the words in the batch. You have two sentences and four words per sentence. Every step has to be computed one at a time. So you only get a batch size of two effectively. And so on modern hardware, which is TPUs and GPUs, the bigger the matrix multiplication, the better it is. You want all three dimensions to be big. So even if you have big hidden layers, your batch size dimension will still be small, unless you have a huge batch, but then that's too expensive for long sequences. But with transformers, it's the total, because it's layer wise attention, the total number, the batch size is the total number of words. So if you have 500 words and then 32 sentences, it's actually 32, 10, 512 is the total batch size. So you get these huge matrix multiplication, and you can take advantage of modern hardware. And so that's kind of why the transformer has taken over, because of these two things. And that's why it was used in GPD and Y at Susan Burke. So now I'm going to talk about Burke. So the problem with the previous model being LMO and GPD and, once we for it, is that the language model is only used left context or right context or a concatenation of both, but really, but language understanding is bidirectional. So there's this clear kind of mismatch between why did everyone train on new direction of models, where you could only see to the left or only see to the right, when we know that in order to understand language, you need to look in both directions. So there's two reasons. So one is that language models historically had been used for typically as features in other systems. So the most direct application of language model would be predictive text, which is directly just saying predict the next word. But the other applications that are actually more common are to use them in a machine translation system or a speech recognition system, where you have these features like translation features or acoustic features, and then you add a language model that says what's the probability of the sentence. And so for this, you want it to be a well-formed distribution. So these pre-trained models we actually don't care about this, but this was kind of something that was kind of people had just been like kind of, I guess, fixed on this idea that language models have to have a distribution, probably a distribution, even though we actually don't care about that. But the other kind of bigger reason is that words can see themselves in a bidirectional encoder. And so what this means is when you build a representation incrementally, so you have your input and then you have your output and it's always offset by one. So we have the start-up sentence token, we predict the first word, then we feed in the second word, we feed in the first word and predict the second word, and so we can encode the sentence once and predict all the words in the sentence with the unidirectional model. And so this gives us good sample efficiency, right? Because if we have a 500 and 12-dimension, like a sequence of 500 words, we don't want to have to only predict one word because it's going to be 500 times as much compute to get the same amount of predictions. If we would just trivially do a bidirectional LSTM or transformer, we would have a situation where you encode your sentence, everything is bidirectional. And so after the first layer, everything can see itself. So this word open, there's a path back down to open. And so it's trivial to predict a word that can, where it's in the input also, right? There's no actual prediction going on there. So the simple solution, which is basically the whole crux of birth, is that let's instead of training a normal language model, let's just predict, mask out, k percent of the words. So the man went to the mask to buy a mask of milk. And so now you can run a bidirectional model on that. And because the words aren't in the input, you can't cheat, right? And so the downside of this is that you're not getting as many predictions per sentence, right? You're only getting, predicting 15 percent of words instead of 100 percent of words. So the upside is that you're getting much more rich model because you're seeing both directions, right? So this value of k is a hyper parameter that we have to just decide on empirically. So we use 15 percent. It turns out that that's actually kind of an optimal value. So we and also people have since then have done more thorough ablation experiments and found that this 15 percent is good. So the reason for doing a certain percent over another is that if you were to do, let's say, 50 percent masking, you would get way more predictions, but you would also mask out like all of your context. And so you can, if you mask out all of your context and you're not getting any, you can't learn contextual models. And if you only do, like, let's say you can mask out one word, that might be optimal maybe, but you have to do way more data processing. So it would be way more expensive to train. And we know that these models are basically just compute bounded. So if you just have enough data, you can just kind of train them infinitely and it'll always do better. So it's really just a trade-off between these two things. So one other little detail in part, which should not have to be super important, is that because the mass token is never seen at fine-tuning time, instead of always replacing a word with the mass token as in this case, we would randomly sometimes predict it with a random word and sometimes keep the same word. So like, so a 100 percent time would say, we went to the store and went to the running, right? And so we wouldn't tell the model which case was which. We would just have to, we would just say, what should this word be, right? And didn't know whether it's right or not. So it could be the same word. So it's 10 percent time, it's the same word. It could be a random word. And so it has to basically be able to maintain a good representation of every word because it doesn't know whether it's really the right word. So it has to actually look at every word and figure out whether this is the right word. So we could potentially even just get away with not using mass token at all, and just doing like this 50 percent of time and this 50 percent of the time. But the reason for not doing that is that, you know, then we'd be corrupting a lot of our data and we don't want it to necessarily corrupt a data because the fact that this is the wrong word might mess up our prediction for some other word over here, right? So whereas a mass token at least it knows that it's not the right word, so it doesn't use that as part of its context. So the other kind of detail of BERT which also now and subsequently may not be, have been that important, is that a lot of these tasks that we're not just learning words, we're want to predict the relationship between sentences. So if question answering in particular, we have a query which is generally a sentence and then we have an answer which is a paragraph or a sentence or a document and we want to, you know, say does this answer the question. So by doing that, we, so we want to have some pre-taining task that actually does a sentence of prediction rather than just a word level prediction. So the way that we did this, which, and we need this to have like an infinite amount of data, right? We're going to generate an infinite amount of data so we don't want this to be an annotated task. So the way that we did this is we just did a next sentence-producing task where we just took two sentences from the same corpus, from the same document and 50% time there from the same document, 50% time there from a random document and then we just said, was this the real next sentence or not? And so if you have like the man went to the store, he bought a gun, a milk, that is the next sentence. If you said the man went to the store, penguins are flightless, that's not the next sentence. So basically now we're forcing the model app pre-taining time to actually make, to look at the full sentences and then make some sort of sentence of a prediction and we hope that this is kind of generalized which is something like question answering where you have a question and answer as sentence, and sentence B. So in terms of our input representation, it looks pretty similar to a normal transformer but we have these additional embeddings which are called segment embeddings. So normal transformer, you would have your input and then you would do word piece segmentation right where you split up, we apply this unsupervised splitting of words into kind of morphological splits but they're usually often not morphological, so it's unsupervised. But you end up with something that's roughly morphological right? And so now you have like no out of vocabulary tokens, everything is represented at the very least you always split into characters. So we use a 30,000 word vocabulary and then we have our token embeddings, then we have our normal position embeddings which is at the bottom. So these are part of the transformer where because transformers unlike LSTMs don't have any sort of locational awareness. So the way to encode that is that you encode an actual embedding for every position. So this is called absolute position embedding, there's other techniques nowadays. And then you have the segment embedding which is this is a sentence A or sentence B. And so this kind of generalizes in more general context. So you can imagine if you're trying to say like you're trying to do like web search, you might say here's my query, here's the title, here's the URL, here's the document content. And so you can kind of just pack these all into a single sequence and then just give them different segment embeddings or type embeddings so that now you get are able to have a much stronger, you're able to kind of just represent everything in this kind of same single sequence where you kind of differentiate it but just the single embedding that's different. And this is all of course learned. And so this is in contrast to kind of the older style where you would typically have a different encoder for every part. So like you would have a different encoder for the query and then maybe the title and the URL. But this case it's all just a single sequence. So we trained on about a 3 billion word corpus which was at the time large now it's not actually even that big compared to what people are training on. We used a batch size which was also large. We trained for about 40 epochs of the data. We trained these two models which are still relatively large. So one of them is 12 layer, 768 and then the other one is 24 layer, 1024. So at the time this is basically like one of the largest models that had been trained although now people are training models that I think are 30 times or more bigger than this in the more recent papers. So things have kind of exploded in terms of compute in the last I know about three years. But yeah. So the fine tuning procedure is, it's pretty straightforward right? So we pre-trained this model for these two tasks. And so now we have an input sequence which is multiple sentences with different type embeddings. We feed them through our transformer model. And now we have the special embedding which I think I didn't mention. So this special embedding, this is basically, it's learned to predict the next sentence prediction task and then this is used also for classification task. But it's not just that we're using the embedding right? We are, we're fine tuning the entire model right? So it's really not that this embedding is intrinsically useful or that the word embedding is intrinsically useful. It's that the weights inside the entire 12 or 24 layer model are useful. And so by fine tuning the entire model you can kind of pick out the salient parts that are important for some downstream task. And so this is the kind of the class specific fine tuning. So if we have a single classification task like let's say sentiment analysis where you say is this a positive or negative review? We encode our sentence with the birth model. And the only parameters that we add are this final output matrix right? So maybe if we have three, like say positive, negative or neutral, this might be a thousand times three right? So it's just 3,000 parameters and 300 million. So a 3,000 new parameters and 300 million old parameters. And we jointly train all 300 million plus 3,000 for this downstream task. But because the vast majority of them are pre-trained, we can kind of adapt to it in only like a few thousand label examples. And similarly for a sentence pair class, we do, we just can count into three sentences with different type embeddings. So we have, if you want to say does the sentence entail this other sentence, you say sentence A, you put it, can count in sentence B, and then also predicts from this token and fine to the entire thing. Similarly, very few additional parameters. For span prediction tasks, you just have kind of a start of span end of span. So you're only adding a few thousand new parameters. And then for tagging tasks, like part of speech tagging, you just have a single sentence. You add every single token or maybe every token except for the word pieces. But like, you, that's kind of free processing. You predict what's the part of speech of this. And so this is really why, so like, bird itself is really a kind of, I would say an incremental improvement over what already existed. So it kind of took transformers, LMO, GPT, really these three ideas and kind of made a pretty simple change on top of them. But the reason why it had such big impact is not just the numbers that I'll show in a few slides. It's really this thing. Because with LMO, there was really no fundamental difference between, it was just contextual embedding. So you still have, like, a lot of deep learning historically has been fun and building new models, right? So you have all of these components, kind of like Lego blocks, right? You have attention layers, feed forward layers, layer normalization, LSTMs, et cetera. And you can just kind of like figure out, say, okay, for this new task, how do I glue these together in a way that's best, right? And so, and so with LMO, it wasn't really, it didn't really change anything fundamentally. It was just, because these were, you just fed it into your existing model and you got to the art. For GPT one, it wasn't, most, like these things didn't really work, right? Because it was a left to right language model. And so you could just kind of take the last token and then predict a classification task, but it didn't really make any sense to predict, like, part of speech tags. Because for the first word, there's no context. So it makes no sense to predict the word with no context. With BERT, the reason why it had such high impact was because it kind of simplified things. And so that's not, I'm not saying that's necessarily a good thing, because as a researcher, or a bad thing. So as a researcher, kind of, ironically, the ultimate goal of research is often like research yourself out of a job, right? It's like, you know, if a physicist, I'm not saying BERT had, anywhere near this impact, like a physicist that came up with like a grand theory of physics, they would kind of, like, they would be like the greatest moment in physics, but also that would kind of eliminate a lot of research, right? And so that's kind of like the end goal of research is kind of solve the problem, right? So BERT kind of has a step where now, like, all of these different causes of problems, there's really, it kind of killed like a lot of the need to do model architecture design, which is kind of unfortunate because that's like really fun. And so that's kind of the impact. And I'm not going to say whether it's like a good or a bad impact, it's kind of like the objective impact. So why it's had so much impact is because it has kind of had this effect on now so many things that used to be like designing fun, you know, models, it's just fitted in and use one of these four recipes and it kind of just works for all of these different tasks. So in terms of actual empirical results, so these are at the time that Hickory's published, of course, things have gotten better since then. So this glue task is a set of, they're all kind of similar in that they're all sentenced pair or sentence classification tests. So like multi NLI would be something like hills and mountains are especially sanctified in Janism. And then hypothesis is Janism hates nature, that's a contradiction, right? So in order for an NLP model to be able to understand this or to be able to answer this correctly and give this label of contradiction, it needs to know that hills and mountains are part of nature, sanctified is a good thing and that hating something is a bad thing and be able to do all of this reasoning, right? So it's pretty complicated reasoning. Similarly for cola, you have to be able to say like the wagon rumbled down the road versus the car honked down the road. And so these things are, you know, one of them to a native English speaker sounds totally fine, the other one sounds weird. And so it's similar and neither you have very much data, right? So you have to be able to generalize on only like a few thousand examples. So birthed base, which is the same size as open AI, it significantly beat open AI, which was the previous data they are. And then the birth large, which was bigger, of course got better results, which is only surprising that it got better results across the board, including on the very, very tiny data, that's only a few thousand examples. That's kind of the more interesting result rather than just the fact that it got better results. Historically, when there is rules of thumb about, if you have some number of examples, how do you design the model size that's optimal for that? And so if you don't do pre-training, like if you keep making the model bigger without pre-training, eventually you'll get worse results because your model overfitting your training data. With pre-training, you basically only ever do like one pass of the day anyways. So there seems to be almost no limit to how big you can make it and still get good results even with a tiny amount of fine-tuning data. And that's really like one of the big takeaways. So I'm not going to, yeah, so the reason why these numbers are, these range are lower is because I took the screenshot after, like this, you know, significantly after, when other bunch of other people had submitted systems. But so this is a question and answer and get a set. So it'd be like, what action did the US take to start this second oil shock? So in this case, there's no answer, right? So it's something you have to be able to predict is that there's no answer in this phase. If you have to be able to predict the answer or say there's no answer. So burp beat the previous state of the art, at the time that it was submitted by about six points, which was a pretty big gain. Now this has kind of gone past human level. But at the time, yeah, it was large. So I'll kind of do some ablation experiments or go through some ablation experiments. So this one, there's four things that I'm comparing here. So this is all burp based size models. So the blue line is kind of the burp based model. The red line is, with we take out the next sentence prediction. So in our case, even though people have subsequently said that they don't think it's important, in our case, we actually did measure it. And it turns out it seemed like it was important to have this next sentence prediction task, especially for kind of question answering task, which is this one. But it seems to have a little bit, at least in all four of them, to have it. So this kind of does indicate that there's some strength in learning some model that learns a relationship between sentences. So then this one is the one that makes an apples to apples comparison between open AI and open AI is GPT1 and BERT, right? Because I also, I made the model bigger, but not per base. So BERT based was the exact same size, but it was China more data. So to make it a fair comparison, I basically retrained my own implementation of open AI's GPT1, which is a yellow line. And we can see that on some of the tests, it's not that far, although this is actually a pretty big gap. This drop is four points, which is a lot. But on some tasks like squad and M.O.P.C, it was way worse. And so for squad it makes sense because squad is a labeling, is a span labeling task. And so if you only have left context, then words at the beginning have basically no context. And so you're asking it to do span labeling on words with almost no context. So it really doesn't make any sense. And so of course it's going to do much worse. So then we also added a, to make it fair, we also added an LSTM on top of it, which is trained from scratch. And this does help a little bit on some of the tasks, but on other ones it doesn't help. So on SWAT it helps because now you have bidirectional context. But on MRC because it's a very small task, it's only got 3,000 labeled examples, it doesn't help at all. So it does show that kind of the mass language model and the next inspection are both important, especially the mass language model. So the other thing, one of the other ablations is that when we apply the mass language model, we're only predicting 15% of words in the sentence. So when you do a left to right language model, you're predicting every single word, conditioning all the words to the left. So one question might be how much does this make it take longer to converge? Even though eventually we know that it converges at a much better point, if you have a limited training budget, it's a better to do a left to right model. And so we see that when you do this mass language model, the bidirectionality starts to improve, like at the very, very beginning because you're doing so many more predictions, it's true that the left to right model does do better at the very, like for, like, epoch one, but then very soon after because the bidirectionality is so important, it starts to take over. And so it's basically better from almost the start to do bidirectionality. And then it takes slightly longer to converge, but the overall convergence is, of course, much higher. And then finally for this oblations, we can see that going from a smaller model, which was 100 million to 300 million parameters, helps a lot, which isn't surprising. What the more surprising thing is that one of these curves, these aren't comparable, you shouldn't compare the curves to each other. The point is to look at the curves as a function of the number of parameters and see that this one is, this one only has 3,000 labeled examples, and this one has 4,000 labeled examples. So in both cases, the curves look very similar, which is surprising because the rule of thumb that you're going to overfit your data if you only have a few labeled examples turns out not to really be true anymore. And there's, you know, in these curves keep going up, right? So now with subsequent papers, we'll talk about, like this, this big one was 300 million parameters, people have gone up to 11 billion parameters and still seeing similar behaviors. So still seeing the curves go way up and gotten to the other results, which is kind of crazy because now we know that, you know, basically there's almost no limit. So another thing I want to talk about before I talk about stuff that's happened since Bert. Is the kind of, is, even though Bert itself was in some ways very simple, which is, you know, not a bad thing, it was very successful immediately. And you know, part of that is the Google brand and like, you know, it got a cute name and stuff like that. But I think that I spent a lot of time with the open social release and particularly looking at other open source releases and figuring out what people didn't like about those. And so I think this is important, like when you're, when you're a PhD student or even working industry as a, and trying to release something. So I kind of just listed the things here that I thought were important for like why it was successful compared to other things. So like, not, I'm not trying to call them out just to be mean because it, but like the open AI GPT-1 release was really, was not very good. And then like I'm sure that they were this because the open AI GPT-2 release was very good. And so, yeah, because it was very hard to run and there was not comment that the TensorFlow code was very, like it worked fine, like I replicated it. But like, the, the TensorFlow code was very non-idiamatic. It used all sorts of weird stuff. The Python code was weird. There was no comments. There was basically no instructions. And then other code bases also are kind of too big. It's like people just want to like say like, we want to have one unified code base for our entire, you know, language team. And so they just put stuff as part of that. And people don't really like that either. So I was very insistent that we do a minimal release. So like this, we're just going to release Bert. It's not going to be part of anything. There's going to be any external dependencies. And it's going to be like very well commented. I think that people, and it was kind of also easy to drop in just the modeling part and just the tokenization part and just the front end, which runs like the training loop. And kind of separate all these out because that way. And so I think because of that, people kind of started using it much quicker. And of course, like all the publicity help. But I think that, you know, it could have easily been not as successful if it had been, you know, done in a different way. So it's just kind of advice. So yeah. So now I'm going to talk about five models that have come out since Bert that have all improved on top of Bert in various ways. There's been more than five. But I'm going to highlight these five, they think they're interesting. A lot of them did come from Google, but it's not because, well, a lot of them involved Google. I would say many of them actually were not, they were interns at Google from various universities who were supervised by Google researchers and also use Google compute. I mean, the reason why a lot of them came from Google is because, like, frankly, like, other than Facebook, Google, and Microsoft, there's not really many, like, people that can, the companies that have the resources to train these huge state of the art models. And so, almost by necessity, it's going to come from one of these labs. So the first one was Roberta. And so this is probably the one that had, like, the least kind of new stuff. It was really just, and so this was University of Washington Facebook. It came out not that long after a birth. And so what they showed was that birth was really under-trained. And so basically, they took, even on the same amount of data, which was, even though I did 40 epochs on the data, if you do it for, like, 200 epochs, you get even better results, like significantly. So they basically trained more epochs on the same data. And they also showed that more data helps, which is also not super-sprising. And they did improve masking and pre-training using a couple of tweaks to that. And they were able to get a state of the art results, which is cool. And so, yeah, but that was a pretty straightforward paper. So the next one is XLNet, which is done by some interns in CMU when they were at Google Brain. And so this actually had some really cool changes. So one of them was, they used this transformer XL, which was actually the precursor done by the same people that were, they were just doing links on all the tasks, did a pre-training. But the big, one of the big innovations of transformer XL is this idea of relative position embeddings. And so with absolute position embeddings, the problem is that every word gets, like, this is word four, this is word five, this is word six. And so they are embeddings, so they do generalize. But in practice, there's a quadratic number of relationships. Like, how does word A3 relate to word 76, right? That's, that's, that's, and once you get bigger, like, 500, a thousand. Now you have a thousand squared total relationships. Like, you have to say, how does word 97 relate to whatever, right? And so that's obviously not optimal once you get to a large size. And so with, with relative position embeddings, you basically can say how much does dog attend to hot and how much should the word dog attend to the previous word. And then you get, and these are nonlinear for, these are linear at first. But then you combine them and then you get a nonlinear, a conditional representation. And you do this in many, many layers. And this ends up being, so then you say, how does this contextual precision of dog, how much does that attend to the previous word. And then you kind of get, can build up. And so this generalizes much better for long sequences. So that's a cool innovation. And then the other one, which is specific to pre-training and not just the model itself, is this idea of permutation language modeling. So this is a little bit hard to explain. I think the paper explained it very formally, I guess. And so, but basically, there's a trick where, so in a left-for-right language model, every word done predicting is based on the word the left, right? But imagine that instead of predicting all the, you can basically take any permutation. So it's like, I'm going to predict the first word, then I'm going to be the third word, then the second word, then the fourth word. And so, that's a totally valid way. And you still get the well-for-and-probability distribution, because it's still predicting one word at a time, given some permutation of the input. And with transformers and with attention, you can actually do this very efficiently, just by masking out your attention probabilities. And so, every single sentence you have, you can kind of sample a single permutation of this. And you can now, you can effectively train a bi-directional model, because this word, it won't be conditioned on every, still on average, every word will only be conditioned on half the words. But this word will be conditioned on, you know, all these words to the left and all these words to the right. And maybe it'll be missing these words, but that's fine. And so, you get much better sample efficiency. So I thought this was a really clever idea. And so, and this was kind of the main innovation of X on it. And so, yeah, they basically get better sample efficiency, because they're able to do this random permutation and kind of take advantage of this. So this wouldn't work with LFTMs, because of this ordering, but because the way that masking is done in transformers, it's just, it's just a mask on the attention. So it actually ends up working very well. And so, they also got, so, yeah, the numbers, they actually ended up being pretty similar. But a lot of these things are hard to compare, because people change the data set and change the size of the model. So it's hard to compare apples to apples. But these two techniques ended up being pretty similar, but I think X on it had more innovations in terms of technique. So Albert, it's called a lightbert for self-supervised learning. And so, this also had a couple of cool innovations. And so the idea here is really massive parameter sharing, with the idea being that, if you share parameters, you're not going to get a better language model, but you're going to get better sample efficiency. You're going to get less overfitting when you fine tune, right? Because if you have a billion parameters and you fine tune them on a 300, on a data set with like a thousand labeled examples, you're still going to overfit very quickly, right? But if you have a much smaller number of parameters, you're going to get less overfitting. So if you get a similarly powerful model with fewer parameters, you're going to get less overfitting. And so they, so there's two major innovations where, so instead of using a word, because the wording of the table is big, right? Because it's the size of your vocabulary, the number of word pieces, times the hidden size. And so it's going to be much bigger than the hidden layer. So first thing is that they use the factorized embedding table. So if they had a hidden size of a thousand, they only use like 128 dimensional input embedding. And then they projected that to a thousand using a matrix. And so instead of having 1024 by 100,000, they would have 100,000 by 100,000 plus 1024 times 100,000, and you multiply these together and multiply these two matrices together. And then effectively you have a 1024 by 100,000 embedding matrix. But you have much fewer parameters. So you're doing parameter tying. Well, not, this isn't a parameter tie, but you're doing parameter reduction in a clever way. The other one is cross layer parameter sharing. So this is similar. It's as simple and it was all, it's, it was, it was been done in previous papers, especially universal transformer. And the idea is that you, you've run a much of transformer layers, but all, let's say if you have 12 layers, all 12 layers just share the same parameters, right? And so that ends up, so now you can have a much bigger model that has fewer parameters than bird has. And so you get less over fitting. And so they got state of the art compared to X on that in Roberta. But one important thing to keep in mind is that Albert is light in terms of parameters, not in terms of speed. So for a, for the mix, for the model that's actually comparable to, to bird, they, they actually did slightly, like, like this model and this model were about the same, but this one was actually slower. So it's only when they started making models that were much bigger in terms of compute than bird, but doing more parameter tying than they started getting good results. And so the, the implication of this is that like, you can, you can reduce the number of parameters, but still nobody's figured out how to reduce the amount of pre-training compute that it would describe, which is, you know, kind of unfortunate. So the next one is T5, which is exploring the limits of transfer-leading with unified text-txt firmware. So this was a paper by Google Brain and other groups in Google where they used just, they used a lot of compute and they did tons of ablation on pre-training. They didn't, like, their goal wasn't to come up with some, with some super clever new pre-training technique. Right, it was really just to carefully ablate every aspect, how much is model size matter, how much is training data matter, how much is clemeness of data matter, like, how much is the exact way that you do the pre-training objective matter, like doing the masking, like, how many spans do you mask? And so they wanted to kind of very clearly do the, and they also wanted to push the limits of size and say, what happens if we have 300 million, a billion, 10 billion parameters, right? And then, so they did tons and tons of ablation and they got state of the art and everything and they're still sitting at the art and everything. And the results, though, are a little bit bleak in the sense that nothing really mattered except making the data, like, like, all of the ablations, it wasn't like, oh, you know, burnt it everything perfectly, it was that, it doesn't matter, like, you could do 20%, 25%, you can do this fine tuning recipe, this fine tuning recipe, it's like, all that really matters is making the model bigger and training it on a more data, and clean data. And so, yeah, it's a little bit of a bleak paper if you are hoping that there is exists some pre-training technique which is super computationally efficient and also can get, you know, very impressive results, which I'm not saying there isn't, but, like, most of this evidence points in that. So the one kind of newest paper that is maybe the most positive in this direction is this paper called Electra. And so, and so this is done by Kevin Clark from here and, uh, and Google Brain. And so, yeah, in this one, it's a pretty clever idea. So basically, the idea is instead of training, instead of training to generate the output, you just train it as a, as a discriminator. And so, you have a local language model, you have, you do some asking, you have a local language model which replaces it, and then you train it to discriminate whether it's the original one or not. And so, the idea here is that you are doing a much, you're, you're, you're getting better sample efficiency for pre-training because you're predicting every, every word, which is actually, I mean, I don't know exactly why it would be that from, from, from, from Berkis, Berkis still, uh, in terms of, because you don't replace with, with the mask with everywhere, you also randomly corrupt it. But, but the, the, the biggest difference is that, um, is that these are kind of contextual every place. So, it's like, it, when I did random masking, and replace with the random word, it was truly a random word. So, most of the time it was completely trivial to, to tell that this was not the right word. You didn't necessarily know which word should be replaced, but in this case, they actually used a intentionally weak but still non-tribule language model to predict which word. So, like, this locally makes sense, the chef ate the meal, but it doesn't make any sense, like, a very strong model will not predict this, right? So, so, that's the idea that you, use a weak model to, to, to, to, to, to the substitution of the use, then you train a strong model to, to, um, do this. So, these results are, I guess it's a big cable, but these results are, they're certainly positive with regard to, uh, previous results in terms of compute versus, um, so, like, for, if we compare this row, so, uh, which is, one tenth of the compute of Bert large, to Bert base, which is also one tenth of the compute of Bert large, it certainly does a lot better than Bert base. Um, but, when they, uh, if you, but, but in terms of state of the art models, um, when they do, you know, the same amount of, uh, compute as their Bert large, which is this one, work a better, to other state of the art models, they're not, in order to get state of the art, or to get similar to state of the art, they basically need to do as much compute as state of the art, so, like, 444x, 5.4x. So, I mean, at scale-down values, they were able to do better, but this is a, still a pretty big gap, like, 4 points. Um, so, it's, it's positive, but it's not, it's certainly not like the silver bullet in terms of, uh, showing that, uh, we can, you know, pre-trained models, much better for, for, for cheaper. So, but, so, the last thing I want to talk about is, um, how we actually serve these models, right? Because, you know, I've said that, like, they're incredibly expensive to train, and nobody has been able to figure out how to make that faster, but, you know, they're being used all over the place, right? So, like, uh, you know, there's news stories, Google has improved 10% of searches by language-assigning, say, a little bit, and then Bing says it's been playing Burt since April, so, and so, this, this is live in Google Search, and Bing Search, and so, these are, like, really low-lanancy services, right, that have, like, a few milliseconds of, of, of latency, and they serve, you know, billions of, of queries a day, so, how, how are they doing this, is it just, like, uh, that, you know, Google and Microsoft are spending billions of dollars on hardware. What they are, but not just for this, right? And so, like, uh, like, it would, it would cost billions of dollars just to serve this if we were actually serving Burt. But, we're serving, uh, not, instead of we're using, we're using model distillation, right? So, this has been around for a while. Um, so, it's, you know, called distillation and model compression. Uh, one, one of the first papers was the Smolk Compression Paper, um, that was, that was done for, uh, I forget exactly what task, but then, and then Hinton's paper, just stealing knowledge in neural networks, is a more well-known version of, or not, not version, but a more well-known, uh, uh, paper on, on distillation. But in reality, the one, the version that, that we use at Google and the version that most people use when they say model distillation for, uh, pre-shanked language models, it's a, um, it's a very simple technique, but, but it's easy to misinterpret what, what, what we mean. So, what we do is we pre-train, we train a state of the art model, whichever is the ones we can most afford to train, right? Because, of course, we can just make it bigger, but we, we set some budget of, you know, we want to train it for a day on some number of TPUs. And then, we fine-tune it, right? So we get a model that's the maximum accuracy, and that's our teacher model, and this is expensive. Then we have a, a large amount of unlabeled input, which is typically for, for most industry applications, you have unlabeled input, because you have, you know, in search, you have, this is what they use to search for, this is what they click on, that's how it's searched into the trained. Uh, and so, you can then just take these, and you, um, and then you just label your examples with them. So you can get billions of these, uh, if you actually want a real service. And then you, so then you, then you run these, you know, query answer pairs through your teacher, and you get a pseudo label, and you're just training much smaller model, much meaning like 50 times, 100 times smaller, to, uh, predict your student, your teacher outputs. And so, and you can generally do this for most techniques. I mean, for most tasks, you can do this, uh, pretty easily, and get a huge 50-200X, uh, compression with no degradation. But the important thing to realize that we're not compressing the pre-train model itself. We haven't really had any luck doing that. So like, you can't actually just take Bert, and then compress it to a smaller model, which you can then fine-tune for all these other tasks. It's only after you've chosen the task, and after you find it, tune it for the task that you, that we were able to do it. So to show some specific results, so let's say we have, let's say we have a Bert large teacher. So this is an Amazon book review script. So this is a paper that, that, I've got to cite it, but this is a paper that my group, uh, published, you know, the ATURK, uh, wrote. And so, um, this has 50,000 labeled examples and 8 million, uh, unlabeled examples. So you, you, you, you find tune on, you pre-train Bert large, normal, like you take the pre-train Bert large, you, uh, you find tune on these 50,000 labels, you get this 88% accuracy, right? Then, uh, and so, but then now, let's say instead of using Bert large, you used a much smaller version. So this one's, according to the size, this one's, you know, uh, a 16th size, whatever, this one's a hundredth of the size, right? So this, this row that's a hundredth of the size, if you were to just train it, if you were to pre-train this on the same Wikipedia book, just like Bert, and then fine tune it, you would get 82% accuracy, which is, you know, a lot worse, 60%, like, 66, absolute worse, which is quite a big drop, right? But then if you were to take this 88% teacher, labeled 8 million examples, which are, of course, held out, this is test, this is test accuracy. Um, and then, uh, and then train this classification model, which says this is a good about review, on these 8 million examples, you can take this model to 100 times smaller and get the same accuracy as the teacher, right? You get the same 80% accuracy. So that's really the, uh, the cool thing with distillation is that you can get models that are much smaller, but you still need to train the big model in the first place. So it doesn't help the training cost. It just helps. It actually works, it's the reason you use this big model to train, to label millions or billions of examples. So it ends up being more expensive than just training Bert, but you can actually serve this model at, at inference time for, for a tiny cost. So the question is, why does distillation work so well? So the big hypothesis is that language modeling is kind of the ultimate NLP task, right? A perfect language model is also a perfect question answering system, a perfect entailment system, sentiment analysis, co-reference, et cetera, right? Because in order to be able to, to do these things, you kind of have to be able, you could construct it as a language model. So when you're training a massive language model, you are learning many millions of latent features, which are effectively the same features that you need for any other task. And so when you're doing a simpler, a fine tuning of a more specific task, what's the fine tuning is basically taking these latent features, which your system happened to learn, and it's some, encoded somewhere in your weights. And you are, it's kind of just tweaking these, which is why I can do it with a single pass over the fine tuning data. And so, but once you figure out which parts are important, then there exists a hypothetically much smaller model size, which can still get the same representation and same generalization, right? So, there's a bunch of examples with this fine tuning model. And now you can learn a model that can really hone in on just these features that are important. And so, it can take them, you know, it can train a model that's 100th the size, and just hone in on these features if you have a lot of pseudo label data. And that's why it works. And so, the evidence really is that it just doesn't work to do self-dissolation, right? And so it must be that it's really just learning a subset of the features for most of these tasks. And so, basically every task but language modeling, we've been able to get distillation to work for. So, this includes tasks that seem really hard like question answering and search. So that does imply that language modeling itself, which is basically language generation also, because that's just a form of language modeling, is fundamentally harder than language understanding, which is not super hard to buy. Or at least it's not fundamentally harder. But given the state of the art, state of the art models for language understanding are fundamentally simpler than what they do, right? So presumably, just doing kind of pattern recognition than models that are generating language. And so that's kind of why all of these classification models can kind of be distilled so well. So basically, in conclusion, the preaching models work really well. They're very expensive. We know how to kind of solve this for inference time and we can do fast inference, but it is still unsolved how to make these fast at training time. And moreover, it seems like a lot of the details about algorithmic improvements for making the training more efficient don't seem to have a ton of benefit in terms of at least getting to the results. And it seems like a lot of choices don't really matter that much. And it's really just about a couple of like compared to just the kind of the simple mass-colon baseline. It's pretty hard to beat that in an Apple's Apple comparison. So yeah, it's a little bit unfortunate for a research perspective. It's definitely good for me from people who want to build NLP systems and who want to, especially domain specific NLP systems, like people who want to adapt to a medical domain or people who only have a tiny amount of data or people who want to do startups or they want to build an actual product and they only have a tiny amount of data. So it's definitely good for that perspective. But certainly, I think from the perspective of sometimes research, as I was saying, the goal of research is to kind of like research yourself out of a job, then it is kind of, you know, it's a little unfortunate from that perspective. But I still think that there's a possibility that there's going to be a breakthrough that kind of shows how to do computational efficiency without, and can kind of show compelling results that you don't need, you know, such an absurdly large model. Or actually, besides the models matter, you don't need such an expensive model to do well. Maybe look from sparsity, right, or something like that where you actually do have a really large model, just sparsely activated in some, using some efficiency tricks or whatever.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.44, "text": " Okay, so I'm going to talk about BERT and also some kind of precursor work and then some", "tokens": [1033, 11, 370, 286, 478, 516, 281, 751, 466, 363, 31479, 293, 611, 512, 733, 295, 41736, 284, 589, 293, 550, 512], "temperature": 0.0, "avg_logprob": -0.2434389931815011, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.08114345371723175}, {"id": 1, "seek": 0, "start": 11.44, "end": 15.72, "text": " follow-up work that's happened in the last year or not, we'll not follow up, but more", "tokens": [1524, 12, 1010, 589, 300, 311, 2011, 294, 264, 1036, 1064, 420, 406, 11, 321, 603, 406, 1524, 493, 11, 457, 544], "temperature": 0.0, "avg_logprob": -0.2434389931815011, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.08114345371723175}, {"id": 2, "seek": 0, "start": 15.72, "end": 19.56, "text": " recent advancements that's happened since then.", "tokens": [5162, 7295, 1117, 300, 311, 2011, 1670, 550, 13], "temperature": 0.0, "avg_logprob": -0.2434389931815011, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.08114345371723175}, {"id": 3, "seek": 0, "start": 19.56, "end": 22.72, "text": " So first we're going to talk about history and background.", "tokens": [407, 700, 321, 434, 516, 281, 751, 466, 2503, 293, 3678, 13], "temperature": 0.0, "avg_logprob": -0.2434389931815011, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.08114345371723175}, {"id": 4, "seek": 0, "start": 22.72, "end": 27.2, "text": " So everyone knows and loves word embeddings in NLP, right?", "tokens": [407, 1518, 3255, 293, 6752, 1349, 12240, 29432, 294, 426, 45196, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2434389931815011, "compression_ratio": 1.5596330275229358, "no_speech_prob": 0.08114345371723175}, {"id": 5, "seek": 2720, "start": 27.2, "end": 33.4, "text": " They're kind of the basis for why neural networks work for NLP.", "tokens": [814, 434, 733, 295, 264, 5143, 337, 983, 18161, 9590, 589, 337, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.18827636954710655, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.0003147760289721191}, {"id": 6, "seek": 2720, "start": 33.4, "end": 40.879999999999995, "text": " Because neural networks work in continuous space vectors and matrices and obviously text", "tokens": [1436, 18161, 9590, 589, 294, 10957, 1901, 18875, 293, 32284, 293, 2745, 2487], "temperature": 0.0, "avg_logprob": -0.18827636954710655, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.0003147760289721191}, {"id": 7, "seek": 2720, "start": 40.879999999999995, "end": 45.92, "text": " is discrete space and so there needed to be something to bridge the gap and it turns", "tokens": [307, 27706, 1901, 293, 370, 456, 2978, 281, 312, 746, 281, 7283, 264, 7417, 293, 309, 4523], "temperature": 0.0, "avg_logprob": -0.18827636954710655, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.0003147760289721191}, {"id": 8, "seek": 2720, "start": 45.92, "end": 49.8, "text": " out that the thing to bridge the gap, it's actually pretty simple, it's just a look-up", "tokens": [484, 300, 264, 551, 281, 7283, 264, 7417, 11, 309, 311, 767, 1238, 2199, 11, 309, 311, 445, 257, 574, 12, 1010], "temperature": 0.0, "avg_logprob": -0.18827636954710655, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.0003147760289721191}, {"id": 9, "seek": 2720, "start": 49.8, "end": 56.56, "text": " table from each, from a set of discrete vocabulary to a vector that's learned discriminatively", "tokens": [3199, 490, 1184, 11, 490, 257, 992, 295, 27706, 19864, 281, 257, 8062, 300, 311, 3264, 20828, 19020], "temperature": 0.0, "avg_logprob": -0.18827636954710655, "compression_ratio": 1.7242798353909465, "no_speech_prob": 0.0003147760289721191}, {"id": 10, "seek": 5656, "start": 56.56, "end": 57.56, "text": " end to end, right?", "tokens": [917, 281, 917, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2396360837496244, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.00021302918321453035}, {"id": 11, "seek": 5656, "start": 57.56, "end": 62.84, "text": " So originally these were just learned like in the original Benjiah 2003, neural language", "tokens": [407, 7993, 613, 645, 445, 3264, 411, 294, 264, 3380, 3964, 4013, 545, 16416, 11, 18161, 2856], "temperature": 0.0, "avg_logprob": -0.2396360837496244, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.00021302918321453035}, {"id": 12, "seek": 5656, "start": 62.84, "end": 67.52000000000001, "text": " on a paper, these were just trained discriminatively end to end and these were actually, and so", "tokens": [322, 257, 3035, 11, 613, 645, 445, 8895, 20828, 19020, 917, 281, 917, 293, 613, 645, 767, 11, 293, 370], "temperature": 0.0, "avg_logprob": -0.2396360837496244, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.00021302918321453035}, {"id": 13, "seek": 5656, "start": 67.52000000000001, "end": 72.48, "text": " then people would train language models and then use these pre-trained, use the embedding", "tokens": [550, 561, 576, 3847, 2856, 5245, 293, 550, 764, 613, 659, 12, 17227, 2001, 11, 764, 264, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.2396360837496244, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.00021302918321453035}, {"id": 14, "seek": 5656, "start": 72.48, "end": 76.92, "text": " layer as pre-trained representations for other tasks.", "tokens": [4583, 382, 659, 12, 17227, 2001, 33358, 337, 661, 9608, 13], "temperature": 0.0, "avg_logprob": -0.2396360837496244, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.00021302918321453035}, {"id": 15, "seek": 5656, "start": 76.92, "end": 79.52000000000001, "text": " But they wouldn't use the rest of the language model, they would just use the embedding layer.", "tokens": [583, 436, 2759, 380, 764, 264, 1472, 295, 264, 2856, 2316, 11, 436, 576, 445, 764, 264, 12240, 3584, 4583, 13], "temperature": 0.0, "avg_logprob": -0.2396360837496244, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.00021302918321453035}, {"id": 16, "seek": 5656, "start": 79.52000000000001, "end": 84.84, "text": " And then word devalc and glove and stuff came along where then people found a much cheaper,", "tokens": [400, 550, 1349, 1905, 304, 66, 293, 26928, 293, 1507, 1361, 2051, 689, 550, 561, 1352, 257, 709, 12284, 11], "temperature": 0.0, "avg_logprob": -0.2396360837496244, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.00021302918321453035}, {"id": 17, "seek": 8484, "start": 84.84, "end": 91.48, "text": " much more scalable way to train where you can just use the statistics of a corpus where", "tokens": [709, 544, 38481, 636, 281, 3847, 689, 291, 393, 445, 764, 264, 12523, 295, 257, 1181, 31624, 689], "temperature": 0.0, "avg_logprob": -0.19625624349294615, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.0127382312202826e-05}, {"id": 18, "seek": 8484, "start": 91.48, "end": 94.56, "text": " it's just a linear model so you don't have to compute these expensive feed-forward layers", "tokens": [309, 311, 445, 257, 8213, 2316, 370, 291, 500, 380, 362, 281, 14722, 613, 5124, 3154, 12, 13305, 7914], "temperature": 0.0, "avg_logprob": -0.19625624349294615, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.0127382312202826e-05}, {"id": 19, "seek": 8484, "start": 94.56, "end": 98.64, "text": " that you're going to throw out anyways and so you can scale up to like billions of tokens", "tokens": [300, 291, 434, 516, 281, 3507, 484, 13448, 293, 370, 291, 393, 4373, 493, 281, 411, 17375, 295, 22667], "temperature": 0.0, "avg_logprob": -0.19625624349294615, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.0127382312202826e-05}, {"id": 20, "seek": 8484, "start": 98.64, "end": 101.12, "text": " on a single CPU, right?", "tokens": [322, 257, 2167, 13199, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19625624349294615, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.0127382312202826e-05}, {"id": 21, "seek": 8484, "start": 101.12, "end": 107.96000000000001, "text": " So the problem though is that these word embeddings are applied in the context-free manner, right?", "tokens": [407, 264, 1154, 1673, 307, 300, 613, 1349, 12240, 29432, 366, 6456, 294, 264, 4319, 12, 10792, 9060, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19625624349294615, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.0127382312202826e-05}, {"id": 22, "seek": 8484, "start": 107.96000000000001, "end": 113.28, "text": " So for like a kind of a simple, to example, the word bank, if you say open a bank account", "tokens": [407, 337, 411, 257, 733, 295, 257, 2199, 11, 281, 1365, 11, 264, 1349, 3765, 11, 498, 291, 584, 1269, 257, 3765, 2696], "temperature": 0.0, "avg_logprob": -0.19625624349294615, "compression_ratio": 1.6494845360824741, "no_speech_prob": 6.0127382312202826e-05}, {"id": 23, "seek": 11328, "start": 113.28, "end": 115.8, "text": " and on a river bank, it's going to be the same embedding.", "tokens": [293, 322, 257, 6810, 3765, 11, 309, 311, 516, 281, 312, 264, 912, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.17719842579739153, "compression_ratio": 1.7322834645669292, "no_speech_prob": 1.7498885426903144e-05}, {"id": 24, "seek": 11328, "start": 115.8, "end": 120.96000000000001, "text": " So people try to do stuff like word sense embeddings where it's not just a single word, it's", "tokens": [407, 561, 853, 281, 360, 1507, 411, 1349, 2020, 12240, 29432, 689, 309, 311, 406, 445, 257, 2167, 1349, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.17719842579739153, "compression_ratio": 1.7322834645669292, "no_speech_prob": 1.7498885426903144e-05}, {"id": 25, "seek": 11328, "start": 120.96000000000001, "end": 126.24000000000001, "text": " a full word sense, but this kind of bank example, it's a little bit of a toy example, right?", "tokens": [257, 1577, 1349, 2020, 11, 457, 341, 733, 295, 3765, 1365, 11, 309, 311, 257, 707, 857, 295, 257, 12058, 1365, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17719842579739153, "compression_ratio": 1.7322834645669292, "no_speech_prob": 1.7498885426903144e-05}, {"id": 26, "seek": 11328, "start": 126.24000000000001, "end": 130.6, "text": " Almost any word has a different meaning depending on the context.", "tokens": [12627, 604, 1349, 575, 257, 819, 3620, 5413, 322, 264, 4319, 13], "temperature": 0.0, "avg_logprob": -0.17719842579739153, "compression_ratio": 1.7322834645669292, "no_speech_prob": 1.7498885426903144e-05}, {"id": 27, "seek": 11328, "start": 130.6, "end": 135.64, "text": " It's very, so even like open the bank account and I went to the bank, those are still", "tokens": [467, 311, 588, 11, 370, 754, 411, 1269, 264, 3765, 2696, 293, 286, 1437, 281, 264, 3765, 11, 729, 366, 920], "temperature": 0.0, "avg_logprob": -0.17719842579739153, "compression_ratio": 1.7322834645669292, "no_speech_prob": 1.7498885426903144e-05}, {"id": 28, "seek": 11328, "start": 135.64, "end": 139.52, "text": " in a semi-different senses of the word bank.", "tokens": [294, 257, 12909, 12, 67, 15790, 17057, 295, 264, 1349, 3765, 13], "temperature": 0.0, "avg_logprob": -0.17719842579739153, "compression_ratio": 1.7322834645669292, "no_speech_prob": 1.7498885426903144e-05}, {"id": 29, "seek": 13952, "start": 139.52, "end": 143.64000000000001, "text": " Kind of them is a, I mean they have different parts of each text kind of, well I guess", "tokens": [9242, 295, 552, 307, 257, 11, 286, 914, 436, 362, 819, 3166, 295, 1184, 2487, 733, 295, 11, 731, 286, 2041], "temperature": 0.0, "avg_logprob": -0.17641759297204396, "compression_ratio": 1.8523985239852399, "no_speech_prob": 9.020746074384078e-05}, {"id": 30, "seek": 13952, "start": 143.64000000000001, "end": 146.24, "text": " not really, but like they're kind of using different senses, right?", "tokens": [406, 534, 11, 457, 411, 436, 434, 733, 295, 1228, 819, 17057, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17641759297204396, "compression_ratio": 1.8523985239852399, "no_speech_prob": 9.020746074384078e-05}, {"id": 31, "seek": 13952, "start": 146.24, "end": 149.96, "text": " And so, yes, so we really need a contextual representation, right?", "tokens": [400, 370, 11, 2086, 11, 370, 321, 534, 643, 257, 35526, 10290, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17641759297204396, "compression_ratio": 1.8523985239852399, "no_speech_prob": 9.020746074384078e-05}, {"id": 32, "seek": 13952, "start": 149.96, "end": 156.8, "text": " So we want something where it's a representation of a word after it's been put into the context", "tokens": [407, 321, 528, 746, 689, 309, 311, 257, 10290, 295, 257, 1349, 934, 309, 311, 668, 829, 666, 264, 4319], "temperature": 0.0, "avg_logprob": -0.17641759297204396, "compression_ratio": 1.8523985239852399, "no_speech_prob": 9.020746074384078e-05}, {"id": 33, "seek": 13952, "start": 156.8, "end": 157.96, "text": " of the sense that we've seen it in, right?", "tokens": [295, 264, 2020, 300, 321, 600, 1612, 309, 294, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17641759297204396, "compression_ratio": 1.8523985239852399, "no_speech_prob": 9.020746074384078e-05}, {"id": 34, "seek": 13952, "start": 157.96, "end": 159.52, "text": " Which would be like at the bottom here.", "tokens": [3013, 576, 312, 411, 412, 264, 2767, 510, 13], "temperature": 0.0, "avg_logprob": -0.17641759297204396, "compression_ratio": 1.8523985239852399, "no_speech_prob": 9.020746074384078e-05}, {"id": 35, "seek": 13952, "start": 159.52, "end": 165.76000000000002, "text": " So kind of for history of contextual representations, the first big paper for this type of contextual", "tokens": [407, 733, 295, 337, 2503, 295, 35526, 33358, 11, 264, 700, 955, 3035, 337, 341, 2010, 295, 35526], "temperature": 0.0, "avg_logprob": -0.17641759297204396, "compression_ratio": 1.8523985239852399, "no_speech_prob": 9.020746074384078e-05}, {"id": 36, "seek": 16576, "start": 165.76, "end": 170.79999999999998, "text": " representation was a paper from Google in 2015 called semi-supervised sequence learning", "tokens": [10290, 390, 257, 3035, 490, 3329, 294, 7546, 1219, 12909, 12, 48172, 24420, 8310, 2539], "temperature": 0.0, "avg_logprob": -0.26038312200290054, "compression_ratio": 1.665680473372781, "no_speech_prob": 0.00023025718110147864}, {"id": 37, "seek": 16576, "start": 170.79999999999998, "end": 172.79999999999998, "text": " from Andrew Dianne Cochle.", "tokens": [490, 10110, 413, 952, 716, 3066, 339, 306, 13], "temperature": 0.0, "avg_logprob": -0.26038312200290054, "compression_ratio": 1.665680473372781, "no_speech_prob": 0.00023025718110147864}, {"id": 38, "seek": 16576, "start": 172.79999999999998, "end": 178.16, "text": " And so in this one, it was actually very similar to papers that came after it, it didn't", "tokens": [400, 370, 294, 341, 472, 11, 309, 390, 767, 588, 2531, 281, 10577, 300, 1361, 934, 309, 11, 309, 994, 380], "temperature": 0.0, "avg_logprob": -0.26038312200290054, "compression_ratio": 1.665680473372781, "no_speech_prob": 0.00023025718110147864}, {"id": 39, "seek": 16576, "start": 178.16, "end": 179.88, "text": " get as much attention for various reasons.", "tokens": [483, 382, 709, 3202, 337, 3683, 4112, 13], "temperature": 0.0, "avg_logprob": -0.26038312200290054, "compression_ratio": 1.665680473372781, "no_speech_prob": 0.00023025718110147864}, {"id": 40, "seek": 16576, "start": 179.88, "end": 184.85999999999999, "text": " So but basically they had some classification task like sentiment classification on", "tokens": [407, 457, 1936, 436, 632, 512, 21538, 5633, 411, 16149, 21538, 322], "temperature": 0.0, "avg_logprob": -0.26038312200290054, "compression_ratio": 1.665680473372781, "no_speech_prob": 0.00023025718110147864}, {"id": 41, "seek": 16576, "start": 184.85999999999999, "end": 187.72, "text": " movie reviews, and they had a big corpus on movie reviews.", "tokens": [3169, 10229, 11, 293, 436, 632, 257, 955, 1181, 31624, 322, 3169, 10229, 13], "temperature": 0.0, "avg_logprob": -0.26038312200290054, "compression_ratio": 1.665680473372781, "no_speech_prob": 0.00023025718110147864}, {"id": 42, "seek": 16576, "start": 187.72, "end": 191.44, "text": " And so then they said what happens if we just take our existing LSTM model and instead", "tokens": [400, 370, 550, 436, 848, 437, 2314, 498, 321, 445, 747, 527, 6741, 441, 6840, 44, 2316, 293, 2602], "temperature": 0.0, "avg_logprob": -0.26038312200290054, "compression_ratio": 1.665680473372781, "no_speech_prob": 0.00023025718110147864}, {"id": 43, "seek": 16576, "start": 191.44, "end": 193.95999999999998, "text": " of just using pre-trained embeddings, which everyone has already been doing since like", "tokens": [295, 445, 1228, 659, 12, 17227, 2001, 12240, 29432, 11, 597, 1518, 575, 1217, 668, 884, 1670, 411], "temperature": 0.0, "avg_logprob": -0.26038312200290054, "compression_ratio": 1.665680473372781, "no_speech_prob": 0.00023025718110147864}, {"id": 44, "seek": 19396, "start": 193.96, "end": 199.4, "text": " at least for the like actually probably since 2003, people had been using pre-trained embeddings,", "tokens": [412, 1935, 337, 264, 411, 767, 1391, 1670, 16416, 11, 561, 632, 668, 1228, 659, 12, 17227, 2001, 12240, 29432, 11], "temperature": 0.0, "avg_logprob": -0.18334909895776022, "compression_ratio": 1.9536423841059603, "no_speech_prob": 3.9396491047227755e-05}, {"id": 45, "seek": 19396, "start": 199.4, "end": 203.64000000000001, "text": " but they said let's actually pre-trained the entire model as a language model and then", "tokens": [457, 436, 848, 718, 311, 767, 659, 12, 17227, 2001, 264, 2302, 2316, 382, 257, 2856, 2316, 293, 550], "temperature": 0.0, "avg_logprob": -0.18334909895776022, "compression_ratio": 1.9536423841059603, "no_speech_prob": 3.9396491047227755e-05}, {"id": 46, "seek": 19396, "start": 203.64000000000001, "end": 206.32, "text": " let's fine tune it for our classification task.", "tokens": [718, 311, 2489, 10864, 309, 337, 527, 21538, 5633, 13], "temperature": 0.0, "avg_logprob": -0.18334909895776022, "compression_ratio": 1.9536423841059603, "no_speech_prob": 3.9396491047227755e-05}, {"id": 47, "seek": 19396, "start": 206.32, "end": 211.60000000000002, "text": " And they got pretty good results but not like stellar results.", "tokens": [400, 436, 658, 1238, 665, 3542, 457, 406, 411, 42333, 3542, 13], "temperature": 0.0, "avg_logprob": -0.18334909895776022, "compression_ratio": 1.9536423841059603, "no_speech_prob": 3.9396491047227755e-05}, {"id": 48, "seek": 19396, "start": 211.60000000000002, "end": 214.04000000000002, "text": " And so now we know that the reason why they didn't get stellar results is they didn't train", "tokens": [400, 370, 586, 321, 458, 300, 264, 1778, 983, 436, 994, 380, 483, 42333, 3542, 307, 436, 994, 380, 3847], "temperature": 0.0, "avg_logprob": -0.18334909895776022, "compression_ratio": 1.9536423841059603, "no_speech_prob": 3.9396491047227755e-05}, {"id": 49, "seek": 19396, "start": 214.04000000000002, "end": 216.8, "text": " on enough data and they, because they basically train on the same corpus that they were training", "tokens": [322, 1547, 1412, 293, 436, 11, 570, 436, 1936, 3847, 322, 264, 912, 1181, 31624, 300, 436, 645, 3097], "temperature": 0.0, "avg_logprob": -0.18334909895776022, "compression_ratio": 1.9536423841059603, "no_speech_prob": 3.9396491047227755e-05}, {"id": 50, "seek": 19396, "start": 216.8, "end": 220.32, "text": " on and they trained the same size model that they were training on.", "tokens": [322, 293, 436, 8895, 264, 912, 2744, 2316, 300, 436, 645, 3097, 322, 13], "temperature": 0.0, "avg_logprob": -0.18334909895776022, "compression_ratio": 1.9536423841059603, "no_speech_prob": 3.9396491047227755e-05}, {"id": 51, "seek": 19396, "start": 220.32, "end": 221.68, "text": " Which we now know needs to be bigger.", "tokens": [3013, 321, 586, 458, 2203, 281, 312, 3801, 13], "temperature": 0.0, "avg_logprob": -0.18334909895776022, "compression_ratio": 1.9536423841059603, "no_speech_prob": 3.9396491047227755e-05}, {"id": 52, "seek": 22168, "start": 221.68, "end": 226.92000000000002, "text": " But that's kind of, this was already kind of a little bit ahead of its time partially", "tokens": [583, 300, 311, 733, 295, 11, 341, 390, 1217, 733, 295, 257, 707, 857, 2286, 295, 1080, 565, 18886], "temperature": 0.0, "avg_logprob": -0.3123759688618027, "compression_ratio": 1.5413533834586466, "no_speech_prob": 7.363554323092103e-05}, {"id": 53, "seek": 22168, "start": 226.92000000000002, "end": 230.4, "text": " because like stuff wasn't, like we didn't have a bit of a computer back then even those", "tokens": [570, 411, 1507, 2067, 380, 11, 411, 321, 994, 380, 362, 257, 857, 295, 257, 3820, 646, 550, 754, 729], "temperature": 0.0, "avg_logprob": -0.3123759688618027, "compression_ratio": 1.5413533834586466, "no_speech_prob": 7.363554323092103e-05}, {"id": 54, "seek": 22168, "start": 230.4, "end": 232.32, "text": " only five years ago.", "tokens": [787, 1732, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.3123759688618027, "compression_ratio": 1.5413533834586466, "no_speech_prob": 7.363554323092103e-05}, {"id": 55, "seek": 22168, "start": 232.32, "end": 234.24, "text": " And it would have been more expensive.", "tokens": [400, 309, 576, 362, 668, 544, 5124, 13], "temperature": 0.0, "avg_logprob": -0.3123759688618027, "compression_ratio": 1.5413533834586466, "no_speech_prob": 7.363554323092103e-05}, {"id": 56, "seek": 22168, "start": 234.24, "end": 242.68, "text": " So and then in 2017, Elmok came out, which was from the University of Washington in AI2.", "tokens": [407, 293, 550, 294, 6591, 11, 2699, 76, 453, 1361, 484, 11, 597, 390, 490, 264, 3535, 295, 6149, 294, 7318, 17, 13], "temperature": 0.0, "avg_logprob": -0.3123759688618027, "compression_ratio": 1.5413533834586466, "no_speech_prob": 7.363554323092103e-05}, {"id": 57, "seek": 22168, "start": 242.68, "end": 248.96, "text": " And so this one, they did something pretty clever where they took, you train a language", "tokens": [400, 370, 341, 472, 11, 436, 630, 746, 1238, 13494, 689, 436, 1890, 11, 291, 3847, 257, 2856], "temperature": 0.0, "avg_logprob": -0.3123759688618027, "compression_ratio": 1.5413533834586466, "no_speech_prob": 7.363554323092103e-05}, {"id": 58, "seek": 24896, "start": 248.96, "end": 252.28, "text": " model on a big corpus, so they trained it on a billion word corpus and they trained a", "tokens": [2316, 322, 257, 955, 1181, 31624, 11, 370, 436, 8895, 309, 322, 257, 5218, 1349, 1181, 31624, 293, 436, 8895, 257], "temperature": 0.0, "avg_logprob": -0.20134617732121393, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.96267413534224e-05}, {"id": 59, "seek": 24896, "start": 252.28, "end": 257.52, "text": " big model, LSTM with 4,000 hidden dimensions which is quite expensive.", "tokens": [955, 2316, 11, 441, 6840, 44, 365, 1017, 11, 1360, 7633, 12819, 597, 307, 1596, 5124, 13], "temperature": 0.0, "avg_logprob": -0.20134617732121393, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.96267413534224e-05}, {"id": 60, "seek": 24896, "start": 257.52, "end": 259.56, "text": " And they trained a bi-directional model.", "tokens": [400, 436, 8895, 257, 3228, 12, 18267, 41048, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20134617732121393, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.96267413534224e-05}, {"id": 61, "seek": 24896, "start": 259.56, "end": 264.40000000000003, "text": " But it was kind of weekly bi-directional where they trained a left-right model and then", "tokens": [583, 309, 390, 733, 295, 12460, 3228, 12, 18267, 41048, 689, 436, 8895, 257, 1411, 12, 1938, 2316, 293, 550], "temperature": 0.0, "avg_logprob": -0.20134617732121393, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.96267413534224e-05}, {"id": 62, "seek": 24896, "start": 264.40000000000003, "end": 268.40000000000003, "text": " a left-right model and then they concatenated the two.", "tokens": [257, 1411, 12, 1938, 2316, 293, 550, 436, 1588, 7186, 770, 264, 732, 13], "temperature": 0.0, "avg_logprob": -0.20134617732121393, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.96267413534224e-05}, {"id": 63, "seek": 24896, "start": 268.40000000000003, "end": 270.68, "text": " And they called these contextual pre-trained embeddings.", "tokens": [400, 436, 1219, 613, 35526, 659, 12, 17227, 2001, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.20134617732121393, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.96267413534224e-05}, {"id": 64, "seek": 24896, "start": 270.68, "end": 276.12, "text": " And so the idea behind Elmok is that this doesn't actually change your existing model architecture.", "tokens": [400, 370, 264, 1558, 2261, 2699, 76, 453, 307, 300, 341, 1177, 380, 767, 1319, 428, 6741, 2316, 9482, 13], "temperature": 0.0, "avg_logprob": -0.20134617732121393, "compression_ratio": 1.868421052631579, "no_speech_prob": 7.96267413534224e-05}, {"id": 65, "seek": 27612, "start": 276.12, "end": 280.68, "text": " You kind of take whatever task specific model architecture that you have, which could", "tokens": [509, 733, 295, 747, 2035, 5633, 2685, 2316, 9482, 300, 291, 362, 11, 597, 727], "temperature": 0.0, "avg_logprob": -0.1876860577127208, "compression_ratio": 1.85, "no_speech_prob": 5.305270678945817e-05}, {"id": 66, "seek": 27612, "start": 280.68, "end": 285.28000000000003, "text": " be for question answering, it might be some sort of fancy model where you do a LSTM over", "tokens": [312, 337, 1168, 13430, 11, 309, 1062, 312, 512, 1333, 295, 10247, 2316, 689, 291, 360, 257, 441, 6840, 44, 670], "temperature": 0.0, "avg_logprob": -0.1876860577127208, "compression_ratio": 1.85, "no_speech_prob": 5.305270678945817e-05}, {"id": 67, "seek": 27612, "start": 285.28000000000003, "end": 290.64, "text": " the source and over the question and over the answer, then you tend to one another and", "tokens": [264, 4009, 293, 670, 264, 1168, 293, 670, 264, 1867, 11, 550, 291, 3928, 281, 472, 1071, 293], "temperature": 0.0, "avg_logprob": -0.1876860577127208, "compression_ratio": 1.85, "no_speech_prob": 5.305270678945817e-05}, {"id": 68, "seek": 27612, "start": 290.64, "end": 292.04, "text": " whatever kind of architecture you have.", "tokens": [2035, 733, 295, 9482, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.1876860577127208, "compression_ratio": 1.85, "no_speech_prob": 5.305270678945817e-05}, {"id": 69, "seek": 27612, "start": 292.04, "end": 298.04, "text": " And wherever you would have put in glove embeddings before, now you put in Elmok embeddings.", "tokens": [400, 8660, 291, 576, 362, 829, 294, 26928, 12240, 29432, 949, 11, 586, 291, 829, 294, 2699, 76, 453, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.1876860577127208, "compression_ratio": 1.85, "no_speech_prob": 5.305270678945817e-05}, {"id": 70, "seek": 27612, "start": 298.04, "end": 303.72, "text": " And so this got set of the art on everything at the time, question answering, semantic", "tokens": [400, 370, 341, 658, 992, 295, 264, 1523, 322, 1203, 412, 264, 565, 11, 1168, 13430, 11, 47982], "temperature": 0.0, "avg_logprob": -0.1876860577127208, "compression_ratio": 1.85, "no_speech_prob": 5.305270678945817e-05}, {"id": 71, "seek": 30372, "start": 303.72, "end": 307.68, "text": " parsing, syntactic parsing, because it was, and so if you just took any existing kind", "tokens": [21156, 278, 11, 23980, 19892, 21156, 278, 11, 570, 309, 390, 11, 293, 370, 498, 291, 445, 1890, 604, 6741, 733], "temperature": 0.0, "avg_logprob": -0.23215047244367928, "compression_ratio": 1.568, "no_speech_prob": 9.165597293758765e-05}, {"id": 72, "seek": 30372, "start": 307.68, "end": 312.36, "text": " of state-of-the-art model, you could fit in, put in Elmok embeddings and get state-of-the-art,", "tokens": [295, 1785, 12, 2670, 12, 3322, 12, 446, 2316, 11, 291, 727, 3318, 294, 11, 829, 294, 2699, 76, 453, 12240, 29432, 293, 483, 1785, 12, 2670, 12, 3322, 12, 446, 11], "temperature": 0.0, "avg_logprob": -0.23215047244367928, "compression_ratio": 1.568, "no_speech_prob": 9.165597293758765e-05}, {"id": 73, "seek": 30372, "start": 312.36, "end": 313.36, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.23215047244367928, "compression_ratio": 1.568, "no_speech_prob": 9.165597293758765e-05}, {"id": 74, "seek": 30372, "start": 313.36, "end": 317.28000000000003, "text": " But they weren't, but these were kind of, the models were kind of fixed.", "tokens": [583, 436, 4999, 380, 11, 457, 613, 645, 733, 295, 11, 264, 5245, 645, 733, 295, 6806, 13], "temperature": 0.0, "avg_logprob": -0.23215047244367928, "compression_ratio": 1.568, "no_speech_prob": 9.165597293758765e-05}, {"id": 75, "seek": 30372, "start": 317.28000000000003, "end": 324.56, "text": " And so then after that, opening AI published, improving language understanding with generative", "tokens": [400, 370, 550, 934, 300, 11, 5193, 7318, 6572, 11, 11470, 2856, 3701, 365, 1337, 1166], "temperature": 0.0, "avg_logprob": -0.23215047244367928, "compression_ratio": 1.568, "no_speech_prob": 9.165597293758765e-05}, {"id": 76, "seek": 30372, "start": 324.56, "end": 327.48, "text": " pre-training, which is called GPT-1.", "tokens": [659, 12, 17227, 1760, 11, 597, 307, 1219, 26039, 51, 12, 16, 13], "temperature": 0.0, "avg_logprob": -0.23215047244367928, "compression_ratio": 1.568, "no_speech_prob": 9.165597293758765e-05}, {"id": 77, "seek": 32748, "start": 327.48, "end": 337.08000000000004, "text": " And so in this, they took a similar large corpus, that of billion words, and they trained a", "tokens": [400, 370, 294, 341, 11, 436, 1890, 257, 2531, 2416, 1181, 31624, 11, 300, 295, 5218, 2283, 11, 293, 436, 8895, 257], "temperature": 0.0, "avg_logprob": -0.2028820695548222, "compression_ratio": 1.9685314685314685, "no_speech_prob": 1.544218866911251e-05}, {"id": 78, "seek": 32748, "start": 337.08000000000004, "end": 338.40000000000003, "text": " very large language model.", "tokens": [588, 2416, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2028820695548222, "compression_ratio": 1.9685314685314685, "no_speech_prob": 1.544218866911251e-05}, {"id": 79, "seek": 32748, "start": 338.40000000000003, "end": 342.20000000000005, "text": " So a 12-layer language model, which at the time was maybe, I don't know whether it was", "tokens": [407, 257, 2272, 12, 8376, 260, 2856, 2316, 11, 597, 412, 264, 565, 390, 1310, 11, 286, 500, 380, 458, 1968, 309, 390], "temperature": 0.0, "avg_logprob": -0.2028820695548222, "compression_ratio": 1.9685314685314685, "no_speech_prob": 1.544218866911251e-05}, {"id": 80, "seek": 32748, "start": 342.20000000000005, "end": 344.08000000000004, "text": " actually a large language model that had been trained at the time, certainly it was", "tokens": [767, 257, 2416, 2856, 2316, 300, 632, 668, 8895, 412, 264, 565, 11, 3297, 309, 390], "temperature": 0.0, "avg_logprob": -0.2028820695548222, "compression_ratio": 1.9685314685314685, "no_speech_prob": 1.544218866911251e-05}, {"id": 81, "seek": 32748, "start": 344.08000000000004, "end": 347.88, "text": " the largest language model that had been trained on that much data for a kind of open-source", "tokens": [264, 6443, 2856, 2316, 300, 632, 668, 8895, 322, 300, 709, 1412, 337, 257, 733, 295, 1269, 12, 41676], "temperature": 0.0, "avg_logprob": -0.2028820695548222, "compression_ratio": 1.9685314685314685, "no_speech_prob": 1.544218866911251e-05}, {"id": 82, "seek": 32748, "start": 347.88, "end": 349.72, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.2028820695548222, "compression_ratio": 1.9685314685314685, "no_speech_prob": 1.544218866911251e-05}, {"id": 83, "seek": 32748, "start": 349.72, "end": 354.0, "text": " And when I first read it, I actually thought that it was too big, not that it was worse,", "tokens": [400, 562, 286, 700, 1401, 309, 11, 286, 767, 1194, 300, 309, 390, 886, 955, 11, 406, 300, 309, 390, 5324, 11], "temperature": 0.0, "avg_logprob": -0.2028820695548222, "compression_ratio": 1.9685314685314685, "no_speech_prob": 1.544218866911251e-05}, {"id": 84, "seek": 32748, "start": 354.0, "end": 356.24, "text": " but that they were kind of just showing off by showing how big of a model they could", "tokens": [457, 300, 436, 645, 733, 295, 445, 4099, 766, 538, 4099, 577, 955, 295, 257, 2316, 436, 727], "temperature": 0.0, "avg_logprob": -0.2028820695548222, "compression_ratio": 1.9685314685314685, "no_speech_prob": 1.544218866911251e-05}, {"id": 85, "seek": 35624, "start": 356.24, "end": 357.24, "text": " train.", "tokens": [3847, 13], "temperature": 0.0, "avg_logprob": -0.16841332689463664, "compression_ratio": 1.7467105263157894, "no_speech_prob": 8.747708488954231e-05}, {"id": 86, "seek": 35624, "start": 357.24, "end": 361.96000000000004, "text": " But now we know that actually this depth that they had was actually kind of the crucial element.", "tokens": [583, 586, 321, 458, 300, 767, 341, 7161, 300, 436, 632, 390, 767, 733, 295, 264, 11462, 4478, 13], "temperature": 0.0, "avg_logprob": -0.16841332689463664, "compression_ratio": 1.7467105263157894, "no_speech_prob": 8.747708488954231e-05}, {"id": 87, "seek": 35624, "start": 361.96000000000004, "end": 363.96000000000004, "text": " So they did something that was like fairly simple, right?", "tokens": [407, 436, 630, 746, 300, 390, 411, 6457, 2199, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16841332689463664, "compression_ratio": 1.7467105263157894, "no_speech_prob": 8.747708488954231e-05}, {"id": 88, "seek": 35624, "start": 363.96000000000004, "end": 367.36, "text": " They just trained a language model, a very large one, and then they just fine-tuned it by", "tokens": [814, 445, 8895, 257, 2856, 2316, 11, 257, 588, 2416, 472, 11, 293, 550, 436, 445, 2489, 12, 83, 43703, 309, 538], "temperature": 0.0, "avg_logprob": -0.16841332689463664, "compression_ratio": 1.7467105263157894, "no_speech_prob": 8.747708488954231e-05}, {"id": 89, "seek": 35624, "start": 367.36, "end": 371.36, "text": " taking the last token and then fine-tuning it for a classification task, right?", "tokens": [1940, 264, 1036, 14862, 293, 550, 2489, 12, 83, 37726, 309, 337, 257, 21538, 5633, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16841332689463664, "compression_ratio": 1.7467105263157894, "no_speech_prob": 8.747708488954231e-05}, {"id": 90, "seek": 35624, "start": 371.36, "end": 373.92, "text": " So is this positive or negative?", "tokens": [407, 307, 341, 3353, 420, 3671, 30], "temperature": 0.0, "avg_logprob": -0.16841332689463664, "compression_ratio": 1.7467105263157894, "no_speech_prob": 8.747708488954231e-05}, {"id": 91, "seek": 35624, "start": 373.92, "end": 378.56, "text": " And they got basically state-of-the-art on lots of different classification tasks.", "tokens": [400, 436, 658, 1936, 1785, 12, 2670, 12, 3322, 12, 446, 322, 3195, 295, 819, 21538, 9608, 13], "temperature": 0.0, "avg_logprob": -0.16841332689463664, "compression_ratio": 1.7467105263157894, "no_speech_prob": 8.747708488954231e-05}, {"id": 92, "seek": 35624, "start": 378.56, "end": 385.04, "text": " But, and so I'm going to actually take a kind of a side here before I go into BERT,", "tokens": [583, 11, 293, 370, 286, 478, 516, 281, 767, 747, 257, 733, 295, 257, 1252, 510, 949, 286, 352, 666, 363, 31479, 11], "temperature": 0.0, "avg_logprob": -0.16841332689463664, "compression_ratio": 1.7467105263157894, "no_speech_prob": 8.747708488954231e-05}, {"id": 93, "seek": 38504, "start": 385.04, "end": 386.04, "text": " which is about transformer.", "tokens": [597, 307, 466, 31782, 13], "temperature": 0.0, "avg_logprob": -0.23009664916992187, "compression_ratio": 1.6842105263157894, "no_speech_prob": 7.480892963940278e-05}, {"id": 94, "seek": 38504, "start": 386.04, "end": 391.56, "text": " So that was the other kind of big thing, like the big precursor that allowed BERT and", "tokens": [407, 300, 390, 264, 661, 733, 295, 955, 551, 11, 411, 264, 955, 41736, 284, 300, 4350, 363, 31479, 293], "temperature": 0.0, "avg_logprob": -0.23009664916992187, "compression_ratio": 1.6842105263157894, "no_speech_prob": 7.480892963940278e-05}, {"id": 95, "seek": 38504, "start": 391.56, "end": 393.24, "text": " GPT to work well, right?", "tokens": [26039, 51, 281, 589, 731, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23009664916992187, "compression_ratio": 1.6842105263157894, "no_speech_prob": 7.480892963940278e-05}, {"id": 96, "seek": 38504, "start": 393.24, "end": 398.48, "text": " So BERT and GPT both use the transformer, which I'm sure you guys have learned about.", "tokens": [407, 363, 31479, 293, 26039, 51, 1293, 764, 264, 31782, 11, 597, 286, 478, 988, 291, 1074, 362, 3264, 466, 13], "temperature": 0.0, "avg_logprob": -0.23009664916992187, "compression_ratio": 1.6842105263157894, "no_speech_prob": 7.480892963940278e-05}, {"id": 97, "seek": 38504, "start": 398.48, "end": 403.32000000000005, "text": " And so I don't need to necessarily go into all the details about it.", "tokens": [400, 370, 286, 500, 380, 643, 281, 4725, 352, 666, 439, 264, 4365, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.23009664916992187, "compression_ratio": 1.6842105263157894, "no_speech_prob": 7.480892963940278e-05}, {"id": 98, "seek": 38504, "start": 403.32000000000005, "end": 409.0, "text": " But, so it has multi-headed attention, feed-forward layers, lay-in-arm.", "tokens": [583, 11, 370, 309, 575, 4825, 12, 28409, 3202, 11, 3154, 12, 13305, 7914, 11, 2360, 12, 259, 12, 4452, 13], "temperature": 0.0, "avg_logprob": -0.23009664916992187, "compression_ratio": 1.6842105263157894, "no_speech_prob": 7.480892963940278e-05}, {"id": 99, "seek": 38504, "start": 409.0, "end": 411.8, "text": " I won't go into all the details because I think you guys already learned about it.", "tokens": [286, 1582, 380, 352, 666, 439, 264, 4365, 570, 286, 519, 291, 1074, 1217, 3264, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.23009664916992187, "compression_ratio": 1.6842105263157894, "no_speech_prob": 7.480892963940278e-05}, {"id": 100, "seek": 41180, "start": 411.8, "end": 417.48, "text": " But the big thing about why this kind of took over is, there's really two advantages", "tokens": [583, 264, 955, 551, 466, 983, 341, 733, 295, 1890, 670, 307, 11, 456, 311, 534, 732, 14906], "temperature": 0.0, "avg_logprob": -0.24179351573087732, "compression_ratio": 1.5879828326180256, "no_speech_prob": 8.090810297289863e-05}, {"id": 101, "seek": 41180, "start": 417.48, "end": 418.72, "text": " versus the LSTM.", "tokens": [5717, 264, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.24179351573087732, "compression_ratio": 1.5879828326180256, "no_speech_prob": 8.090810297289863e-05}, {"id": 102, "seek": 41180, "start": 418.72, "end": 420.88, "text": " One is that there's no locality bias.", "tokens": [1485, 307, 300, 456, 311, 572, 1628, 1860, 12577, 13], "temperature": 0.0, "avg_logprob": -0.24179351573087732, "compression_ratio": 1.5879828326180256, "no_speech_prob": 8.090810297289863e-05}, {"id": 103, "seek": 41180, "start": 420.88, "end": 426.96000000000004, "text": " And so, longest since context has an equal to the opportunity to short distance context,", "tokens": [400, 370, 11, 15438, 1670, 4319, 575, 364, 2681, 281, 264, 2650, 281, 2099, 4560, 4319, 11], "temperature": 0.0, "avg_logprob": -0.24179351573087732, "compression_ratio": 1.5879828326180256, "no_speech_prob": 8.090810297289863e-05}, {"id": 104, "seek": 41180, "start": 426.96000000000004, "end": 429.08000000000004, "text": " which is important.", "tokens": [597, 307, 1021, 13], "temperature": 0.0, "avg_logprob": -0.24179351573087732, "compression_ratio": 1.5879828326180256, "no_speech_prob": 8.090810297289863e-05}, {"id": 105, "seek": 41180, "start": 429.08000000000004, "end": 434.64, "text": " So for like normal language understanding, that's the locality bias of LSTM's is generally", "tokens": [407, 337, 411, 2710, 2856, 3701, 11, 300, 311, 264, 1628, 1860, 12577, 295, 441, 6840, 44, 311, 307, 5101], "temperature": 0.0, "avg_logprob": -0.24179351573087732, "compression_ratio": 1.5879828326180256, "no_speech_prob": 8.090810297289863e-05}, {"id": 106, "seek": 41180, "start": 434.64, "end": 438.0, "text": " considered to be a good thing.", "tokens": [4888, 281, 312, 257, 665, 551, 13], "temperature": 0.0, "avg_logprob": -0.24179351573087732, "compression_ratio": 1.5879828326180256, "no_speech_prob": 8.090810297289863e-05}, {"id": 107, "seek": 43800, "start": 438.0, "end": 442.56, "text": " Because local context is more relevant than longest since context.", "tokens": [1436, 2654, 4319, 307, 544, 7340, 813, 15438, 1670, 4319, 13], "temperature": 0.0, "avg_logprob": -0.19013956052447678, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5611606512684375e-05}, {"id": 108, "seek": 43800, "start": 442.56, "end": 447.08, "text": " But the way that GPT and BERT and other models work is that they actually can catenate", "tokens": [583, 264, 636, 300, 26039, 51, 293, 363, 31479, 293, 661, 5245, 589, 307, 300, 436, 767, 393, 3857, 268, 473], "temperature": 0.0, "avg_logprob": -0.19013956052447678, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5611606512684375e-05}, {"id": 109, "seek": 43800, "start": 447.08, "end": 448.56, "text": " context.", "tokens": [4319, 13], "temperature": 0.0, "avg_logprob": -0.19013956052447678, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5611606512684375e-05}, {"id": 110, "seek": 43800, "start": 448.56, "end": 455.32, "text": " And so if you have a model that says does sentence one entail sentence two, the way that it was", "tokens": [400, 370, 498, 291, 362, 257, 2316, 300, 1619, 775, 8174, 472, 948, 864, 8174, 732, 11, 264, 636, 300, 309, 390], "temperature": 0.0, "avg_logprob": -0.19013956052447678, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5611606512684375e-05}, {"id": 111, "seek": 43800, "start": 455.32, "end": 459.68, "text": " done historically, meaning like before GPT, was that you would like encode them both,", "tokens": [1096, 16180, 11, 3620, 411, 949, 26039, 51, 11, 390, 300, 291, 576, 411, 2058, 1429, 552, 1293, 11], "temperature": 0.0, "avg_logprob": -0.19013956052447678, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5611606512684375e-05}, {"id": 112, "seek": 43800, "start": 459.68, "end": 463.52, "text": " let's say with an LSTM, then you would do attention from one to the other.", "tokens": [718, 311, 584, 365, 364, 441, 6840, 44, 11, 550, 291, 576, 360, 3202, 490, 472, 281, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.19013956052447678, "compression_ratio": 1.6827309236947792, "no_speech_prob": 5.5611606512684375e-05}, {"id": 113, "seek": 46352, "start": 463.52, "end": 469.44, "text": " With a transformer, you can just put them into the same sequence and give them separate", "tokens": [2022, 257, 31782, 11, 291, 393, 445, 829, 552, 666, 264, 912, 8310, 293, 976, 552, 4994], "temperature": 0.0, "avg_logprob": -0.2559975501029722, "compression_ratio": 1.9799196787148594, "no_speech_prob": 5.2221239457139745e-05}, {"id": 114, "seek": 46352, "start": 469.44, "end": 471.47999999999996, "text": " sequence of adding to it at a separated token.", "tokens": [8310, 295, 5127, 281, 309, 412, 257, 12005, 14862, 13], "temperature": 0.0, "avg_logprob": -0.2559975501029722, "compression_ratio": 1.9799196787148594, "no_speech_prob": 5.2221239457139745e-05}, {"id": 115, "seek": 46352, "start": 471.47999999999996, "end": 476.76, "text": " And it will learn how to, and then it can, things can attend to its own sentence locally.", "tokens": [400, 309, 486, 1466, 577, 281, 11, 293, 550, 309, 393, 11, 721, 393, 6888, 281, 1080, 1065, 8174, 16143, 13], "temperature": 0.0, "avg_logprob": -0.2559975501029722, "compression_ratio": 1.9799196787148594, "no_speech_prob": 5.2221239457139745e-05}, {"id": 116, "seek": 46352, "start": 476.76, "end": 482.84, "text": " But it can also attend all the way to the other sentence for almost, for no, it's just", "tokens": [583, 309, 393, 611, 6888, 439, 264, 636, 281, 264, 661, 8174, 337, 1920, 11, 337, 572, 11, 309, 311, 445], "temperature": 0.0, "avg_logprob": -0.2559975501029722, "compression_ratio": 1.9799196787148594, "no_speech_prob": 5.2221239457139745e-05}, {"id": 117, "seek": 46352, "start": 482.84, "end": 484.91999999999996, "text": " as easy for it to attend all the way to the other sentence.", "tokens": [382, 1858, 337, 309, 281, 6888, 439, 264, 636, 281, 264, 661, 8174, 13], "temperature": 0.0, "avg_logprob": -0.2559975501029722, "compression_ratio": 1.9799196787148594, "no_speech_prob": 5.2221239457139745e-05}, {"id": 118, "seek": 46352, "start": 484.91999999999996, "end": 487.68, "text": " And so when you do this, kind of you can just pack everything into a single sequence and", "tokens": [400, 370, 562, 291, 360, 341, 11, 733, 295, 291, 393, 445, 2844, 1203, 666, 257, 2167, 8310, 293], "temperature": 0.0, "avg_logprob": -0.2559975501029722, "compression_ratio": 1.9799196787148594, "no_speech_prob": 5.2221239457139745e-05}, {"id": 119, "seek": 46352, "start": 487.68, "end": 489.35999999999996, "text": " then everything will be learned.", "tokens": [550, 1203, 486, 312, 3264, 13], "temperature": 0.0, "avg_logprob": -0.2559975501029722, "compression_ratio": 1.9799196787148594, "no_speech_prob": 5.2221239457139745e-05}, {"id": 120, "seek": 48936, "start": 489.36, "end": 493.72, "text": " Rather than having to do this as part of the model architecture, which ends up being a", "tokens": [16571, 813, 1419, 281, 360, 341, 382, 644, 295, 264, 2316, 9482, 11, 597, 5314, 493, 885, 257], "temperature": 0.0, "avg_logprob": -0.17223554091020063, "compression_ratio": 1.6588235294117648, "no_speech_prob": 3.589809057302773e-05}, {"id": 121, "seek": 48936, "start": 493.72, "end": 497.56, "text": " pretty important thing about simplifying these models.", "tokens": [1238, 1021, 551, 466, 6883, 5489, 613, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17223554091020063, "compression_ratio": 1.6588235294117648, "no_speech_prob": 3.589809057302773e-05}, {"id": 122, "seek": 48936, "start": 497.56, "end": 505.64, "text": " And so the other thing is that having a, with transformers, with LSTMs, let's say this", "tokens": [400, 370, 264, 661, 551, 307, 300, 1419, 257, 11, 365, 4088, 433, 11, 365, 441, 6840, 26386, 11, 718, 311, 584, 341], "temperature": 0.0, "avg_logprob": -0.17223554091020063, "compression_ratio": 1.6588235294117648, "no_speech_prob": 3.589809057302773e-05}, {"id": 123, "seek": 48936, "start": 505.64, "end": 507.44, "text": " is a batch and these are the words in the batch.", "tokens": [307, 257, 15245, 293, 613, 366, 264, 2283, 294, 264, 15245, 13], "temperature": 0.0, "avg_logprob": -0.17223554091020063, "compression_ratio": 1.6588235294117648, "no_speech_prob": 3.589809057302773e-05}, {"id": 124, "seek": 48936, "start": 507.44, "end": 510.04, "text": " You have two sentences and four words per sentence.", "tokens": [509, 362, 732, 16579, 293, 1451, 2283, 680, 8174, 13], "temperature": 0.0, "avg_logprob": -0.17223554091020063, "compression_ratio": 1.6588235294117648, "no_speech_prob": 3.589809057302773e-05}, {"id": 125, "seek": 48936, "start": 510.04, "end": 512.8000000000001, "text": " Every step has to be computed one at a time.", "tokens": [2048, 1823, 575, 281, 312, 40610, 472, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.17223554091020063, "compression_ratio": 1.6588235294117648, "no_speech_prob": 3.589809057302773e-05}, {"id": 126, "seek": 48936, "start": 512.8000000000001, "end": 515.72, "text": " So you only get a batch size of two effectively.", "tokens": [407, 291, 787, 483, 257, 15245, 2744, 295, 732, 8659, 13], "temperature": 0.0, "avg_logprob": -0.17223554091020063, "compression_ratio": 1.6588235294117648, "no_speech_prob": 3.589809057302773e-05}, {"id": 127, "seek": 51572, "start": 515.72, "end": 520.6800000000001, "text": " And so on modern hardware, which is TPUs and GPUs, the bigger the matrix multiplication,", "tokens": [400, 370, 322, 4363, 8837, 11, 597, 307, 314, 8115, 82, 293, 18407, 82, 11, 264, 3801, 264, 8141, 27290, 11], "temperature": 0.0, "avg_logprob": -0.15772011855552936, "compression_ratio": 1.7766990291262137, "no_speech_prob": 1.3840473002346698e-05}, {"id": 128, "seek": 51572, "start": 520.6800000000001, "end": 521.6800000000001, "text": " the better it is.", "tokens": [264, 1101, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.15772011855552936, "compression_ratio": 1.7766990291262137, "no_speech_prob": 1.3840473002346698e-05}, {"id": 129, "seek": 51572, "start": 521.6800000000001, "end": 523.76, "text": " You want all three dimensions to be big.", "tokens": [509, 528, 439, 1045, 12819, 281, 312, 955, 13], "temperature": 0.0, "avg_logprob": -0.15772011855552936, "compression_ratio": 1.7766990291262137, "no_speech_prob": 1.3840473002346698e-05}, {"id": 130, "seek": 51572, "start": 523.76, "end": 527.9200000000001, "text": " So even if you have big hidden layers, your batch size dimension will still be small,", "tokens": [407, 754, 498, 291, 362, 955, 7633, 7914, 11, 428, 15245, 2744, 10139, 486, 920, 312, 1359, 11], "temperature": 0.0, "avg_logprob": -0.15772011855552936, "compression_ratio": 1.7766990291262137, "no_speech_prob": 1.3840473002346698e-05}, {"id": 131, "seek": 51572, "start": 527.9200000000001, "end": 531.36, "text": " unless you have a huge batch, but then that's too expensive for long sequences.", "tokens": [5969, 291, 362, 257, 2603, 15245, 11, 457, 550, 300, 311, 886, 5124, 337, 938, 22978, 13], "temperature": 0.0, "avg_logprob": -0.15772011855552936, "compression_ratio": 1.7766990291262137, "no_speech_prob": 1.3840473002346698e-05}, {"id": 132, "seek": 51572, "start": 531.36, "end": 537.76, "text": " But with transformers, it's the total, because it's layer wise attention, the total number,", "tokens": [583, 365, 4088, 433, 11, 309, 311, 264, 3217, 11, 570, 309, 311, 4583, 10829, 3202, 11, 264, 3217, 1230, 11], "temperature": 0.0, "avg_logprob": -0.15772011855552936, "compression_ratio": 1.7766990291262137, "no_speech_prob": 1.3840473002346698e-05}, {"id": 133, "seek": 51572, "start": 537.76, "end": 538.96, "text": " the batch size is the total number of words.", "tokens": [264, 15245, 2744, 307, 264, 3217, 1230, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.15772011855552936, "compression_ratio": 1.7766990291262137, "no_speech_prob": 1.3840473002346698e-05}, {"id": 134, "seek": 51572, "start": 538.96, "end": 545.1600000000001, "text": " So if you have 500 words and then 32 sentences, it's actually 32, 10, 512 is the total batch size.", "tokens": [407, 498, 291, 362, 5923, 2283, 293, 550, 8858, 16579, 11, 309, 311, 767, 8858, 11, 1266, 11, 1025, 4762, 307, 264, 3217, 15245, 2744, 13], "temperature": 0.0, "avg_logprob": -0.15772011855552936, "compression_ratio": 1.7766990291262137, "no_speech_prob": 1.3840473002346698e-05}, {"id": 135, "seek": 54516, "start": 545.16, "end": 550.7199999999999, "text": " So you get these huge matrix multiplication, and you can take advantage of modern hardware.", "tokens": [407, 291, 483, 613, 2603, 8141, 27290, 11, 293, 291, 393, 747, 5002, 295, 4363, 8837, 13], "temperature": 0.0, "avg_logprob": -0.3014887571334839, "compression_ratio": 1.5565217391304347, "no_speech_prob": 4.398957753437571e-05}, {"id": 136, "seek": 54516, "start": 550.7199999999999, "end": 554.0, "text": " And so that's kind of why the transformer has taken over, because of these two things.", "tokens": [400, 370, 300, 311, 733, 295, 983, 264, 31782, 575, 2726, 670, 11, 570, 295, 613, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.3014887571334839, "compression_ratio": 1.5565217391304347, "no_speech_prob": 4.398957753437571e-05}, {"id": 137, "seek": 54516, "start": 554.0, "end": 556.36, "text": " And that's why it was used in GPD and Y at Susan Burke.", "tokens": [400, 300, 311, 983, 309, 390, 1143, 294, 460, 17349, 293, 398, 412, 15160, 37396, 13], "temperature": 0.0, "avg_logprob": -0.3014887571334839, "compression_ratio": 1.5565217391304347, "no_speech_prob": 4.398957753437571e-05}, {"id": 138, "seek": 54516, "start": 556.36, "end": 560.04, "text": " So now I'm going to talk about Burke.", "tokens": [407, 586, 286, 478, 516, 281, 751, 466, 37396, 13], "temperature": 0.0, "avg_logprob": -0.3014887571334839, "compression_ratio": 1.5565217391304347, "no_speech_prob": 4.398957753437571e-05}, {"id": 139, "seek": 54516, "start": 560.04, "end": 569.76, "text": " So the problem with the previous model being LMO and GPD and, once we for it, is that", "tokens": [407, 264, 1154, 365, 264, 3894, 2316, 885, 441, 18976, 293, 460, 17349, 293, 11, 1564, 321, 337, 309, 11, 307, 300], "temperature": 0.0, "avg_logprob": -0.3014887571334839, "compression_ratio": 1.5565217391304347, "no_speech_prob": 4.398957753437571e-05}, {"id": 140, "seek": 56976, "start": 569.76, "end": 575.2, "text": " the language model is only used left context or right context or a concatenation of both,", "tokens": [264, 2856, 2316, 307, 787, 1143, 1411, 4319, 420, 558, 4319, 420, 257, 1588, 7186, 399, 295, 1293, 11], "temperature": 0.0, "avg_logprob": -0.26217075459008077, "compression_ratio": 1.8458149779735682, "no_speech_prob": 4.130190791329369e-05}, {"id": 141, "seek": 56976, "start": 575.2, "end": 579.2, "text": " but really, but language understanding is bidirectional.", "tokens": [457, 534, 11, 457, 2856, 3701, 307, 12957, 621, 41048, 13], "temperature": 0.0, "avg_logprob": -0.26217075459008077, "compression_ratio": 1.8458149779735682, "no_speech_prob": 4.130190791329369e-05}, {"id": 142, "seek": 56976, "start": 579.2, "end": 585.48, "text": " So there's this clear kind of mismatch between why did everyone train on new direction of", "tokens": [407, 456, 311, 341, 1850, 733, 295, 23220, 852, 1296, 983, 630, 1518, 3847, 322, 777, 3513, 295], "temperature": 0.0, "avg_logprob": -0.26217075459008077, "compression_ratio": 1.8458149779735682, "no_speech_prob": 4.130190791329369e-05}, {"id": 143, "seek": 56976, "start": 585.48, "end": 590.4399999999999, "text": " models, where you could only see to the left or only see to the right, when we know that", "tokens": [5245, 11, 689, 291, 727, 787, 536, 281, 264, 1411, 420, 787, 536, 281, 264, 558, 11, 562, 321, 458, 300], "temperature": 0.0, "avg_logprob": -0.26217075459008077, "compression_ratio": 1.8458149779735682, "no_speech_prob": 4.130190791329369e-05}, {"id": 144, "seek": 56976, "start": 590.4399999999999, "end": 593.12, "text": " in order to understand language, you need to look in both directions.", "tokens": [294, 1668, 281, 1223, 2856, 11, 291, 643, 281, 574, 294, 1293, 11095, 13], "temperature": 0.0, "avg_logprob": -0.26217075459008077, "compression_ratio": 1.8458149779735682, "no_speech_prob": 4.130190791329369e-05}, {"id": 145, "seek": 56976, "start": 593.12, "end": 595.56, "text": " So there's two reasons.", "tokens": [407, 456, 311, 732, 4112, 13], "temperature": 0.0, "avg_logprob": -0.26217075459008077, "compression_ratio": 1.8458149779735682, "no_speech_prob": 4.130190791329369e-05}, {"id": 146, "seek": 59556, "start": 595.56, "end": 601.16, "text": " So one is that language models historically had been used for typically as features in", "tokens": [407, 472, 307, 300, 2856, 5245, 16180, 632, 668, 1143, 337, 5850, 382, 4122, 294], "temperature": 0.0, "avg_logprob": -0.1920970183152419, "compression_ratio": 1.864353312302839, "no_speech_prob": 4.6817498514428735e-05}, {"id": 147, "seek": 59556, "start": 601.16, "end": 602.16, "text": " other systems.", "tokens": [661, 3652, 13], "temperature": 0.0, "avg_logprob": -0.1920970183152419, "compression_ratio": 1.864353312302839, "no_speech_prob": 4.6817498514428735e-05}, {"id": 148, "seek": 59556, "start": 602.16, "end": 605.92, "text": " So the most direct application of language model would be predictive text, which is directly", "tokens": [407, 264, 881, 2047, 3861, 295, 2856, 2316, 576, 312, 35521, 2487, 11, 597, 307, 3838], "temperature": 0.0, "avg_logprob": -0.1920970183152419, "compression_ratio": 1.864353312302839, "no_speech_prob": 4.6817498514428735e-05}, {"id": 149, "seek": 59556, "start": 605.92, "end": 607.1199999999999, "text": " just saying predict the next word.", "tokens": [445, 1566, 6069, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1920970183152419, "compression_ratio": 1.864353312302839, "no_speech_prob": 4.6817498514428735e-05}, {"id": 150, "seek": 59556, "start": 607.1199999999999, "end": 610.76, "text": " But the other applications that are actually more common are to use them in a machine", "tokens": [583, 264, 661, 5821, 300, 366, 767, 544, 2689, 366, 281, 764, 552, 294, 257, 3479], "temperature": 0.0, "avg_logprob": -0.1920970183152419, "compression_ratio": 1.864353312302839, "no_speech_prob": 4.6817498514428735e-05}, {"id": 151, "seek": 59556, "start": 610.76, "end": 615.0, "text": " translation system or a speech recognition system, where you have these features like translation", "tokens": [12853, 1185, 420, 257, 6218, 11150, 1185, 11, 689, 291, 362, 613, 4122, 411, 12853], "temperature": 0.0, "avg_logprob": -0.1920970183152419, "compression_ratio": 1.864353312302839, "no_speech_prob": 4.6817498514428735e-05}, {"id": 152, "seek": 59556, "start": 615.0, "end": 619.4, "text": " features or acoustic features, and then you add a language model that says what's the", "tokens": [4122, 420, 26753, 4122, 11, 293, 550, 291, 909, 257, 2856, 2316, 300, 1619, 437, 311, 264], "temperature": 0.0, "avg_logprob": -0.1920970183152419, "compression_ratio": 1.864353312302839, "no_speech_prob": 4.6817498514428735e-05}, {"id": 153, "seek": 59556, "start": 619.4, "end": 620.76, "text": " probability of the sentence.", "tokens": [8482, 295, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.1920970183152419, "compression_ratio": 1.864353312302839, "no_speech_prob": 4.6817498514428735e-05}, {"id": 154, "seek": 59556, "start": 620.76, "end": 623.16, "text": " And so for this, you want it to be a well-formed distribution.", "tokens": [400, 370, 337, 341, 11, 291, 528, 309, 281, 312, 257, 731, 12, 22892, 7316, 13], "temperature": 0.0, "avg_logprob": -0.1920970183152419, "compression_ratio": 1.864353312302839, "no_speech_prob": 4.6817498514428735e-05}, {"id": 155, "seek": 62316, "start": 623.16, "end": 626.16, "text": " So these pre-trained models we actually don't care about this, but this was kind of something", "tokens": [407, 613, 659, 12, 17227, 2001, 5245, 321, 767, 500, 380, 1127, 466, 341, 11, 457, 341, 390, 733, 295, 746], "temperature": 0.0, "avg_logprob": -0.1691016161883319, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.131638343096711e-05}, {"id": 156, "seek": 62316, "start": 626.16, "end": 632.0, "text": " that was kind of people had just been like kind of, I guess, fixed on this idea that language", "tokens": [300, 390, 733, 295, 561, 632, 445, 668, 411, 733, 295, 11, 286, 2041, 11, 6806, 322, 341, 1558, 300, 2856], "temperature": 0.0, "avg_logprob": -0.1691016161883319, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.131638343096711e-05}, {"id": 157, "seek": 62316, "start": 632.0, "end": 635.12, "text": " models have to have a distribution, probably a distribution, even though we actually don't", "tokens": [5245, 362, 281, 362, 257, 7316, 11, 1391, 257, 7316, 11, 754, 1673, 321, 767, 500, 380], "temperature": 0.0, "avg_logprob": -0.1691016161883319, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.131638343096711e-05}, {"id": 158, "seek": 62316, "start": 635.12, "end": 636.4399999999999, "text": " care about that.", "tokens": [1127, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.1691016161883319, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.131638343096711e-05}, {"id": 159, "seek": 62316, "start": 636.4399999999999, "end": 640.0799999999999, "text": " But the other kind of bigger reason is that words can see themselves in a bidirectional", "tokens": [583, 264, 661, 733, 295, 3801, 1778, 307, 300, 2283, 393, 536, 2969, 294, 257, 12957, 621, 41048], "temperature": 0.0, "avg_logprob": -0.1691016161883319, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.131638343096711e-05}, {"id": 160, "seek": 62316, "start": 640.0799999999999, "end": 642.04, "text": " encoder.", "tokens": [2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1691016161883319, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.131638343096711e-05}, {"id": 161, "seek": 62316, "start": 642.04, "end": 650.36, "text": " And so what this means is when you build a representation incrementally, so you have your input and", "tokens": [400, 370, 437, 341, 1355, 307, 562, 291, 1322, 257, 10290, 26200, 379, 11, 370, 291, 362, 428, 4846, 293], "temperature": 0.0, "avg_logprob": -0.1691016161883319, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.131638343096711e-05}, {"id": 162, "seek": 62316, "start": 650.36, "end": 652.92, "text": " then you have your output and it's always offset by one.", "tokens": [550, 291, 362, 428, 5598, 293, 309, 311, 1009, 18687, 538, 472, 13], "temperature": 0.0, "avg_logprob": -0.1691016161883319, "compression_ratio": 1.8484848484848484, "no_speech_prob": 4.131638343096711e-05}, {"id": 163, "seek": 65292, "start": 652.92, "end": 658.4799999999999, "text": " So we have the start-up sentence token, we predict the first word, then we feed in the", "tokens": [407, 321, 362, 264, 722, 12, 1010, 8174, 14862, 11, 321, 6069, 264, 700, 1349, 11, 550, 321, 3154, 294, 264], "temperature": 0.0, "avg_logprob": -0.17088688203018076, "compression_ratio": 1.8790035587188612, "no_speech_prob": 1.7487865989096463e-05}, {"id": 164, "seek": 65292, "start": 658.4799999999999, "end": 661.9599999999999, "text": " second word, we feed in the first word and predict the second word, and so we can encode", "tokens": [1150, 1349, 11, 321, 3154, 294, 264, 700, 1349, 293, 6069, 264, 1150, 1349, 11, 293, 370, 321, 393, 2058, 1429], "temperature": 0.0, "avg_logprob": -0.17088688203018076, "compression_ratio": 1.8790035587188612, "no_speech_prob": 1.7487865989096463e-05}, {"id": 165, "seek": 65292, "start": 661.9599999999999, "end": 665.68, "text": " the sentence once and predict all the words in the sentence with the unidirectional model.", "tokens": [264, 8174, 1564, 293, 6069, 439, 264, 2283, 294, 264, 8174, 365, 264, 517, 327, 621, 41048, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17088688203018076, "compression_ratio": 1.8790035587188612, "no_speech_prob": 1.7487865989096463e-05}, {"id": 166, "seek": 65292, "start": 665.68, "end": 667.52, "text": " And so this gives us good sample efficiency, right?", "tokens": [400, 370, 341, 2709, 505, 665, 6889, 10493, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17088688203018076, "compression_ratio": 1.8790035587188612, "no_speech_prob": 1.7487865989096463e-05}, {"id": 167, "seek": 65292, "start": 667.52, "end": 671.8399999999999, "text": " Because if we have a 500 and 12-dimension, like a sequence of 500 words, we don't want", "tokens": [1436, 498, 321, 362, 257, 5923, 293, 2272, 12, 13595, 3378, 11, 411, 257, 8310, 295, 5923, 2283, 11, 321, 500, 380, 528], "temperature": 0.0, "avg_logprob": -0.17088688203018076, "compression_ratio": 1.8790035587188612, "no_speech_prob": 1.7487865989096463e-05}, {"id": 168, "seek": 65292, "start": 671.8399999999999, "end": 677.88, "text": " to have to only predict one word because it's going to be 500 times as much compute to", "tokens": [281, 362, 281, 787, 6069, 472, 1349, 570, 309, 311, 516, 281, 312, 5923, 1413, 382, 709, 14722, 281], "temperature": 0.0, "avg_logprob": -0.17088688203018076, "compression_ratio": 1.8790035587188612, "no_speech_prob": 1.7487865989096463e-05}, {"id": 169, "seek": 65292, "start": 677.88, "end": 680.56, "text": " get the same amount of predictions.", "tokens": [483, 264, 912, 2372, 295, 21264, 13], "temperature": 0.0, "avg_logprob": -0.17088688203018076, "compression_ratio": 1.8790035587188612, "no_speech_prob": 1.7487865989096463e-05}, {"id": 170, "seek": 68056, "start": 680.56, "end": 684.88, "text": " If we would just trivially do a bidirectional LSTM or transformer, we would have a situation", "tokens": [759, 321, 576, 445, 1376, 85, 2270, 360, 257, 12957, 621, 41048, 441, 6840, 44, 420, 31782, 11, 321, 576, 362, 257, 2590], "temperature": 0.0, "avg_logprob": -0.1792670996614205, "compression_ratio": 1.7127659574468086, "no_speech_prob": 1.0448487046232913e-05}, {"id": 171, "seek": 68056, "start": 684.88, "end": 688.8, "text": " where you encode your sentence, everything is bidirectional.", "tokens": [689, 291, 2058, 1429, 428, 8174, 11, 1203, 307, 12957, 621, 41048, 13], "temperature": 0.0, "avg_logprob": -0.1792670996614205, "compression_ratio": 1.7127659574468086, "no_speech_prob": 1.0448487046232913e-05}, {"id": 172, "seek": 68056, "start": 688.8, "end": 692.28, "text": " And so after the first layer, everything can see itself.", "tokens": [400, 370, 934, 264, 700, 4583, 11, 1203, 393, 536, 2564, 13], "temperature": 0.0, "avg_logprob": -0.1792670996614205, "compression_ratio": 1.7127659574468086, "no_speech_prob": 1.0448487046232913e-05}, {"id": 173, "seek": 68056, "start": 692.28, "end": 695.1199999999999, "text": " So this word open, there's a path back down to open.", "tokens": [407, 341, 1349, 1269, 11, 456, 311, 257, 3100, 646, 760, 281, 1269, 13], "temperature": 0.0, "avg_logprob": -0.1792670996614205, "compression_ratio": 1.7127659574468086, "no_speech_prob": 1.0448487046232913e-05}, {"id": 174, "seek": 68056, "start": 695.1199999999999, "end": 699.28, "text": " And so it's trivial to predict a word that can, where it's in the input also, right?", "tokens": [400, 370, 309, 311, 26703, 281, 6069, 257, 1349, 300, 393, 11, 689, 309, 311, 294, 264, 4846, 611, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1792670996614205, "compression_ratio": 1.7127659574468086, "no_speech_prob": 1.0448487046232913e-05}, {"id": 175, "seek": 68056, "start": 699.28, "end": 702.0, "text": " There's no actual prediction going on there.", "tokens": [821, 311, 572, 3539, 17630, 516, 322, 456, 13], "temperature": 0.0, "avg_logprob": -0.1792670996614205, "compression_ratio": 1.7127659574468086, "no_speech_prob": 1.0448487046232913e-05}, {"id": 176, "seek": 68056, "start": 702.0, "end": 707.7199999999999, "text": " So the simple solution, which is basically the whole crux of birth, is that let's instead", "tokens": [407, 264, 2199, 3827, 11, 597, 307, 1936, 264, 1379, 5140, 87, 295, 3965, 11, 307, 300, 718, 311, 2602], "temperature": 0.0, "avg_logprob": -0.1792670996614205, "compression_ratio": 1.7127659574468086, "no_speech_prob": 1.0448487046232913e-05}, {"id": 177, "seek": 70772, "start": 707.72, "end": 715.28, "text": " of training a normal language model, let's just predict, mask out, k percent of the words.", "tokens": [295, 3097, 257, 2710, 2856, 2316, 11, 718, 311, 445, 6069, 11, 6094, 484, 11, 350, 3043, 295, 264, 2283, 13], "temperature": 0.0, "avg_logprob": -0.18108085824661896, "compression_ratio": 1.757936507936508, "no_speech_prob": 7.363838085439056e-05}, {"id": 178, "seek": 70772, "start": 715.28, "end": 719.64, "text": " So the man went to the mask to buy a mask of milk.", "tokens": [407, 264, 587, 1437, 281, 264, 6094, 281, 2256, 257, 6094, 295, 5392, 13], "temperature": 0.0, "avg_logprob": -0.18108085824661896, "compression_ratio": 1.757936507936508, "no_speech_prob": 7.363838085439056e-05}, {"id": 179, "seek": 70772, "start": 719.64, "end": 723.32, "text": " And so now you can run a bidirectional model on that.", "tokens": [400, 370, 586, 291, 393, 1190, 257, 12957, 621, 41048, 2316, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.18108085824661896, "compression_ratio": 1.757936507936508, "no_speech_prob": 7.363838085439056e-05}, {"id": 180, "seek": 70772, "start": 723.32, "end": 728.48, "text": " And because the words aren't in the input, you can't cheat, right?", "tokens": [400, 570, 264, 2283, 3212, 380, 294, 264, 4846, 11, 291, 393, 380, 17470, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18108085824661896, "compression_ratio": 1.757936507936508, "no_speech_prob": 7.363838085439056e-05}, {"id": 181, "seek": 70772, "start": 728.48, "end": 733.2, "text": " And so the downside of this is that you're not getting as many predictions per sentence,", "tokens": [400, 370, 264, 25060, 295, 341, 307, 300, 291, 434, 406, 1242, 382, 867, 21264, 680, 8174, 11], "temperature": 0.0, "avg_logprob": -0.18108085824661896, "compression_ratio": 1.757936507936508, "no_speech_prob": 7.363838085439056e-05}, {"id": 182, "seek": 70772, "start": 733.2, "end": 734.2, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.18108085824661896, "compression_ratio": 1.757936507936508, "no_speech_prob": 7.363838085439056e-05}, {"id": 183, "seek": 70772, "start": 734.2, "end": 737.0400000000001, "text": " You're only getting, predicting 15 percent of words instead of 100 percent of words.", "tokens": [509, 434, 787, 1242, 11, 32884, 2119, 3043, 295, 2283, 2602, 295, 2319, 3043, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.18108085824661896, "compression_ratio": 1.757936507936508, "no_speech_prob": 7.363838085439056e-05}, {"id": 184, "seek": 73704, "start": 737.04, "end": 739.8399999999999, "text": " So the upside is that you're getting much more rich model because you're seeing both", "tokens": [407, 264, 14119, 307, 300, 291, 434, 1242, 709, 544, 4593, 2316, 570, 291, 434, 2577, 1293], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 185, "seek": 73704, "start": 739.8399999999999, "end": 741.16, "text": " directions, right?", "tokens": [11095, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 186, "seek": 73704, "start": 741.16, "end": 746.3199999999999, "text": " So this value of k is a hyper parameter that we have to just decide on empirically.", "tokens": [407, 341, 2158, 295, 350, 307, 257, 9848, 13075, 300, 321, 362, 281, 445, 4536, 322, 25790, 984, 13], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 187, "seek": 73704, "start": 746.3199999999999, "end": 747.7199999999999, "text": " So we use 15 percent.", "tokens": [407, 321, 764, 2119, 3043, 13], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 188, "seek": 73704, "start": 747.7199999999999, "end": 750.04, "text": " It turns out that that's actually kind of an optimal value.", "tokens": [467, 4523, 484, 300, 300, 311, 767, 733, 295, 364, 16252, 2158, 13], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 189, "seek": 73704, "start": 750.04, "end": 754.5999999999999, "text": " So we and also people have since then have done more thorough ablation experiments and", "tokens": [407, 321, 293, 611, 561, 362, 1670, 550, 362, 1096, 544, 12934, 410, 24278, 12050, 293], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 190, "seek": 73704, "start": 754.5999999999999, "end": 756.56, "text": " found that this 15 percent is good.", "tokens": [1352, 300, 341, 2119, 3043, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 191, "seek": 73704, "start": 756.56, "end": 760.1999999999999, "text": " So the reason for doing a certain percent over another is that if you were to do, let's", "tokens": [407, 264, 1778, 337, 884, 257, 1629, 3043, 670, 1071, 307, 300, 498, 291, 645, 281, 360, 11, 718, 311], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 192, "seek": 73704, "start": 760.1999999999999, "end": 763.9599999999999, "text": " say, 50 percent masking, you would get way more predictions, but you would also mask", "tokens": [584, 11, 2625, 3043, 31226, 11, 291, 576, 483, 636, 544, 21264, 11, 457, 291, 576, 611, 6094], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 193, "seek": 73704, "start": 763.9599999999999, "end": 766.12, "text": " out like all of your context.", "tokens": [484, 411, 439, 295, 428, 4319, 13], "temperature": 0.0, "avg_logprob": -0.1664605458577474, "compression_ratio": 1.7708333333333333, "no_speech_prob": 4.468183033168316e-05}, {"id": 194, "seek": 76612, "start": 766.12, "end": 772.04, "text": " And so you can, if you mask out all of your context and you're not getting any, you can't", "tokens": [400, 370, 291, 393, 11, 498, 291, 6094, 484, 439, 295, 428, 4319, 293, 291, 434, 406, 1242, 604, 11, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.20921727589198522, "compression_ratio": 1.7972508591065293, "no_speech_prob": 1.8628967154654674e-05}, {"id": 195, "seek": 76612, "start": 772.04, "end": 773.04, "text": " learn contextual models.", "tokens": [1466, 35526, 5245, 13], "temperature": 0.0, "avg_logprob": -0.20921727589198522, "compression_ratio": 1.7972508591065293, "no_speech_prob": 1.8628967154654674e-05}, {"id": 196, "seek": 76612, "start": 773.04, "end": 778.96, "text": " And if you only do, like, let's say you can mask out one word, that might be optimal maybe,", "tokens": [400, 498, 291, 787, 360, 11, 411, 11, 718, 311, 584, 291, 393, 6094, 484, 472, 1349, 11, 300, 1062, 312, 16252, 1310, 11], "temperature": 0.0, "avg_logprob": -0.20921727589198522, "compression_ratio": 1.7972508591065293, "no_speech_prob": 1.8628967154654674e-05}, {"id": 197, "seek": 76612, "start": 778.96, "end": 780.88, "text": " but you have to do way more data processing.", "tokens": [457, 291, 362, 281, 360, 636, 544, 1412, 9007, 13], "temperature": 0.0, "avg_logprob": -0.20921727589198522, "compression_ratio": 1.7972508591065293, "no_speech_prob": 1.8628967154654674e-05}, {"id": 198, "seek": 76612, "start": 780.88, "end": 782.16, "text": " So it would be way more expensive to train.", "tokens": [407, 309, 576, 312, 636, 544, 5124, 281, 3847, 13], "temperature": 0.0, "avg_logprob": -0.20921727589198522, "compression_ratio": 1.7972508591065293, "no_speech_prob": 1.8628967154654674e-05}, {"id": 199, "seek": 76612, "start": 782.16, "end": 785.36, "text": " And we know that these models are basically just compute bounded.", "tokens": [400, 321, 458, 300, 613, 5245, 366, 1936, 445, 14722, 37498, 13], "temperature": 0.0, "avg_logprob": -0.20921727589198522, "compression_ratio": 1.7972508591065293, "no_speech_prob": 1.8628967154654674e-05}, {"id": 200, "seek": 76612, "start": 785.36, "end": 788.72, "text": " So if you just have enough data, you can just kind of train them infinitely and it'll", "tokens": [407, 498, 291, 445, 362, 1547, 1412, 11, 291, 393, 445, 733, 295, 3847, 552, 36227, 293, 309, 603], "temperature": 0.0, "avg_logprob": -0.20921727589198522, "compression_ratio": 1.7972508591065293, "no_speech_prob": 1.8628967154654674e-05}, {"id": 201, "seek": 76612, "start": 788.72, "end": 790.04, "text": " always do better.", "tokens": [1009, 360, 1101, 13], "temperature": 0.0, "avg_logprob": -0.20921727589198522, "compression_ratio": 1.7972508591065293, "no_speech_prob": 1.8628967154654674e-05}, {"id": 202, "seek": 76612, "start": 790.04, "end": 794.28, "text": " So it's really just a trade-off between these two things.", "tokens": [407, 309, 311, 534, 445, 257, 4923, 12, 4506, 1296, 613, 732, 721, 13], "temperature": 0.0, "avg_logprob": -0.20921727589198522, "compression_ratio": 1.7972508591065293, "no_speech_prob": 1.8628967154654674e-05}, {"id": 203, "seek": 79428, "start": 794.28, "end": 799.52, "text": " So one other little detail in part, which should not have to be super important, is that", "tokens": [407, 472, 661, 707, 2607, 294, 644, 11, 597, 820, 406, 362, 281, 312, 1687, 1021, 11, 307, 300], "temperature": 0.0, "avg_logprob": -0.2029653969457594, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.8626204109750688e-05}, {"id": 204, "seek": 79428, "start": 799.52, "end": 804.8399999999999, "text": " because the mass token is never seen at fine-tuning time, instead of always replacing a word", "tokens": [570, 264, 2758, 14862, 307, 1128, 1612, 412, 2489, 12, 83, 37726, 565, 11, 2602, 295, 1009, 19139, 257, 1349], "temperature": 0.0, "avg_logprob": -0.2029653969457594, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.8626204109750688e-05}, {"id": 205, "seek": 79428, "start": 804.8399999999999, "end": 810.88, "text": " with the mass token as in this case, we would randomly sometimes predict it with a random", "tokens": [365, 264, 2758, 14862, 382, 294, 341, 1389, 11, 321, 576, 16979, 2171, 6069, 309, 365, 257, 4974], "temperature": 0.0, "avg_logprob": -0.2029653969457594, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.8626204109750688e-05}, {"id": 206, "seek": 79428, "start": 810.88, "end": 812.52, "text": " word and sometimes keep the same word.", "tokens": [1349, 293, 2171, 1066, 264, 912, 1349, 13], "temperature": 0.0, "avg_logprob": -0.2029653969457594, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.8626204109750688e-05}, {"id": 207, "seek": 79428, "start": 812.52, "end": 816.04, "text": " So like, so a 100 percent time would say, we went to the store and went to the running,", "tokens": [407, 411, 11, 370, 257, 2319, 3043, 565, 576, 584, 11, 321, 1437, 281, 264, 3531, 293, 1437, 281, 264, 2614, 11], "temperature": 0.0, "avg_logprob": -0.2029653969457594, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.8626204109750688e-05}, {"id": 208, "seek": 79428, "start": 816.04, "end": 817.04, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.2029653969457594, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.8626204109750688e-05}, {"id": 209, "seek": 79428, "start": 817.04, "end": 822.8, "text": " And so we wouldn't tell the model which case was which.", "tokens": [400, 370, 321, 2759, 380, 980, 264, 2316, 597, 1389, 390, 597, 13], "temperature": 0.0, "avg_logprob": -0.2029653969457594, "compression_ratio": 1.746212121212121, "no_speech_prob": 1.8626204109750688e-05}, {"id": 210, "seek": 82280, "start": 822.8, "end": 828.52, "text": " We would just have to, we would just say, what should this word be, right?", "tokens": [492, 576, 445, 362, 281, 11, 321, 576, 445, 584, 11, 437, 820, 341, 1349, 312, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 211, "seek": 82280, "start": 828.52, "end": 830.0, "text": " And didn't know whether it's right or not.", "tokens": [400, 994, 380, 458, 1968, 309, 311, 558, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 212, "seek": 82280, "start": 830.0, "end": 831.0, "text": " So it could be the same word.", "tokens": [407, 309, 727, 312, 264, 912, 1349, 13], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 213, "seek": 82280, "start": 831.0, "end": 832.8, "text": " So it's 10 percent time, it's the same word.", "tokens": [407, 309, 311, 1266, 3043, 565, 11, 309, 311, 264, 912, 1349, 13], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 214, "seek": 82280, "start": 832.8, "end": 833.8399999999999, "text": " It could be a random word.", "tokens": [467, 727, 312, 257, 4974, 1349, 13], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 215, "seek": 82280, "start": 833.8399999999999, "end": 838.8399999999999, "text": " And so it has to basically be able to maintain a good representation of every word because", "tokens": [400, 370, 309, 575, 281, 1936, 312, 1075, 281, 6909, 257, 665, 10290, 295, 633, 1349, 570], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 216, "seek": 82280, "start": 838.8399999999999, "end": 840.68, "text": " it doesn't know whether it's really the right word.", "tokens": [309, 1177, 380, 458, 1968, 309, 311, 534, 264, 558, 1349, 13], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 217, "seek": 82280, "start": 840.68, "end": 843.24, "text": " So it has to actually look at every word and figure out whether this is the right word.", "tokens": [407, 309, 575, 281, 767, 574, 412, 633, 1349, 293, 2573, 484, 1968, 341, 307, 264, 558, 1349, 13], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 218, "seek": 82280, "start": 843.24, "end": 846.0, "text": " So we could potentially even just get away with not using mass token at all, and just doing", "tokens": [407, 321, 727, 7263, 754, 445, 483, 1314, 365, 406, 1228, 2758, 14862, 412, 439, 11, 293, 445, 884], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 219, "seek": 82280, "start": 846.0, "end": 848.88, "text": " like this 50 percent of time and this 50 percent of the time.", "tokens": [411, 341, 2625, 3043, 295, 565, 293, 341, 2625, 3043, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.22155325753348215, "compression_ratio": 2.013333333333333, "no_speech_prob": 4.6104447392281145e-05}, {"id": 220, "seek": 84888, "start": 848.88, "end": 852.8, "text": " But the reason for not doing that is that, you know, then we'd be corrupting a lot of our", "tokens": [583, 264, 1778, 337, 406, 884, 300, 307, 300, 11, 291, 458, 11, 550, 321, 1116, 312, 17366, 278, 257, 688, 295, 527], "temperature": 0.0, "avg_logprob": -0.196906570560676, "compression_ratio": 1.6884057971014492, "no_speech_prob": 2.1440908312797546e-05}, {"id": 221, "seek": 84888, "start": 852.8, "end": 856.16, "text": " data and we don't want it to necessarily corrupt a data because the fact that this is the", "tokens": [1412, 293, 321, 500, 380, 528, 309, 281, 4725, 17366, 257, 1412, 570, 264, 1186, 300, 341, 307, 264], "temperature": 0.0, "avg_logprob": -0.196906570560676, "compression_ratio": 1.6884057971014492, "no_speech_prob": 2.1440908312797546e-05}, {"id": 222, "seek": 84888, "start": 856.16, "end": 859.52, "text": " wrong word might mess up our prediction for some other word over here, right?", "tokens": [2085, 1349, 1062, 2082, 493, 527, 17630, 337, 512, 661, 1349, 670, 510, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.196906570560676, "compression_ratio": 1.6884057971014492, "no_speech_prob": 2.1440908312797546e-05}, {"id": 223, "seek": 84888, "start": 859.52, "end": 863.04, "text": " So whereas a mass token at least it knows that it's not the right word, so it doesn't", "tokens": [407, 9735, 257, 2758, 14862, 412, 1935, 309, 3255, 300, 309, 311, 406, 264, 558, 1349, 11, 370, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.196906570560676, "compression_ratio": 1.6884057971014492, "no_speech_prob": 2.1440908312797546e-05}, {"id": 224, "seek": 84888, "start": 863.04, "end": 866.96, "text": " use that as part of its context.", "tokens": [764, 300, 382, 644, 295, 1080, 4319, 13], "temperature": 0.0, "avg_logprob": -0.196906570560676, "compression_ratio": 1.6884057971014492, "no_speech_prob": 2.1440908312797546e-05}, {"id": 225, "seek": 84888, "start": 866.96, "end": 873.88, "text": " So the other kind of detail of BERT which also now and subsequently may not be, have been", "tokens": [407, 264, 661, 733, 295, 2607, 295, 363, 31479, 597, 611, 586, 293, 26514, 815, 406, 312, 11, 362, 668], "temperature": 0.0, "avg_logprob": -0.196906570560676, "compression_ratio": 1.6884057971014492, "no_speech_prob": 2.1440908312797546e-05}, {"id": 226, "seek": 87388, "start": 873.88, "end": 880.6, "text": " that important, is that a lot of these tasks that we're not just learning words, we're", "tokens": [300, 1021, 11, 307, 300, 257, 688, 295, 613, 9608, 300, 321, 434, 406, 445, 2539, 2283, 11, 321, 434], "temperature": 0.0, "avg_logprob": -0.2143794127873012, "compression_ratio": 1.8760330578512396, "no_speech_prob": 1.1840526894957293e-05}, {"id": 227, "seek": 87388, "start": 880.6, "end": 882.76, "text": " want to predict the relationship between sentences.", "tokens": [528, 281, 6069, 264, 2480, 1296, 16579, 13], "temperature": 0.0, "avg_logprob": -0.2143794127873012, "compression_ratio": 1.8760330578512396, "no_speech_prob": 1.1840526894957293e-05}, {"id": 228, "seek": 87388, "start": 882.76, "end": 887.36, "text": " So if question answering in particular, we have a query which is generally a sentence", "tokens": [407, 498, 1168, 13430, 294, 1729, 11, 321, 362, 257, 14581, 597, 307, 5101, 257, 8174], "temperature": 0.0, "avg_logprob": -0.2143794127873012, "compression_ratio": 1.8760330578512396, "no_speech_prob": 1.1840526894957293e-05}, {"id": 229, "seek": 87388, "start": 887.36, "end": 894.8, "text": " and then we have an answer which is a paragraph or a sentence or a document and we want to,", "tokens": [293, 550, 321, 362, 364, 1867, 597, 307, 257, 18865, 420, 257, 8174, 420, 257, 4166, 293, 321, 528, 281, 11], "temperature": 0.0, "avg_logprob": -0.2143794127873012, "compression_ratio": 1.8760330578512396, "no_speech_prob": 1.1840526894957293e-05}, {"id": 230, "seek": 87388, "start": 894.8, "end": 896.8, "text": " you know, say does this answer the question.", "tokens": [291, 458, 11, 584, 775, 341, 1867, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2143794127873012, "compression_ratio": 1.8760330578512396, "no_speech_prob": 1.1840526894957293e-05}, {"id": 231, "seek": 87388, "start": 896.8, "end": 903.72, "text": " So by doing that, we, so we want to have some pre-taining task that actually does a sentence", "tokens": [407, 538, 884, 300, 11, 321, 11, 370, 321, 528, 281, 362, 512, 659, 12, 83, 3686, 5633, 300, 767, 775, 257, 8174], "temperature": 0.0, "avg_logprob": -0.2143794127873012, "compression_ratio": 1.8760330578512396, "no_speech_prob": 1.1840526894957293e-05}, {"id": 232, "seek": 90372, "start": 903.72, "end": 906.36, "text": " of prediction rather than just a word level prediction.", "tokens": [295, 17630, 2831, 813, 445, 257, 1349, 1496, 17630, 13], "temperature": 0.0, "avg_logprob": -0.2077503072804418, "compression_ratio": 2.0522388059701493, "no_speech_prob": 5.735722879762761e-05}, {"id": 233, "seek": 90372, "start": 906.36, "end": 909.4, "text": " So the way that we did this, which, and we need this to have like an infinite amount", "tokens": [407, 264, 636, 300, 321, 630, 341, 11, 597, 11, 293, 321, 643, 341, 281, 362, 411, 364, 13785, 2372], "temperature": 0.0, "avg_logprob": -0.2077503072804418, "compression_ratio": 2.0522388059701493, "no_speech_prob": 5.735722879762761e-05}, {"id": 234, "seek": 90372, "start": 909.4, "end": 910.4, "text": " of data, right?", "tokens": [295, 1412, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2077503072804418, "compression_ratio": 2.0522388059701493, "no_speech_prob": 5.735722879762761e-05}, {"id": 235, "seek": 90372, "start": 910.4, "end": 913.0, "text": " We're going to generate an infinite amount of data so we don't want this to be an annotated", "tokens": [492, 434, 516, 281, 8460, 364, 13785, 2372, 295, 1412, 370, 321, 500, 380, 528, 341, 281, 312, 364, 25339, 770], "temperature": 0.0, "avg_logprob": -0.2077503072804418, "compression_ratio": 2.0522388059701493, "no_speech_prob": 5.735722879762761e-05}, {"id": 236, "seek": 90372, "start": 913.0, "end": 914.9200000000001, "text": " task.", "tokens": [5633, 13], "temperature": 0.0, "avg_logprob": -0.2077503072804418, "compression_ratio": 2.0522388059701493, "no_speech_prob": 5.735722879762761e-05}, {"id": 237, "seek": 90372, "start": 914.9200000000001, "end": 920.9200000000001, "text": " So the way that we did this is we just did a next sentence-producing task where we just", "tokens": [407, 264, 636, 300, 321, 630, 341, 307, 321, 445, 630, 257, 958, 8174, 12, 14314, 2175, 5633, 689, 321, 445], "temperature": 0.0, "avg_logprob": -0.2077503072804418, "compression_ratio": 2.0522388059701493, "no_speech_prob": 5.735722879762761e-05}, {"id": 238, "seek": 90372, "start": 920.9200000000001, "end": 926.4, "text": " took two sentences from the same corpus, from the same document and 50% time there from", "tokens": [1890, 732, 16579, 490, 264, 912, 1181, 31624, 11, 490, 264, 912, 4166, 293, 2625, 4, 565, 456, 490], "temperature": 0.0, "avg_logprob": -0.2077503072804418, "compression_ratio": 2.0522388059701493, "no_speech_prob": 5.735722879762761e-05}, {"id": 239, "seek": 90372, "start": 926.4, "end": 931.28, "text": " the same document, 50% time there from a random document and then we just said, was this", "tokens": [264, 912, 4166, 11, 2625, 4, 565, 456, 490, 257, 4974, 4166, 293, 550, 321, 445, 848, 11, 390, 341], "temperature": 0.0, "avg_logprob": -0.2077503072804418, "compression_ratio": 2.0522388059701493, "no_speech_prob": 5.735722879762761e-05}, {"id": 240, "seek": 90372, "start": 931.28, "end": 933.4, "text": " the real next sentence or not?", "tokens": [264, 957, 958, 8174, 420, 406, 30], "temperature": 0.0, "avg_logprob": -0.2077503072804418, "compression_ratio": 2.0522388059701493, "no_speech_prob": 5.735722879762761e-05}, {"id": 241, "seek": 93340, "start": 933.4, "end": 937.0, "text": " And so if you have like the man went to the store, he bought a gun, a milk, that is the next", "tokens": [400, 370, 498, 291, 362, 411, 264, 587, 1437, 281, 264, 3531, 11, 415, 4243, 257, 3874, 11, 257, 5392, 11, 300, 307, 264, 958], "temperature": 0.0, "avg_logprob": -0.23300437344849564, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.0608604497974738e-05}, {"id": 242, "seek": 93340, "start": 937.0, "end": 938.0, "text": " sentence.", "tokens": [8174, 13], "temperature": 0.0, "avg_logprob": -0.23300437344849564, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.0608604497974738e-05}, {"id": 243, "seek": 93340, "start": 938.0, "end": 940.48, "text": " If you said the man went to the store, penguins are flightless, that's not the next", "tokens": [759, 291, 848, 264, 587, 1437, 281, 264, 3531, 11, 17289, 44790, 366, 7018, 1832, 11, 300, 311, 406, 264, 958], "temperature": 0.0, "avg_logprob": -0.23300437344849564, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.0608604497974738e-05}, {"id": 244, "seek": 93340, "start": 940.48, "end": 941.48, "text": " sentence.", "tokens": [8174, 13], "temperature": 0.0, "avg_logprob": -0.23300437344849564, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.0608604497974738e-05}, {"id": 245, "seek": 93340, "start": 941.48, "end": 945.0, "text": " So basically now we're forcing the model app pre-taining time to actually make, to look", "tokens": [407, 1936, 586, 321, 434, 19030, 264, 2316, 724, 659, 12, 83, 3686, 565, 281, 767, 652, 11, 281, 574], "temperature": 0.0, "avg_logprob": -0.23300437344849564, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.0608604497974738e-05}, {"id": 246, "seek": 93340, "start": 945.0, "end": 948.9599999999999, "text": " at the full sentences and then make some sort of sentence of a prediction and we hope that", "tokens": [412, 264, 1577, 16579, 293, 550, 652, 512, 1333, 295, 8174, 295, 257, 17630, 293, 321, 1454, 300], "temperature": 0.0, "avg_logprob": -0.23300437344849564, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.0608604497974738e-05}, {"id": 247, "seek": 93340, "start": 948.9599999999999, "end": 955.1999999999999, "text": " this is kind of generalized which is something like question answering where you have a question", "tokens": [341, 307, 733, 295, 44498, 597, 307, 746, 411, 1168, 13430, 689, 291, 362, 257, 1168], "temperature": 0.0, "avg_logprob": -0.23300437344849564, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.0608604497974738e-05}, {"id": 248, "seek": 93340, "start": 955.1999999999999, "end": 960.04, "text": " and answer as sentence, and sentence B.", "tokens": [293, 1867, 382, 8174, 11, 293, 8174, 363, 13], "temperature": 0.0, "avg_logprob": -0.23300437344849564, "compression_ratio": 1.8823529411764706, "no_speech_prob": 1.0608604497974738e-05}, {"id": 249, "seek": 96004, "start": 960.04, "end": 967.04, "text": " So in terms of our input representation, it looks pretty similar to a normal transformer", "tokens": [407, 294, 2115, 295, 527, 4846, 10290, 11, 309, 1542, 1238, 2531, 281, 257, 2710, 31782], "temperature": 0.0, "avg_logprob": -0.1799466634534069, "compression_ratio": 1.7818930041152263, "no_speech_prob": 3.217318226234056e-05}, {"id": 250, "seek": 96004, "start": 967.04, "end": 972.0799999999999, "text": " but we have these additional embeddings which are called segment embeddings.", "tokens": [457, 321, 362, 613, 4497, 12240, 29432, 597, 366, 1219, 9469, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.1799466634534069, "compression_ratio": 1.7818930041152263, "no_speech_prob": 3.217318226234056e-05}, {"id": 251, "seek": 96004, "start": 972.0799999999999, "end": 977.24, "text": " So normal transformer, you would have your input and then you would do word piece segmentation", "tokens": [407, 2710, 31782, 11, 291, 576, 362, 428, 4846, 293, 550, 291, 576, 360, 1349, 2522, 9469, 399], "temperature": 0.0, "avg_logprob": -0.1799466634534069, "compression_ratio": 1.7818930041152263, "no_speech_prob": 3.217318226234056e-05}, {"id": 252, "seek": 96004, "start": 977.24, "end": 985.8399999999999, "text": " right where you split up, we apply this unsupervised splitting of words into kind of morphological", "tokens": [558, 689, 291, 7472, 493, 11, 321, 3079, 341, 2693, 12879, 24420, 30348, 295, 2283, 666, 733, 295, 25778, 4383], "temperature": 0.0, "avg_logprob": -0.1799466634534069, "compression_ratio": 1.7818930041152263, "no_speech_prob": 3.217318226234056e-05}, {"id": 253, "seek": 96004, "start": 985.8399999999999, "end": 989.52, "text": " splits but they're usually often not morphological, so it's unsupervised.", "tokens": [37741, 457, 436, 434, 2673, 2049, 406, 25778, 4383, 11, 370, 309, 311, 2693, 12879, 24420, 13], "temperature": 0.0, "avg_logprob": -0.1799466634534069, "compression_ratio": 1.7818930041152263, "no_speech_prob": 3.217318226234056e-05}, {"id": 254, "seek": 98952, "start": 989.52, "end": 992.04, "text": " But you end up with something that's roughly morphological right?", "tokens": [583, 291, 917, 493, 365, 746, 300, 311, 9810, 25778, 4383, 558, 30], "temperature": 0.0, "avg_logprob": -0.2537110646565755, "compression_ratio": 1.6824817518248176, "no_speech_prob": 6.102934275986627e-05}, {"id": 255, "seek": 98952, "start": 992.04, "end": 997.04, "text": " And so now you have like no out of vocabulary tokens, everything is represented at the", "tokens": [400, 370, 586, 291, 362, 411, 572, 484, 295, 19864, 22667, 11, 1203, 307, 10379, 412, 264], "temperature": 0.0, "avg_logprob": -0.2537110646565755, "compression_ratio": 1.6824817518248176, "no_speech_prob": 6.102934275986627e-05}, {"id": 256, "seek": 98952, "start": 997.04, "end": 999.36, "text": " very least you always split into characters.", "tokens": [588, 1935, 291, 1009, 7472, 666, 4342, 13], "temperature": 0.0, "avg_logprob": -0.2537110646565755, "compression_ratio": 1.6824817518248176, "no_speech_prob": 6.102934275986627e-05}, {"id": 257, "seek": 98952, "start": 999.36, "end": 1005.52, "text": " So we use a 30,000 word vocabulary and then we have our token embeddings, then we have", "tokens": [407, 321, 764, 257, 2217, 11, 1360, 1349, 19864, 293, 550, 321, 362, 527, 14862, 12240, 29432, 11, 550, 321, 362], "temperature": 0.0, "avg_logprob": -0.2537110646565755, "compression_ratio": 1.6824817518248176, "no_speech_prob": 6.102934275986627e-05}, {"id": 258, "seek": 98952, "start": 1005.52, "end": 1007.4399999999999, "text": " our normal position embeddings which is at the bottom.", "tokens": [527, 2710, 2535, 12240, 29432, 597, 307, 412, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.2537110646565755, "compression_ratio": 1.6824817518248176, "no_speech_prob": 6.102934275986627e-05}, {"id": 259, "seek": 98952, "start": 1007.4399999999999, "end": 1011.4399999999999, "text": " So these are part of the transformer where because transformers unlike LSTMs don't have", "tokens": [407, 613, 366, 644, 295, 264, 31782, 689, 570, 4088, 433, 8343, 441, 6840, 26386, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.2537110646565755, "compression_ratio": 1.6824817518248176, "no_speech_prob": 6.102934275986627e-05}, {"id": 260, "seek": 98952, "start": 1011.4399999999999, "end": 1015.6, "text": " any sort of locational awareness.", "tokens": [604, 1333, 295, 1628, 1478, 8888, 13], "temperature": 0.0, "avg_logprob": -0.2537110646565755, "compression_ratio": 1.6824817518248176, "no_speech_prob": 6.102934275986627e-05}, {"id": 261, "seek": 101560, "start": 1015.6, "end": 1020.8000000000001, "text": " So the way to encode that is that you encode an actual embedding for every position.", "tokens": [407, 264, 636, 281, 2058, 1429, 300, 307, 300, 291, 2058, 1429, 364, 3539, 12240, 3584, 337, 633, 2535, 13], "temperature": 0.0, "avg_logprob": -0.19410974748672977, "compression_ratio": 1.830827067669173, "no_speech_prob": 9.601755300536752e-05}, {"id": 262, "seek": 101560, "start": 1020.8000000000001, "end": 1025.8, "text": " So this is called absolute position embedding, there's other techniques nowadays.", "tokens": [407, 341, 307, 1219, 8236, 2535, 12240, 3584, 11, 456, 311, 661, 7512, 13434, 13], "temperature": 0.0, "avg_logprob": -0.19410974748672977, "compression_ratio": 1.830827067669173, "no_speech_prob": 9.601755300536752e-05}, {"id": 263, "seek": 101560, "start": 1025.8, "end": 1030.56, "text": " And then you have the segment embedding which is this is a sentence A or sentence B.", "tokens": [400, 550, 291, 362, 264, 9469, 12240, 3584, 597, 307, 341, 307, 257, 8174, 316, 420, 8174, 363, 13], "temperature": 0.0, "avg_logprob": -0.19410974748672977, "compression_ratio": 1.830827067669173, "no_speech_prob": 9.601755300536752e-05}, {"id": 264, "seek": 101560, "start": 1030.56, "end": 1034.32, "text": " And so this kind of generalizes in more general context.", "tokens": [400, 370, 341, 733, 295, 2674, 5660, 294, 544, 2674, 4319, 13], "temperature": 0.0, "avg_logprob": -0.19410974748672977, "compression_ratio": 1.830827067669173, "no_speech_prob": 9.601755300536752e-05}, {"id": 265, "seek": 101560, "start": 1034.32, "end": 1037.56, "text": " So you can imagine if you're trying to say like you're trying to do like web search, you", "tokens": [407, 291, 393, 3811, 498, 291, 434, 1382, 281, 584, 411, 291, 434, 1382, 281, 360, 411, 3670, 3164, 11, 291], "temperature": 0.0, "avg_logprob": -0.19410974748672977, "compression_ratio": 1.830827067669173, "no_speech_prob": 9.601755300536752e-05}, {"id": 266, "seek": 101560, "start": 1037.56, "end": 1044.76, "text": " might say here's my query, here's the title, here's the URL, here's the document content.", "tokens": [1062, 584, 510, 311, 452, 14581, 11, 510, 311, 264, 4876, 11, 510, 311, 264, 12905, 11, 510, 311, 264, 4166, 2701, 13], "temperature": 0.0, "avg_logprob": -0.19410974748672977, "compression_ratio": 1.830827067669173, "no_speech_prob": 9.601755300536752e-05}, {"id": 267, "seek": 104476, "start": 1044.76, "end": 1047.4, "text": " And so you can kind of just pack these all into a single sequence and then just give them", "tokens": [400, 370, 291, 393, 733, 295, 445, 2844, 613, 439, 666, 257, 2167, 8310, 293, 550, 445, 976, 552], "temperature": 0.0, "avg_logprob": -0.18098823295151892, "compression_ratio": 1.9545454545454546, "no_speech_prob": 4.4674128730548546e-05}, {"id": 268, "seek": 104476, "start": 1047.4, "end": 1054.96, "text": " different segment embeddings or type embeddings so that now you get are able to have a much", "tokens": [819, 9469, 12240, 29432, 420, 2010, 12240, 29432, 370, 300, 586, 291, 483, 366, 1075, 281, 362, 257, 709], "temperature": 0.0, "avg_logprob": -0.18098823295151892, "compression_ratio": 1.9545454545454546, "no_speech_prob": 4.4674128730548546e-05}, {"id": 269, "seek": 104476, "start": 1054.96, "end": 1061.92, "text": " stronger, you're able to kind of just represent everything in this kind of same single sequence", "tokens": [7249, 11, 291, 434, 1075, 281, 733, 295, 445, 2906, 1203, 294, 341, 733, 295, 912, 2167, 8310], "temperature": 0.0, "avg_logprob": -0.18098823295151892, "compression_ratio": 1.9545454545454546, "no_speech_prob": 4.4674128730548546e-05}, {"id": 270, "seek": 104476, "start": 1061.92, "end": 1064.6, "text": " where you kind of differentiate it but just the single embedding that's different.", "tokens": [689, 291, 733, 295, 23203, 309, 457, 445, 264, 2167, 12240, 3584, 300, 311, 819, 13], "temperature": 0.0, "avg_logprob": -0.18098823295151892, "compression_ratio": 1.9545454545454546, "no_speech_prob": 4.4674128730548546e-05}, {"id": 271, "seek": 104476, "start": 1064.6, "end": 1067.52, "text": " And this is all of course learned.", "tokens": [400, 341, 307, 439, 295, 1164, 3264, 13], "temperature": 0.0, "avg_logprob": -0.18098823295151892, "compression_ratio": 1.9545454545454546, "no_speech_prob": 4.4674128730548546e-05}, {"id": 272, "seek": 104476, "start": 1067.52, "end": 1070.32, "text": " And so this is in contrast to kind of the older style where you would typically have a", "tokens": [400, 370, 341, 307, 294, 8712, 281, 733, 295, 264, 4906, 3758, 689, 291, 576, 5850, 362, 257], "temperature": 0.0, "avg_logprob": -0.18098823295151892, "compression_ratio": 1.9545454545454546, "no_speech_prob": 4.4674128730548546e-05}, {"id": 273, "seek": 104476, "start": 1070.32, "end": 1071.72, "text": " different encoder for every part.", "tokens": [819, 2058, 19866, 337, 633, 644, 13], "temperature": 0.0, "avg_logprob": -0.18098823295151892, "compression_ratio": 1.9545454545454546, "no_speech_prob": 4.4674128730548546e-05}, {"id": 274, "seek": 107172, "start": 1071.72, "end": 1074.68, "text": " So like you would have a different encoder for the query and then maybe the title and the", "tokens": [407, 411, 291, 576, 362, 257, 819, 2058, 19866, 337, 264, 14581, 293, 550, 1310, 264, 4876, 293, 264], "temperature": 0.0, "avg_logprob": -0.23422284333602242, "compression_ratio": 1.6616541353383458, "no_speech_prob": 2.2467882445198484e-05}, {"id": 275, "seek": 107172, "start": 1074.68, "end": 1075.68, "text": " URL.", "tokens": [12905, 13], "temperature": 0.0, "avg_logprob": -0.23422284333602242, "compression_ratio": 1.6616541353383458, "no_speech_prob": 2.2467882445198484e-05}, {"id": 276, "seek": 107172, "start": 1075.68, "end": 1078.2, "text": " But this case it's all just a single sequence.", "tokens": [583, 341, 1389, 309, 311, 439, 445, 257, 2167, 8310, 13], "temperature": 0.0, "avg_logprob": -0.23422284333602242, "compression_ratio": 1.6616541353383458, "no_speech_prob": 2.2467882445198484e-05}, {"id": 277, "seek": 107172, "start": 1078.2, "end": 1083.0, "text": " So we trained on about a 3 billion word corpus which was at the time large now it's not", "tokens": [407, 321, 8895, 322, 466, 257, 805, 5218, 1349, 1181, 31624, 597, 390, 412, 264, 565, 2416, 586, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.23422284333602242, "compression_ratio": 1.6616541353383458, "no_speech_prob": 2.2467882445198484e-05}, {"id": 278, "seek": 107172, "start": 1083.0, "end": 1087.56, "text": " actually even that big compared to what people are training on.", "tokens": [767, 754, 300, 955, 5347, 281, 437, 561, 366, 3097, 322, 13], "temperature": 0.0, "avg_logprob": -0.23422284333602242, "compression_ratio": 1.6616541353383458, "no_speech_prob": 2.2467882445198484e-05}, {"id": 279, "seek": 107172, "start": 1087.56, "end": 1092.8, "text": " We used a batch size which was also large.", "tokens": [492, 1143, 257, 15245, 2744, 597, 390, 611, 2416, 13], "temperature": 0.0, "avg_logprob": -0.23422284333602242, "compression_ratio": 1.6616541353383458, "no_speech_prob": 2.2467882445198484e-05}, {"id": 280, "seek": 107172, "start": 1092.8, "end": 1094.52, "text": " We trained for about 40 epochs of the data.", "tokens": [492, 8895, 337, 466, 3356, 30992, 28346, 295, 264, 1412, 13], "temperature": 0.0, "avg_logprob": -0.23422284333602242, "compression_ratio": 1.6616541353383458, "no_speech_prob": 2.2467882445198484e-05}, {"id": 281, "seek": 107172, "start": 1094.52, "end": 1098.72, "text": " We trained these two models which are still relatively large.", "tokens": [492, 8895, 613, 732, 5245, 597, 366, 920, 7226, 2416, 13], "temperature": 0.0, "avg_logprob": -0.23422284333602242, "compression_ratio": 1.6616541353383458, "no_speech_prob": 2.2467882445198484e-05}, {"id": 282, "seek": 109872, "start": 1098.72, "end": 1102.64, "text": " So one of them is 12 layer, 768 and then the other one is 24 layer, 1024.", "tokens": [407, 472, 295, 552, 307, 2272, 4583, 11, 24733, 23, 293, 550, 264, 661, 472, 307, 4022, 4583, 11, 1266, 7911, 13], "temperature": 0.0, "avg_logprob": -0.2576889105602703, "compression_ratio": 1.654275092936803, "no_speech_prob": 3.703751281136647e-05}, {"id": 283, "seek": 109872, "start": 1102.64, "end": 1106.28, "text": " So at the time this is basically like one of the largest models that had been trained", "tokens": [407, 412, 264, 565, 341, 307, 1936, 411, 472, 295, 264, 6443, 5245, 300, 632, 668, 8895], "temperature": 0.0, "avg_logprob": -0.2576889105602703, "compression_ratio": 1.654275092936803, "no_speech_prob": 3.703751281136647e-05}, {"id": 284, "seek": 109872, "start": 1106.28, "end": 1112.44, "text": " although now people are training models that I think are 30 times or more bigger than this", "tokens": [4878, 586, 561, 366, 3097, 5245, 300, 286, 519, 366, 2217, 1413, 420, 544, 3801, 813, 341], "temperature": 0.0, "avg_logprob": -0.2576889105602703, "compression_ratio": 1.654275092936803, "no_speech_prob": 3.703751281136647e-05}, {"id": 285, "seek": 109872, "start": 1112.44, "end": 1113.52, "text": " in the more recent papers.", "tokens": [294, 264, 544, 5162, 10577, 13], "temperature": 0.0, "avg_logprob": -0.2576889105602703, "compression_ratio": 1.654275092936803, "no_speech_prob": 3.703751281136647e-05}, {"id": 286, "seek": 109872, "start": 1113.52, "end": 1119.4, "text": " So things have kind of exploded in terms of compute in the last I know about three years.", "tokens": [407, 721, 362, 733, 295, 27049, 294, 2115, 295, 14722, 294, 264, 1036, 286, 458, 466, 1045, 924, 13], "temperature": 0.0, "avg_logprob": -0.2576889105602703, "compression_ratio": 1.654275092936803, "no_speech_prob": 3.703751281136647e-05}, {"id": 287, "seek": 109872, "start": 1119.4, "end": 1122.08, "text": " But yeah.", "tokens": [583, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2576889105602703, "compression_ratio": 1.654275092936803, "no_speech_prob": 3.703751281136647e-05}, {"id": 288, "seek": 109872, "start": 1122.08, "end": 1127.92, "text": " So the fine tuning procedure is, it's pretty straightforward right?", "tokens": [407, 264, 2489, 15164, 10747, 307, 11, 309, 311, 1238, 15325, 558, 30], "temperature": 0.0, "avg_logprob": -0.2576889105602703, "compression_ratio": 1.654275092936803, "no_speech_prob": 3.703751281136647e-05}, {"id": 289, "seek": 112792, "start": 1127.92, "end": 1130.76, "text": " So we pre-trained this model for these two tasks.", "tokens": [407, 321, 659, 12, 17227, 2001, 341, 2316, 337, 613, 732, 9608, 13], "temperature": 0.0, "avg_logprob": -0.19537650214301217, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0001123346810345538}, {"id": 290, "seek": 112792, "start": 1130.76, "end": 1134.6000000000001, "text": " And so now we have an input sequence which is multiple sentences with different type", "tokens": [400, 370, 586, 321, 362, 364, 4846, 8310, 597, 307, 3866, 16579, 365, 819, 2010], "temperature": 0.0, "avg_logprob": -0.19537650214301217, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0001123346810345538}, {"id": 291, "seek": 112792, "start": 1134.6000000000001, "end": 1135.6000000000001, "text": " embeddings.", "tokens": [12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.19537650214301217, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0001123346810345538}, {"id": 292, "seek": 112792, "start": 1135.6000000000001, "end": 1140.16, "text": " We feed them through our transformer model.", "tokens": [492, 3154, 552, 807, 527, 31782, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19537650214301217, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0001123346810345538}, {"id": 293, "seek": 112792, "start": 1140.16, "end": 1143.76, "text": " And now we have the special embedding which I think I didn't mention.", "tokens": [400, 586, 321, 362, 264, 2121, 12240, 3584, 597, 286, 519, 286, 994, 380, 2152, 13], "temperature": 0.0, "avg_logprob": -0.19537650214301217, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0001123346810345538}, {"id": 294, "seek": 112792, "start": 1143.76, "end": 1148.96, "text": " So this special embedding, this is basically, it's learned to predict the next sentence", "tokens": [407, 341, 2121, 12240, 3584, 11, 341, 307, 1936, 11, 309, 311, 3264, 281, 6069, 264, 958, 8174], "temperature": 0.0, "avg_logprob": -0.19537650214301217, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0001123346810345538}, {"id": 295, "seek": 112792, "start": 1148.96, "end": 1154.3600000000001, "text": " prediction task and then this is used also for classification task.", "tokens": [17630, 5633, 293, 550, 341, 307, 1143, 611, 337, 21538, 5633, 13], "temperature": 0.0, "avg_logprob": -0.19537650214301217, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0001123346810345538}, {"id": 296, "seek": 112792, "start": 1154.3600000000001, "end": 1156.52, "text": " But it's not just that we're using the embedding right?", "tokens": [583, 309, 311, 406, 445, 300, 321, 434, 1228, 264, 12240, 3584, 558, 30], "temperature": 0.0, "avg_logprob": -0.19537650214301217, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0001123346810345538}, {"id": 297, "seek": 115652, "start": 1156.52, "end": 1159.2, "text": " We are, we're fine tuning the entire model right?", "tokens": [492, 366, 11, 321, 434, 2489, 15164, 264, 2302, 2316, 558, 30], "temperature": 0.0, "avg_logprob": -0.16388458180650373, "compression_ratio": 1.9054054054054055, "no_speech_prob": 4.264087328920141e-05}, {"id": 298, "seek": 115652, "start": 1159.2, "end": 1162.72, "text": " So it's really not that this embedding is intrinsically useful or that the word embedding", "tokens": [407, 309, 311, 534, 406, 300, 341, 12240, 3584, 307, 28621, 984, 4420, 420, 300, 264, 1349, 12240, 3584], "temperature": 0.0, "avg_logprob": -0.16388458180650373, "compression_ratio": 1.9054054054054055, "no_speech_prob": 4.264087328920141e-05}, {"id": 299, "seek": 115652, "start": 1162.72, "end": 1163.8799999999999, "text": " is intrinsically useful.", "tokens": [307, 28621, 984, 4420, 13], "temperature": 0.0, "avg_logprob": -0.16388458180650373, "compression_ratio": 1.9054054054054055, "no_speech_prob": 4.264087328920141e-05}, {"id": 300, "seek": 115652, "start": 1163.8799999999999, "end": 1169.48, "text": " It's that the weights inside the entire 12 or 24 layer model are useful.", "tokens": [467, 311, 300, 264, 17443, 1854, 264, 2302, 2272, 420, 4022, 4583, 2316, 366, 4420, 13], "temperature": 0.0, "avg_logprob": -0.16388458180650373, "compression_ratio": 1.9054054054054055, "no_speech_prob": 4.264087328920141e-05}, {"id": 301, "seek": 115652, "start": 1169.48, "end": 1173.92, "text": " And so by fine tuning the entire model you can kind of pick out the salient parts that", "tokens": [400, 370, 538, 2489, 15164, 264, 2302, 2316, 291, 393, 733, 295, 1888, 484, 264, 1845, 1196, 3166, 300], "temperature": 0.0, "avg_logprob": -0.16388458180650373, "compression_ratio": 1.9054054054054055, "no_speech_prob": 4.264087328920141e-05}, {"id": 302, "seek": 115652, "start": 1173.92, "end": 1177.44, "text": " are important for some downstream task.", "tokens": [366, 1021, 337, 512, 30621, 5633, 13], "temperature": 0.0, "avg_logprob": -0.16388458180650373, "compression_ratio": 1.9054054054054055, "no_speech_prob": 4.264087328920141e-05}, {"id": 303, "seek": 115652, "start": 1177.44, "end": 1182.68, "text": " And so this is the kind of the class specific fine tuning.", "tokens": [400, 370, 341, 307, 264, 733, 295, 264, 1508, 2685, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.16388458180650373, "compression_ratio": 1.9054054054054055, "no_speech_prob": 4.264087328920141e-05}, {"id": 304, "seek": 118268, "start": 1182.68, "end": 1189.44, "text": " So if we have a single classification task like let's say sentiment analysis where you", "tokens": [407, 498, 321, 362, 257, 2167, 21538, 5633, 411, 718, 311, 584, 16149, 5215, 689, 291], "temperature": 0.0, "avg_logprob": -0.28774547576904297, "compression_ratio": 1.7906976744186047, "no_speech_prob": 4.1976873035309836e-05}, {"id": 305, "seek": 118268, "start": 1189.44, "end": 1192.24, "text": " say is this a positive or negative review?", "tokens": [584, 307, 341, 257, 3353, 420, 3671, 3131, 30], "temperature": 0.0, "avg_logprob": -0.28774547576904297, "compression_ratio": 1.7906976744186047, "no_speech_prob": 4.1976873035309836e-05}, {"id": 306, "seek": 118268, "start": 1192.24, "end": 1194.88, "text": " We encode our sentence with the birth model.", "tokens": [492, 2058, 1429, 527, 8174, 365, 264, 3965, 2316, 13], "temperature": 0.0, "avg_logprob": -0.28774547576904297, "compression_ratio": 1.7906976744186047, "no_speech_prob": 4.1976873035309836e-05}, {"id": 307, "seek": 118268, "start": 1194.88, "end": 1198.44, "text": " And the only parameters that we add are this final output matrix right?", "tokens": [400, 264, 787, 9834, 300, 321, 909, 366, 341, 2572, 5598, 8141, 558, 30], "temperature": 0.0, "avg_logprob": -0.28774547576904297, "compression_ratio": 1.7906976744186047, "no_speech_prob": 4.1976873035309836e-05}, {"id": 308, "seek": 118268, "start": 1198.44, "end": 1202.8400000000001, "text": " So maybe if we have three, like say positive, negative or neutral, this might be a thousand", "tokens": [407, 1310, 498, 321, 362, 1045, 11, 411, 584, 3353, 11, 3671, 420, 10598, 11, 341, 1062, 312, 257, 4714], "temperature": 0.0, "avg_logprob": -0.28774547576904297, "compression_ratio": 1.7906976744186047, "no_speech_prob": 4.1976873035309836e-05}, {"id": 309, "seek": 118268, "start": 1202.8400000000001, "end": 1203.8400000000001, "text": " times three right?", "tokens": [1413, 1045, 558, 30], "temperature": 0.0, "avg_logprob": -0.28774547576904297, "compression_ratio": 1.7906976744186047, "no_speech_prob": 4.1976873035309836e-05}, {"id": 310, "seek": 118268, "start": 1203.8400000000001, "end": 1207.6000000000001, "text": " So it's just 3,000 parameters and 300 million.", "tokens": [407, 309, 311, 445, 805, 11, 1360, 9834, 293, 6641, 2459, 13], "temperature": 0.0, "avg_logprob": -0.28774547576904297, "compression_ratio": 1.7906976744186047, "no_speech_prob": 4.1976873035309836e-05}, {"id": 311, "seek": 118268, "start": 1207.6000000000001, "end": 1211.72, "text": " So a 3,000 new parameters and 300 million old parameters.", "tokens": [407, 257, 805, 11, 1360, 777, 9834, 293, 6641, 2459, 1331, 9834, 13], "temperature": 0.0, "avg_logprob": -0.28774547576904297, "compression_ratio": 1.7906976744186047, "no_speech_prob": 4.1976873035309836e-05}, {"id": 312, "seek": 121172, "start": 1211.72, "end": 1218.04, "text": " And we jointly train all 300 million plus 3,000 for this downstream task.", "tokens": [400, 321, 46557, 3847, 439, 6641, 2459, 1804, 805, 11, 1360, 337, 341, 30621, 5633, 13], "temperature": 0.0, "avg_logprob": -0.239497799343533, "compression_ratio": 1.7194719471947195, "no_speech_prob": 1.8626704331836663e-05}, {"id": 313, "seek": 121172, "start": 1218.04, "end": 1222.0, "text": " But because the vast majority of them are pre-trained, we can kind of adapt to it in only like a", "tokens": [583, 570, 264, 8369, 6286, 295, 552, 366, 659, 12, 17227, 2001, 11, 321, 393, 733, 295, 6231, 281, 309, 294, 787, 411, 257], "temperature": 0.0, "avg_logprob": -0.239497799343533, "compression_ratio": 1.7194719471947195, "no_speech_prob": 1.8626704331836663e-05}, {"id": 314, "seek": 121172, "start": 1222.0, "end": 1224.4, "text": " few thousand label examples.", "tokens": [1326, 4714, 7645, 5110, 13], "temperature": 0.0, "avg_logprob": -0.239497799343533, "compression_ratio": 1.7194719471947195, "no_speech_prob": 1.8626704331836663e-05}, {"id": 315, "seek": 121172, "start": 1224.4, "end": 1229.08, "text": " And similarly for a sentence pair class, we do, we just can count into three sentences", "tokens": [400, 14138, 337, 257, 8174, 6119, 1508, 11, 321, 360, 11, 321, 445, 393, 1207, 666, 1045, 16579], "temperature": 0.0, "avg_logprob": -0.239497799343533, "compression_ratio": 1.7194719471947195, "no_speech_prob": 1.8626704331836663e-05}, {"id": 316, "seek": 121172, "start": 1229.08, "end": 1230.24, "text": " with different type embeddings.", "tokens": [365, 819, 2010, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.239497799343533, "compression_ratio": 1.7194719471947195, "no_speech_prob": 1.8626704331836663e-05}, {"id": 317, "seek": 121172, "start": 1230.24, "end": 1233.76, "text": " So we have, if you want to say does the sentence entail this other sentence, you say sentence", "tokens": [407, 321, 362, 11, 498, 291, 528, 281, 584, 775, 264, 8174, 948, 864, 341, 661, 8174, 11, 291, 584, 8174], "temperature": 0.0, "avg_logprob": -0.239497799343533, "compression_ratio": 1.7194719471947195, "no_speech_prob": 1.8626704331836663e-05}, {"id": 318, "seek": 121172, "start": 1233.76, "end": 1239.68, "text": " A, you put it, can count in sentence B, and then also predicts from this token and fine", "tokens": [316, 11, 291, 829, 309, 11, 393, 1207, 294, 8174, 363, 11, 293, 550, 611, 6069, 82, 490, 341, 14862, 293, 2489], "temperature": 0.0, "avg_logprob": -0.239497799343533, "compression_ratio": 1.7194719471947195, "no_speech_prob": 1.8626704331836663e-05}, {"id": 319, "seek": 121172, "start": 1239.68, "end": 1240.68, "text": " to the entire thing.", "tokens": [281, 264, 2302, 551, 13], "temperature": 0.0, "avg_logprob": -0.239497799343533, "compression_ratio": 1.7194719471947195, "no_speech_prob": 1.8626704331836663e-05}, {"id": 320, "seek": 124068, "start": 1240.68, "end": 1243.3200000000002, "text": " Similarly, very few additional parameters.", "tokens": [13157, 11, 588, 1326, 4497, 9834, 13], "temperature": 0.0, "avg_logprob": -0.23330157143729074, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.808186299167573e-05}, {"id": 321, "seek": 124068, "start": 1243.3200000000002, "end": 1249.2, "text": " For span prediction tasks, you just have kind of a start of span end of span.", "tokens": [1171, 16174, 17630, 9608, 11, 291, 445, 362, 733, 295, 257, 722, 295, 16174, 917, 295, 16174, 13], "temperature": 0.0, "avg_logprob": -0.23330157143729074, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.808186299167573e-05}, {"id": 322, "seek": 124068, "start": 1249.2, "end": 1252.04, "text": " So you're only adding a few thousand new parameters.", "tokens": [407, 291, 434, 787, 5127, 257, 1326, 4714, 777, 9834, 13], "temperature": 0.0, "avg_logprob": -0.23330157143729074, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.808186299167573e-05}, {"id": 323, "seek": 124068, "start": 1252.04, "end": 1261.48, "text": " And then for tagging tasks, like part of speech tagging, you just have a single sentence.", "tokens": [400, 550, 337, 6162, 3249, 9608, 11, 411, 644, 295, 6218, 6162, 3249, 11, 291, 445, 362, 257, 2167, 8174, 13], "temperature": 0.0, "avg_logprob": -0.23330157143729074, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.808186299167573e-05}, {"id": 324, "seek": 124068, "start": 1261.48, "end": 1265.28, "text": " You add every single token or maybe every token except for the word pieces.", "tokens": [509, 909, 633, 2167, 14862, 420, 1310, 633, 14862, 3993, 337, 264, 1349, 3755, 13], "temperature": 0.0, "avg_logprob": -0.23330157143729074, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.808186299167573e-05}, {"id": 325, "seek": 124068, "start": 1265.28, "end": 1268.24, "text": " But like, you, that's kind of free processing.", "tokens": [583, 411, 11, 291, 11, 300, 311, 733, 295, 1737, 9007, 13], "temperature": 0.0, "avg_logprob": -0.23330157143729074, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.808186299167573e-05}, {"id": 326, "seek": 126824, "start": 1268.24, "end": 1270.96, "text": " You predict what's the part of speech of this.", "tokens": [509, 6069, 437, 311, 264, 644, 295, 6218, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.19949036378126878, "compression_ratio": 1.5436507936507937, "no_speech_prob": 5.062888521933928e-05}, {"id": 327, "seek": 126824, "start": 1270.96, "end": 1279.16, "text": " And so this is really why, so like, bird itself is really a kind of, I would say an incremental", "tokens": [400, 370, 341, 307, 534, 983, 11, 370, 411, 11, 5255, 2564, 307, 534, 257, 733, 295, 11, 286, 576, 584, 364, 35759], "temperature": 0.0, "avg_logprob": -0.19949036378126878, "compression_ratio": 1.5436507936507937, "no_speech_prob": 5.062888521933928e-05}, {"id": 328, "seek": 126824, "start": 1279.16, "end": 1280.76, "text": " improvement over what already existed.", "tokens": [10444, 670, 437, 1217, 13135, 13], "temperature": 0.0, "avg_logprob": -0.19949036378126878, "compression_ratio": 1.5436507936507937, "no_speech_prob": 5.062888521933928e-05}, {"id": 329, "seek": 126824, "start": 1280.76, "end": 1288.84, "text": " So it kind of took transformers, LMO, GPT, really these three ideas and kind of made", "tokens": [407, 309, 733, 295, 1890, 4088, 433, 11, 441, 18976, 11, 26039, 51, 11, 534, 613, 1045, 3487, 293, 733, 295, 1027], "temperature": 0.0, "avg_logprob": -0.19949036378126878, "compression_ratio": 1.5436507936507937, "no_speech_prob": 5.062888521933928e-05}, {"id": 330, "seek": 126824, "start": 1288.84, "end": 1293.32, "text": " a pretty simple change on top of them.", "tokens": [257, 1238, 2199, 1319, 322, 1192, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.19949036378126878, "compression_ratio": 1.5436507936507937, "no_speech_prob": 5.062888521933928e-05}, {"id": 331, "seek": 126824, "start": 1293.32, "end": 1297.92, "text": " But the reason why it had such big impact is not just the numbers that I'll show in", "tokens": [583, 264, 1778, 983, 309, 632, 1270, 955, 2712, 307, 406, 445, 264, 3547, 300, 286, 603, 855, 294], "temperature": 0.0, "avg_logprob": -0.19949036378126878, "compression_ratio": 1.5436507936507937, "no_speech_prob": 5.062888521933928e-05}, {"id": 332, "seek": 129792, "start": 1297.92, "end": 1298.92, "text": " a few slides.", "tokens": [257, 1326, 9788, 13], "temperature": 0.0, "avg_logprob": -0.2345778563312281, "compression_ratio": 1.578125, "no_speech_prob": 0.00016340211732313037}, {"id": 333, "seek": 129792, "start": 1298.92, "end": 1300.88, "text": " It's really this thing.", "tokens": [467, 311, 534, 341, 551, 13], "temperature": 0.0, "avg_logprob": -0.2345778563312281, "compression_ratio": 1.578125, "no_speech_prob": 0.00016340211732313037}, {"id": 334, "seek": 129792, "start": 1300.88, "end": 1308.4, "text": " Because with LMO, there was really no fundamental difference between, it was just contextual", "tokens": [1436, 365, 441, 18976, 11, 456, 390, 534, 572, 8088, 2649, 1296, 11, 309, 390, 445, 35526], "temperature": 0.0, "avg_logprob": -0.2345778563312281, "compression_ratio": 1.578125, "no_speech_prob": 0.00016340211732313037}, {"id": 335, "seek": 129792, "start": 1308.4, "end": 1309.4, "text": " embedding.", "tokens": [12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.2345778563312281, "compression_ratio": 1.578125, "no_speech_prob": 0.00016340211732313037}, {"id": 336, "seek": 129792, "start": 1309.4, "end": 1314.68, "text": " So you still have, like, a lot of deep learning historically has been fun and building new", "tokens": [407, 291, 920, 362, 11, 411, 11, 257, 688, 295, 2452, 2539, 16180, 575, 668, 1019, 293, 2390, 777], "temperature": 0.0, "avg_logprob": -0.2345778563312281, "compression_ratio": 1.578125, "no_speech_prob": 0.00016340211732313037}, {"id": 337, "seek": 129792, "start": 1314.68, "end": 1315.68, "text": " models, right?", "tokens": [5245, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2345778563312281, "compression_ratio": 1.578125, "no_speech_prob": 0.00016340211732313037}, {"id": 338, "seek": 129792, "start": 1315.68, "end": 1318.28, "text": " So you have all of these components, kind of like Lego blocks, right?", "tokens": [407, 291, 362, 439, 295, 613, 6677, 11, 733, 295, 411, 28761, 8474, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2345778563312281, "compression_ratio": 1.578125, "no_speech_prob": 0.00016340211732313037}, {"id": 339, "seek": 129792, "start": 1318.28, "end": 1324.92, "text": " You have attention layers, feed forward layers, layer normalization, LSTMs, et cetera.", "tokens": [509, 362, 3202, 7914, 11, 3154, 2128, 7914, 11, 4583, 2710, 2144, 11, 441, 6840, 26386, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.2345778563312281, "compression_ratio": 1.578125, "no_speech_prob": 0.00016340211732313037}, {"id": 340, "seek": 132492, "start": 1324.92, "end": 1330.24, "text": " And you can just kind of like figure out, say, okay, for this new task, how do I glue", "tokens": [400, 291, 393, 445, 733, 295, 411, 2573, 484, 11, 584, 11, 1392, 11, 337, 341, 777, 5633, 11, 577, 360, 286, 8998], "temperature": 0.0, "avg_logprob": -0.1924640110560826, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6098881284706295e-05}, {"id": 341, "seek": 132492, "start": 1330.24, "end": 1332.8400000000001, "text": " these together in a way that's best, right?", "tokens": [613, 1214, 294, 257, 636, 300, 311, 1151, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1924640110560826, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6098881284706295e-05}, {"id": 342, "seek": 132492, "start": 1332.8400000000001, "end": 1339.44, "text": " And so, and so with LMO, it wasn't really, it didn't really change anything fundamentally.", "tokens": [400, 370, 11, 293, 370, 365, 441, 18976, 11, 309, 2067, 380, 534, 11, 309, 994, 380, 534, 1319, 1340, 17879, 13], "temperature": 0.0, "avg_logprob": -0.1924640110560826, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6098881284706295e-05}, {"id": 343, "seek": 132492, "start": 1339.44, "end": 1342.6000000000001, "text": " It was just, because these were, you just fed it into your existing model and you got", "tokens": [467, 390, 445, 11, 570, 613, 645, 11, 291, 445, 4636, 309, 666, 428, 6741, 2316, 293, 291, 658], "temperature": 0.0, "avg_logprob": -0.1924640110560826, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6098881284706295e-05}, {"id": 344, "seek": 132492, "start": 1342.6000000000001, "end": 1343.6000000000001, "text": " to the art.", "tokens": [281, 264, 1523, 13], "temperature": 0.0, "avg_logprob": -0.1924640110560826, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6098881284706295e-05}, {"id": 345, "seek": 132492, "start": 1343.6000000000001, "end": 1349.96, "text": " For GPT one, it wasn't, most, like these things didn't really work, right?", "tokens": [1171, 26039, 51, 472, 11, 309, 2067, 380, 11, 881, 11, 411, 613, 721, 994, 380, 534, 589, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1924640110560826, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6098881284706295e-05}, {"id": 346, "seek": 132492, "start": 1349.96, "end": 1351.68, "text": " Because it was a left to right language model.", "tokens": [1436, 309, 390, 257, 1411, 281, 558, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1924640110560826, "compression_ratio": 1.6923076923076923, "no_speech_prob": 4.6098881284706295e-05}, {"id": 347, "seek": 135168, "start": 1351.68, "end": 1355.76, "text": " And so you could just kind of take the last token and then predict a classification task,", "tokens": [400, 370, 291, 727, 445, 733, 295, 747, 264, 1036, 14862, 293, 550, 6069, 257, 21538, 5633, 11], "temperature": 0.0, "avg_logprob": -0.202046153991203, "compression_ratio": 1.7216117216117217, "no_speech_prob": 1.3843872693541925e-05}, {"id": 348, "seek": 135168, "start": 1355.76, "end": 1359.3200000000002, "text": " but it didn't really make any sense to predict, like, part of speech tags.", "tokens": [457, 309, 994, 380, 534, 652, 604, 2020, 281, 6069, 11, 411, 11, 644, 295, 6218, 18632, 13], "temperature": 0.0, "avg_logprob": -0.202046153991203, "compression_ratio": 1.7216117216117217, "no_speech_prob": 1.3843872693541925e-05}, {"id": 349, "seek": 135168, "start": 1359.3200000000002, "end": 1361.44, "text": " Because for the first word, there's no context.", "tokens": [1436, 337, 264, 700, 1349, 11, 456, 311, 572, 4319, 13], "temperature": 0.0, "avg_logprob": -0.202046153991203, "compression_ratio": 1.7216117216117217, "no_speech_prob": 1.3843872693541925e-05}, {"id": 350, "seek": 135168, "start": 1361.44, "end": 1365.16, "text": " So it makes no sense to predict the word with no context.", "tokens": [407, 309, 1669, 572, 2020, 281, 6069, 264, 1349, 365, 572, 4319, 13], "temperature": 0.0, "avg_logprob": -0.202046153991203, "compression_ratio": 1.7216117216117217, "no_speech_prob": 1.3843872693541925e-05}, {"id": 351, "seek": 135168, "start": 1365.16, "end": 1371.48, "text": " With BERT, the reason why it had such high impact was because it kind of simplified things.", "tokens": [2022, 363, 31479, 11, 264, 1778, 983, 309, 632, 1270, 1090, 2712, 390, 570, 309, 733, 295, 26335, 721, 13], "temperature": 0.0, "avg_logprob": -0.202046153991203, "compression_ratio": 1.7216117216117217, "no_speech_prob": 1.3843872693541925e-05}, {"id": 352, "seek": 135168, "start": 1371.48, "end": 1376.5600000000002, "text": " And so that's not, I'm not saying that's necessarily a good thing, because as a researcher,", "tokens": [400, 370, 300, 311, 406, 11, 286, 478, 406, 1566, 300, 311, 4725, 257, 665, 551, 11, 570, 382, 257, 21751, 11], "temperature": 0.0, "avg_logprob": -0.202046153991203, "compression_ratio": 1.7216117216117217, "no_speech_prob": 1.3843872693541925e-05}, {"id": 353, "seek": 135168, "start": 1376.5600000000002, "end": 1377.8400000000001, "text": " or a bad thing.", "tokens": [420, 257, 1578, 551, 13], "temperature": 0.0, "avg_logprob": -0.202046153991203, "compression_ratio": 1.7216117216117217, "no_speech_prob": 1.3843872693541925e-05}, {"id": 354, "seek": 137784, "start": 1377.84, "end": 1381.9199999999998, "text": " So as a researcher, kind of, ironically, the ultimate goal of research is often like research", "tokens": [407, 382, 257, 21751, 11, 733, 295, 11, 41082, 11, 264, 9705, 3387, 295, 2132, 307, 2049, 411, 2132], "temperature": 0.0, "avg_logprob": -0.2488859936707002, "compression_ratio": 1.913533834586466, "no_speech_prob": 7.598068623337895e-05}, {"id": 355, "seek": 137784, "start": 1381.9199999999998, "end": 1383.4399999999998, "text": " yourself out of a job, right?", "tokens": [1803, 484, 295, 257, 1691, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2488859936707002, "compression_ratio": 1.913533834586466, "no_speech_prob": 7.598068623337895e-05}, {"id": 356, "seek": 137784, "start": 1383.4399999999998, "end": 1387.84, "text": " It's like, you know, if a physicist, I'm not saying BERT had, anywhere near this impact,", "tokens": [467, 311, 411, 11, 291, 458, 11, 498, 257, 42466, 11, 286, 478, 406, 1566, 363, 31479, 632, 11, 4992, 2651, 341, 2712, 11], "temperature": 0.0, "avg_logprob": -0.2488859936707002, "compression_ratio": 1.913533834586466, "no_speech_prob": 7.598068623337895e-05}, {"id": 357, "seek": 137784, "start": 1387.84, "end": 1391.12, "text": " like a physicist that came up with like a grand theory of physics, they would kind of,", "tokens": [411, 257, 42466, 300, 1361, 493, 365, 411, 257, 2697, 5261, 295, 10649, 11, 436, 576, 733, 295, 11], "temperature": 0.0, "avg_logprob": -0.2488859936707002, "compression_ratio": 1.913533834586466, "no_speech_prob": 7.598068623337895e-05}, {"id": 358, "seek": 137784, "start": 1391.12, "end": 1395.28, "text": " like, they would be like the greatest moment in physics, but also that would kind of eliminate", "tokens": [411, 11, 436, 576, 312, 411, 264, 6636, 1623, 294, 10649, 11, 457, 611, 300, 576, 733, 295, 13819], "temperature": 0.0, "avg_logprob": -0.2488859936707002, "compression_ratio": 1.913533834586466, "no_speech_prob": 7.598068623337895e-05}, {"id": 359, "seek": 137784, "start": 1395.28, "end": 1396.28, "text": " a lot of research, right?", "tokens": [257, 688, 295, 2132, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2488859936707002, "compression_ratio": 1.913533834586466, "no_speech_prob": 7.598068623337895e-05}, {"id": 360, "seek": 137784, "start": 1396.28, "end": 1400.56, "text": " And so that's kind of like the end goal of research is kind of solve the problem, right?", "tokens": [400, 370, 300, 311, 733, 295, 411, 264, 917, 3387, 295, 2132, 307, 733, 295, 5039, 264, 1154, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2488859936707002, "compression_ratio": 1.913533834586466, "no_speech_prob": 7.598068623337895e-05}, {"id": 361, "seek": 140056, "start": 1400.56, "end": 1410.24, "text": " So BERT kind of has a step where now, like, all of these different causes of problems,", "tokens": [407, 363, 31479, 733, 295, 575, 257, 1823, 689, 586, 11, 411, 11, 439, 295, 613, 819, 7700, 295, 2740, 11], "temperature": 0.0, "avg_logprob": -0.23245352217294638, "compression_ratio": 1.68, "no_speech_prob": 4.68204052594956e-05}, {"id": 362, "seek": 140056, "start": 1410.24, "end": 1415.9199999999998, "text": " there's really, it kind of killed like a lot of the need to do model architecture design,", "tokens": [456, 311, 534, 11, 309, 733, 295, 4652, 411, 257, 688, 295, 264, 643, 281, 360, 2316, 9482, 1715, 11], "temperature": 0.0, "avg_logprob": -0.23245352217294638, "compression_ratio": 1.68, "no_speech_prob": 4.68204052594956e-05}, {"id": 363, "seek": 140056, "start": 1415.9199999999998, "end": 1419.2, "text": " which is kind of unfortunate because that's like really fun.", "tokens": [597, 307, 733, 295, 17843, 570, 300, 311, 411, 534, 1019, 13], "temperature": 0.0, "avg_logprob": -0.23245352217294638, "compression_ratio": 1.68, "no_speech_prob": 4.68204052594956e-05}, {"id": 364, "seek": 140056, "start": 1419.2, "end": 1421.84, "text": " And so that's kind of the impact.", "tokens": [400, 370, 300, 311, 733, 295, 264, 2712, 13], "temperature": 0.0, "avg_logprob": -0.23245352217294638, "compression_ratio": 1.68, "no_speech_prob": 4.68204052594956e-05}, {"id": 365, "seek": 140056, "start": 1421.84, "end": 1425.12, "text": " And I'm not going to say whether it's like a good or a bad impact, it's kind of like the", "tokens": [400, 286, 478, 406, 516, 281, 584, 1968, 309, 311, 411, 257, 665, 420, 257, 1578, 2712, 11, 309, 311, 733, 295, 411, 264], "temperature": 0.0, "avg_logprob": -0.23245352217294638, "compression_ratio": 1.68, "no_speech_prob": 4.68204052594956e-05}, {"id": 366, "seek": 140056, "start": 1425.12, "end": 1426.12, "text": " objective impact.", "tokens": [10024, 2712, 13], "temperature": 0.0, "avg_logprob": -0.23245352217294638, "compression_ratio": 1.68, "no_speech_prob": 4.68204052594956e-05}, {"id": 367, "seek": 142612, "start": 1426.12, "end": 1432.7199999999998, "text": " So why it's had so much impact is because it has kind of had this effect on now so many", "tokens": [407, 983, 309, 311, 632, 370, 709, 2712, 307, 570, 309, 575, 733, 295, 632, 341, 1802, 322, 586, 370, 867], "temperature": 0.0, "avg_logprob": -0.17659959338960193, "compression_ratio": 1.7202797202797202, "no_speech_prob": 3.534828283591196e-05}, {"id": 368, "seek": 142612, "start": 1432.7199999999998, "end": 1436.9199999999998, "text": " things that used to be like designing fun, you know, models, it's just fitted in and", "tokens": [721, 300, 1143, 281, 312, 411, 14685, 1019, 11, 291, 458, 11, 5245, 11, 309, 311, 445, 26321, 294, 293], "temperature": 0.0, "avg_logprob": -0.17659959338960193, "compression_ratio": 1.7202797202797202, "no_speech_prob": 3.534828283591196e-05}, {"id": 369, "seek": 142612, "start": 1436.9199999999998, "end": 1441.9599999999998, "text": " use one of these four recipes and it kind of just works for all of these different tasks.", "tokens": [764, 472, 295, 613, 1451, 13035, 293, 309, 733, 295, 445, 1985, 337, 439, 295, 613, 819, 9608, 13], "temperature": 0.0, "avg_logprob": -0.17659959338960193, "compression_ratio": 1.7202797202797202, "no_speech_prob": 3.534828283591196e-05}, {"id": 370, "seek": 142612, "start": 1441.9599999999998, "end": 1445.4399999999998, "text": " So in terms of actual empirical results, so these are at the time that Hickory's published,", "tokens": [407, 294, 2115, 295, 3539, 31886, 3542, 11, 370, 613, 366, 412, 264, 565, 300, 389, 618, 827, 311, 6572, 11], "temperature": 0.0, "avg_logprob": -0.17659959338960193, "compression_ratio": 1.7202797202797202, "no_speech_prob": 3.534828283591196e-05}, {"id": 371, "seek": 142612, "start": 1445.4399999999998, "end": 1449.32, "text": " of course, things have gotten better since then.", "tokens": [295, 1164, 11, 721, 362, 5768, 1101, 1670, 550, 13], "temperature": 0.0, "avg_logprob": -0.17659959338960193, "compression_ratio": 1.7202797202797202, "no_speech_prob": 3.534828283591196e-05}, {"id": 372, "seek": 142612, "start": 1449.32, "end": 1454.8799999999999, "text": " So this glue task is a set of, they're all kind of similar in that they're all sentenced", "tokens": [407, 341, 8998, 5633, 307, 257, 992, 295, 11, 436, 434, 439, 733, 295, 2531, 294, 300, 436, 434, 439, 30954], "temperature": 0.0, "avg_logprob": -0.17659959338960193, "compression_ratio": 1.7202797202797202, "no_speech_prob": 3.534828283591196e-05}, {"id": 373, "seek": 145488, "start": 1454.88, "end": 1456.64, "text": " pair or sentence classification tests.", "tokens": [6119, 420, 8174, 21538, 6921, 13], "temperature": 0.0, "avg_logprob": -0.17188623547554016, "compression_ratio": 1.852112676056338, "no_speech_prob": 6.203455996001139e-05}, {"id": 374, "seek": 145488, "start": 1456.64, "end": 1461.88, "text": " So like multi NLI would be something like hills and mountains are especially sanctified", "tokens": [407, 411, 4825, 426, 48718, 576, 312, 746, 411, 21379, 293, 10233, 366, 2318, 21794, 2587], "temperature": 0.0, "avg_logprob": -0.17188623547554016, "compression_ratio": 1.852112676056338, "no_speech_prob": 6.203455996001139e-05}, {"id": 375, "seek": 145488, "start": 1461.88, "end": 1462.88, "text": " in Janism.", "tokens": [294, 4956, 1434, 13], "temperature": 0.0, "avg_logprob": -0.17188623547554016, "compression_ratio": 1.852112676056338, "no_speech_prob": 6.203455996001139e-05}, {"id": 376, "seek": 145488, "start": 1462.88, "end": 1466.0, "text": " And then hypothesis is Janism hates nature, that's a contradiction, right?", "tokens": [400, 550, 17291, 307, 4956, 1434, 23000, 3687, 11, 300, 311, 257, 34937, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17188623547554016, "compression_ratio": 1.852112676056338, "no_speech_prob": 6.203455996001139e-05}, {"id": 377, "seek": 145488, "start": 1466.0, "end": 1470.16, "text": " So in order for an NLP model to be able to understand this or to be able to answer this", "tokens": [407, 294, 1668, 337, 364, 426, 45196, 2316, 281, 312, 1075, 281, 1223, 341, 420, 281, 312, 1075, 281, 1867, 341], "temperature": 0.0, "avg_logprob": -0.17188623547554016, "compression_ratio": 1.852112676056338, "no_speech_prob": 6.203455996001139e-05}, {"id": 378, "seek": 145488, "start": 1470.16, "end": 1475.92, "text": " correctly and give this label of contradiction, it needs to know that hills and mountains", "tokens": [8944, 293, 976, 341, 7645, 295, 34937, 11, 309, 2203, 281, 458, 300, 21379, 293, 10233], "temperature": 0.0, "avg_logprob": -0.17188623547554016, "compression_ratio": 1.852112676056338, "no_speech_prob": 6.203455996001139e-05}, {"id": 379, "seek": 145488, "start": 1475.92, "end": 1481.24, "text": " are part of nature, sanctified is a good thing and that hating something is a bad thing", "tokens": [366, 644, 295, 3687, 11, 21794, 2587, 307, 257, 665, 551, 293, 300, 45082, 746, 307, 257, 1578, 551], "temperature": 0.0, "avg_logprob": -0.17188623547554016, "compression_ratio": 1.852112676056338, "no_speech_prob": 6.203455996001139e-05}, {"id": 380, "seek": 145488, "start": 1481.24, "end": 1483.0400000000002, "text": " and be able to do all of this reasoning, right?", "tokens": [293, 312, 1075, 281, 360, 439, 295, 341, 21577, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17188623547554016, "compression_ratio": 1.852112676056338, "no_speech_prob": 6.203455996001139e-05}, {"id": 381, "seek": 148304, "start": 1483.04, "end": 1485.2, "text": " So it's pretty complicated reasoning.", "tokens": [407, 309, 311, 1238, 6179, 21577, 13], "temperature": 0.0, "avg_logprob": -0.2013940290971236, "compression_ratio": 1.7044534412955465, "no_speech_prob": 3.704059781739488e-05}, {"id": 382, "seek": 148304, "start": 1485.2, "end": 1488.48, "text": " Similarly for cola, you have to be able to say like the wagon rumbled down the road", "tokens": [13157, 337, 40495, 11, 291, 362, 281, 312, 1075, 281, 584, 411, 264, 34453, 367, 19928, 760, 264, 3060], "temperature": 0.0, "avg_logprob": -0.2013940290971236, "compression_ratio": 1.7044534412955465, "no_speech_prob": 3.704059781739488e-05}, {"id": 383, "seek": 148304, "start": 1488.48, "end": 1490.8, "text": " versus the car honked down the road.", "tokens": [5717, 264, 1032, 2157, 34601, 760, 264, 3060, 13], "temperature": 0.0, "avg_logprob": -0.2013940290971236, "compression_ratio": 1.7044534412955465, "no_speech_prob": 3.704059781739488e-05}, {"id": 384, "seek": 148304, "start": 1490.8, "end": 1497.8, "text": " And so these things are, you know, one of them to a native English speaker sounds totally", "tokens": [400, 370, 613, 721, 366, 11, 291, 458, 11, 472, 295, 552, 281, 257, 8470, 3669, 8145, 3263, 3879], "temperature": 0.0, "avg_logprob": -0.2013940290971236, "compression_ratio": 1.7044534412955465, "no_speech_prob": 3.704059781739488e-05}, {"id": 385, "seek": 148304, "start": 1497.8, "end": 1499.28, "text": " fine, the other one sounds weird.", "tokens": [2489, 11, 264, 661, 472, 3263, 3657, 13], "temperature": 0.0, "avg_logprob": -0.2013940290971236, "compression_ratio": 1.7044534412955465, "no_speech_prob": 3.704059781739488e-05}, {"id": 386, "seek": 148304, "start": 1499.28, "end": 1501.76, "text": " And so it's similar and neither you have very much data, right?", "tokens": [400, 370, 309, 311, 2531, 293, 9662, 291, 362, 588, 709, 1412, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2013940290971236, "compression_ratio": 1.7044534412955465, "no_speech_prob": 3.704059781739488e-05}, {"id": 387, "seek": 148304, "start": 1501.76, "end": 1507.3999999999999, "text": " So you have to be able to generalize on only like a few thousand examples.", "tokens": [407, 291, 362, 281, 312, 1075, 281, 2674, 1125, 322, 787, 411, 257, 1326, 4714, 5110, 13], "temperature": 0.0, "avg_logprob": -0.2013940290971236, "compression_ratio": 1.7044534412955465, "no_speech_prob": 3.704059781739488e-05}, {"id": 388, "seek": 150740, "start": 1507.4, "end": 1514.16, "text": " So birthed base, which is the same size as open AI, it significantly beat open AI, which", "tokens": [407, 3965, 292, 3096, 11, 597, 307, 264, 912, 2744, 382, 1269, 7318, 11, 309, 10591, 4224, 1269, 7318, 11, 597], "temperature": 0.0, "avg_logprob": -0.2450386712310511, "compression_ratio": 1.8132780082987552, "no_speech_prob": 1.9823761249426752e-05}, {"id": 389, "seek": 150740, "start": 1514.16, "end": 1516.52, "text": " was the previous data they are.", "tokens": [390, 264, 3894, 1412, 436, 366, 13], "temperature": 0.0, "avg_logprob": -0.2450386712310511, "compression_ratio": 1.8132780082987552, "no_speech_prob": 1.9823761249426752e-05}, {"id": 390, "seek": 150740, "start": 1516.52, "end": 1522.92, "text": " And then the birth large, which was bigger, of course got better results, which is only", "tokens": [400, 550, 264, 3965, 2416, 11, 597, 390, 3801, 11, 295, 1164, 658, 1101, 3542, 11, 597, 307, 787], "temperature": 0.0, "avg_logprob": -0.2450386712310511, "compression_ratio": 1.8132780082987552, "no_speech_prob": 1.9823761249426752e-05}, {"id": 391, "seek": 150740, "start": 1522.92, "end": 1526.8400000000001, "text": " surprising that it got better results across the board, including on the very, very tiny", "tokens": [8830, 300, 309, 658, 1101, 3542, 2108, 264, 3150, 11, 3009, 322, 264, 588, 11, 588, 5870], "temperature": 0.0, "avg_logprob": -0.2450386712310511, "compression_ratio": 1.8132780082987552, "no_speech_prob": 1.9823761249426752e-05}, {"id": 392, "seek": 150740, "start": 1526.8400000000001, "end": 1528.8000000000002, "text": " data, that's only a few thousand examples.", "tokens": [1412, 11, 300, 311, 787, 257, 1326, 4714, 5110, 13], "temperature": 0.0, "avg_logprob": -0.2450386712310511, "compression_ratio": 1.8132780082987552, "no_speech_prob": 1.9823761249426752e-05}, {"id": 393, "seek": 150740, "start": 1528.8000000000002, "end": 1532.3600000000001, "text": " That's kind of the more interesting result rather than just the fact that it got better", "tokens": [663, 311, 733, 295, 264, 544, 1880, 1874, 2831, 813, 445, 264, 1186, 300, 309, 658, 1101], "temperature": 0.0, "avg_logprob": -0.2450386712310511, "compression_ratio": 1.8132780082987552, "no_speech_prob": 1.9823761249426752e-05}, {"id": 394, "seek": 150740, "start": 1532.3600000000001, "end": 1533.3600000000001, "text": " results.", "tokens": [3542, 13], "temperature": 0.0, "avg_logprob": -0.2450386712310511, "compression_ratio": 1.8132780082987552, "no_speech_prob": 1.9823761249426752e-05}, {"id": 395, "seek": 153336, "start": 1533.36, "end": 1539.84, "text": " Historically, when there is rules of thumb about, if you have some number of examples, how", "tokens": [25108, 984, 11, 562, 456, 307, 4474, 295, 9298, 466, 11, 498, 291, 362, 512, 1230, 295, 5110, 11, 577], "temperature": 0.0, "avg_logprob": -0.16427173369970077, "compression_ratio": 1.8509316770186335, "no_speech_prob": 5.736892489949241e-05}, {"id": 396, "seek": 153336, "start": 1539.84, "end": 1542.4799999999998, "text": " do you design the model size that's optimal for that?", "tokens": [360, 291, 1715, 264, 2316, 2744, 300, 311, 16252, 337, 300, 30], "temperature": 0.0, "avg_logprob": -0.16427173369970077, "compression_ratio": 1.8509316770186335, "no_speech_prob": 5.736892489949241e-05}, {"id": 397, "seek": 153336, "start": 1542.4799999999998, "end": 1545.9599999999998, "text": " And so if you don't do pre-training, like if you keep making the model bigger without", "tokens": [400, 370, 498, 291, 500, 380, 360, 659, 12, 17227, 1760, 11, 411, 498, 291, 1066, 1455, 264, 2316, 3801, 1553], "temperature": 0.0, "avg_logprob": -0.16427173369970077, "compression_ratio": 1.8509316770186335, "no_speech_prob": 5.736892489949241e-05}, {"id": 398, "seek": 153336, "start": 1545.9599999999998, "end": 1549.8799999999999, "text": " pre-training, eventually you'll get worse results because your model overfitting your", "tokens": [659, 12, 17227, 1760, 11, 4728, 291, 603, 483, 5324, 3542, 570, 428, 2316, 670, 69, 2414, 428], "temperature": 0.0, "avg_logprob": -0.16427173369970077, "compression_ratio": 1.8509316770186335, "no_speech_prob": 5.736892489949241e-05}, {"id": 399, "seek": 153336, "start": 1549.8799999999999, "end": 1551.32, "text": " training data.", "tokens": [3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16427173369970077, "compression_ratio": 1.8509316770186335, "no_speech_prob": 5.736892489949241e-05}, {"id": 400, "seek": 153336, "start": 1551.32, "end": 1554.4399999999998, "text": " With pre-training, you basically only ever do like one pass of the day anyways.", "tokens": [2022, 659, 12, 17227, 1760, 11, 291, 1936, 787, 1562, 360, 411, 472, 1320, 295, 264, 786, 13448, 13], "temperature": 0.0, "avg_logprob": -0.16427173369970077, "compression_ratio": 1.8509316770186335, "no_speech_prob": 5.736892489949241e-05}, {"id": 401, "seek": 153336, "start": 1554.4399999999998, "end": 1558.8, "text": " So there seems to be almost no limit to how big you can make it and still get good results", "tokens": [407, 456, 2544, 281, 312, 1920, 572, 4948, 281, 577, 955, 291, 393, 652, 309, 293, 920, 483, 665, 3542], "temperature": 0.0, "avg_logprob": -0.16427173369970077, "compression_ratio": 1.8509316770186335, "no_speech_prob": 5.736892489949241e-05}, {"id": 402, "seek": 153336, "start": 1558.8, "end": 1560.6799999999998, "text": " even with a tiny amount of fine-tuning data.", "tokens": [754, 365, 257, 5870, 2372, 295, 2489, 12, 83, 37726, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16427173369970077, "compression_ratio": 1.8509316770186335, "no_speech_prob": 5.736892489949241e-05}, {"id": 403, "seek": 153336, "start": 1560.6799999999998, "end": 1563.24, "text": " And that's really like one of the big takeaways.", "tokens": [400, 300, 311, 534, 411, 472, 295, 264, 955, 45584, 13], "temperature": 0.0, "avg_logprob": -0.16427173369970077, "compression_ratio": 1.8509316770186335, "no_speech_prob": 5.736892489949241e-05}, {"id": 404, "seek": 156324, "start": 1563.24, "end": 1572.84, "text": " So I'm not going to, yeah, so the reason why these numbers are, these range are lower", "tokens": [407, 286, 478, 406, 516, 281, 11, 1338, 11, 370, 264, 1778, 983, 613, 3547, 366, 11, 613, 3613, 366, 3126], "temperature": 0.0, "avg_logprob": -0.41643092317401237, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.00015838751278351992}, {"id": 405, "seek": 156324, "start": 1572.84, "end": 1579.52, "text": " is because I took the screenshot after, like this, you know, significantly after, when", "tokens": [307, 570, 286, 1890, 264, 27712, 934, 11, 411, 341, 11, 291, 458, 11, 10591, 934, 11, 562], "temperature": 0.0, "avg_logprob": -0.41643092317401237, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.00015838751278351992}, {"id": 406, "seek": 156324, "start": 1579.52, "end": 1582.08, "text": " other bunch of other people had submitted systems.", "tokens": [661, 3840, 295, 661, 561, 632, 14405, 3652, 13], "temperature": 0.0, "avg_logprob": -0.41643092317401237, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.00015838751278351992}, {"id": 407, "seek": 156324, "start": 1582.08, "end": 1586.1200000000001, "text": " But so this is a question and answer and get a set.", "tokens": [583, 370, 341, 307, 257, 1168, 293, 1867, 293, 483, 257, 992, 13], "temperature": 0.0, "avg_logprob": -0.41643092317401237, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.00015838751278351992}, {"id": 408, "seek": 156324, "start": 1586.1200000000001, "end": 1588.92, "text": " So it'd be like, what action did the US take to start this second oil shock?", "tokens": [407, 309, 1116, 312, 411, 11, 437, 3069, 630, 264, 2546, 747, 281, 722, 341, 1150, 3184, 5588, 30], "temperature": 0.0, "avg_logprob": -0.41643092317401237, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.00015838751278351992}, {"id": 409, "seek": 156324, "start": 1588.92, "end": 1590.96, "text": " So in this case, there's no answer, right?", "tokens": [407, 294, 341, 1389, 11, 456, 311, 572, 1867, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.41643092317401237, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.00015838751278351992}, {"id": 410, "seek": 159096, "start": 1590.96, "end": 1593.68, "text": " So it's something you have to be able to predict is that there's no answer in this phase.", "tokens": [407, 309, 311, 746, 291, 362, 281, 312, 1075, 281, 6069, 307, 300, 456, 311, 572, 1867, 294, 341, 5574, 13], "temperature": 0.0, "avg_logprob": -0.2275138141439973, "compression_ratio": 1.8860294117647058, "no_speech_prob": 4.132865069550462e-05}, {"id": 411, "seek": 159096, "start": 1593.68, "end": 1596.24, "text": " If you have to be able to predict the answer or say there's no answer.", "tokens": [759, 291, 362, 281, 312, 1075, 281, 6069, 264, 1867, 420, 584, 456, 311, 572, 1867, 13], "temperature": 0.0, "avg_logprob": -0.2275138141439973, "compression_ratio": 1.8860294117647058, "no_speech_prob": 4.132865069550462e-05}, {"id": 412, "seek": 159096, "start": 1596.24, "end": 1602.48, "text": " So burp beat the previous state of the art, at the time that it was submitted by about", "tokens": [407, 2779, 79, 4224, 264, 3894, 1785, 295, 264, 1523, 11, 412, 264, 565, 300, 309, 390, 14405, 538, 466], "temperature": 0.0, "avg_logprob": -0.2275138141439973, "compression_ratio": 1.8860294117647058, "no_speech_prob": 4.132865069550462e-05}, {"id": 413, "seek": 159096, "start": 1602.48, "end": 1604.52, "text": " six points, which was a pretty big gain.", "tokens": [2309, 2793, 11, 597, 390, 257, 1238, 955, 6052, 13], "temperature": 0.0, "avg_logprob": -0.2275138141439973, "compression_ratio": 1.8860294117647058, "no_speech_prob": 4.132865069550462e-05}, {"id": 414, "seek": 159096, "start": 1604.52, "end": 1607.76, "text": " Now this has kind of gone past human level.", "tokens": [823, 341, 575, 733, 295, 2780, 1791, 1952, 1496, 13], "temperature": 0.0, "avg_logprob": -0.2275138141439973, "compression_ratio": 1.8860294117647058, "no_speech_prob": 4.132865069550462e-05}, {"id": 415, "seek": 159096, "start": 1607.76, "end": 1610.56, "text": " But at the time, yeah, it was large.", "tokens": [583, 412, 264, 565, 11, 1338, 11, 309, 390, 2416, 13], "temperature": 0.0, "avg_logprob": -0.2275138141439973, "compression_ratio": 1.8860294117647058, "no_speech_prob": 4.132865069550462e-05}, {"id": 416, "seek": 159096, "start": 1610.56, "end": 1614.28, "text": " So I'll kind of do some ablation experiments or go through some ablation experiments.", "tokens": [407, 286, 603, 733, 295, 360, 512, 410, 24278, 12050, 420, 352, 807, 512, 410, 24278, 12050, 13], "temperature": 0.0, "avg_logprob": -0.2275138141439973, "compression_ratio": 1.8860294117647058, "no_speech_prob": 4.132865069550462e-05}, {"id": 417, "seek": 159096, "start": 1614.28, "end": 1617.8, "text": " So this one, there's four things that I'm comparing here.", "tokens": [407, 341, 472, 11, 456, 311, 1451, 721, 300, 286, 478, 15763, 510, 13], "temperature": 0.0, "avg_logprob": -0.2275138141439973, "compression_ratio": 1.8860294117647058, "no_speech_prob": 4.132865069550462e-05}, {"id": 418, "seek": 161780, "start": 1617.8, "end": 1621.44, "text": " So this is all burp based size models.", "tokens": [407, 341, 307, 439, 2779, 79, 2361, 2744, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17758737530624658, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.078192301269155e-05}, {"id": 419, "seek": 161780, "start": 1621.44, "end": 1625.8799999999999, "text": " So the blue line is kind of the burp based model.", "tokens": [407, 264, 3344, 1622, 307, 733, 295, 264, 2779, 79, 2361, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17758737530624658, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.078192301269155e-05}, {"id": 420, "seek": 161780, "start": 1625.8799999999999, "end": 1629.12, "text": " The red line is, with we take out the next sentence prediction.", "tokens": [440, 2182, 1622, 307, 11, 365, 321, 747, 484, 264, 958, 8174, 17630, 13], "temperature": 0.0, "avg_logprob": -0.17758737530624658, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.078192301269155e-05}, {"id": 421, "seek": 161780, "start": 1629.12, "end": 1632.68, "text": " So in our case, even though people have subsequently said that they don't think it's important,", "tokens": [407, 294, 527, 1389, 11, 754, 1673, 561, 362, 26514, 848, 300, 436, 500, 380, 519, 309, 311, 1021, 11], "temperature": 0.0, "avg_logprob": -0.17758737530624658, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.078192301269155e-05}, {"id": 422, "seek": 161780, "start": 1632.68, "end": 1633.8799999999999, "text": " in our case, we actually did measure it.", "tokens": [294, 527, 1389, 11, 321, 767, 630, 3481, 309, 13], "temperature": 0.0, "avg_logprob": -0.17758737530624658, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.078192301269155e-05}, {"id": 423, "seek": 161780, "start": 1633.8799999999999, "end": 1639.44, "text": " And it turns out it seemed like it was important to have this next sentence prediction task,", "tokens": [400, 309, 4523, 484, 309, 6576, 411, 309, 390, 1021, 281, 362, 341, 958, 8174, 17630, 5633, 11], "temperature": 0.0, "avg_logprob": -0.17758737530624658, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.078192301269155e-05}, {"id": 424, "seek": 161780, "start": 1639.44, "end": 1645.48, "text": " especially for kind of question answering task, which is this one.", "tokens": [2318, 337, 733, 295, 1168, 13430, 5633, 11, 597, 307, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.17758737530624658, "compression_ratio": 1.7677165354330708, "no_speech_prob": 2.078192301269155e-05}, {"id": 425, "seek": 164548, "start": 1645.48, "end": 1648.68, "text": " But it seems to have a little bit, at least in all four of them, to have it.", "tokens": [583, 309, 2544, 281, 362, 257, 707, 857, 11, 412, 1935, 294, 439, 1451, 295, 552, 11, 281, 362, 309, 13], "temperature": 0.0, "avg_logprob": -0.31157927079634234, "compression_ratio": 1.652014652014652, "no_speech_prob": 6.707580178044736e-05}, {"id": 426, "seek": 164548, "start": 1648.68, "end": 1652.48, "text": " So this kind of does indicate that there's some strength in learning some model that learns", "tokens": [407, 341, 733, 295, 775, 13330, 300, 456, 311, 512, 3800, 294, 2539, 512, 2316, 300, 27152], "temperature": 0.0, "avg_logprob": -0.31157927079634234, "compression_ratio": 1.652014652014652, "no_speech_prob": 6.707580178044736e-05}, {"id": 427, "seek": 164548, "start": 1652.48, "end": 1657.6, "text": " a relationship between sentences.", "tokens": [257, 2480, 1296, 16579, 13], "temperature": 0.0, "avg_logprob": -0.31157927079634234, "compression_ratio": 1.652014652014652, "no_speech_prob": 6.707580178044736e-05}, {"id": 428, "seek": 164548, "start": 1657.6, "end": 1665.24, "text": " So then this one is the one that makes an apples to apples comparison between open AI and", "tokens": [407, 550, 341, 472, 307, 264, 472, 300, 1669, 364, 16814, 281, 16814, 9660, 1296, 1269, 7318, 293], "temperature": 0.0, "avg_logprob": -0.31157927079634234, "compression_ratio": 1.652014652014652, "no_speech_prob": 6.707580178044736e-05}, {"id": 429, "seek": 164548, "start": 1665.24, "end": 1668.04, "text": " open AI is GPT1 and BERT, right?", "tokens": [1269, 7318, 307, 26039, 51, 16, 293, 363, 31479, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.31157927079634234, "compression_ratio": 1.652014652014652, "no_speech_prob": 6.707580178044736e-05}, {"id": 430, "seek": 164548, "start": 1668.04, "end": 1670.92, "text": " Because I also, I made the model bigger, but not per base.", "tokens": [1436, 286, 611, 11, 286, 1027, 264, 2316, 3801, 11, 457, 406, 680, 3096, 13], "temperature": 0.0, "avg_logprob": -0.31157927079634234, "compression_ratio": 1.652014652014652, "no_speech_prob": 6.707580178044736e-05}, {"id": 431, "seek": 164548, "start": 1670.92, "end": 1673.48, "text": " So BERT based was the exact same size, but it was China more data.", "tokens": [407, 363, 31479, 2361, 390, 264, 1900, 912, 2744, 11, 457, 309, 390, 3533, 544, 1412, 13], "temperature": 0.0, "avg_logprob": -0.31157927079634234, "compression_ratio": 1.652014652014652, "no_speech_prob": 6.707580178044736e-05}, {"id": 432, "seek": 167348, "start": 1673.48, "end": 1678.56, "text": " So to make it a fair comparison, I basically retrained my own implementation of open AI's", "tokens": [407, 281, 652, 309, 257, 3143, 9660, 11, 286, 1936, 1533, 31774, 452, 1065, 11420, 295, 1269, 7318, 311], "temperature": 0.0, "avg_logprob": -0.2997798728942871, "compression_ratio": 1.448888888888889, "no_speech_prob": 4.754042674903758e-05}, {"id": 433, "seek": 167348, "start": 1678.56, "end": 1684.24, "text": " GPT1, which is a yellow line.", "tokens": [26039, 51, 16, 11, 597, 307, 257, 5566, 1622, 13], "temperature": 0.0, "avg_logprob": -0.2997798728942871, "compression_ratio": 1.448888888888889, "no_speech_prob": 4.754042674903758e-05}, {"id": 434, "seek": 167348, "start": 1684.24, "end": 1688.0, "text": " And we can see that on some of the tests, it's not that far, although this is actually a", "tokens": [400, 321, 393, 536, 300, 322, 512, 295, 264, 6921, 11, 309, 311, 406, 300, 1400, 11, 4878, 341, 307, 767, 257], "temperature": 0.0, "avg_logprob": -0.2997798728942871, "compression_ratio": 1.448888888888889, "no_speech_prob": 4.754042674903758e-05}, {"id": 435, "seek": 167348, "start": 1688.0, "end": 1689.0, "text": " pretty big gap.", "tokens": [1238, 955, 7417, 13], "temperature": 0.0, "avg_logprob": -0.2997798728942871, "compression_ratio": 1.448888888888889, "no_speech_prob": 4.754042674903758e-05}, {"id": 436, "seek": 167348, "start": 1689.0, "end": 1692.6, "text": " This drop is four points, which is a lot.", "tokens": [639, 3270, 307, 1451, 2793, 11, 597, 307, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.2997798728942871, "compression_ratio": 1.448888888888889, "no_speech_prob": 4.754042674903758e-05}, {"id": 437, "seek": 167348, "start": 1692.6, "end": 1696.44, "text": " But on some tasks like squad and M.O.P.C, it was way worse.", "tokens": [583, 322, 512, 9608, 411, 15310, 293, 376, 13, 46, 13, 47, 13, 34, 11, 309, 390, 636, 5324, 13], "temperature": 0.0, "avg_logprob": -0.2997798728942871, "compression_ratio": 1.448888888888889, "no_speech_prob": 4.754042674903758e-05}, {"id": 438, "seek": 169644, "start": 1696.44, "end": 1704.44, "text": " And so for squad it makes sense because squad is a labeling, is a span labeling task.", "tokens": [400, 370, 337, 15310, 309, 1669, 2020, 570, 15310, 307, 257, 40244, 11, 307, 257, 16174, 40244, 5633, 13], "temperature": 0.0, "avg_logprob": -0.16252080055132304, "compression_ratio": 1.8419243986254294, "no_speech_prob": 1.8339836969971657e-05}, {"id": 439, "seek": 169644, "start": 1704.44, "end": 1709.48, "text": " And so if you only have left context, then words at the beginning have basically no context.", "tokens": [400, 370, 498, 291, 787, 362, 1411, 4319, 11, 550, 2283, 412, 264, 2863, 362, 1936, 572, 4319, 13], "temperature": 0.0, "avg_logprob": -0.16252080055132304, "compression_ratio": 1.8419243986254294, "no_speech_prob": 1.8339836969971657e-05}, {"id": 440, "seek": 169644, "start": 1709.48, "end": 1712.88, "text": " And so you're asking it to do span labeling on words with almost no context.", "tokens": [400, 370, 291, 434, 3365, 309, 281, 360, 16174, 40244, 322, 2283, 365, 1920, 572, 4319, 13], "temperature": 0.0, "avg_logprob": -0.16252080055132304, "compression_ratio": 1.8419243986254294, "no_speech_prob": 1.8339836969971657e-05}, {"id": 441, "seek": 169644, "start": 1712.88, "end": 1715.1200000000001, "text": " So it really doesn't make any sense.", "tokens": [407, 309, 534, 1177, 380, 652, 604, 2020, 13], "temperature": 0.0, "avg_logprob": -0.16252080055132304, "compression_ratio": 1.8419243986254294, "no_speech_prob": 1.8339836969971657e-05}, {"id": 442, "seek": 169644, "start": 1715.1200000000001, "end": 1716.72, "text": " And so of course it's going to do much worse.", "tokens": [400, 370, 295, 1164, 309, 311, 516, 281, 360, 709, 5324, 13], "temperature": 0.0, "avg_logprob": -0.16252080055132304, "compression_ratio": 1.8419243986254294, "no_speech_prob": 1.8339836969971657e-05}, {"id": 443, "seek": 169644, "start": 1716.72, "end": 1720.8400000000001, "text": " So then we also added a, to make it fair, we also added an LSTM on top of it, which is", "tokens": [407, 550, 321, 611, 3869, 257, 11, 281, 652, 309, 3143, 11, 321, 611, 3869, 364, 441, 6840, 44, 322, 1192, 295, 309, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.16252080055132304, "compression_ratio": 1.8419243986254294, "no_speech_prob": 1.8339836969971657e-05}, {"id": 444, "seek": 169644, "start": 1720.8400000000001, "end": 1722.04, "text": " trained from scratch.", "tokens": [8895, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.16252080055132304, "compression_ratio": 1.8419243986254294, "no_speech_prob": 1.8339836969971657e-05}, {"id": 445, "seek": 169644, "start": 1722.04, "end": 1725.76, "text": " And this does help a little bit on some of the tasks, but on other ones it doesn't help.", "tokens": [400, 341, 775, 854, 257, 707, 857, 322, 512, 295, 264, 9608, 11, 457, 322, 661, 2306, 309, 1177, 380, 854, 13], "temperature": 0.0, "avg_logprob": -0.16252080055132304, "compression_ratio": 1.8419243986254294, "no_speech_prob": 1.8339836969971657e-05}, {"id": 446, "seek": 172576, "start": 1725.76, "end": 1728.8, "text": " So on SWAT it helps because now you have bidirectional context.", "tokens": [407, 322, 20346, 2218, 309, 3665, 570, 586, 291, 362, 12957, 621, 41048, 4319, 13], "temperature": 0.0, "avg_logprob": -0.24712124992819393, "compression_ratio": 1.7061068702290076, "no_speech_prob": 3.533941708155908e-05}, {"id": 447, "seek": 172576, "start": 1728.8, "end": 1732.68, "text": " But on MRC because it's a very small task, it's only got 3,000 labeled examples, it doesn't", "tokens": [583, 322, 9808, 34, 570, 309, 311, 257, 588, 1359, 5633, 11, 309, 311, 787, 658, 805, 11, 1360, 21335, 5110, 11, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.24712124992819393, "compression_ratio": 1.7061068702290076, "no_speech_prob": 3.533941708155908e-05}, {"id": 448, "seek": 172576, "start": 1732.68, "end": 1734.48, "text": " help at all.", "tokens": [854, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.24712124992819393, "compression_ratio": 1.7061068702290076, "no_speech_prob": 3.533941708155908e-05}, {"id": 449, "seek": 172576, "start": 1734.48, "end": 1740.24, "text": " So it does show that kind of the mass language model and the next inspection are both important,", "tokens": [407, 309, 775, 855, 300, 733, 295, 264, 2758, 2856, 2316, 293, 264, 958, 22085, 366, 1293, 1021, 11], "temperature": 0.0, "avg_logprob": -0.24712124992819393, "compression_ratio": 1.7061068702290076, "no_speech_prob": 3.533941708155908e-05}, {"id": 450, "seek": 172576, "start": 1740.24, "end": 1744.12, "text": " especially the mass language model.", "tokens": [2318, 264, 2758, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.24712124992819393, "compression_ratio": 1.7061068702290076, "no_speech_prob": 3.533941708155908e-05}, {"id": 451, "seek": 172576, "start": 1744.12, "end": 1751.44, "text": " So the other thing, one of the other ablations is that when we apply the mass language model,", "tokens": [407, 264, 661, 551, 11, 472, 295, 264, 661, 410, 75, 763, 307, 300, 562, 321, 3079, 264, 2758, 2856, 2316, 11], "temperature": 0.0, "avg_logprob": -0.24712124992819393, "compression_ratio": 1.7061068702290076, "no_speech_prob": 3.533941708155908e-05}, {"id": 452, "seek": 172576, "start": 1751.44, "end": 1755.2, "text": " we're only predicting 15% of words in the sentence.", "tokens": [321, 434, 787, 32884, 2119, 4, 295, 2283, 294, 264, 8174, 13], "temperature": 0.0, "avg_logprob": -0.24712124992819393, "compression_ratio": 1.7061068702290076, "no_speech_prob": 3.533941708155908e-05}, {"id": 453, "seek": 175520, "start": 1755.2, "end": 1757.68, "text": " So when you do a left to right language model, you're predicting every single word,", "tokens": [407, 562, 291, 360, 257, 1411, 281, 558, 2856, 2316, 11, 291, 434, 32884, 633, 2167, 1349, 11], "temperature": 0.0, "avg_logprob": -0.16805450951875145, "compression_ratio": 1.8006644518272426, "no_speech_prob": 6.915126141393557e-05}, {"id": 454, "seek": 175520, "start": 1757.68, "end": 1760.04, "text": " conditioning all the words to the left.", "tokens": [21901, 439, 264, 2283, 281, 264, 1411, 13], "temperature": 0.0, "avg_logprob": -0.16805450951875145, "compression_ratio": 1.8006644518272426, "no_speech_prob": 6.915126141393557e-05}, {"id": 455, "seek": 175520, "start": 1760.04, "end": 1764.1200000000001, "text": " So one question might be how much does this make it take longer to converge?", "tokens": [407, 472, 1168, 1062, 312, 577, 709, 775, 341, 652, 309, 747, 2854, 281, 41881, 30], "temperature": 0.0, "avg_logprob": -0.16805450951875145, "compression_ratio": 1.8006644518272426, "no_speech_prob": 6.915126141393557e-05}, {"id": 456, "seek": 175520, "start": 1764.1200000000001, "end": 1767.3600000000001, "text": " Even though eventually we know that it converges at a much better point, if you have a limited", "tokens": [2754, 1673, 4728, 321, 458, 300, 309, 9652, 2880, 412, 257, 709, 1101, 935, 11, 498, 291, 362, 257, 5567], "temperature": 0.0, "avg_logprob": -0.16805450951875145, "compression_ratio": 1.8006644518272426, "no_speech_prob": 6.915126141393557e-05}, {"id": 457, "seek": 175520, "start": 1767.3600000000001, "end": 1770.3600000000001, "text": " training budget, it's a better to do a left to right model.", "tokens": [3097, 4706, 11, 309, 311, 257, 1101, 281, 360, 257, 1411, 281, 558, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16805450951875145, "compression_ratio": 1.8006644518272426, "no_speech_prob": 6.915126141393557e-05}, {"id": 458, "seek": 175520, "start": 1770.3600000000001, "end": 1777.0800000000002, "text": " And so we see that when you do this mass language model, the bidirectionality starts to improve,", "tokens": [400, 370, 321, 536, 300, 562, 291, 360, 341, 2758, 2856, 2316, 11, 264, 12957, 621, 41048, 507, 3719, 281, 3470, 11], "temperature": 0.0, "avg_logprob": -0.16805450951875145, "compression_ratio": 1.8006644518272426, "no_speech_prob": 6.915126141393557e-05}, {"id": 459, "seek": 175520, "start": 1777.0800000000002, "end": 1779.96, "text": " like at the very, very beginning because you're doing so many more predictions, it's true", "tokens": [411, 412, 264, 588, 11, 588, 2863, 570, 291, 434, 884, 370, 867, 544, 21264, 11, 309, 311, 2074], "temperature": 0.0, "avg_logprob": -0.16805450951875145, "compression_ratio": 1.8006644518272426, "no_speech_prob": 6.915126141393557e-05}, {"id": 460, "seek": 177996, "start": 1779.96, "end": 1786.0, "text": " that the left to right model does do better at the very, like for, like, epoch one, but", "tokens": [300, 264, 1411, 281, 558, 2316, 775, 360, 1101, 412, 264, 588, 11, 411, 337, 11, 411, 11, 30992, 339, 472, 11, 457], "temperature": 0.0, "avg_logprob": -0.21956964718398228, "compression_ratio": 1.7115384615384615, "no_speech_prob": 2.2815525881014764e-05}, {"id": 461, "seek": 177996, "start": 1786.0, "end": 1790.32, "text": " then very soon after because the bidirectionality is so important, it starts to take over.", "tokens": [550, 588, 2321, 934, 570, 264, 12957, 621, 41048, 507, 307, 370, 1021, 11, 309, 3719, 281, 747, 670, 13], "temperature": 0.0, "avg_logprob": -0.21956964718398228, "compression_ratio": 1.7115384615384615, "no_speech_prob": 2.2815525881014764e-05}, {"id": 462, "seek": 177996, "start": 1790.32, "end": 1793.96, "text": " And so it's basically better from almost the start to do bidirectionality.", "tokens": [400, 370, 309, 311, 1936, 1101, 490, 1920, 264, 722, 281, 360, 12957, 621, 41048, 507, 13], "temperature": 0.0, "avg_logprob": -0.21956964718398228, "compression_ratio": 1.7115384615384615, "no_speech_prob": 2.2815525881014764e-05}, {"id": 463, "seek": 177996, "start": 1793.96, "end": 1797.64, "text": " And then it takes slightly longer to converge, but the overall convergence is, of course, much", "tokens": [400, 550, 309, 2516, 4748, 2854, 281, 41881, 11, 457, 264, 4787, 32181, 307, 11, 295, 1164, 11, 709], "temperature": 0.0, "avg_logprob": -0.21956964718398228, "compression_ratio": 1.7115384615384615, "no_speech_prob": 2.2815525881014764e-05}, {"id": 464, "seek": 177996, "start": 1797.64, "end": 1801.3600000000001, "text": " higher.", "tokens": [2946, 13], "temperature": 0.0, "avg_logprob": -0.21956964718398228, "compression_ratio": 1.7115384615384615, "no_speech_prob": 2.2815525881014764e-05}, {"id": 465, "seek": 180136, "start": 1801.36, "end": 1810.6, "text": " And then finally for this oblations, we can see that going from a smaller model, which", "tokens": [400, 550, 2721, 337, 341, 23740, 763, 11, 321, 393, 536, 300, 516, 490, 257, 4356, 2316, 11, 597], "temperature": 0.0, "avg_logprob": -0.1777304398386102, "compression_ratio": 1.7056277056277056, "no_speech_prob": 4.709573204308981e-06}, {"id": 466, "seek": 180136, "start": 1810.6, "end": 1814.52, "text": " was 100 million to 300 million parameters, helps a lot, which isn't surprising.", "tokens": [390, 2319, 2459, 281, 6641, 2459, 9834, 11, 3665, 257, 688, 11, 597, 1943, 380, 8830, 13], "temperature": 0.0, "avg_logprob": -0.1777304398386102, "compression_ratio": 1.7056277056277056, "no_speech_prob": 4.709573204308981e-06}, {"id": 467, "seek": 180136, "start": 1814.52, "end": 1820.9199999999998, "text": " What the more surprising thing is that one of these curves, these aren't comparable,", "tokens": [708, 264, 544, 8830, 551, 307, 300, 472, 295, 613, 19490, 11, 613, 3212, 380, 25323, 11], "temperature": 0.0, "avg_logprob": -0.1777304398386102, "compression_ratio": 1.7056277056277056, "no_speech_prob": 4.709573204308981e-06}, {"id": 468, "seek": 180136, "start": 1820.9199999999998, "end": 1822.24, "text": " you shouldn't compare the curves to each other.", "tokens": [291, 4659, 380, 6794, 264, 19490, 281, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.1777304398386102, "compression_ratio": 1.7056277056277056, "no_speech_prob": 4.709573204308981e-06}, {"id": 469, "seek": 180136, "start": 1822.24, "end": 1829.08, "text": " The point is to look at the curves as a function of the number of parameters and see that this", "tokens": [440, 935, 307, 281, 574, 412, 264, 19490, 382, 257, 2445, 295, 264, 1230, 295, 9834, 293, 536, 300, 341], "temperature": 0.0, "avg_logprob": -0.1777304398386102, "compression_ratio": 1.7056277056277056, "no_speech_prob": 4.709573204308981e-06}, {"id": 470, "seek": 182908, "start": 1829.08, "end": 1835.8, "text": " one is, this one only has 3,000 labeled examples, and this one has 4,000 labeled examples.", "tokens": [472, 307, 11, 341, 472, 787, 575, 805, 11, 1360, 21335, 5110, 11, 293, 341, 472, 575, 1017, 11, 1360, 21335, 5110, 13], "temperature": 0.0, "avg_logprob": -0.2017439664420435, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.0001123056499636732}, {"id": 471, "seek": 182908, "start": 1835.8, "end": 1841.9199999999998, "text": " So in both cases, the curves look very similar, which is surprising because the rule of thumb", "tokens": [407, 294, 1293, 3331, 11, 264, 19490, 574, 588, 2531, 11, 597, 307, 8830, 570, 264, 4978, 295, 9298], "temperature": 0.0, "avg_logprob": -0.2017439664420435, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.0001123056499636732}, {"id": 472, "seek": 182908, "start": 1841.9199999999998, "end": 1846.72, "text": " that you're going to overfit your data if you only have a few labeled examples turns", "tokens": [300, 291, 434, 516, 281, 670, 6845, 428, 1412, 498, 291, 787, 362, 257, 1326, 21335, 5110, 4523], "temperature": 0.0, "avg_logprob": -0.2017439664420435, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.0001123056499636732}, {"id": 473, "seek": 182908, "start": 1846.72, "end": 1848.24, "text": " out not to really be true anymore.", "tokens": [484, 406, 281, 534, 312, 2074, 3602, 13], "temperature": 0.0, "avg_logprob": -0.2017439664420435, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.0001123056499636732}, {"id": 474, "seek": 182908, "start": 1848.24, "end": 1851.8799999999999, "text": " And there's, you know, in these curves keep going up, right?", "tokens": [400, 456, 311, 11, 291, 458, 11, 294, 613, 19490, 1066, 516, 493, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2017439664420435, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.0001123056499636732}, {"id": 475, "seek": 182908, "start": 1851.8799999999999, "end": 1856.3999999999999, "text": " So now with subsequent papers, we'll talk about, like this, this big one was 300 million", "tokens": [407, 586, 365, 19962, 10577, 11, 321, 603, 751, 466, 11, 411, 341, 11, 341, 955, 472, 390, 6641, 2459], "temperature": 0.0, "avg_logprob": -0.2017439664420435, "compression_ratio": 1.6814814814814816, "no_speech_prob": 0.0001123056499636732}, {"id": 476, "seek": 185640, "start": 1856.4, "end": 1859.96, "text": " parameters, people have gone up to 11 billion parameters and still seeing similar behaviors.", "tokens": [9834, 11, 561, 362, 2780, 493, 281, 2975, 5218, 9834, 293, 920, 2577, 2531, 15501, 13], "temperature": 0.0, "avg_logprob": -0.26017234364493946, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.586076127248816e-05}, {"id": 477, "seek": 185640, "start": 1859.96, "end": 1863.2, "text": " So still seeing the curves go way up and gotten to the other results, which is kind of crazy", "tokens": [407, 920, 2577, 264, 19490, 352, 636, 493, 293, 5768, 281, 264, 661, 3542, 11, 597, 307, 733, 295, 3219], "temperature": 0.0, "avg_logprob": -0.26017234364493946, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.586076127248816e-05}, {"id": 478, "seek": 185640, "start": 1863.2, "end": 1869.0400000000002, "text": " because now we know that, you know, basically there's almost no limit.", "tokens": [570, 586, 321, 458, 300, 11, 291, 458, 11, 1936, 456, 311, 1920, 572, 4948, 13], "temperature": 0.0, "avg_logprob": -0.26017234364493946, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.586076127248816e-05}, {"id": 479, "seek": 185640, "start": 1869.0400000000002, "end": 1873.44, "text": " So another thing I want to talk about before I talk about stuff that's happened since", "tokens": [407, 1071, 551, 286, 528, 281, 751, 466, 949, 286, 751, 466, 1507, 300, 311, 2011, 1670], "temperature": 0.0, "avg_logprob": -0.26017234364493946, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.586076127248816e-05}, {"id": 480, "seek": 185640, "start": 1873.44, "end": 1874.44, "text": " Bert.", "tokens": [29594, 13], "temperature": 0.0, "avg_logprob": -0.26017234364493946, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.586076127248816e-05}, {"id": 481, "seek": 185640, "start": 1874.44, "end": 1882.44, "text": " Is the kind of, is, even though Bert itself was in some ways very simple, which is, you", "tokens": [1119, 264, 733, 295, 11, 307, 11, 754, 1673, 29594, 2564, 390, 294, 512, 2098, 588, 2199, 11, 597, 307, 11, 291], "temperature": 0.0, "avg_logprob": -0.26017234364493946, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.586076127248816e-05}, {"id": 482, "seek": 185640, "start": 1882.44, "end": 1886.24, "text": " know, not a bad thing, it was very successful immediately.", "tokens": [458, 11, 406, 257, 1578, 551, 11, 309, 390, 588, 4406, 4258, 13], "temperature": 0.0, "avg_logprob": -0.26017234364493946, "compression_ratio": 1.706896551724138, "no_speech_prob": 3.586076127248816e-05}, {"id": 483, "seek": 188624, "start": 1886.24, "end": 1889.1200000000001, "text": " And you know, part of that is the Google brand and like, you know, it got a cute name", "tokens": [400, 291, 458, 11, 644, 295, 300, 307, 264, 3329, 3360, 293, 411, 11, 291, 458, 11, 309, 658, 257, 4052, 1315], "temperature": 0.0, "avg_logprob": -0.24435654239378113, "compression_ratio": 1.8071895424836601, "no_speech_prob": 7.011331763351336e-05}, {"id": 484, "seek": 188624, "start": 1889.1200000000001, "end": 1890.1200000000001, "text": " and stuff like that.", "tokens": [293, 1507, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.24435654239378113, "compression_ratio": 1.8071895424836601, "no_speech_prob": 7.011331763351336e-05}, {"id": 485, "seek": 188624, "start": 1890.1200000000001, "end": 1892.52, "text": " But I think that I spent a lot of time with the open social release and particularly", "tokens": [583, 286, 519, 300, 286, 4418, 257, 688, 295, 565, 365, 264, 1269, 2093, 4374, 293, 4098], "temperature": 0.0, "avg_logprob": -0.24435654239378113, "compression_ratio": 1.8071895424836601, "no_speech_prob": 7.011331763351336e-05}, {"id": 486, "seek": 188624, "start": 1892.52, "end": 1896.44, "text": " looking at other open source releases and figuring out what people didn't like about", "tokens": [1237, 412, 661, 1269, 4009, 16952, 293, 15213, 484, 437, 561, 994, 380, 411, 466], "temperature": 0.0, "avg_logprob": -0.24435654239378113, "compression_ratio": 1.8071895424836601, "no_speech_prob": 7.011331763351336e-05}, {"id": 487, "seek": 188624, "start": 1896.44, "end": 1898.64, "text": " those.", "tokens": [729, 13], "temperature": 0.0, "avg_logprob": -0.24435654239378113, "compression_ratio": 1.8071895424836601, "no_speech_prob": 7.011331763351336e-05}, {"id": 488, "seek": 188624, "start": 1898.64, "end": 1902.44, "text": " And so I think this is important, like when you're, when you're a PhD student or even", "tokens": [400, 370, 286, 519, 341, 307, 1021, 11, 411, 562, 291, 434, 11, 562, 291, 434, 257, 14476, 3107, 420, 754], "temperature": 0.0, "avg_logprob": -0.24435654239378113, "compression_ratio": 1.8071895424836601, "no_speech_prob": 7.011331763351336e-05}, {"id": 489, "seek": 188624, "start": 1902.44, "end": 1905.28, "text": " working industry as a, and trying to release something.", "tokens": [1364, 3518, 382, 257, 11, 293, 1382, 281, 4374, 746, 13], "temperature": 0.0, "avg_logprob": -0.24435654239378113, "compression_ratio": 1.8071895424836601, "no_speech_prob": 7.011331763351336e-05}, {"id": 490, "seek": 188624, "start": 1905.28, "end": 1910.92, "text": " So I kind of just listed the things here that I thought were important for like why", "tokens": [407, 286, 733, 295, 445, 10052, 264, 721, 510, 300, 286, 1194, 645, 1021, 337, 411, 983], "temperature": 0.0, "avg_logprob": -0.24435654239378113, "compression_ratio": 1.8071895424836601, "no_speech_prob": 7.011331763351336e-05}, {"id": 491, "seek": 188624, "start": 1910.92, "end": 1915.64, "text": " it was successful compared to other things.", "tokens": [309, 390, 4406, 5347, 281, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.24435654239378113, "compression_ratio": 1.8071895424836601, "no_speech_prob": 7.011331763351336e-05}, {"id": 492, "seek": 191564, "start": 1915.64, "end": 1918.4, "text": " So like, not, I'm not trying to call them out just to be mean because it, but like the", "tokens": [407, 411, 11, 406, 11, 286, 478, 406, 1382, 281, 818, 552, 484, 445, 281, 312, 914, 570, 309, 11, 457, 411, 264], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 493, "seek": 191564, "start": 1918.4, "end": 1921.4, "text": " open AI GPT-1 release was really, was not very good.", "tokens": [1269, 7318, 26039, 51, 12, 16, 4374, 390, 534, 11, 390, 406, 588, 665, 13], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 494, "seek": 191564, "start": 1921.4, "end": 1924.88, "text": " And then like I'm sure that they were this because the open AI GPT-2 release was very", "tokens": [400, 550, 411, 286, 478, 988, 300, 436, 645, 341, 570, 264, 1269, 7318, 26039, 51, 12, 17, 4374, 390, 588], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 495, "seek": 191564, "start": 1924.88, "end": 1925.88, "text": " good.", "tokens": [665, 13], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 496, "seek": 191564, "start": 1925.88, "end": 1931.4, "text": " And so, yeah, because it was very hard to run and there was not comment that the TensorFlow", "tokens": [400, 370, 11, 1338, 11, 570, 309, 390, 588, 1152, 281, 1190, 293, 456, 390, 406, 2871, 300, 264, 37624], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 497, "seek": 191564, "start": 1931.4, "end": 1934.48, "text": " code was very, like it worked fine, like I replicated it.", "tokens": [3089, 390, 588, 11, 411, 309, 2732, 2489, 11, 411, 286, 46365, 309, 13], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 498, "seek": 191564, "start": 1934.48, "end": 1937.68, "text": " But like, the, the TensorFlow code was very non-idiamatic.", "tokens": [583, 411, 11, 264, 11, 264, 37624, 3089, 390, 588, 2107, 12, 327, 2918, 2399, 13], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 499, "seek": 191564, "start": 1937.68, "end": 1938.96, "text": " It used all sorts of weird stuff.", "tokens": [467, 1143, 439, 7527, 295, 3657, 1507, 13], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 500, "seek": 191564, "start": 1938.96, "end": 1939.96, "text": " The Python code was weird.", "tokens": [440, 15329, 3089, 390, 3657, 13], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 501, "seek": 191564, "start": 1939.96, "end": 1940.96, "text": " There was no comments.", "tokens": [821, 390, 572, 3053, 13], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 502, "seek": 191564, "start": 1940.96, "end": 1941.88, "text": " There was basically no instructions.", "tokens": [821, 390, 1936, 572, 9415, 13], "temperature": 0.0, "avg_logprob": -0.2611177206039429, "compression_ratio": 1.9479166666666667, "no_speech_prob": 9.167192911263555e-05}, {"id": 503, "seek": 194188, "start": 1941.88, "end": 1946.3200000000002, "text": " And then other code bases also are kind of too big.", "tokens": [400, 550, 661, 3089, 17949, 611, 366, 733, 295, 886, 955, 13], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 504, "seek": 194188, "start": 1946.3200000000002, "end": 1949.6000000000001, "text": " It's like people just want to like say like, we want to have one unified code base for", "tokens": [467, 311, 411, 561, 445, 528, 281, 411, 584, 411, 11, 321, 528, 281, 362, 472, 26787, 3089, 3096, 337], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 505, "seek": 194188, "start": 1949.6000000000001, "end": 1952.3200000000002, "text": " our entire, you know, language team.", "tokens": [527, 2302, 11, 291, 458, 11, 2856, 1469, 13], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 506, "seek": 194188, "start": 1952.3200000000002, "end": 1953.64, "text": " And so they just put stuff as part of that.", "tokens": [400, 370, 436, 445, 829, 1507, 382, 644, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 507, "seek": 194188, "start": 1953.64, "end": 1954.64, "text": " And people don't really like that either.", "tokens": [400, 561, 500, 380, 534, 411, 300, 2139, 13], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 508, "seek": 194188, "start": 1954.64, "end": 1957.3200000000002, "text": " So I was very insistent that we do a minimal release.", "tokens": [407, 286, 390, 588, 13466, 317, 300, 321, 360, 257, 13206, 4374, 13], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 509, "seek": 194188, "start": 1957.3200000000002, "end": 1959.0400000000002, "text": " So like this, we're just going to release Bert.", "tokens": [407, 411, 341, 11, 321, 434, 445, 516, 281, 4374, 29594, 13], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 510, "seek": 194188, "start": 1959.0400000000002, "end": 1960.0400000000002, "text": " It's not going to be part of anything.", "tokens": [467, 311, 406, 516, 281, 312, 644, 295, 1340, 13], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 511, "seek": 194188, "start": 1960.0400000000002, "end": 1961.68, "text": " There's going to be any external dependencies.", "tokens": [821, 311, 516, 281, 312, 604, 8320, 36606, 13], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 512, "seek": 194188, "start": 1961.68, "end": 1964.2, "text": " And it's going to be like very well commented.", "tokens": [400, 309, 311, 516, 281, 312, 411, 588, 731, 26940, 13], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 513, "seek": 194188, "start": 1964.2, "end": 1968.1200000000001, "text": " I think that people, and it was kind of also easy to drop in just the modeling part and", "tokens": [286, 519, 300, 561, 11, 293, 309, 390, 733, 295, 611, 1858, 281, 3270, 294, 445, 264, 15983, 644, 293], "temperature": 0.0, "avg_logprob": -0.21555113792419434, "compression_ratio": 1.853968253968254, "no_speech_prob": 0.00020321369811426848}, {"id": 514, "seek": 196812, "start": 1968.12, "end": 1973.04, "text": " just the tokenization part and just the front end, which runs like the training loop.", "tokens": [445, 264, 14862, 2144, 644, 293, 445, 264, 1868, 917, 11, 597, 6676, 411, 264, 3097, 6367, 13], "temperature": 0.0, "avg_logprob": -0.21793765192446501, "compression_ratio": 1.7113821138211383, "no_speech_prob": 0.00010385156201664358}, {"id": 515, "seek": 196812, "start": 1973.04, "end": 1975.52, "text": " And kind of separate all these out because that way.", "tokens": [400, 733, 295, 4994, 439, 613, 484, 570, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.21793765192446501, "compression_ratio": 1.7113821138211383, "no_speech_prob": 0.00010385156201664358}, {"id": 516, "seek": 196812, "start": 1975.52, "end": 1980.04, "text": " And so I think because of that, people kind of started using it much quicker.", "tokens": [400, 370, 286, 519, 570, 295, 300, 11, 561, 733, 295, 1409, 1228, 309, 709, 16255, 13], "temperature": 0.0, "avg_logprob": -0.21793765192446501, "compression_ratio": 1.7113821138211383, "no_speech_prob": 0.00010385156201664358}, {"id": 517, "seek": 196812, "start": 1980.04, "end": 1982.4799999999998, "text": " And of course, like all the publicity help.", "tokens": [400, 295, 1164, 11, 411, 439, 264, 37264, 854, 13], "temperature": 0.0, "avg_logprob": -0.21793765192446501, "compression_ratio": 1.7113821138211383, "no_speech_prob": 0.00010385156201664358}, {"id": 518, "seek": 196812, "start": 1982.4799999999998, "end": 1988.12, "text": " But I think that, you know, it could have easily been not as successful if it had been,", "tokens": [583, 286, 519, 300, 11, 291, 458, 11, 309, 727, 362, 3612, 668, 406, 382, 4406, 498, 309, 632, 668, 11], "temperature": 0.0, "avg_logprob": -0.21793765192446501, "compression_ratio": 1.7113821138211383, "no_speech_prob": 0.00010385156201664358}, {"id": 519, "seek": 196812, "start": 1988.12, "end": 1989.12, "text": " you know, done in a different way.", "tokens": [291, 458, 11, 1096, 294, 257, 819, 636, 13], "temperature": 0.0, "avg_logprob": -0.21793765192446501, "compression_ratio": 1.7113821138211383, "no_speech_prob": 0.00010385156201664358}, {"id": 520, "seek": 196812, "start": 1989.12, "end": 1992.3999999999999, "text": " So it's just kind of advice.", "tokens": [407, 309, 311, 445, 733, 295, 5192, 13], "temperature": 0.0, "avg_logprob": -0.21793765192446501, "compression_ratio": 1.7113821138211383, "no_speech_prob": 0.00010385156201664358}, {"id": 521, "seek": 196812, "start": 1992.3999999999999, "end": 1994.9199999999998, "text": " So yeah.", "tokens": [407, 1338, 13], "temperature": 0.0, "avg_logprob": -0.21793765192446501, "compression_ratio": 1.7113821138211383, "no_speech_prob": 0.00010385156201664358}, {"id": 522, "seek": 199492, "start": 1994.92, "end": 2000.6000000000001, "text": " So now I'm going to talk about five models that have come out since Bert that have all improved", "tokens": [407, 586, 286, 478, 516, 281, 751, 466, 1732, 5245, 300, 362, 808, 484, 1670, 29594, 300, 362, 439, 9689], "temperature": 0.0, "avg_logprob": -0.25770160800120867, "compression_ratio": 1.7535714285714286, "no_speech_prob": 7.478494080714881e-05}, {"id": 523, "seek": 199492, "start": 2000.6000000000001, "end": 2002.0, "text": " on top of Bert in various ways.", "tokens": [322, 1192, 295, 29594, 294, 3683, 2098, 13], "temperature": 0.0, "avg_logprob": -0.25770160800120867, "compression_ratio": 1.7535714285714286, "no_speech_prob": 7.478494080714881e-05}, {"id": 524, "seek": 199492, "start": 2002.0, "end": 2003.04, "text": " There's been more than five.", "tokens": [821, 311, 668, 544, 813, 1732, 13], "temperature": 0.0, "avg_logprob": -0.25770160800120867, "compression_ratio": 1.7535714285714286, "no_speech_prob": 7.478494080714881e-05}, {"id": 525, "seek": 199492, "start": 2003.04, "end": 2007.0, "text": " But I'm going to highlight these five, they think they're interesting.", "tokens": [583, 286, 478, 516, 281, 5078, 613, 1732, 11, 436, 519, 436, 434, 1880, 13], "temperature": 0.0, "avg_logprob": -0.25770160800120867, "compression_ratio": 1.7535714285714286, "no_speech_prob": 7.478494080714881e-05}, {"id": 526, "seek": 199492, "start": 2007.0, "end": 2011.3200000000002, "text": " A lot of them did come from Google, but it's not because, well, a lot of them involved", "tokens": [316, 688, 295, 552, 630, 808, 490, 3329, 11, 457, 309, 311, 406, 570, 11, 731, 11, 257, 688, 295, 552, 3288], "temperature": 0.0, "avg_logprob": -0.25770160800120867, "compression_ratio": 1.7535714285714286, "no_speech_prob": 7.478494080714881e-05}, {"id": 527, "seek": 199492, "start": 2011.3200000000002, "end": 2012.3200000000002, "text": " Google.", "tokens": [3329, 13], "temperature": 0.0, "avg_logprob": -0.25770160800120867, "compression_ratio": 1.7535714285714286, "no_speech_prob": 7.478494080714881e-05}, {"id": 528, "seek": 199492, "start": 2012.3200000000002, "end": 2017.5600000000002, "text": " I would say many of them actually were not, they were interns at Google from various", "tokens": [286, 576, 584, 867, 295, 552, 767, 645, 406, 11, 436, 645, 46145, 412, 3329, 490, 3683], "temperature": 0.0, "avg_logprob": -0.25770160800120867, "compression_ratio": 1.7535714285714286, "no_speech_prob": 7.478494080714881e-05}, {"id": 529, "seek": 199492, "start": 2017.5600000000002, "end": 2021.8400000000001, "text": " universities who were supervised by Google researchers and also use Google compute.", "tokens": [11779, 567, 645, 46533, 538, 3329, 10309, 293, 611, 764, 3329, 14722, 13], "temperature": 0.0, "avg_logprob": -0.25770160800120867, "compression_ratio": 1.7535714285714286, "no_speech_prob": 7.478494080714881e-05}, {"id": 530, "seek": 202184, "start": 2021.84, "end": 2024.9199999999998, "text": " I mean, the reason why a lot of them came from Google is because, like, frankly, like,", "tokens": [286, 914, 11, 264, 1778, 983, 257, 688, 295, 552, 1361, 490, 3329, 307, 570, 11, 411, 11, 11939, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.2315739731290447, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.26074730057735e-05}, {"id": 531, "seek": 202184, "start": 2024.9199999999998, "end": 2030.9599999999998, "text": " other than Facebook, Google, and Microsoft, there's not really many, like, people that can,", "tokens": [661, 813, 4384, 11, 3329, 11, 293, 8116, 11, 456, 311, 406, 534, 867, 11, 411, 11, 561, 300, 393, 11], "temperature": 0.0, "avg_logprob": -0.2315739731290447, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.26074730057735e-05}, {"id": 532, "seek": 202184, "start": 2030.9599999999998, "end": 2034.1599999999999, "text": " the companies that have the resources to train these huge state of the art models.", "tokens": [264, 3431, 300, 362, 264, 3593, 281, 3847, 613, 2603, 1785, 295, 264, 1523, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2315739731290447, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.26074730057735e-05}, {"id": 533, "seek": 202184, "start": 2034.1599999999999, "end": 2041.36, "text": " And so, almost by necessity, it's going to come from one of these labs.", "tokens": [400, 370, 11, 1920, 538, 24217, 11, 309, 311, 516, 281, 808, 490, 472, 295, 613, 20339, 13], "temperature": 0.0, "avg_logprob": -0.2315739731290447, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.26074730057735e-05}, {"id": 534, "seek": 202184, "start": 2041.36, "end": 2044.76, "text": " So the first one was Roberta.", "tokens": [407, 264, 700, 472, 390, 15800, 1328, 13], "temperature": 0.0, "avg_logprob": -0.2315739731290447, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.26074730057735e-05}, {"id": 535, "seek": 202184, "start": 2044.76, "end": 2047.76, "text": " And so this is probably the one that had, like, the least kind of new stuff.", "tokens": [400, 370, 341, 307, 1391, 264, 472, 300, 632, 11, 411, 11, 264, 1935, 733, 295, 777, 1507, 13], "temperature": 0.0, "avg_logprob": -0.2315739731290447, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.26074730057735e-05}, {"id": 536, "seek": 202184, "start": 2047.76, "end": 2051.4, "text": " It was really just, and so this was University of Washington Facebook.", "tokens": [467, 390, 534, 445, 11, 293, 370, 341, 390, 3535, 295, 6149, 4384, 13], "temperature": 0.0, "avg_logprob": -0.2315739731290447, "compression_ratio": 1.7380952380952381, "no_speech_prob": 4.26074730057735e-05}, {"id": 537, "seek": 205140, "start": 2051.4, "end": 2053.96, "text": " It came out not that long after a birth.", "tokens": [467, 1361, 484, 406, 300, 938, 934, 257, 3965, 13], "temperature": 0.0, "avg_logprob": -0.20749577731950908, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.0001252498768735677}, {"id": 538, "seek": 205140, "start": 2053.96, "end": 2057.6800000000003, "text": " And so what they showed was that birth was really under-trained.", "tokens": [400, 370, 437, 436, 4712, 390, 300, 3965, 390, 534, 833, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.20749577731950908, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.0001252498768735677}, {"id": 539, "seek": 205140, "start": 2057.6800000000003, "end": 2062.8, "text": " And so basically, they took, even on the same amount of data, which was, even though I", "tokens": [400, 370, 1936, 11, 436, 1890, 11, 754, 322, 264, 912, 2372, 295, 1412, 11, 597, 390, 11, 754, 1673, 286], "temperature": 0.0, "avg_logprob": -0.20749577731950908, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.0001252498768735677}, {"id": 540, "seek": 205140, "start": 2062.8, "end": 2067.2000000000003, "text": " did 40 epochs on the data, if you do it for, like, 200 epochs, you get even better results,", "tokens": [630, 3356, 30992, 28346, 322, 264, 1412, 11, 498, 291, 360, 309, 337, 11, 411, 11, 2331, 30992, 28346, 11, 291, 483, 754, 1101, 3542, 11], "temperature": 0.0, "avg_logprob": -0.20749577731950908, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.0001252498768735677}, {"id": 541, "seek": 205140, "start": 2067.2000000000003, "end": 2069.52, "text": " like significantly.", "tokens": [411, 10591, 13], "temperature": 0.0, "avg_logprob": -0.20749577731950908, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.0001252498768735677}, {"id": 542, "seek": 205140, "start": 2069.52, "end": 2073.64, "text": " So they basically trained more epochs on the same data.", "tokens": [407, 436, 1936, 8895, 544, 30992, 28346, 322, 264, 912, 1412, 13], "temperature": 0.0, "avg_logprob": -0.20749577731950908, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.0001252498768735677}, {"id": 543, "seek": 205140, "start": 2073.64, "end": 2076.52, "text": " And they also showed that more data helps, which is also not super-sprising.", "tokens": [400, 436, 611, 4712, 300, 544, 1412, 3665, 11, 597, 307, 611, 406, 1687, 12, 4952, 42125, 13], "temperature": 0.0, "avg_logprob": -0.20749577731950908, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.0001252498768735677}, {"id": 544, "seek": 205140, "start": 2076.52, "end": 2080.4, "text": " And they did improve masking and pre-training using a couple of tweaks to that.", "tokens": [400, 436, 630, 3470, 31226, 293, 659, 12, 17227, 1760, 1228, 257, 1916, 295, 46664, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.20749577731950908, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.0001252498768735677}, {"id": 545, "seek": 208040, "start": 2080.4, "end": 2084.7200000000003, "text": " And they were able to get a state of the art results, which is cool.", "tokens": [400, 436, 645, 1075, 281, 483, 257, 1785, 295, 264, 1523, 3542, 11, 597, 307, 1627, 13], "temperature": 0.0, "avg_logprob": -0.23101516661605215, "compression_ratio": 1.7116104868913857, "no_speech_prob": 1.544325459690299e-05}, {"id": 546, "seek": 208040, "start": 2084.7200000000003, "end": 2089.52, "text": " And so, yeah, but that was a pretty straightforward paper.", "tokens": [400, 370, 11, 1338, 11, 457, 300, 390, 257, 1238, 15325, 3035, 13], "temperature": 0.0, "avg_logprob": -0.23101516661605215, "compression_ratio": 1.7116104868913857, "no_speech_prob": 1.544325459690299e-05}, {"id": 547, "seek": 208040, "start": 2089.52, "end": 2095.0, "text": " So the next one is XLNet, which is done by some interns in CMU when they were at Google", "tokens": [407, 264, 958, 472, 307, 37210, 31890, 11, 597, 307, 1096, 538, 512, 46145, 294, 20424, 52, 562, 436, 645, 412, 3329], "temperature": 0.0, "avg_logprob": -0.23101516661605215, "compression_ratio": 1.7116104868913857, "no_speech_prob": 1.544325459690299e-05}, {"id": 548, "seek": 208040, "start": 2095.0, "end": 2096.0, "text": " Brain.", "tokens": [29783, 13], "temperature": 0.0, "avg_logprob": -0.23101516661605215, "compression_ratio": 1.7116104868913857, "no_speech_prob": 1.544325459690299e-05}, {"id": 549, "seek": 208040, "start": 2096.0, "end": 2099.36, "text": " And so this actually had some really cool changes.", "tokens": [400, 370, 341, 767, 632, 512, 534, 1627, 2962, 13], "temperature": 0.0, "avg_logprob": -0.23101516661605215, "compression_ratio": 1.7116104868913857, "no_speech_prob": 1.544325459690299e-05}, {"id": 550, "seek": 208040, "start": 2099.36, "end": 2104.84, "text": " So one of them was, they used this transformer XL, which was actually the precursor done", "tokens": [407, 472, 295, 552, 390, 11, 436, 1143, 341, 31782, 37210, 11, 597, 390, 767, 264, 41736, 284, 1096], "temperature": 0.0, "avg_logprob": -0.23101516661605215, "compression_ratio": 1.7116104868913857, "no_speech_prob": 1.544325459690299e-05}, {"id": 551, "seek": 208040, "start": 2104.84, "end": 2109.88, "text": " by the same people that were, they were just doing links on all the tasks, did a pre-training.", "tokens": [538, 264, 912, 561, 300, 645, 11, 436, 645, 445, 884, 6123, 322, 439, 264, 9608, 11, 630, 257, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.23101516661605215, "compression_ratio": 1.7116104868913857, "no_speech_prob": 1.544325459690299e-05}, {"id": 552, "seek": 210988, "start": 2109.88, "end": 2115.76, "text": " But the big, one of the big innovations of transformer XL is this idea of relative position", "tokens": [583, 264, 955, 11, 472, 295, 264, 955, 24283, 295, 31782, 37210, 307, 341, 1558, 295, 4972, 2535], "temperature": 0.0, "avg_logprob": -0.2429290082719591, "compression_ratio": 1.869718309859155, "no_speech_prob": 7.84105432103388e-05}, {"id": 553, "seek": 210988, "start": 2115.76, "end": 2116.76, "text": " embeddings.", "tokens": [12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.2429290082719591, "compression_ratio": 1.869718309859155, "no_speech_prob": 7.84105432103388e-05}, {"id": 554, "seek": 210988, "start": 2116.76, "end": 2123.7200000000003, "text": " And so with absolute position embeddings, the problem is that every word gets, like, this", "tokens": [400, 370, 365, 8236, 2535, 12240, 29432, 11, 264, 1154, 307, 300, 633, 1349, 2170, 11, 411, 11, 341], "temperature": 0.0, "avg_logprob": -0.2429290082719591, "compression_ratio": 1.869718309859155, "no_speech_prob": 7.84105432103388e-05}, {"id": 555, "seek": 210988, "start": 2123.7200000000003, "end": 2126.2400000000002, "text": " is word four, this is word five, this is word six.", "tokens": [307, 1349, 1451, 11, 341, 307, 1349, 1732, 11, 341, 307, 1349, 2309, 13], "temperature": 0.0, "avg_logprob": -0.2429290082719591, "compression_ratio": 1.869718309859155, "no_speech_prob": 7.84105432103388e-05}, {"id": 556, "seek": 210988, "start": 2126.2400000000002, "end": 2128.04, "text": " And so they are embeddings, so they do generalize.", "tokens": [400, 370, 436, 366, 12240, 29432, 11, 370, 436, 360, 2674, 1125, 13], "temperature": 0.0, "avg_logprob": -0.2429290082719591, "compression_ratio": 1.869718309859155, "no_speech_prob": 7.84105432103388e-05}, {"id": 557, "seek": 210988, "start": 2128.04, "end": 2130.96, "text": " But in practice, there's a quadratic number of relationships.", "tokens": [583, 294, 3124, 11, 456, 311, 257, 37262, 1230, 295, 6159, 13], "temperature": 0.0, "avg_logprob": -0.2429290082719591, "compression_ratio": 1.869718309859155, "no_speech_prob": 7.84105432103388e-05}, {"id": 558, "seek": 210988, "start": 2130.96, "end": 2133.6800000000003, "text": " Like, how does word A3 relate to word 76, right?", "tokens": [1743, 11, 577, 775, 1349, 316, 18, 10961, 281, 1349, 24733, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2429290082719591, "compression_ratio": 1.869718309859155, "no_speech_prob": 7.84105432103388e-05}, {"id": 559, "seek": 210988, "start": 2133.6800000000003, "end": 2137.4, "text": " That's, that's, that's, and once you get bigger, like, 500, a thousand.", "tokens": [663, 311, 11, 300, 311, 11, 300, 311, 11, 293, 1564, 291, 483, 3801, 11, 411, 11, 5923, 11, 257, 4714, 13], "temperature": 0.0, "avg_logprob": -0.2429290082719591, "compression_ratio": 1.869718309859155, "no_speech_prob": 7.84105432103388e-05}, {"id": 560, "seek": 210988, "start": 2137.4, "end": 2139.44, "text": " Now you have a thousand squared total relationships.", "tokens": [823, 291, 362, 257, 4714, 8889, 3217, 6159, 13], "temperature": 0.0, "avg_logprob": -0.2429290082719591, "compression_ratio": 1.869718309859155, "no_speech_prob": 7.84105432103388e-05}, {"id": 561, "seek": 213944, "start": 2139.44, "end": 2142.16, "text": " Like, you have to say, how does word 97 relate to whatever, right?", "tokens": [1743, 11, 291, 362, 281, 584, 11, 577, 775, 1349, 23399, 10961, 281, 2035, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.22385089708411177, "compression_ratio": 1.795275590551181, "no_speech_prob": 3.118810127489269e-05}, {"id": 562, "seek": 213944, "start": 2142.16, "end": 2146.52, "text": " And so that's obviously not optimal once you get to a large size.", "tokens": [400, 370, 300, 311, 2745, 406, 16252, 1564, 291, 483, 281, 257, 2416, 2744, 13], "temperature": 0.0, "avg_logprob": -0.22385089708411177, "compression_ratio": 1.795275590551181, "no_speech_prob": 3.118810127489269e-05}, {"id": 563, "seek": 213944, "start": 2146.52, "end": 2156.12, "text": " And so with, with relative position embeddings, you basically can say how much does dog", "tokens": [400, 370, 365, 11, 365, 4972, 2535, 12240, 29432, 11, 291, 1936, 393, 584, 577, 709, 775, 3000], "temperature": 0.0, "avg_logprob": -0.22385089708411177, "compression_ratio": 1.795275590551181, "no_speech_prob": 3.118810127489269e-05}, {"id": 564, "seek": 213944, "start": 2156.12, "end": 2160.76, "text": " attend to hot and how much should the word dog attend to the previous word.", "tokens": [6888, 281, 2368, 293, 577, 709, 820, 264, 1349, 3000, 6888, 281, 264, 3894, 1349, 13], "temperature": 0.0, "avg_logprob": -0.22385089708411177, "compression_ratio": 1.795275590551181, "no_speech_prob": 3.118810127489269e-05}, {"id": 565, "seek": 213944, "start": 2160.76, "end": 2164.04, "text": " And then you get, and these are nonlinear for, these are linear at first.", "tokens": [400, 550, 291, 483, 11, 293, 613, 366, 2107, 28263, 337, 11, 613, 366, 8213, 412, 700, 13], "temperature": 0.0, "avg_logprob": -0.22385089708411177, "compression_ratio": 1.795275590551181, "no_speech_prob": 3.118810127489269e-05}, {"id": 566, "seek": 213944, "start": 2164.04, "end": 2167.84, "text": " But then you combine them and then you get a nonlinear, a conditional representation.", "tokens": [583, 550, 291, 10432, 552, 293, 550, 291, 483, 257, 2107, 28263, 11, 257, 27708, 10290, 13], "temperature": 0.0, "avg_logprob": -0.22385089708411177, "compression_ratio": 1.795275590551181, "no_speech_prob": 3.118810127489269e-05}, {"id": 567, "seek": 216784, "start": 2167.84, "end": 2169.48, "text": " And you do this in many, many layers.", "tokens": [400, 291, 360, 341, 294, 867, 11, 867, 7914, 13], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 568, "seek": 216784, "start": 2169.48, "end": 2172.0, "text": " And this ends up being, so then you say, how does this contextual", "tokens": [400, 341, 5314, 493, 885, 11, 370, 550, 291, 584, 11, 577, 775, 341, 35526], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 569, "seek": 216784, "start": 2172.0, "end": 2174.76, "text": " precision of dog, how much does that attend to the previous word.", "tokens": [18356, 295, 3000, 11, 577, 709, 775, 300, 6888, 281, 264, 3894, 1349, 13], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 570, "seek": 216784, "start": 2174.76, "end": 2176.2400000000002, "text": " And then you kind of get, can build up.", "tokens": [400, 550, 291, 733, 295, 483, 11, 393, 1322, 493, 13], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 571, "seek": 216784, "start": 2176.2400000000002, "end": 2178.4, "text": " And so this generalizes much better for long sequences.", "tokens": [400, 370, 341, 2674, 5660, 709, 1101, 337, 938, 22978, 13], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 572, "seek": 216784, "start": 2178.4, "end": 2180.88, "text": " So that's a cool innovation.", "tokens": [407, 300, 311, 257, 1627, 8504, 13], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 573, "seek": 216784, "start": 2180.88, "end": 2185.44, "text": " And then the other one, which is specific to pre-training and not just the model itself,", "tokens": [400, 550, 264, 661, 472, 11, 597, 307, 2685, 281, 659, 12, 17227, 1760, 293, 406, 445, 264, 2316, 2564, 11], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 574, "seek": 216784, "start": 2185.44, "end": 2187.1200000000003, "text": " is this idea of permutation language modeling.", "tokens": [307, 341, 1558, 295, 4784, 11380, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 575, "seek": 216784, "start": 2187.1200000000003, "end": 2188.88, "text": " So this is a little bit hard to explain.", "tokens": [407, 341, 307, 257, 707, 857, 1152, 281, 2903, 13], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 576, "seek": 216784, "start": 2191.1200000000003, "end": 2194.6400000000003, "text": " I think the paper explained it very formally, I guess.", "tokens": [286, 519, 264, 3035, 8825, 309, 588, 25983, 11, 286, 2041, 13], "temperature": 0.0, "avg_logprob": -0.2408315363064618, "compression_ratio": 1.7417218543046358, "no_speech_prob": 6.011535879224539e-05}, {"id": 577, "seek": 219464, "start": 2194.64, "end": 2200.72, "text": " And so, but basically, there's a trick where, so in a left-for-right language model,", "tokens": [400, 370, 11, 457, 1936, 11, 456, 311, 257, 4282, 689, 11, 370, 294, 257, 1411, 12, 2994, 12, 1938, 2856, 2316, 11], "temperature": 0.0, "avg_logprob": -0.21729347229003906, "compression_ratio": 1.8311258278145695, "no_speech_prob": 1.405706370860571e-05}, {"id": 578, "seek": 219464, "start": 2200.72, "end": 2204.48, "text": " every word done predicting is based on the word the left, right?", "tokens": [633, 1349, 1096, 32884, 307, 2361, 322, 264, 1349, 264, 1411, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21729347229003906, "compression_ratio": 1.8311258278145695, "no_speech_prob": 1.405706370860571e-05}, {"id": 579, "seek": 219464, "start": 2204.48, "end": 2208.7599999999998, "text": " But imagine that instead of predicting all the, you can basically take any permutation.", "tokens": [583, 3811, 300, 2602, 295, 32884, 439, 264, 11, 291, 393, 1936, 747, 604, 4784, 11380, 13], "temperature": 0.0, "avg_logprob": -0.21729347229003906, "compression_ratio": 1.8311258278145695, "no_speech_prob": 1.405706370860571e-05}, {"id": 580, "seek": 219464, "start": 2208.7599999999998, "end": 2212.72, "text": " So it's like, I'm going to predict the first word, then I'm going to be the third word,", "tokens": [407, 309, 311, 411, 11, 286, 478, 516, 281, 6069, 264, 700, 1349, 11, 550, 286, 478, 516, 281, 312, 264, 2636, 1349, 11], "temperature": 0.0, "avg_logprob": -0.21729347229003906, "compression_ratio": 1.8311258278145695, "no_speech_prob": 1.405706370860571e-05}, {"id": 581, "seek": 219464, "start": 2212.72, "end": 2215.0, "text": " then the second word, then the fourth word.", "tokens": [550, 264, 1150, 1349, 11, 550, 264, 6409, 1349, 13], "temperature": 0.0, "avg_logprob": -0.21729347229003906, "compression_ratio": 1.8311258278145695, "no_speech_prob": 1.405706370860571e-05}, {"id": 582, "seek": 219464, "start": 2215.0, "end": 2218.08, "text": " And so, that's a totally valid way.", "tokens": [400, 370, 11, 300, 311, 257, 3879, 7363, 636, 13], "temperature": 0.0, "avg_logprob": -0.21729347229003906, "compression_ratio": 1.8311258278145695, "no_speech_prob": 1.405706370860571e-05}, {"id": 583, "seek": 219464, "start": 2218.08, "end": 2220.52, "text": " And you still get the well-for-and-probability distribution, because it's still predicting", "tokens": [400, 291, 920, 483, 264, 731, 12, 2994, 12, 474, 12, 41990, 2310, 7316, 11, 570, 309, 311, 920, 32884], "temperature": 0.0, "avg_logprob": -0.21729347229003906, "compression_ratio": 1.8311258278145695, "no_speech_prob": 1.405706370860571e-05}, {"id": 584, "seek": 219464, "start": 2220.52, "end": 2224.12, "text": " one word at a time, given some permutation of the input.", "tokens": [472, 1349, 412, 257, 565, 11, 2212, 512, 4784, 11380, 295, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.21729347229003906, "compression_ratio": 1.8311258278145695, "no_speech_prob": 1.405706370860571e-05}, {"id": 585, "seek": 222412, "start": 2224.12, "end": 2227.0, "text": " And with transformers and with attention, you can actually do this very efficiently,", "tokens": [400, 365, 4088, 433, 293, 365, 3202, 11, 291, 393, 767, 360, 341, 588, 19621, 11], "temperature": 0.0, "avg_logprob": -0.19953079223632814, "compression_ratio": 1.9831081081081081, "no_speech_prob": 4.262582297087647e-05}, {"id": 586, "seek": 222412, "start": 2227.0, "end": 2229.48, "text": " just by masking out your attention probabilities.", "tokens": [445, 538, 31226, 484, 428, 3202, 33783, 13], "temperature": 0.0, "avg_logprob": -0.19953079223632814, "compression_ratio": 1.9831081081081081, "no_speech_prob": 4.262582297087647e-05}, {"id": 587, "seek": 222412, "start": 2229.48, "end": 2235.96, "text": " And so, every single sentence you have, you can kind of sample a single permutation of this.", "tokens": [400, 370, 11, 633, 2167, 8174, 291, 362, 11, 291, 393, 733, 295, 6889, 257, 2167, 4784, 11380, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.19953079223632814, "compression_ratio": 1.9831081081081081, "no_speech_prob": 4.262582297087647e-05}, {"id": 588, "seek": 222412, "start": 2235.96, "end": 2241.96, "text": " And you can now, you can effectively train a bi-directional model, because this word,", "tokens": [400, 291, 393, 586, 11, 291, 393, 8659, 3847, 257, 3228, 12, 18267, 41048, 2316, 11, 570, 341, 1349, 11], "temperature": 0.0, "avg_logprob": -0.19953079223632814, "compression_ratio": 1.9831081081081081, "no_speech_prob": 4.262582297087647e-05}, {"id": 589, "seek": 222412, "start": 2241.96, "end": 2243.92, "text": " it won't be conditioned on every, still on average,", "tokens": [309, 1582, 380, 312, 35833, 322, 633, 11, 920, 322, 4274, 11], "temperature": 0.0, "avg_logprob": -0.19953079223632814, "compression_ratio": 1.9831081081081081, "no_speech_prob": 4.262582297087647e-05}, {"id": 590, "seek": 222412, "start": 2243.92, "end": 2245.7999999999997, "text": " every word will only be conditioned on half the words.", "tokens": [633, 1349, 486, 787, 312, 35833, 322, 1922, 264, 2283, 13], "temperature": 0.0, "avg_logprob": -0.19953079223632814, "compression_ratio": 1.9831081081081081, "no_speech_prob": 4.262582297087647e-05}, {"id": 591, "seek": 222412, "start": 2245.7999999999997, "end": 2250.3199999999997, "text": " But this word will be conditioned on, you know, all these words to the left and all these words to the right.", "tokens": [583, 341, 1349, 486, 312, 35833, 322, 11, 291, 458, 11, 439, 613, 2283, 281, 264, 1411, 293, 439, 613, 2283, 281, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.19953079223632814, "compression_ratio": 1.9831081081081081, "no_speech_prob": 4.262582297087647e-05}, {"id": 592, "seek": 222412, "start": 2250.3199999999997, "end": 2252.56, "text": " And maybe it'll be missing these words, but that's fine.", "tokens": [400, 1310, 309, 603, 312, 5361, 613, 2283, 11, 457, 300, 311, 2489, 13], "temperature": 0.0, "avg_logprob": -0.19953079223632814, "compression_ratio": 1.9831081081081081, "no_speech_prob": 4.262582297087647e-05}, {"id": 593, "seek": 225256, "start": 2252.56, "end": 2255.36, "text": " And so, you get much better sample efficiency.", "tokens": [400, 370, 11, 291, 483, 709, 1101, 6889, 10493, 13], "temperature": 0.0, "avg_logprob": -0.220886476578251, "compression_ratio": 1.752851711026616, "no_speech_prob": 5.7374149037059397e-05}, {"id": 594, "seek": 225256, "start": 2255.36, "end": 2258.12, "text": " So I thought this was a really clever idea.", "tokens": [407, 286, 1194, 341, 390, 257, 534, 13494, 1558, 13], "temperature": 0.0, "avg_logprob": -0.220886476578251, "compression_ratio": 1.752851711026616, "no_speech_prob": 5.7374149037059397e-05}, {"id": 595, "seek": 225256, "start": 2258.12, "end": 2261.08, "text": " And so, and this was kind of the main innovation of X on it.", "tokens": [400, 370, 11, 293, 341, 390, 733, 295, 264, 2135, 8504, 295, 1783, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.220886476578251, "compression_ratio": 1.752851711026616, "no_speech_prob": 5.7374149037059397e-05}, {"id": 596, "seek": 225256, "start": 2261.08, "end": 2267.04, "text": " And so, yeah, they basically get better sample efficiency, because they're able to", "tokens": [400, 370, 11, 1338, 11, 436, 1936, 483, 1101, 6889, 10493, 11, 570, 436, 434, 1075, 281], "temperature": 0.0, "avg_logprob": -0.220886476578251, "compression_ratio": 1.752851711026616, "no_speech_prob": 5.7374149037059397e-05}, {"id": 597, "seek": 225256, "start": 2267.04, "end": 2270.16, "text": " do this random permutation and kind of take advantage of this.", "tokens": [360, 341, 4974, 4784, 11380, 293, 733, 295, 747, 5002, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.220886476578251, "compression_ratio": 1.752851711026616, "no_speech_prob": 5.7374149037059397e-05}, {"id": 598, "seek": 225256, "start": 2270.16, "end": 2274.72, "text": " So this wouldn't work with LFTMs, because of this ordering, but", "tokens": [407, 341, 2759, 380, 589, 365, 441, 25469, 26386, 11, 570, 295, 341, 21739, 11, 457], "temperature": 0.0, "avg_logprob": -0.220886476578251, "compression_ratio": 1.752851711026616, "no_speech_prob": 5.7374149037059397e-05}, {"id": 599, "seek": 225256, "start": 2274.72, "end": 2277.68, "text": " because the way that masking is done in transformers, it's just,", "tokens": [570, 264, 636, 300, 31226, 307, 1096, 294, 4088, 433, 11, 309, 311, 445, 11], "temperature": 0.0, "avg_logprob": -0.220886476578251, "compression_ratio": 1.752851711026616, "no_speech_prob": 5.7374149037059397e-05}, {"id": 600, "seek": 225256, "start": 2279.4, "end": 2281.88, "text": " it's just a mask on the attention.", "tokens": [309, 311, 445, 257, 6094, 322, 264, 3202, 13], "temperature": 0.0, "avg_logprob": -0.220886476578251, "compression_ratio": 1.752851711026616, "no_speech_prob": 5.7374149037059397e-05}, {"id": 601, "seek": 228188, "start": 2281.88, "end": 2283.8, "text": " So it actually ends up working very well.", "tokens": [407, 309, 767, 5314, 493, 1364, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.2759473690619835, "compression_ratio": 1.774891774891775, "no_speech_prob": 7.140071829780936e-05}, {"id": 602, "seek": 228188, "start": 2283.8, "end": 2290.8, "text": " And so, they also got, so, yeah, the numbers, they actually ended up being pretty similar.", "tokens": [400, 370, 11, 436, 611, 658, 11, 370, 11, 1338, 11, 264, 3547, 11, 436, 767, 4590, 493, 885, 1238, 2531, 13], "temperature": 0.0, "avg_logprob": -0.2759473690619835, "compression_ratio": 1.774891774891775, "no_speech_prob": 7.140071829780936e-05}, {"id": 603, "seek": 228188, "start": 2292.2000000000003, "end": 2296.28, "text": " But a lot of these things are hard to compare, because people change the data set and", "tokens": [583, 257, 688, 295, 613, 721, 366, 1152, 281, 6794, 11, 570, 561, 1319, 264, 1412, 992, 293], "temperature": 0.0, "avg_logprob": -0.2759473690619835, "compression_ratio": 1.774891774891775, "no_speech_prob": 7.140071829780936e-05}, {"id": 604, "seek": 228188, "start": 2296.28, "end": 2298.92, "text": " change the size of the model.", "tokens": [1319, 264, 2744, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2759473690619835, "compression_ratio": 1.774891774891775, "no_speech_prob": 7.140071829780936e-05}, {"id": 605, "seek": 228188, "start": 2298.92, "end": 2300.44, "text": " So it's hard to compare apples to apples.", "tokens": [407, 309, 311, 1152, 281, 6794, 16814, 281, 16814, 13], "temperature": 0.0, "avg_logprob": -0.2759473690619835, "compression_ratio": 1.774891774891775, "no_speech_prob": 7.140071829780936e-05}, {"id": 606, "seek": 228188, "start": 2300.44, "end": 2306.4, "text": " But these two techniques ended up being pretty similar, but I think X on it had more innovations in terms of technique.", "tokens": [583, 613, 732, 7512, 4590, 493, 885, 1238, 2531, 11, 457, 286, 519, 1783, 322, 309, 632, 544, 24283, 294, 2115, 295, 6532, 13], "temperature": 0.0, "avg_logprob": -0.2759473690619835, "compression_ratio": 1.774891774891775, "no_speech_prob": 7.140071829780936e-05}, {"id": 607, "seek": 230640, "start": 2306.4, "end": 2313.2400000000002, "text": " So Albert, it's called a lightbert for self-supervised learning.", "tokens": [407, 20812, 11, 309, 311, 1219, 257, 1442, 4290, 337, 2698, 12, 48172, 24420, 2539, 13], "temperature": 0.0, "avg_logprob": -0.23085673650105795, "compression_ratio": 1.8436482084690553, "no_speech_prob": 8.347532275365666e-05}, {"id": 608, "seek": 230640, "start": 2313.2400000000002, "end": 2317.1600000000003, "text": " And so, this also had a couple of cool innovations.", "tokens": [400, 370, 11, 341, 611, 632, 257, 1916, 295, 1627, 24283, 13], "temperature": 0.0, "avg_logprob": -0.23085673650105795, "compression_ratio": 1.8436482084690553, "no_speech_prob": 8.347532275365666e-05}, {"id": 609, "seek": 230640, "start": 2317.1600000000003, "end": 2321.7200000000003, "text": " And so the idea here is really massive parameter sharing, with the idea being that,", "tokens": [400, 370, 264, 1558, 510, 307, 534, 5994, 13075, 5414, 11, 365, 264, 1558, 885, 300, 11], "temperature": 0.0, "avg_logprob": -0.23085673650105795, "compression_ratio": 1.8436482084690553, "no_speech_prob": 8.347532275365666e-05}, {"id": 610, "seek": 230640, "start": 2321.7200000000003, "end": 2324.28, "text": " if you share parameters, you're not going to get a better language model, but", "tokens": [498, 291, 2073, 9834, 11, 291, 434, 406, 516, 281, 483, 257, 1101, 2856, 2316, 11, 457], "temperature": 0.0, "avg_logprob": -0.23085673650105795, "compression_ratio": 1.8436482084690553, "no_speech_prob": 8.347532275365666e-05}, {"id": 611, "seek": 230640, "start": 2324.28, "end": 2326.2400000000002, "text": " you're going to get better sample efficiency.", "tokens": [291, 434, 516, 281, 483, 1101, 6889, 10493, 13], "temperature": 0.0, "avg_logprob": -0.23085673650105795, "compression_ratio": 1.8436482084690553, "no_speech_prob": 8.347532275365666e-05}, {"id": 612, "seek": 230640, "start": 2326.2400000000002, "end": 2328.32, "text": " You're going to get less overfitting when you fine tune, right?", "tokens": [509, 434, 516, 281, 483, 1570, 670, 69, 2414, 562, 291, 2489, 10864, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23085673650105795, "compression_ratio": 1.8436482084690553, "no_speech_prob": 8.347532275365666e-05}, {"id": 613, "seek": 230640, "start": 2328.32, "end": 2331.88, "text": " Because if you have a billion parameters and you fine tune them on a 300,", "tokens": [1436, 498, 291, 362, 257, 5218, 9834, 293, 291, 2489, 10864, 552, 322, 257, 6641, 11], "temperature": 0.0, "avg_logprob": -0.23085673650105795, "compression_ratio": 1.8436482084690553, "no_speech_prob": 8.347532275365666e-05}, {"id": 614, "seek": 230640, "start": 2331.88, "end": 2335.76, "text": " on a data set with like a thousand labeled examples, you're still going to overfit very quickly, right?", "tokens": [322, 257, 1412, 992, 365, 411, 257, 4714, 21335, 5110, 11, 291, 434, 920, 516, 281, 670, 6845, 588, 2661, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23085673650105795, "compression_ratio": 1.8436482084690553, "no_speech_prob": 8.347532275365666e-05}, {"id": 615, "seek": 233576, "start": 2335.76, "end": 2339.36, "text": " But if you have a much smaller number of parameters, you're going to get less overfitting.", "tokens": [583, 498, 291, 362, 257, 709, 4356, 1230, 295, 9834, 11, 291, 434, 516, 281, 483, 1570, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.20825330358352104, "compression_ratio": 1.900709219858156, "no_speech_prob": 4.907641050522216e-05}, {"id": 616, "seek": 233576, "start": 2339.36, "end": 2342.48, "text": " So if you get a similarly powerful model with fewer parameters, you're going to get less overfitting.", "tokens": [407, 498, 291, 483, 257, 14138, 4005, 2316, 365, 13366, 9834, 11, 291, 434, 516, 281, 483, 1570, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.20825330358352104, "compression_ratio": 1.900709219858156, "no_speech_prob": 4.907641050522216e-05}, {"id": 617, "seek": 233576, "start": 2343.5600000000004, "end": 2347.0, "text": " And so they, so there's two major innovations where,", "tokens": [400, 370, 436, 11, 370, 456, 311, 732, 2563, 24283, 689, 11], "temperature": 0.0, "avg_logprob": -0.20825330358352104, "compression_ratio": 1.900709219858156, "no_speech_prob": 4.907641050522216e-05}, {"id": 618, "seek": 233576, "start": 2347.0, "end": 2349.6800000000003, "text": " so instead of using a word, because the wording of the table is big, right?", "tokens": [370, 2602, 295, 1228, 257, 1349, 11, 570, 264, 47602, 295, 264, 3199, 307, 955, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.20825330358352104, "compression_ratio": 1.900709219858156, "no_speech_prob": 4.907641050522216e-05}, {"id": 619, "seek": 233576, "start": 2349.6800000000003, "end": 2355.48, "text": " Because it's the size of your vocabulary, the number of word pieces, times the hidden size.", "tokens": [1436, 309, 311, 264, 2744, 295, 428, 19864, 11, 264, 1230, 295, 1349, 3755, 11, 1413, 264, 7633, 2744, 13], "temperature": 0.0, "avg_logprob": -0.20825330358352104, "compression_ratio": 1.900709219858156, "no_speech_prob": 4.907641050522216e-05}, {"id": 620, "seek": 233576, "start": 2355.48, "end": 2358.76, "text": " And so it's going to be much bigger than the hidden layer.", "tokens": [400, 370, 309, 311, 516, 281, 312, 709, 3801, 813, 264, 7633, 4583, 13], "temperature": 0.0, "avg_logprob": -0.20825330358352104, "compression_ratio": 1.900709219858156, "no_speech_prob": 4.907641050522216e-05}, {"id": 621, "seek": 233576, "start": 2358.76, "end": 2361.28, "text": " So first thing is that they use the factorized embedding table.", "tokens": [407, 700, 551, 307, 300, 436, 764, 264, 5952, 1602, 12240, 3584, 3199, 13], "temperature": 0.0, "avg_logprob": -0.20825330358352104, "compression_ratio": 1.900709219858156, "no_speech_prob": 4.907641050522216e-05}, {"id": 622, "seek": 236128, "start": 2361.28, "end": 2368.84, "text": " So if they had a hidden size of a thousand, they only use like 128 dimensional input embedding.", "tokens": [407, 498, 436, 632, 257, 7633, 2744, 295, 257, 4714, 11, 436, 787, 764, 411, 29810, 18795, 4846, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.18923587458474295, "compression_ratio": 1.8227848101265822, "no_speech_prob": 3.4801683796104044e-05}, {"id": 623, "seek": 236128, "start": 2368.84, "end": 2373.44, "text": " And then they projected that to a thousand using a matrix.", "tokens": [400, 550, 436, 26231, 300, 281, 257, 4714, 1228, 257, 8141, 13], "temperature": 0.0, "avg_logprob": -0.18923587458474295, "compression_ratio": 1.8227848101265822, "no_speech_prob": 3.4801683796104044e-05}, {"id": 624, "seek": 236128, "start": 2373.44, "end": 2380.96, "text": " And so instead of having 1024 by 100,000, they would have 100,000 by 100,000 plus 1024 times 100,000,", "tokens": [400, 370, 2602, 295, 1419, 1266, 7911, 538, 2319, 11, 1360, 11, 436, 576, 362, 2319, 11, 1360, 538, 2319, 11, 1360, 1804, 1266, 7911, 1413, 2319, 11, 1360, 11], "temperature": 0.0, "avg_logprob": -0.18923587458474295, "compression_ratio": 1.8227848101265822, "no_speech_prob": 3.4801683796104044e-05}, {"id": 625, "seek": 236128, "start": 2380.96, "end": 2383.6800000000003, "text": " and you multiply these together and multiply these two matrices together.", "tokens": [293, 291, 12972, 613, 1214, 293, 12972, 613, 732, 32284, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18923587458474295, "compression_ratio": 1.8227848101265822, "no_speech_prob": 3.4801683796104044e-05}, {"id": 626, "seek": 236128, "start": 2383.6800000000003, "end": 2388.84, "text": " And then effectively you have a 1024 by 100,000 embedding matrix.", "tokens": [400, 550, 8659, 291, 362, 257, 1266, 7911, 538, 2319, 11, 1360, 12240, 3584, 8141, 13], "temperature": 0.0, "avg_logprob": -0.18923587458474295, "compression_ratio": 1.8227848101265822, "no_speech_prob": 3.4801683796104044e-05}, {"id": 627, "seek": 236128, "start": 2388.84, "end": 2390.1200000000003, "text": " But you have much fewer parameters.", "tokens": [583, 291, 362, 709, 13366, 9834, 13], "temperature": 0.0, "avg_logprob": -0.18923587458474295, "compression_ratio": 1.8227848101265822, "no_speech_prob": 3.4801683796104044e-05}, {"id": 628, "seek": 239012, "start": 2390.12, "end": 2391.7999999999997, "text": " So you're doing parameter tying.", "tokens": [407, 291, 434, 884, 13075, 32405, 13], "temperature": 0.0, "avg_logprob": -0.24693892035685794, "compression_ratio": 1.8344827586206895, "no_speech_prob": 3.882180681102909e-05}, {"id": 629, "seek": 239012, "start": 2391.7999999999997, "end": 2395.24, "text": " Well, not, this isn't a parameter tie, but you're doing parameter reduction in a clever way.", "tokens": [1042, 11, 406, 11, 341, 1943, 380, 257, 13075, 7582, 11, 457, 291, 434, 884, 13075, 11004, 294, 257, 13494, 636, 13], "temperature": 0.0, "avg_logprob": -0.24693892035685794, "compression_ratio": 1.8344827586206895, "no_speech_prob": 3.882180681102909e-05}, {"id": 630, "seek": 239012, "start": 2396.56, "end": 2398.56, "text": " The other one is cross layer parameter sharing.", "tokens": [440, 661, 472, 307, 3278, 4583, 13075, 5414, 13], "temperature": 0.0, "avg_logprob": -0.24693892035685794, "compression_ratio": 1.8344827586206895, "no_speech_prob": 3.882180681102909e-05}, {"id": 631, "seek": 239012, "start": 2398.56, "end": 2399.8399999999997, "text": " So this is similar.", "tokens": [407, 341, 307, 2531, 13], "temperature": 0.0, "avg_logprob": -0.24693892035685794, "compression_ratio": 1.8344827586206895, "no_speech_prob": 3.882180681102909e-05}, {"id": 632, "seek": 239012, "start": 2399.8399999999997, "end": 2406.72, "text": " It's as simple and it was all, it's, it was, it was been done in previous papers, especially universal transformer.", "tokens": [467, 311, 382, 2199, 293, 309, 390, 439, 11, 309, 311, 11, 309, 390, 11, 309, 390, 668, 1096, 294, 3894, 10577, 11, 2318, 11455, 31782, 13], "temperature": 0.0, "avg_logprob": -0.24693892035685794, "compression_ratio": 1.8344827586206895, "no_speech_prob": 3.882180681102909e-05}, {"id": 633, "seek": 239012, "start": 2406.72, "end": 2410.3199999999997, "text": " And the idea is that you, you've run a much of transformer layers, but all,", "tokens": [400, 264, 1558, 307, 300, 291, 11, 291, 600, 1190, 257, 709, 295, 31782, 7914, 11, 457, 439, 11], "temperature": 0.0, "avg_logprob": -0.24693892035685794, "compression_ratio": 1.8344827586206895, "no_speech_prob": 3.882180681102909e-05}, {"id": 634, "seek": 239012, "start": 2410.3199999999997, "end": 2413.44, "text": " let's say if you have 12 layers, all 12 layers just share the same parameters, right?", "tokens": [718, 311, 584, 498, 291, 362, 2272, 7914, 11, 439, 2272, 7914, 445, 2073, 264, 912, 9834, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24693892035685794, "compression_ratio": 1.8344827586206895, "no_speech_prob": 3.882180681102909e-05}, {"id": 635, "seek": 239012, "start": 2413.44, "end": 2419.7599999999998, "text": " And so that ends up, so now you can have a much bigger model", "tokens": [400, 370, 300, 5314, 493, 11, 370, 586, 291, 393, 362, 257, 709, 3801, 2316], "temperature": 0.0, "avg_logprob": -0.24693892035685794, "compression_ratio": 1.8344827586206895, "no_speech_prob": 3.882180681102909e-05}, {"id": 636, "seek": 241976, "start": 2419.76, "end": 2422.36, "text": " that has fewer parameters than bird has.", "tokens": [300, 575, 13366, 9834, 813, 5255, 575, 13], "temperature": 0.0, "avg_logprob": -0.20709532433813746, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.610325413523242e-05}, {"id": 637, "seek": 241976, "start": 2422.36, "end": 2424.2400000000002, "text": " And so you get less over fitting.", "tokens": [400, 370, 291, 483, 1570, 670, 15669, 13], "temperature": 0.0, "avg_logprob": -0.20709532433813746, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.610325413523242e-05}, {"id": 638, "seek": 241976, "start": 2424.2400000000002, "end": 2430.1200000000003, "text": " And so they got state of the art compared to X on that in Roberta.", "tokens": [400, 370, 436, 658, 1785, 295, 264, 1523, 5347, 281, 1783, 322, 300, 294, 15800, 1328, 13], "temperature": 0.0, "avg_logprob": -0.20709532433813746, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.610325413523242e-05}, {"id": 639, "seek": 241976, "start": 2430.1200000000003, "end": 2433.92, "text": " But one important thing to keep in mind is that Albert is light in terms of parameters,", "tokens": [583, 472, 1021, 551, 281, 1066, 294, 1575, 307, 300, 20812, 307, 1442, 294, 2115, 295, 9834, 11], "temperature": 0.0, "avg_logprob": -0.20709532433813746, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.610325413523242e-05}, {"id": 640, "seek": 241976, "start": 2433.92, "end": 2435.0800000000004, "text": " not in terms of speed.", "tokens": [406, 294, 2115, 295, 3073, 13], "temperature": 0.0, "avg_logprob": -0.20709532433813746, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.610325413523242e-05}, {"id": 641, "seek": 241976, "start": 2435.0800000000004, "end": 2447.1600000000003, "text": " So for a, for the mix, for the model that's actually comparable to, to bird,", "tokens": [407, 337, 257, 11, 337, 264, 2890, 11, 337, 264, 2316, 300, 311, 767, 25323, 281, 11, 281, 5255, 11], "temperature": 0.0, "avg_logprob": -0.20709532433813746, "compression_ratio": 1.6532663316582914, "no_speech_prob": 4.610325413523242e-05}, {"id": 642, "seek": 244716, "start": 2447.16, "end": 2453.64, "text": " they, they actually did slightly, like, like this model and this model were about the same,", "tokens": [436, 11, 436, 767, 630, 4748, 11, 411, 11, 411, 341, 2316, 293, 341, 2316, 645, 466, 264, 912, 11], "temperature": 0.0, "avg_logprob": -0.170491868415765, "compression_ratio": 1.8443579766536966, "no_speech_prob": 4.984796032658778e-05}, {"id": 643, "seek": 244716, "start": 2453.64, "end": 2455.0, "text": " but this one was actually slower.", "tokens": [457, 341, 472, 390, 767, 14009, 13], "temperature": 0.0, "avg_logprob": -0.170491868415765, "compression_ratio": 1.8443579766536966, "no_speech_prob": 4.984796032658778e-05}, {"id": 644, "seek": 244716, "start": 2455.0, "end": 2461.2, "text": " So it's only when they started making models that were much bigger in terms of compute than", "tokens": [407, 309, 311, 787, 562, 436, 1409, 1455, 5245, 300, 645, 709, 3801, 294, 2115, 295, 14722, 813], "temperature": 0.0, "avg_logprob": -0.170491868415765, "compression_ratio": 1.8443579766536966, "no_speech_prob": 4.984796032658778e-05}, {"id": 645, "seek": 244716, "start": 2461.2, "end": 2465.44, "text": " bird, but doing more parameter tying than they started getting good results.", "tokens": [5255, 11, 457, 884, 544, 13075, 32405, 813, 436, 1409, 1242, 665, 3542, 13], "temperature": 0.0, "avg_logprob": -0.170491868415765, "compression_ratio": 1.8443579766536966, "no_speech_prob": 4.984796032658778e-05}, {"id": 646, "seek": 244716, "start": 2465.44, "end": 2471.3599999999997, "text": " And so the, the implication of this is that like, you can, you can reduce the number of parameters,", "tokens": [400, 370, 264, 11, 264, 37814, 295, 341, 307, 300, 411, 11, 291, 393, 11, 291, 393, 5407, 264, 1230, 295, 9834, 11], "temperature": 0.0, "avg_logprob": -0.170491868415765, "compression_ratio": 1.8443579766536966, "no_speech_prob": 4.984796032658778e-05}, {"id": 647, "seek": 244716, "start": 2471.3599999999997, "end": 2476.3599999999997, "text": " but still nobody's figured out how to reduce the amount of pre-training compute", "tokens": [457, 920, 5079, 311, 8932, 484, 577, 281, 5407, 264, 2372, 295, 659, 12, 17227, 1760, 14722], "temperature": 0.0, "avg_logprob": -0.170491868415765, "compression_ratio": 1.8443579766536966, "no_speech_prob": 4.984796032658778e-05}, {"id": 648, "seek": 247636, "start": 2476.36, "end": 2479.2400000000002, "text": " that it would describe, which is, you know, kind of unfortunate.", "tokens": [300, 309, 576, 6786, 11, 597, 307, 11, 291, 458, 11, 733, 295, 17843, 13], "temperature": 0.0, "avg_logprob": -0.28146165211995444, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.00010067569382954389}, {"id": 649, "seek": 247636, "start": 2479.2400000000002, "end": 2485.7200000000003, "text": " So the next one is T5, which is exploring the limits of transfer-leading with unified", "tokens": [407, 264, 958, 472, 307, 314, 20, 11, 597, 307, 12736, 264, 10406, 295, 5003, 12, 28012, 365, 26787], "temperature": 0.0, "avg_logprob": -0.28146165211995444, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.00010067569382954389}, {"id": 650, "seek": 247636, "start": 2485.7200000000003, "end": 2486.84, "text": " text-txt firmware.", "tokens": [2487, 12, 83, 734, 30289, 13], "temperature": 0.0, "avg_logprob": -0.28146165211995444, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.00010067569382954389}, {"id": 651, "seek": 247636, "start": 2486.84, "end": 2493.36, "text": " So this was a paper by Google Brain and other groups in Google where they used just,", "tokens": [407, 341, 390, 257, 3035, 538, 3329, 29783, 293, 661, 3935, 294, 3329, 689, 436, 1143, 445, 11], "temperature": 0.0, "avg_logprob": -0.28146165211995444, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.00010067569382954389}, {"id": 652, "seek": 247636, "start": 2493.36, "end": 2498.1600000000003, "text": " they used a lot of compute and they did tons of ablation on pre-training.", "tokens": [436, 1143, 257, 688, 295, 14722, 293, 436, 630, 9131, 295, 410, 24278, 322, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.28146165211995444, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.00010067569382954389}, {"id": 653, "seek": 247636, "start": 2498.1600000000003, "end": 2501.96, "text": " They didn't, like, their goal wasn't to come up with some, with some super clever new", "tokens": [814, 994, 380, 11, 411, 11, 641, 3387, 2067, 380, 281, 808, 493, 365, 512, 11, 365, 512, 1687, 13494, 777], "temperature": 0.0, "avg_logprob": -0.28146165211995444, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.00010067569382954389}, {"id": 654, "seek": 247636, "start": 2501.96, "end": 2502.96, "text": " pre-training technique.", "tokens": [659, 12, 17227, 1760, 6532, 13], "temperature": 0.0, "avg_logprob": -0.28146165211995444, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.00010067569382954389}, {"id": 655, "seek": 250296, "start": 2502.96, "end": 2506.84, "text": " Right, it was really just to carefully ablate every aspect, how much is model size matter,", "tokens": [1779, 11, 309, 390, 534, 445, 281, 7500, 410, 17593, 633, 4171, 11, 577, 709, 307, 2316, 2744, 1871, 11], "temperature": 0.0, "avg_logprob": -0.26483267979906094, "compression_ratio": 1.9461279461279462, "no_speech_prob": 0.000123366538900882}, {"id": 656, "seek": 250296, "start": 2506.84, "end": 2509.7200000000003, "text": " how much is training data matter, how much is clemeness of data matter, like, how much", "tokens": [577, 709, 307, 3097, 1412, 1871, 11, 577, 709, 307, 1233, 2558, 442, 295, 1412, 1871, 11, 411, 11, 577, 709], "temperature": 0.0, "avg_logprob": -0.26483267979906094, "compression_ratio": 1.9461279461279462, "no_speech_prob": 0.000123366538900882}, {"id": 657, "seek": 250296, "start": 2509.7200000000003, "end": 2513.08, "text": " is the exact way that you do the pre-training objective matter, like doing the masking,", "tokens": [307, 264, 1900, 636, 300, 291, 360, 264, 659, 12, 17227, 1760, 10024, 1871, 11, 411, 884, 264, 31226, 11], "temperature": 0.0, "avg_logprob": -0.26483267979906094, "compression_ratio": 1.9461279461279462, "no_speech_prob": 0.000123366538900882}, {"id": 658, "seek": 250296, "start": 2513.08, "end": 2514.7200000000003, "text": " like, how many spans do you mask?", "tokens": [411, 11, 577, 867, 44086, 360, 291, 6094, 30], "temperature": 0.0, "avg_logprob": -0.26483267979906094, "compression_ratio": 1.9461279461279462, "no_speech_prob": 0.000123366538900882}, {"id": 659, "seek": 250296, "start": 2514.7200000000003, "end": 2520.2, "text": " And so they wanted to kind of very clearly do the, and they also wanted to push the limits", "tokens": [400, 370, 436, 1415, 281, 733, 295, 588, 4448, 360, 264, 11, 293, 436, 611, 1415, 281, 2944, 264, 10406], "temperature": 0.0, "avg_logprob": -0.26483267979906094, "compression_ratio": 1.9461279461279462, "no_speech_prob": 0.000123366538900882}, {"id": 660, "seek": 250296, "start": 2520.2, "end": 2524.6, "text": " of size and say, what happens if we have 300 million, a billion, 10 billion parameters,", "tokens": [295, 2744, 293, 584, 11, 437, 2314, 498, 321, 362, 6641, 2459, 11, 257, 5218, 11, 1266, 5218, 9834, 11], "temperature": 0.0, "avg_logprob": -0.26483267979906094, "compression_ratio": 1.9461279461279462, "no_speech_prob": 0.000123366538900882}, {"id": 661, "seek": 250296, "start": 2524.6, "end": 2526.6, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.26483267979906094, "compression_ratio": 1.9461279461279462, "no_speech_prob": 0.000123366538900882}, {"id": 662, "seek": 250296, "start": 2526.6, "end": 2532.12, "text": " And then, so they did tons and tons of ablation and they got state of the art and everything", "tokens": [400, 550, 11, 370, 436, 630, 9131, 293, 9131, 295, 410, 24278, 293, 436, 658, 1785, 295, 264, 1523, 293, 1203], "temperature": 0.0, "avg_logprob": -0.26483267979906094, "compression_ratio": 1.9461279461279462, "no_speech_prob": 0.000123366538900882}, {"id": 663, "seek": 253212, "start": 2532.12, "end": 2534.4, "text": " and they're still sitting at the art and everything.", "tokens": [293, 436, 434, 920, 3798, 412, 264, 1523, 293, 1203, 13], "temperature": 0.0, "avg_logprob": -0.22827990849812826, "compression_ratio": 1.860377358490566, "no_speech_prob": 4.3305357394274324e-05}, {"id": 664, "seek": 253212, "start": 2534.4, "end": 2542.16, "text": " And the results, though, are a little bit bleak in the sense that nothing really mattered", "tokens": [400, 264, 3542, 11, 1673, 11, 366, 257, 707, 857, 5408, 514, 294, 264, 2020, 300, 1825, 534, 44282], "temperature": 0.0, "avg_logprob": -0.22827990849812826, "compression_ratio": 1.860377358490566, "no_speech_prob": 4.3305357394274324e-05}, {"id": 665, "seek": 253212, "start": 2542.16, "end": 2547.7599999999998, "text": " except making the data, like, like, all of the ablations, it wasn't like, oh, you know,", "tokens": [3993, 1455, 264, 1412, 11, 411, 11, 411, 11, 439, 295, 264, 410, 75, 763, 11, 309, 2067, 380, 411, 11, 1954, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.22827990849812826, "compression_ratio": 1.860377358490566, "no_speech_prob": 4.3305357394274324e-05}, {"id": 666, "seek": 253212, "start": 2547.7599999999998, "end": 2551.3599999999997, "text": " burnt it everything perfectly, it was that, it doesn't matter, like, you could do 20%,", "tokens": [18901, 309, 1203, 6239, 11, 309, 390, 300, 11, 309, 1177, 380, 1871, 11, 411, 11, 291, 727, 360, 945, 4, 11], "temperature": 0.0, "avg_logprob": -0.22827990849812826, "compression_ratio": 1.860377358490566, "no_speech_prob": 4.3305357394274324e-05}, {"id": 667, "seek": 253212, "start": 2551.3599999999997, "end": 2555.68, "text": " 25%, you can do this fine tuning recipe, this fine tuning recipe, it's like, all that", "tokens": [3552, 8923, 291, 393, 360, 341, 2489, 15164, 6782, 11, 341, 2489, 15164, 6782, 11, 309, 311, 411, 11, 439, 300], "temperature": 0.0, "avg_logprob": -0.22827990849812826, "compression_ratio": 1.860377358490566, "no_speech_prob": 4.3305357394274324e-05}, {"id": 668, "seek": 253212, "start": 2555.68, "end": 2560.3199999999997, "text": " really matters is making the model bigger and training it on a more data, and clean data.", "tokens": [534, 7001, 307, 1455, 264, 2316, 3801, 293, 3097, 309, 322, 257, 544, 1412, 11, 293, 2541, 1412, 13], "temperature": 0.0, "avg_logprob": -0.22827990849812826, "compression_ratio": 1.860377358490566, "no_speech_prob": 4.3305357394274324e-05}, {"id": 669, "seek": 256032, "start": 2560.32, "end": 2568.7200000000003, "text": " And so, yeah, it's a little bit of a bleak paper if you are hoping that there is exists", "tokens": [400, 370, 11, 1338, 11, 309, 311, 257, 707, 857, 295, 257, 5408, 514, 3035, 498, 291, 366, 7159, 300, 456, 307, 8198], "temperature": 0.0, "avg_logprob": -0.18046142941429502, "compression_ratio": 1.5968379446640317, "no_speech_prob": 8.21460344013758e-05}, {"id": 670, "seek": 256032, "start": 2568.7200000000003, "end": 2573.0, "text": " some pre-training technique which is super computationally efficient and also can get,", "tokens": [512, 659, 12, 17227, 1760, 6532, 597, 307, 1687, 24903, 379, 7148, 293, 611, 393, 483, 11], "temperature": 0.0, "avg_logprob": -0.18046142941429502, "compression_ratio": 1.5968379446640317, "no_speech_prob": 8.21460344013758e-05}, {"id": 671, "seek": 256032, "start": 2573.0, "end": 2576.32, "text": " you know, very impressive results, which I'm not saying there isn't, but, like, most", "tokens": [291, 458, 11, 588, 8992, 3542, 11, 597, 286, 478, 406, 1566, 456, 1943, 380, 11, 457, 11, 411, 11, 881], "temperature": 0.0, "avg_logprob": -0.18046142941429502, "compression_ratio": 1.5968379446640317, "no_speech_prob": 8.21460344013758e-05}, {"id": 672, "seek": 256032, "start": 2576.32, "end": 2578.92, "text": " of this evidence points in that.", "tokens": [295, 341, 4467, 2793, 294, 300, 13], "temperature": 0.0, "avg_logprob": -0.18046142941429502, "compression_ratio": 1.5968379446640317, "no_speech_prob": 8.21460344013758e-05}, {"id": 673, "seek": 256032, "start": 2578.92, "end": 2584.1200000000003, "text": " So the one kind of newest paper that is maybe the most positive in this direction is this", "tokens": [407, 264, 472, 733, 295, 17569, 3035, 300, 307, 1310, 264, 881, 3353, 294, 341, 3513, 307, 341], "temperature": 0.0, "avg_logprob": -0.18046142941429502, "compression_ratio": 1.5968379446640317, "no_speech_prob": 8.21460344013758e-05}, {"id": 674, "seek": 256032, "start": 2584.1200000000003, "end": 2586.84, "text": " paper called Electra.", "tokens": [3035, 1219, 12575, 424, 13], "temperature": 0.0, "avg_logprob": -0.18046142941429502, "compression_ratio": 1.5968379446640317, "no_speech_prob": 8.21460344013758e-05}, {"id": 675, "seek": 258684, "start": 2586.84, "end": 2594.08, "text": " And so, and so this is done by Kevin Clark from here and, uh, and Google Brain.", "tokens": [400, 370, 11, 293, 370, 341, 307, 1096, 538, 9954, 18572, 490, 510, 293, 11, 2232, 11, 293, 3329, 29783, 13], "temperature": 0.0, "avg_logprob": -0.19301294278697806, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.0001158276863861829}, {"id": 676, "seek": 258684, "start": 2594.08, "end": 2597.08, "text": " And so, yeah, in this one, it's a pretty clever idea.", "tokens": [400, 370, 11, 1338, 11, 294, 341, 472, 11, 309, 311, 257, 1238, 13494, 1558, 13], "temperature": 0.0, "avg_logprob": -0.19301294278697806, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.0001158276863861829}, {"id": 677, "seek": 258684, "start": 2597.08, "end": 2603.6000000000004, "text": " So basically, the idea is instead of training, instead of training to generate the output,", "tokens": [407, 1936, 11, 264, 1558, 307, 2602, 295, 3097, 11, 2602, 295, 3097, 281, 8460, 264, 5598, 11], "temperature": 0.0, "avg_logprob": -0.19301294278697806, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.0001158276863861829}, {"id": 678, "seek": 258684, "start": 2603.6000000000004, "end": 2605.56, "text": " you just train it as a, as a discriminator.", "tokens": [291, 445, 3847, 309, 382, 257, 11, 382, 257, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.19301294278697806, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.0001158276863861829}, {"id": 679, "seek": 258684, "start": 2605.56, "end": 2610.48, "text": " And so, you have a local language model, you have, you do some asking, you have a local", "tokens": [400, 370, 11, 291, 362, 257, 2654, 2856, 2316, 11, 291, 362, 11, 291, 360, 512, 3365, 11, 291, 362, 257, 2654], "temperature": 0.0, "avg_logprob": -0.19301294278697806, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.0001158276863861829}, {"id": 680, "seek": 258684, "start": 2610.48, "end": 2613.76, "text": " language model which replaces it, and then you train it to discriminate whether it's", "tokens": [2856, 2316, 597, 46734, 309, 11, 293, 550, 291, 3847, 309, 281, 47833, 1968, 309, 311], "temperature": 0.0, "avg_logprob": -0.19301294278697806, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.0001158276863861829}, {"id": 681, "seek": 261376, "start": 2613.76, "end": 2617.4, "text": " the original one or not.", "tokens": [264, 3380, 472, 420, 406, 13], "temperature": 0.0, "avg_logprob": -0.2843363587285431, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.20056816842407e-05}, {"id": 682, "seek": 261376, "start": 2617.4, "end": 2620.5200000000004, "text": " And so, the idea here is that you are doing a much, you're, you're, you're getting better", "tokens": [400, 370, 11, 264, 1558, 510, 307, 300, 291, 366, 884, 257, 709, 11, 291, 434, 11, 291, 434, 11, 291, 434, 1242, 1101], "temperature": 0.0, "avg_logprob": -0.2843363587285431, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.20056816842407e-05}, {"id": 683, "seek": 261376, "start": 2620.5200000000004, "end": 2626.5200000000004, "text": " sample efficiency for pre-training because you're predicting every, every word, which", "tokens": [6889, 10493, 337, 659, 12, 17227, 1760, 570, 291, 434, 32884, 633, 11, 633, 1349, 11, 597], "temperature": 0.0, "avg_logprob": -0.2843363587285431, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.20056816842407e-05}, {"id": 684, "seek": 261376, "start": 2626.5200000000004, "end": 2630.28, "text": " is actually, I mean, I don't know exactly why it would be that from, from, from, from", "tokens": [307, 767, 11, 286, 914, 11, 286, 500, 380, 458, 2293, 983, 309, 576, 312, 300, 490, 11, 490, 11, 490, 11, 490], "temperature": 0.0, "avg_logprob": -0.2843363587285431, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.20056816842407e-05}, {"id": 685, "seek": 261376, "start": 2630.28, "end": 2635.0, "text": " Berkis, Berkis still, uh, in terms of, because you don't replace with, with the mask with", "tokens": [5637, 74, 271, 11, 5637, 74, 271, 920, 11, 2232, 11, 294, 2115, 295, 11, 570, 291, 500, 380, 7406, 365, 11, 365, 264, 6094, 365], "temperature": 0.0, "avg_logprob": -0.2843363587285431, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.20056816842407e-05}, {"id": 686, "seek": 261376, "start": 2635.0, "end": 2636.6000000000004, "text": " everywhere, you also randomly corrupt it.", "tokens": [5315, 11, 291, 611, 16979, 17366, 309, 13], "temperature": 0.0, "avg_logprob": -0.2843363587285431, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.20056816842407e-05}, {"id": 687, "seek": 261376, "start": 2636.6000000000004, "end": 2641.88, "text": " But, but the, the, the biggest difference is that, um, is that these are kind of contextual", "tokens": [583, 11, 457, 264, 11, 264, 11, 264, 3880, 2649, 307, 300, 11, 1105, 11, 307, 300, 613, 366, 733, 295, 35526], "temperature": 0.0, "avg_logprob": -0.2843363587285431, "compression_ratio": 1.7894736842105263, "no_speech_prob": 6.20056816842407e-05}, {"id": 688, "seek": 264188, "start": 2641.88, "end": 2645.7200000000003, "text": " every place. So, it's like, it, when I did random masking, and replace with the random", "tokens": [633, 1081, 13, 407, 11, 309, 311, 411, 11, 309, 11, 562, 286, 630, 4974, 31226, 11, 293, 7406, 365, 264, 4974], "temperature": 0.2, "avg_logprob": -0.297367791334788, "compression_ratio": 1.9491525423728813, "no_speech_prob": 7.248390465974808e-05}, {"id": 689, "seek": 264188, "start": 2645.7200000000003, "end": 2646.7200000000003, "text": " word, it was truly a random word.", "tokens": [1349, 11, 309, 390, 4908, 257, 4974, 1349, 13], "temperature": 0.2, "avg_logprob": -0.297367791334788, "compression_ratio": 1.9491525423728813, "no_speech_prob": 7.248390465974808e-05}, {"id": 690, "seek": 264188, "start": 2646.7200000000003, "end": 2649.56, "text": " So, most of the time it was completely trivial to, to tell that this was not the right word.", "tokens": [407, 11, 881, 295, 264, 565, 309, 390, 2584, 26703, 281, 11, 281, 980, 300, 341, 390, 406, 264, 558, 1349, 13], "temperature": 0.2, "avg_logprob": -0.297367791334788, "compression_ratio": 1.9491525423728813, "no_speech_prob": 7.248390465974808e-05}, {"id": 691, "seek": 264188, "start": 2649.56, "end": 2652.44, "text": " You didn't necessarily know which word should be replaced, but in this case, they actually", "tokens": [509, 994, 380, 4725, 458, 597, 1349, 820, 312, 10772, 11, 457, 294, 341, 1389, 11, 436, 767], "temperature": 0.2, "avg_logprob": -0.297367791334788, "compression_ratio": 1.9491525423728813, "no_speech_prob": 7.248390465974808e-05}, {"id": 692, "seek": 264188, "start": 2652.44, "end": 2657.12, "text": " used a intentionally weak but still non-tribule language model to predict which word.", "tokens": [1143, 257, 22062, 5336, 457, 920, 2107, 12, 83, 2024, 2271, 2856, 2316, 281, 6069, 597, 1349, 13], "temperature": 0.2, "avg_logprob": -0.297367791334788, "compression_ratio": 1.9491525423728813, "no_speech_prob": 7.248390465974808e-05}, {"id": 693, "seek": 264188, "start": 2657.12, "end": 2661.0, "text": " So, like, this locally makes sense, the chef ate the meal, but it doesn't make any sense,", "tokens": [407, 11, 411, 11, 341, 16143, 1669, 2020, 11, 264, 10530, 8468, 264, 6791, 11, 457, 309, 1177, 380, 652, 604, 2020, 11], "temperature": 0.2, "avg_logprob": -0.297367791334788, "compression_ratio": 1.9491525423728813, "no_speech_prob": 7.248390465974808e-05}, {"id": 694, "seek": 264188, "start": 2661.0, "end": 2663.04, "text": " like, a very strong model will not predict this, right?", "tokens": [411, 11, 257, 588, 2068, 2316, 486, 406, 6069, 341, 11, 558, 30], "temperature": 0.2, "avg_logprob": -0.297367791334788, "compression_ratio": 1.9491525423728813, "no_speech_prob": 7.248390465974808e-05}, {"id": 695, "seek": 264188, "start": 2663.04, "end": 2667.0, "text": " So, so, that's the idea that you, use a weak model to, to, to, to, to, to the substitution", "tokens": [407, 11, 370, 11, 300, 311, 264, 1558, 300, 291, 11, 764, 257, 5336, 2316, 281, 11, 281, 11, 281, 11, 281, 11, 281, 11, 281, 264, 35827], "temperature": 0.2, "avg_logprob": -0.297367791334788, "compression_ratio": 1.9491525423728813, "no_speech_prob": 7.248390465974808e-05}, {"id": 696, "seek": 264188, "start": 2667.0, "end": 2669.92, "text": " of the use, then you train a strong model to, to, um, do this.", "tokens": [295, 264, 764, 11, 550, 291, 3847, 257, 2068, 2316, 281, 11, 281, 11, 1105, 11, 360, 341, 13], "temperature": 0.2, "avg_logprob": -0.297367791334788, "compression_ratio": 1.9491525423728813, "no_speech_prob": 7.248390465974808e-05}, {"id": 697, "seek": 266992, "start": 2669.92, "end": 2676.44, "text": " So, these results are, I guess it's a big cable, but these results are, they're certainly", "tokens": [407, 11, 613, 3542, 366, 11, 286, 2041, 309, 311, 257, 955, 8220, 11, 457, 613, 3542, 366, 11, 436, 434, 3297], "temperature": 0.0, "avg_logprob": -0.28595294044131325, "compression_ratio": 1.8487804878048781, "no_speech_prob": 9.166907693725079e-05}, {"id": 698, "seek": 266992, "start": 2676.44, "end": 2684.04, "text": " positive with regard to, uh, previous results in terms of compute versus, um, so, like,", "tokens": [3353, 365, 3843, 281, 11, 2232, 11, 3894, 3542, 294, 2115, 295, 14722, 5717, 11, 1105, 11, 370, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.28595294044131325, "compression_ratio": 1.8487804878048781, "no_speech_prob": 9.166907693725079e-05}, {"id": 699, "seek": 266992, "start": 2684.04, "end": 2691.2000000000003, "text": " for, if we compare this row, so, uh, which is, one tenth of the compute of Bert large,", "tokens": [337, 11, 498, 321, 6794, 341, 5386, 11, 370, 11, 2232, 11, 597, 307, 11, 472, 27269, 295, 264, 14722, 295, 29594, 2416, 11], "temperature": 0.0, "avg_logprob": -0.28595294044131325, "compression_ratio": 1.8487804878048781, "no_speech_prob": 9.166907693725079e-05}, {"id": 700, "seek": 266992, "start": 2691.2000000000003, "end": 2693.88, "text": " to Bert base, which is also one tenth of the compute of Bert large, it certainly does", "tokens": [281, 29594, 3096, 11, 597, 307, 611, 472, 27269, 295, 264, 14722, 295, 29594, 2416, 11, 309, 3297, 775], "temperature": 0.0, "avg_logprob": -0.28595294044131325, "compression_ratio": 1.8487804878048781, "no_speech_prob": 9.166907693725079e-05}, {"id": 701, "seek": 266992, "start": 2693.88, "end": 2695.52, "text": " a lot better than Bert base.", "tokens": [257, 688, 1101, 813, 29594, 3096, 13], "temperature": 0.0, "avg_logprob": -0.28595294044131325, "compression_ratio": 1.8487804878048781, "no_speech_prob": 9.166907693725079e-05}, {"id": 702, "seek": 269552, "start": 2695.52, "end": 2705.2, "text": " Um, but, when they, uh, if you, but, but in terms of state of the art models, um, when", "tokens": [3301, 11, 457, 11, 562, 436, 11, 2232, 11, 498, 291, 11, 457, 11, 457, 294, 2115, 295, 1785, 295, 264, 1523, 5245, 11, 1105, 11, 562], "temperature": 0.0, "avg_logprob": -0.19448734584607577, "compression_ratio": 1.8941176470588235, "no_speech_prob": 3.480492887319997e-05}, {"id": 703, "seek": 269552, "start": 2705.2, "end": 2711.12, "text": " they do, you know, the same amount of, uh, compute as their Bert large, which is this one,", "tokens": [436, 360, 11, 291, 458, 11, 264, 912, 2372, 295, 11, 2232, 11, 14722, 382, 641, 29594, 2416, 11, 597, 307, 341, 472, 11], "temperature": 0.0, "avg_logprob": -0.19448734584607577, "compression_ratio": 1.8941176470588235, "no_speech_prob": 3.480492887319997e-05}, {"id": 704, "seek": 269552, "start": 2711.12, "end": 2713.96, "text": " work a better, to other state of the art models, they're not, in order to get state of", "tokens": [589, 257, 1101, 11, 281, 661, 1785, 295, 264, 1523, 5245, 11, 436, 434, 406, 11, 294, 1668, 281, 483, 1785, 295], "temperature": 0.0, "avg_logprob": -0.19448734584607577, "compression_ratio": 1.8941176470588235, "no_speech_prob": 3.480492887319997e-05}, {"id": 705, "seek": 269552, "start": 2713.96, "end": 2716.7599999999998, "text": " the art, or to get similar to state of the art, they basically need to do as much compute", "tokens": [264, 1523, 11, 420, 281, 483, 2531, 281, 1785, 295, 264, 1523, 11, 436, 1936, 643, 281, 360, 382, 709, 14722], "temperature": 0.0, "avg_logprob": -0.19448734584607577, "compression_ratio": 1.8941176470588235, "no_speech_prob": 3.480492887319997e-05}, {"id": 706, "seek": 269552, "start": 2716.7599999999998, "end": 2719.8, "text": " as state of the art, so, like, 444x, 5.4x.", "tokens": [382, 1785, 295, 264, 1523, 11, 370, 11, 411, 11, 1017, 13912, 87, 11, 1025, 13, 19, 87, 13], "temperature": 0.0, "avg_logprob": -0.19448734584607577, "compression_ratio": 1.8941176470588235, "no_speech_prob": 3.480492887319997e-05}, {"id": 707, "seek": 269552, "start": 2719.8, "end": 2723.56, "text": " So, I mean, at scale-down values, they were able to do better, but this is a, still a", "tokens": [407, 11, 286, 914, 11, 412, 4373, 12, 5093, 4190, 11, 436, 645, 1075, 281, 360, 1101, 11, 457, 341, 307, 257, 11, 920, 257], "temperature": 0.0, "avg_logprob": -0.19448734584607577, "compression_ratio": 1.8941176470588235, "no_speech_prob": 3.480492887319997e-05}, {"id": 708, "seek": 272356, "start": 2723.56, "end": 2725.56, "text": " pretty big gap, like, 4 points.", "tokens": [1238, 955, 7417, 11, 411, 11, 1017, 2793, 13], "temperature": 0.0, "avg_logprob": -0.20782556459885235, "compression_ratio": 1.6023166023166022, "no_speech_prob": 8.09081393526867e-05}, {"id": 709, "seek": 272356, "start": 2725.56, "end": 2729.48, "text": " Um, so, it's, it's positive, but it's not, it's certainly not like the silver bullet", "tokens": [3301, 11, 370, 11, 309, 311, 11, 309, 311, 3353, 11, 457, 309, 311, 406, 11, 309, 311, 3297, 406, 411, 264, 8753, 11632], "temperature": 0.0, "avg_logprob": -0.20782556459885235, "compression_ratio": 1.6023166023166022, "no_speech_prob": 8.09081393526867e-05}, {"id": 710, "seek": 272356, "start": 2729.48, "end": 2737.88, "text": " in terms of, uh, showing that, uh, we can, you know, pre-trained models, much better", "tokens": [294, 2115, 295, 11, 2232, 11, 4099, 300, 11, 2232, 11, 321, 393, 11, 291, 458, 11, 659, 12, 17227, 2001, 5245, 11, 709, 1101], "temperature": 0.0, "avg_logprob": -0.20782556459885235, "compression_ratio": 1.6023166023166022, "no_speech_prob": 8.09081393526867e-05}, {"id": 711, "seek": 272356, "start": 2737.88, "end": 2739.84, "text": " for, for, for cheaper.", "tokens": [337, 11, 337, 11, 337, 12284, 13], "temperature": 0.0, "avg_logprob": -0.20782556459885235, "compression_ratio": 1.6023166023166022, "no_speech_prob": 8.09081393526867e-05}, {"id": 712, "seek": 272356, "start": 2739.84, "end": 2744.2, "text": " So, but, so, the last thing I want to talk about is, um, how we actually serve these models,", "tokens": [407, 11, 457, 11, 370, 11, 264, 1036, 551, 286, 528, 281, 751, 466, 307, 11, 1105, 11, 577, 321, 767, 4596, 613, 5245, 11], "temperature": 0.0, "avg_logprob": -0.20782556459885235, "compression_ratio": 1.6023166023166022, "no_speech_prob": 8.09081393526867e-05}, {"id": 713, "seek": 272356, "start": 2744.2, "end": 2745.2, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.20782556459885235, "compression_ratio": 1.6023166023166022, "no_speech_prob": 8.09081393526867e-05}, {"id": 714, "seek": 272356, "start": 2745.2, "end": 2750.72, "text": " Because, you know, I've said that, like, they're incredibly expensive to train, and nobody", "tokens": [1436, 11, 291, 458, 11, 286, 600, 848, 300, 11, 411, 11, 436, 434, 6252, 5124, 281, 3847, 11, 293, 5079], "temperature": 0.0, "avg_logprob": -0.20782556459885235, "compression_ratio": 1.6023166023166022, "no_speech_prob": 8.09081393526867e-05}, {"id": 715, "seek": 275072, "start": 2750.72, "end": 2754.6, "text": " has been able to figure out how to make that faster, but, you know, they're being used all", "tokens": [575, 668, 1075, 281, 2573, 484, 577, 281, 652, 300, 4663, 11, 457, 11, 291, 458, 11, 436, 434, 885, 1143, 439], "temperature": 0.0, "avg_logprob": -0.2744643169900645, "compression_ratio": 1.7428571428571429, "no_speech_prob": 5.9160138334846124e-05}, {"id": 716, "seek": 275072, "start": 2754.6, "end": 2755.6, "text": " over the place, right?", "tokens": [670, 264, 1081, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2744643169900645, "compression_ratio": 1.7428571428571429, "no_speech_prob": 5.9160138334846124e-05}, {"id": 717, "seek": 275072, "start": 2755.6, "end": 2760.16, "text": " So, like, uh, you know, there's news stories, Google has improved 10% of searches by", "tokens": [407, 11, 411, 11, 2232, 11, 291, 458, 11, 456, 311, 2583, 3676, 11, 3329, 575, 9689, 1266, 4, 295, 26701, 538], "temperature": 0.0, "avg_logprob": -0.2744643169900645, "compression_ratio": 1.7428571428571429, "no_speech_prob": 5.9160138334846124e-05}, {"id": 718, "seek": 275072, "start": 2760.16, "end": 2763.12, "text": " language-assigning, say, a little bit, and then Bing says it's been playing Burt since", "tokens": [2856, 12, 640, 9676, 11, 584, 11, 257, 707, 857, 11, 293, 550, 30755, 1619, 309, 311, 668, 2433, 363, 6224, 1670], "temperature": 0.0, "avg_logprob": -0.2744643169900645, "compression_ratio": 1.7428571428571429, "no_speech_prob": 5.9160138334846124e-05}, {"id": 719, "seek": 275072, "start": 2763.12, "end": 2766.72, "text": " April, so, and so, this, this is live in Google Search, and Bing Search, and so, these", "tokens": [6929, 11, 370, 11, 293, 370, 11, 341, 11, 341, 307, 1621, 294, 3329, 17180, 11, 293, 30755, 17180, 11, 293, 370, 11, 613], "temperature": 0.0, "avg_logprob": -0.2744643169900645, "compression_ratio": 1.7428571428571429, "no_speech_prob": 5.9160138334846124e-05}, {"id": 720, "seek": 275072, "start": 2766.72, "end": 2770.0, "text": " are, like, really low-lanancy services, right, that have, like, a few milliseconds of,", "tokens": [366, 11, 411, 11, 534, 2295, 12, 8658, 6717, 3328, 11, 558, 11, 300, 362, 11, 411, 11, 257, 1326, 34184, 295, 11], "temperature": 0.0, "avg_logprob": -0.2744643169900645, "compression_ratio": 1.7428571428571429, "no_speech_prob": 5.9160138334846124e-05}, {"id": 721, "seek": 275072, "start": 2770.0, "end": 2777.52, "text": " of, of latency, and they serve, you know, billions of, of queries a day, so, how, how are", "tokens": [295, 11, 295, 27043, 11, 293, 436, 4596, 11, 291, 458, 11, 17375, 295, 11, 295, 24109, 257, 786, 11, 370, 11, 577, 11, 577, 366], "temperature": 0.0, "avg_logprob": -0.2744643169900645, "compression_ratio": 1.7428571428571429, "no_speech_prob": 5.9160138334846124e-05}, {"id": 722, "seek": 277752, "start": 2777.52, "end": 2782.52, "text": " they doing this, is it just, like, uh, that, you know, Google and Microsoft are spending", "tokens": [436, 884, 341, 11, 307, 309, 445, 11, 411, 11, 2232, 11, 300, 11, 291, 458, 11, 3329, 293, 8116, 366, 6434], "temperature": 0.0, "avg_logprob": -0.23231618063790457, "compression_ratio": 1.7859778597785978, "no_speech_prob": 3.319371535326354e-05}, {"id": 723, "seek": 277752, "start": 2782.52, "end": 2783.72, "text": " billions of dollars on hardware.", "tokens": [17375, 295, 3808, 322, 8837, 13], "temperature": 0.0, "avg_logprob": -0.23231618063790457, "compression_ratio": 1.7859778597785978, "no_speech_prob": 3.319371535326354e-05}, {"id": 724, "seek": 277752, "start": 2783.72, "end": 2785.48, "text": " What they are, but not just for this, right?", "tokens": [708, 436, 366, 11, 457, 406, 445, 337, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23231618063790457, "compression_ratio": 1.7859778597785978, "no_speech_prob": 3.319371535326354e-05}, {"id": 725, "seek": 277752, "start": 2785.48, "end": 2790.72, "text": " And so, like, uh, like, it would, it would cost billions of dollars just to serve this", "tokens": [400, 370, 11, 411, 11, 2232, 11, 411, 11, 309, 576, 11, 309, 576, 2063, 17375, 295, 3808, 445, 281, 4596, 341], "temperature": 0.0, "avg_logprob": -0.23231618063790457, "compression_ratio": 1.7859778597785978, "no_speech_prob": 3.319371535326354e-05}, {"id": 726, "seek": 277752, "start": 2790.72, "end": 2792.24, "text": " if we were actually serving Burt.", "tokens": [498, 321, 645, 767, 8148, 363, 6224, 13], "temperature": 0.0, "avg_logprob": -0.23231618063790457, "compression_ratio": 1.7859778597785978, "no_speech_prob": 3.319371535326354e-05}, {"id": 727, "seek": 277752, "start": 2792.24, "end": 2796.2, "text": " But, we're serving, uh, not, instead of we're using, we're using model distillation, right?", "tokens": [583, 11, 321, 434, 8148, 11, 2232, 11, 406, 11, 2602, 295, 321, 434, 1228, 11, 321, 434, 1228, 2316, 42923, 399, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23231618063790457, "compression_ratio": 1.7859778597785978, "no_speech_prob": 3.319371535326354e-05}, {"id": 728, "seek": 277752, "start": 2796.2, "end": 2798.0, "text": " So, this has been around for a while.", "tokens": [407, 11, 341, 575, 668, 926, 337, 257, 1339, 13], "temperature": 0.0, "avg_logprob": -0.23231618063790457, "compression_ratio": 1.7859778597785978, "no_speech_prob": 3.319371535326354e-05}, {"id": 729, "seek": 277752, "start": 2798.0, "end": 2802.08, "text": " Um, so, it's, you know, called distillation and model compression.", "tokens": [3301, 11, 370, 11, 309, 311, 11, 291, 458, 11, 1219, 42923, 399, 293, 2316, 19355, 13], "temperature": 0.0, "avg_logprob": -0.23231618063790457, "compression_ratio": 1.7859778597785978, "no_speech_prob": 3.319371535326354e-05}, {"id": 730, "seek": 280208, "start": 2802.08, "end": 2807.64, "text": " Uh, one, one of the first papers was the Smolk Compression Paper, um, that was, that was", "tokens": [4019, 11, 472, 11, 472, 295, 264, 700, 10577, 390, 264, 3915, 401, 74, 6620, 2775, 24990, 11, 1105, 11, 300, 390, 11, 300, 390], "temperature": 0.0, "avg_logprob": -0.29318740740925275, "compression_ratio": 1.8201438848920863, "no_speech_prob": 3.3702552173053846e-05}, {"id": 731, "seek": 280208, "start": 2807.64, "end": 2812.6, "text": " done for, uh, I forget exactly what task, but then, and then Hinton's paper, just stealing", "tokens": [1096, 337, 11, 2232, 11, 286, 2870, 2293, 437, 5633, 11, 457, 550, 11, 293, 550, 389, 12442, 311, 3035, 11, 445, 19757], "temperature": 0.0, "avg_logprob": -0.29318740740925275, "compression_ratio": 1.8201438848920863, "no_speech_prob": 3.3702552173053846e-05}, {"id": 732, "seek": 280208, "start": 2812.6, "end": 2815.48, "text": " knowledge in neural networks, is a more well-known version of, or not, not version, but", "tokens": [3601, 294, 18161, 9590, 11, 307, 257, 544, 731, 12, 6861, 3037, 295, 11, 420, 406, 11, 406, 3037, 11, 457], "temperature": 0.0, "avg_logprob": -0.29318740740925275, "compression_ratio": 1.8201438848920863, "no_speech_prob": 3.3702552173053846e-05}, {"id": 733, "seek": 280208, "start": 2815.48, "end": 2819.56, "text": " a more well-known, uh, uh, paper on, on distillation.", "tokens": [257, 544, 731, 12, 6861, 11, 2232, 11, 2232, 11, 3035, 322, 11, 322, 42923, 399, 13], "temperature": 0.0, "avg_logprob": -0.29318740740925275, "compression_ratio": 1.8201438848920863, "no_speech_prob": 3.3702552173053846e-05}, {"id": 734, "seek": 280208, "start": 2819.56, "end": 2822.96, "text": " But in reality, the one, the version that, that we use at Google and the version that", "tokens": [583, 294, 4103, 11, 264, 472, 11, 264, 3037, 300, 11, 300, 321, 764, 412, 3329, 293, 264, 3037, 300], "temperature": 0.0, "avg_logprob": -0.29318740740925275, "compression_ratio": 1.8201438848920863, "no_speech_prob": 3.3702552173053846e-05}, {"id": 735, "seek": 280208, "start": 2822.96, "end": 2828.72, "text": " most people use when they say model distillation for, uh, pre-shanked language models, it's a, um,", "tokens": [881, 561, 764, 562, 436, 584, 2316, 42923, 399, 337, 11, 2232, 11, 659, 12, 2716, 657, 292, 2856, 5245, 11, 309, 311, 257, 11, 1105, 11], "temperature": 0.0, "avg_logprob": -0.29318740740925275, "compression_ratio": 1.8201438848920863, "no_speech_prob": 3.3702552173053846e-05}, {"id": 736, "seek": 282872, "start": 2828.72, "end": 2832.6, "text": " it's a very simple technique, but, but it's easy to misinterpret what, what, what we mean.", "tokens": [309, 311, 257, 588, 2199, 6532, 11, 457, 11, 457, 309, 311, 1858, 281, 3346, 41935, 437, 11, 437, 11, 437, 321, 914, 13], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 737, "seek": 282872, "start": 2832.6, "end": 2837.24, "text": " So, what we do is we pre-train, we train a state of the art model, whichever is the", "tokens": [407, 11, 437, 321, 360, 307, 321, 659, 12, 83, 7146, 11, 321, 3847, 257, 1785, 295, 264, 1523, 2316, 11, 24123, 307, 264], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 738, "seek": 282872, "start": 2837.24, "end": 2839.4399999999996, "text": " ones we can most afford to train, right?", "tokens": [2306, 321, 393, 881, 6157, 281, 3847, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 739, "seek": 282872, "start": 2839.4399999999996, "end": 2842.64, "text": " Because, of course, we can just make it bigger, but we, we set some budget of, you know,", "tokens": [1436, 11, 295, 1164, 11, 321, 393, 445, 652, 309, 3801, 11, 457, 321, 11, 321, 992, 512, 4706, 295, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 740, "seek": 282872, "start": 2842.64, "end": 2845.7599999999998, "text": " we want to train it for a day on some number of TPUs.", "tokens": [321, 528, 281, 3847, 309, 337, 257, 786, 322, 512, 1230, 295, 314, 8115, 82, 13], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 741, "seek": 282872, "start": 2845.7599999999998, "end": 2847.2799999999997, "text": " And then, we fine-tune it, right?", "tokens": [400, 550, 11, 321, 2489, 12, 83, 2613, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 742, "seek": 282872, "start": 2847.2799999999997, "end": 2849.7999999999997, "text": " So we get a model that's the maximum accuracy, and that's our teacher model, and this is", "tokens": [407, 321, 483, 257, 2316, 300, 311, 264, 6674, 14170, 11, 293, 300, 311, 527, 5027, 2316, 11, 293, 341, 307], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 743, "seek": 282872, "start": 2849.7999999999997, "end": 2850.7999999999997, "text": " expensive.", "tokens": [5124, 13], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 744, "seek": 282872, "start": 2850.7999999999997, "end": 2854.7999999999997, "text": " Then we have a, a large amount of unlabeled input, which is typically for, for most industry", "tokens": [1396, 321, 362, 257, 11, 257, 2416, 2372, 295, 32118, 18657, 292, 4846, 11, 597, 307, 5850, 337, 11, 337, 881, 3518], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 745, "seek": 282872, "start": 2854.7999999999997, "end": 2858.48, "text": " applications, you have unlabeled input, because you have, you know, in search, you have,", "tokens": [5821, 11, 291, 362, 32118, 18657, 292, 4846, 11, 570, 291, 362, 11, 291, 458, 11, 294, 3164, 11, 291, 362, 11], "temperature": 0.0, "avg_logprob": -0.21797078578316387, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.00017115824448410422}, {"id": 746, "seek": 285848, "start": 2858.48, "end": 2861.04, "text": " this is what they use to search for, this is what they click on, that's how it's searched", "tokens": [341, 307, 437, 436, 764, 281, 3164, 337, 11, 341, 307, 437, 436, 2052, 322, 11, 300, 311, 577, 309, 311, 22961], "temperature": 0.0, "avg_logprob": -0.22260801494121552, "compression_ratio": 1.821705426356589, "no_speech_prob": 6.388259498635307e-05}, {"id": 747, "seek": 285848, "start": 2861.04, "end": 2862.04, "text": " into the trained.", "tokens": [666, 264, 8895, 13], "temperature": 0.0, "avg_logprob": -0.22260801494121552, "compression_ratio": 1.821705426356589, "no_speech_prob": 6.388259498635307e-05}, {"id": 748, "seek": 285848, "start": 2862.04, "end": 2868.12, "text": " Uh, and so, you can then just take these, and you, um, and then you just label your examples", "tokens": [4019, 11, 293, 370, 11, 291, 393, 550, 445, 747, 613, 11, 293, 291, 11, 1105, 11, 293, 550, 291, 445, 7645, 428, 5110], "temperature": 0.0, "avg_logprob": -0.22260801494121552, "compression_ratio": 1.821705426356589, "no_speech_prob": 6.388259498635307e-05}, {"id": 749, "seek": 285848, "start": 2868.12, "end": 2869.12, "text": " with them.", "tokens": [365, 552, 13], "temperature": 0.0, "avg_logprob": -0.22260801494121552, "compression_ratio": 1.821705426356589, "no_speech_prob": 6.388259498635307e-05}, {"id": 750, "seek": 285848, "start": 2869.12, "end": 2872.88, "text": " So you can get billions of these, uh, if you actually want a real service.", "tokens": [407, 291, 393, 483, 17375, 295, 613, 11, 2232, 11, 498, 291, 767, 528, 257, 957, 2643, 13], "temperature": 0.0, "avg_logprob": -0.22260801494121552, "compression_ratio": 1.821705426356589, "no_speech_prob": 6.388259498635307e-05}, {"id": 751, "seek": 285848, "start": 2872.88, "end": 2878.28, "text": " And then you, so then you, then you run these, you know, query answer pairs through your", "tokens": [400, 550, 291, 11, 370, 550, 291, 11, 550, 291, 1190, 613, 11, 291, 458, 11, 14581, 1867, 15494, 807, 428], "temperature": 0.0, "avg_logprob": -0.22260801494121552, "compression_ratio": 1.821705426356589, "no_speech_prob": 6.388259498635307e-05}, {"id": 752, "seek": 285848, "start": 2878.28, "end": 2883.48, "text": " teacher, and you get a pseudo label, and you're just training much smaller model, much meaning", "tokens": [5027, 11, 293, 291, 483, 257, 35899, 7645, 11, 293, 291, 434, 445, 3097, 709, 4356, 2316, 11, 709, 3620], "temperature": 0.0, "avg_logprob": -0.22260801494121552, "compression_ratio": 1.821705426356589, "no_speech_prob": 6.388259498635307e-05}, {"id": 753, "seek": 288348, "start": 2883.48, "end": 2888.88, "text": " like 50 times, 100 times smaller, to, uh, predict your student, your teacher outputs.", "tokens": [411, 2625, 1413, 11, 2319, 1413, 4356, 11, 281, 11, 2232, 11, 6069, 428, 3107, 11, 428, 5027, 23930, 13], "temperature": 0.0, "avg_logprob": -0.17940098530537374, "compression_ratio": 1.7275641025641026, "no_speech_prob": 3.8223297451622784e-05}, {"id": 754, "seek": 288348, "start": 2888.88, "end": 2891.4, "text": " And so, and you can generally do this for most techniques.", "tokens": [400, 370, 11, 293, 291, 393, 5101, 360, 341, 337, 881, 7512, 13], "temperature": 0.0, "avg_logprob": -0.17940098530537374, "compression_ratio": 1.7275641025641026, "no_speech_prob": 3.8223297451622784e-05}, {"id": 755, "seek": 288348, "start": 2891.4, "end": 2897.52, "text": " I mean, for most tasks, you can do this, uh, pretty easily, and get a huge 50-200X, uh,", "tokens": [286, 914, 11, 337, 881, 9608, 11, 291, 393, 360, 341, 11, 2232, 11, 1238, 3612, 11, 293, 483, 257, 2603, 2625, 12, 7629, 55, 11, 2232, 11], "temperature": 0.0, "avg_logprob": -0.17940098530537374, "compression_ratio": 1.7275641025641026, "no_speech_prob": 3.8223297451622784e-05}, {"id": 756, "seek": 288348, "start": 2897.52, "end": 2899.92, "text": " compression with no degradation.", "tokens": [19355, 365, 572, 40519, 13], "temperature": 0.0, "avg_logprob": -0.17940098530537374, "compression_ratio": 1.7275641025641026, "no_speech_prob": 3.8223297451622784e-05}, {"id": 757, "seek": 288348, "start": 2899.92, "end": 2903.48, "text": " But the important thing to realize that we're not compressing the pre-train model itself.", "tokens": [583, 264, 1021, 551, 281, 4325, 300, 321, 434, 406, 14778, 278, 264, 659, 12, 83, 7146, 2316, 2564, 13], "temperature": 0.0, "avg_logprob": -0.17940098530537374, "compression_ratio": 1.7275641025641026, "no_speech_prob": 3.8223297451622784e-05}, {"id": 758, "seek": 288348, "start": 2903.48, "end": 2905.32, "text": " We haven't really had any luck doing that.", "tokens": [492, 2378, 380, 534, 632, 604, 3668, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.17940098530537374, "compression_ratio": 1.7275641025641026, "no_speech_prob": 3.8223297451622784e-05}, {"id": 759, "seek": 288348, "start": 2905.32, "end": 2908.96, "text": " So like, you can't actually just take Bert, and then compress it to a smaller model, which", "tokens": [407, 411, 11, 291, 393, 380, 767, 445, 747, 29594, 11, 293, 550, 14778, 309, 281, 257, 4356, 2316, 11, 597], "temperature": 0.0, "avg_logprob": -0.17940098530537374, "compression_ratio": 1.7275641025641026, "no_speech_prob": 3.8223297451622784e-05}, {"id": 760, "seek": 288348, "start": 2908.96, "end": 2910.52, "text": " you can then fine-tune for all these other tasks.", "tokens": [291, 393, 550, 2489, 12, 83, 2613, 337, 439, 613, 661, 9608, 13], "temperature": 0.0, "avg_logprob": -0.17940098530537374, "compression_ratio": 1.7275641025641026, "no_speech_prob": 3.8223297451622784e-05}, {"id": 761, "seek": 291052, "start": 2910.52, "end": 2914.7599999999998, "text": " It's only after you've chosen the task, and after you find it, tune it for the task that", "tokens": [467, 311, 787, 934, 291, 600, 8614, 264, 5633, 11, 293, 934, 291, 915, 309, 11, 10864, 309, 337, 264, 5633, 300], "temperature": 0.0, "avg_logprob": -0.2858047225848347, "compression_ratio": 1.730909090909091, "no_speech_prob": 2.8847240173490718e-05}, {"id": 762, "seek": 291052, "start": 2914.7599999999998, "end": 2917.48, "text": " you, that we were able to do it.", "tokens": [291, 11, 300, 321, 645, 1075, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.2858047225848347, "compression_ratio": 1.730909090909091, "no_speech_prob": 2.8847240173490718e-05}, {"id": 763, "seek": 291052, "start": 2917.48, "end": 2923.0, "text": " So to show some specific results, so let's say we have, let's say we have a Bert large", "tokens": [407, 281, 855, 512, 2685, 3542, 11, 370, 718, 311, 584, 321, 362, 11, 718, 311, 584, 321, 362, 257, 29594, 2416], "temperature": 0.0, "avg_logprob": -0.2858047225848347, "compression_ratio": 1.730909090909091, "no_speech_prob": 2.8847240173490718e-05}, {"id": 764, "seek": 291052, "start": 2923.0, "end": 2924.0, "text": " teacher.", "tokens": [5027, 13], "temperature": 0.0, "avg_logprob": -0.2858047225848347, "compression_ratio": 1.730909090909091, "no_speech_prob": 2.8847240173490718e-05}, {"id": 765, "seek": 291052, "start": 2924.0, "end": 2925.0, "text": " So this is an Amazon book review script.", "tokens": [407, 341, 307, 364, 6795, 1446, 3131, 5755, 13], "temperature": 0.0, "avg_logprob": -0.2858047225848347, "compression_ratio": 1.730909090909091, "no_speech_prob": 2.8847240173490718e-05}, {"id": 766, "seek": 291052, "start": 2925.0, "end": 2928.8, "text": " So this is a paper that, that, I've got to cite it, but this is a paper that my group,", "tokens": [407, 341, 307, 257, 3035, 300, 11, 300, 11, 286, 600, 658, 281, 37771, 309, 11, 457, 341, 307, 257, 3035, 300, 452, 1594, 11], "temperature": 0.0, "avg_logprob": -0.2858047225848347, "compression_ratio": 1.730909090909091, "no_speech_prob": 2.8847240173490718e-05}, {"id": 767, "seek": 291052, "start": 2928.8, "end": 2931.7599999999998, "text": " uh, published, you know, the ATURK, uh, wrote.", "tokens": [2232, 11, 6572, 11, 291, 458, 11, 264, 8872, 7932, 42, 11, 2232, 11, 4114, 13], "temperature": 0.0, "avg_logprob": -0.2858047225848347, "compression_ratio": 1.730909090909091, "no_speech_prob": 2.8847240173490718e-05}, {"id": 768, "seek": 291052, "start": 2931.7599999999998, "end": 2939.88, "text": " And so, um, this has 50,000 labeled examples and 8 million, uh, unlabeled examples.", "tokens": [400, 370, 11, 1105, 11, 341, 575, 2625, 11, 1360, 21335, 5110, 293, 1649, 2459, 11, 2232, 11, 32118, 18657, 292, 5110, 13], "temperature": 0.0, "avg_logprob": -0.2858047225848347, "compression_ratio": 1.730909090909091, "no_speech_prob": 2.8847240173490718e-05}, {"id": 769, "seek": 293988, "start": 2939.88, "end": 2943.56, "text": " So you, you, you, you find tune on, you pre-train Bert large, normal, like you take the pre-train", "tokens": [407, 291, 11, 291, 11, 291, 11, 291, 915, 10864, 322, 11, 291, 659, 12, 83, 7146, 29594, 2416, 11, 2710, 11, 411, 291, 747, 264, 659, 12, 83, 7146], "temperature": 0.0, "avg_logprob": -0.2392268034578101, "compression_ratio": 1.950381679389313, "no_speech_prob": 3.420898428885266e-05}, {"id": 770, "seek": 293988, "start": 2943.56, "end": 2948.6, "text": " Bert large, you, uh, you find tune on these 50,000 labels, you get this 88% accuracy,", "tokens": [29594, 2416, 11, 291, 11, 2232, 11, 291, 915, 10864, 322, 613, 2625, 11, 1360, 16949, 11, 291, 483, 341, 24587, 4, 14170, 11], "temperature": 0.0, "avg_logprob": -0.2392268034578101, "compression_ratio": 1.950381679389313, "no_speech_prob": 3.420898428885266e-05}, {"id": 771, "seek": 293988, "start": 2948.6, "end": 2949.92, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.2392268034578101, "compression_ratio": 1.950381679389313, "no_speech_prob": 3.420898428885266e-05}, {"id": 772, "seek": 293988, "start": 2949.92, "end": 2955.52, "text": " Then, uh, and so, but then now, let's say instead of using Bert large, you used a much smaller", "tokens": [1396, 11, 2232, 11, 293, 370, 11, 457, 550, 586, 11, 718, 311, 584, 2602, 295, 1228, 29594, 2416, 11, 291, 1143, 257, 709, 4356], "temperature": 0.0, "avg_logprob": -0.2392268034578101, "compression_ratio": 1.950381679389313, "no_speech_prob": 3.420898428885266e-05}, {"id": 773, "seek": 293988, "start": 2955.52, "end": 2956.52, "text": " version.", "tokens": [3037, 13], "temperature": 0.0, "avg_logprob": -0.2392268034578101, "compression_ratio": 1.950381679389313, "no_speech_prob": 3.420898428885266e-05}, {"id": 774, "seek": 293988, "start": 2956.52, "end": 2960.8, "text": " So this one's, according to the size, this one's, you know, uh, a 16th size, whatever,", "tokens": [407, 341, 472, 311, 11, 4650, 281, 264, 2744, 11, 341, 472, 311, 11, 291, 458, 11, 2232, 11, 257, 3165, 392, 2744, 11, 2035, 11], "temperature": 0.0, "avg_logprob": -0.2392268034578101, "compression_ratio": 1.950381679389313, "no_speech_prob": 3.420898428885266e-05}, {"id": 775, "seek": 293988, "start": 2960.8, "end": 2962.6800000000003, "text": " this one's a hundredth of the size, right?", "tokens": [341, 472, 311, 257, 3262, 392, 295, 264, 2744, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2392268034578101, "compression_ratio": 1.950381679389313, "no_speech_prob": 3.420898428885266e-05}, {"id": 776, "seek": 293988, "start": 2962.6800000000003, "end": 2966.0, "text": " So this, this row that's a hundredth of the size, if you were to just train it, if you", "tokens": [407, 341, 11, 341, 5386, 300, 311, 257, 3262, 392, 295, 264, 2744, 11, 498, 291, 645, 281, 445, 3847, 309, 11, 498, 291], "temperature": 0.0, "avg_logprob": -0.2392268034578101, "compression_ratio": 1.950381679389313, "no_speech_prob": 3.420898428885266e-05}, {"id": 777, "seek": 296600, "start": 2966.0, "end": 2972.04, "text": " were to pre-train this on the same Wikipedia book, just like Bert, and then fine tune it,", "tokens": [645, 281, 659, 12, 83, 7146, 341, 322, 264, 912, 28999, 1446, 11, 445, 411, 29594, 11, 293, 550, 2489, 10864, 309, 11], "temperature": 0.0, "avg_logprob": -0.23553565098689153, "compression_ratio": 1.712686567164179, "no_speech_prob": 4.004313450423069e-05}, {"id": 778, "seek": 296600, "start": 2972.04, "end": 2977.72, "text": " you would get 82% accuracy, which is, you know, a lot worse, 60%, like, 66, absolute", "tokens": [291, 576, 483, 29097, 4, 14170, 11, 597, 307, 11, 291, 458, 11, 257, 688, 5324, 11, 4060, 8923, 411, 11, 21126, 11, 8236], "temperature": 0.0, "avg_logprob": -0.23553565098689153, "compression_ratio": 1.712686567164179, "no_speech_prob": 4.004313450423069e-05}, {"id": 779, "seek": 296600, "start": 2977.72, "end": 2979.36, "text": " worse, which is quite a big drop, right?", "tokens": [5324, 11, 597, 307, 1596, 257, 955, 3270, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.23553565098689153, "compression_ratio": 1.712686567164179, "no_speech_prob": 4.004313450423069e-05}, {"id": 780, "seek": 296600, "start": 2979.36, "end": 2985.84, "text": " But then if you were to take this 88% teacher, labeled 8 million examples, which are, of", "tokens": [583, 550, 498, 291, 645, 281, 747, 341, 24587, 4, 5027, 11, 21335, 1649, 2459, 5110, 11, 597, 366, 11, 295], "temperature": 0.0, "avg_logprob": -0.23553565098689153, "compression_ratio": 1.712686567164179, "no_speech_prob": 4.004313450423069e-05}, {"id": 781, "seek": 296600, "start": 2985.84, "end": 2988.6, "text": " course, held out, this is test, this is test accuracy.", "tokens": [1164, 11, 5167, 484, 11, 341, 307, 1500, 11, 341, 307, 1500, 14170, 13], "temperature": 0.0, "avg_logprob": -0.23553565098689153, "compression_ratio": 1.712686567164179, "no_speech_prob": 4.004313450423069e-05}, {"id": 782, "seek": 296600, "start": 2988.6, "end": 2995.96, "text": " Um, and then, uh, and then train this classification model, which says this is a good about review,", "tokens": [3301, 11, 293, 550, 11, 2232, 11, 293, 550, 3847, 341, 21538, 2316, 11, 597, 1619, 341, 307, 257, 665, 466, 3131, 11], "temperature": 0.0, "avg_logprob": -0.23553565098689153, "compression_ratio": 1.712686567164179, "no_speech_prob": 4.004313450423069e-05}, {"id": 783, "seek": 299596, "start": 2995.96, "end": 2999.48, "text": " on these 8 million examples, you can take this model to 100 times smaller and get the same", "tokens": [322, 613, 1649, 2459, 5110, 11, 291, 393, 747, 341, 2316, 281, 2319, 1413, 4356, 293, 483, 264, 912], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 784, "seek": 299596, "start": 2999.48, "end": 3000.48, "text": " accuracy as the teacher, right?", "tokens": [14170, 382, 264, 5027, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 785, "seek": 299596, "start": 3000.48, "end": 3002.28, "text": " You get the same 80% accuracy.", "tokens": [509, 483, 264, 912, 4688, 4, 14170, 13], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 786, "seek": 299596, "start": 3002.28, "end": 3006.7200000000003, "text": " So that's really the, uh, the cool thing with distillation is that you can get models", "tokens": [407, 300, 311, 534, 264, 11, 2232, 11, 264, 1627, 551, 365, 42923, 399, 307, 300, 291, 393, 483, 5245], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 787, "seek": 299596, "start": 3006.7200000000003, "end": 3009.16, "text": " that are much smaller, but you still need to train the big model in the first place.", "tokens": [300, 366, 709, 4356, 11, 457, 291, 920, 643, 281, 3847, 264, 955, 2316, 294, 264, 700, 1081, 13], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 788, "seek": 299596, "start": 3009.16, "end": 3010.16, "text": " So it doesn't help the training cost.", "tokens": [407, 309, 1177, 380, 854, 264, 3097, 2063, 13], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 789, "seek": 299596, "start": 3010.16, "end": 3011.16, "text": " It just helps.", "tokens": [467, 445, 3665, 13], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 790, "seek": 299596, "start": 3011.16, "end": 3013.84, "text": " It actually works, it's the reason you use this big model to train, to label millions or", "tokens": [467, 767, 1985, 11, 309, 311, 264, 1778, 291, 764, 341, 955, 2316, 281, 3847, 11, 281, 7645, 6803, 420], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 791, "seek": 299596, "start": 3013.84, "end": 3014.84, "text": " billions of examples.", "tokens": [17375, 295, 5110, 13], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 792, "seek": 299596, "start": 3014.84, "end": 3017.6, "text": " So it ends up being more expensive than just training Bert, but you can actually serve", "tokens": [407, 309, 5314, 493, 885, 544, 5124, 813, 445, 3097, 29594, 11, 457, 291, 393, 767, 4596], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 793, "seek": 299596, "start": 3017.6, "end": 3021.8, "text": " this model at, at inference time for, for a tiny cost.", "tokens": [341, 2316, 412, 11, 412, 38253, 565, 337, 11, 337, 257, 5870, 2063, 13], "temperature": 0.0, "avg_logprob": -0.24658797292998344, "compression_ratio": 1.8805970149253732, "no_speech_prob": 5.142383088241331e-05}, {"id": 794, "seek": 302180, "start": 3021.8, "end": 3026.92, "text": " So the question is, why does distillation work so well?", "tokens": [407, 264, 1168, 307, 11, 983, 775, 42923, 399, 589, 370, 731, 30], "temperature": 0.0, "avg_logprob": -0.16448551087867558, "compression_ratio": 1.8056537102473498, "no_speech_prob": 4.610402538673952e-05}, {"id": 795, "seek": 302180, "start": 3026.92, "end": 3032.04, "text": " So the big hypothesis is that language modeling is kind of the ultimate NLP task, right?", "tokens": [407, 264, 955, 17291, 307, 300, 2856, 15983, 307, 733, 295, 264, 9705, 426, 45196, 5633, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16448551087867558, "compression_ratio": 1.8056537102473498, "no_speech_prob": 4.610402538673952e-05}, {"id": 796, "seek": 302180, "start": 3032.04, "end": 3036.8, "text": " A perfect language model is also a perfect question answering system, a perfect entailment", "tokens": [316, 2176, 2856, 2316, 307, 611, 257, 2176, 1168, 13430, 1185, 11, 257, 2176, 948, 864, 518], "temperature": 0.0, "avg_logprob": -0.16448551087867558, "compression_ratio": 1.8056537102473498, "no_speech_prob": 4.610402538673952e-05}, {"id": 797, "seek": 302180, "start": 3036.8, "end": 3039.7200000000003, "text": " system, sentiment analysis, co-reference, et cetera, right?", "tokens": [1185, 11, 16149, 5215, 11, 598, 12, 265, 5158, 11, 1030, 11458, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16448551087867558, "compression_ratio": 1.8056537102473498, "no_speech_prob": 4.610402538673952e-05}, {"id": 798, "seek": 302180, "start": 3039.7200000000003, "end": 3042.04, "text": " Because in order to be able to, to do these things, you kind of have to be able, you", "tokens": [1436, 294, 1668, 281, 312, 1075, 281, 11, 281, 360, 613, 721, 11, 291, 733, 295, 362, 281, 312, 1075, 11, 291], "temperature": 0.0, "avg_logprob": -0.16448551087867558, "compression_ratio": 1.8056537102473498, "no_speech_prob": 4.610402538673952e-05}, {"id": 799, "seek": 302180, "start": 3042.04, "end": 3044.84, "text": " could construct it as a language model.", "tokens": [727, 7690, 309, 382, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16448551087867558, "compression_ratio": 1.8056537102473498, "no_speech_prob": 4.610402538673952e-05}, {"id": 800, "seek": 302180, "start": 3044.84, "end": 3049.6000000000004, "text": " So when you're training a massive language model, you are learning many millions of latent", "tokens": [407, 562, 291, 434, 3097, 257, 5994, 2856, 2316, 11, 291, 366, 2539, 867, 6803, 295, 48994], "temperature": 0.0, "avg_logprob": -0.16448551087867558, "compression_ratio": 1.8056537102473498, "no_speech_prob": 4.610402538673952e-05}, {"id": 801, "seek": 304960, "start": 3049.6, "end": 3053.7999999999997, "text": " features, which are effectively the same features that you need for any other task.", "tokens": [4122, 11, 597, 366, 8659, 264, 912, 4122, 300, 291, 643, 337, 604, 661, 5633, 13], "temperature": 0.0, "avg_logprob": -0.19380767672669655, "compression_ratio": 1.8245614035087718, "no_speech_prob": 4.6095152356429026e-05}, {"id": 802, "seek": 304960, "start": 3053.7999999999997, "end": 3057.8399999999997, "text": " And so when you're doing a simpler, a fine tuning of a more specific task, what's the fine", "tokens": [400, 370, 562, 291, 434, 884, 257, 18587, 11, 257, 2489, 15164, 295, 257, 544, 2685, 5633, 11, 437, 311, 264, 2489], "temperature": 0.0, "avg_logprob": -0.19380767672669655, "compression_ratio": 1.8245614035087718, "no_speech_prob": 4.6095152356429026e-05}, {"id": 803, "seek": 304960, "start": 3057.8399999999997, "end": 3060.96, "text": " tuning is basically taking these latent features, which your system happened to learn,", "tokens": [15164, 307, 1936, 1940, 613, 48994, 4122, 11, 597, 428, 1185, 2011, 281, 1466, 11], "temperature": 0.0, "avg_logprob": -0.19380767672669655, "compression_ratio": 1.8245614035087718, "no_speech_prob": 4.6095152356429026e-05}, {"id": 804, "seek": 304960, "start": 3060.96, "end": 3063.16, "text": " and it's some, encoded somewhere in your weights.", "tokens": [293, 309, 311, 512, 11, 2058, 12340, 4079, 294, 428, 17443, 13], "temperature": 0.0, "avg_logprob": -0.19380767672669655, "compression_ratio": 1.8245614035087718, "no_speech_prob": 4.6095152356429026e-05}, {"id": 805, "seek": 304960, "start": 3063.16, "end": 3066.24, "text": " And you are, it's kind of just tweaking these, which is why I can do it with a single pass", "tokens": [400, 291, 366, 11, 309, 311, 733, 295, 445, 6986, 2456, 613, 11, 597, 307, 983, 286, 393, 360, 309, 365, 257, 2167, 1320], "temperature": 0.0, "avg_logprob": -0.19380767672669655, "compression_ratio": 1.8245614035087718, "no_speech_prob": 4.6095152356429026e-05}, {"id": 806, "seek": 304960, "start": 3066.24, "end": 3069.04, "text": " over the fine tuning data.", "tokens": [670, 264, 2489, 15164, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19380767672669655, "compression_ratio": 1.8245614035087718, "no_speech_prob": 4.6095152356429026e-05}, {"id": 807, "seek": 304960, "start": 3069.04, "end": 3073.24, "text": " And so, but once you figure out which parts are important, then there exists a hypothetically", "tokens": [400, 370, 11, 457, 1564, 291, 2573, 484, 597, 3166, 366, 1021, 11, 550, 456, 8198, 257, 24371, 22652], "temperature": 0.0, "avg_logprob": -0.19380767672669655, "compression_ratio": 1.8245614035087718, "no_speech_prob": 4.6095152356429026e-05}, {"id": 808, "seek": 304960, "start": 3073.24, "end": 3077.64, "text": " much smaller model size, which can still get the same representation and same generalization,", "tokens": [709, 4356, 2316, 2744, 11, 597, 393, 920, 483, 264, 912, 10290, 293, 912, 2674, 2144, 11], "temperature": 0.0, "avg_logprob": -0.19380767672669655, "compression_ratio": 1.8245614035087718, "no_speech_prob": 4.6095152356429026e-05}, {"id": 809, "seek": 304960, "start": 3077.64, "end": 3078.64, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.19380767672669655, "compression_ratio": 1.8245614035087718, "no_speech_prob": 4.6095152356429026e-05}, {"id": 810, "seek": 307864, "start": 3078.64, "end": 3081.2799999999997, "text": " So, there's a bunch of examples with this fine tuning model.", "tokens": [407, 11, 456, 311, 257, 3840, 295, 5110, 365, 341, 2489, 15164, 2316, 13], "temperature": 0.0, "avg_logprob": -0.24486357776831227, "compression_ratio": 1.8597122302158273, "no_speech_prob": 7.600230310345069e-05}, {"id": 811, "seek": 307864, "start": 3081.2799999999997, "end": 3083.92, "text": " And now you can learn a model that can really hone in on just these features that are important.", "tokens": [400, 586, 291, 393, 1466, 257, 2316, 300, 393, 534, 43212, 294, 322, 445, 613, 4122, 300, 366, 1021, 13], "temperature": 0.0, "avg_logprob": -0.24486357776831227, "compression_ratio": 1.8597122302158273, "no_speech_prob": 7.600230310345069e-05}, {"id": 812, "seek": 307864, "start": 3083.92, "end": 3092.44, "text": " And so, it can take them, you know, it can train a model that's 100th the size, and just", "tokens": [400, 370, 11, 309, 393, 747, 552, 11, 291, 458, 11, 309, 393, 3847, 257, 2316, 300, 311, 2319, 392, 264, 2744, 11, 293, 445], "temperature": 0.0, "avg_logprob": -0.24486357776831227, "compression_ratio": 1.8597122302158273, "no_speech_prob": 7.600230310345069e-05}, {"id": 813, "seek": 307864, "start": 3092.44, "end": 3096.48, "text": " hone in on these features if you have a lot of pseudo label data.", "tokens": [43212, 294, 322, 613, 4122, 498, 291, 362, 257, 688, 295, 35899, 7645, 1412, 13], "temperature": 0.0, "avg_logprob": -0.24486357776831227, "compression_ratio": 1.8597122302158273, "no_speech_prob": 7.600230310345069e-05}, {"id": 814, "seek": 307864, "start": 3096.48, "end": 3097.48, "text": " And that's why it works.", "tokens": [400, 300, 311, 983, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.24486357776831227, "compression_ratio": 1.8597122302158273, "no_speech_prob": 7.600230310345069e-05}, {"id": 815, "seek": 307864, "start": 3097.48, "end": 3101.7999999999997, "text": " And so, the evidence really is that it just doesn't work to do self-dissolation, right?", "tokens": [400, 370, 11, 264, 4467, 534, 307, 300, 309, 445, 1177, 380, 589, 281, 360, 2698, 12, 67, 891, 401, 399, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.24486357776831227, "compression_ratio": 1.8597122302158273, "no_speech_prob": 7.600230310345069e-05}, {"id": 816, "seek": 307864, "start": 3101.7999999999997, "end": 3106.52, "text": " And so it must be that it's really just learning a subset of the features for most of these", "tokens": [400, 370, 309, 1633, 312, 300, 309, 311, 534, 445, 2539, 257, 25993, 295, 264, 4122, 337, 881, 295, 613], "temperature": 0.0, "avg_logprob": -0.24486357776831227, "compression_ratio": 1.8597122302158273, "no_speech_prob": 7.600230310345069e-05}, {"id": 817, "seek": 310652, "start": 3106.52, "end": 3109.4, "text": " tasks.", "tokens": [9608, 13], "temperature": 0.0, "avg_logprob": -0.2760256294892213, "compression_ratio": 1.8366533864541832, "no_speech_prob": 8.091077324934304e-05}, {"id": 818, "seek": 310652, "start": 3109.4, "end": 3113.48, "text": " And so, basically every task but language modeling, we've been able to get distillation to work", "tokens": [400, 370, 11, 1936, 633, 5633, 457, 2856, 15983, 11, 321, 600, 668, 1075, 281, 483, 42923, 399, 281, 589], "temperature": 0.0, "avg_logprob": -0.2760256294892213, "compression_ratio": 1.8366533864541832, "no_speech_prob": 8.091077324934304e-05}, {"id": 819, "seek": 310652, "start": 3113.48, "end": 3114.48, "text": " for.", "tokens": [337, 13], "temperature": 0.0, "avg_logprob": -0.2760256294892213, "compression_ratio": 1.8366533864541832, "no_speech_prob": 8.091077324934304e-05}, {"id": 820, "seek": 310652, "start": 3114.48, "end": 3118.72, "text": " So, this includes tasks that seem really hard like question answering and search.", "tokens": [407, 11, 341, 5974, 9608, 300, 1643, 534, 1152, 411, 1168, 13430, 293, 3164, 13], "temperature": 0.0, "avg_logprob": -0.2760256294892213, "compression_ratio": 1.8366533864541832, "no_speech_prob": 8.091077324934304e-05}, {"id": 821, "seek": 310652, "start": 3118.72, "end": 3123.92, "text": " So that does imply that language modeling itself, which is basically language generation", "tokens": [407, 300, 775, 33616, 300, 2856, 15983, 2564, 11, 597, 307, 1936, 2856, 5125], "temperature": 0.0, "avg_logprob": -0.2760256294892213, "compression_ratio": 1.8366533864541832, "no_speech_prob": 8.091077324934304e-05}, {"id": 822, "seek": 310652, "start": 3123.92, "end": 3128.48, "text": " also, because that's just a form of language modeling, is fundamentally harder than language", "tokens": [611, 11, 570, 300, 311, 445, 257, 1254, 295, 2856, 15983, 11, 307, 17879, 6081, 813, 2856], "temperature": 0.0, "avg_logprob": -0.2760256294892213, "compression_ratio": 1.8366533864541832, "no_speech_prob": 8.091077324934304e-05}, {"id": 823, "seek": 310652, "start": 3128.48, "end": 3132.48, "text": " understanding, which is not super hard to buy.", "tokens": [3701, 11, 597, 307, 406, 1687, 1152, 281, 2256, 13], "temperature": 0.0, "avg_logprob": -0.2760256294892213, "compression_ratio": 1.8366533864541832, "no_speech_prob": 8.091077324934304e-05}, {"id": 824, "seek": 310652, "start": 3132.48, "end": 3134.32, "text": " Or at least it's not fundamentally harder.", "tokens": [1610, 412, 1935, 309, 311, 406, 17879, 6081, 13], "temperature": 0.0, "avg_logprob": -0.2760256294892213, "compression_ratio": 1.8366533864541832, "no_speech_prob": 8.091077324934304e-05}, {"id": 825, "seek": 313432, "start": 3134.32, "end": 3138.2000000000003, "text": " But given the state of the art, state of the art models for language understanding are", "tokens": [583, 2212, 264, 1785, 295, 264, 1523, 11, 1785, 295, 264, 1523, 5245, 337, 2856, 3701, 366], "temperature": 0.0, "avg_logprob": -0.2238503104762027, "compression_ratio": 1.688259109311741, "no_speech_prob": 1.9217013687011786e-05}, {"id": 826, "seek": 313432, "start": 3138.2000000000003, "end": 3140.88, "text": " fundamentally simpler than what they do, right?", "tokens": [17879, 18587, 813, 437, 436, 360, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2238503104762027, "compression_ratio": 1.688259109311741, "no_speech_prob": 1.9217013687011786e-05}, {"id": 827, "seek": 313432, "start": 3140.88, "end": 3147.1200000000003, "text": " So presumably, just doing kind of pattern recognition than models that are generating language.", "tokens": [407, 26742, 11, 445, 884, 733, 295, 5102, 11150, 813, 5245, 300, 366, 17746, 2856, 13], "temperature": 0.0, "avg_logprob": -0.2238503104762027, "compression_ratio": 1.688259109311741, "no_speech_prob": 1.9217013687011786e-05}, {"id": 828, "seek": 313432, "start": 3147.1200000000003, "end": 3153.36, "text": " And so that's kind of why all of these classification models can kind of be distilled so well.", "tokens": [400, 370, 300, 311, 733, 295, 983, 439, 295, 613, 21538, 5245, 393, 733, 295, 312, 1483, 6261, 370, 731, 13], "temperature": 0.0, "avg_logprob": -0.2238503104762027, "compression_ratio": 1.688259109311741, "no_speech_prob": 1.9217013687011786e-05}, {"id": 829, "seek": 313432, "start": 3153.36, "end": 3158.04, "text": " So basically, in conclusion, the preaching models work really well.", "tokens": [407, 1936, 11, 294, 10063, 11, 264, 25381, 5245, 589, 534, 731, 13], "temperature": 0.0, "avg_logprob": -0.2238503104762027, "compression_ratio": 1.688259109311741, "no_speech_prob": 1.9217013687011786e-05}, {"id": 830, "seek": 313432, "start": 3158.04, "end": 3159.1200000000003, "text": " They're very expensive.", "tokens": [814, 434, 588, 5124, 13], "temperature": 0.0, "avg_logprob": -0.2238503104762027, "compression_ratio": 1.688259109311741, "no_speech_prob": 1.9217013687011786e-05}, {"id": 831, "seek": 315912, "start": 3159.12, "end": 3165.16, "text": " We know how to kind of solve this for inference time and we can do fast inference, but it", "tokens": [492, 458, 577, 281, 733, 295, 5039, 341, 337, 38253, 565, 293, 321, 393, 360, 2370, 38253, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.15911114493081735, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.82479260629043e-05}, {"id": 832, "seek": 315912, "start": 3165.16, "end": 3172.8399999999997, "text": " is still unsolved how to make these fast at training time.", "tokens": [307, 920, 2693, 29110, 577, 281, 652, 613, 2370, 412, 3097, 565, 13], "temperature": 0.0, "avg_logprob": -0.15911114493081735, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.82479260629043e-05}, {"id": 833, "seek": 315912, "start": 3172.8399999999997, "end": 3179.7599999999998, "text": " And moreover, it seems like a lot of the details about algorithmic improvements for making", "tokens": [400, 544, 3570, 11, 309, 2544, 411, 257, 688, 295, 264, 4365, 466, 9284, 299, 13797, 337, 1455], "temperature": 0.0, "avg_logprob": -0.15911114493081735, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.82479260629043e-05}, {"id": 834, "seek": 315912, "start": 3179.7599999999998, "end": 3184.56, "text": " the training more efficient don't seem to have a ton of benefit in terms of at least", "tokens": [264, 3097, 544, 7148, 500, 380, 1643, 281, 362, 257, 2952, 295, 5121, 294, 2115, 295, 412, 1935], "temperature": 0.0, "avg_logprob": -0.15911114493081735, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.82479260629043e-05}, {"id": 835, "seek": 315912, "start": 3184.56, "end": 3186.16, "text": " getting to the results.", "tokens": [1242, 281, 264, 3542, 13], "temperature": 0.0, "avg_logprob": -0.15911114493081735, "compression_ratio": 1.6571428571428573, "no_speech_prob": 5.82479260629043e-05}, {"id": 836, "seek": 318616, "start": 3186.16, "end": 3189.08, "text": " And it seems like a lot of choices don't really matter that much.", "tokens": [400, 309, 2544, 411, 257, 688, 295, 7994, 500, 380, 534, 1871, 300, 709, 13], "temperature": 0.0, "avg_logprob": -0.2904295603434245, "compression_ratio": 1.75, "no_speech_prob": 0.0001418757310602814}, {"id": 837, "seek": 318616, "start": 3189.08, "end": 3194.12, "text": " And it's really just about a couple of like compared to just the kind of the simple", "tokens": [400, 309, 311, 534, 445, 466, 257, 1916, 295, 411, 5347, 281, 445, 264, 733, 295, 264, 2199], "temperature": 0.0, "avg_logprob": -0.2904295603434245, "compression_ratio": 1.75, "no_speech_prob": 0.0001418757310602814}, {"id": 838, "seek": 318616, "start": 3194.12, "end": 3195.12, "text": " mass-colon baseline.", "tokens": [2758, 12, 8768, 266, 20518, 13], "temperature": 0.0, "avg_logprob": -0.2904295603434245, "compression_ratio": 1.75, "no_speech_prob": 0.0001418757310602814}, {"id": 839, "seek": 318616, "start": 3195.12, "end": 3199.2799999999997, "text": " It's pretty hard to beat that in an Apple's Apple comparison.", "tokens": [467, 311, 1238, 1152, 281, 4224, 300, 294, 364, 6373, 311, 6373, 9660, 13], "temperature": 0.0, "avg_logprob": -0.2904295603434245, "compression_ratio": 1.75, "no_speech_prob": 0.0001418757310602814}, {"id": 840, "seek": 318616, "start": 3199.2799999999997, "end": 3204.6, "text": " So yeah, it's a little bit unfortunate for a research perspective.", "tokens": [407, 1338, 11, 309, 311, 257, 707, 857, 17843, 337, 257, 2132, 4585, 13], "temperature": 0.0, "avg_logprob": -0.2904295603434245, "compression_ratio": 1.75, "no_speech_prob": 0.0001418757310602814}, {"id": 841, "seek": 318616, "start": 3204.6, "end": 3209.7999999999997, "text": " It's definitely good for me from people who want to build NLP systems and who want to,", "tokens": [467, 311, 2138, 665, 337, 385, 490, 561, 567, 528, 281, 1322, 426, 45196, 3652, 293, 567, 528, 281, 11], "temperature": 0.0, "avg_logprob": -0.2904295603434245, "compression_ratio": 1.75, "no_speech_prob": 0.0001418757310602814}, {"id": 842, "seek": 318616, "start": 3209.7999999999997, "end": 3214.2, "text": " especially domain specific NLP systems, like people who want to adapt to a medical domain", "tokens": [2318, 9274, 2685, 426, 45196, 3652, 11, 411, 561, 567, 528, 281, 6231, 281, 257, 4625, 9274], "temperature": 0.0, "avg_logprob": -0.2904295603434245, "compression_ratio": 1.75, "no_speech_prob": 0.0001418757310602814}, {"id": 843, "seek": 321420, "start": 3214.2, "end": 3217.56, "text": " or people who only have a tiny amount of data or people who want to do startups or they", "tokens": [420, 561, 567, 787, 362, 257, 5870, 2372, 295, 1412, 420, 561, 567, 528, 281, 360, 28041, 420, 436], "temperature": 0.0, "avg_logprob": -0.24087944326474686, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.0001631361519685015}, {"id": 844, "seek": 321420, "start": 3217.56, "end": 3220.2799999999997, "text": " want to build an actual product and they only have a tiny amount of data.", "tokens": [528, 281, 1322, 364, 3539, 1674, 293, 436, 787, 362, 257, 5870, 2372, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.24087944326474686, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.0001631361519685015}, {"id": 845, "seek": 321420, "start": 3220.2799999999997, "end": 3221.2799999999997, "text": " So it's definitely good for that perspective.", "tokens": [407, 309, 311, 2138, 665, 337, 300, 4585, 13], "temperature": 0.0, "avg_logprob": -0.24087944326474686, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.0001631361519685015}, {"id": 846, "seek": 321420, "start": 3221.2799999999997, "end": 3229.12, "text": " But certainly, I think from the perspective of sometimes research, as I was saying, the", "tokens": [583, 3297, 11, 286, 519, 490, 264, 4585, 295, 2171, 2132, 11, 382, 286, 390, 1566, 11, 264], "temperature": 0.0, "avg_logprob": -0.24087944326474686, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.0001631361519685015}, {"id": 847, "seek": 321420, "start": 3229.12, "end": 3233.24, "text": " goal of research is to kind of like research yourself out of a job, then it is kind of,", "tokens": [3387, 295, 2132, 307, 281, 733, 295, 411, 2132, 1803, 484, 295, 257, 1691, 11, 550, 309, 307, 733, 295, 11], "temperature": 0.0, "avg_logprob": -0.24087944326474686, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.0001631361519685015}, {"id": 848, "seek": 321420, "start": 3233.24, "end": 3236.8399999999997, "text": " you know, it's a little unfortunate from that perspective.", "tokens": [291, 458, 11, 309, 311, 257, 707, 17843, 490, 300, 4585, 13], "temperature": 0.0, "avg_logprob": -0.24087944326474686, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.0001631361519685015}, {"id": 849, "seek": 321420, "start": 3236.8399999999997, "end": 3240.2799999999997, "text": " But I still think that there's a possibility that there's going to be a breakthrough that", "tokens": [583, 286, 920, 519, 300, 456, 311, 257, 7959, 300, 456, 311, 516, 281, 312, 257, 22397, 300], "temperature": 0.0, "avg_logprob": -0.24087944326474686, "compression_ratio": 1.906810035842294, "no_speech_prob": 0.0001631361519685015}, {"id": 850, "seek": 324028, "start": 3240.28, "end": 3247.1600000000003, "text": " kind of shows how to do computational efficiency without, and can kind of show compelling results", "tokens": [733, 295, 3110, 577, 281, 360, 28270, 10493, 1553, 11, 293, 393, 733, 295, 855, 20050, 3542], "temperature": 0.0, "avg_logprob": -0.26154383491067323, "compression_ratio": 1.736842105263158, "no_speech_prob": 6.008973650750704e-05}, {"id": 851, "seek": 324028, "start": 3247.1600000000003, "end": 3250.2000000000003, "text": " that you don't need, you know, such an absurdly large model.", "tokens": [300, 291, 500, 380, 643, 11, 291, 458, 11, 1270, 364, 19774, 356, 2416, 2316, 13], "temperature": 0.0, "avg_logprob": -0.26154383491067323, "compression_ratio": 1.736842105263158, "no_speech_prob": 6.008973650750704e-05}, {"id": 852, "seek": 324028, "start": 3250.2000000000003, "end": 3255.92, "text": " Or actually, besides the models matter, you don't need such an expensive model to do well.", "tokens": [1610, 767, 11, 11868, 264, 5245, 1871, 11, 291, 500, 380, 643, 1270, 364, 5124, 2316, 281, 360, 731, 13], "temperature": 0.0, "avg_logprob": -0.26154383491067323, "compression_ratio": 1.736842105263158, "no_speech_prob": 6.008973650750704e-05}, {"id": 853, "seek": 324028, "start": 3255.92, "end": 3259.0800000000004, "text": " Maybe look from sparsity, right, or something like that where you actually do have a really", "tokens": [2704, 574, 490, 637, 685, 507, 11, 558, 11, 420, 746, 411, 300, 689, 291, 767, 360, 362, 257, 534], "temperature": 0.0, "avg_logprob": -0.26154383491067323, "compression_ratio": 1.736842105263158, "no_speech_prob": 6.008973650750704e-05}, {"id": 854, "seek": 325908, "start": 3259.08, "end": 3271.44, "text": " large model, just sparsely activated in some, using some efficiency tricks or whatever.", "tokens": [50364, 2416, 2316, 11, 445, 637, 685, 736, 18157, 294, 512, 11, 1228, 512, 10493, 11733, 420, 2035, 13, 50982], "temperature": 0.0, "avg_logprob": -0.4370144435337612, "compression_ratio": 1.0875, "no_speech_prob": 3.813915100181475e-05}], "language": "en"}