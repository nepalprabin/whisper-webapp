{"text": " Hi everybody, welcome back to CS224N. First just a couple of announcements. Originally this was going to be the day when assignment five was due, but as you've seen, we're giving you one extra day. So it's now due Friday at 430. We do realize that assignment five has been a bit of a tough challenge for many people, though we've been trying to help people out and offer a thousand otherwise. So I hope at the end of the day it will seem like it was a really good learning experience to really get some much more kind of close hands on, look at how transformers work, rather than it's simply being loading up a transformer as a black mystery box. After Friday, I guess there's no rest since we do really hope that you can sort of get a lot of help. Basically immediately transition to working on final projects and so it's basically four weeks to go on final projects and in particular we're hoping to get feedback on your project proposals back by next Tuesday to help that process or on when people the after get started on them soon. And you know, it's just maybe a good moment to say that we do really appreciate all the people have been putting tons of effort into these assignments and we like that sort of all the keenness we're seeing from the students. Okay, so with that out of the way today, I'm delighted to have giving today's lecture on your language generation, Antoine Bosleau, who's at present a postdoc at Stanford. He's someone who's done a lot of work on natural language generation in his previous life as a university, Washington PhD student and next year he's going to be taking up a position as a professor in Switzerland. Okay, so welcome and Trams. Thanks Chris, that's a very kind introduction. It's great to be here giving this lecture. I'm going to see CS224N particularly on one of my favorite topics in deep learning for NLP natural language generation. So hopefully by the end of this lecture, most of you will have at least learned a bit about NLG with deep learning and hopefully be motivated to start doing some research on it or launch a start up in NLG or perhaps go work on it in a larger organization. Okay, so to start, I think it might be really helpful to define what we meet at a high level when we talk about natural language generation because over the last few years, the definition has actually sort of changed and has really grown as a subfield to really encapsulate any part of NLP that involves the production of written or spoken language. So in other words, if you're given some inputs and your goal is to generate text to describe, respond, translate or summary is that piece of text. NLG really focuses on how you can actually build a system that can automatically produce a coherent and useful written piece of text for that human consumption. And it used to be a much more limited research area since many tasks that we now view as NLG problems didn't actually involve much text production prior to neural networks. But now that scope has expanded considerably and we have this much larger area to sort of work in. Unfortunately, we're not quite yet at the level of the types of A.I. NLG tools that we've seen in pop culture and that we like to imagine. But we are starting to see many areas where NLG tools are having a massive impact to start with machine translation is kind of the classical example of an NLG task these days ever since the task moved to neural networks and a NLG framework around a 2014 or so. And now we've seen a rapid improvement in the quality and applicability of translation systems. In fact, you can often use Google Translates for most of your kind of retail translation needs as a good starting point. Similarly, NLG technologies really underpin some of the dialogue systems that you might interact with on a daily basis. Any time you use, let's say, Siri, Alexa, Cortana, Google Home, Bixby, or pretty much any other major companies dialogue system. There's a good chance that there's a neural NLG component embedded in that system that's involved with providing you an answer to your query. And there's really still a ton of progress to be made in this area. And it's led to some major companies to actually crowdsource chatbot technologies from researchers and students such as yourself to continue to try to make big advances in this area. We're also seeing lots of NLG technologies used in areas such as summarization, where systems often have to aggregate information from potentially multiple sources and rephrase the most salient content in a shortened but still very engaging way. While our go-to example for summarization is generally related to, let's say, generating news highlights, summarization systems have actually achieved broad applicability in many areas where we ingest content such as summarizing emails or summarizing meeting transcripts. And there's actually many more areas not listed here. I didn't actually put it on the slide, but a few months back, a tool called Semantic Scholar actually developed a neural system for generating summaries of scientific papers, which is something that I personally end up using quite a bit as an example of how humans can interact with these technologies. But these modalities aren't actually limited to text in or text out. So actually the classical NLG area that I mentioned earlier is how the task used to be framed was actually around what we now call data to text generation. So can you learn to compile or let's say, summarize the most interesting facts from a table or a knowledge graph or some type of data stream. That way humans can get the most most interesting and salient content that's being presented in these data structures rapidly and in easier to ingest format than having to look through the structures themselves. We've also seen a lot of recent work in visual description that tries to use language to describe the content and images or videos. So around 2014 or so we start to see the first neural NLG systems in the space. And they've really continued to mature in the last six years. And now we actually tackle much more challenging description tasks such as generating full descriptive paragraphs of scenes or generating streams of visual generating descriptions for streams of visual contents such as in video captioning and these tools have really broad applicability in different areas of AI. And finally the last sort of application I kind of want to mention is that we've also started seeing NLG systems being developed in more creative applications such as story generation where AI systems can now help humans write short stories, blog posts or even full books in some case as creative writing assistants. In another area such as post tree generation we can actually have kind of full automated settings where you can have AI agents that can generate something like a sonnet and in fact condition that's on a lot of demands that are given through a user interface. So I hope that by this point I've really given you a look into the breadth of NLG applications and how it sort of encompasses any task that you might think of that involves production of text. And each of these tasks really requires different algorithms and different models and a different way of designing the system to get right. But what they have in common is that a lot of our power is by next generation advances in deep learning for NLG. And the goal of today is to really give you the introduction to these topics that really allows you to contribute to the next era of these technologies and designing deep learning systems for NLG. And so I think to start what might be interesting to do is to quickly reap cap topics that you may have seen in previous lectures, but which are going to be very relevant today when we're trying to design an NLG system. And what we're effectively trying to do in the setting is take a sequence of tokens as inputs and produce new text that is conditioned on this input. And then what we typically call the auto regressive setting, which is the most common sort of text generation setting, we take these produced tokens of text and we feed them back into our model to generate the next token in the sequence that we want to generate. But so to really understand what's going on in an auto regressive NLG system, what we really need to start to do is look at what happens for the generation of an individual token since further stage is really depend on taking that generated token and passing it back in as input and doing the same thing. So what happens at a low level is that your model takes in this sequence of inputs, so these Ys and it computes a vector of scores using the model itself. And each index in that vector corresponds to the score for a token in your vocabulary. So the only tokens that your model is actually allowed to generate. And then what you do is that you compute a probability distribution over these scores using what we call a softmax function to compute a probability estimate for each token in your vocabulary given the context that precedes it. And as a shorthand, I'll just mention that sometimes I'll remove the W from this probability equation, but just know that when I write out the probability of a token Y at time t, what I mean is the probability that Y of t is a particular word. So it's kind of a variable assignment. But so what actually is the output of what we typically call a text generation model at this point is actually this vector of scores. And then that vector gets past the softmax function to give us a probability distribution over the set of tokens in the vocabulary. And then to actually generate a token, we can define what we call a decoding algorithm. That is a function that takes in this distribution peak over all the tokens of the vocabulary. And you know, it defines a function for selecting a function from this distribution as the next token that is produced by our NLG system. And for that distribution to be calibrated in such a way that it means anything, we need to train the model to actually be able to do the task. So the most common way of training text generation models is to use maximum likelihood training. And despite its name, we don't actually maximize likelihoods. We actually minimize negative log likelihoods. And what that actually is is just a multi-class classification task where each word in our vocabulary is a class that can be predicted by the model. And so at each step in the sequence, we're actually trying to predict the class that corresponds to the word that comes next in the sequence of text that we're trying to train on. And this word is often called the gold or ground truth token as just kind of interchangeable vocabulary that we use. And another term for this training algorithm is teacher forcing. So you might see these expressions kind of used interchangeably if you read papers on this topic. But so at each step, you're really computing a loss term that is the negative log likelihood of predicting this gold token y1 at every step. So in these slides, whenever you see an asterisk next to a y, that means that this is a gold token that comes from a training sequence. And you can do this for multiple steps, adding up the log likelihoods along the way. Eventually, you're going to arrive at the end of your of your gold sequence. And you'll be able to compute gradients with respect to this sum-dLoss term for every parameter in your model, which allows you to update it so that the next time around when you see this sequence, your model is more confident in the probability that this sequence is a correct sequence given the same context it has seen before. But most of this should just be a recap from previous lectures on language modeling and machine translation. But now that we've got that out of the way, let's get to the fun part and talk about some new topics. The first one of which is decoding, which is actually one of my favorite topics in natural language generation research. So if you recall, your decoding algorithm is really the function that takes in this this induced probability distribution from your model over the next possible tokens that can be generated and selects which one of those tokens should be outputted. So once your model is trained, this distribution should be meaningful. And you want to be able to generate a sensible next token. And then you can use these generated next tokens. So the blue y hats here, as input in the next step of the model, which allows you to recompute a new distribution decode a new token, repeat the process and eventually end up with a full sequence that your model has now generated given a fixed starting sequence of text. And so let's talk a bit about the algorithms that we can use to decode tokens from this distribution. So you've actually already seen some of these in a previous lecture on neural machine translation, I believe, where you started off by by seeing a relatively simple decoding algorithm that nonetheless remains very popular, argmax decoding. And with argmax decoding, you pretty much just take the highest probability token from your distribution as the decoded token and feed it back into the model for to get the distribution of the next step. And you keep repeating this process and it's very nice and it's very convenient. And you've also, I think, learned about beam search where you can scale up these these greedy methods by doing a wider search over the set of tokens that follow, the set of most likely tokens that follow to try to find a seek a sub sequence that is a lower overall negative log likelihood, even if it in the intermediate step, it tends to be higher than what would be the argmax decoded token. And while these greedy methods work great for machine translation and in other tests as well, such as summarization, they do tend to be problematic in many other text generation tasks, particularly ones that end up being more open-ended. So one of these big problems that they have is that they often end up repeating themselves. So here in this example from from Holtsman at all 2020, we can see that after around 20 or sorry, yet 60 tokens or so of generation, the model really devolves into just repeating the same thing over and over again. And this actually tends to happen a lot in text generation systems. Repetition was actually one of the biggest problems that we that we tried to tackle in text generation for many years and then still face to this day. And you know, I think it's worth taking a look at why repetition happens a bit more analytically so you can perhaps better understand the interaction between your model and your decoding algorithm. So just as a quick little visual demonstration, here I'm showing you the step by step negative log likelihoods from two different language models, one based on recurrent neural networks and one based on a transformer language model called GPT. And I'm showing this this plot for a particular phrase I don't know, which for anyone who's worked in chatbots has probably seen many times potentially in nightmares. It's not a very interesting plot, though, you know, what we do see is that the transformer model does tend to be a bit less confident in the probability of each word on the recurrent neural network does. What's more interesting though is what happens if I repeat this same phrase multiple times in a row. And one of the things that we notice here is that the repetition of this phrase actually causes the token level negative log likelihoods to get lower and lower for each of these tokens, which actually means that the model is becoming more confident that these are the right tokens and that they should probably follow the preceding context as we generate it more times. And this doesn't really subside as the sequence gets longer and longer. And as you keep repeating the same phrases again over and over again, the model becomes more and more confident the next time around it should say the same thing. And, you know, while this actually kind of makes sense and that if you, you know, say the phrase, let's say I'm tired 15 times, it's a fair bet that on a 16th time you're actually going to say it again, it's not necessarily the behavior that we want our generation systems to get stuck in. Another interesting thing to note here as an aside is that this behavior is actually less problematic in recurrent neural networks than in transformer language models. So you can see that for the LSTM, the curve flat, flat ends after a certain point. And so if you remember in perhaps a previous lecture on why we might like transformer language models, one of their benefits is that they don't have the temporal bottleneck of tracking a state, which a recurrent neural network tends to have. And so the removal of that bottleneck actually ends up making them more prone to repetitive behavior when you use greedy algorithms to decode. So what can we actually do to reduce repetition since it's a pretty big problem in these systems? Well, there are actually quite a few proposed approaches in the last few years, some which I'll summarize here, which, you know, range from the kind of hacky, but surprisingly effective, don't repeat any end grams at inference time. But there's also been training time approaches to do it, such as having a loss function that minimizes the similarity between hidden activations at different steps or coverage loss that penalizes attending to the same tokens over time. So if you, you know, change the inputs that your model is allowed to focus on, it's naturally going to produce different text or more recently an unlikely hood objective that actually penalizes outputting the same words, which we'll talk a bit more about later. But the truth is, is that the problem here really lies in using greedy algorithms in the first place. In many applications of, you know, human language, humans don't actually speak in a probability maximizing way. So if you look at this plot from Ultimate All 2020, it shows the per-time step probability of human written text in orange and beam search decoded text in blue on the same graph. And what you can see is that beam search decoded text tends to be very high probability with little variance over time. And this makes a lot of sense, you know, because it's literally trying to maximize the probability of the sequences that it produces. And a big part of that is maximizing the, you know, step by step probability of the tokens that it uses. But meanwhile, we can see that human written text is a lot more variable, often actually dipping into very low probability territory. That actually makes a lot of sense. If we could always, you know, predict human text with high probability, you know, there really be no reason to listen to each other's comments. We know what we were going to say. But so ultimately, what we want to be able to do is to match the uncertainty of human language patterns in how we decode text, which is why in many applications that tend to have this higher variability, sampling from these distributions has kind of become a go-to decoding method, particularly in creative generation tasks. And so with sampling, we take the distribution over tokens that's produced in our model, and we generate a token randomly, according to the probability mass that is assigned to each potential option. So rather than doing any type of greedy step, we use the probability on each token to give us a chance that that token is generated. And so this does allow us to kind of get much more, you know, stochasticity and the types of tokens that are generated. But a challenge that pops up here is that these distributions tend to be over a very large vocabulary. And so even if there's clearly tokens that have a higher chance of being generated, the tail of the distribution can often be spread over a much larger number of possible tokens. And so this becomes a bit of a problem because these tokens in the long tail are probably completely irrelevant to the current context. So they shouldn't have any chance of being selected individually. But as a group, it ends up that there's a decent chance that you could output a completely irrelevant token. Even if 90% of your probability mass is on relevant tokens, that means that you have a one-intent chance of outputting something that completely throws off your entire text generation pipeline and is completely enane. So to mitigate this, the field has developed a new set of algorithms that tries to prune these distributions at inference time. And so top-case sampling is kind of the most obvious way to do this. So here we recognize that most of the tokens in our vocabulary should have no probability of being selected at all. So we just truncate the set of tokens that were allowed to sample from to be the K tokens with the highest amount of probability mass of the distributions. And common values of K are often 5, 10, 20, sometimes up to 100. But really, it's a hyperparameter that you end up setting as the designer of this system. In general, though, what's important to note is that the higher you make K, the more you'll be able to generate diverse outputs, which is good, because that's what we're trying to do. But you're also going to increase the chance of letting that long tail seep in and generating something that's completely irrelevant to the current context. Oh, sorry. Meanwhile, if you decrease K, your outputs are going to be safer from these long tail effects, but your text may end up being boring and generic because your sampling algorithm starts to look a lot more greedy in nature. And this kind of shows the problem of having a fixed K as the number of tokens that you can generate from your distribution. If your distribution is certain points is pretty flat, such as in this example, she said, I never blank, you might not want to truncate a lot of interesting options that, you know, using a small value of K when there's so many good choices that could fit in this potential context. You know, conversely in a different example, you might want to cut off much more than let's say your minimum K options, because only a subset of them end up being quite suitable. And, you know, a higher K than is really necessary, lets that long tail seep in and potentially, you know, ruin your generation. And so, you know, in response to this top P or nucleus sampling is a way around this issue. So here, instead of sampling from a fixed number of tokens at each step, you sample from a fixed amount of probability mass. And so depending on the flatness of your distribution, you end up including a variable number of tokens that is, that is kind of dynamically changing depending on, you know, how that probability mass is spread across the distribution. And so, you know, to kind of describe this visually, if you have, you know, three different distributions at a particular step to generate a particular token, they're each going to prune a different number of tokens from the available set that you can sample from, depending on what the value of P is and what the peakingness of that distribution actually ends up being. So, you know, I keep talking about this concept of flatness of a distribution, being kind of critical and understanding how many tokens we can actually end up sampling from. And in fact, as we try to use sampling algorithms, we might find that the model that we've learned may not actually be producing probability distributions that lend themselves very nicely to using these types of sampling algorithms. You know, the distributions might be too flat, they might be too peaky. And in fact, what we might want to do is rescale those distributions to better fit the decoding algorithm that we might want to use. And we can do this with a method that's, you know, goes by a variety of different names, which I call temperature scaling. And here what you do is that you apply a linear coefficient to every score for each token. Before you pass it through the softmax, that temperature coefficient is the same for every token. It's not dynamically changing amongst your vocabulary. It stays the same. But what happens is that that change ends up being amplified by the softmax function. And what ends up happening is that if your temperature coefficient is greater than one, you're actually going to make your probability distribution much more uniform. In other words, you're going to make it flatter. Meanwhile, if your temperature coefficient is less than one, these scores are going to increase, which is going to make your distributions more spiky. And make the probability mass kind of be pushed towards the most likely tokens. One last thing to note about temperature is that it's not actually a decoding algorithm. It's just a way of rebalancing your probability distribution. So in fact, it can be applied to all of the sampling algorithms I described before. And some greedy decoding algorithms as well. The only one whose behavior is not affected by softmax temperature scaling is argmax decoding. Because even though you change the relative magnitudes of the probability mass in your distribution, you don't actually change the relative ranking amongst tokens in that distribution. So argmax decoding will give you the exact same output as before. But now that we're thinking about how we might be changed to the distribution that's produced by our model, we might realize that we might want to change more than the relative magnitudes I mentioned. And also instead change how they're ranked with respect to one another. Maybe in fact, our model is not a perfect approximation of what the distribution over tokens should be. You know, perhaps the training was done right or we didn't have enough training data to actually make it well calibrated. And so if we decide that our model isn't well calibrated for the task that we're doing, we may want to bring in outside information at decoding time. And so here I want to talk a bit about new classes of methods that let us change our model prediction distributions at inference time, rather than relying on a fixed static model that's that's only been trained once. And a cool way of doing this that came out last year is to actually use cane years neighbor language models, which allow you to to recalibrate your output probability distribution by using phrase statistics from let's say a much larger corpus. And so, you know, what you do in this method is that you initialize a large database of phrases along with vector representations for those phrases. And then at decoding time, you can search for the most freight similar phrases in the database. And so what you do is that, you know, you take the representation of the context that you have from your model. And then compute a similarity function with all the representations of the phrases that you have stored. And you know, based off the relative differences amongst these different phrases to your current context. You can compute a distribution over these most similar phrases. And then you can take the next tokens that follow these phrases. And then you can add the statistics around those phrases to the distribution from your model. And so this allows you to really rebalance the probability distribution that your model has given you with this this this induced distribution over phrases and interpolate them together to get a different estimate of how likely certain words are. And then one question right now is, how do you know what to cage cash. Yeah, so that's a really good question. I guess, you know, the answer there is that you could probably, you know, decide on what might be a salient set to phrases, depending on let's say named entities that you might be interested in. And then you know that you know that you know that your models distribution doesn't handle very well. But I'm pretty sure in this work, they took every phrase in their training corpus, cashed it and then relied on very efficient algorithms for doing this search over over over representation similarity to actually find the most likely once. And then you did prune the number of phrases that they actually used to make this distribution, the phrase distributions that it wasn't over the entire corpus. So it's fantastic that we can now rebalance these distributions if we find that our model is is is doing poorly for you know, particularly you know this might be relevant if let's say we're jumping into a new domain. So we've trained a nice big generation model to on Wikipedia text and now we're going into something that's you know more. More linked to stories, you know, we might want to use this type of system to get distributions from phrases from there. But you know it's also possible that we may not always have a good database of phrases to help us calibrate output distributions for all the types of text that we want to generate. And so luckily last year there's there's also been you know new approaches that look at doing this in a gradient based way. And so the idea here is that you can actually define some type of external objective using a classifier that we typically call a discriminator. And in this figure from the paper that proposed it, they called an attribute model. And what that classifier does is that it's approximates some property that you'd like to encourage your text to exhibit as you decode. So perhaps it's a sentiment classifier because you are working on a dialogue model that and you want to encourage positive sending comments. So as you generate text, you input the output of your text generation model to this attribute model. And there's some tricks on how you should do that in order to actually not have it be a discrete token that you provide to the model but instead a distribution over tokens. And so the thing is that you know if you do this the right way by using the soft distribution of tokens as inputs to the attribute model, it's going to be able to compute a score for the sequence that it receives so that you know if it's a sentiment classifier, it can evaluate how positive of the sequence you provided to it. So what you can do is that you can compute gradients with respect to this property and back propagate those gradients back to your text generation model directly. And but instead of updating the parameters, which is what you would do during training, you instead update the intermediate activations at each layer of your model, which you can then forward propagate to compute a new distribution over the sets of tokens. It's a neat trick that allows you to do real time distribution updating based on some outside discriminator that's allowing you to update your internal representations of the sequence such that it hopefully generates something more positive at the output in this case. So these distribution rebalancing methods either you know based off of nearest neighbor search or on using some type of discriminator are quite promising and interesting, but they also end up being quite computationally intensive. In the first case, you're essentially doing a search over you know thousands of phrases to to rebalance your distribution. In the second, you're doing you know multiple forwards and backwards passes at every step to try and make the tokens exhibit a particular behavior more. And unfortunately neither of them actually stop you from decoding bad sequences either it's possible, and even after you rebalance your distribution, you're still generating something that looks terrible. So in practice, something that we often use in text generation to improve our sequence outputs are what are called re-rankers. And so what we do here is that we actually decode multiple sequences, perhaps using sampling or a wider greedy search, say maybe 10. And then what we can do is that we can initialize a score to evaluate the sequences we produce and re-rank the sequences according to the score. And the simplest thing we can do is to actually just score them by the likelihood given given by the model for example, you know especially if we're using a sampling algorithm, we might want to you know make sure that we didn't generate something that you know totally deviated from good text, which would tend to have a very high perplexity. So you know this perplexity is perhaps a good re-ranking function. It's just important to be careful as well that you remember repetitive sequences tend to have very low perplexity as well. And so if you you know rank by perplexity, you're likely to just generate something you were trying to avoid in the first case. But you know we can also make our re-ranker is evaluate more complex behaviors. So in the same way that we could use gradient based methods to update our distributions to exhibit more complex behaviors, we can actually just you know take those same attribute models and use them as re-rankers to re-rank a fixed set of sequences rather than have them back propagate gradients to the main model. And so we can use them to rank things such as style, discourse, factuality, logical consistency, you know, but you know just be careful if your re-ranker ends up being poorly calculated, you know just because you've trained a classifier to predict whether a sentence makes a factual statement doesn't actually mean that it will be good at ranking different factual statements with respect to one another. And finally a nice thing about re-rankers as well is that you can use multiple re-rankers in parallel. There's multiple properties you want to score and come up with let's say a weighted average of different ranking scores to decide on what might be the best sequence according to different properties. But so you know to recap what we've talked about in terms of decoding, you know I just I want to mention that decoding is still a very challenging problem in natural language generation that we haven't really figured out yet. Our algorithms still don't you know really reflect the way that humans choose words when they speak. And our best approaches you know are currently based on trying to calibrate probability distributions produced by models to perhaps be more representative of human likelihood. But the truth is you know the human language distribution is it's quite noisy and doesn't reflect the simple properties that are decoding algorithms often capture such as probability maximization. But different decoding algorithms do allow us to perhaps inject biases that encourage different properties of going here at natural language generation. And that's allowed us to make promising improvements in this area as well. And in fact some of the most impactful advances in an algae over the last few years have really come from simple but very effective modifications to decoding algorithms. Because you can often have impact across a very large number of tasks by making a good change to a decoding algorithm. But really there's there's still a lot more work to be done in the space and hopefully you know many of you will be the ones to make these next breakthroughs. I'm happy to take questions at this point if any of popped up. Here's one question. How do you evaluate how do you tell whether rebalance distribution is better. And if I editorialize from there I guess you're admitting on this slide that you can't just look at the probability. So yes you you can't just look at the probability. I mean there is a certain amount of trust that happens that if you don't trust that your ranker is giving you a better assessment of whether you're you've produced a quality piece of text. Perhaps you shouldn't be using that re ranker in the first place and you know hopefully you've you've actually means tested that re ranker to actually show that it has that it improves the quality of text. And there's a lot more about how you can actually evaluate the quality of text later on. Though I should warn you ahead of time that the answers are not as. Not as direct and complete as you might want them to be and in fact there's there's a lot of room for interpretation in how you would actually do that. Maybe you should go on about that later but I guess people are puzzled on that because there's another question that's asking. If you said you don't know how to make a model choose was like a human. How do we model different humans from different background. Yeah that's that's a really good question. The answer to that is that you could potentially try to you know do do kind of fine tuning on the language distribution of a particular human starting from let's say a pre train language model since you'll probably never have enough data to only use a single humans outputs. Or you could try to do some of these rebalancing methods that we've spoken about to perhaps use those to actually make your distributions approach a particular humans language distribution more closely. So in the gradient based methods that I described I could perhaps train a single language model only on my type of language. Even if I have a much larger one that's trained on a much larger corpus of language from different speakers but then try to make it so that my model ranks the outputs of the main model. I've got a question. Nuclear sampling and top case sampling are really effective in practice and you made the argument that there are all these little tiny things with very little probability mass but it sums up to more probability mass. But if it sums up to more probability mass than they actually should have under the real distribution of human language shouldn't our models have been trained to put less probability mass on them. So why don't we why aren't our language models better in that case like why do we have this issue if that's if they actually are getting more probability and they should. Yeah, that's a really good question. I think the answer to that is that the way we train them which which I'll get to in a bit is really trying to is really trying to model the distribution of human language as it sees it in its training corpus and it's actually surprisingly effective at doing that. But a but at the same time when we actually start to use these language models out of the box in the NLT task that we work with first of all there's there's always you know slight defiations in the distribution of text that we're actually trying to model for the task we're doing and what the large corpus we trained on was in the first place which can make these these decoding algorithms less effective. And you know the second thing I would notice that even though these these decoding algorithms are quite effective in practice. They aren't doing what humans do when we speak we're at no point when a human speaks are we potentially trying to maximize the probability of a potential next token or randomly select a word amongst a set of tokens we ultimately have world models that drive how we select the tokens that we choose to say in order to make our points. And that's very different from what we get in probabilistic language models now you know should we throw away probabilistic language models no because as you mentioned they end up working quite well. But at some point we also do need to mitigate these differences between how humans end up speaking and how language models end up model language. So you know now that now the john has primed as perfectly for what comes next. You know let's let's jump back into training these models because you know the pipeline you know that I framed earlier being you know you train your model then you choose your decoding algorithm depending on properties you're interested in is is is great but the truth is there's interactions between your decoding model and your training algorithm that you might want to be thinking about during training which is not really what we're doing right now. And so if you recall the training algorithm that we've proposed up to this point is one where we just try to minimize the negative log likelihood of the next token in a sequence given the preceding ones at every step. And you know as I mentioned this actually works pretty well for training auto regress the models of human language but it actually causes a few issues which which john hinted at. So in the next few slides i'm going to talk a bit about these issues and then highlight some some training solutions to these problems that I found interesting or that I think are important from from the last few years. So the first issue is actually one that I hinted at in the last section which is that training with maximum likelihood. So I wanted to discourage textual diversity and I showed this sequence on a slide earlier as an example of greedy algorithms being prone to generating repetitive sequences which you know it's just about the worst form of diversity that you could that you could get. And so many algorithms are just trying to maximize the probability of the sequences that they produce so it can really only be prone to to repetitive and undiverse phrases if those phrases are scored highly by the model to begin with. And that ends up being one of the issues with maximum likelihood training is that it tends to end up favoring generic expressions because those are the ones that are you know often the most likely in human language production. As we all know and as I mentioned earlier human language production isn't about maximizing the likelihood of the words that we produce so even though we might produce generic phrases more often than engineering phrases that's not the goal that we're setting out with when we speak. There's a lot more to communication that really isn't synthesized by a training objective that tries to maximize you know probability over the human language that's being read so how can we end up mitigating this problem. You know an interesting approach that I really like that came out last year was was actually you know proposed by by wellic it all which was called unlikely hood training. And so here what you do is that you actually discourage the production of particular tokens by the model in certain contexts and so it happens that this loss term here decreases as the probability of the why neg tokens decreases so for any token that you don't want to generate as the probability of generating that token goes down. So so so does the loss term which means that you're not actually updating the model as much for the to for this particular behavior. What's important though is that you still have your teacher forcing objective so what's going to happen is that the models going to learn to capture both the distribution of language from the training corbis which it needs to do to learn how to generate text but also it's going to learn how to not say particular words that you might not want it to. And then what you can do is that you can set this list of words that you don't want the model to generate to actually be words that you've already generated before and so in essence you're you're teaching the model to not say the same things again and that's just naturally going to limit the amount of repetition that your model is going to be able to spit out and you're going to be able to generate more diverse texts as a result as well. But a second and very important issue that comes from training with with maximum likelihood is you know what we often call exposure bias, which is that the context that we train on to generate the next token are going to look different from the ones that we see a generation time. And so why might that be. The key to the challenge that happens during training is that we always get a token from a gold document or human text as it put it's the gold sequence as we call it. But then during generation we're feeding our previously generated tokens back into the model as the input rather than these these teacher force tokens that are that are from the gold documents. And so if tokens is actually quite affected by things like the distributions produced by our model and the decoding algorithm we use to get tokens. And so you know can this end up being a problem. Well yes, because as we've seen before the types of text that our model generates are often you know not a very close approximation of human language patterns in the training set. And so it could be an imbalance between the type of text that our model has learned to predict learn to predict and to expect to see and the type of text that it will see once we actually start decoding. And so once your model starts receiving its own inputs, which are going to deviate from the distribution of text to expect it's going to be very challenging for it to be able to generate coherent text going forward because it's not going to really know how to synthesize its own information that it's generated. And so there's a variety of ways to try to counter this exposure bias issue and many more that that continue to come out. And fortunately there's not really enough time to talk about all of them. So I've added slides to discuss two of them that are based on semi supervised learning here. But I really want to focus more on two other approaches that I personally find very interesting. The first is called sequence rewriting. And so you know in this setting your model first learns to retrieve a sequence from an existing database of human written prototypes. So it's kind of like our nearest neighbor decoders earlier when we cashed a bunch of phrases here you cash a bunch of sequences that might be similar to the one that you're supposed to produce for this new situation. And then what we do is that once we take this sequence and retrieve it, we learn to edit it by doing things like adding, removing or modifying tokens to more accurately reflect the context that we're actually given rather than the one that this original sequence was designed for in the first place. And so we can still use an algorithm here that tries to maximize like the training, but because there's this sort of latent variable of retrieving the right prototype that's involved, it makes it less likely that are that our generated text ends up suffering from exposure bias, because you're already starting from something that looks more like a training sequence that you might have seen. Another general class of possibilities we can do is to let our model learn to generate text by learning from its own samples. And you know this naturally maps itself nicely to reinforcement learning, which is actually one of my favorite ways to learn how to generate text. In this setting, you're going to cast your text generation model as a mark off decision process, where your state s is the models representation of the preceding context that you see your actions, a are the words that can be generated. Your policy is the decoder and your rewards are provided by some type of external score. And here you can learn many different behaviors for your text generation model by rewarding it when it exhibits those behaviors. So to kind of quickly join this framing with the perspective of the text generation models that we've seen so far, you're going to be taking actions by sampling words, hat, why from the distribution. And then you're going to feed them back into the input to get a new state, which is what we've been doing at every point. What's different though is that as you generate text, you're using some external reward function to compute rewards for each token that you generate. So you're rewarding every action that you take. And then you scale the sample loss on this on this particular token that you generate by by this reward, which is going to encourage the model to generate the sequence in similar context if the reward is high. So very clearly, you're minimizing the negative log likelihood of your sample token. So here it's not a gold token. Notice the hats that is on the Y expression in the reward function. And then you're going to compute a reward for that token and scale this negative log likelihood by that reward. So if the reward is high, the model is going to be more likely to generate this same sequence in a similar context in the future, if the reward is low, it will be less likely. But this sort of brings up a natural question, you know, what can we actually use as a reward to encourage the behaviors we want in this text generation system. And then you can use the value of the text generation system to use as you design this your generation pipeline, a common practice in the early days of using RL for text generation was to set the reward to be the final evaluation metric that you were going to evaluate on. And so here instead of having a unique reward for each generated token at every time step, you would just take the final sequence score that you get and reward every token in the generated sequence with that value. And so it's absolutely magical. You would set your evaluation metric as the reward and you'd end up learning to get more reward because that's what RL algorithms do, which in turn means that you were learning to generate sequences that do better on your evaluation metric. So an LG benchmark scores were shooting through the roof. And so it's making real progress. But it was actually all a lie, you know, turns out as I'll talk about later evaluation metrics, particularly for text generation are just approximations. And it's not always clear that optimizing to those approximations is going to lead to more and better coherent text generation. And so instead oftentimes what ends up happening is that it just learns to exploit the noise in the evaluation metric. And in fact, in their large work where they introduced Google's neural machine translation system in 2016, you know, Google researchers generally found that training machine translation models with RL and blue scores as rewards, didn't actually improve the translation quality at all, even if it did lead to higher blue scores. But so designing your reward function is a very important problem in RL for actually learning the behavior that you want. And I've listed some cool work here on how you can actually learn to tie fairly complex behaviors to reward functions by actually initializing the scores that use as rewards as neural networks that you know get trained on an auxiliary task ahead of time, but can then be used to provide scores as rewards to the system that you produce. So to go back to our example earlier of trying to create a dialogue agent that is very positive and only says positive things, you could use a sentiment classifier to produce a reward for the sequences that you generate. And so that's a lot of fun. And unfortunately, despite all the fun that you can have using RL to train text generation engines, there's a bit of a dark side to which is that reinforcement learning algorithms can be notoriously unstable. And so to get these text generation systems to learn with RL, you often have to be thorough in tuning different dials in your model setup accurately. There's many of them, two of them that I think are worth mentioning are one that you always need to pre train with teacher forcing you generally can't train with reinforcement learning from scratch. And also you need to provide some type of baseline reward that your model should be achieving. So for example, blue score, which I described earlier is always a positive value, unless it's zero. But that means that if you use it alone as a reward, every single sequence that you sample ends up, you know, being encouraged in the future. So what you want to have is some type of baseline. That is an expectation of how much reward you should be getting, which can be subtracted from the reward that you actually get so you can discourage certain behaviors as well. One last note about this is that neural networks are quite good at finding the easiest way to learn something. So, you know, if there's a way to exploit your reward function. It'll find a way to do it, particularly that's easier than learning the behavior that you want to learn. So something to remember that's kind of important if you try to use reinforcement learning for text generation systems, particularly because there's such a large action action space for words that it can generate to try to accomplish certain behaviors. So, you know, to end this section, I just want to start off by saying that in general, we still use teacher forcing as a as a primary means of learning to generate coherent text. It has diversity issues, but it still lets us learn a model with with decent text generation abilities. One thing I haven't focused too much on in this lecture is the type of model that you can use to actually generate text because they tend to be less universal and much more designed to very specific and tasks. But in general, a common approach in an LG is to try to design a neural architecture that allows your model to be perhaps less sensitive to the problems of teacher forcing or to address them with additional loss terms that are perhaps tasks specific. Exposure bias, though, is a problem everywhere, pretty much regardless of your of your neural architecture. And you know, to kind of mitigate it, you can either train your model to be more resistant to its own distribution changes through things like semi supervised learning, or you can change your your your pipeline, so that you're learning to make modifications to an existing sequence that you retrieve from your training set rather than trying to learn how to generate sequences from scratch. And the caveat there is that, you know, as the type of text that you're generating gets longer and longer, doing this, this kind of retrieval and editing becomes just as challenging as generating from scratch in many cases. And finally, you can use reinforcement learning as another means of of of learning from your own examples. And, you know, and in effect, you can also use it to encourage different behaviors than just likely with maximization. But this type of learning can end up being quite unstable and unless your reward is well shaped, the model can often learn to exploit it. And then you can also use the same kind of training, you know, like the other. So online language simulators where you can train with our online, or are you only talking about training offline. No, in this setting, we're generally talking about training offline. So you you train it all ahead of time and then you use your model as it's been trained the first time. Okay, yeah, so now this we've we finally reached the section that I hinted at earlier on it on a very important topic, evaluation. And you know, to be honest is something that we should be thinking about before we even start designing a model or training algorithm or or decoding algorithm. It's, you know, how can we actually check that our method is is how can we actually go into a value that our method is even working. I want to talk a bit about three types of evaluation metrics. So first we'll talk about automatic e-val metrics, because generally you need to be able to rapidly prototype and diagnose failures in your model. So it's essential to be able to have this this quick feedback, even if it's very course and an automatic evaluation metrics. And you know, we've traditionally use what I'm calling content overlap metrics, which focus on how much a sequence explicitly resembles another sequence, usually in terms of word or phrase matching. And so, lately there's also been a new automatic evaluations that are that are model based where we try to use advances and embeddings and neural models to define more implicit similarity measures between sequences. And finally, we'll talk a bit about human evaluations, which are kind of the gold standard of of evaluating text generation. And they also do have downsides that that we'll get to as well. And I just want to note that some of these slides here are actually repurposed from slides from us, each of you must who's a leading expert on an LG evaluation. But so let's jump in. So content overlap metrics generally compute an explicit similarity score between two sequences, the one that's been generated by your model and some gold standard reference sequence that was attached to the to the inputs that you had. So in other words, a sequence that you know was an appropriate generation for the inputs you had. And these metrics are often a popular starting point because they're fast and very efficient, which is their main benefit, as long as you have a reference sequence to compare to you can compute these scores rapidly to get feedback. And I'm going to categorize them into two zones here. First, and gram overlap metrics that compute different functions of word and word like overlap and semantic overlap metrics, which involve more complex overlap functions based off semantic structures. Unfortunately, besides being fast and efficient, most and gram overlap metrics don't actually give you a great approximation of sequence quality a lot of times. They're already not ideal for something like machine translation where there can be multiple ways to translate the same sequence with things like synonyms and they get progressively much worse for tasks that are more open ended than empty. So for example, in summarization, you know, the longer output texts make it naturally harder to measure something using word match. And it's something like dialogue. It's incredibly open ended. And in fact, you can have multiple responses to a particular utterance that you know mean the same thing, but don't use any common words. And so we can illustrate this with a simple fairly contrived example where you know have a dialogue context utterance that asks a question such as, you know, are you going to Antoine's incredible CS 224 lecture, a completely unbiased reference text from, you know, derived from a human. And then a dialogue agent that spits out different answers such as, yes, which gets a pretty high score on our n gram overlap metric or you know it, which already scores a lot lower despite depicting, you know, the exact same idea or up, which actually gets a score of zero. And meanwhile, a completely incorrect answer heck no gets the highest score out of all of them, which kind of points to the issue, you know, when you use n gram overlap measures. In a lot of applications, you're going to be missing the salient elements of what the generated sequence should capture. And instead, you know, the getting stylistic similarities between text, even as you miss the most important context. And if you prefer empirical validations to contrived examples, it's actually been shown that many dialogue evaluation metrics don't really correlate well with human judgments at all. And this gets worse as your sequence length increases typically. So with an open ended task like story generation, you can get improved scores by just matching a whole lot of stopboards that have nothing to do with the content of the story itself. And we also have another category of overlap metrics that I'll call semantic overlap metrics because they don't necessarily tied directly to the words you used. But instead try to create conceptual representations of the generated and reference outputs instead. So the center one here spice, for example, creates a scene graph of your of your generated text and then compares that to your to your reference caption to see how similar this this more semantic representation and stuff being. But clearly there's there's there's some limitations to how well explicit content overlap metrics can do, particularly as we start thinking about more open and to task. So in response of the last years, there's been a focus on using model based metrics, whose representations come from, you know, machine learning models and where they can be used actually evaluate the fidelity of generated text. And these are nice because there's no more needing explicit matches between words and your reference and generated text instead you can rely on much more implicit notions of similarity that you get from word embeddings. And so, you know, there's been a lot of models develop in this area, some of the original ones kind of focused on, you know, defining composition functions over the embeddings of words and your generated and reference sequences and then computing a distance between the compositions of the two sequences. Some more, you know, so some more involved takes on this idea are things like word movers distance where here you actually try to map word vectors in both your generated and reference sequences into pairs with one another. So each word vector is paired to another work vector in the opposite sequence and the distance between them is computed and then you allow the evaluation metric to actually compute the optimal matchings between these pairs of words such as the total distance is minimized and bird score, which has become quite popular over the last year. So as an evaluation metric is pretty much just word movers distance but using contextualized bird embeddings to compute these distances. Sentence movers similarity is kind of another extension of word movers similarity that adds sentence embeddings from recurrent no networks to this distance optimization and that allows it to be more effective for evaluating let's say long multi sentence text as opposed to just single single sentences. And finally last year there was a new model named blurt, which is actually a regression model that's based on birds and here it takes a pair of sentences, the reference on the generated one and returns a score that indicates to what extent the candidate is grammatical and conveys the meaning of the reference text. So you know we can we can talk about a lot more evaluation metrics that are computed automatically and you know there's there's far more of them than I then I actually mentioned though they do tend to fit in those two categories that I described. But it's it's important to remember that at the end of the day the true mark on an LG systems performance is whether it's valuable to the human user that has to interact with it or read the text that's produced from it. And unfortunately automatic metrics tend to fall short of replicating human opinions of the quality of generated text and that's why you know for this reason human evaluations are viewed as the most important form of evaluation for text generation systems almost all work in an LG generally includes some form of human evaluation particularly if the task is more open ended. And if it doesn't well you know let me be frank you should probably be skeptical of any claim that's being made by that work. And finally you know another use of human evaluations is that in addition to to to evaluating model performance you can also use them to actually train new machine learning models that are meant to be evaluate that are meant to serve as evaluation scoring functions themselves. So I guess the you know the main thing to mention about human evaluations since you know we can we can talk about them for a while is that they're both very simple but also very difficult to run. You know they're simple because you generally just need to find judges and ask them to rate an intrinsic dimension of the quality of your generated text. So you have to define a set of criteria that you decide is important for the task that you're designing a system for and that can be things like fluency where you just you measure things like grammar spelling or choice does this actually look like human language. So the text does the text accurately reflect facts that are described in the context, you know common sense doesn't sort of follow the logical rules of the world that we might expect. But once you once you've defined these criteria you can have humans evaluate the generated text for how well it actually produces text that that that that caters to the to these particular criteria. So here is that you know while these you know dimensions are common and repeated across different evaluations. They can you know often be referred to by other names and you know explain to evaluators in different terms and be measured in different ways and in fact one of the problems with human evaluations is that across works they tend to be very unstandardized which can make the replication of human results quite difficult and that's why when you read text generation papers you rarely see a comparison of human evaluation scores between two different studies. Even if they evaluated the same dimensions. But you know another set of issues with human evaluations beyond the fact that they're slow expensive and unstandardized is that you know humans themselves aren't actually perfect. And you know I guess as a few negatives that I can say about humans you know even though we're all humans is that we tend to not be very consistent folks you know often changing our minds about how we've used something depending on something as as you know trivial as the time of day. And I don't always reason in the way that we're expected to when presented with with a task such as evaluating something we can lose concentrated lose concentration and not really be focused on what we're doing and you know we can often misinterpret what let's say human evaluation is asking us to do such that we inject our own biases into the task. And you know on top of all these things you know when we run human evaluations we're also dealing with the fact that one of the big motivators that our human judges have is to do the task as quickly as possible which isn't a great mix particularly if we want them to really give us high quality ratings. But you know humans are kind of the best thing that we that we have to actually give us the most accurate assessments of whether text generation systems are are doing well so we so we do the best that we can. I'm actually going to skip this slide but I mentioned earlier that we know one of the things that we can also do is use human ratings to train models to actually predict scores for text itself and so two systems that that that do some things along these lines provided citations to here so that you can take a look at them if you're curious later on. So you know the takeaways I kind of want you to get from the section are that you know evaluation is quite hard and particularly in text generation so content overlap metrics do provide a good starting point for evaluating the quality of generated text you know if you run these and grab overlap metrics and they show scores that are worse than they should be that's the first sign that you that you have problem but they're generally not not good enough on their own. Model based metrics tend to be you know more correlated with human judgments than content overlap once particularly as the tasks become more open ended such as you know dialogue and storytelling but the downside there's that they're they're not very interpretable you know unlike with a content overlap metric where you can say exactly oh this is why this score is this way it's because these words match up with these words with a model based metric you you get a much more implicit definition of similarity which you know while useful is also less. Interpretable human judgments are absolutely critical because even if they're inconsistent and sometimes don't do the tasks that you want them to humans are actually able to intrinsically evaluate dimensions that we that we don't even know how to formulate using any type of automatic metric. But lastly slightly unrelated to what I've spoken about the section you know I just I just want to say that the number one evaluator of any NLG system that you create should should really be you you know a look at your models outputs as perhaps you do a project that involves an LG can really be worth days weeks and sometimes months of staring at evaluation metrics that are that are perhaps a bit informative so if you design an LG systems make sure to evaluate your own generations very consistently. So in this last section I think it's quite important to talk about ethical topics in natural language generation as well because you know ultimately while NLG really allows us to tackle new and interesting applications you know if we're not capable we can end up deploying fairly dangerous and harmful systems. And as a warning I just want to say that you know some of the content on the next use slides can you know is going to potentially be quite uncomfortable. But I think it's important to make very clear how these systems can go very very wrong. And without picking on a particular example I do perhaps think that you know that one of the most famous examples of this was the Tay dialogue chatbots which was released onto Twitter in 2016. And within 24 hours it had started you know making some very nasty comments that that exhibited racist sexist anti-submitting white supremacist leanings which you know is ultimately probably not with the designers of Tay had in mind. So what actually ended up going wrong with Tay well you know here's the thing Tay behaved exactly as we should have expected it would. It was designed to learn to exhibit the conversational patterns of the users that it interacted with and it did that you know NLG models are very good at capturing the language distribution of their training examples that's been the one thing that we've been remarkably consistent on. You know the last few years and it turns out if they're training examples end up having toxic content they will learn to repeat that content. And this is perhaps no clearer than if you learn at what pre-trained language models have to say about let's say different demographics. So if you remember you know large pre-trained language models that underlie many modern NLG systems they're trained on massive corporate text which are often opaque and crawled from online resources. If it turns out that those corporate have toxic content the language models are going to learn it's and in fact make it even worse. And then it turns out that if you prompt these language models for certain pieces of information it can spit out that toxic content showing you know very different opinions across you know gender races sexual orientations. Now you know I can see that it would actually be rare to ask a language model to weigh in with their opinions on this matter but you do have to ask yourself if this type of information is encoded in the model in some way in what other ways could these learn patterns end up being reflected by this model once it's actually deployed in practice. And that kind of leads us to the second problem with these language models. And we actually don't really know how information ends up being learned and encoded by them which means that we don't have a rigorous understanding of what types of inputs are going to trigger what types of outputs. And in fact Wallace it all showed in their EM and LP 2019 work that this was a big problem because if you prime these models with particularly adversarial inputs they would generally devolve immediately into producing very toxic content. In other words what it took to 24 hours to learn how to do these systems can kind of do out of the box if prying with the wrong examples. And unfortunately the wrong examples. And so it ends up being a lot less nasty than we might have expected a lot less nasty than the ones on the previous slide at the very least. But you know in a work at EM and LP findings last year. And it was not as consistent as in the previous work but it was still often enough. And these examples really go to show that we need to be careful with how these systems are deployed. If you have an NLG system you need safeguards to stop it from outputting harmful content. And also the number of toxic and bi toxicity and biosefaction today you know a model that can be primed to generate incorrect or unfactual information can be quite dangerous too. And also NLG models shouldn't be deployed without an understanding of who its users will be. And there's always going to be adversarial users for any model that you create even if you can't think of them in the moment. And so the final point which is that you know the advances in NLG have really you know allowed us to build text production systems for many new applications. You know as we do this though it's it's important to ask you know does the content that we're building a system to automatically generate. And so for easy human ingestion you know does it really need to be generated automatically. And I think a good example of this is the work of sellers at all at NURBS 2019 which you know showed off the potential dangers of fake news generators from pre train language models. I actually thought this was a great work and it highlighted you know many of the defenses that you could be developed against fake news generations system. But you know the point is more so that you should always imagine that any tool that you create could be used in a negative way. So a storytelling NLG system can also potentially be you know repurposed to do fake news generation. And you should really always ask yourself whether the positive applications of a particular technology outweigh the potential negative ones. And that turns out to often not be an easy question. So I guess as concluding thoughts for today. You know I just want to mention that you know if you start interacting with NLG systems and practice you're quickly going to see the fairly large limitations that they tend to have. Even in tasks where we've you know achieved a larger amount of progress at building systems that can do the task fairly well. There's still a lot of improvements that can be made to make them even better. And pretty much any NLG task at the same time evaluating it effectively remains a huge challenge. And you often have to rely on humans to give us the best estimates of how well our system is doing. And so an area where a large improvement would really you know kind of bootstrap larger improvements in many other areas of NLG would be to find better automatic evaluation. On the other hand on a very you know optimistic note I do want to say that with the advent of large skill language models. You know deep NLG research hasn't hasn't been reset but it's never been easier to jump in the space and start playing around with the systems and designing cool new tools that can that can help humans and just content and information more rapidly and more efficiently. And as a result I think that it's you know one of the most exciting areas of NLP to work in and I think that that you know if you start working in it as well you'll feel the same way and I would encourage you to do so. Thanks a lot for having me today. It was really exciting.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.0, "text": " Hi everybody, welcome back to CS224N.", "tokens": [2421, 2201, 11, 2928, 646, 281, 9460, 7490, 19, 45, 13], "temperature": 0.0, "avg_logprob": -0.2285471077424934, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.0912797674536705}, {"id": 1, "seek": 0, "start": 9.0, "end": 13.0, "text": " First just a couple of announcements.", "tokens": [2386, 445, 257, 1916, 295, 23785, 13], "temperature": 0.0, "avg_logprob": -0.2285471077424934, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.0912797674536705}, {"id": 2, "seek": 0, "start": 13.0, "end": 16.0, "text": " Originally this was going to be the day when assignment five was due,", "tokens": [28696, 341, 390, 516, 281, 312, 264, 786, 562, 15187, 1732, 390, 3462, 11], "temperature": 0.0, "avg_logprob": -0.2285471077424934, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.0912797674536705}, {"id": 3, "seek": 0, "start": 16.0, "end": 20.0, "text": " but as you've seen, we're giving you one extra day.", "tokens": [457, 382, 291, 600, 1612, 11, 321, 434, 2902, 291, 472, 2857, 786, 13], "temperature": 0.0, "avg_logprob": -0.2285471077424934, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.0912797674536705}, {"id": 4, "seek": 0, "start": 20.0, "end": 22.0, "text": " So it's now due Friday at 430.", "tokens": [407, 309, 311, 586, 3462, 6984, 412, 1017, 3446, 13], "temperature": 0.0, "avg_logprob": -0.2285471077424934, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.0912797674536705}, {"id": 5, "seek": 0, "start": 22.0, "end": 27.0, "text": " We do realize that assignment five has been a bit of a tough", "tokens": [492, 360, 4325, 300, 15187, 1732, 575, 668, 257, 857, 295, 257, 4930], "temperature": 0.0, "avg_logprob": -0.2285471077424934, "compression_ratio": 1.4236453201970443, "no_speech_prob": 0.0912797674536705}, {"id": 6, "seek": 2700, "start": 27.0, "end": 31.0, "text": " challenge for many people, though we've been trying to help people out and offer", "tokens": [3430, 337, 867, 561, 11, 1673, 321, 600, 668, 1382, 281, 854, 561, 484, 293, 2626], "temperature": 0.0, "avg_logprob": -0.1685353914896647, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.001146534807048738}, {"id": 7, "seek": 2700, "start": 31.0, "end": 32.0, "text": " a thousand otherwise.", "tokens": [257, 4714, 5911, 13], "temperature": 0.0, "avg_logprob": -0.1685353914896647, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.001146534807048738}, {"id": 8, "seek": 2700, "start": 32.0, "end": 37.0, "text": " So I hope at the end of the day it will seem like it was a really good learning experience", "tokens": [407, 286, 1454, 412, 264, 917, 295, 264, 786, 309, 486, 1643, 411, 309, 390, 257, 534, 665, 2539, 1752], "temperature": 0.0, "avg_logprob": -0.1685353914896647, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.001146534807048738}, {"id": 9, "seek": 2700, "start": 37.0, "end": 43.0, "text": " to really get some much more kind of close hands on, look at how transformers work,", "tokens": [281, 534, 483, 512, 709, 544, 733, 295, 1998, 2377, 322, 11, 574, 412, 577, 4088, 433, 589, 11], "temperature": 0.0, "avg_logprob": -0.1685353914896647, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.001146534807048738}, {"id": 10, "seek": 2700, "start": 43.0, "end": 50.0, "text": " rather than it's simply being loading up a transformer as a black mystery box.", "tokens": [2831, 813, 309, 311, 2935, 885, 15114, 493, 257, 31782, 382, 257, 2211, 11422, 2424, 13], "temperature": 0.0, "avg_logprob": -0.1685353914896647, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.001146534807048738}, {"id": 11, "seek": 2700, "start": 50.0, "end": 56.0, "text": " After Friday, I guess there's no rest since we do really hope that you can sort of", "tokens": [2381, 6984, 11, 286, 2041, 456, 311, 572, 1472, 1670, 321, 360, 534, 1454, 300, 291, 393, 1333, 295], "temperature": 0.0, "avg_logprob": -0.1685353914896647, "compression_ratio": 1.6441947565543071, "no_speech_prob": 0.001146534807048738}, {"id": 12, "seek": 5600, "start": 56.0, "end": 57.0, "text": " get a lot of help.", "tokens": [483, 257, 688, 295, 854, 13], "temperature": 0.0, "avg_logprob": -0.3534607101272751, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0007939363713376224}, {"id": 13, "seek": 5600, "start": 57.0, "end": 61.0, "text": " Basically immediately transition to working on final projects and so it's basically", "tokens": [8537, 4258, 6034, 281, 1364, 322, 2572, 4455, 293, 370, 309, 311, 1936], "temperature": 0.0, "avg_logprob": -0.3534607101272751, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0007939363713376224}, {"id": 14, "seek": 5600, "start": 61.0, "end": 66.0, "text": " four weeks to go on final projects and in particular we're hoping to get feedback", "tokens": [1451, 3259, 281, 352, 322, 2572, 4455, 293, 294, 1729, 321, 434, 7159, 281, 483, 5824], "temperature": 0.0, "avg_logprob": -0.3534607101272751, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0007939363713376224}, {"id": 15, "seek": 5600, "start": 66.0, "end": 71.0, "text": " on your project proposals back by next Tuesday to help that process or on", "tokens": [322, 428, 1716, 20198, 646, 538, 958, 10017, 281, 854, 300, 1399, 420, 322], "temperature": 0.0, "avg_logprob": -0.3534607101272751, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0007939363713376224}, {"id": 16, "seek": 5600, "start": 71.0, "end": 76.0, "text": " when people the after get started on them soon.", "tokens": [562, 561, 264, 934, 483, 1409, 322, 552, 2321, 13], "temperature": 0.0, "avg_logprob": -0.3534607101272751, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0007939363713376224}, {"id": 17, "seek": 5600, "start": 76.0, "end": 83.0, "text": " And you know, it's just maybe a good moment to say that we do really appreciate", "tokens": [400, 291, 458, 11, 309, 311, 445, 1310, 257, 665, 1623, 281, 584, 300, 321, 360, 534, 4449], "temperature": 0.0, "avg_logprob": -0.3534607101272751, "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.0007939363713376224}, {"id": 18, "seek": 8300, "start": 83.0, "end": 88.0, "text": " all the people have been putting tons of effort into these assignments and we like", "tokens": [439, 264, 561, 362, 668, 3372, 9131, 295, 4630, 666, 613, 22546, 293, 321, 411], "temperature": 0.0, "avg_logprob": -0.21583668066530812, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.000886550813447684}, {"id": 19, "seek": 8300, "start": 88.0, "end": 92.0, "text": " that sort of all the keenness we're seeing from the students.", "tokens": [300, 1333, 295, 439, 264, 20297, 1287, 321, 434, 2577, 490, 264, 1731, 13], "temperature": 0.0, "avg_logprob": -0.21583668066530812, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.000886550813447684}, {"id": 20, "seek": 8300, "start": 92.0, "end": 99.0, "text": " Okay, so with that out of the way today, I'm delighted to have giving today's lecture on", "tokens": [1033, 11, 370, 365, 300, 484, 295, 264, 636, 965, 11, 286, 478, 18783, 281, 362, 2902, 965, 311, 7991, 322], "temperature": 0.0, "avg_logprob": -0.21583668066530812, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.000886550813447684}, {"id": 21, "seek": 8300, "start": 99.0, "end": 104.0, "text": " your language generation, Antoine Bosleau, who's at present a postdoc at Stanford.", "tokens": [428, 2856, 5125, 11, 5130, 44454, 22264, 306, 1459, 11, 567, 311, 412, 1974, 257, 2183, 39966, 412, 20374, 13], "temperature": 0.0, "avg_logprob": -0.21583668066530812, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.000886550813447684}, {"id": 22, "seek": 8300, "start": 104.0, "end": 109.0, "text": " He's someone who's done a lot of work on natural language generation in his", "tokens": [634, 311, 1580, 567, 311, 1096, 257, 688, 295, 589, 322, 3303, 2856, 5125, 294, 702], "temperature": 0.0, "avg_logprob": -0.21583668066530812, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.000886550813447684}, {"id": 23, "seek": 10900, "start": 109.0, "end": 116.0, "text": " previous life as a university, Washington PhD student and next year he's going to be", "tokens": [3894, 993, 382, 257, 5454, 11, 6149, 14476, 3107, 293, 958, 1064, 415, 311, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.25370108191646745, "compression_ratio": 1.3804347826086956, "no_speech_prob": 0.0001820699981180951}, {"id": 24, "seek": 10900, "start": 116.0, "end": 121.0, "text": " taking up a position as a professor in Switzerland.", "tokens": [1940, 493, 257, 2535, 382, 257, 8304, 294, 23312, 13], "temperature": 0.0, "avg_logprob": -0.25370108191646745, "compression_ratio": 1.3804347826086956, "no_speech_prob": 0.0001820699981180951}, {"id": 25, "seek": 10900, "start": 121.0, "end": 124.0, "text": " Okay, so welcome and Trams.", "tokens": [1033, 11, 370, 2928, 293, 1765, 4070, 13], "temperature": 0.0, "avg_logprob": -0.25370108191646745, "compression_ratio": 1.3804347826086956, "no_speech_prob": 0.0001820699981180951}, {"id": 26, "seek": 10900, "start": 124.0, "end": 128.0, "text": " Thanks Chris, that's a very kind introduction.", "tokens": [2561, 6688, 11, 300, 311, 257, 588, 733, 9339, 13], "temperature": 0.0, "avg_logprob": -0.25370108191646745, "compression_ratio": 1.3804347826086956, "no_speech_prob": 0.0001820699981180951}, {"id": 27, "seek": 10900, "start": 128.0, "end": 133.0, "text": " It's great to be here giving this lecture.", "tokens": [467, 311, 869, 281, 312, 510, 2902, 341, 7991, 13], "temperature": 0.0, "avg_logprob": -0.25370108191646745, "compression_ratio": 1.3804347826086956, "no_speech_prob": 0.0001820699981180951}, {"id": 28, "seek": 13300, "start": 133.0, "end": 140.0, "text": " I'm going to see CS224N particularly on one of my favorite topics in deep learning", "tokens": [286, 478, 516, 281, 536, 9460, 7490, 19, 45, 4098, 322, 472, 295, 452, 2954, 8378, 294, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.18348746048776726, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00011950611951760948}, {"id": 29, "seek": 13300, "start": 140.0, "end": 144.0, "text": " for NLP natural language generation.", "tokens": [337, 426, 45196, 3303, 2856, 5125, 13], "temperature": 0.0, "avg_logprob": -0.18348746048776726, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00011950611951760948}, {"id": 30, "seek": 13300, "start": 144.0, "end": 149.0, "text": " So hopefully by the end of this lecture, most of you will have at least learned a bit", "tokens": [407, 4696, 538, 264, 917, 295, 341, 7991, 11, 881, 295, 291, 486, 362, 412, 1935, 3264, 257, 857], "temperature": 0.0, "avg_logprob": -0.18348746048776726, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00011950611951760948}, {"id": 31, "seek": 13300, "start": 149.0, "end": 154.0, "text": " about NLG with deep learning and hopefully be motivated to start doing some research", "tokens": [466, 426, 43, 38, 365, 2452, 2539, 293, 4696, 312, 14515, 281, 722, 884, 512, 2132], "temperature": 0.0, "avg_logprob": -0.18348746048776726, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00011950611951760948}, {"id": 32, "seek": 13300, "start": 154.0, "end": 162.0, "text": " on it or launch a start up in NLG or perhaps go work on it in a larger organization.", "tokens": [322, 309, 420, 4025, 257, 722, 493, 294, 426, 43, 38, 420, 4317, 352, 589, 322, 309, 294, 257, 4833, 4475, 13], "temperature": 0.0, "avg_logprob": -0.18348746048776726, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.00011950611951760948}, {"id": 33, "seek": 16200, "start": 162.0, "end": 168.0, "text": " Okay, so to start, I think it might be really helpful to define what we meet at a high", "tokens": [1033, 11, 370, 281, 722, 11, 286, 519, 309, 1062, 312, 534, 4961, 281, 6964, 437, 321, 1677, 412, 257, 1090], "temperature": 0.0, "avg_logprob": -0.11105208280609875, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.00010066544928122312}, {"id": 34, "seek": 16200, "start": 168.0, "end": 173.0, "text": " level when we talk about natural language generation because over the last few", "tokens": [1496, 562, 321, 751, 466, 3303, 2856, 5125, 570, 670, 264, 1036, 1326], "temperature": 0.0, "avg_logprob": -0.11105208280609875, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.00010066544928122312}, {"id": 35, "seek": 16200, "start": 173.0, "end": 177.0, "text": " years, the definition has actually sort of changed and has really grown as a", "tokens": [924, 11, 264, 7123, 575, 767, 1333, 295, 3105, 293, 575, 534, 7709, 382, 257], "temperature": 0.0, "avg_logprob": -0.11105208280609875, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.00010066544928122312}, {"id": 36, "seek": 16200, "start": 177.0, "end": 184.0, "text": " subfield to really encapsulate any part of NLP that involves the production of", "tokens": [1422, 7610, 281, 534, 38745, 5256, 604, 644, 295, 426, 45196, 300, 11626, 264, 4265, 295], "temperature": 0.0, "avg_logprob": -0.11105208280609875, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.00010066544928122312}, {"id": 37, "seek": 16200, "start": 184.0, "end": 187.0, "text": " written or spoken language.", "tokens": [3720, 420, 10759, 2856, 13], "temperature": 0.0, "avg_logprob": -0.11105208280609875, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.00010066544928122312}, {"id": 38, "seek": 18700, "start": 187.0, "end": 192.0, "text": " So in other words, if you're given some inputs and your goal is to generate text", "tokens": [407, 294, 661, 2283, 11, 498, 291, 434, 2212, 512, 15743, 293, 428, 3387, 307, 281, 8460, 2487], "temperature": 0.0, "avg_logprob": -0.1338442393711635, "compression_ratio": 1.5967078189300412, "no_speech_prob": 5.561376019613817e-05}, {"id": 39, "seek": 18700, "start": 192.0, "end": 198.0, "text": " to describe, respond, translate or summary is that piece of text.", "tokens": [281, 6786, 11, 4196, 11, 13799, 420, 12691, 307, 300, 2522, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1338442393711635, "compression_ratio": 1.5967078189300412, "no_speech_prob": 5.561376019613817e-05}, {"id": 40, "seek": 18700, "start": 198.0, "end": 202.0, "text": " NLG really focuses on how you can actually build a system that can automatically", "tokens": [426, 43, 38, 534, 16109, 322, 577, 291, 393, 767, 1322, 257, 1185, 300, 393, 6772], "temperature": 0.0, "avg_logprob": -0.1338442393711635, "compression_ratio": 1.5967078189300412, "no_speech_prob": 5.561376019613817e-05}, {"id": 41, "seek": 18700, "start": 202.0, "end": 211.0, "text": " produce a coherent and useful written piece of text for that human consumption.", "tokens": [5258, 257, 36239, 293, 4420, 3720, 2522, 295, 2487, 337, 300, 1952, 12126, 13], "temperature": 0.0, "avg_logprob": -0.1338442393711635, "compression_ratio": 1.5967078189300412, "no_speech_prob": 5.561376019613817e-05}, {"id": 42, "seek": 18700, "start": 211.0, "end": 215.0, "text": " And it used to be a much more limited research area since many tasks that we now", "tokens": [400, 309, 1143, 281, 312, 257, 709, 544, 5567, 2132, 1859, 1670, 867, 9608, 300, 321, 586], "temperature": 0.0, "avg_logprob": -0.1338442393711635, "compression_ratio": 1.5967078189300412, "no_speech_prob": 5.561376019613817e-05}, {"id": 43, "seek": 21500, "start": 215.0, "end": 220.0, "text": " view as NLG problems didn't actually involve much text production prior to", "tokens": [1910, 382, 426, 43, 38, 2740, 994, 380, 767, 9494, 709, 2487, 4265, 4059, 281], "temperature": 0.0, "avg_logprob": -0.10541956765311104, "compression_ratio": 1.5719844357976653, "no_speech_prob": 5.561232319450937e-05}, {"id": 44, "seek": 21500, "start": 220.0, "end": 225.0, "text": " neural networks. But now that scope has expanded considerably and we have this", "tokens": [18161, 9590, 13, 583, 586, 300, 11923, 575, 14342, 31308, 293, 321, 362, 341], "temperature": 0.0, "avg_logprob": -0.10541956765311104, "compression_ratio": 1.5719844357976653, "no_speech_prob": 5.561232319450937e-05}, {"id": 45, "seek": 21500, "start": 225.0, "end": 229.0, "text": " much larger area to sort of work in.", "tokens": [709, 4833, 1859, 281, 1333, 295, 589, 294, 13], "temperature": 0.0, "avg_logprob": -0.10541956765311104, "compression_ratio": 1.5719844357976653, "no_speech_prob": 5.561232319450937e-05}, {"id": 46, "seek": 21500, "start": 229.0, "end": 234.0, "text": " Unfortunately, we're not quite yet at the level of the types of A.I. NLG", "tokens": [8590, 11, 321, 434, 406, 1596, 1939, 412, 264, 1496, 295, 264, 3467, 295, 316, 13, 40, 13, 426, 43, 38], "temperature": 0.0, "avg_logprob": -0.10541956765311104, "compression_ratio": 1.5719844357976653, "no_speech_prob": 5.561232319450937e-05}, {"id": 47, "seek": 21500, "start": 234.0, "end": 238.0, "text": " tools that we've seen in pop culture and that we like to imagine.", "tokens": [3873, 300, 321, 600, 1612, 294, 1665, 3713, 293, 300, 321, 411, 281, 3811, 13], "temperature": 0.0, "avg_logprob": -0.10541956765311104, "compression_ratio": 1.5719844357976653, "no_speech_prob": 5.561232319450937e-05}, {"id": 48, "seek": 21500, "start": 238.0, "end": 243.0, "text": " But we are starting to see many areas where NLG tools are having a massive", "tokens": [583, 321, 366, 2891, 281, 536, 867, 3179, 689, 426, 43, 38, 3873, 366, 1419, 257, 5994], "temperature": 0.0, "avg_logprob": -0.10541956765311104, "compression_ratio": 1.5719844357976653, "no_speech_prob": 5.561232319450937e-05}, {"id": 49, "seek": 24300, "start": 243.0, "end": 251.0, "text": " impact to start with machine translation is kind of the classical example of an", "tokens": [2712, 281, 722, 365, 3479, 12853, 307, 733, 295, 264, 13735, 1365, 295, 364], "temperature": 0.0, "avg_logprob": -0.17101540622940983, "compression_ratio": 1.5022421524663676, "no_speech_prob": 5.475224679685198e-05}, {"id": 50, "seek": 24300, "start": 251.0, "end": 258.0, "text": " NLG task these days ever since the task moved to neural networks and", "tokens": [426, 43, 38, 5633, 613, 1708, 1562, 1670, 264, 5633, 4259, 281, 18161, 9590, 293], "temperature": 0.0, "avg_logprob": -0.17101540622940983, "compression_ratio": 1.5022421524663676, "no_speech_prob": 5.475224679685198e-05}, {"id": 51, "seek": 24300, "start": 258.0, "end": 262.0, "text": " a NLG framework around a 2014 or so.", "tokens": [257, 426, 43, 38, 8388, 926, 257, 8227, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.17101540622940983, "compression_ratio": 1.5022421524663676, "no_speech_prob": 5.475224679685198e-05}, {"id": 52, "seek": 24300, "start": 262.0, "end": 266.0, "text": " And now we've seen a rapid improvement in the quality and applicability of", "tokens": [400, 586, 321, 600, 1612, 257, 7558, 10444, 294, 264, 3125, 293, 2580, 2310, 295], "temperature": 0.0, "avg_logprob": -0.17101540622940983, "compression_ratio": 1.5022421524663676, "no_speech_prob": 5.475224679685198e-05}, {"id": 53, "seek": 24300, "start": 266.0, "end": 270.0, "text": " translation systems. In fact, you can often use Google Translates for most", "tokens": [12853, 3652, 13, 682, 1186, 11, 291, 393, 2049, 764, 3329, 6531, 75, 1024, 337, 881], "temperature": 0.0, "avg_logprob": -0.17101540622940983, "compression_ratio": 1.5022421524663676, "no_speech_prob": 5.475224679685198e-05}, {"id": 54, "seek": 27000, "start": 270.0, "end": 277.0, "text": " of your kind of retail translation needs as a good starting point.", "tokens": [295, 428, 733, 295, 10800, 12853, 2203, 382, 257, 665, 2891, 935, 13], "temperature": 0.0, "avg_logprob": -0.13846095748569653, "compression_ratio": 1.5084033613445378, "no_speech_prob": 5.919728573644534e-05}, {"id": 55, "seek": 27000, "start": 277.0, "end": 282.0, "text": " Similarly, NLG technologies really underpin some of the dialogue systems that", "tokens": [13157, 11, 426, 43, 38, 7943, 534, 833, 17836, 512, 295, 264, 10221, 3652, 300], "temperature": 0.0, "avg_logprob": -0.13846095748569653, "compression_ratio": 1.5084033613445378, "no_speech_prob": 5.919728573644534e-05}, {"id": 56, "seek": 27000, "start": 282.0, "end": 287.0, "text": " you might interact with on a daily basis. Any time you use, let's say,", "tokens": [291, 1062, 4648, 365, 322, 257, 5212, 5143, 13, 2639, 565, 291, 764, 11, 718, 311, 584, 11], "temperature": 0.0, "avg_logprob": -0.13846095748569653, "compression_ratio": 1.5084033613445378, "no_speech_prob": 5.919728573644534e-05}, {"id": 57, "seek": 27000, "start": 287.0, "end": 292.0, "text": " Siri, Alexa, Cortana, Google Home, Bixby, or pretty much any other major", "tokens": [33682, 11, 22595, 11, 28522, 2095, 11, 3329, 8719, 11, 363, 970, 2322, 11, 420, 1238, 709, 604, 661, 2563], "temperature": 0.0, "avg_logprob": -0.13846095748569653, "compression_ratio": 1.5084033613445378, "no_speech_prob": 5.919728573644534e-05}, {"id": 58, "seek": 27000, "start": 292.0, "end": 295.0, "text": " companies dialogue system. There's a good chance that there's a neural", "tokens": [3431, 10221, 1185, 13, 821, 311, 257, 665, 2931, 300, 456, 311, 257, 18161], "temperature": 0.0, "avg_logprob": -0.13846095748569653, "compression_ratio": 1.5084033613445378, "no_speech_prob": 5.919728573644534e-05}, {"id": 59, "seek": 29500, "start": 295.0, "end": 300.0, "text": " NLG component embedded in that system that's involved with providing you an answer", "tokens": [426, 43, 38, 6542, 16741, 294, 300, 1185, 300, 311, 3288, 365, 6530, 291, 364, 1867], "temperature": 0.0, "avg_logprob": -0.10875832914101957, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00010718344856286421}, {"id": 60, "seek": 29500, "start": 300.0, "end": 304.0, "text": " to your query. And there's really still a ton of progress to be made in this area.", "tokens": [281, 428, 14581, 13, 400, 456, 311, 534, 920, 257, 2952, 295, 4205, 281, 312, 1027, 294, 341, 1859, 13], "temperature": 0.0, "avg_logprob": -0.10875832914101957, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00010718344856286421}, {"id": 61, "seek": 29500, "start": 304.0, "end": 308.0, "text": " And it's led to some major companies to actually crowdsource chatbot", "tokens": [400, 309, 311, 4684, 281, 512, 2563, 3431, 281, 767, 26070, 2948, 5081, 18870], "temperature": 0.0, "avg_logprob": -0.10875832914101957, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00010718344856286421}, {"id": 62, "seek": 29500, "start": 308.0, "end": 313.0, "text": " technologies from researchers and students such as yourself to continue to try to", "tokens": [7943, 490, 10309, 293, 1731, 1270, 382, 1803, 281, 2354, 281, 853, 281], "temperature": 0.0, "avg_logprob": -0.10875832914101957, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00010718344856286421}, {"id": 63, "seek": 29500, "start": 313.0, "end": 316.0, "text": " make big advances in this area.", "tokens": [652, 955, 25297, 294, 341, 1859, 13], "temperature": 0.0, "avg_logprob": -0.10875832914101957, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00010718344856286421}, {"id": 64, "seek": 29500, "start": 316.0, "end": 322.0, "text": " We're also seeing lots of NLG technologies used in areas such as", "tokens": [492, 434, 611, 2577, 3195, 295, 426, 43, 38, 7943, 1143, 294, 3179, 1270, 382], "temperature": 0.0, "avg_logprob": -0.10875832914101957, "compression_ratio": 1.6454183266932272, "no_speech_prob": 0.00010718344856286421}, {"id": 65, "seek": 32200, "start": 322.0, "end": 328.0, "text": " summarization, where systems often have to aggregate information from", "tokens": [14611, 2144, 11, 689, 3652, 2049, 362, 281, 26118, 1589, 490], "temperature": 0.0, "avg_logprob": -0.13187795397879062, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.0001464956731069833}, {"id": 66, "seek": 32200, "start": 328.0, "end": 334.0, "text": " potentially multiple sources and rephrase the most salient content in a", "tokens": [7263, 3866, 7139, 293, 319, 44598, 651, 264, 881, 1845, 1196, 2701, 294, 257], "temperature": 0.0, "avg_logprob": -0.13187795397879062, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.0001464956731069833}, {"id": 67, "seek": 32200, "start": 334.0, "end": 338.0, "text": " shortened but still very engaging way.", "tokens": [45183, 457, 920, 588, 11268, 636, 13], "temperature": 0.0, "avg_logprob": -0.13187795397879062, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.0001464956731069833}, {"id": 68, "seek": 32200, "start": 338.0, "end": 343.0, "text": " While our go-to example for summarization is generally related to, let's say,", "tokens": [3987, 527, 352, 12, 1353, 1365, 337, 14611, 2144, 307, 5101, 4077, 281, 11, 718, 311, 584, 11], "temperature": 0.0, "avg_logprob": -0.13187795397879062, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.0001464956731069833}, {"id": 69, "seek": 32200, "start": 343.0, "end": 347.0, "text": " generating news highlights, summarization systems have actually achieved", "tokens": [17746, 2583, 14254, 11, 14611, 2144, 3652, 362, 767, 11042], "temperature": 0.0, "avg_logprob": -0.13187795397879062, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.0001464956731069833}, {"id": 70, "seek": 32200, "start": 347.0, "end": 351.0, "text": " broad applicability in many areas where we ingest content such as", "tokens": [4152, 2580, 2310, 294, 867, 3179, 689, 321, 3957, 377, 2701, 1270, 382], "temperature": 0.0, "avg_logprob": -0.13187795397879062, "compression_ratio": 1.6610878661087867, "no_speech_prob": 0.0001464956731069833}, {"id": 71, "seek": 35100, "start": 351.0, "end": 356.0, "text": " summarizing emails or summarizing meeting transcripts.", "tokens": [14611, 3319, 12524, 420, 14611, 3319, 3440, 24444, 82, 13], "temperature": 0.0, "avg_logprob": -0.10537049600056239, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.842406193958595e-05}, {"id": 72, "seek": 35100, "start": 356.0, "end": 359.0, "text": " And there's actually many more areas not listed here. I didn't actually put it", "tokens": [400, 456, 311, 767, 867, 544, 3179, 406, 10052, 510, 13, 286, 994, 380, 767, 829, 309], "temperature": 0.0, "avg_logprob": -0.10537049600056239, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.842406193958595e-05}, {"id": 73, "seek": 35100, "start": 359.0, "end": 363.0, "text": " on the slide, but a few months back, a tool called Semantic Scholar actually", "tokens": [322, 264, 4137, 11, 457, 257, 1326, 2493, 646, 11, 257, 2290, 1219, 14421, 7128, 2065, 15276, 767], "temperature": 0.0, "avg_logprob": -0.10537049600056239, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.842406193958595e-05}, {"id": 74, "seek": 35100, "start": 363.0, "end": 367.0, "text": " developed a neural system for generating summaries of scientific papers, which is", "tokens": [4743, 257, 18161, 1185, 337, 17746, 8367, 4889, 295, 8134, 10577, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.10537049600056239, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.842406193958595e-05}, {"id": 75, "seek": 35100, "start": 367.0, "end": 371.0, "text": " something that I personally end up using quite a bit as an example of how", "tokens": [746, 300, 286, 5665, 917, 493, 1228, 1596, 257, 857, 382, 364, 1365, 295, 577], "temperature": 0.0, "avg_logprob": -0.10537049600056239, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.842406193958595e-05}, {"id": 76, "seek": 35100, "start": 371.0, "end": 375.0, "text": " humans can interact with these technologies.", "tokens": [6255, 393, 4648, 365, 613, 7943, 13], "temperature": 0.0, "avg_logprob": -0.10537049600056239, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.842406193958595e-05}, {"id": 77, "seek": 35100, "start": 375.0, "end": 379.0, "text": " But these modalities aren't actually limited to text in or text out.", "tokens": [583, 613, 1072, 16110, 3212, 380, 767, 5567, 281, 2487, 294, 420, 2487, 484, 13], "temperature": 0.0, "avg_logprob": -0.10537049600056239, "compression_ratio": 1.6551724137931034, "no_speech_prob": 7.842406193958595e-05}, {"id": 78, "seek": 37900, "start": 379.0, "end": 383.0, "text": " So actually the classical NLG area that I mentioned earlier is how the", "tokens": [407, 767, 264, 13735, 426, 43, 38, 1859, 300, 286, 2835, 3071, 307, 577, 264], "temperature": 0.0, "avg_logprob": -0.1462194827886728, "compression_ratio": 1.6931818181818181, "no_speech_prob": 7.367477519437671e-05}, {"id": 79, "seek": 37900, "start": 383.0, "end": 387.0, "text": " task used to be framed was actually around what we now call data to text", "tokens": [5633, 1143, 281, 312, 30420, 390, 767, 926, 437, 321, 586, 818, 1412, 281, 2487], "temperature": 0.0, "avg_logprob": -0.1462194827886728, "compression_ratio": 1.6931818181818181, "no_speech_prob": 7.367477519437671e-05}, {"id": 80, "seek": 37900, "start": 387.0, "end": 391.0, "text": " generation. So can you learn to compile or let's say, summarize the most", "tokens": [5125, 13, 407, 393, 291, 1466, 281, 31413, 420, 718, 311, 584, 11, 20858, 264, 881], "temperature": 0.0, "avg_logprob": -0.1462194827886728, "compression_ratio": 1.6931818181818181, "no_speech_prob": 7.367477519437671e-05}, {"id": 81, "seek": 37900, "start": 391.0, "end": 397.0, "text": " interesting facts from a table or a knowledge graph or some type of data stream.", "tokens": [1880, 9130, 490, 257, 3199, 420, 257, 3601, 4295, 420, 512, 2010, 295, 1412, 4309, 13], "temperature": 0.0, "avg_logprob": -0.1462194827886728, "compression_ratio": 1.6931818181818181, "no_speech_prob": 7.367477519437671e-05}, {"id": 82, "seek": 37900, "start": 397.0, "end": 400.0, "text": " That way humans can get the most most interesting and salient content that's", "tokens": [663, 636, 6255, 393, 483, 264, 881, 881, 1880, 293, 1845, 1196, 2701, 300, 311], "temperature": 0.0, "avg_logprob": -0.1462194827886728, "compression_ratio": 1.6931818181818181, "no_speech_prob": 7.367477519437671e-05}, {"id": 83, "seek": 37900, "start": 400.0, "end": 406.0, "text": " being presented in these data structures rapidly and in easier to ingest", "tokens": [885, 8212, 294, 613, 1412, 9227, 12910, 293, 294, 3571, 281, 3957, 377], "temperature": 0.0, "avg_logprob": -0.1462194827886728, "compression_ratio": 1.6931818181818181, "no_speech_prob": 7.367477519437671e-05}, {"id": 84, "seek": 40600, "start": 406.0, "end": 411.0, "text": " format than having to look through the structures themselves.", "tokens": [7877, 813, 1419, 281, 574, 807, 264, 9227, 2969, 13], "temperature": 0.0, "avg_logprob": -0.11395906894765001, "compression_ratio": 1.58984375, "no_speech_prob": 1.406300361850299e-05}, {"id": 85, "seek": 40600, "start": 411.0, "end": 416.0, "text": " We've also seen a lot of recent work in visual description that tries to", "tokens": [492, 600, 611, 1612, 257, 688, 295, 5162, 589, 294, 5056, 3855, 300, 9898, 281], "temperature": 0.0, "avg_logprob": -0.11395906894765001, "compression_ratio": 1.58984375, "no_speech_prob": 1.406300361850299e-05}, {"id": 86, "seek": 40600, "start": 416.0, "end": 419.0, "text": " use language to describe the content and images or videos.", "tokens": [764, 2856, 281, 6786, 264, 2701, 293, 5267, 420, 2145, 13], "temperature": 0.0, "avg_logprob": -0.11395906894765001, "compression_ratio": 1.58984375, "no_speech_prob": 1.406300361850299e-05}, {"id": 87, "seek": 40600, "start": 419.0, "end": 425.0, "text": " So around 2014 or so we start to see the first neural NLG systems in the", "tokens": [407, 926, 8227, 420, 370, 321, 722, 281, 536, 264, 700, 18161, 426, 43, 38, 3652, 294, 264], "temperature": 0.0, "avg_logprob": -0.11395906894765001, "compression_ratio": 1.58984375, "no_speech_prob": 1.406300361850299e-05}, {"id": 88, "seek": 40600, "start": 425.0, "end": 429.0, "text": " space. And they've really continued to mature in the last six years.", "tokens": [1901, 13, 400, 436, 600, 534, 7014, 281, 14442, 294, 264, 1036, 2309, 924, 13], "temperature": 0.0, "avg_logprob": -0.11395906894765001, "compression_ratio": 1.58984375, "no_speech_prob": 1.406300361850299e-05}, {"id": 89, "seek": 40600, "start": 429.0, "end": 433.0, "text": " And now we actually tackle much more challenging description tasks such", "tokens": [400, 586, 321, 767, 14896, 709, 544, 7595, 3855, 9608, 1270], "temperature": 0.0, "avg_logprob": -0.11395906894765001, "compression_ratio": 1.58984375, "no_speech_prob": 1.406300361850299e-05}, {"id": 90, "seek": 43300, "start": 433.0, "end": 438.0, "text": " as generating full descriptive paragraphs of scenes or generating", "tokens": [382, 17746, 1577, 42585, 48910, 295, 8026, 420, 17746], "temperature": 0.0, "avg_logprob": -0.13786648540962032, "compression_ratio": 1.685589519650655, "no_speech_prob": 3.5353528801351786e-05}, {"id": 91, "seek": 43300, "start": 438.0, "end": 442.0, "text": " streams of visual generating descriptions for streams of visual", "tokens": [15842, 295, 5056, 17746, 24406, 337, 15842, 295, 5056], "temperature": 0.0, "avg_logprob": -0.13786648540962032, "compression_ratio": 1.685589519650655, "no_speech_prob": 3.5353528801351786e-05}, {"id": 92, "seek": 43300, "start": 442.0, "end": 447.0, "text": " contents such as in video captioning and these tools have really broad", "tokens": [15768, 1270, 382, 294, 960, 31974, 278, 293, 613, 3873, 362, 534, 4152], "temperature": 0.0, "avg_logprob": -0.13786648540962032, "compression_ratio": 1.685589519650655, "no_speech_prob": 3.5353528801351786e-05}, {"id": 93, "seek": 43300, "start": 447.0, "end": 451.0, "text": " applicability in different areas of AI.", "tokens": [2580, 2310, 294, 819, 3179, 295, 7318, 13], "temperature": 0.0, "avg_logprob": -0.13786648540962032, "compression_ratio": 1.685589519650655, "no_speech_prob": 3.5353528801351786e-05}, {"id": 94, "seek": 43300, "start": 451.0, "end": 455.0, "text": " And finally the last sort of application I kind of want to mention is that we've", "tokens": [400, 2721, 264, 1036, 1333, 295, 3861, 286, 733, 295, 528, 281, 2152, 307, 300, 321, 600], "temperature": 0.0, "avg_logprob": -0.13786648540962032, "compression_ratio": 1.685589519650655, "no_speech_prob": 3.5353528801351786e-05}, {"id": 95, "seek": 43300, "start": 455.0, "end": 459.0, "text": " also started seeing NLG systems being developed in more creative", "tokens": [611, 1409, 2577, 426, 43, 38, 3652, 885, 4743, 294, 544, 5880], "temperature": 0.0, "avg_logprob": -0.13786648540962032, "compression_ratio": 1.685589519650655, "no_speech_prob": 3.5353528801351786e-05}, {"id": 96, "seek": 45900, "start": 459.0, "end": 464.0, "text": " applications such as story generation where AI systems can now help humans", "tokens": [5821, 1270, 382, 1657, 5125, 689, 7318, 3652, 393, 586, 854, 6255], "temperature": 0.0, "avg_logprob": -0.15004881392133998, "compression_ratio": 1.6956521739130435, "no_speech_prob": 7.601742254337296e-05}, {"id": 97, "seek": 45900, "start": 464.0, "end": 469.0, "text": " write short stories, blog posts or even full books in some case as creative", "tokens": [2464, 2099, 3676, 11, 6968, 12300, 420, 754, 1577, 3642, 294, 512, 1389, 382, 5880], "temperature": 0.0, "avg_logprob": -0.15004881392133998, "compression_ratio": 1.6956521739130435, "no_speech_prob": 7.601742254337296e-05}, {"id": 98, "seek": 45900, "start": 469.0, "end": 474.0, "text": " writing assistants. In another area such as post tree generation we can", "tokens": [3579, 34949, 13, 682, 1071, 1859, 1270, 382, 2183, 4230, 5125, 321, 393], "temperature": 0.0, "avg_logprob": -0.15004881392133998, "compression_ratio": 1.6956521739130435, "no_speech_prob": 7.601742254337296e-05}, {"id": 99, "seek": 45900, "start": 474.0, "end": 478.0, "text": " actually have kind of full automated settings where you can have AI agents", "tokens": [767, 362, 733, 295, 1577, 18473, 6257, 689, 291, 393, 362, 7318, 12554], "temperature": 0.0, "avg_logprob": -0.15004881392133998, "compression_ratio": 1.6956521739130435, "no_speech_prob": 7.601742254337296e-05}, {"id": 100, "seek": 45900, "start": 478.0, "end": 482.0, "text": " that can generate something like a sonnet and in fact condition that's on a", "tokens": [300, 393, 8460, 746, 411, 257, 1872, 7129, 293, 294, 1186, 4188, 300, 311, 322, 257], "temperature": 0.0, "avg_logprob": -0.15004881392133998, "compression_ratio": 1.6956521739130435, "no_speech_prob": 7.601742254337296e-05}, {"id": 101, "seek": 45900, "start": 482.0, "end": 486.0, "text": " lot of demands that are given through a user interface.", "tokens": [688, 295, 15107, 300, 366, 2212, 807, 257, 4195, 9226, 13], "temperature": 0.0, "avg_logprob": -0.15004881392133998, "compression_ratio": 1.6956521739130435, "no_speech_prob": 7.601742254337296e-05}, {"id": 102, "seek": 48600, "start": 486.0, "end": 491.0, "text": " So I hope that by this point I've really given you a look into the breadth of", "tokens": [407, 286, 1454, 300, 538, 341, 935, 286, 600, 534, 2212, 291, 257, 574, 666, 264, 35862, 295], "temperature": 0.0, "avg_logprob": -0.1340945931368096, "compression_ratio": 1.592274678111588, "no_speech_prob": 7.842375634936616e-05}, {"id": 103, "seek": 48600, "start": 491.0, "end": 498.0, "text": " NLG applications and how it sort of encompasses any task that you might", "tokens": [426, 43, 38, 5821, 293, 577, 309, 1333, 295, 49866, 604, 5633, 300, 291, 1062], "temperature": 0.0, "avg_logprob": -0.1340945931368096, "compression_ratio": 1.592274678111588, "no_speech_prob": 7.842375634936616e-05}, {"id": 104, "seek": 48600, "start": 498.0, "end": 503.0, "text": " think of that involves production of text. And each of these tasks really", "tokens": [519, 295, 300, 11626, 4265, 295, 2487, 13, 400, 1184, 295, 613, 9608, 534], "temperature": 0.0, "avg_logprob": -0.1340945931368096, "compression_ratio": 1.592274678111588, "no_speech_prob": 7.842375634936616e-05}, {"id": 105, "seek": 48600, "start": 503.0, "end": 507.0, "text": " requires different algorithms and different models and a different way of", "tokens": [7029, 819, 14642, 293, 819, 5245, 293, 257, 819, 636, 295], "temperature": 0.0, "avg_logprob": -0.1340945931368096, "compression_ratio": 1.592274678111588, "no_speech_prob": 7.842375634936616e-05}, {"id": 106, "seek": 48600, "start": 507.0, "end": 511.0, "text": " designing the system to get right. But what they have in common is that a", "tokens": [14685, 264, 1185, 281, 483, 558, 13, 583, 437, 436, 362, 294, 2689, 307, 300, 257], "temperature": 0.0, "avg_logprob": -0.1340945931368096, "compression_ratio": 1.592274678111588, "no_speech_prob": 7.842375634936616e-05}, {"id": 107, "seek": 51100, "start": 511.0, "end": 519.0, "text": " lot of our power is by next generation advances in deep learning for NLG.", "tokens": [688, 295, 527, 1347, 307, 538, 958, 5125, 25297, 294, 2452, 2539, 337, 426, 43, 38, 13], "temperature": 0.0, "avg_logprob": -0.2109836485327744, "compression_ratio": 1.6504854368932038, "no_speech_prob": 2.8852793548139744e-05}, {"id": 108, "seek": 51100, "start": 519.0, "end": 524.0, "text": " And the goal of today is to really give you the introduction to these", "tokens": [400, 264, 3387, 295, 965, 307, 281, 534, 976, 291, 264, 9339, 281, 613], "temperature": 0.0, "avg_logprob": -0.2109836485327744, "compression_ratio": 1.6504854368932038, "no_speech_prob": 2.8852793548139744e-05}, {"id": 109, "seek": 51100, "start": 524.0, "end": 528.0, "text": " topics that really allows you to contribute to the next era of these", "tokens": [8378, 300, 534, 4045, 291, 281, 10586, 281, 264, 958, 4249, 295, 613], "temperature": 0.0, "avg_logprob": -0.2109836485327744, "compression_ratio": 1.6504854368932038, "no_speech_prob": 2.8852793548139744e-05}, {"id": 110, "seek": 51100, "start": 528.0, "end": 532.0, "text": " technologies and designing deep learning systems for NLG.", "tokens": [7943, 293, 14685, 2452, 2539, 3652, 337, 426, 43, 38, 13], "temperature": 0.0, "avg_logprob": -0.2109836485327744, "compression_ratio": 1.6504854368932038, "no_speech_prob": 2.8852793548139744e-05}, {"id": 111, "seek": 51100, "start": 532.0, "end": 537.0, "text": " And so I think to start what might be interesting to do is to quickly", "tokens": [400, 370, 286, 519, 281, 722, 437, 1062, 312, 1880, 281, 360, 307, 281, 2661], "temperature": 0.0, "avg_logprob": -0.2109836485327744, "compression_ratio": 1.6504854368932038, "no_speech_prob": 2.8852793548139744e-05}, {"id": 112, "seek": 53700, "start": 537.0, "end": 541.0, "text": " reap cap topics that you may have seen in previous lectures, but which are", "tokens": [39178, 1410, 8378, 300, 291, 815, 362, 1612, 294, 3894, 16564, 11, 457, 597, 366], "temperature": 0.0, "avg_logprob": -0.11676339122736565, "compression_ratio": 1.7338403041825095, "no_speech_prob": 8.092032658169046e-05}, {"id": 113, "seek": 53700, "start": 541.0, "end": 547.0, "text": " going to be very relevant today when we're trying to design an NLG system.", "tokens": [516, 281, 312, 588, 7340, 965, 562, 321, 434, 1382, 281, 1715, 364, 426, 43, 38, 1185, 13], "temperature": 0.0, "avg_logprob": -0.11676339122736565, "compression_ratio": 1.7338403041825095, "no_speech_prob": 8.092032658169046e-05}, {"id": 114, "seek": 53700, "start": 547.0, "end": 551.0, "text": " And what we're effectively trying to do in the setting is take a sequence of", "tokens": [400, 437, 321, 434, 8659, 1382, 281, 360, 294, 264, 3287, 307, 747, 257, 8310, 295], "temperature": 0.0, "avg_logprob": -0.11676339122736565, "compression_ratio": 1.7338403041825095, "no_speech_prob": 8.092032658169046e-05}, {"id": 115, "seek": 53700, "start": 551.0, "end": 556.0, "text": " tokens as inputs and produce new text that is conditioned on this input.", "tokens": [22667, 382, 15743, 293, 5258, 777, 2487, 300, 307, 35833, 322, 341, 4846, 13], "temperature": 0.0, "avg_logprob": -0.11676339122736565, "compression_ratio": 1.7338403041825095, "no_speech_prob": 8.092032658169046e-05}, {"id": 116, "seek": 53700, "start": 556.0, "end": 560.0, "text": " And then what we typically call the auto regressive setting, which is the most", "tokens": [400, 550, 437, 321, 5850, 818, 264, 8399, 1121, 22733, 3287, 11, 597, 307, 264, 881], "temperature": 0.0, "avg_logprob": -0.11676339122736565, "compression_ratio": 1.7338403041825095, "no_speech_prob": 8.092032658169046e-05}, {"id": 117, "seek": 53700, "start": 560.0, "end": 565.0, "text": " common sort of text generation setting, we take these produced tokens of text", "tokens": [2689, 1333, 295, 2487, 5125, 3287, 11, 321, 747, 613, 7126, 22667, 295, 2487], "temperature": 0.0, "avg_logprob": -0.11676339122736565, "compression_ratio": 1.7338403041825095, "no_speech_prob": 8.092032658169046e-05}, {"id": 118, "seek": 56500, "start": 565.0, "end": 569.0, "text": " and we feed them back into our model to generate the next token in the sequence", "tokens": [293, 321, 3154, 552, 646, 666, 527, 2316, 281, 8460, 264, 958, 14862, 294, 264, 8310], "temperature": 0.0, "avg_logprob": -0.08262053921691373, "compression_ratio": 1.8134328358208955, "no_speech_prob": 7.721326255705208e-05}, {"id": 119, "seek": 56500, "start": 569.0, "end": 572.0, "text": " that we want to generate.", "tokens": [300, 321, 528, 281, 8460, 13], "temperature": 0.0, "avg_logprob": -0.08262053921691373, "compression_ratio": 1.8134328358208955, "no_speech_prob": 7.721326255705208e-05}, {"id": 120, "seek": 56500, "start": 572.0, "end": 577.0, "text": " But so to really understand what's going on in an auto regressive NLG system,", "tokens": [583, 370, 281, 534, 1223, 437, 311, 516, 322, 294, 364, 8399, 1121, 22733, 426, 43, 38, 1185, 11], "temperature": 0.0, "avg_logprob": -0.08262053921691373, "compression_ratio": 1.8134328358208955, "no_speech_prob": 7.721326255705208e-05}, {"id": 121, "seek": 56500, "start": 577.0, "end": 581.0, "text": " what we really need to start to do is look at what happens for the generation of an", "tokens": [437, 321, 534, 643, 281, 722, 281, 360, 307, 574, 412, 437, 2314, 337, 264, 5125, 295, 364], "temperature": 0.0, "avg_logprob": -0.08262053921691373, "compression_ratio": 1.8134328358208955, "no_speech_prob": 7.721326255705208e-05}, {"id": 122, "seek": 56500, "start": 581.0, "end": 585.0, "text": " individual token since further stage is really depend on taking that generated", "tokens": [2609, 14862, 1670, 3052, 3233, 307, 534, 5672, 322, 1940, 300, 10833], "temperature": 0.0, "avg_logprob": -0.08262053921691373, "compression_ratio": 1.8134328358208955, "no_speech_prob": 7.721326255705208e-05}, {"id": 123, "seek": 56500, "start": 585.0, "end": 588.0, "text": " token and passing it back in as input and doing the same thing.", "tokens": [14862, 293, 8437, 309, 646, 294, 382, 4846, 293, 884, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.08262053921691373, "compression_ratio": 1.8134328358208955, "no_speech_prob": 7.721326255705208e-05}, {"id": 124, "seek": 56500, "start": 588.0, "end": 592.0, "text": " So what happens at a low level is that your model takes in this sequence of", "tokens": [407, 437, 2314, 412, 257, 2295, 1496, 307, 300, 428, 2316, 2516, 294, 341, 8310, 295], "temperature": 0.0, "avg_logprob": -0.08262053921691373, "compression_ratio": 1.8134328358208955, "no_speech_prob": 7.721326255705208e-05}, {"id": 125, "seek": 59200, "start": 592.0, "end": 598.0, "text": " inputs, so these Ys and it computes a vector of scores using the model itself.", "tokens": [15743, 11, 370, 613, 398, 82, 293, 309, 715, 1819, 257, 8062, 295, 13444, 1228, 264, 2316, 2564, 13], "temperature": 0.0, "avg_logprob": -0.1262429698129718, "compression_ratio": 1.7772727272727273, "no_speech_prob": 3.480638770270161e-05}, {"id": 126, "seek": 59200, "start": 598.0, "end": 603.0, "text": " And each index in that vector corresponds to the score for a token in your", "tokens": [400, 1184, 8186, 294, 300, 8062, 23249, 281, 264, 6175, 337, 257, 14862, 294, 428], "temperature": 0.0, "avg_logprob": -0.1262429698129718, "compression_ratio": 1.7772727272727273, "no_speech_prob": 3.480638770270161e-05}, {"id": 127, "seek": 59200, "start": 603.0, "end": 607.0, "text": " vocabulary. So the only tokens that your model is actually allowed to generate.", "tokens": [19864, 13, 407, 264, 787, 22667, 300, 428, 2316, 307, 767, 4350, 281, 8460, 13], "temperature": 0.0, "avg_logprob": -0.1262429698129718, "compression_ratio": 1.7772727272727273, "no_speech_prob": 3.480638770270161e-05}, {"id": 128, "seek": 59200, "start": 607.0, "end": 611.0, "text": " And then what you do is that you compute a probability distribution over these", "tokens": [400, 550, 437, 291, 360, 307, 300, 291, 14722, 257, 8482, 7316, 670, 613], "temperature": 0.0, "avg_logprob": -0.1262429698129718, "compression_ratio": 1.7772727272727273, "no_speech_prob": 3.480638770270161e-05}, {"id": 129, "seek": 59200, "start": 611.0, "end": 616.0, "text": " scores using what we call a softmax function to compute a probability estimate", "tokens": [13444, 1228, 437, 321, 818, 257, 2787, 41167, 2445, 281, 14722, 257, 8482, 12539], "temperature": 0.0, "avg_logprob": -0.1262429698129718, "compression_ratio": 1.7772727272727273, "no_speech_prob": 3.480638770270161e-05}, {"id": 130, "seek": 61600, "start": 616.0, "end": 624.0, "text": " for each token in your vocabulary given the context that precedes it.", "tokens": [337, 1184, 14862, 294, 428, 19864, 2212, 264, 4319, 300, 16969, 279, 309, 13], "temperature": 0.0, "avg_logprob": -0.11253902938339737, "compression_ratio": 1.6, "no_speech_prob": 3.023980389116332e-05}, {"id": 131, "seek": 61600, "start": 624.0, "end": 629.0, "text": " And as a shorthand, I'll just mention that sometimes I'll remove the W from this", "tokens": [400, 382, 257, 402, 2652, 474, 11, 286, 603, 445, 2152, 300, 2171, 286, 603, 4159, 264, 343, 490, 341], "temperature": 0.0, "avg_logprob": -0.11253902938339737, "compression_ratio": 1.6, "no_speech_prob": 3.023980389116332e-05}, {"id": 132, "seek": 61600, "start": 629.0, "end": 633.0, "text": " probability equation, but just know that when I write out the probability of a token", "tokens": [8482, 5367, 11, 457, 445, 458, 300, 562, 286, 2464, 484, 264, 8482, 295, 257, 14862], "temperature": 0.0, "avg_logprob": -0.11253902938339737, "compression_ratio": 1.6, "no_speech_prob": 3.023980389116332e-05}, {"id": 133, "seek": 61600, "start": 633.0, "end": 638.0, "text": " Y at time t, what I mean is the probability that Y of t is a particular word.", "tokens": [398, 412, 565, 256, 11, 437, 286, 914, 307, 264, 8482, 300, 398, 295, 256, 307, 257, 1729, 1349, 13], "temperature": 0.0, "avg_logprob": -0.11253902938339737, "compression_ratio": 1.6, "no_speech_prob": 3.023980389116332e-05}, {"id": 134, "seek": 61600, "start": 638.0, "end": 643.0, "text": " So it's kind of a variable assignment.", "tokens": [407, 309, 311, 733, 295, 257, 7006, 15187, 13], "temperature": 0.0, "avg_logprob": -0.11253902938339737, "compression_ratio": 1.6, "no_speech_prob": 3.023980389116332e-05}, {"id": 135, "seek": 64300, "start": 643.0, "end": 648.0, "text": " But so what actually is the output of what we typically call a text generation model", "tokens": [583, 370, 437, 767, 307, 264, 5598, 295, 437, 321, 5850, 818, 257, 2487, 5125, 2316], "temperature": 0.0, "avg_logprob": -0.14982283115386963, "compression_ratio": 1.9227799227799227, "no_speech_prob": 5.9205656725680456e-05}, {"id": 136, "seek": 64300, "start": 648.0, "end": 652.0, "text": " at this point is actually this vector of scores. And then that vector gets past", "tokens": [412, 341, 935, 307, 767, 341, 8062, 295, 13444, 13, 400, 550, 300, 8062, 2170, 1791], "temperature": 0.0, "avg_logprob": -0.14982283115386963, "compression_ratio": 1.9227799227799227, "no_speech_prob": 5.9205656725680456e-05}, {"id": 137, "seek": 64300, "start": 652.0, "end": 657.0, "text": " the softmax function to give us a probability distribution over the set of tokens in the", "tokens": [264, 2787, 41167, 2445, 281, 976, 505, 257, 8482, 7316, 670, 264, 992, 295, 22667, 294, 264], "temperature": 0.0, "avg_logprob": -0.14982283115386963, "compression_ratio": 1.9227799227799227, "no_speech_prob": 5.9205656725680456e-05}, {"id": 138, "seek": 64300, "start": 657.0, "end": 662.0, "text": " vocabulary. And then to actually generate a token, we can define what we call a", "tokens": [19864, 13, 400, 550, 281, 767, 8460, 257, 14862, 11, 321, 393, 6964, 437, 321, 818, 257], "temperature": 0.0, "avg_logprob": -0.14982283115386963, "compression_ratio": 1.9227799227799227, "no_speech_prob": 5.9205656725680456e-05}, {"id": 139, "seek": 64300, "start": 662.0, "end": 667.0, "text": " decoding algorithm. That is a function that takes in this distribution peak over", "tokens": [979, 8616, 9284, 13, 663, 307, 257, 2445, 300, 2516, 294, 341, 7316, 10651, 670], "temperature": 0.0, "avg_logprob": -0.14982283115386963, "compression_ratio": 1.9227799227799227, "no_speech_prob": 5.9205656725680456e-05}, {"id": 140, "seek": 64300, "start": 667.0, "end": 672.0, "text": " all the tokens of the vocabulary. And you know, it defines a function for selecting", "tokens": [439, 264, 22667, 295, 264, 19864, 13, 400, 291, 458, 11, 309, 23122, 257, 2445, 337, 18182], "temperature": 0.0, "avg_logprob": -0.14982283115386963, "compression_ratio": 1.9227799227799227, "no_speech_prob": 5.9205656725680456e-05}, {"id": 141, "seek": 67200, "start": 672.0, "end": 677.0, "text": " a function from this distribution as the next token that is produced by our", "tokens": [257, 2445, 490, 341, 7316, 382, 264, 958, 14862, 300, 307, 7126, 538, 527], "temperature": 0.0, "avg_logprob": -0.12526074525351835, "compression_ratio": 1.7898832684824904, "no_speech_prob": 7.966441626194865e-05}, {"id": 142, "seek": 67200, "start": 677.0, "end": 682.0, "text": " NLG system. And for that distribution to be calibrated in such a way that", "tokens": [426, 43, 38, 1185, 13, 400, 337, 300, 7316, 281, 312, 21583, 5468, 294, 1270, 257, 636, 300], "temperature": 0.0, "avg_logprob": -0.12526074525351835, "compression_ratio": 1.7898832684824904, "no_speech_prob": 7.966441626194865e-05}, {"id": 143, "seek": 67200, "start": 682.0, "end": 687.0, "text": " it means anything, we need to train the model to actually be able to do the task.", "tokens": [309, 1355, 1340, 11, 321, 643, 281, 3847, 264, 2316, 281, 767, 312, 1075, 281, 360, 264, 5633, 13], "temperature": 0.0, "avg_logprob": -0.12526074525351835, "compression_ratio": 1.7898832684824904, "no_speech_prob": 7.966441626194865e-05}, {"id": 144, "seek": 67200, "start": 687.0, "end": 692.0, "text": " So the most common way of training text generation models is to use maximum", "tokens": [407, 264, 881, 2689, 636, 295, 3097, 2487, 5125, 5245, 307, 281, 764, 6674], "temperature": 0.0, "avg_logprob": -0.12526074525351835, "compression_ratio": 1.7898832684824904, "no_speech_prob": 7.966441626194865e-05}, {"id": 145, "seek": 67200, "start": 692.0, "end": 697.0, "text": " likelihood training. And despite its name, we don't actually maximize", "tokens": [22119, 3097, 13, 400, 7228, 1080, 1315, 11, 321, 500, 380, 767, 19874], "temperature": 0.0, "avg_logprob": -0.12526074525351835, "compression_ratio": 1.7898832684824904, "no_speech_prob": 7.966441626194865e-05}, {"id": 146, "seek": 67200, "start": 697.0, "end": 701.0, "text": " likelihoods. We actually minimize negative log likelihoods. And what that actually", "tokens": [22119, 82, 13, 492, 767, 17522, 3671, 3565, 22119, 82, 13, 400, 437, 300, 767], "temperature": 0.0, "avg_logprob": -0.12526074525351835, "compression_ratio": 1.7898832684824904, "no_speech_prob": 7.966441626194865e-05}, {"id": 147, "seek": 70100, "start": 701.0, "end": 706.0, "text": " is is just a multi-class classification task where each word in our vocabulary", "tokens": [307, 307, 445, 257, 4825, 12, 11665, 21538, 5633, 689, 1184, 1349, 294, 527, 19864], "temperature": 0.0, "avg_logprob": -0.08663071326489719, "compression_ratio": 1.8137651821862348, "no_speech_prob": 7.030621782178059e-05}, {"id": 148, "seek": 70100, "start": 706.0, "end": 710.0, "text": " is a class that can be predicted by the model. And so at each step in the", "tokens": [307, 257, 1508, 300, 393, 312, 19147, 538, 264, 2316, 13, 400, 370, 412, 1184, 1823, 294, 264], "temperature": 0.0, "avg_logprob": -0.08663071326489719, "compression_ratio": 1.8137651821862348, "no_speech_prob": 7.030621782178059e-05}, {"id": 149, "seek": 70100, "start": 710.0, "end": 714.0, "text": " sequence, we're actually trying to predict the class that corresponds to the word", "tokens": [8310, 11, 321, 434, 767, 1382, 281, 6069, 264, 1508, 300, 23249, 281, 264, 1349], "temperature": 0.0, "avg_logprob": -0.08663071326489719, "compression_ratio": 1.8137651821862348, "no_speech_prob": 7.030621782178059e-05}, {"id": 150, "seek": 70100, "start": 714.0, "end": 718.0, "text": " that comes next in the sequence of text that we're trying to train on.", "tokens": [300, 1487, 958, 294, 264, 8310, 295, 2487, 300, 321, 434, 1382, 281, 3847, 322, 13], "temperature": 0.0, "avg_logprob": -0.08663071326489719, "compression_ratio": 1.8137651821862348, "no_speech_prob": 7.030621782178059e-05}, {"id": 151, "seek": 70100, "start": 718.0, "end": 723.0, "text": " And this word is often called the gold or ground truth token as just kind of", "tokens": [400, 341, 1349, 307, 2049, 1219, 264, 3821, 420, 2727, 3494, 14862, 382, 445, 733, 295], "temperature": 0.0, "avg_logprob": -0.08663071326489719, "compression_ratio": 1.8137651821862348, "no_speech_prob": 7.030621782178059e-05}, {"id": 152, "seek": 70100, "start": 723.0, "end": 728.0, "text": " interchangeable vocabulary that we use. And another term for this", "tokens": [30358, 712, 19864, 300, 321, 764, 13, 400, 1071, 1433, 337, 341], "temperature": 0.0, "avg_logprob": -0.08663071326489719, "compression_ratio": 1.8137651821862348, "no_speech_prob": 7.030621782178059e-05}, {"id": 153, "seek": 72800, "start": 728.0, "end": 732.0, "text": " training algorithm is teacher forcing. So you might see these expressions", "tokens": [3097, 9284, 307, 5027, 19030, 13, 407, 291, 1062, 536, 613, 15277], "temperature": 0.0, "avg_logprob": -0.09777124722798665, "compression_ratio": 1.657992565055762, "no_speech_prob": 2.627282628964167e-05}, {"id": 154, "seek": 72800, "start": 732.0, "end": 737.0, "text": " kind of used interchangeably if you read papers on this topic.", "tokens": [733, 295, 1143, 30358, 1188, 498, 291, 1401, 10577, 322, 341, 4829, 13], "temperature": 0.0, "avg_logprob": -0.09777124722798665, "compression_ratio": 1.657992565055762, "no_speech_prob": 2.627282628964167e-05}, {"id": 155, "seek": 72800, "start": 737.0, "end": 740.0, "text": " But so at each step, you're really computing a loss term that is the negative", "tokens": [583, 370, 412, 1184, 1823, 11, 291, 434, 534, 15866, 257, 4470, 1433, 300, 307, 264, 3671], "temperature": 0.0, "avg_logprob": -0.09777124722798665, "compression_ratio": 1.657992565055762, "no_speech_prob": 2.627282628964167e-05}, {"id": 156, "seek": 72800, "start": 740.0, "end": 745.0, "text": " log likelihood of predicting this gold token y1 at every step. So in these slides,", "tokens": [3565, 22119, 295, 32884, 341, 3821, 14862, 288, 16, 412, 633, 1823, 13, 407, 294, 613, 9788, 11], "temperature": 0.0, "avg_logprob": -0.09777124722798665, "compression_ratio": 1.657992565055762, "no_speech_prob": 2.627282628964167e-05}, {"id": 157, "seek": 72800, "start": 745.0, "end": 749.0, "text": " whenever you see an asterisk next to a y, that means that this is a gold", "tokens": [5699, 291, 536, 364, 257, 3120, 7797, 958, 281, 257, 288, 11, 300, 1355, 300, 341, 307, 257, 3821], "temperature": 0.0, "avg_logprob": -0.09777124722798665, "compression_ratio": 1.657992565055762, "no_speech_prob": 2.627282628964167e-05}, {"id": 158, "seek": 72800, "start": 749.0, "end": 753.0, "text": " token that comes from a training sequence. And you can do this for multiple", "tokens": [14862, 300, 1487, 490, 257, 3097, 8310, 13, 400, 291, 393, 360, 341, 337, 3866], "temperature": 0.0, "avg_logprob": -0.09777124722798665, "compression_ratio": 1.657992565055762, "no_speech_prob": 2.627282628964167e-05}, {"id": 159, "seek": 75300, "start": 753.0, "end": 759.0, "text": " steps, adding up the log likelihoods along the way. Eventually, you're going", "tokens": [4439, 11, 5127, 493, 264, 3565, 22119, 82, 2051, 264, 636, 13, 17586, 11, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.1564949997910508, "compression_ratio": 1.7026022304832713, "no_speech_prob": 1.4063343769521452e-05}, {"id": 160, "seek": 75300, "start": 759.0, "end": 762.0, "text": " to arrive at the end of your of your gold sequence. And you'll be able to", "tokens": [281, 8881, 412, 264, 917, 295, 428, 295, 428, 3821, 8310, 13, 400, 291, 603, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.1564949997910508, "compression_ratio": 1.7026022304832713, "no_speech_prob": 1.4063343769521452e-05}, {"id": 161, "seek": 75300, "start": 762.0, "end": 767.0, "text": " compute gradients with respect to this sum-dLoss term for every parameter in", "tokens": [14722, 2771, 2448, 365, 3104, 281, 341, 2408, 12, 67, 43, 772, 1433, 337, 633, 13075, 294], "temperature": 0.0, "avg_logprob": -0.1564949997910508, "compression_ratio": 1.7026022304832713, "no_speech_prob": 1.4063343769521452e-05}, {"id": 162, "seek": 75300, "start": 767.0, "end": 771.0, "text": " your model, which allows you to update it so that the next time around when you see", "tokens": [428, 2316, 11, 597, 4045, 291, 281, 5623, 309, 370, 300, 264, 958, 565, 926, 562, 291, 536], "temperature": 0.0, "avg_logprob": -0.1564949997910508, "compression_ratio": 1.7026022304832713, "no_speech_prob": 1.4063343769521452e-05}, {"id": 163, "seek": 75300, "start": 771.0, "end": 775.0, "text": " this sequence, your model is more confident in the probability that this sequence", "tokens": [341, 8310, 11, 428, 2316, 307, 544, 6679, 294, 264, 8482, 300, 341, 8310], "temperature": 0.0, "avg_logprob": -0.1564949997910508, "compression_ratio": 1.7026022304832713, "no_speech_prob": 1.4063343769521452e-05}, {"id": 164, "seek": 75300, "start": 775.0, "end": 779.0, "text": " is a correct sequence given the same context it has seen before.", "tokens": [307, 257, 3006, 8310, 2212, 264, 912, 4319, 309, 575, 1612, 949, 13], "temperature": 0.0, "avg_logprob": -0.1564949997910508, "compression_ratio": 1.7026022304832713, "no_speech_prob": 1.4063343769521452e-05}, {"id": 165, "seek": 77900, "start": 779.0, "end": 783.0, "text": " But most of this should just be a recap from previous lectures on language", "tokens": [583, 881, 295, 341, 820, 445, 312, 257, 20928, 490, 3894, 16564, 322, 2856], "temperature": 0.0, "avg_logprob": -0.07673414548238118, "compression_ratio": 1.673003802281369, "no_speech_prob": 7.367779471678659e-05}, {"id": 166, "seek": 77900, "start": 783.0, "end": 788.0, "text": " modeling and machine translation. But now that we've got that out of the", "tokens": [15983, 293, 3479, 12853, 13, 583, 586, 300, 321, 600, 658, 300, 484, 295, 264], "temperature": 0.0, "avg_logprob": -0.07673414548238118, "compression_ratio": 1.673003802281369, "no_speech_prob": 7.367779471678659e-05}, {"id": 167, "seek": 77900, "start": 788.0, "end": 793.0, "text": " way, let's get to the fun part and talk about some new topics.", "tokens": [636, 11, 718, 311, 483, 281, 264, 1019, 644, 293, 751, 466, 512, 777, 8378, 13], "temperature": 0.0, "avg_logprob": -0.07673414548238118, "compression_ratio": 1.673003802281369, "no_speech_prob": 7.367779471678659e-05}, {"id": 168, "seek": 77900, "start": 793.0, "end": 796.0, "text": " The first one of which is decoding, which is actually one of my favorite", "tokens": [440, 700, 472, 295, 597, 307, 979, 8616, 11, 597, 307, 767, 472, 295, 452, 2954], "temperature": 0.0, "avg_logprob": -0.07673414548238118, "compression_ratio": 1.673003802281369, "no_speech_prob": 7.367779471678659e-05}, {"id": 169, "seek": 77900, "start": 796.0, "end": 802.0, "text": " topics in natural language generation research. So if you recall, your decoding", "tokens": [8378, 294, 3303, 2856, 5125, 2132, 13, 407, 498, 291, 9901, 11, 428, 979, 8616], "temperature": 0.0, "avg_logprob": -0.07673414548238118, "compression_ratio": 1.673003802281369, "no_speech_prob": 7.367779471678659e-05}, {"id": 170, "seek": 77900, "start": 802.0, "end": 806.0, "text": " algorithm is really the function that takes in this this induced probability", "tokens": [9284, 307, 534, 264, 2445, 300, 2516, 294, 341, 341, 33991, 8482], "temperature": 0.0, "avg_logprob": -0.07673414548238118, "compression_ratio": 1.673003802281369, "no_speech_prob": 7.367779471678659e-05}, {"id": 171, "seek": 80600, "start": 806.0, "end": 811.0, "text": " distribution from your model over the next possible tokens that can be generated", "tokens": [7316, 490, 428, 2316, 670, 264, 958, 1944, 22667, 300, 393, 312, 10833], "temperature": 0.0, "avg_logprob": -0.10233289888589689, "compression_ratio": 1.834061135371179, "no_speech_prob": 9.609532571630552e-05}, {"id": 172, "seek": 80600, "start": 811.0, "end": 816.0, "text": " and selects which one of those tokens should be outputted.", "tokens": [293, 3048, 82, 597, 472, 295, 729, 22667, 820, 312, 5598, 14727, 13], "temperature": 0.0, "avg_logprob": -0.10233289888589689, "compression_ratio": 1.834061135371179, "no_speech_prob": 9.609532571630552e-05}, {"id": 173, "seek": 80600, "start": 816.0, "end": 820.0, "text": " So once your model is trained, this distribution should be meaningful.", "tokens": [407, 1564, 428, 2316, 307, 8895, 11, 341, 7316, 820, 312, 10995, 13], "temperature": 0.0, "avg_logprob": -0.10233289888589689, "compression_ratio": 1.834061135371179, "no_speech_prob": 9.609532571630552e-05}, {"id": 174, "seek": 80600, "start": 820.0, "end": 824.0, "text": " And you want to be able to generate a sensible next token.", "tokens": [400, 291, 528, 281, 312, 1075, 281, 8460, 257, 25380, 958, 14862, 13], "temperature": 0.0, "avg_logprob": -0.10233289888589689, "compression_ratio": 1.834061135371179, "no_speech_prob": 9.609532571630552e-05}, {"id": 175, "seek": 80600, "start": 824.0, "end": 828.0, "text": " And then you can use these generated next tokens. So the blue y hats here,", "tokens": [400, 550, 291, 393, 764, 613, 10833, 958, 22667, 13, 407, 264, 3344, 288, 20549, 510, 11], "temperature": 0.0, "avg_logprob": -0.10233289888589689, "compression_ratio": 1.834061135371179, "no_speech_prob": 9.609532571630552e-05}, {"id": 176, "seek": 80600, "start": 828.0, "end": 832.0, "text": " as input in the next step of the model, which allows you to recompute a new", "tokens": [382, 4846, 294, 264, 958, 1823, 295, 264, 2316, 11, 597, 4045, 291, 281, 48000, 1169, 257, 777], "temperature": 0.0, "avg_logprob": -0.10233289888589689, "compression_ratio": 1.834061135371179, "no_speech_prob": 9.609532571630552e-05}, {"id": 177, "seek": 83200, "start": 832.0, "end": 837.0, "text": " distribution decode a new token, repeat the process and eventually end up with a", "tokens": [7316, 979, 1429, 257, 777, 14862, 11, 7149, 264, 1399, 293, 4728, 917, 493, 365, 257], "temperature": 0.0, "avg_logprob": -0.13859752693561592, "compression_ratio": 1.6076923076923078, "no_speech_prob": 4.26454862463288e-05}, {"id": 178, "seek": 83200, "start": 837.0, "end": 842.0, "text": " full sequence that your model has now generated given a fixed starting sequence of", "tokens": [1577, 8310, 300, 428, 2316, 575, 586, 10833, 2212, 257, 6806, 2891, 8310, 295], "temperature": 0.0, "avg_logprob": -0.13859752693561592, "compression_ratio": 1.6076923076923078, "no_speech_prob": 4.26454862463288e-05}, {"id": 179, "seek": 83200, "start": 842.0, "end": 843.0, "text": " text.", "tokens": [2487, 13], "temperature": 0.0, "avg_logprob": -0.13859752693561592, "compression_ratio": 1.6076923076923078, "no_speech_prob": 4.26454862463288e-05}, {"id": 180, "seek": 83200, "start": 843.0, "end": 847.0, "text": " And so let's talk a bit about the algorithms that we can use to decode", "tokens": [400, 370, 718, 311, 751, 257, 857, 466, 264, 14642, 300, 321, 393, 764, 281, 979, 1429], "temperature": 0.0, "avg_logprob": -0.13859752693561592, "compression_ratio": 1.6076923076923078, "no_speech_prob": 4.26454862463288e-05}, {"id": 181, "seek": 83200, "start": 847.0, "end": 850.0, "text": " tokens from this distribution.", "tokens": [22667, 490, 341, 7316, 13], "temperature": 0.0, "avg_logprob": -0.13859752693561592, "compression_ratio": 1.6076923076923078, "no_speech_prob": 4.26454862463288e-05}, {"id": 182, "seek": 83200, "start": 850.0, "end": 854.0, "text": " So you've actually already seen some of these in a previous lecture on", "tokens": [407, 291, 600, 767, 1217, 1612, 512, 295, 613, 294, 257, 3894, 7991, 322], "temperature": 0.0, "avg_logprob": -0.13859752693561592, "compression_ratio": 1.6076923076923078, "no_speech_prob": 4.26454862463288e-05}, {"id": 183, "seek": 83200, "start": 854.0, "end": 859.0, "text": " neural machine translation, I believe, where you started off by by seeing a", "tokens": [18161, 3479, 12853, 11, 286, 1697, 11, 689, 291, 1409, 766, 538, 538, 2577, 257], "temperature": 0.0, "avg_logprob": -0.13859752693561592, "compression_ratio": 1.6076923076923078, "no_speech_prob": 4.26454862463288e-05}, {"id": 184, "seek": 85900, "start": 859.0, "end": 864.0, "text": " relatively simple decoding algorithm that nonetheless remains very popular,", "tokens": [7226, 2199, 979, 8616, 9284, 300, 26756, 7023, 588, 3743, 11], "temperature": 0.0, "avg_logprob": -0.1270844351570561, "compression_ratio": 1.728301886792453, "no_speech_prob": 2.5865705538308248e-05}, {"id": 185, "seek": 85900, "start": 864.0, "end": 868.0, "text": " argmax decoding. And with argmax decoding, you pretty much just take the highest", "tokens": [3882, 41167, 979, 8616, 13, 400, 365, 3882, 41167, 979, 8616, 11, 291, 1238, 709, 445, 747, 264, 6343], "temperature": 0.0, "avg_logprob": -0.1270844351570561, "compression_ratio": 1.728301886792453, "no_speech_prob": 2.5865705538308248e-05}, {"id": 186, "seek": 85900, "start": 868.0, "end": 873.0, "text": " probability token from your distribution as the decoded token and feed it back", "tokens": [8482, 14862, 490, 428, 7316, 382, 264, 979, 12340, 14862, 293, 3154, 309, 646], "temperature": 0.0, "avg_logprob": -0.1270844351570561, "compression_ratio": 1.728301886792453, "no_speech_prob": 2.5865705538308248e-05}, {"id": 187, "seek": 85900, "start": 873.0, "end": 877.0, "text": " into the model for to get the distribution of the next step. And you keep repeating", "tokens": [666, 264, 2316, 337, 281, 483, 264, 7316, 295, 264, 958, 1823, 13, 400, 291, 1066, 18617], "temperature": 0.0, "avg_logprob": -0.1270844351570561, "compression_ratio": 1.728301886792453, "no_speech_prob": 2.5865705538308248e-05}, {"id": 188, "seek": 85900, "start": 877.0, "end": 881.0, "text": " this process and it's very nice and it's very convenient.", "tokens": [341, 1399, 293, 309, 311, 588, 1481, 293, 309, 311, 588, 10851, 13], "temperature": 0.0, "avg_logprob": -0.1270844351570561, "compression_ratio": 1.728301886792453, "no_speech_prob": 2.5865705538308248e-05}, {"id": 189, "seek": 85900, "start": 881.0, "end": 885.0, "text": " And you've also, I think, learned about beam search where you can scale up these", "tokens": [400, 291, 600, 611, 11, 286, 519, 11, 3264, 466, 14269, 3164, 689, 291, 393, 4373, 493, 613], "temperature": 0.0, "avg_logprob": -0.1270844351570561, "compression_ratio": 1.728301886792453, "no_speech_prob": 2.5865705538308248e-05}, {"id": 190, "seek": 88500, "start": 885.0, "end": 891.0, "text": " these greedy methods by doing a wider search over the set of tokens that follow,", "tokens": [613, 28228, 7150, 538, 884, 257, 11842, 3164, 670, 264, 992, 295, 22667, 300, 1524, 11], "temperature": 0.0, "avg_logprob": -0.1203739983694894, "compression_ratio": 1.7455357142857142, "no_speech_prob": 4.5394823246169835e-05}, {"id": 191, "seek": 88500, "start": 891.0, "end": 896.0, "text": " the set of most likely tokens that follow to try to find a seek a sub sequence that", "tokens": [264, 992, 295, 881, 3700, 22667, 300, 1524, 281, 853, 281, 915, 257, 8075, 257, 1422, 8310, 300], "temperature": 0.0, "avg_logprob": -0.1203739983694894, "compression_ratio": 1.7455357142857142, "no_speech_prob": 4.5394823246169835e-05}, {"id": 192, "seek": 88500, "start": 896.0, "end": 902.0, "text": " is a lower overall negative log likelihood, even if it in the intermediate step,", "tokens": [307, 257, 3126, 4787, 3671, 3565, 22119, 11, 754, 498, 309, 294, 264, 19376, 1823, 11], "temperature": 0.0, "avg_logprob": -0.1203739983694894, "compression_ratio": 1.7455357142857142, "no_speech_prob": 4.5394823246169835e-05}, {"id": 193, "seek": 88500, "start": 902.0, "end": 907.0, "text": " it tends to be higher than what would be the argmax decoded token.", "tokens": [309, 12258, 281, 312, 2946, 813, 437, 576, 312, 264, 3882, 41167, 979, 12340, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1203739983694894, "compression_ratio": 1.7455357142857142, "no_speech_prob": 4.5394823246169835e-05}, {"id": 194, "seek": 88500, "start": 907.0, "end": 913.0, "text": " And while these greedy methods work great for machine translation and in other", "tokens": [400, 1339, 613, 28228, 7150, 589, 869, 337, 3479, 12853, 293, 294, 661], "temperature": 0.0, "avg_logprob": -0.1203739983694894, "compression_ratio": 1.7455357142857142, "no_speech_prob": 4.5394823246169835e-05}, {"id": 195, "seek": 91300, "start": 913.0, "end": 918.0, "text": " tests as well, such as summarization, they do tend to be problematic in many other", "tokens": [6921, 382, 731, 11, 1270, 382, 14611, 2144, 11, 436, 360, 3928, 281, 312, 19011, 294, 867, 661], "temperature": 0.0, "avg_logprob": -0.1469325204180856, "compression_ratio": 1.694736842105263, "no_speech_prob": 9.760572720551863e-05}, {"id": 196, "seek": 91300, "start": 918.0, "end": 923.0, "text": " text generation tasks, particularly ones that end up being more open-ended.", "tokens": [2487, 5125, 9608, 11, 4098, 2306, 300, 917, 493, 885, 544, 1269, 12, 3502, 13], "temperature": 0.0, "avg_logprob": -0.1469325204180856, "compression_ratio": 1.694736842105263, "no_speech_prob": 9.760572720551863e-05}, {"id": 197, "seek": 91300, "start": 923.0, "end": 927.0, "text": " So one of these big problems that they have is that they often end up repeating", "tokens": [407, 472, 295, 613, 955, 2740, 300, 436, 362, 307, 300, 436, 2049, 917, 493, 18617], "temperature": 0.0, "avg_logprob": -0.1469325204180856, "compression_ratio": 1.694736842105263, "no_speech_prob": 9.760572720551863e-05}, {"id": 198, "seek": 91300, "start": 927.0, "end": 932.0, "text": " themselves. So here in this example from from Holtsman at all 2020, we can see that", "tokens": [2969, 13, 407, 510, 294, 341, 1365, 490, 490, 11086, 1373, 1601, 412, 439, 4808, 11, 321, 393, 536, 300], "temperature": 0.0, "avg_logprob": -0.1469325204180856, "compression_ratio": 1.694736842105263, "no_speech_prob": 9.760572720551863e-05}, {"id": 199, "seek": 91300, "start": 932.0, "end": 938.0, "text": " after around 20 or sorry, yet 60 tokens or so of generation, the model really", "tokens": [934, 926, 945, 420, 2597, 11, 1939, 4060, 22667, 420, 370, 295, 5125, 11, 264, 2316, 534], "temperature": 0.0, "avg_logprob": -0.1469325204180856, "compression_ratio": 1.694736842105263, "no_speech_prob": 9.760572720551863e-05}, {"id": 200, "seek": 91300, "start": 938.0, "end": 942.0, "text": " devolves into just repeating the same thing over and over again. And this actually", "tokens": [1905, 401, 977, 666, 445, 18617, 264, 912, 551, 670, 293, 670, 797, 13, 400, 341, 767], "temperature": 0.0, "avg_logprob": -0.1469325204180856, "compression_ratio": 1.694736842105263, "no_speech_prob": 9.760572720551863e-05}, {"id": 201, "seek": 94200, "start": 942.0, "end": 947.0, "text": " tends to happen a lot in text generation systems. Repetition was actually one of", "tokens": [12258, 281, 1051, 257, 688, 294, 2487, 5125, 3652, 13, 3696, 302, 849, 390, 767, 472, 295], "temperature": 0.0, "avg_logprob": -0.10688317477048098, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.502250471385196e-05}, {"id": 202, "seek": 94200, "start": 947.0, "end": 951.0, "text": " the biggest problems that we that we tried to tackle in text generation for many", "tokens": [264, 3880, 2740, 300, 321, 300, 321, 3031, 281, 14896, 294, 2487, 5125, 337, 867], "temperature": 0.0, "avg_logprob": -0.10688317477048098, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.502250471385196e-05}, {"id": 203, "seek": 94200, "start": 951.0, "end": 956.0, "text": " years and then still face to this day. And you know, I think it's worth taking a", "tokens": [924, 293, 550, 920, 1851, 281, 341, 786, 13, 400, 291, 458, 11, 286, 519, 309, 311, 3163, 1940, 257], "temperature": 0.0, "avg_logprob": -0.10688317477048098, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.502250471385196e-05}, {"id": 204, "seek": 94200, "start": 956.0, "end": 962.0, "text": " look at why repetition happens a bit more analytically so you can perhaps", "tokens": [574, 412, 983, 30432, 2314, 257, 857, 544, 10783, 984, 370, 291, 393, 4317], "temperature": 0.0, "avg_logprob": -0.10688317477048098, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.502250471385196e-05}, {"id": 205, "seek": 94200, "start": 962.0, "end": 967.0, "text": " better understand the interaction between your model and your decoding algorithm.", "tokens": [1101, 1223, 264, 9285, 1296, 428, 2316, 293, 428, 979, 8616, 9284, 13], "temperature": 0.0, "avg_logprob": -0.10688317477048098, "compression_ratio": 1.6936170212765957, "no_speech_prob": 6.502250471385196e-05}, {"id": 206, "seek": 96700, "start": 967.0, "end": 972.0, "text": " So just as a quick little visual demonstration, here I'm showing you the step by step", "tokens": [407, 445, 382, 257, 1702, 707, 5056, 16520, 11, 510, 286, 478, 4099, 291, 264, 1823, 538, 1823], "temperature": 0.0, "avg_logprob": -0.12686342888690055, "compression_ratio": 1.5891472868217054, "no_speech_prob": 0.00014200354053173214}, {"id": 207, "seek": 96700, "start": 972.0, "end": 976.0, "text": " negative log likelihoods from two different language models, one based on recurrent", "tokens": [3671, 3565, 22119, 82, 490, 732, 819, 2856, 5245, 11, 472, 2361, 322, 18680, 1753], "temperature": 0.0, "avg_logprob": -0.12686342888690055, "compression_ratio": 1.5891472868217054, "no_speech_prob": 0.00014200354053173214}, {"id": 208, "seek": 96700, "start": 976.0, "end": 981.0, "text": " neural networks and one based on a transformer language model called GPT.", "tokens": [18161, 9590, 293, 472, 2361, 322, 257, 31782, 2856, 2316, 1219, 26039, 51, 13], "temperature": 0.0, "avg_logprob": -0.12686342888690055, "compression_ratio": 1.5891472868217054, "no_speech_prob": 0.00014200354053173214}, {"id": 209, "seek": 96700, "start": 981.0, "end": 987.0, "text": " And I'm showing this this plot for a particular phrase I don't know, which for", "tokens": [400, 286, 478, 4099, 341, 341, 7542, 337, 257, 1729, 9535, 286, 500, 380, 458, 11, 597, 337], "temperature": 0.0, "avg_logprob": -0.12686342888690055, "compression_ratio": 1.5891472868217054, "no_speech_prob": 0.00014200354053173214}, {"id": 210, "seek": 96700, "start": 987.0, "end": 993.0, "text": " anyone who's worked in chatbots has probably seen many times potentially in nightmares.", "tokens": [2878, 567, 311, 2732, 294, 5081, 65, 1971, 575, 1391, 1612, 867, 1413, 7263, 294, 36911, 13], "temperature": 0.0, "avg_logprob": -0.12686342888690055, "compression_ratio": 1.5891472868217054, "no_speech_prob": 0.00014200354053173214}, {"id": 211, "seek": 99300, "start": 993.0, "end": 998.0, "text": " It's not a very interesting plot, though, you know, what we do see is that the", "tokens": [467, 311, 406, 257, 588, 1880, 7542, 11, 1673, 11, 291, 458, 11, 437, 321, 360, 536, 307, 300, 264], "temperature": 0.0, "avg_logprob": -0.06043169431597273, "compression_ratio": 1.6654545454545455, "no_speech_prob": 2.35514289670391e-05}, {"id": 212, "seek": 99300, "start": 998.0, "end": 1002.0, "text": " transformer model does tend to be a bit less confident in the probability of each", "tokens": [31782, 2316, 775, 3928, 281, 312, 257, 857, 1570, 6679, 294, 264, 8482, 295, 1184], "temperature": 0.0, "avg_logprob": -0.06043169431597273, "compression_ratio": 1.6654545454545455, "no_speech_prob": 2.35514289670391e-05}, {"id": 213, "seek": 99300, "start": 1002.0, "end": 1006.0, "text": " word on the recurrent neural network does. What's more interesting though is what", "tokens": [1349, 322, 264, 18680, 1753, 18161, 3209, 775, 13, 708, 311, 544, 1880, 1673, 307, 437], "temperature": 0.0, "avg_logprob": -0.06043169431597273, "compression_ratio": 1.6654545454545455, "no_speech_prob": 2.35514289670391e-05}, {"id": 214, "seek": 99300, "start": 1006.0, "end": 1010.0, "text": " happens if I repeat this same phrase multiple times in a row.", "tokens": [2314, 498, 286, 7149, 341, 912, 9535, 3866, 1413, 294, 257, 5386, 13], "temperature": 0.0, "avg_logprob": -0.06043169431597273, "compression_ratio": 1.6654545454545455, "no_speech_prob": 2.35514289670391e-05}, {"id": 215, "seek": 99300, "start": 1010.0, "end": 1014.0, "text": " And one of the things that we notice here is that the repetition of this phrase", "tokens": [400, 472, 295, 264, 721, 300, 321, 3449, 510, 307, 300, 264, 30432, 295, 341, 9535], "temperature": 0.0, "avg_logprob": -0.06043169431597273, "compression_ratio": 1.6654545454545455, "no_speech_prob": 2.35514289670391e-05}, {"id": 216, "seek": 99300, "start": 1014.0, "end": 1020.0, "text": " actually causes the token level negative log likelihoods to get lower and", "tokens": [767, 7700, 264, 14862, 1496, 3671, 3565, 22119, 82, 281, 483, 3126, 293], "temperature": 0.0, "avg_logprob": -0.06043169431597273, "compression_ratio": 1.6654545454545455, "no_speech_prob": 2.35514289670391e-05}, {"id": 217, "seek": 102000, "start": 1020.0, "end": 1023.0, "text": " lower for each of these tokens, which actually means that the model is becoming", "tokens": [3126, 337, 1184, 295, 613, 22667, 11, 597, 767, 1355, 300, 264, 2316, 307, 5617], "temperature": 0.0, "avg_logprob": -0.11423398952672978, "compression_ratio": 1.8, "no_speech_prob": 4.469099803827703e-05}, {"id": 218, "seek": 102000, "start": 1023.0, "end": 1028.0, "text": " more confident that these are the right tokens and that they should probably follow", "tokens": [544, 6679, 300, 613, 366, 264, 558, 22667, 293, 300, 436, 820, 1391, 1524], "temperature": 0.0, "avg_logprob": -0.11423398952672978, "compression_ratio": 1.8, "no_speech_prob": 4.469099803827703e-05}, {"id": 219, "seek": 102000, "start": 1028.0, "end": 1031.0, "text": " the preceding context as we generate it more times.", "tokens": [264, 16969, 278, 4319, 382, 321, 8460, 309, 544, 1413, 13], "temperature": 0.0, "avg_logprob": -0.11423398952672978, "compression_ratio": 1.8, "no_speech_prob": 4.469099803827703e-05}, {"id": 220, "seek": 102000, "start": 1031.0, "end": 1036.0, "text": " And this doesn't really subside as the sequence gets longer and longer.", "tokens": [400, 341, 1177, 380, 534, 2090, 482, 382, 264, 8310, 2170, 2854, 293, 2854, 13], "temperature": 0.0, "avg_logprob": -0.11423398952672978, "compression_ratio": 1.8, "no_speech_prob": 4.469099803827703e-05}, {"id": 221, "seek": 102000, "start": 1036.0, "end": 1041.0, "text": " And as you keep repeating the same phrases again over and over again, the model", "tokens": [400, 382, 291, 1066, 18617, 264, 912, 20312, 797, 670, 293, 670, 797, 11, 264, 2316], "temperature": 0.0, "avg_logprob": -0.11423398952672978, "compression_ratio": 1.8, "no_speech_prob": 4.469099803827703e-05}, {"id": 222, "seek": 102000, "start": 1041.0, "end": 1047.0, "text": " becomes more and more confident the next time around it should say the same thing.", "tokens": [3643, 544, 293, 544, 6679, 264, 958, 565, 926, 309, 820, 584, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.11423398952672978, "compression_ratio": 1.8, "no_speech_prob": 4.469099803827703e-05}, {"id": 223, "seek": 104700, "start": 1047.0, "end": 1051.0, "text": " And, you know, while this actually kind of makes sense and that if you, you know,", "tokens": [400, 11, 291, 458, 11, 1339, 341, 767, 733, 295, 1669, 2020, 293, 300, 498, 291, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.11151282276426043, "compression_ratio": 1.6988847583643123, "no_speech_prob": 7.367787475232035e-05}, {"id": 224, "seek": 104700, "start": 1051.0, "end": 1056.0, "text": " say the phrase, let's say I'm tired 15 times, it's a fair bet that on a 16th", "tokens": [584, 264, 9535, 11, 718, 311, 584, 286, 478, 5868, 2119, 1413, 11, 309, 311, 257, 3143, 778, 300, 322, 257, 3165, 392], "temperature": 0.0, "avg_logprob": -0.11151282276426043, "compression_ratio": 1.6988847583643123, "no_speech_prob": 7.367787475232035e-05}, {"id": 225, "seek": 104700, "start": 1056.0, "end": 1059.0, "text": " time you're actually going to say it again, it's not necessarily the behavior that", "tokens": [565, 291, 434, 767, 516, 281, 584, 309, 797, 11, 309, 311, 406, 4725, 264, 5223, 300], "temperature": 0.0, "avg_logprob": -0.11151282276426043, "compression_ratio": 1.6988847583643123, "no_speech_prob": 7.367787475232035e-05}, {"id": 226, "seek": 104700, "start": 1059.0, "end": 1063.0, "text": " we want our generation systems to get stuck in.", "tokens": [321, 528, 527, 5125, 3652, 281, 483, 5541, 294, 13], "temperature": 0.0, "avg_logprob": -0.11151282276426043, "compression_ratio": 1.6988847583643123, "no_speech_prob": 7.367787475232035e-05}, {"id": 227, "seek": 104700, "start": 1063.0, "end": 1067.0, "text": " Another interesting thing to note here as an aside is that this behavior is actually", "tokens": [3996, 1880, 551, 281, 3637, 510, 382, 364, 7359, 307, 300, 341, 5223, 307, 767], "temperature": 0.0, "avg_logprob": -0.11151282276426043, "compression_ratio": 1.6988847583643123, "no_speech_prob": 7.367787475232035e-05}, {"id": 228, "seek": 104700, "start": 1067.0, "end": 1072.0, "text": " less problematic in recurrent neural networks than in transformer language models.", "tokens": [1570, 19011, 294, 18680, 1753, 18161, 9590, 813, 294, 31782, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11151282276426043, "compression_ratio": 1.6988847583643123, "no_speech_prob": 7.367787475232035e-05}, {"id": 229, "seek": 107200, "start": 1072.0, "end": 1077.0, "text": " So you can see that for the LSTM, the curve flat, flat ends after a certain point.", "tokens": [407, 291, 393, 536, 300, 337, 264, 441, 6840, 44, 11, 264, 7605, 4962, 11, 4962, 5314, 934, 257, 1629, 935, 13], "temperature": 0.0, "avg_logprob": -0.12419456619400161, "compression_ratio": 1.6215277777777777, "no_speech_prob": 2.014441452047322e-05}, {"id": 230, "seek": 107200, "start": 1077.0, "end": 1081.0, "text": " And so if you remember in perhaps a previous lecture on why we might like", "tokens": [400, 370, 498, 291, 1604, 294, 4317, 257, 3894, 7991, 322, 983, 321, 1062, 411], "temperature": 0.0, "avg_logprob": -0.12419456619400161, "compression_ratio": 1.6215277777777777, "no_speech_prob": 2.014441452047322e-05}, {"id": 231, "seek": 107200, "start": 1081.0, "end": 1085.0, "text": " transformer language models, one of their benefits is that they don't have the temporal", "tokens": [31782, 2856, 5245, 11, 472, 295, 641, 5311, 307, 300, 436, 500, 380, 362, 264, 30881], "temperature": 0.0, "avg_logprob": -0.12419456619400161, "compression_ratio": 1.6215277777777777, "no_speech_prob": 2.014441452047322e-05}, {"id": 232, "seek": 107200, "start": 1085.0, "end": 1090.0, "text": " bottleneck of tracking a state, which a recurrent neural network tends to have.", "tokens": [44641, 547, 295, 11603, 257, 1785, 11, 597, 257, 18680, 1753, 18161, 3209, 12258, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.12419456619400161, "compression_ratio": 1.6215277777777777, "no_speech_prob": 2.014441452047322e-05}, {"id": 233, "seek": 107200, "start": 1090.0, "end": 1094.0, "text": " And so the removal of that bottleneck actually ends up making them more prone to repetitive", "tokens": [400, 370, 264, 17933, 295, 300, 44641, 547, 767, 5314, 493, 1455, 552, 544, 25806, 281, 29404], "temperature": 0.0, "avg_logprob": -0.12419456619400161, "compression_ratio": 1.6215277777777777, "no_speech_prob": 2.014441452047322e-05}, {"id": 234, "seek": 107200, "start": 1094.0, "end": 1099.0, "text": " behavior when you use greedy algorithms to decode.", "tokens": [5223, 562, 291, 764, 28228, 14642, 281, 979, 1429, 13], "temperature": 0.0, "avg_logprob": -0.12419456619400161, "compression_ratio": 1.6215277777777777, "no_speech_prob": 2.014441452047322e-05}, {"id": 235, "seek": 109900, "start": 1099.0, "end": 1103.0, "text": " So what can we actually do to reduce repetition since it's a pretty big problem in these", "tokens": [407, 437, 393, 321, 767, 360, 281, 5407, 30432, 1670, 309, 311, 257, 1238, 955, 1154, 294, 613], "temperature": 0.0, "avg_logprob": -0.13193668095411454, "compression_ratio": 1.6430976430976432, "no_speech_prob": 1.805736610549502e-05}, {"id": 236, "seek": 109900, "start": 1103.0, "end": 1107.0, "text": " systems? Well, there are actually quite a few proposed approaches in the last few", "tokens": [3652, 30, 1042, 11, 456, 366, 767, 1596, 257, 1326, 10348, 11587, 294, 264, 1036, 1326], "temperature": 0.0, "avg_logprob": -0.13193668095411454, "compression_ratio": 1.6430976430976432, "no_speech_prob": 1.805736610549502e-05}, {"id": 237, "seek": 109900, "start": 1107.0, "end": 1112.0, "text": " years, some which I'll summarize here, which, you know, range from the kind of hacky,", "tokens": [924, 11, 512, 597, 286, 603, 20858, 510, 11, 597, 11, 291, 458, 11, 3613, 490, 264, 733, 295, 10339, 88, 11], "temperature": 0.0, "avg_logprob": -0.13193668095411454, "compression_ratio": 1.6430976430976432, "no_speech_prob": 1.805736610549502e-05}, {"id": 238, "seek": 109900, "start": 1112.0, "end": 1117.0, "text": " but surprisingly effective, don't repeat any end grams at inference time.", "tokens": [457, 17600, 4942, 11, 500, 380, 7149, 604, 917, 11899, 412, 38253, 565, 13], "temperature": 0.0, "avg_logprob": -0.13193668095411454, "compression_ratio": 1.6430976430976432, "no_speech_prob": 1.805736610549502e-05}, {"id": 239, "seek": 109900, "start": 1117.0, "end": 1121.0, "text": " But there's also been training time approaches to do it, such as having a loss", "tokens": [583, 456, 311, 611, 668, 3097, 565, 11587, 281, 360, 309, 11, 1270, 382, 1419, 257, 4470], "temperature": 0.0, "avg_logprob": -0.13193668095411454, "compression_ratio": 1.6430976430976432, "no_speech_prob": 1.805736610549502e-05}, {"id": 240, "seek": 109900, "start": 1121.0, "end": 1126.0, "text": " function that minimizes the similarity between hidden activations at different", "tokens": [2445, 300, 4464, 5660, 264, 32194, 1296, 7633, 2430, 763, 412, 819], "temperature": 0.0, "avg_logprob": -0.13193668095411454, "compression_ratio": 1.6430976430976432, "no_speech_prob": 1.805736610549502e-05}, {"id": 241, "seek": 112600, "start": 1126.0, "end": 1131.0, "text": " steps or coverage loss that penalizes attending to the same tokens over time.", "tokens": [4439, 420, 9645, 4470, 300, 13661, 5660, 15862, 281, 264, 912, 22667, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.16651687236747356, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.302332621999085e-05}, {"id": 242, "seek": 112600, "start": 1131.0, "end": 1134.0, "text": " So if you, you know, change the inputs that your model is allowed to focus on,", "tokens": [407, 498, 291, 11, 291, 458, 11, 1319, 264, 15743, 300, 428, 2316, 307, 4350, 281, 1879, 322, 11], "temperature": 0.0, "avg_logprob": -0.16651687236747356, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.302332621999085e-05}, {"id": 243, "seek": 112600, "start": 1134.0, "end": 1138.0, "text": " it's naturally going to produce different text or more recently an unlikely", "tokens": [309, 311, 8195, 516, 281, 5258, 819, 2487, 420, 544, 3938, 364, 17518], "temperature": 0.0, "avg_logprob": -0.16651687236747356, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.302332621999085e-05}, {"id": 244, "seek": 112600, "start": 1138.0, "end": 1142.0, "text": " hood objective that actually penalizes outputting the same words, which we'll talk a bit", "tokens": [13376, 10024, 300, 767, 13661, 5660, 5598, 783, 264, 912, 2283, 11, 597, 321, 603, 751, 257, 857], "temperature": 0.0, "avg_logprob": -0.16651687236747356, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.302332621999085e-05}, {"id": 245, "seek": 112600, "start": 1142.0, "end": 1145.0, "text": " more about later.", "tokens": [544, 466, 1780, 13], "temperature": 0.0, "avg_logprob": -0.16651687236747356, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.302332621999085e-05}, {"id": 246, "seek": 112600, "start": 1145.0, "end": 1151.0, "text": " But the truth is, is that the problem here really lies in using greedy algorithms", "tokens": [583, 264, 3494, 307, 11, 307, 300, 264, 1154, 510, 534, 9134, 294, 1228, 28228, 14642], "temperature": 0.0, "avg_logprob": -0.16651687236747356, "compression_ratio": 1.6317829457364341, "no_speech_prob": 6.302332621999085e-05}, {"id": 247, "seek": 115100, "start": 1151.0, "end": 1156.0, "text": " in the first place. In many applications of, you know, human language,", "tokens": [294, 264, 700, 1081, 13, 682, 867, 5821, 295, 11, 291, 458, 11, 1952, 2856, 11], "temperature": 0.0, "avg_logprob": -0.11763523646763392, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.704966002260335e-05}, {"id": 248, "seek": 115100, "start": 1156.0, "end": 1160.0, "text": " humans don't actually speak in a probability maximizing way.", "tokens": [6255, 500, 380, 767, 1710, 294, 257, 8482, 5138, 3319, 636, 13], "temperature": 0.0, "avg_logprob": -0.11763523646763392, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.704966002260335e-05}, {"id": 249, "seek": 115100, "start": 1160.0, "end": 1165.0, "text": " So if you look at this plot from Ultimate All 2020, it shows the per-time step", "tokens": [407, 498, 291, 574, 412, 341, 7542, 490, 26570, 1057, 4808, 11, 309, 3110, 264, 680, 12, 3766, 1823], "temperature": 0.0, "avg_logprob": -0.11763523646763392, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.704966002260335e-05}, {"id": 250, "seek": 115100, "start": 1165.0, "end": 1170.0, "text": " probability of human written text in orange and beam search decoded text in blue", "tokens": [8482, 295, 1952, 3720, 2487, 294, 7671, 293, 14269, 3164, 979, 12340, 2487, 294, 3344], "temperature": 0.0, "avg_logprob": -0.11763523646763392, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.704966002260335e-05}, {"id": 251, "seek": 115100, "start": 1170.0, "end": 1174.0, "text": " on the same graph. And what you can see is that beam search decoded text tends to be", "tokens": [322, 264, 912, 4295, 13, 400, 437, 291, 393, 536, 307, 300, 14269, 3164, 979, 12340, 2487, 12258, 281, 312], "temperature": 0.0, "avg_logprob": -0.11763523646763392, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.704966002260335e-05}, {"id": 252, "seek": 115100, "start": 1174.0, "end": 1178.0, "text": " very high probability with little variance over time.", "tokens": [588, 1090, 8482, 365, 707, 21977, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.11763523646763392, "compression_ratio": 1.6538461538461537, "no_speech_prob": 3.704966002260335e-05}, {"id": 253, "seek": 117800, "start": 1178.0, "end": 1182.0, "text": " And this makes a lot of sense, you know, because it's literally trying to maximize", "tokens": [400, 341, 1669, 257, 688, 295, 2020, 11, 291, 458, 11, 570, 309, 311, 3736, 1382, 281, 19874], "temperature": 0.0, "avg_logprob": -0.10219685959093498, "compression_ratio": 1.8885017421602788, "no_speech_prob": 2.586523987702094e-05}, {"id": 254, "seek": 117800, "start": 1182.0, "end": 1184.0, "text": " the probability of the sequences that it produces.", "tokens": [264, 8482, 295, 264, 22978, 300, 309, 14725, 13], "temperature": 0.0, "avg_logprob": -0.10219685959093498, "compression_ratio": 1.8885017421602788, "no_speech_prob": 2.586523987702094e-05}, {"id": 255, "seek": 117800, "start": 1184.0, "end": 1188.0, "text": " And a big part of that is maximizing the, you know, step by step probability", "tokens": [400, 257, 955, 644, 295, 300, 307, 5138, 3319, 264, 11, 291, 458, 11, 1823, 538, 1823, 8482], "temperature": 0.0, "avg_logprob": -0.10219685959093498, "compression_ratio": 1.8885017421602788, "no_speech_prob": 2.586523987702094e-05}, {"id": 256, "seek": 117800, "start": 1188.0, "end": 1190.0, "text": " of the tokens that it uses.", "tokens": [295, 264, 22667, 300, 309, 4960, 13], "temperature": 0.0, "avg_logprob": -0.10219685959093498, "compression_ratio": 1.8885017421602788, "no_speech_prob": 2.586523987702094e-05}, {"id": 257, "seek": 117800, "start": 1190.0, "end": 1193.0, "text": " But meanwhile, we can see that human written text is a lot more variable,", "tokens": [583, 29252, 11, 321, 393, 536, 300, 1952, 3720, 2487, 307, 257, 688, 544, 7006, 11], "temperature": 0.0, "avg_logprob": -0.10219685959093498, "compression_ratio": 1.8885017421602788, "no_speech_prob": 2.586523987702094e-05}, {"id": 258, "seek": 117800, "start": 1193.0, "end": 1196.0, "text": " often actually dipping into very low probability territory.", "tokens": [2049, 767, 35584, 666, 588, 2295, 8482, 11360, 13], "temperature": 0.0, "avg_logprob": -0.10219685959093498, "compression_ratio": 1.8885017421602788, "no_speech_prob": 2.586523987702094e-05}, {"id": 259, "seek": 117800, "start": 1196.0, "end": 1200.0, "text": " That actually makes a lot of sense. If we could always, you know, predict human text", "tokens": [663, 767, 1669, 257, 688, 295, 2020, 13, 759, 321, 727, 1009, 11, 291, 458, 11, 6069, 1952, 2487], "temperature": 0.0, "avg_logprob": -0.10219685959093498, "compression_ratio": 1.8885017421602788, "no_speech_prob": 2.586523987702094e-05}, {"id": 260, "seek": 117800, "start": 1200.0, "end": 1205.0, "text": " with high probability, you know, there really be no reason to listen to each other's", "tokens": [365, 1090, 8482, 11, 291, 458, 11, 456, 534, 312, 572, 1778, 281, 2140, 281, 1184, 661, 311], "temperature": 0.0, "avg_logprob": -0.10219685959093498, "compression_ratio": 1.8885017421602788, "no_speech_prob": 2.586523987702094e-05}, {"id": 261, "seek": 120500, "start": 1205.0, "end": 1208.0, "text": " comments. We know what we were going to say.", "tokens": [3053, 13, 492, 458, 437, 321, 645, 516, 281, 584, 13], "temperature": 0.0, "avg_logprob": -0.13957510642635013, "compression_ratio": 1.6383763837638377, "no_speech_prob": 2.1781872419524007e-05}, {"id": 262, "seek": 120500, "start": 1208.0, "end": 1212.0, "text": " But so ultimately, what we want to be able to do is to match the uncertainty of human", "tokens": [583, 370, 6284, 11, 437, 321, 528, 281, 312, 1075, 281, 360, 307, 281, 2995, 264, 15697, 295, 1952], "temperature": 0.0, "avg_logprob": -0.13957510642635013, "compression_ratio": 1.6383763837638377, "no_speech_prob": 2.1781872419524007e-05}, {"id": 263, "seek": 120500, "start": 1212.0, "end": 1217.0, "text": " language patterns in how we decode text, which is why in many applications that", "tokens": [2856, 8294, 294, 577, 321, 979, 1429, 2487, 11, 597, 307, 983, 294, 867, 5821, 300], "temperature": 0.0, "avg_logprob": -0.13957510642635013, "compression_ratio": 1.6383763837638377, "no_speech_prob": 2.1781872419524007e-05}, {"id": 264, "seek": 120500, "start": 1217.0, "end": 1223.0, "text": " tend to have this higher variability, sampling from these distributions has kind of", "tokens": [3928, 281, 362, 341, 2946, 35709, 11, 21179, 490, 613, 37870, 575, 733, 295], "temperature": 0.0, "avg_logprob": -0.13957510642635013, "compression_ratio": 1.6383763837638377, "no_speech_prob": 2.1781872419524007e-05}, {"id": 265, "seek": 120500, "start": 1223.0, "end": 1229.0, "text": " become a go-to decoding method, particularly in creative generation tasks.", "tokens": [1813, 257, 352, 12, 1353, 979, 8616, 3170, 11, 4098, 294, 5880, 5125, 9608, 13], "temperature": 0.0, "avg_logprob": -0.13957510642635013, "compression_ratio": 1.6383763837638377, "no_speech_prob": 2.1781872419524007e-05}, {"id": 266, "seek": 120500, "start": 1229.0, "end": 1234.0, "text": " And so with sampling, we take the distribution over tokens that's produced", "tokens": [400, 370, 365, 21179, 11, 321, 747, 264, 7316, 670, 22667, 300, 311, 7126], "temperature": 0.0, "avg_logprob": -0.13957510642635013, "compression_ratio": 1.6383763837638377, "no_speech_prob": 2.1781872419524007e-05}, {"id": 267, "seek": 123400, "start": 1234.0, "end": 1240.0, "text": " in our model, and we generate a token randomly, according to the probability", "tokens": [294, 527, 2316, 11, 293, 321, 8460, 257, 14862, 16979, 11, 4650, 281, 264, 8482], "temperature": 0.0, "avg_logprob": -0.1348061210230777, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.9856374971568584e-05}, {"id": 268, "seek": 123400, "start": 1240.0, "end": 1243.0, "text": " mass that is assigned to each potential option.", "tokens": [2758, 300, 307, 13279, 281, 1184, 3995, 3614, 13], "temperature": 0.0, "avg_logprob": -0.1348061210230777, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.9856374971568584e-05}, {"id": 269, "seek": 123400, "start": 1243.0, "end": 1248.0, "text": " So rather than doing any type of greedy step, we use the probability on each token", "tokens": [407, 2831, 813, 884, 604, 2010, 295, 28228, 1823, 11, 321, 764, 264, 8482, 322, 1184, 14862], "temperature": 0.0, "avg_logprob": -0.1348061210230777, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.9856374971568584e-05}, {"id": 270, "seek": 123400, "start": 1248.0, "end": 1254.0, "text": " to give us a chance that that token is generated.", "tokens": [281, 976, 505, 257, 2931, 300, 300, 14862, 307, 10833, 13], "temperature": 0.0, "avg_logprob": -0.1348061210230777, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.9856374971568584e-05}, {"id": 271, "seek": 123400, "start": 1254.0, "end": 1258.0, "text": " And so this does allow us to kind of get much more, you know,", "tokens": [400, 370, 341, 775, 2089, 505, 281, 733, 295, 483, 709, 544, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1348061210230777, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.9856374971568584e-05}, {"id": 272, "seek": 123400, "start": 1258.0, "end": 1261.0, "text": " stochasticity and the types of tokens that are generated.", "tokens": [342, 8997, 2750, 507, 293, 264, 3467, 295, 22667, 300, 366, 10833, 13], "temperature": 0.0, "avg_logprob": -0.1348061210230777, "compression_ratio": 1.6905829596412556, "no_speech_prob": 4.9856374971568584e-05}, {"id": 273, "seek": 126100, "start": 1261.0, "end": 1265.0, "text": " But a challenge that pops up here is that these distributions tend to be", "tokens": [583, 257, 3430, 300, 16795, 493, 510, 307, 300, 613, 37870, 3928, 281, 312], "temperature": 0.0, "avg_logprob": -0.06756877899169922, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.7534619221114554e-05}, {"id": 274, "seek": 126100, "start": 1265.0, "end": 1267.0, "text": " over a very large vocabulary.", "tokens": [670, 257, 588, 2416, 19864, 13], "temperature": 0.0, "avg_logprob": -0.06756877899169922, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.7534619221114554e-05}, {"id": 275, "seek": 126100, "start": 1267.0, "end": 1271.0, "text": " And so even if there's clearly tokens that have a higher chance of being", "tokens": [400, 370, 754, 498, 456, 311, 4448, 22667, 300, 362, 257, 2946, 2931, 295, 885], "temperature": 0.0, "avg_logprob": -0.06756877899169922, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.7534619221114554e-05}, {"id": 276, "seek": 126100, "start": 1271.0, "end": 1277.0, "text": " generated, the tail of the distribution can often be spread over a much larger", "tokens": [10833, 11, 264, 6838, 295, 264, 7316, 393, 2049, 312, 3974, 670, 257, 709, 4833], "temperature": 0.0, "avg_logprob": -0.06756877899169922, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.7534619221114554e-05}, {"id": 277, "seek": 126100, "start": 1277.0, "end": 1279.0, "text": " number of possible tokens.", "tokens": [1230, 295, 1944, 22667, 13], "temperature": 0.0, "avg_logprob": -0.06756877899169922, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.7534619221114554e-05}, {"id": 278, "seek": 126100, "start": 1279.0, "end": 1284.0, "text": " And so this becomes a bit of a problem because these tokens in the long tail", "tokens": [400, 370, 341, 3643, 257, 857, 295, 257, 1154, 570, 613, 22667, 294, 264, 938, 6838], "temperature": 0.0, "avg_logprob": -0.06756877899169922, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.7534619221114554e-05}, {"id": 279, "seek": 126100, "start": 1284.0, "end": 1287.0, "text": " are probably completely irrelevant to the current context.", "tokens": [366, 1391, 2584, 28682, 281, 264, 2190, 4319, 13], "temperature": 0.0, "avg_logprob": -0.06756877899169922, "compression_ratio": 1.7020408163265306, "no_speech_prob": 2.7534619221114554e-05}, {"id": 280, "seek": 128700, "start": 1287.0, "end": 1291.0, "text": " So they shouldn't have any chance of being selected individually.", "tokens": [407, 436, 4659, 380, 362, 604, 2931, 295, 885, 8209, 16652, 13], "temperature": 0.0, "avg_logprob": -0.1270070340898302, "compression_ratio": 1.6886446886446886, "no_speech_prob": 2.0144485461059958e-05}, {"id": 281, "seek": 128700, "start": 1291.0, "end": 1295.0, "text": " But as a group, it ends up that there's a decent chance that you could output", "tokens": [583, 382, 257, 1594, 11, 309, 5314, 493, 300, 456, 311, 257, 8681, 2931, 300, 291, 727, 5598], "temperature": 0.0, "avg_logprob": -0.1270070340898302, "compression_ratio": 1.6886446886446886, "no_speech_prob": 2.0144485461059958e-05}, {"id": 282, "seek": 128700, "start": 1295.0, "end": 1297.0, "text": " a completely irrelevant token.", "tokens": [257, 2584, 28682, 14862, 13], "temperature": 0.0, "avg_logprob": -0.1270070340898302, "compression_ratio": 1.6886446886446886, "no_speech_prob": 2.0144485461059958e-05}, {"id": 283, "seek": 128700, "start": 1297.0, "end": 1301.0, "text": " Even if 90% of your probability mass is on relevant tokens, that means that you have", "tokens": [2754, 498, 4289, 4, 295, 428, 8482, 2758, 307, 322, 7340, 22667, 11, 300, 1355, 300, 291, 362], "temperature": 0.0, "avg_logprob": -0.1270070340898302, "compression_ratio": 1.6886446886446886, "no_speech_prob": 2.0144485461059958e-05}, {"id": 284, "seek": 128700, "start": 1301.0, "end": 1305.0, "text": " a one-intent chance of outputting something that completely throws off your entire", "tokens": [257, 472, 12, 686, 317, 2931, 295, 5598, 783, 746, 300, 2584, 19251, 766, 428, 2302], "temperature": 0.0, "avg_logprob": -0.1270070340898302, "compression_ratio": 1.6886446886446886, "no_speech_prob": 2.0144485461059958e-05}, {"id": 285, "seek": 128700, "start": 1305.0, "end": 1309.0, "text": " text generation pipeline and is completely enane.", "tokens": [2487, 5125, 15517, 293, 307, 2584, 465, 1929, 13], "temperature": 0.0, "avg_logprob": -0.1270070340898302, "compression_ratio": 1.6886446886446886, "no_speech_prob": 2.0144485461059958e-05}, {"id": 286, "seek": 128700, "start": 1309.0, "end": 1314.0, "text": " So to mitigate this, the field has developed a new set of algorithms", "tokens": [407, 281, 27336, 341, 11, 264, 2519, 575, 4743, 257, 777, 992, 295, 14642], "temperature": 0.0, "avg_logprob": -0.1270070340898302, "compression_ratio": 1.6886446886446886, "no_speech_prob": 2.0144485461059958e-05}, {"id": 287, "seek": 131400, "start": 1314.0, "end": 1317.0, "text": " that tries to prune these distributions at inference time.", "tokens": [300, 9898, 281, 582, 2613, 613, 37870, 412, 38253, 565, 13], "temperature": 0.0, "avg_logprob": -0.10269314480811051, "compression_ratio": 1.6978723404255318, "no_speech_prob": 4.3998730689054355e-05}, {"id": 288, "seek": 131400, "start": 1317.0, "end": 1322.0, "text": " And so top-case sampling is kind of the most obvious way to do this.", "tokens": [400, 370, 1192, 12, 9765, 21179, 307, 733, 295, 264, 881, 6322, 636, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.10269314480811051, "compression_ratio": 1.6978723404255318, "no_speech_prob": 4.3998730689054355e-05}, {"id": 289, "seek": 131400, "start": 1322.0, "end": 1327.0, "text": " So here we recognize that most of the tokens in our vocabulary should have no", "tokens": [407, 510, 321, 5521, 300, 881, 295, 264, 22667, 294, 527, 19864, 820, 362, 572], "temperature": 0.0, "avg_logprob": -0.10269314480811051, "compression_ratio": 1.6978723404255318, "no_speech_prob": 4.3998730689054355e-05}, {"id": 290, "seek": 131400, "start": 1327.0, "end": 1329.0, "text": " probability of being selected at all.", "tokens": [8482, 295, 885, 8209, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.10269314480811051, "compression_ratio": 1.6978723404255318, "no_speech_prob": 4.3998730689054355e-05}, {"id": 291, "seek": 131400, "start": 1329.0, "end": 1333.0, "text": " So we just truncate the set of tokens that were allowed to sample from to be", "tokens": [407, 321, 445, 504, 409, 66, 473, 264, 992, 295, 22667, 300, 645, 4350, 281, 6889, 490, 281, 312], "temperature": 0.0, "avg_logprob": -0.10269314480811051, "compression_ratio": 1.6978723404255318, "no_speech_prob": 4.3998730689054355e-05}, {"id": 292, "seek": 131400, "start": 1333.0, "end": 1339.0, "text": " the K tokens with the highest amount of probability mass of the distributions.", "tokens": [264, 591, 22667, 365, 264, 6343, 2372, 295, 8482, 2758, 295, 264, 37870, 13], "temperature": 0.0, "avg_logprob": -0.10269314480811051, "compression_ratio": 1.6978723404255318, "no_speech_prob": 4.3998730689054355e-05}, {"id": 293, "seek": 133900, "start": 1339.0, "end": 1344.0, "text": " And common values of K are often 5, 10, 20, sometimes up to 100.", "tokens": [400, 2689, 4190, 295, 591, 366, 2049, 1025, 11, 1266, 11, 945, 11, 2171, 493, 281, 2319, 13], "temperature": 0.0, "avg_logprob": -0.08641253411769867, "compression_ratio": 1.6555183946488294, "no_speech_prob": 4.133335460210219e-05}, {"id": 294, "seek": 133900, "start": 1344.0, "end": 1349.0, "text": " But really, it's a hyperparameter that you end up setting as the designer of this system.", "tokens": [583, 534, 11, 309, 311, 257, 9848, 2181, 335, 2398, 300, 291, 917, 493, 3287, 382, 264, 11795, 295, 341, 1185, 13], "temperature": 0.0, "avg_logprob": -0.08641253411769867, "compression_ratio": 1.6555183946488294, "no_speech_prob": 4.133335460210219e-05}, {"id": 295, "seek": 133900, "start": 1349.0, "end": 1353.0, "text": " In general, though, what's important to note is that the higher you make K,", "tokens": [682, 2674, 11, 1673, 11, 437, 311, 1021, 281, 3637, 307, 300, 264, 2946, 291, 652, 591, 11], "temperature": 0.0, "avg_logprob": -0.08641253411769867, "compression_ratio": 1.6555183946488294, "no_speech_prob": 4.133335460210219e-05}, {"id": 296, "seek": 133900, "start": 1353.0, "end": 1357.0, "text": " the more you'll be able to generate diverse outputs, which is good,", "tokens": [264, 544, 291, 603, 312, 1075, 281, 8460, 9521, 23930, 11, 597, 307, 665, 11], "temperature": 0.0, "avg_logprob": -0.08641253411769867, "compression_ratio": 1.6555183946488294, "no_speech_prob": 4.133335460210219e-05}, {"id": 297, "seek": 133900, "start": 1357.0, "end": 1359.0, "text": " because that's what we're trying to do.", "tokens": [570, 300, 311, 437, 321, 434, 1382, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.08641253411769867, "compression_ratio": 1.6555183946488294, "no_speech_prob": 4.133335460210219e-05}, {"id": 298, "seek": 133900, "start": 1359.0, "end": 1363.0, "text": " But you're also going to increase the chance of letting that long tail seep in", "tokens": [583, 291, 434, 611, 516, 281, 3488, 264, 2931, 295, 8295, 300, 938, 6838, 536, 79, 294], "temperature": 0.0, "avg_logprob": -0.08641253411769867, "compression_ratio": 1.6555183946488294, "no_speech_prob": 4.133335460210219e-05}, {"id": 299, "seek": 133900, "start": 1363.0, "end": 1367.0, "text": " and generating something that's completely irrelevant to the current context.", "tokens": [293, 17746, 746, 300, 311, 2584, 28682, 281, 264, 2190, 4319, 13], "temperature": 0.0, "avg_logprob": -0.08641253411769867, "compression_ratio": 1.6555183946488294, "no_speech_prob": 4.133335460210219e-05}, {"id": 300, "seek": 136700, "start": 1367.0, "end": 1372.0, "text": " Oh, sorry. Meanwhile, if you decrease K, your outputs are going to be safer", "tokens": [876, 11, 2597, 13, 13879, 11, 498, 291, 11514, 591, 11, 428, 23930, 366, 516, 281, 312, 15856], "temperature": 0.0, "avg_logprob": -0.10155586131568094, "compression_ratio": 1.6452830188679246, "no_speech_prob": 5.224882625043392e-05}, {"id": 301, "seek": 136700, "start": 1372.0, "end": 1376.0, "text": " from these long tail effects, but your text may end up being boring and generic", "tokens": [490, 613, 938, 6838, 5065, 11, 457, 428, 2487, 815, 917, 493, 885, 9989, 293, 19577], "temperature": 0.0, "avg_logprob": -0.10155586131568094, "compression_ratio": 1.6452830188679246, "no_speech_prob": 5.224882625043392e-05}, {"id": 302, "seek": 136700, "start": 1376.0, "end": 1381.0, "text": " because your sampling algorithm starts to look a lot more greedy in nature.", "tokens": [570, 428, 21179, 9284, 3719, 281, 574, 257, 688, 544, 28228, 294, 3687, 13], "temperature": 0.0, "avg_logprob": -0.10155586131568094, "compression_ratio": 1.6452830188679246, "no_speech_prob": 5.224882625043392e-05}, {"id": 303, "seek": 136700, "start": 1381.0, "end": 1386.0, "text": " And this kind of shows the problem of having a fixed K as the number of tokens", "tokens": [400, 341, 733, 295, 3110, 264, 1154, 295, 1419, 257, 6806, 591, 382, 264, 1230, 295, 22667], "temperature": 0.0, "avg_logprob": -0.10155586131568094, "compression_ratio": 1.6452830188679246, "no_speech_prob": 5.224882625043392e-05}, {"id": 304, "seek": 136700, "start": 1386.0, "end": 1389.0, "text": " that you can generate from your distribution.", "tokens": [300, 291, 393, 8460, 490, 428, 7316, 13], "temperature": 0.0, "avg_logprob": -0.10155586131568094, "compression_ratio": 1.6452830188679246, "no_speech_prob": 5.224882625043392e-05}, {"id": 305, "seek": 136700, "start": 1389.0, "end": 1393.0, "text": " If your distribution is certain points is pretty flat, such as in this example,", "tokens": [759, 428, 7316, 307, 1629, 2793, 307, 1238, 4962, 11, 1270, 382, 294, 341, 1365, 11], "temperature": 0.0, "avg_logprob": -0.10155586131568094, "compression_ratio": 1.6452830188679246, "no_speech_prob": 5.224882625043392e-05}, {"id": 306, "seek": 139300, "start": 1393.0, "end": 1398.0, "text": " she said, I never blank, you might not want to truncate a lot of interesting options", "tokens": [750, 848, 11, 286, 1128, 8247, 11, 291, 1062, 406, 528, 281, 504, 409, 66, 473, 257, 688, 295, 1880, 3956], "temperature": 0.0, "avg_logprob": -0.09341447281114983, "compression_ratio": 1.7138047138047139, "no_speech_prob": 6.301926623564214e-05}, {"id": 307, "seek": 139300, "start": 1398.0, "end": 1402.0, "text": " that, you know, using a small value of K when there's so many good choices", "tokens": [300, 11, 291, 458, 11, 1228, 257, 1359, 2158, 295, 591, 562, 456, 311, 370, 867, 665, 7994], "temperature": 0.0, "avg_logprob": -0.09341447281114983, "compression_ratio": 1.7138047138047139, "no_speech_prob": 6.301926623564214e-05}, {"id": 308, "seek": 139300, "start": 1402.0, "end": 1406.0, "text": " that could fit in this potential context.", "tokens": [300, 727, 3318, 294, 341, 3995, 4319, 13], "temperature": 0.0, "avg_logprob": -0.09341447281114983, "compression_ratio": 1.7138047138047139, "no_speech_prob": 6.301926623564214e-05}, {"id": 309, "seek": 139300, "start": 1406.0, "end": 1410.0, "text": " You know, conversely in a different example, you might want to cut off much more", "tokens": [509, 458, 11, 2615, 736, 294, 257, 819, 1365, 11, 291, 1062, 528, 281, 1723, 766, 709, 544], "temperature": 0.0, "avg_logprob": -0.09341447281114983, "compression_ratio": 1.7138047138047139, "no_speech_prob": 6.301926623564214e-05}, {"id": 310, "seek": 139300, "start": 1410.0, "end": 1415.0, "text": " than let's say your minimum K options, because only a subset of them end up being quite suitable.", "tokens": [813, 718, 311, 584, 428, 7285, 591, 3956, 11, 570, 787, 257, 25993, 295, 552, 917, 493, 885, 1596, 12873, 13], "temperature": 0.0, "avg_logprob": -0.09341447281114983, "compression_ratio": 1.7138047138047139, "no_speech_prob": 6.301926623564214e-05}, {"id": 311, "seek": 139300, "start": 1415.0, "end": 1419.0, "text": " And, you know, a higher K than is really necessary, lets that long tail seep in", "tokens": [400, 11, 291, 458, 11, 257, 2946, 591, 813, 307, 534, 4818, 11, 6653, 300, 938, 6838, 536, 79, 294], "temperature": 0.0, "avg_logprob": -0.09341447281114983, "compression_ratio": 1.7138047138047139, "no_speech_prob": 6.301926623564214e-05}, {"id": 312, "seek": 139300, "start": 1419.0, "end": 1422.0, "text": " and potentially, you know, ruin your generation.", "tokens": [293, 7263, 11, 291, 458, 11, 15514, 428, 5125, 13], "temperature": 0.0, "avg_logprob": -0.09341447281114983, "compression_ratio": 1.7138047138047139, "no_speech_prob": 6.301926623564214e-05}, {"id": 313, "seek": 142200, "start": 1422.0, "end": 1428.0, "text": " And so, you know, in response to this top P or nucleus sampling is a way around this issue.", "tokens": [400, 370, 11, 291, 458, 11, 294, 4134, 281, 341, 1192, 430, 420, 28055, 21179, 307, 257, 636, 926, 341, 2734, 13], "temperature": 0.0, "avg_logprob": -0.09166907372875749, "compression_ratio": 1.8408163265306123, "no_speech_prob": 2.4299451979459263e-05}, {"id": 314, "seek": 142200, "start": 1428.0, "end": 1432.0, "text": " So here, instead of sampling from a fixed number of tokens at each step,", "tokens": [407, 510, 11, 2602, 295, 21179, 490, 257, 6806, 1230, 295, 22667, 412, 1184, 1823, 11], "temperature": 0.0, "avg_logprob": -0.09166907372875749, "compression_ratio": 1.8408163265306123, "no_speech_prob": 2.4299451979459263e-05}, {"id": 315, "seek": 142200, "start": 1432.0, "end": 1435.0, "text": " you sample from a fixed amount of probability mass.", "tokens": [291, 6889, 490, 257, 6806, 2372, 295, 8482, 2758, 13], "temperature": 0.0, "avg_logprob": -0.09166907372875749, "compression_ratio": 1.8408163265306123, "no_speech_prob": 2.4299451979459263e-05}, {"id": 316, "seek": 142200, "start": 1435.0, "end": 1439.0, "text": " And so depending on the flatness of your distribution,", "tokens": [400, 370, 5413, 322, 264, 4962, 1287, 295, 428, 7316, 11], "temperature": 0.0, "avg_logprob": -0.09166907372875749, "compression_ratio": 1.8408163265306123, "no_speech_prob": 2.4299451979459263e-05}, {"id": 317, "seek": 142200, "start": 1439.0, "end": 1446.0, "text": " you end up including a variable number of tokens that is, that is kind of dynamically", "tokens": [291, 917, 493, 3009, 257, 7006, 1230, 295, 22667, 300, 307, 11, 300, 307, 733, 295, 43492], "temperature": 0.0, "avg_logprob": -0.09166907372875749, "compression_ratio": 1.8408163265306123, "no_speech_prob": 2.4299451979459263e-05}, {"id": 318, "seek": 142200, "start": 1446.0, "end": 1451.0, "text": " changing depending on, you know, how that probability mass is spread across the distribution.", "tokens": [4473, 5413, 322, 11, 291, 458, 11, 577, 300, 8482, 2758, 307, 3974, 2108, 264, 7316, 13], "temperature": 0.0, "avg_logprob": -0.09166907372875749, "compression_ratio": 1.8408163265306123, "no_speech_prob": 2.4299451979459263e-05}, {"id": 319, "seek": 145100, "start": 1451.0, "end": 1456.0, "text": " And so, you know, to kind of describe this visually, if you have, you know,", "tokens": [400, 370, 11, 291, 458, 11, 281, 733, 295, 6786, 341, 19622, 11, 498, 291, 362, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.11271417552027209, "compression_ratio": 1.712962962962963, "no_speech_prob": 3.373591243871488e-05}, {"id": 320, "seek": 145100, "start": 1456.0, "end": 1460.0, "text": " three different distributions at a particular step to generate a particular token,", "tokens": [1045, 819, 37870, 412, 257, 1729, 1823, 281, 8460, 257, 1729, 14862, 11], "temperature": 0.0, "avg_logprob": -0.11271417552027209, "compression_ratio": 1.712962962962963, "no_speech_prob": 3.373591243871488e-05}, {"id": 321, "seek": 145100, "start": 1460.0, "end": 1466.0, "text": " they're each going to prune a different number of tokens from the available set that you can sample from,", "tokens": [436, 434, 1184, 516, 281, 582, 2613, 257, 819, 1230, 295, 22667, 490, 264, 2435, 992, 300, 291, 393, 6889, 490, 11], "temperature": 0.0, "avg_logprob": -0.11271417552027209, "compression_ratio": 1.712962962962963, "no_speech_prob": 3.373591243871488e-05}, {"id": 322, "seek": 145100, "start": 1466.0, "end": 1478.0, "text": " depending on what the value of P is and what the peakingness of that distribution actually ends up being.", "tokens": [5413, 322, 437, 264, 2158, 295, 430, 307, 293, 437, 264, 520, 2456, 1287, 295, 300, 7316, 767, 5314, 493, 885, 13], "temperature": 0.0, "avg_logprob": -0.11271417552027209, "compression_ratio": 1.712962962962963, "no_speech_prob": 3.373591243871488e-05}, {"id": 323, "seek": 147800, "start": 1478.0, "end": 1483.0, "text": " So, you know, I keep talking about this concept of flatness of a distribution,", "tokens": [407, 11, 291, 458, 11, 286, 1066, 1417, 466, 341, 3410, 295, 4962, 1287, 295, 257, 7316, 11], "temperature": 0.0, "avg_logprob": -0.07059664996165149, "compression_ratio": 1.7969924812030076, "no_speech_prob": 3.705160634126514e-05}, {"id": 324, "seek": 147800, "start": 1483.0, "end": 1489.0, "text": " being kind of critical and understanding how many tokens we can actually end up sampling from.", "tokens": [885, 733, 295, 4924, 293, 3701, 577, 867, 22667, 321, 393, 767, 917, 493, 21179, 490, 13], "temperature": 0.0, "avg_logprob": -0.07059664996165149, "compression_ratio": 1.7969924812030076, "no_speech_prob": 3.705160634126514e-05}, {"id": 325, "seek": 147800, "start": 1489.0, "end": 1495.0, "text": " And in fact, as we try to use sampling algorithms, we might find that the model that we've learned", "tokens": [400, 294, 1186, 11, 382, 321, 853, 281, 764, 21179, 14642, 11, 321, 1062, 915, 300, 264, 2316, 300, 321, 600, 3264], "temperature": 0.0, "avg_logprob": -0.07059664996165149, "compression_ratio": 1.7969924812030076, "no_speech_prob": 3.705160634126514e-05}, {"id": 326, "seek": 147800, "start": 1495.0, "end": 1501.0, "text": " may not actually be producing probability distributions that lend themselves very nicely to using these types of sampling algorithms.", "tokens": [815, 406, 767, 312, 10501, 8482, 37870, 300, 21774, 2969, 588, 9594, 281, 1228, 613, 3467, 295, 21179, 14642, 13], "temperature": 0.0, "avg_logprob": -0.07059664996165149, "compression_ratio": 1.7969924812030076, "no_speech_prob": 3.705160634126514e-05}, {"id": 327, "seek": 147800, "start": 1501.0, "end": 1505.0, "text": " You know, the distributions might be too flat, they might be too peaky.", "tokens": [509, 458, 11, 264, 37870, 1062, 312, 886, 4962, 11, 436, 1062, 312, 886, 520, 15681, 13], "temperature": 0.0, "avg_logprob": -0.07059664996165149, "compression_ratio": 1.7969924812030076, "no_speech_prob": 3.705160634126514e-05}, {"id": 328, "seek": 150500, "start": 1505.0, "end": 1512.0, "text": " And in fact, what we might want to do is rescale those distributions to better fit the decoding algorithm that we might want to use.", "tokens": [400, 294, 1186, 11, 437, 321, 1062, 528, 281, 360, 307, 9610, 1220, 729, 37870, 281, 1101, 3318, 264, 979, 8616, 9284, 300, 321, 1062, 528, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.084317261832101, "compression_ratio": 1.7480314960629921, "no_speech_prob": 2.0783736545126885e-05}, {"id": 329, "seek": 150500, "start": 1512.0, "end": 1519.0, "text": " And we can do this with a method that's, you know, goes by a variety of different names, which I call temperature scaling.", "tokens": [400, 321, 393, 360, 341, 365, 257, 3170, 300, 311, 11, 291, 458, 11, 1709, 538, 257, 5673, 295, 819, 5288, 11, 597, 286, 818, 4292, 21589, 13], "temperature": 0.0, "avg_logprob": -0.084317261832101, "compression_ratio": 1.7480314960629921, "no_speech_prob": 2.0783736545126885e-05}, {"id": 330, "seek": 150500, "start": 1519.0, "end": 1525.0, "text": " And here what you do is that you apply a linear coefficient to every score for each token.", "tokens": [400, 510, 437, 291, 360, 307, 300, 291, 3079, 257, 8213, 17619, 281, 633, 6175, 337, 1184, 14862, 13], "temperature": 0.0, "avg_logprob": -0.084317261832101, "compression_ratio": 1.7480314960629921, "no_speech_prob": 2.0783736545126885e-05}, {"id": 331, "seek": 150500, "start": 1525.0, "end": 1530.0, "text": " Before you pass it through the softmax, that temperature coefficient is the same for every token.", "tokens": [4546, 291, 1320, 309, 807, 264, 2787, 41167, 11, 300, 4292, 17619, 307, 264, 912, 337, 633, 14862, 13], "temperature": 0.0, "avg_logprob": -0.084317261832101, "compression_ratio": 1.7480314960629921, "no_speech_prob": 2.0783736545126885e-05}, {"id": 332, "seek": 153000, "start": 1530.0, "end": 1535.0, "text": " It's not dynamically changing amongst your vocabulary. It stays the same.", "tokens": [467, 311, 406, 43492, 4473, 12918, 428, 19864, 13, 467, 10834, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.09042190250597502, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.429927553748712e-05}, {"id": 333, "seek": 153000, "start": 1535.0, "end": 1541.0, "text": " But what happens is that that change ends up being amplified by the softmax function.", "tokens": [583, 437, 2314, 307, 300, 300, 1319, 5314, 493, 885, 49237, 538, 264, 2787, 41167, 2445, 13], "temperature": 0.0, "avg_logprob": -0.09042190250597502, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.429927553748712e-05}, {"id": 334, "seek": 153000, "start": 1541.0, "end": 1550.0, "text": " And what ends up happening is that if your temperature coefficient is greater than one, you're actually going to make your probability distribution much more uniform.", "tokens": [400, 437, 5314, 493, 2737, 307, 300, 498, 428, 4292, 17619, 307, 5044, 813, 472, 11, 291, 434, 767, 516, 281, 652, 428, 8482, 7316, 709, 544, 9452, 13], "temperature": 0.0, "avg_logprob": -0.09042190250597502, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.429927553748712e-05}, {"id": 335, "seek": 153000, "start": 1550.0, "end": 1553.0, "text": " In other words, you're going to make it flatter.", "tokens": [682, 661, 2283, 11, 291, 434, 516, 281, 652, 309, 41247, 13], "temperature": 0.0, "avg_logprob": -0.09042190250597502, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.429927553748712e-05}, {"id": 336, "seek": 153000, "start": 1553.0, "end": 1559.0, "text": " Meanwhile, if your temperature coefficient is less than one, these scores are going to increase, which is going to make your distributions more spiky.", "tokens": [13879, 11, 498, 428, 4292, 17619, 307, 1570, 813, 472, 11, 613, 13444, 366, 516, 281, 3488, 11, 597, 307, 516, 281, 652, 428, 37870, 544, 637, 1035, 88, 13], "temperature": 0.0, "avg_logprob": -0.09042190250597502, "compression_ratio": 1.8989169675090252, "no_speech_prob": 2.429927553748712e-05}, {"id": 337, "seek": 155900, "start": 1559.0, "end": 1564.0, "text": " And make the probability mass kind of be pushed towards the most likely tokens.", "tokens": [400, 652, 264, 8482, 2758, 733, 295, 312, 9152, 3030, 264, 881, 3700, 22667, 13], "temperature": 0.0, "avg_logprob": -0.11241878653472324, "compression_ratio": 1.6940298507462686, "no_speech_prob": 3.3735264878487214e-05}, {"id": 338, "seek": 155900, "start": 1564.0, "end": 1569.0, "text": " One last thing to note about temperature is that it's not actually a decoding algorithm.", "tokens": [1485, 1036, 551, 281, 3637, 466, 4292, 307, 300, 309, 311, 406, 767, 257, 979, 8616, 9284, 13], "temperature": 0.0, "avg_logprob": -0.11241878653472324, "compression_ratio": 1.6940298507462686, "no_speech_prob": 3.3735264878487214e-05}, {"id": 339, "seek": 155900, "start": 1569.0, "end": 1573.0, "text": " It's just a way of rebalancing your probability distribution.", "tokens": [467, 311, 445, 257, 636, 295, 319, 2645, 8779, 428, 8482, 7316, 13], "temperature": 0.0, "avg_logprob": -0.11241878653472324, "compression_ratio": 1.6940298507462686, "no_speech_prob": 3.3735264878487214e-05}, {"id": 340, "seek": 155900, "start": 1573.0, "end": 1577.0, "text": " So in fact, it can be applied to all of the sampling algorithms I described before.", "tokens": [407, 294, 1186, 11, 309, 393, 312, 6456, 281, 439, 295, 264, 21179, 14642, 286, 7619, 949, 13], "temperature": 0.0, "avg_logprob": -0.11241878653472324, "compression_ratio": 1.6940298507462686, "no_speech_prob": 3.3735264878487214e-05}, {"id": 341, "seek": 155900, "start": 1577.0, "end": 1581.0, "text": " And some greedy decoding algorithms as well.", "tokens": [400, 512, 28228, 979, 8616, 14642, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11241878653472324, "compression_ratio": 1.6940298507462686, "no_speech_prob": 3.3735264878487214e-05}, {"id": 342, "seek": 155900, "start": 1581.0, "end": 1587.0, "text": " The only one whose behavior is not affected by softmax temperature scaling is argmax decoding.", "tokens": [440, 787, 472, 6104, 5223, 307, 406, 8028, 538, 2787, 41167, 4292, 21589, 307, 3882, 41167, 979, 8616, 13], "temperature": 0.0, "avg_logprob": -0.11241878653472324, "compression_ratio": 1.6940298507462686, "no_speech_prob": 3.3735264878487214e-05}, {"id": 343, "seek": 158700, "start": 1587.0, "end": 1598.0, "text": " Because even though you change the relative magnitudes of the probability mass in your distribution, you don't actually change the relative ranking amongst tokens in that distribution.", "tokens": [1436, 754, 1673, 291, 1319, 264, 4972, 4944, 16451, 295, 264, 8482, 2758, 294, 428, 7316, 11, 291, 500, 380, 767, 1319, 264, 4972, 17833, 12918, 22667, 294, 300, 7316, 13], "temperature": 0.0, "avg_logprob": -0.08528138124025784, "compression_ratio": 1.5432098765432098, "no_speech_prob": 7.843093044357374e-05}, {"id": 344, "seek": 158700, "start": 1598.0, "end": 1606.0, "text": " So argmax decoding will give you the exact same output as before.", "tokens": [407, 3882, 41167, 979, 8616, 486, 976, 291, 264, 1900, 912, 5598, 382, 949, 13], "temperature": 0.0, "avg_logprob": -0.08528138124025784, "compression_ratio": 1.5432098765432098, "no_speech_prob": 7.843093044357374e-05}, {"id": 345, "seek": 160600, "start": 1606.0, "end": 1617.0, "text": " But now that we're thinking about how we might be changed to the distribution that's produced by our model, we might realize that we might want to change more than the relative magnitudes I mentioned.", "tokens": [583, 586, 300, 321, 434, 1953, 466, 577, 321, 1062, 312, 3105, 281, 264, 7316, 300, 311, 7126, 538, 527, 2316, 11, 321, 1062, 4325, 300, 321, 1062, 528, 281, 1319, 544, 813, 264, 4972, 4944, 16451, 286, 2835, 13], "temperature": 0.0, "avg_logprob": -0.08717144989385837, "compression_ratio": 1.6563876651982379, "no_speech_prob": 7.482133514713496e-05}, {"id": 346, "seek": 160600, "start": 1617.0, "end": 1620.0, "text": " And also instead change how they're ranked with respect to one another.", "tokens": [400, 611, 2602, 1319, 577, 436, 434, 20197, 365, 3104, 281, 472, 1071, 13], "temperature": 0.0, "avg_logprob": -0.08717144989385837, "compression_ratio": 1.6563876651982379, "no_speech_prob": 7.482133514713496e-05}, {"id": 347, "seek": 160600, "start": 1620.0, "end": 1629.0, "text": " Maybe in fact, our model is not a perfect approximation of what the distribution over tokens should be.", "tokens": [2704, 294, 1186, 11, 527, 2316, 307, 406, 257, 2176, 28023, 295, 437, 264, 7316, 670, 22667, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.08717144989385837, "compression_ratio": 1.6563876651982379, "no_speech_prob": 7.482133514713496e-05}, {"id": 348, "seek": 162900, "start": 1629.0, "end": 1636.0, "text": " You know, perhaps the training was done right or we didn't have enough training data to actually make it well calibrated.", "tokens": [509, 458, 11, 4317, 264, 3097, 390, 1096, 558, 420, 321, 994, 380, 362, 1547, 3097, 1412, 281, 767, 652, 309, 731, 21583, 5468, 13], "temperature": 0.0, "avg_logprob": -0.08949877999045631, "compression_ratio": 1.7428571428571429, "no_speech_prob": 7.60165712563321e-05}, {"id": 349, "seek": 162900, "start": 1636.0, "end": 1644.0, "text": " And so if we decide that our model isn't well calibrated for the task that we're doing, we may want to bring in outside information at decoding time.", "tokens": [400, 370, 498, 321, 4536, 300, 527, 2316, 1943, 380, 731, 21583, 5468, 337, 264, 5633, 300, 321, 434, 884, 11, 321, 815, 528, 281, 1565, 294, 2380, 1589, 412, 979, 8616, 565, 13], "temperature": 0.0, "avg_logprob": -0.08949877999045631, "compression_ratio": 1.7428571428571429, "no_speech_prob": 7.60165712563321e-05}, {"id": 350, "seek": 162900, "start": 1644.0, "end": 1658.0, "text": " And so here I want to talk a bit about new classes of methods that let us change our model prediction distributions at inference time, rather than relying on a fixed static model that's that's only been trained once.", "tokens": [400, 370, 510, 286, 528, 281, 751, 257, 857, 466, 777, 5359, 295, 7150, 300, 718, 505, 1319, 527, 2316, 17630, 37870, 412, 38253, 565, 11, 2831, 813, 24140, 322, 257, 6806, 13437, 2316, 300, 311, 300, 311, 787, 668, 8895, 1564, 13], "temperature": 0.0, "avg_logprob": -0.08949877999045631, "compression_ratio": 1.7428571428571429, "no_speech_prob": 7.60165712563321e-05}, {"id": 351, "seek": 165800, "start": 1658.0, "end": 1676.0, "text": " And a cool way of doing this that came out last year is to actually use cane years neighbor language models, which allow you to to recalibrate your output probability distribution by using phrase statistics from let's say a much larger corpus.", "tokens": [400, 257, 1627, 636, 295, 884, 341, 300, 1361, 484, 1036, 1064, 307, 281, 767, 764, 27518, 924, 5987, 2856, 5245, 11, 597, 2089, 291, 281, 281, 850, 304, 897, 4404, 428, 5598, 8482, 7316, 538, 1228, 9535, 12523, 490, 718, 311, 584, 257, 709, 4833, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.16189152339719376, "compression_ratio": 1.4550898203592815, "no_speech_prob": 8.749114931561053e-05}, {"id": 352, "seek": 167600, "start": 1676.0, "end": 1687.0, "text": " And so, you know, what you do in this method is that you initialize a large database of phrases along with vector representations for those phrases.", "tokens": [400, 370, 11, 291, 458, 11, 437, 291, 360, 294, 341, 3170, 307, 300, 291, 5883, 1125, 257, 2416, 8149, 295, 20312, 2051, 365, 8062, 33358, 337, 729, 20312, 13], "temperature": 0.0, "avg_logprob": -0.10821807815367918, "compression_ratio": 1.8071065989847717, "no_speech_prob": 3.8223759474931285e-05}, {"id": 353, "seek": 167600, "start": 1687.0, "end": 1693.0, "text": " And then at decoding time, you can search for the most freight similar phrases in the database.", "tokens": [400, 550, 412, 979, 8616, 565, 11, 291, 393, 3164, 337, 264, 881, 37181, 2531, 20312, 294, 264, 8149, 13], "temperature": 0.0, "avg_logprob": -0.10821807815367918, "compression_ratio": 1.8071065989847717, "no_speech_prob": 3.8223759474931285e-05}, {"id": 354, "seek": 167600, "start": 1693.0, "end": 1699.0, "text": " And so what you do is that, you know, you take the representation of the context that you have from your model.", "tokens": [400, 370, 437, 291, 360, 307, 300, 11, 291, 458, 11, 291, 747, 264, 10290, 295, 264, 4319, 300, 291, 362, 490, 428, 2316, 13], "temperature": 0.0, "avg_logprob": -0.10821807815367918, "compression_ratio": 1.8071065989847717, "no_speech_prob": 3.8223759474931285e-05}, {"id": 355, "seek": 169900, "start": 1699.0, "end": 1710.0, "text": " And then compute a similarity function with all the representations of the phrases that you have stored. And you know, based off the relative differences amongst these different phrases to your current context.", "tokens": [400, 550, 14722, 257, 32194, 2445, 365, 439, 264, 33358, 295, 264, 20312, 300, 291, 362, 12187, 13, 400, 291, 458, 11, 2361, 766, 264, 4972, 7300, 12918, 613, 819, 20312, 281, 428, 2190, 4319, 13], "temperature": 0.0, "avg_logprob": -0.12260504924889767, "compression_ratio": 1.7474226804123711, "no_speech_prob": 2.111143658112269e-05}, {"id": 356, "seek": 169900, "start": 1710.0, "end": 1719.0, "text": " You can compute a distribution over these most similar phrases. And then you can take the next tokens that follow these phrases.", "tokens": [509, 393, 14722, 257, 7316, 670, 613, 881, 2531, 20312, 13, 400, 550, 291, 393, 747, 264, 958, 22667, 300, 1524, 613, 20312, 13], "temperature": 0.0, "avg_logprob": -0.12260504924889767, "compression_ratio": 1.7474226804123711, "no_speech_prob": 2.111143658112269e-05}, {"id": 357, "seek": 171900, "start": 1719.0, "end": 1742.0, "text": " And then you can add the statistics around those phrases to the distribution from your model. And so this allows you to really rebalance the probability distribution that your model has given you with this this this induced distribution over phrases and interpolate them together to get a different estimate of how likely certain words are.", "tokens": [400, 550, 291, 393, 909, 264, 12523, 926, 729, 20312, 281, 264, 7316, 490, 428, 2316, 13, 400, 370, 341, 4045, 291, 281, 534, 319, 29215, 264, 8482, 7316, 300, 428, 2316, 575, 2212, 291, 365, 341, 341, 341, 33991, 7316, 670, 20312, 293, 44902, 473, 552, 1214, 281, 483, 257, 819, 12539, 295, 577, 3700, 1629, 2283, 366, 13], "temperature": 0.0, "avg_logprob": -0.13140365481376648, "compression_ratio": 1.7435897435897436, "no_speech_prob": 3.763282438740134e-05}, {"id": 358, "seek": 174200, "start": 1742.0, "end": 1750.0, "text": " And then one question right now is, how do you know what to cage cash.", "tokens": [400, 550, 472, 1168, 558, 586, 307, 11, 577, 360, 291, 458, 437, 281, 17302, 6388, 13], "temperature": 0.0, "avg_logprob": -0.1999171776107595, "compression_ratio": 1.6310160427807487, "no_speech_prob": 0.00013759336434304714}, {"id": 359, "seek": 174200, "start": 1750.0, "end": 1754.0, "text": " Yeah, so that's a really good question.", "tokens": [865, 11, 370, 300, 311, 257, 534, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1999171776107595, "compression_ratio": 1.6310160427807487, "no_speech_prob": 0.00013759336434304714}, {"id": 360, "seek": 174200, "start": 1754.0, "end": 1763.0, "text": " I guess, you know, the answer there is that you could probably, you know, decide on what might be a salient set to phrases, depending on let's say named entities that you might be interested in.", "tokens": [286, 2041, 11, 291, 458, 11, 264, 1867, 456, 307, 300, 291, 727, 1391, 11, 291, 458, 11, 4536, 322, 437, 1062, 312, 257, 1845, 1196, 992, 281, 20312, 11, 5413, 322, 718, 311, 584, 4926, 16667, 300, 291, 1062, 312, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.1999171776107595, "compression_ratio": 1.6310160427807487, "no_speech_prob": 0.00013759336434304714}, {"id": 361, "seek": 176300, "start": 1763.0, "end": 1784.0, "text": " And then you know that you know that you know that your models distribution doesn't handle very well. But I'm pretty sure in this work, they took every phrase in their training corpus, cashed it and then relied on very efficient algorithms for doing this search over over over representation similarity to actually find the most likely once.", "tokens": [400, 550, 291, 458, 300, 291, 458, 300, 291, 458, 300, 428, 5245, 7316, 1177, 380, 4813, 588, 731, 13, 583, 286, 478, 1238, 988, 294, 341, 589, 11, 436, 1890, 633, 9535, 294, 641, 3097, 1181, 31624, 11, 6388, 292, 309, 293, 550, 35463, 322, 588, 7148, 14642, 337, 884, 341, 3164, 670, 670, 670, 10290, 32194, 281, 767, 915, 264, 881, 3700, 1564, 13], "temperature": 0.0, "avg_logprob": -0.2877584729875837, "compression_ratio": 1.6553398058252426, "no_speech_prob": 2.5068815375561826e-05}, {"id": 362, "seek": 178400, "start": 1784.0, "end": 1800.0, "text": " And then you did prune the number of phrases that they actually used to make this distribution, the phrase distributions that it wasn't over the entire corpus.", "tokens": [400, 550, 291, 630, 582, 2613, 264, 1230, 295, 20312, 300, 436, 767, 1143, 281, 652, 341, 7316, 11, 264, 9535, 37870, 300, 309, 2067, 380, 670, 264, 2302, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.17127479447258842, "compression_ratio": 1.394736842105263, "no_speech_prob": 2.0460531231947243e-05}, {"id": 363, "seek": 180000, "start": 1800.0, "end": 1821.0, "text": " So it's fantastic that we can now rebalance these distributions if we find that our model is is is doing poorly for you know, particularly you know this might be relevant if let's say we're jumping into a new domain. So we've trained a nice big generation model to on Wikipedia text and now we're going into something that's you know more.", "tokens": [407, 309, 311, 5456, 300, 321, 393, 586, 319, 29215, 613, 37870, 498, 321, 915, 300, 527, 2316, 307, 307, 307, 884, 22271, 337, 291, 458, 11, 4098, 291, 458, 341, 1062, 312, 7340, 498, 718, 311, 584, 321, 434, 11233, 666, 257, 777, 9274, 13, 407, 321, 600, 8895, 257, 1481, 955, 5125, 2316, 281, 322, 28999, 2487, 293, 586, 321, 434, 516, 666, 746, 300, 311, 291, 458, 544, 13], "temperature": 0.0, "avg_logprob": -0.1533633407793547, "compression_ratio": 1.6066350710900474, "no_speech_prob": 8.011000318219885e-06}, {"id": 364, "seek": 182100, "start": 1821.0, "end": 1838.0, "text": " More linked to stories, you know, we might want to use this type of system to get distributions from phrases from there. But you know it's also possible that we may not always have a good database of phrases to help us calibrate output distributions for all the types of text that we want to generate.", "tokens": [5048, 9408, 281, 3676, 11, 291, 458, 11, 321, 1062, 528, 281, 764, 341, 2010, 295, 1185, 281, 483, 37870, 490, 20312, 490, 456, 13, 583, 291, 458, 309, 311, 611, 1944, 300, 321, 815, 406, 1009, 362, 257, 665, 8149, 295, 20312, 281, 854, 505, 21583, 4404, 5598, 37870, 337, 439, 264, 3467, 295, 2487, 300, 321, 528, 281, 8460, 13], "temperature": 0.0, "avg_logprob": -0.09501120538422556, "compression_ratio": 1.627027027027027, "no_speech_prob": 4.908121263724752e-05}, {"id": 365, "seek": 183800, "start": 1838.0, "end": 1856.0, "text": " And so luckily last year there's there's also been you know new approaches that look at doing this in a gradient based way. And so the idea here is that you can actually define some type of external objective using a classifier that we typically call a discriminator.", "tokens": [400, 370, 22880, 1036, 1064, 456, 311, 456, 311, 611, 668, 291, 458, 777, 11587, 300, 574, 412, 884, 341, 294, 257, 16235, 2361, 636, 13, 400, 370, 264, 1558, 510, 307, 300, 291, 393, 767, 6964, 512, 2010, 295, 8320, 10024, 1228, 257, 1508, 9902, 300, 321, 5850, 818, 257, 20828, 1639, 13], "temperature": 0.0, "avg_logprob": -0.0894059312754664, "compression_ratio": 1.5170454545454546, "no_speech_prob": 8.939029612520244e-06}, {"id": 366, "seek": 185600, "start": 1856.0, "end": 1869.0, "text": " And in this figure from the paper that proposed it, they called an attribute model. And what that classifier does is that it's approximates some property that you'd like to encourage your text to exhibit as you decode.", "tokens": [400, 294, 341, 2573, 490, 264, 3035, 300, 10348, 309, 11, 436, 1219, 364, 19667, 2316, 13, 400, 437, 300, 1508, 9902, 775, 307, 300, 309, 311, 8542, 1024, 512, 4707, 300, 291, 1116, 411, 281, 5373, 428, 2487, 281, 20487, 382, 291, 979, 1429, 13], "temperature": 0.0, "avg_logprob": -0.10274215844961312, "compression_ratio": 1.646788990825688, "no_speech_prob": 5.738214895245619e-05}, {"id": 367, "seek": 185600, "start": 1869.0, "end": 1878.0, "text": " So perhaps it's a sentiment classifier because you are working on a dialogue model that and you want to encourage positive sending comments.", "tokens": [407, 4317, 309, 311, 257, 16149, 1508, 9902, 570, 291, 366, 1364, 322, 257, 10221, 2316, 300, 293, 291, 528, 281, 5373, 3353, 7750, 3053, 13], "temperature": 0.0, "avg_logprob": -0.10274215844961312, "compression_ratio": 1.646788990825688, "no_speech_prob": 5.738214895245619e-05}, {"id": 368, "seek": 187800, "start": 1878.0, "end": 1896.0, "text": " So as you generate text, you input the output of your text generation model to this attribute model. And there's some tricks on how you should do that in order to actually not have it be a discrete token that you provide to the model but instead a distribution over tokens.", "tokens": [407, 382, 291, 8460, 2487, 11, 291, 4846, 264, 5598, 295, 428, 2487, 5125, 2316, 281, 341, 19667, 2316, 13, 400, 456, 311, 512, 11733, 322, 577, 291, 820, 360, 300, 294, 1668, 281, 767, 406, 362, 309, 312, 257, 27706, 14862, 300, 291, 2893, 281, 264, 2316, 457, 2602, 257, 7316, 670, 22667, 13], "temperature": 0.0, "avg_logprob": -0.13862503181069585, "compression_ratio": 1.5964912280701755, "no_speech_prob": 5.6490789575036615e-05}, {"id": 369, "seek": 189600, "start": 1896.0, "end": 1914.0, "text": " And so the thing is that you know if you do this the right way by using the soft distribution of tokens as inputs to the attribute model, it's going to be able to compute a score for the sequence that it receives so that you know if it's a sentiment classifier, it can evaluate how positive of the sequence you provided to it.", "tokens": [400, 370, 264, 551, 307, 300, 291, 458, 498, 291, 360, 341, 264, 558, 636, 538, 1228, 264, 2787, 7316, 295, 22667, 382, 15743, 281, 264, 19667, 2316, 11, 309, 311, 516, 281, 312, 1075, 281, 14722, 257, 6175, 337, 264, 8310, 300, 309, 20717, 370, 300, 291, 458, 498, 309, 311, 257, 16149, 1508, 9902, 11, 309, 393, 13059, 577, 3353, 295, 264, 8310, 291, 5649, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.16677026490907412, "compression_ratio": 1.6717948717948719, "no_speech_prob": 2.5070208721444942e-05}, {"id": 370, "seek": 191400, "start": 1914.0, "end": 1926.0, "text": " So what you can do is that you can compute gradients with respect to this property and back propagate those gradients back to your text generation model directly.", "tokens": [407, 437, 291, 393, 360, 307, 300, 291, 393, 14722, 2771, 2448, 365, 3104, 281, 341, 4707, 293, 646, 48256, 729, 2771, 2448, 646, 281, 428, 2487, 5125, 2316, 3838, 13], "temperature": 0.0, "avg_logprob": -0.08471899032592774, "compression_ratio": 1.7829787234042553, "no_speech_prob": 1.4062729860597756e-05}, {"id": 371, "seek": 191400, "start": 1926.0, "end": 1939.0, "text": " And but instead of updating the parameters, which is what you would do during training, you instead update the intermediate activations at each layer of your model, which you can then forward propagate to compute a new distribution over the sets of tokens.", "tokens": [400, 457, 2602, 295, 25113, 264, 9834, 11, 597, 307, 437, 291, 576, 360, 1830, 3097, 11, 291, 2602, 5623, 264, 19376, 2430, 763, 412, 1184, 4583, 295, 428, 2316, 11, 597, 291, 393, 550, 2128, 48256, 281, 14722, 257, 777, 7316, 670, 264, 6352, 295, 22667, 13], "temperature": 0.0, "avg_logprob": -0.08471899032592774, "compression_ratio": 1.7829787234042553, "no_speech_prob": 1.4062729860597756e-05}, {"id": 372, "seek": 193900, "start": 1939.0, "end": 1957.0, "text": " It's a neat trick that allows you to do real time distribution updating based on some outside discriminator that's allowing you to update your internal representations of the sequence such that it hopefully generates something more positive at the output in this case.", "tokens": [467, 311, 257, 10654, 4282, 300, 4045, 291, 281, 360, 957, 565, 7316, 25113, 2361, 322, 512, 2380, 20828, 1639, 300, 311, 8293, 291, 281, 5623, 428, 6920, 33358, 295, 264, 8310, 1270, 300, 309, 4696, 23815, 746, 544, 3353, 412, 264, 5598, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.07977495006486482, "compression_ratio": 1.558139534883721, "no_speech_prob": 4.539061774266884e-05}, {"id": 373, "seek": 195700, "start": 1957.0, "end": 1971.0, "text": " So these distribution rebalancing methods either you know based off of nearest neighbor search or on using some type of discriminator are quite promising and interesting, but they also end up being quite computationally intensive.", "tokens": [407, 613, 7316, 319, 2645, 8779, 7150, 2139, 291, 458, 2361, 766, 295, 23831, 5987, 3164, 420, 322, 1228, 512, 2010, 295, 20828, 1639, 366, 1596, 20257, 293, 1880, 11, 457, 436, 611, 917, 493, 885, 1596, 24903, 379, 18957, 13], "temperature": 0.0, "avg_logprob": -0.1462101936340332, "compression_ratio": 1.4743589743589745, "no_speech_prob": 1.862828321463894e-05}, {"id": 374, "seek": 197100, "start": 1971.0, "end": 1985.0, "text": " In the first case, you're essentially doing a search over you know thousands of phrases to to rebalance your distribution. In the second, you're doing you know multiple forwards and backwards passes at every step to try and make the tokens exhibit a particular behavior more.", "tokens": [682, 264, 700, 1389, 11, 291, 434, 4476, 884, 257, 3164, 670, 291, 458, 5383, 295, 20312, 281, 281, 319, 29215, 428, 7316, 13, 682, 264, 1150, 11, 291, 434, 884, 291, 458, 3866, 30126, 293, 12204, 11335, 412, 633, 1823, 281, 853, 293, 652, 264, 22667, 20487, 257, 1729, 5223, 544, 13], "temperature": 0.0, "avg_logprob": -0.13991046704744037, "compression_ratio": 1.7454545454545454, "no_speech_prob": 2.014346137002576e-05}, {"id": 375, "seek": 197100, "start": 1985.0, "end": 1996.0, "text": " And unfortunately neither of them actually stop you from decoding bad sequences either it's possible, and even after you rebalance your distribution, you're still generating something that looks terrible.", "tokens": [400, 7015, 9662, 295, 552, 767, 1590, 291, 490, 979, 8616, 1578, 22978, 2139, 309, 311, 1944, 11, 293, 754, 934, 291, 319, 29215, 428, 7316, 11, 291, 434, 920, 17746, 746, 300, 1542, 6237, 13], "temperature": 0.0, "avg_logprob": -0.13991046704744037, "compression_ratio": 1.7454545454545454, "no_speech_prob": 2.014346137002576e-05}, {"id": 376, "seek": 199600, "start": 1996.0, "end": 2003.0, "text": " So in practice, something that we often use in text generation to improve our sequence outputs are what are called re-rankers.", "tokens": [407, 294, 3124, 11, 746, 300, 321, 2049, 764, 294, 2487, 5125, 281, 3470, 527, 8310, 23930, 366, 437, 366, 1219, 319, 12, 20479, 433, 13], "temperature": 0.0, "avg_logprob": -0.11122673814014722, "compression_ratio": 1.7191489361702128, "no_speech_prob": 9.027071064338088e-05}, {"id": 377, "seek": 199600, "start": 2003.0, "end": 2012.0, "text": " And so what we do here is that we actually decode multiple sequences, perhaps using sampling or a wider greedy search, say maybe 10.", "tokens": [400, 370, 437, 321, 360, 510, 307, 300, 321, 767, 979, 1429, 3866, 22978, 11, 4317, 1228, 21179, 420, 257, 11842, 28228, 3164, 11, 584, 1310, 1266, 13], "temperature": 0.0, "avg_logprob": -0.11122673814014722, "compression_ratio": 1.7191489361702128, "no_speech_prob": 9.027071064338088e-05}, {"id": 378, "seek": 199600, "start": 2012.0, "end": 2022.0, "text": " And then what we can do is that we can initialize a score to evaluate the sequences we produce and re-rank the sequences according to the score.", "tokens": [400, 550, 437, 321, 393, 360, 307, 300, 321, 393, 5883, 1125, 257, 6175, 281, 13059, 264, 22978, 321, 5258, 293, 319, 12, 20479, 264, 22978, 4650, 281, 264, 6175, 13], "temperature": 0.0, "avg_logprob": -0.11122673814014722, "compression_ratio": 1.7191489361702128, "no_speech_prob": 9.027071064338088e-05}, {"id": 379, "seek": 202200, "start": 2022.0, "end": 2040.0, "text": " And the simplest thing we can do is to actually just score them by the likelihood given given by the model for example, you know especially if we're using a sampling algorithm, we might want to you know make sure that we didn't generate something that you know totally deviated from good text, which would tend to have a very high perplexity.", "tokens": [400, 264, 22811, 551, 321, 393, 360, 307, 281, 767, 445, 6175, 552, 538, 264, 22119, 2212, 2212, 538, 264, 2316, 337, 1365, 11, 291, 458, 2318, 498, 321, 434, 1228, 257, 21179, 9284, 11, 321, 1062, 528, 281, 291, 458, 652, 988, 300, 321, 994, 380, 8460, 746, 300, 291, 458, 3879, 31219, 770, 490, 665, 2487, 11, 597, 576, 3928, 281, 362, 257, 588, 1090, 680, 18945, 507, 13], "temperature": 0.0, "avg_logprob": -0.14000199635823568, "compression_ratio": 1.6132075471698113, "no_speech_prob": 0.00010888228280236945}, {"id": 380, "seek": 204000, "start": 2040.0, "end": 2052.0, "text": " So you know this perplexity is perhaps a good re-ranking function. It's just important to be careful as well that you remember repetitive sequences tend to have very low perplexity as well.", "tokens": [407, 291, 458, 341, 680, 18945, 507, 307, 4317, 257, 665, 319, 12, 20479, 278, 2445, 13, 467, 311, 445, 1021, 281, 312, 5026, 382, 731, 300, 291, 1604, 29404, 22978, 3928, 281, 362, 588, 2295, 680, 18945, 507, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.1118580335146421, "compression_ratio": 1.5693069306930694, "no_speech_prob": 5.9203644923400134e-05}, {"id": 381, "seek": 204000, "start": 2052.0, "end": 2059.0, "text": " And so if you you know rank by perplexity, you're likely to just generate something you were trying to avoid in the first case.", "tokens": [400, 370, 498, 291, 291, 458, 6181, 538, 680, 18945, 507, 11, 291, 434, 3700, 281, 445, 8460, 746, 291, 645, 1382, 281, 5042, 294, 264, 700, 1389, 13], "temperature": 0.0, "avg_logprob": -0.1118580335146421, "compression_ratio": 1.5693069306930694, "no_speech_prob": 5.9203644923400134e-05}, {"id": 382, "seek": 205900, "start": 2059.0, "end": 2081.0, "text": " But you know we can also make our re-ranker is evaluate more complex behaviors. So in the same way that we could use gradient based methods to update our distributions to exhibit more complex behaviors, we can actually just you know take those same attribute models and use them as re-rankers to re-rank a fixed set of sequences rather than have them back propagate gradients to the main model.", "tokens": [583, 291, 458, 321, 393, 611, 652, 527, 319, 12, 20479, 260, 307, 13059, 544, 3997, 15501, 13, 407, 294, 264, 912, 636, 300, 321, 727, 764, 16235, 2361, 7150, 281, 5623, 527, 37870, 281, 20487, 544, 3997, 15501, 11, 321, 393, 767, 445, 291, 458, 747, 729, 912, 19667, 5245, 293, 764, 552, 382, 319, 12, 20479, 433, 281, 319, 12, 20479, 257, 6806, 992, 295, 22978, 2831, 813, 362, 552, 646, 48256, 2771, 2448, 281, 264, 2135, 2316, 13], "temperature": 0.0, "avg_logprob": -0.09587458442239201, "compression_ratio": 1.7433628318584071, "no_speech_prob": 7.030562846921384e-05}, {"id": 383, "seek": 208100, "start": 2081.0, "end": 2104.0, "text": " And so we can use them to rank things such as style, discourse, factuality, logical consistency, you know, but you know just be careful if your re-ranker ends up being poorly calculated, you know just because you've trained a classifier to predict whether a sentence makes a factual statement doesn't actually mean that it will be good at ranking different factual statements with respect to one another.", "tokens": [400, 370, 321, 393, 764, 552, 281, 6181, 721, 1270, 382, 3758, 11, 23938, 11, 48029, 507, 11, 14978, 14416, 11, 291, 458, 11, 457, 291, 458, 445, 312, 5026, 498, 428, 319, 12, 20479, 260, 5314, 493, 885, 22271, 15598, 11, 291, 458, 445, 570, 291, 600, 8895, 257, 1508, 9902, 281, 6069, 1968, 257, 8174, 1669, 257, 48029, 5629, 1177, 380, 767, 914, 300, 309, 486, 312, 665, 412, 17833, 819, 48029, 12363, 365, 3104, 281, 472, 1071, 13], "temperature": 0.0, "avg_logprob": -0.09520343331729665, "compression_ratio": 1.6833333333333333, "no_speech_prob": 3.590984488255344e-05}, {"id": 384, "seek": 210400, "start": 2104.0, "end": 2122.0, "text": " And finally a nice thing about re-rankers as well is that you can use multiple re-rankers in parallel. There's multiple properties you want to score and come up with let's say a weighted average of different ranking scores to decide on what might be the best sequence according to different properties.", "tokens": [400, 2721, 257, 1481, 551, 466, 319, 12, 20479, 433, 382, 731, 307, 300, 291, 393, 764, 3866, 319, 12, 20479, 433, 294, 8952, 13, 821, 311, 3866, 7221, 291, 528, 281, 6175, 293, 808, 493, 365, 718, 311, 584, 257, 32807, 4274, 295, 819, 17833, 13444, 281, 4536, 322, 437, 1062, 312, 264, 1151, 8310, 4650, 281, 819, 7221, 13], "temperature": 0.0, "avg_logprob": -0.07788735169630784, "compression_ratio": 1.6063829787234043, "no_speech_prob": 5.4752574214944616e-05}, {"id": 385, "seek": 212200, "start": 2122.0, "end": 2134.0, "text": " But so you know to recap what we've talked about in terms of decoding, you know I just I want to mention that decoding is still a very challenging problem in natural language generation that we haven't really figured out yet.", "tokens": [583, 370, 291, 458, 281, 20928, 437, 321, 600, 2825, 466, 294, 2115, 295, 979, 8616, 11, 291, 458, 286, 445, 286, 528, 281, 2152, 300, 979, 8616, 307, 920, 257, 588, 7595, 1154, 294, 3303, 2856, 5125, 300, 321, 2378, 380, 534, 8932, 484, 1939, 13], "temperature": 0.0, "avg_logprob": -0.08148334540572821, "compression_ratio": 1.6733333333333333, "no_speech_prob": 5.2243933168938383e-05}, {"id": 386, "seek": 212200, "start": 2134.0, "end": 2139.0, "text": " Our algorithms still don't you know really reflect the way that humans choose words when they speak.", "tokens": [2621, 14642, 920, 500, 380, 291, 458, 534, 5031, 264, 636, 300, 6255, 2826, 2283, 562, 436, 1710, 13], "temperature": 0.0, "avg_logprob": -0.08148334540572821, "compression_ratio": 1.6733333333333333, "no_speech_prob": 5.2243933168938383e-05}, {"id": 387, "seek": 212200, "start": 2139.0, "end": 2149.0, "text": " And our best approaches you know are currently based on trying to calibrate probability distributions produced by models to perhaps be more representative of human likelihood.", "tokens": [400, 527, 1151, 11587, 291, 458, 366, 4362, 2361, 322, 1382, 281, 21583, 4404, 8482, 37870, 7126, 538, 5245, 281, 4317, 312, 544, 12424, 295, 1952, 22119, 13], "temperature": 0.0, "avg_logprob": -0.08148334540572821, "compression_ratio": 1.6733333333333333, "no_speech_prob": 5.2243933168938383e-05}, {"id": 388, "seek": 214900, "start": 2149.0, "end": 2162.0, "text": " But the truth is you know the human language distribution is it's quite noisy and doesn't reflect the simple properties that are decoding algorithms often capture such as probability maximization.", "tokens": [583, 264, 3494, 307, 291, 458, 264, 1952, 2856, 7316, 307, 309, 311, 1596, 24518, 293, 1177, 380, 5031, 264, 2199, 7221, 300, 366, 979, 8616, 14642, 2049, 7983, 1270, 382, 8482, 5138, 2144, 13], "temperature": 0.0, "avg_logprob": -0.12563806680532602, "compression_ratio": 1.6698564593301435, "no_speech_prob": 9.46045620366931e-05}, {"id": 389, "seek": 214900, "start": 2162.0, "end": 2170.0, "text": " But different decoding algorithms do allow us to perhaps inject biases that encourage different properties of going here at natural language generation.", "tokens": [583, 819, 979, 8616, 14642, 360, 2089, 505, 281, 4317, 10711, 32152, 300, 5373, 819, 7221, 295, 516, 510, 412, 3303, 2856, 5125, 13], "temperature": 0.0, "avg_logprob": -0.12563806680532602, "compression_ratio": 1.6698564593301435, "no_speech_prob": 9.46045620366931e-05}, {"id": 390, "seek": 217000, "start": 2170.0, "end": 2183.0, "text": " And that's allowed us to make promising improvements in this area as well. And in fact some of the most impactful advances in an algae over the last few years have really come from simple but very effective modifications to decoding algorithms.", "tokens": [400, 300, 311, 4350, 505, 281, 652, 20257, 13797, 294, 341, 1859, 382, 731, 13, 400, 294, 1186, 512, 295, 264, 881, 30842, 25297, 294, 364, 32658, 670, 264, 1036, 1326, 924, 362, 534, 808, 490, 2199, 457, 588, 4942, 26881, 281, 979, 8616, 14642, 13], "temperature": 0.0, "avg_logprob": -0.09757201295149953, "compression_ratio": 1.6425339366515836, "no_speech_prob": 4.832162449019961e-05}, {"id": 391, "seek": 217000, "start": 2183.0, "end": 2190.0, "text": " Because you can often have impact across a very large number of tasks by making a good change to a decoding algorithm.", "tokens": [1436, 291, 393, 2049, 362, 2712, 2108, 257, 588, 2416, 1230, 295, 9608, 538, 1455, 257, 665, 1319, 281, 257, 979, 8616, 9284, 13], "temperature": 0.0, "avg_logprob": -0.09757201295149953, "compression_ratio": 1.6425339366515836, "no_speech_prob": 4.832162449019961e-05}, {"id": 392, "seek": 219000, "start": 2190.0, "end": 2199.0, "text": " But really there's there's still a lot more work to be done in the space and hopefully you know many of you will be the ones to make these next breakthroughs.", "tokens": [583, 534, 456, 311, 456, 311, 920, 257, 688, 544, 589, 281, 312, 1096, 294, 264, 1901, 293, 4696, 291, 458, 867, 295, 291, 486, 312, 264, 2306, 281, 652, 613, 958, 22397, 82, 13], "temperature": 0.0, "avg_logprob": -0.15055952526274183, "compression_ratio": 1.661596958174905, "no_speech_prob": 4.6095079596852884e-05}, {"id": 393, "seek": 219000, "start": 2199.0, "end": 2202.0, "text": " I'm happy to take questions at this point if any of popped up.", "tokens": [286, 478, 2055, 281, 747, 1651, 412, 341, 935, 498, 604, 295, 21545, 493, 13], "temperature": 0.0, "avg_logprob": -0.15055952526274183, "compression_ratio": 1.661596958174905, "no_speech_prob": 4.6095079596852884e-05}, {"id": 394, "seek": 219000, "start": 2202.0, "end": 2209.0, "text": " Here's one question. How do you evaluate how do you tell whether rebalance distribution is better.", "tokens": [1692, 311, 472, 1168, 13, 1012, 360, 291, 13059, 577, 360, 291, 980, 1968, 319, 29215, 7316, 307, 1101, 13], "temperature": 0.0, "avg_logprob": -0.15055952526274183, "compression_ratio": 1.661596958174905, "no_speech_prob": 4.6095079596852884e-05}, {"id": 395, "seek": 219000, "start": 2209.0, "end": 2217.0, "text": " And if I editorialize from there I guess you're admitting on this slide that you can't just look at the probability.", "tokens": [400, 498, 286, 33412, 1125, 490, 456, 286, 2041, 291, 434, 44056, 322, 341, 4137, 300, 291, 393, 380, 445, 574, 412, 264, 8482, 13], "temperature": 0.0, "avg_logprob": -0.15055952526274183, "compression_ratio": 1.661596958174905, "no_speech_prob": 4.6095079596852884e-05}, {"id": 396, "seek": 221700, "start": 2217.0, "end": 2232.0, "text": " So yes you you can't just look at the probability. I mean there is a certain amount of trust that happens that if you don't trust that your ranker is giving you a better assessment of whether you're you've produced a quality piece of text.", "tokens": [407, 2086, 291, 291, 393, 380, 445, 574, 412, 264, 8482, 13, 286, 914, 456, 307, 257, 1629, 2372, 295, 3361, 300, 2314, 300, 498, 291, 500, 380, 3361, 300, 428, 6181, 260, 307, 2902, 291, 257, 1101, 9687, 295, 1968, 291, 434, 291, 600, 7126, 257, 3125, 2522, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.13343077366895015, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.00016860896721482277}, {"id": 397, "seek": 221700, "start": 2232.0, "end": 2242.0, "text": " Perhaps you shouldn't be using that re ranker in the first place and you know hopefully you've you've actually means tested that re ranker to actually show that it has that it improves the quality of text.", "tokens": [10517, 291, 4659, 380, 312, 1228, 300, 319, 6181, 260, 294, 264, 700, 1081, 293, 291, 458, 4696, 291, 600, 291, 600, 767, 1355, 8246, 300, 319, 6181, 260, 281, 767, 855, 300, 309, 575, 300, 309, 24771, 264, 3125, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.13343077366895015, "compression_ratio": 1.8089430894308942, "no_speech_prob": 0.00016860896721482277}, {"id": 398, "seek": 224200, "start": 2242.0, "end": 2247.0, "text": " And there's a lot more about how you can actually evaluate the quality of text later on.", "tokens": [400, 456, 311, 257, 688, 544, 466, 577, 291, 393, 767, 13059, 264, 3125, 295, 2487, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.1255323169300857, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.00012923782924190164}, {"id": 399, "seek": 224200, "start": 2247.0, "end": 2252.0, "text": " Though I should warn you ahead of time that the answers are not as.", "tokens": [10404, 286, 820, 12286, 291, 2286, 295, 565, 300, 264, 6338, 366, 406, 382, 13], "temperature": 0.0, "avg_logprob": -0.1255323169300857, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.00012923782924190164}, {"id": 400, "seek": 224200, "start": 2252.0, "end": 2261.0, "text": " Not as direct and complete as you might want them to be and in fact there's there's a lot of room for interpretation in how you would actually do that.", "tokens": [1726, 382, 2047, 293, 3566, 382, 291, 1062, 528, 552, 281, 312, 293, 294, 1186, 456, 311, 456, 311, 257, 688, 295, 1808, 337, 14174, 294, 577, 291, 576, 767, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.1255323169300857, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.00012923782924190164}, {"id": 401, "seek": 224200, "start": 2261.0, "end": 2271.0, "text": " Maybe you should go on about that later but I guess people are puzzled on that because there's another question that's asking.", "tokens": [2704, 291, 820, 352, 322, 466, 300, 1780, 457, 286, 2041, 561, 366, 18741, 1493, 322, 300, 570, 456, 311, 1071, 1168, 300, 311, 3365, 13], "temperature": 0.0, "avg_logprob": -0.1255323169300857, "compression_ratio": 1.7261904761904763, "no_speech_prob": 0.00012923782924190164}, {"id": 402, "seek": 227100, "start": 2271.0, "end": 2276.0, "text": " If you said you don't know how to make a model choose was like a human.", "tokens": [759, 291, 848, 291, 500, 380, 458, 577, 281, 652, 257, 2316, 2826, 390, 411, 257, 1952, 13], "temperature": 0.0, "avg_logprob": -0.2418993037679921, "compression_ratio": 1.403225806451613, "no_speech_prob": 8.882806287147105e-05}, {"id": 403, "seek": 227100, "start": 2276.0, "end": 2284.0, "text": " How do we model different humans from different background.", "tokens": [1012, 360, 321, 2316, 819, 6255, 490, 819, 3678, 13], "temperature": 0.0, "avg_logprob": -0.2418993037679921, "compression_ratio": 1.403225806451613, "no_speech_prob": 8.882806287147105e-05}, {"id": 404, "seek": 227100, "start": 2284.0, "end": 2289.0, "text": " Yeah that's that's a really good question.", "tokens": [865, 300, 311, 300, 311, 257, 534, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2418993037679921, "compression_ratio": 1.403225806451613, "no_speech_prob": 8.882806287147105e-05}, {"id": 405, "seek": 228900, "start": 2289.0, "end": 2308.0, "text": " The answer to that is that you could potentially try to you know do do kind of fine tuning on the language distribution of a particular human starting from let's say a pre train language model since you'll probably never have enough data to only use a single humans outputs.", "tokens": [440, 1867, 281, 300, 307, 300, 291, 727, 7263, 853, 281, 291, 458, 360, 360, 733, 295, 2489, 15164, 322, 264, 2856, 7316, 295, 257, 1729, 1952, 2891, 490, 718, 311, 584, 257, 659, 3847, 2856, 2316, 1670, 291, 603, 1391, 1128, 362, 1547, 1412, 281, 787, 764, 257, 2167, 6255, 23930, 13], "temperature": 0.0, "avg_logprob": -0.10243962940416838, "compression_ratio": 1.5307262569832403, "no_speech_prob": 6.501976895378903e-05}, {"id": 406, "seek": 230800, "start": 2308.0, "end": 2320.0, "text": " Or you could try to do some of these rebalancing methods that we've spoken about to perhaps use those to actually make your distributions approach a particular humans language distribution more closely.", "tokens": [1610, 291, 727, 853, 281, 360, 512, 295, 613, 319, 2645, 8779, 7150, 300, 321, 600, 10759, 466, 281, 4317, 764, 729, 281, 767, 652, 428, 37870, 3109, 257, 1729, 6255, 2856, 7316, 544, 8185, 13], "temperature": 0.0, "avg_logprob": -0.08409818013509114, "compression_ratio": 1.626865671641791, "no_speech_prob": 2.0781682906090282e-05}, {"id": 407, "seek": 230800, "start": 2320.0, "end": 2327.0, "text": " So in the gradient based methods that I described I could perhaps train a single language model only on my type of language.", "tokens": [407, 294, 264, 16235, 2361, 7150, 300, 286, 7619, 286, 727, 4317, 3847, 257, 2167, 2856, 2316, 787, 322, 452, 2010, 295, 2856, 13], "temperature": 0.0, "avg_logprob": -0.08409818013509114, "compression_ratio": 1.626865671641791, "no_speech_prob": 2.0781682906090282e-05}, {"id": 408, "seek": 232700, "start": 2327.0, "end": 2340.0, "text": " Even if I have a much larger one that's trained on a much larger corpus of language from different speakers but then try to make it so that my model ranks the outputs of the main model.", "tokens": [2754, 498, 286, 362, 257, 709, 4833, 472, 300, 311, 8895, 322, 257, 709, 4833, 1181, 31624, 295, 2856, 490, 819, 9518, 457, 550, 853, 281, 652, 309, 370, 300, 452, 2316, 21406, 264, 23930, 295, 264, 2135, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11088954485379733, "compression_ratio": 1.4507042253521127, "no_speech_prob": 8.3464416093193e-05}, {"id": 409, "seek": 232700, "start": 2340.0, "end": 2343.0, "text": " I've got a question.", "tokens": [286, 600, 658, 257, 1168, 13], "temperature": 0.0, "avg_logprob": -0.11088954485379733, "compression_ratio": 1.4507042253521127, "no_speech_prob": 8.3464416093193e-05}, {"id": 410, "seek": 234300, "start": 2343.0, "end": 2354.0, "text": " Nuclear sampling and top case sampling are really effective in practice and you made the argument that there are all these little tiny things with very little probability mass but it sums up to more probability mass.", "tokens": [42528, 21179, 293, 1192, 1389, 21179, 366, 534, 4942, 294, 3124, 293, 291, 1027, 264, 6770, 300, 456, 366, 439, 613, 707, 5870, 721, 365, 588, 707, 8482, 2758, 457, 309, 34499, 493, 281, 544, 8482, 2758, 13], "temperature": 0.0, "avg_logprob": -0.1220165491104126, "compression_ratio": 1.852017937219731, "no_speech_prob": 3.819858829956502e-05}, {"id": 411, "seek": 234300, "start": 2354.0, "end": 2366.0, "text": " But if it sums up to more probability mass than they actually should have under the real distribution of human language shouldn't our models have been trained to put less probability mass on them.", "tokens": [583, 498, 309, 34499, 493, 281, 544, 8482, 2758, 813, 436, 767, 820, 362, 833, 264, 957, 7316, 295, 1952, 2856, 4659, 380, 527, 5245, 362, 668, 8895, 281, 829, 1570, 8482, 2758, 322, 552, 13], "temperature": 0.0, "avg_logprob": -0.1220165491104126, "compression_ratio": 1.852017937219731, "no_speech_prob": 3.819858829956502e-05}, {"id": 412, "seek": 236600, "start": 2366.0, "end": 2379.0, "text": " So why don't we why aren't our language models better in that case like why do we have this issue if that's if they actually are getting more probability and they should.", "tokens": [407, 983, 500, 380, 321, 983, 3212, 380, 527, 2856, 5245, 1101, 294, 300, 1389, 411, 983, 360, 321, 362, 341, 2734, 498, 300, 311, 498, 436, 767, 366, 1242, 544, 8482, 293, 436, 820, 13], "temperature": 0.0, "avg_logprob": -0.18516140358120786, "compression_ratio": 1.4680851063829787, "no_speech_prob": 9.604999650036916e-05}, {"id": 413, "seek": 236600, "start": 2379.0, "end": 2383.0, "text": " Yeah, that's a really good question.", "tokens": [865, 11, 300, 311, 257, 534, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.18516140358120786, "compression_ratio": 1.4680851063829787, "no_speech_prob": 9.604999650036916e-05}, {"id": 414, "seek": 238300, "start": 2383.0, "end": 2400.0, "text": " I think the answer to that is that the way we train them which which I'll get to in a bit is really trying to is really trying to model the distribution of human language as it sees it in its training corpus and it's actually surprisingly effective at doing that.", "tokens": [286, 519, 264, 1867, 281, 300, 307, 300, 264, 636, 321, 3847, 552, 597, 597, 286, 603, 483, 281, 294, 257, 857, 307, 534, 1382, 281, 307, 534, 1382, 281, 2316, 264, 7316, 295, 1952, 2856, 382, 309, 8194, 309, 294, 1080, 3097, 1181, 31624, 293, 309, 311, 767, 17600, 4942, 412, 884, 300, 13], "temperature": 0.0, "avg_logprob": -0.11957087759244239, "compression_ratio": 1.6335403726708075, "no_speech_prob": 7.140034722397104e-05}, {"id": 415, "seek": 240000, "start": 2400.0, "end": 2420.0, "text": " But a but at the same time when we actually start to use these language models out of the box in the NLT task that we work with first of all there's there's always you know slight defiations in the distribution of text that we're actually trying to model for the task we're doing and what the large corpus we trained on was in the first place which can make these these decoding algorithms less effective.", "tokens": [583, 257, 457, 412, 264, 912, 565, 562, 321, 767, 722, 281, 764, 613, 2856, 5245, 484, 295, 264, 2424, 294, 264, 426, 43, 51, 5633, 300, 321, 589, 365, 700, 295, 439, 456, 311, 456, 311, 1009, 291, 458, 4036, 1060, 72, 763, 294, 264, 7316, 295, 2487, 300, 321, 434, 767, 1382, 281, 2316, 337, 264, 5633, 321, 434, 884, 293, 437, 264, 2416, 1181, 31624, 321, 8895, 322, 390, 294, 264, 700, 1081, 597, 393, 652, 613, 613, 979, 8616, 14642, 1570, 4942, 13], "temperature": 0.0, "avg_logprob": -0.15896737444531786, "compression_ratio": 1.7161016949152543, "no_speech_prob": 1.644021540414542e-05}, {"id": 416, "seek": 242000, "start": 2420.0, "end": 2446.0, "text": " And you know the second thing I would notice that even though these these decoding algorithms are quite effective in practice. They aren't doing what humans do when we speak we're at no point when a human speaks are we potentially trying to maximize the probability of a potential next token or randomly select a word amongst a set of tokens we ultimately have world models that drive how we select the tokens that we choose to say in order to make our points.", "tokens": [400, 291, 458, 264, 1150, 551, 286, 576, 3449, 300, 754, 1673, 613, 613, 979, 8616, 14642, 366, 1596, 4942, 294, 3124, 13, 814, 3212, 380, 884, 437, 6255, 360, 562, 321, 1710, 321, 434, 412, 572, 935, 562, 257, 1952, 10789, 366, 321, 7263, 1382, 281, 19874, 264, 8482, 295, 257, 3995, 958, 14862, 420, 16979, 3048, 257, 1349, 12918, 257, 992, 295, 22667, 321, 6284, 362, 1002, 5245, 300, 3332, 577, 321, 3048, 264, 22667, 300, 321, 2826, 281, 584, 294, 1668, 281, 652, 527, 2793, 13], "temperature": 0.0, "avg_logprob": -0.10150546412314138, "compression_ratio": 1.7037037037037037, "no_speech_prob": 8.479558164253831e-05}, {"id": 417, "seek": 244600, "start": 2446.0, "end": 2457.0, "text": " And that's very different from what we get in probabilistic language models now you know should we throw away probabilistic language models no because as you mentioned they end up working quite well.", "tokens": [400, 300, 311, 588, 819, 490, 437, 321, 483, 294, 31959, 3142, 2856, 5245, 586, 291, 458, 820, 321, 3507, 1314, 31959, 3142, 2856, 5245, 572, 570, 382, 291, 2835, 436, 917, 493, 1364, 1596, 731, 13], "temperature": 0.0, "avg_logprob": -0.09655778748648507, "compression_ratio": 1.7692307692307692, "no_speech_prob": 7.841550541343167e-05}, {"id": 418, "seek": 244600, "start": 2457.0, "end": 2470.0, "text": " But at some point we also do need to mitigate these differences between how humans end up speaking and how language models end up model language.", "tokens": [583, 412, 512, 935, 321, 611, 360, 643, 281, 27336, 613, 7300, 1296, 577, 6255, 917, 493, 4124, 293, 577, 2856, 5245, 917, 493, 2316, 2856, 13], "temperature": 0.0, "avg_logprob": -0.09655778748648507, "compression_ratio": 1.7692307692307692, "no_speech_prob": 7.841550541343167e-05}, {"id": 419, "seek": 247000, "start": 2470.0, "end": 2476.0, "text": " So you know now that now the john has primed as perfectly for what comes next.", "tokens": [407, 291, 458, 586, 300, 586, 264, 35097, 575, 2886, 292, 382, 6239, 337, 437, 1487, 958, 13], "temperature": 0.0, "avg_logprob": -0.12266522153801875, "compression_ratio": 1.9, "no_speech_prob": 7.71989653003402e-05}, {"id": 420, "seek": 247000, "start": 2476.0, "end": 2499.0, "text": " You know let's let's jump back into training these models because you know the pipeline you know that I framed earlier being you know you train your model then you choose your decoding algorithm depending on properties you're interested in is is is great but the truth is there's interactions between your decoding model and your training algorithm that you might want to be thinking about during training which is not really what we're doing right now.", "tokens": [509, 458, 718, 311, 718, 311, 3012, 646, 666, 3097, 613, 5245, 570, 291, 458, 264, 15517, 291, 458, 300, 286, 30420, 3071, 885, 291, 458, 291, 3847, 428, 2316, 550, 291, 2826, 428, 979, 8616, 9284, 5413, 322, 7221, 291, 434, 3102, 294, 307, 307, 307, 869, 457, 264, 3494, 307, 456, 311, 13280, 1296, 428, 979, 8616, 2316, 293, 428, 3097, 9284, 300, 291, 1062, 528, 281, 312, 1953, 466, 1830, 3097, 597, 307, 406, 534, 437, 321, 434, 884, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.12266522153801875, "compression_ratio": 1.9, "no_speech_prob": 7.71989653003402e-05}, {"id": 421, "seek": 249900, "start": 2499.0, "end": 2513.0, "text": " And so if you recall the training algorithm that we've proposed up to this point is one where we just try to minimize the negative log likelihood of the next token in a sequence given the preceding ones at every step.", "tokens": [400, 370, 498, 291, 9901, 264, 3097, 9284, 300, 321, 600, 10348, 493, 281, 341, 935, 307, 472, 689, 321, 445, 853, 281, 17522, 264, 3671, 3565, 22119, 295, 264, 958, 14862, 294, 257, 8310, 2212, 264, 16969, 278, 2306, 412, 633, 1823, 13], "temperature": 0.0, "avg_logprob": -0.11499035779167624, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.00017397831834387034}, {"id": 422, "seek": 249900, "start": 2513.0, "end": 2526.0, "text": " And you know as I mentioned this actually works pretty well for training auto regress the models of human language but it actually causes a few issues which which john hinted at.", "tokens": [400, 291, 458, 382, 286, 2835, 341, 767, 1985, 1238, 731, 337, 3097, 8399, 1121, 735, 264, 5245, 295, 1952, 2856, 457, 309, 767, 7700, 257, 1326, 2663, 597, 597, 35097, 12075, 292, 412, 13], "temperature": 0.0, "avg_logprob": -0.11499035779167624, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.00017397831834387034}, {"id": 423, "seek": 252600, "start": 2526.0, "end": 2539.0, "text": " So in the next few slides i'm going to talk a bit about these issues and then highlight some some training solutions to these problems that I found interesting or that I think are important from from the last few years.", "tokens": [407, 294, 264, 958, 1326, 9788, 741, 478, 516, 281, 751, 257, 857, 466, 613, 2663, 293, 550, 5078, 512, 512, 3097, 6547, 281, 613, 2740, 300, 286, 1352, 1880, 420, 300, 286, 519, 366, 1021, 490, 490, 264, 1036, 1326, 924, 13], "temperature": 0.0, "avg_logprob": -0.08418510384755591, "compression_ratio": 1.661764705882353, "no_speech_prob": 8.748729305807501e-05}, {"id": 424, "seek": 252600, "start": 2539.0, "end": 2546.0, "text": " So the first issue is actually one that I hinted at in the last section which is that training with maximum likelihood.", "tokens": [407, 264, 700, 2734, 307, 767, 472, 300, 286, 12075, 292, 412, 294, 264, 1036, 3541, 597, 307, 300, 3097, 365, 6674, 22119, 13], "temperature": 0.0, "avg_logprob": -0.08418510384755591, "compression_ratio": 1.661764705882353, "no_speech_prob": 8.748729305807501e-05}, {"id": 425, "seek": 254600, "start": 2546.0, "end": 2563.0, "text": " So I wanted to discourage textual diversity and I showed this sequence on a slide earlier as an example of greedy algorithms being prone to generating repetitive sequences which you know it's just about the worst form of diversity that you could that you could get.", "tokens": [407, 286, 1415, 281, 21497, 609, 2487, 901, 8811, 293, 286, 4712, 341, 8310, 322, 257, 4137, 3071, 382, 364, 1365, 295, 28228, 14642, 885, 25806, 281, 17746, 29404, 22978, 597, 291, 458, 309, 311, 445, 466, 264, 5855, 1254, 295, 8811, 300, 291, 727, 300, 291, 727, 483, 13], "temperature": 0.0, "avg_logprob": -0.2549978362189399, "compression_ratio": 1.5317919075144508, "no_speech_prob": 4.1983490518759936e-05}, {"id": 426, "seek": 256300, "start": 2563.0, "end": 2577.0, "text": " And so many algorithms are just trying to maximize the probability of the sequences that they produce so it can really only be prone to to repetitive and undiverse phrases if those phrases are scored highly by the model to begin with.", "tokens": [400, 370, 867, 14642, 366, 445, 1382, 281, 19874, 264, 8482, 295, 264, 22978, 300, 436, 5258, 370, 309, 393, 534, 787, 312, 25806, 281, 281, 29404, 293, 674, 5376, 20312, 498, 729, 20312, 366, 18139, 5405, 538, 264, 2316, 281, 1841, 365, 13], "temperature": 0.0, "avg_logprob": -0.09807334775510042, "compression_ratio": 1.7201492537313432, "no_speech_prob": 1.3629830391437281e-05}, {"id": 427, "seek": 256300, "start": 2577.0, "end": 2590.0, "text": " And that ends up being one of the issues with maximum likelihood training is that it tends to end up favoring generic expressions because those are the ones that are you know often the most likely in human language production.", "tokens": [400, 300, 5314, 493, 885, 472, 295, 264, 2663, 365, 6674, 22119, 3097, 307, 300, 309, 12258, 281, 917, 493, 2294, 278, 19577, 15277, 570, 729, 366, 264, 2306, 300, 366, 291, 458, 2049, 264, 881, 3700, 294, 1952, 2856, 4265, 13], "temperature": 0.0, "avg_logprob": -0.09807334775510042, "compression_ratio": 1.7201492537313432, "no_speech_prob": 1.3629830391437281e-05}, {"id": 428, "seek": 259000, "start": 2590.0, "end": 2604.0, "text": " As we all know and as I mentioned earlier human language production isn't about maximizing the likelihood of the words that we produce so even though we might produce generic phrases more often than engineering phrases that's not the goal that we're setting out with when we speak.", "tokens": [1018, 321, 439, 458, 293, 382, 286, 2835, 3071, 1952, 2856, 4265, 1943, 380, 466, 5138, 3319, 264, 22119, 295, 264, 2283, 300, 321, 5258, 370, 754, 1673, 321, 1062, 5258, 19577, 20312, 544, 2049, 813, 7043, 20312, 300, 311, 406, 264, 3387, 300, 321, 434, 3287, 484, 365, 562, 321, 1710, 13], "temperature": 0.0, "avg_logprob": -0.09167239245246439, "compression_ratio": 1.740484429065744, "no_speech_prob": 5.224669803283177e-05}, {"id": 429, "seek": 259000, "start": 2604.0, "end": 2617.0, "text": " There's a lot more to communication that really isn't synthesized by a training objective that tries to maximize you know probability over the human language that's being read so how can we end up mitigating this problem.", "tokens": [821, 311, 257, 688, 544, 281, 6101, 300, 534, 1943, 380, 26617, 1602, 538, 257, 3097, 10024, 300, 9898, 281, 19874, 291, 458, 8482, 670, 264, 1952, 2856, 300, 311, 885, 1401, 370, 577, 393, 321, 917, 493, 15699, 990, 341, 1154, 13], "temperature": 0.0, "avg_logprob": -0.09167239245246439, "compression_ratio": 1.740484429065744, "no_speech_prob": 5.224669803283177e-05}, {"id": 430, "seek": 261700, "start": 2617.0, "end": 2628.0, "text": " You know an interesting approach that I really like that came out last year was was actually you know proposed by by wellic it all which was called unlikely hood training.", "tokens": [509, 458, 364, 1880, 3109, 300, 286, 534, 411, 300, 1361, 484, 1036, 1064, 390, 390, 767, 291, 458, 10348, 538, 538, 731, 299, 309, 439, 597, 390, 1219, 17518, 13376, 3097, 13], "temperature": 0.0, "avg_logprob": -0.13522235084982479, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0001686071918811649}, {"id": 431, "seek": 261700, "start": 2628.0, "end": 2646.0, "text": " And so here what you do is that you actually discourage the production of particular tokens by the model in certain contexts and so it happens that this loss term here decreases as the probability of the why neg tokens decreases so for any token that you don't want to generate as the probability of generating that token goes down.", "tokens": [400, 370, 510, 437, 291, 360, 307, 300, 291, 767, 21497, 609, 264, 4265, 295, 1729, 22667, 538, 264, 2316, 294, 1629, 30628, 293, 370, 309, 2314, 300, 341, 4470, 1433, 510, 24108, 382, 264, 8482, 295, 264, 983, 2485, 22667, 24108, 370, 337, 604, 14862, 300, 291, 500, 380, 528, 281, 8460, 382, 264, 8482, 295, 17746, 300, 14862, 1709, 760, 13], "temperature": 0.0, "avg_logprob": -0.13522235084982479, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.0001686071918811649}, {"id": 432, "seek": 264600, "start": 2646.0, "end": 2654.0, "text": " So so so does the loss term which means that you're not actually updating the model as much for the to for this particular behavior.", "tokens": [407, 370, 370, 775, 264, 4470, 1433, 597, 1355, 300, 291, 434, 406, 767, 25113, 264, 2316, 382, 709, 337, 264, 281, 337, 341, 1729, 5223, 13], "temperature": 0.0, "avg_logprob": -0.0967495533136221, "compression_ratio": 1.872093023255814, "no_speech_prob": 7.96578242443502e-05}, {"id": 433, "seek": 264600, "start": 2654.0, "end": 2674.0, "text": " What's important though is that you still have your teacher forcing objective so what's going to happen is that the models going to learn to capture both the distribution of language from the training corbis which it needs to do to learn how to generate text but also it's going to learn how to not say particular words that you might not want it to.", "tokens": [708, 311, 1021, 1673, 307, 300, 291, 920, 362, 428, 5027, 19030, 10024, 370, 437, 311, 516, 281, 1051, 307, 300, 264, 5245, 516, 281, 1466, 281, 7983, 1293, 264, 7316, 295, 2856, 490, 264, 3097, 1181, 65, 271, 597, 309, 2203, 281, 360, 281, 1466, 577, 281, 8460, 2487, 457, 611, 309, 311, 516, 281, 1466, 577, 281, 406, 584, 1729, 2283, 300, 291, 1062, 406, 528, 309, 281, 13], "temperature": 0.0, "avg_logprob": -0.0967495533136221, "compression_ratio": 1.872093023255814, "no_speech_prob": 7.96578242443502e-05}, {"id": 434, "seek": 267400, "start": 2674.0, "end": 2700.0, "text": " And then what you can do is that you can set this list of words that you don't want the model to generate to actually be words that you've already generated before and so in essence you're you're teaching the model to not say the same things again and that's just naturally going to limit the amount of repetition that your model is going to be able to spit out and you're going to be able to generate more diverse texts as a result as well.", "tokens": [400, 550, 437, 291, 393, 360, 307, 300, 291, 393, 992, 341, 1329, 295, 2283, 300, 291, 500, 380, 528, 264, 2316, 281, 8460, 281, 767, 312, 2283, 300, 291, 600, 1217, 10833, 949, 293, 370, 294, 12801, 291, 434, 291, 434, 4571, 264, 2316, 281, 406, 584, 264, 912, 721, 797, 293, 300, 311, 445, 8195, 516, 281, 4948, 264, 2372, 295, 30432, 300, 428, 2316, 307, 516, 281, 312, 1075, 281, 22127, 484, 293, 291, 434, 516, 281, 312, 1075, 281, 8460, 544, 9521, 15765, 382, 257, 1874, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.06399676234451766, "compression_ratio": 1.9427312775330396, "no_speech_prob": 4.068926500622183e-05}, {"id": 435, "seek": 270000, "start": 2700.0, "end": 2718.0, "text": " But a second and very important issue that comes from training with with maximum likelihood is you know what we often call exposure bias, which is that the context that we train on to generate the next token are going to look different from the ones that we see a generation time.", "tokens": [583, 257, 1150, 293, 588, 1021, 2734, 300, 1487, 490, 3097, 365, 365, 6674, 22119, 307, 291, 458, 437, 321, 2049, 818, 10420, 12577, 11, 597, 307, 300, 264, 4319, 300, 321, 3847, 322, 281, 8460, 264, 958, 14862, 366, 516, 281, 574, 819, 490, 264, 2306, 300, 321, 536, 257, 5125, 565, 13], "temperature": 0.0, "avg_logprob": -0.09337596750971097, "compression_ratio": 1.577319587628866, "no_speech_prob": 9.759268868947402e-05}, {"id": 436, "seek": 270000, "start": 2718.0, "end": 2721.0, "text": " And so why might that be.", "tokens": [400, 370, 983, 1062, 300, 312, 13], "temperature": 0.0, "avg_logprob": -0.09337596750971097, "compression_ratio": 1.577319587628866, "no_speech_prob": 9.759268868947402e-05}, {"id": 437, "seek": 272100, "start": 2721.0, "end": 2731.0, "text": " The key to the challenge that happens during training is that we always get a token from a gold document or human text as it put it's the gold sequence as we call it.", "tokens": [440, 2141, 281, 264, 3430, 300, 2314, 1830, 3097, 307, 300, 321, 1009, 483, 257, 14862, 490, 257, 3821, 4166, 420, 1952, 2487, 382, 309, 829, 309, 311, 264, 3821, 8310, 382, 321, 818, 309, 13], "temperature": 0.0, "avg_logprob": -0.2348065125314813, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.00011233385157538578}, {"id": 438, "seek": 272100, "start": 2731.0, "end": 2744.0, "text": " But then during generation we're feeding our previously generated tokens back into the model as the input rather than these these teacher force tokens that are that are from the gold documents.", "tokens": [583, 550, 1830, 5125, 321, 434, 12919, 527, 8046, 10833, 22667, 646, 666, 264, 2316, 382, 264, 4846, 2831, 813, 613, 613, 5027, 3464, 22667, 300, 366, 300, 366, 490, 264, 3821, 8512, 13], "temperature": 0.0, "avg_logprob": -0.2348065125314813, "compression_ratio": 1.722488038277512, "no_speech_prob": 0.00011233385157538578}, {"id": 439, "seek": 274400, "start": 2744.0, "end": 2753.0, "text": " And so if tokens is actually quite affected by things like the distributions produced by our model and the decoding algorithm we use to get tokens.", "tokens": [400, 370, 498, 22667, 307, 767, 1596, 8028, 538, 721, 411, 264, 37870, 7126, 538, 527, 2316, 293, 264, 979, 8616, 9284, 321, 764, 281, 483, 22667, 13], "temperature": 0.0, "avg_logprob": -0.15193039537912392, "compression_ratio": 1.606837606837607, "no_speech_prob": 2.9769529646728188e-05}, {"id": 440, "seek": 274400, "start": 2753.0, "end": 2755.0, "text": " And so you know can this end up being a problem.", "tokens": [400, 370, 291, 458, 393, 341, 917, 493, 885, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.15193039537912392, "compression_ratio": 1.606837606837607, "no_speech_prob": 2.9769529646728188e-05}, {"id": 441, "seek": 274400, "start": 2755.0, "end": 2767.0, "text": " Well yes, because as we've seen before the types of text that our model generates are often you know not a very close approximation of human language patterns in the training set.", "tokens": [1042, 2086, 11, 570, 382, 321, 600, 1612, 949, 264, 3467, 295, 2487, 300, 527, 2316, 23815, 366, 2049, 291, 458, 406, 257, 588, 1998, 28023, 295, 1952, 2856, 8294, 294, 264, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.15193039537912392, "compression_ratio": 1.606837606837607, "no_speech_prob": 2.9769529646728188e-05}, {"id": 442, "seek": 276700, "start": 2767.0, "end": 2779.0, "text": " And so it could be an imbalance between the type of text that our model has learned to predict learn to predict and to expect to see and the type of text that it will see once we actually start decoding.", "tokens": [400, 370, 309, 727, 312, 364, 43007, 1296, 264, 2010, 295, 2487, 300, 527, 2316, 575, 3264, 281, 6069, 1466, 281, 6069, 293, 281, 2066, 281, 536, 293, 264, 2010, 295, 2487, 300, 309, 486, 536, 1564, 321, 767, 722, 979, 8616, 13], "temperature": 0.0, "avg_logprob": -0.1159042925448031, "compression_ratio": 1.8864468864468864, "no_speech_prob": 9.607829269953072e-05}, {"id": 443, "seek": 276700, "start": 2779.0, "end": 2794.0, "text": " And so once your model starts receiving its own inputs, which are going to deviate from the distribution of text to expect it's going to be very challenging for it to be able to generate coherent text going forward because it's not going to really know how to synthesize its own information that it's generated.", "tokens": [400, 370, 1564, 428, 2316, 3719, 10040, 1080, 1065, 15743, 11, 597, 366, 516, 281, 1905, 13024, 490, 264, 7316, 295, 2487, 281, 2066, 309, 311, 516, 281, 312, 588, 7595, 337, 309, 281, 312, 1075, 281, 8460, 36239, 2487, 516, 2128, 570, 309, 311, 406, 516, 281, 534, 458, 577, 281, 26617, 1125, 1080, 1065, 1589, 300, 309, 311, 10833, 13], "temperature": 0.0, "avg_logprob": -0.1159042925448031, "compression_ratio": 1.8864468864468864, "no_speech_prob": 9.607829269953072e-05}, {"id": 444, "seek": 279400, "start": 2794.0, "end": 2802.0, "text": " And so there's a variety of ways to try to counter this exposure bias issue and many more that that continue to come out.", "tokens": [400, 370, 456, 311, 257, 5673, 295, 2098, 281, 853, 281, 5682, 341, 10420, 12577, 2734, 293, 867, 544, 300, 300, 2354, 281, 808, 484, 13], "temperature": 0.0, "avg_logprob": -0.08952455742414607, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.00013762939488515258}, {"id": 445, "seek": 279400, "start": 2802.0, "end": 2811.0, "text": " And fortunately there's not really enough time to talk about all of them. So I've added slides to discuss two of them that are based on semi supervised learning here.", "tokens": [400, 25511, 456, 311, 406, 534, 1547, 565, 281, 751, 466, 439, 295, 552, 13, 407, 286, 600, 3869, 9788, 281, 2248, 732, 295, 552, 300, 366, 2361, 322, 12909, 46533, 2539, 510, 13], "temperature": 0.0, "avg_logprob": -0.08952455742414607, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.00013762939488515258}, {"id": 446, "seek": 279400, "start": 2811.0, "end": 2817.0, "text": " But I really want to focus more on two other approaches that I personally find very interesting.", "tokens": [583, 286, 534, 528, 281, 1879, 544, 322, 732, 661, 11587, 300, 286, 5665, 915, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.08952455742414607, "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.00013762939488515258}, {"id": 447, "seek": 281700, "start": 2817.0, "end": 2830.0, "text": " The first is called sequence rewriting. And so you know in this setting your model first learns to retrieve a sequence from an existing database of human written prototypes.", "tokens": [440, 700, 307, 1219, 8310, 319, 19868, 13, 400, 370, 291, 458, 294, 341, 3287, 428, 2316, 700, 27152, 281, 30254, 257, 8310, 490, 364, 6741, 8149, 295, 1952, 3720, 42197, 13], "temperature": 0.0, "avg_logprob": -0.0790967033022926, "compression_ratio": 1.6595744680851063, "no_speech_prob": 1.6440642866655253e-05}, {"id": 448, "seek": 281700, "start": 2830.0, "end": 2841.0, "text": " So it's kind of like our nearest neighbor decoders earlier when we cashed a bunch of phrases here you cash a bunch of sequences that might be similar to the one that you're supposed to produce for this new situation.", "tokens": [407, 309, 311, 733, 295, 411, 527, 23831, 5987, 979, 378, 433, 3071, 562, 321, 6388, 292, 257, 3840, 295, 20312, 510, 291, 6388, 257, 3840, 295, 22978, 300, 1062, 312, 2531, 281, 264, 472, 300, 291, 434, 3442, 281, 5258, 337, 341, 777, 2590, 13], "temperature": 0.0, "avg_logprob": -0.0790967033022926, "compression_ratio": 1.6595744680851063, "no_speech_prob": 1.6440642866655253e-05}, {"id": 449, "seek": 284100, "start": 2841.0, "end": 2859.0, "text": " And then what we do is that once we take this sequence and retrieve it, we learn to edit it by doing things like adding, removing or modifying tokens to more accurately reflect the context that we're actually given rather than the one that this original sequence was designed for in the first place.", "tokens": [400, 550, 437, 321, 360, 307, 300, 1564, 321, 747, 341, 8310, 293, 30254, 309, 11, 321, 1466, 281, 8129, 309, 538, 884, 721, 411, 5127, 11, 12720, 420, 42626, 22667, 281, 544, 20095, 5031, 264, 4319, 300, 321, 434, 767, 2212, 2831, 813, 264, 472, 300, 341, 3380, 8310, 390, 4761, 337, 294, 264, 700, 1081, 13], "temperature": 0.0, "avg_logprob": -0.06678406653865691, "compression_ratio": 1.582010582010582, "no_speech_prob": 4.5393033360596746e-05}, {"id": 450, "seek": 285900, "start": 2859.0, "end": 2879.0, "text": " And so we can still use an algorithm here that tries to maximize like the training, but because there's this sort of latent variable of retrieving the right prototype that's involved, it makes it less likely that are that our generated text ends up suffering from exposure bias, because you're already starting from something that looks more like a training sequence that you might have seen.", "tokens": [400, 370, 321, 393, 920, 764, 364, 9284, 510, 300, 9898, 281, 19874, 411, 264, 3097, 11, 457, 570, 456, 311, 341, 1333, 295, 48994, 7006, 295, 19817, 798, 264, 558, 19475, 300, 311, 3288, 11, 309, 1669, 309, 1570, 3700, 300, 366, 300, 527, 10833, 2487, 5314, 493, 7755, 490, 10420, 12577, 11, 570, 291, 434, 1217, 2891, 490, 746, 300, 1542, 544, 411, 257, 3097, 8310, 300, 291, 1062, 362, 1612, 13], "temperature": 0.0, "avg_logprob": -0.077789367773594, "compression_ratio": 1.6470588235294117, "no_speech_prob": 9.972271072911099e-06}, {"id": 451, "seek": 287900, "start": 2879.0, "end": 2897.0, "text": " Another general class of possibilities we can do is to let our model learn to generate text by learning from its own samples. And you know this naturally maps itself nicely to reinforcement learning, which is actually one of my favorite ways to learn how to generate text.", "tokens": [3996, 2674, 1508, 295, 12178, 321, 393, 360, 307, 281, 718, 527, 2316, 1466, 281, 8460, 2487, 538, 2539, 490, 1080, 1065, 10938, 13, 400, 291, 458, 341, 8195, 11317, 2564, 9594, 281, 29280, 2539, 11, 597, 307, 767, 472, 295, 452, 2954, 2098, 281, 1466, 577, 281, 8460, 2487, 13], "temperature": 0.0, "avg_logprob": -0.10947668769142845, "compression_ratio": 1.5632183908045978, "no_speech_prob": 5.063496428192593e-05}, {"id": 452, "seek": 289700, "start": 2897.0, "end": 2911.0, "text": " In this setting, you're going to cast your text generation model as a mark off decision process, where your state s is the models representation of the preceding context that you see your actions, a are the words that can be generated.", "tokens": [682, 341, 3287, 11, 291, 434, 516, 281, 4193, 428, 2487, 5125, 2316, 382, 257, 1491, 766, 3537, 1399, 11, 689, 428, 1785, 262, 307, 264, 5245, 10290, 295, 264, 16969, 278, 4319, 300, 291, 536, 428, 5909, 11, 257, 366, 264, 2283, 300, 393, 312, 10833, 13], "temperature": 0.0, "avg_logprob": -0.1510325769583384, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.5935029296088032e-05}, {"id": 453, "seek": 289700, "start": 2911.0, "end": 2917.0, "text": " Your policy is the decoder and your rewards are provided by some type of external score.", "tokens": [2260, 3897, 307, 264, 979, 19866, 293, 428, 17203, 366, 5649, 538, 512, 2010, 295, 8320, 6175, 13], "temperature": 0.0, "avg_logprob": -0.1510325769583384, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.5935029296088032e-05}, {"id": 454, "seek": 289700, "start": 2917.0, "end": 2926.0, "text": " And here you can learn many different behaviors for your text generation model by rewarding it when it exhibits those behaviors.", "tokens": [400, 510, 291, 393, 1466, 867, 819, 15501, 337, 428, 2487, 5125, 2316, 538, 20063, 309, 562, 309, 39205, 729, 15501, 13], "temperature": 0.0, "avg_logprob": -0.1510325769583384, "compression_ratio": 1.7976190476190477, "no_speech_prob": 1.5935029296088032e-05}, {"id": 455, "seek": 292600, "start": 2926.0, "end": 2939.0, "text": " So to kind of quickly join this framing with the perspective of the text generation models that we've seen so far, you're going to be taking actions by sampling words, hat, why from the distribution.", "tokens": [407, 281, 733, 295, 2661, 3917, 341, 28971, 365, 264, 4585, 295, 264, 2487, 5125, 5245, 300, 321, 600, 1612, 370, 1400, 11, 291, 434, 516, 281, 312, 1940, 5909, 538, 21179, 2283, 11, 2385, 11, 983, 490, 264, 7316, 13], "temperature": 0.0, "avg_logprob": -0.11925940764577765, "compression_ratio": 1.5812807881773399, "no_speech_prob": 4.3996591557515785e-05}, {"id": 456, "seek": 292600, "start": 2939.0, "end": 2945.0, "text": " And then you're going to feed them back into the input to get a new state, which is what we've been doing at every point.", "tokens": [400, 550, 291, 434, 516, 281, 3154, 552, 646, 666, 264, 4846, 281, 483, 257, 777, 1785, 11, 597, 307, 437, 321, 600, 668, 884, 412, 633, 935, 13], "temperature": 0.0, "avg_logprob": -0.11925940764577765, "compression_ratio": 1.5812807881773399, "no_speech_prob": 4.3996591557515785e-05}, {"id": 457, "seek": 294500, "start": 2945.0, "end": 2956.0, "text": " What's different though is that as you generate text, you're using some external reward function to compute rewards for each token that you generate. So you're rewarding every action that you take.", "tokens": [708, 311, 819, 1673, 307, 300, 382, 291, 8460, 2487, 11, 291, 434, 1228, 512, 8320, 7782, 2445, 281, 14722, 17203, 337, 1184, 14862, 300, 291, 8460, 13, 407, 291, 434, 20063, 633, 3069, 300, 291, 747, 13], "temperature": 0.0, "avg_logprob": -0.08397119746488683, "compression_ratio": 1.7929515418502202, "no_speech_prob": 3.4806686016963795e-05}, {"id": 458, "seek": 294500, "start": 2956.0, "end": 2969.0, "text": " And then you scale the sample loss on this on this particular token that you generate by by this reward, which is going to encourage the model to generate the sequence in similar context if the reward is high.", "tokens": [400, 550, 291, 4373, 264, 6889, 4470, 322, 341, 322, 341, 1729, 14862, 300, 291, 8460, 538, 538, 341, 7782, 11, 597, 307, 516, 281, 5373, 264, 2316, 281, 8460, 264, 8310, 294, 2531, 4319, 498, 264, 7782, 307, 1090, 13], "temperature": 0.0, "avg_logprob": -0.08397119746488683, "compression_ratio": 1.7929515418502202, "no_speech_prob": 3.4806686016963795e-05}, {"id": 459, "seek": 296900, "start": 2969.0, "end": 2982.0, "text": " So very clearly, you're minimizing the negative log likelihood of your sample token. So here it's not a gold token. Notice the hats that is on the Y expression in the reward function.", "tokens": [407, 588, 4448, 11, 291, 434, 46608, 264, 3671, 3565, 22119, 295, 428, 6889, 14862, 13, 407, 510, 309, 311, 406, 257, 3821, 14862, 13, 13428, 264, 20549, 300, 307, 322, 264, 398, 6114, 294, 264, 7782, 2445, 13], "temperature": 0.0, "avg_logprob": -0.12483118541205107, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.8855292839580216e-05}, {"id": 460, "seek": 296900, "start": 2982.0, "end": 2988.0, "text": " And then you're going to compute a reward for that token and scale this negative log likelihood by that reward.", "tokens": [400, 550, 291, 434, 516, 281, 14722, 257, 7782, 337, 300, 14862, 293, 4373, 341, 3671, 3565, 22119, 538, 300, 7782, 13], "temperature": 0.0, "avg_logprob": -0.12483118541205107, "compression_ratio": 1.6388888888888888, "no_speech_prob": 2.8855292839580216e-05}, {"id": 461, "seek": 298800, "start": 2988.0, "end": 2999.0, "text": " So if the reward is high, the model is going to be more likely to generate this same sequence in a similar context in the future, if the reward is low, it will be less likely.", "tokens": [407, 498, 264, 7782, 307, 1090, 11, 264, 2316, 307, 516, 281, 312, 544, 3700, 281, 8460, 341, 912, 8310, 294, 257, 2531, 4319, 294, 264, 2027, 11, 498, 264, 7782, 307, 2295, 11, 309, 486, 312, 1570, 3700, 13], "temperature": 0.0, "avg_logprob": -0.08717160888865025, "compression_ratio": 1.6633663366336633, "no_speech_prob": 2.931042035925202e-05}, {"id": 462, "seek": 298800, "start": 2999.0, "end": 3007.0, "text": " But this sort of brings up a natural question, you know, what can we actually use as a reward to encourage the behaviors we want in this text generation system.", "tokens": [583, 341, 1333, 295, 5607, 493, 257, 3303, 1168, 11, 291, 458, 11, 437, 393, 321, 767, 764, 382, 257, 7782, 281, 5373, 264, 15501, 321, 528, 294, 341, 2487, 5125, 1185, 13], "temperature": 0.0, "avg_logprob": -0.08717160888865025, "compression_ratio": 1.6633663366336633, "no_speech_prob": 2.931042035925202e-05}, {"id": 463, "seek": 300700, "start": 3007.0, "end": 3022.0, "text": " And then you can use the value of the text generation system to use as you design this your generation pipeline, a common practice in the early days of using RL for text generation was to set the reward to be the final evaluation metric that you were going to evaluate on.", "tokens": [400, 550, 291, 393, 764, 264, 2158, 295, 264, 2487, 5125, 1185, 281, 764, 382, 291, 1715, 341, 428, 5125, 15517, 11, 257, 2689, 3124, 294, 264, 2440, 1708, 295, 1228, 497, 43, 337, 2487, 5125, 390, 281, 992, 264, 7782, 281, 312, 264, 2572, 13344, 20678, 300, 291, 645, 516, 281, 13059, 322, 13], "temperature": 0.0, "avg_logprob": -0.3302922716327742, "compression_ratio": 1.8875968992248062, "no_speech_prob": 3.4266686270711944e-05}, {"id": 464, "seek": 300700, "start": 3022.0, "end": 3034.0, "text": " And so here instead of having a unique reward for each generated token at every time step, you would just take the final sequence score that you get and reward every token in the generated sequence with that value.", "tokens": [400, 370, 510, 2602, 295, 1419, 257, 3845, 7782, 337, 1184, 10833, 14862, 412, 633, 565, 1823, 11, 291, 576, 445, 747, 264, 2572, 8310, 6175, 300, 291, 483, 293, 7782, 633, 14862, 294, 264, 10833, 8310, 365, 300, 2158, 13], "temperature": 0.0, "avg_logprob": -0.3302922716327742, "compression_ratio": 1.8875968992248062, "no_speech_prob": 3.4266686270711944e-05}, {"id": 465, "seek": 303400, "start": 3034.0, "end": 3049.0, "text": " And so it's absolutely magical. You would set your evaluation metric as the reward and you'd end up learning to get more reward because that's what RL algorithms do, which in turn means that you were learning to generate sequences that do better on your evaluation metric.", "tokens": [400, 370, 309, 311, 3122, 12066, 13, 509, 576, 992, 428, 13344, 20678, 382, 264, 7782, 293, 291, 1116, 917, 493, 2539, 281, 483, 544, 7782, 570, 300, 311, 437, 497, 43, 14642, 360, 11, 597, 294, 1261, 1355, 300, 291, 645, 2539, 281, 8460, 22978, 300, 360, 1101, 322, 428, 13344, 20678, 13], "temperature": 0.0, "avg_logprob": -0.17445831567468778, "compression_ratio": 1.6019417475728155, "no_speech_prob": 6.813912477809936e-05}, {"id": 466, "seek": 303400, "start": 3049.0, "end": 3052.0, "text": " So an LG benchmark scores were shooting through the roof.", "tokens": [407, 364, 25449, 18927, 13444, 645, 5942, 807, 264, 8418, 13], "temperature": 0.0, "avg_logprob": -0.17445831567468778, "compression_ratio": 1.6019417475728155, "no_speech_prob": 6.813912477809936e-05}, {"id": 467, "seek": 305200, "start": 3052.0, "end": 3064.0, "text": " And so it's making real progress. But it was actually all a lie, you know, turns out as I'll talk about later evaluation metrics, particularly for text generation are just approximations.", "tokens": [400, 370, 309, 311, 1455, 957, 4205, 13, 583, 309, 390, 767, 439, 257, 4544, 11, 291, 458, 11, 4523, 484, 382, 286, 603, 751, 466, 1780, 13344, 16367, 11, 4098, 337, 2487, 5125, 366, 445, 8542, 763, 13], "temperature": 0.0, "avg_logprob": -0.2233764952507572, "compression_ratio": 1.5829145728643217, "no_speech_prob": 7.965959957800806e-05}, {"id": 468, "seek": 305200, "start": 3064.0, "end": 3072.0, "text": " And it's not always clear that optimizing to those approximations is going to lead to more and better coherent text generation.", "tokens": [400, 309, 311, 406, 1009, 1850, 300, 40425, 281, 729, 8542, 763, 307, 516, 281, 1477, 281, 544, 293, 1101, 36239, 2487, 5125, 13], "temperature": 0.0, "avg_logprob": -0.2233764952507572, "compression_ratio": 1.5829145728643217, "no_speech_prob": 7.965959957800806e-05}, {"id": 469, "seek": 307200, "start": 3072.0, "end": 3101.0, "text": " And so instead oftentimes what ends up happening is that it just learns to exploit the noise in the evaluation metric. And in fact, in their large work where they introduced Google's neural machine translation system in 2016, you know, Google researchers generally found that training machine translation models with RL and blue scores as rewards, didn't actually improve the translation quality at all, even if it did lead to higher blue scores.", "tokens": [400, 370, 2602, 18349, 437, 5314, 493, 2737, 307, 300, 309, 445, 27152, 281, 25924, 264, 5658, 294, 264, 13344, 20678, 13, 400, 294, 1186, 11, 294, 641, 2416, 589, 689, 436, 7268, 3329, 311, 18161, 3479, 12853, 1185, 294, 6549, 11, 291, 458, 11, 3329, 10309, 5101, 1352, 300, 3097, 3479, 12853, 5245, 365, 497, 43, 293, 3344, 13444, 382, 17203, 11, 994, 380, 767, 3470, 264, 12853, 3125, 412, 439, 11, 754, 498, 309, 630, 1477, 281, 2946, 3344, 13444, 13], "temperature": 0.0, "avg_logprob": -0.11975596416955707, "compression_ratio": 1.6457564575645756, "no_speech_prob": 2.7533336833585054e-05}, {"id": 470, "seek": 310100, "start": 3101.0, "end": 3110.0, "text": " But so designing your reward function is a very important problem in RL for actually learning the behavior that you want.", "tokens": [583, 370, 14685, 428, 7782, 2445, 307, 257, 588, 1021, 1154, 294, 497, 43, 337, 767, 2539, 264, 5223, 300, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.07793601523054407, "compression_ratio": 1.748091603053435, "no_speech_prob": 6.20373830315657e-05}, {"id": 471, "seek": 310100, "start": 3110.0, "end": 3130.0, "text": " And I've listed some cool work here on how you can actually learn to tie fairly complex behaviors to reward functions by actually initializing the scores that use as rewards as neural networks that you know get trained on an auxiliary task ahead of time, but can then be used to provide scores as rewards to the system that you produce.", "tokens": [400, 286, 600, 10052, 512, 1627, 589, 510, 322, 577, 291, 393, 767, 1466, 281, 7582, 6457, 3997, 15501, 281, 7782, 6828, 538, 767, 5883, 3319, 264, 13444, 300, 764, 382, 17203, 382, 18161, 9590, 300, 291, 458, 483, 8895, 322, 364, 43741, 5633, 2286, 295, 565, 11, 457, 393, 550, 312, 1143, 281, 2893, 13444, 382, 17203, 281, 264, 1185, 300, 291, 5258, 13], "temperature": 0.0, "avg_logprob": -0.07793601523054407, "compression_ratio": 1.748091603053435, "no_speech_prob": 6.20373830315657e-05}, {"id": 472, "seek": 313000, "start": 3130.0, "end": 3144.0, "text": " So to go back to our example earlier of trying to create a dialogue agent that is very positive and only says positive things, you could use a sentiment classifier to produce a reward for the sequences that you generate.", "tokens": [407, 281, 352, 646, 281, 527, 1365, 3071, 295, 1382, 281, 1884, 257, 10221, 9461, 300, 307, 588, 3353, 293, 787, 1619, 3353, 721, 11, 291, 727, 764, 257, 16149, 1508, 9902, 281, 5258, 257, 7782, 337, 264, 22978, 300, 291, 8460, 13], "temperature": 0.0, "avg_logprob": -0.08266208911764211, "compression_ratio": 1.467455621301775, "no_speech_prob": 7.367537182290107e-05}, {"id": 473, "seek": 313000, "start": 3144.0, "end": 3148.0, "text": " And so that's a lot of fun.", "tokens": [400, 370, 300, 311, 257, 688, 295, 1019, 13], "temperature": 0.0, "avg_logprob": -0.08266208911764211, "compression_ratio": 1.467455621301775, "no_speech_prob": 7.367537182290107e-05}, {"id": 474, "seek": 314800, "start": 3148.0, "end": 3160.0, "text": " And unfortunately, despite all the fun that you can have using RL to train text generation engines, there's a bit of a dark side to which is that reinforcement learning algorithms can be notoriously unstable.", "tokens": [400, 7015, 11, 7228, 439, 264, 1019, 300, 291, 393, 362, 1228, 497, 43, 281, 3847, 2487, 5125, 12982, 11, 456, 311, 257, 857, 295, 257, 2877, 1252, 281, 597, 307, 300, 29280, 2539, 14642, 393, 312, 46772, 8994, 23742, 13], "temperature": 0.0, "avg_logprob": -0.07947126413002992, "compression_ratio": 1.584070796460177, "no_speech_prob": 0.00010069532436318696}, {"id": 475, "seek": 314800, "start": 3160.0, "end": 3169.0, "text": " And so to get these text generation systems to learn with RL, you often have to be thorough in tuning different dials in your model setup accurately.", "tokens": [400, 370, 281, 483, 613, 2487, 5125, 3652, 281, 1466, 365, 497, 43, 11, 291, 2049, 362, 281, 312, 12934, 294, 15164, 819, 5502, 82, 294, 428, 2316, 8657, 20095, 13], "temperature": 0.0, "avg_logprob": -0.07947126413002992, "compression_ratio": 1.584070796460177, "no_speech_prob": 0.00010069532436318696}, {"id": 476, "seek": 316900, "start": 3169.0, "end": 3179.0, "text": " There's many of them, two of them that I think are worth mentioning are one that you always need to pre train with teacher forcing you generally can't train with reinforcement learning from scratch.", "tokens": [821, 311, 867, 295, 552, 11, 732, 295, 552, 300, 286, 519, 366, 3163, 18315, 366, 472, 300, 291, 1009, 643, 281, 659, 3847, 365, 5027, 19030, 291, 5101, 393, 380, 3847, 365, 29280, 2539, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.10017747657243596, "compression_ratio": 1.610655737704918, "no_speech_prob": 7.030043343547732e-05}, {"id": 477, "seek": 316900, "start": 3179.0, "end": 3184.0, "text": " And also you need to provide some type of baseline reward that your model should be achieving.", "tokens": [400, 611, 291, 643, 281, 2893, 512, 2010, 295, 20518, 7782, 300, 428, 2316, 820, 312, 19626, 13], "temperature": 0.0, "avg_logprob": -0.10017747657243596, "compression_ratio": 1.610655737704918, "no_speech_prob": 7.030043343547732e-05}, {"id": 478, "seek": 316900, "start": 3184.0, "end": 3190.0, "text": " So for example, blue score, which I described earlier is always a positive value, unless it's zero.", "tokens": [407, 337, 1365, 11, 3344, 6175, 11, 597, 286, 7619, 3071, 307, 1009, 257, 3353, 2158, 11, 5969, 309, 311, 4018, 13], "temperature": 0.0, "avg_logprob": -0.10017747657243596, "compression_ratio": 1.610655737704918, "no_speech_prob": 7.030043343547732e-05}, {"id": 479, "seek": 319000, "start": 3190.0, "end": 3200.0, "text": " But that means that if you use it alone as a reward, every single sequence that you sample ends up, you know, being encouraged in the future. So what you want to have is some type of baseline.", "tokens": [583, 300, 1355, 300, 498, 291, 764, 309, 3312, 382, 257, 7782, 11, 633, 2167, 8310, 300, 291, 6889, 5314, 493, 11, 291, 458, 11, 885, 14658, 294, 264, 2027, 13, 407, 437, 291, 528, 281, 362, 307, 512, 2010, 295, 20518, 13], "temperature": 0.0, "avg_logprob": -0.07148902189163935, "compression_ratio": 1.6355555555555557, "no_speech_prob": 2.3184031306300312e-05}, {"id": 480, "seek": 319000, "start": 3200.0, "end": 3210.0, "text": " That is an expectation of how much reward you should be getting, which can be subtracted from the reward that you actually get so you can discourage certain behaviors as well.", "tokens": [663, 307, 364, 14334, 295, 577, 709, 7782, 291, 820, 312, 1242, 11, 597, 393, 312, 16390, 292, 490, 264, 7782, 300, 291, 767, 483, 370, 291, 393, 21497, 609, 1629, 15501, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.07148902189163935, "compression_ratio": 1.6355555555555557, "no_speech_prob": 2.3184031306300312e-05}, {"id": 481, "seek": 321000, "start": 3210.0, "end": 3221.0, "text": " One last note about this is that neural networks are quite good at finding the easiest way to learn something. So, you know, if there's a way to exploit your reward function.", "tokens": [1485, 1036, 3637, 466, 341, 307, 300, 18161, 9590, 366, 1596, 665, 412, 5006, 264, 12889, 636, 281, 1466, 746, 13, 407, 11, 291, 458, 11, 498, 456, 311, 257, 636, 281, 25924, 428, 7782, 2445, 13], "temperature": 0.0, "avg_logprob": -0.09396834032876152, "compression_ratio": 1.8156996587030716, "no_speech_prob": 7.721051224507391e-05}, {"id": 482, "seek": 321000, "start": 3221.0, "end": 3226.0, "text": " It'll find a way to do it, particularly that's easier than learning the behavior that you want to learn.", "tokens": [467, 603, 915, 257, 636, 281, 360, 309, 11, 4098, 300, 311, 3571, 813, 2539, 264, 5223, 300, 291, 528, 281, 1466, 13], "temperature": 0.0, "avg_logprob": -0.09396834032876152, "compression_ratio": 1.8156996587030716, "no_speech_prob": 7.721051224507391e-05}, {"id": 483, "seek": 321000, "start": 3226.0, "end": 3239.0, "text": " So something to remember that's kind of important if you try to use reinforcement learning for text generation systems, particularly because there's such a large action action space for words that it can generate to try to accomplish certain behaviors.", "tokens": [407, 746, 281, 1604, 300, 311, 733, 295, 1021, 498, 291, 853, 281, 764, 29280, 2539, 337, 2487, 5125, 3652, 11, 4098, 570, 456, 311, 1270, 257, 2416, 3069, 3069, 1901, 337, 2283, 300, 309, 393, 8460, 281, 853, 281, 9021, 1629, 15501, 13], "temperature": 0.0, "avg_logprob": -0.09396834032876152, "compression_ratio": 1.8156996587030716, "no_speech_prob": 7.721051224507391e-05}, {"id": 484, "seek": 323900, "start": 3239.0, "end": 3252.0, "text": " So, you know, to end this section, I just want to start off by saying that in general, we still use teacher forcing as a as a primary means of learning to generate coherent text.", "tokens": [407, 11, 291, 458, 11, 281, 917, 341, 3541, 11, 286, 445, 528, 281, 722, 766, 538, 1566, 300, 294, 2674, 11, 321, 920, 764, 5027, 19030, 382, 257, 382, 257, 6194, 1355, 295, 2539, 281, 8460, 36239, 2487, 13], "temperature": 0.0, "avg_logprob": -0.10614513628410571, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.00019711906497832388}, {"id": 485, "seek": 323900, "start": 3252.0, "end": 3260.0, "text": " It has diversity issues, but it still lets us learn a model with with decent text generation abilities.", "tokens": [467, 575, 8811, 2663, 11, 457, 309, 920, 6653, 505, 1466, 257, 2316, 365, 365, 8681, 2487, 5125, 11582, 13], "temperature": 0.0, "avg_logprob": -0.10614513628410571, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.00019711906497832388}, {"id": 486, "seek": 326000, "start": 3260.0, "end": 3272.0, "text": " One thing I haven't focused too much on in this lecture is the type of model that you can use to actually generate text because they tend to be less universal and much more designed to very specific and tasks.", "tokens": [1485, 551, 286, 2378, 380, 5178, 886, 709, 322, 294, 341, 7991, 307, 264, 2010, 295, 2316, 300, 291, 393, 764, 281, 767, 8460, 2487, 570, 436, 3928, 281, 312, 1570, 11455, 293, 709, 544, 4761, 281, 588, 2685, 293, 9608, 13], "temperature": 0.0, "avg_logprob": -0.11799073470266241, "compression_ratio": 1.7100371747211895, "no_speech_prob": 7.141036621760577e-05}, {"id": 487, "seek": 326000, "start": 3272.0, "end": 3288.0, "text": " But in general, a common approach in an LG is to try to design a neural architecture that allows your model to be perhaps less sensitive to the problems of teacher forcing or to address them with additional loss terms that are perhaps tasks specific.", "tokens": [583, 294, 2674, 11, 257, 2689, 3109, 294, 364, 25449, 307, 281, 853, 281, 1715, 257, 18161, 9482, 300, 4045, 428, 2316, 281, 312, 4317, 1570, 9477, 281, 264, 2740, 295, 5027, 19030, 420, 281, 2985, 552, 365, 4497, 4470, 2115, 300, 366, 4317, 9608, 2685, 13], "temperature": 0.0, "avg_logprob": -0.11799073470266241, "compression_ratio": 1.7100371747211895, "no_speech_prob": 7.141036621760577e-05}, {"id": 488, "seek": 328800, "start": 3288.0, "end": 3294.0, "text": " Exposure bias, though, is a problem everywhere, pretty much regardless of your of your neural architecture.", "tokens": [21391, 7641, 12577, 11, 1673, 11, 307, 257, 1154, 5315, 11, 1238, 709, 10060, 295, 428, 295, 428, 18161, 9482, 13], "temperature": 0.0, "avg_logprob": -0.14107917053530913, "compression_ratio": 1.7597173144876326, "no_speech_prob": 0.00018229596025776118}, {"id": 489, "seek": 328800, "start": 3294.0, "end": 3315.0, "text": " And you know, to kind of mitigate it, you can either train your model to be more resistant to its own distribution changes through things like semi supervised learning, or you can change your your your pipeline, so that you're learning to make modifications to an existing sequence that you retrieve from your training set rather than trying to learn how to generate sequences from scratch.", "tokens": [400, 291, 458, 11, 281, 733, 295, 27336, 309, 11, 291, 393, 2139, 3847, 428, 2316, 281, 312, 544, 20383, 281, 1080, 1065, 7316, 2962, 807, 721, 411, 12909, 46533, 2539, 11, 420, 291, 393, 1319, 428, 428, 428, 15517, 11, 370, 300, 291, 434, 2539, 281, 652, 26881, 281, 364, 6741, 8310, 300, 291, 30254, 490, 428, 3097, 992, 2831, 813, 1382, 281, 1466, 577, 281, 8460, 22978, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.14107917053530913, "compression_ratio": 1.7597173144876326, "no_speech_prob": 0.00018229596025776118}, {"id": 490, "seek": 331500, "start": 3315.0, "end": 3327.0, "text": " And the caveat there is that, you know, as the type of text that you're generating gets longer and longer, doing this, this kind of retrieval and editing becomes just as challenging as generating from scratch in many cases.", "tokens": [400, 264, 43012, 456, 307, 300, 11, 291, 458, 11, 382, 264, 2010, 295, 2487, 300, 291, 434, 17746, 2170, 2854, 293, 2854, 11, 884, 341, 11, 341, 733, 295, 19817, 3337, 293, 10000, 3643, 445, 382, 7595, 382, 17746, 490, 8459, 294, 867, 3331, 13], "temperature": 0.0, "avg_logprob": -0.14636649025811088, "compression_ratio": 1.6336633663366336, "no_speech_prob": 4.198287570034154e-05}, {"id": 491, "seek": 331500, "start": 3327.0, "end": 3332.0, "text": " And finally, you can use reinforcement learning as another means of of of learning from your own examples.", "tokens": [400, 2721, 11, 291, 393, 764, 29280, 2539, 382, 1071, 1355, 295, 295, 295, 2539, 490, 428, 1065, 5110, 13], "temperature": 0.0, "avg_logprob": -0.14636649025811088, "compression_ratio": 1.6336633663366336, "no_speech_prob": 4.198287570034154e-05}, {"id": 492, "seek": 333200, "start": 3332.0, "end": 3346.0, "text": " And, you know, and in effect, you can also use it to encourage different behaviors than just likely with maximization. But this type of learning can end up being quite unstable and unless your reward is well shaped, the model can often learn to exploit it.", "tokens": [400, 11, 291, 458, 11, 293, 294, 1802, 11, 291, 393, 611, 764, 309, 281, 5373, 819, 15501, 813, 445, 3700, 365, 5138, 2144, 13, 583, 341, 2010, 295, 2539, 393, 917, 493, 885, 1596, 23742, 293, 5969, 428, 7782, 307, 731, 13475, 11, 264, 2316, 393, 2049, 1466, 281, 25924, 309, 13], "temperature": 0.0, "avg_logprob": -0.12844160146880568, "compression_ratio": 1.4628571428571429, "no_speech_prob": 1.643716313992627e-05}, {"id": 493, "seek": 334600, "start": 3346.0, "end": 3363.0, "text": " And then you can also use the same kind of training, you know, like the other. So online language simulators where you can train with our online, or are you only talking about training offline.", "tokens": [400, 550, 291, 393, 611, 764, 264, 912, 733, 295, 3097, 11, 291, 458, 11, 411, 264, 661, 13, 407, 2950, 2856, 1034, 39265, 689, 291, 393, 3847, 365, 527, 2950, 11, 420, 366, 291, 787, 1417, 466, 3097, 21857, 13], "temperature": 0.2, "avg_logprob": -0.8049086676703558, "compression_ratio": 1.4732824427480915, "no_speech_prob": 1.5439174603670835e-05}, {"id": 494, "seek": 336300, "start": 3363.0, "end": 3383.0, "text": " No, in this setting, we're generally talking about training offline. So you you train it all ahead of time and then you use your model as it's been trained the first time. Okay, yeah, so now this we've we finally reached the section that I hinted at earlier on it on a very important topic, evaluation.", "tokens": [883, 11, 294, 341, 3287, 11, 321, 434, 5101, 1417, 466, 3097, 21857, 13, 407, 291, 291, 3847, 309, 439, 2286, 295, 565, 293, 550, 291, 764, 428, 2316, 382, 309, 311, 668, 8895, 264, 700, 565, 13, 1033, 11, 1338, 11, 370, 586, 341, 321, 600, 321, 2721, 6488, 264, 3541, 300, 286, 12075, 292, 412, 3071, 322, 309, 322, 257, 588, 1021, 4829, 11, 13344, 13], "temperature": 0.0, "avg_logprob": -0.19753440221150717, "compression_ratio": 1.5487179487179488, "no_speech_prob": 1.7499372916063294e-05}, {"id": 495, "seek": 338300, "start": 3383.0, "end": 3398.0, "text": " And you know, to be honest is something that we should be thinking about before we even start designing a model or training algorithm or or decoding algorithm. It's, you know, how can we actually check that our method is is how can we actually go into a value that our method is even working.", "tokens": [400, 291, 458, 11, 281, 312, 3245, 307, 746, 300, 321, 820, 312, 1953, 466, 949, 321, 754, 722, 14685, 257, 2316, 420, 3097, 9284, 420, 420, 979, 8616, 9284, 13, 467, 311, 11, 291, 458, 11, 577, 393, 321, 767, 1520, 300, 527, 3170, 307, 307, 577, 393, 321, 767, 352, 666, 257, 2158, 300, 527, 3170, 307, 754, 1364, 13], "temperature": 0.0, "avg_logprob": -0.16967108755400687, "compression_ratio": 1.7380952380952381, "no_speech_prob": 9.024844621308148e-05}, {"id": 496, "seek": 339800, "start": 3398.0, "end": 3417.0, "text": " I want to talk a bit about three types of evaluation metrics. So first we'll talk about automatic e-val metrics, because generally you need to be able to rapidly prototype and diagnose failures in your model. So it's essential to be able to have this this quick feedback, even if it's very course and an automatic evaluation metrics.", "tokens": [286, 528, 281, 751, 257, 857, 466, 1045, 3467, 295, 13344, 16367, 13, 407, 700, 321, 603, 751, 466, 12509, 308, 12, 3337, 16367, 11, 570, 5101, 291, 643, 281, 312, 1075, 281, 12910, 19475, 293, 36238, 20774, 294, 428, 2316, 13, 407, 309, 311, 7115, 281, 312, 1075, 281, 362, 341, 341, 1702, 5824, 11, 754, 498, 309, 311, 588, 1164, 293, 364, 12509, 13344, 16367, 13], "temperature": 0.0, "avg_logprob": -0.1638702286614312, "compression_ratio": 1.6485148514851484, "no_speech_prob": 6.107991794124246e-05}, {"id": 497, "seek": 341700, "start": 3417.0, "end": 3429.0, "text": " And you know, we've traditionally use what I'm calling content overlap metrics, which focus on how much a sequence explicitly resembles another sequence, usually in terms of word or phrase matching.", "tokens": [400, 291, 458, 11, 321, 600, 19067, 764, 437, 286, 478, 5141, 2701, 19959, 16367, 11, 597, 1879, 322, 577, 709, 257, 8310, 20803, 34433, 1071, 8310, 11, 2673, 294, 2115, 295, 1349, 420, 9535, 14324, 13], "temperature": 0.0, "avg_logprob": -0.12593578710788633, "compression_ratio": 1.356164383561644, "no_speech_prob": 3.5910445149056613e-05}, {"id": 498, "seek": 342900, "start": 3429.0, "end": 3449.0, "text": " And so, lately there's also been a new automatic evaluations that are that are model based where we try to use advances and embeddings and neural models to define more implicit similarity measures between sequences. And finally, we'll talk a bit about human evaluations, which are kind of the gold standard of of evaluating text generation.", "tokens": [400, 370, 11, 12881, 456, 311, 611, 668, 257, 777, 12509, 43085, 300, 366, 300, 366, 2316, 2361, 689, 321, 853, 281, 764, 25297, 293, 12240, 29432, 293, 18161, 5245, 281, 6964, 544, 26947, 32194, 8000, 1296, 22978, 13, 400, 2721, 11, 321, 603, 751, 257, 857, 466, 1952, 43085, 11, 597, 366, 733, 295, 264, 3821, 3832, 295, 295, 27479, 2487, 5125, 13], "temperature": 0.2, "avg_logprob": -0.1878059190862319, "compression_ratio": 1.596244131455399, "no_speech_prob": 5.062041600467637e-05}, {"id": 499, "seek": 344900, "start": 3449.0, "end": 3463.0, "text": " And they also do have downsides that that we'll get to as well. And I just want to note that some of these slides here are actually repurposed from slides from us, each of you must who's a leading expert on an LG evaluation.", "tokens": [400, 436, 611, 360, 362, 21554, 1875, 300, 300, 321, 603, 483, 281, 382, 731, 13, 400, 286, 445, 528, 281, 3637, 300, 512, 295, 613, 9788, 510, 366, 767, 1085, 20130, 1744, 490, 9788, 490, 505, 11, 1184, 295, 291, 1633, 567, 311, 257, 5775, 5844, 322, 364, 25449, 13344, 13], "temperature": 0.0, "avg_logprob": -0.22982638222830637, "compression_ratio": 1.4736842105263157, "no_speech_prob": 1.4057931366551202e-05}, {"id": 500, "seek": 346300, "start": 3463.0, "end": 3487.0, "text": " But so let's jump in. So content overlap metrics generally compute an explicit similarity score between two sequences, the one that's been generated by your model and some gold standard reference sequence that was attached to the to the inputs that you had. So in other words, a sequence that you know was an appropriate generation for the inputs you had.", "tokens": [583, 370, 718, 311, 3012, 294, 13, 407, 2701, 19959, 16367, 5101, 14722, 364, 13691, 32194, 6175, 1296, 732, 22978, 11, 264, 472, 300, 311, 668, 10833, 538, 428, 2316, 293, 512, 3821, 3832, 6408, 8310, 300, 390, 8570, 281, 264, 281, 264, 15743, 300, 291, 632, 13, 407, 294, 661, 2283, 11, 257, 8310, 300, 291, 458, 390, 364, 6854, 5125, 337, 264, 15743, 291, 632, 13], "temperature": 0.0, "avg_logprob": -0.08829922808541192, "compression_ratio": 1.6824644549763033, "no_speech_prob": 1.473648262617644e-05}, {"id": 501, "seek": 348700, "start": 3487.0, "end": 3499.0, "text": " And these metrics are often a popular starting point because they're fast and very efficient, which is their main benefit, as long as you have a reference sequence to compare to you can compute these scores rapidly to get feedback.", "tokens": [400, 613, 16367, 366, 2049, 257, 3743, 2891, 935, 570, 436, 434, 2370, 293, 588, 7148, 11, 597, 307, 641, 2135, 5121, 11, 382, 938, 382, 291, 362, 257, 6408, 8310, 281, 6794, 281, 291, 393, 14722, 613, 13444, 12910, 281, 483, 5824, 13], "temperature": 0.0, "avg_logprob": -0.12242542066072162, "compression_ratio": 1.7545126353790614, "no_speech_prob": 7.842999912099913e-05}, {"id": 502, "seek": 348700, "start": 3499.0, "end": 3513.0, "text": " And I'm going to categorize them into two zones here. First, and gram overlap metrics that compute different functions of word and word like overlap and semantic overlap metrics, which involve more complex overlap functions based off semantic structures.", "tokens": [400, 286, 478, 516, 281, 19250, 1125, 552, 666, 732, 16025, 510, 13, 2386, 11, 293, 21353, 19959, 16367, 300, 14722, 819, 6828, 295, 1349, 293, 1349, 411, 19959, 293, 47982, 19959, 16367, 11, 597, 9494, 544, 3997, 19959, 6828, 2361, 766, 47982, 9227, 13], "temperature": 0.0, "avg_logprob": -0.12242542066072162, "compression_ratio": 1.7545126353790614, "no_speech_prob": 7.842999912099913e-05}, {"id": 503, "seek": 351300, "start": 3513.0, "end": 3526.0, "text": " Unfortunately, besides being fast and efficient, most and gram overlap metrics don't actually give you a great approximation of sequence quality a lot of times.", "tokens": [8590, 11, 11868, 885, 2370, 293, 7148, 11, 881, 293, 21353, 19959, 16367, 500, 380, 767, 976, 291, 257, 869, 28023, 295, 8310, 3125, 257, 688, 295, 1413, 13], "temperature": 0.0, "avg_logprob": -0.1185542597915187, "compression_ratio": 1.3008130081300813, "no_speech_prob": 8.749102562433109e-05}, {"id": 504, "seek": 352600, "start": 3526.0, "end": 3541.0, "text": " They're already not ideal for something like machine translation where there can be multiple ways to translate the same sequence with things like synonyms and they get progressively much worse for tasks that are more open ended than empty.", "tokens": [814, 434, 1217, 406, 7157, 337, 746, 411, 3479, 12853, 689, 456, 393, 312, 3866, 2098, 281, 13799, 264, 912, 8310, 365, 721, 411, 5451, 2526, 2592, 293, 436, 483, 46667, 709, 5324, 337, 9608, 300, 366, 544, 1269, 4590, 813, 6707, 13], "temperature": 0.0, "avg_logprob": -0.12842988332112631, "compression_ratio": 1.5991379310344827, "no_speech_prob": 3.218662823201157e-05}, {"id": 505, "seek": 352600, "start": 3541.0, "end": 3549.0, "text": " So for example, in summarization, you know, the longer output texts make it naturally harder to measure something using word match.", "tokens": [407, 337, 1365, 11, 294, 14611, 2144, 11, 291, 458, 11, 264, 2854, 5598, 15765, 652, 309, 8195, 6081, 281, 3481, 746, 1228, 1349, 2995, 13], "temperature": 0.0, "avg_logprob": -0.12842988332112631, "compression_ratio": 1.5991379310344827, "no_speech_prob": 3.218662823201157e-05}, {"id": 506, "seek": 354900, "start": 3549.0, "end": 3562.0, "text": " And it's something like dialogue. It's incredibly open ended. And in fact, you can have multiple responses to a particular utterance that you know mean the same thing, but don't use any common words.", "tokens": [400, 309, 311, 746, 411, 10221, 13, 467, 311, 6252, 1269, 4590, 13, 400, 294, 1186, 11, 291, 393, 362, 3866, 13019, 281, 257, 1729, 17567, 719, 300, 291, 458, 914, 264, 912, 551, 11, 457, 500, 380, 764, 604, 2689, 2283, 13], "temperature": 0.0, "avg_logprob": -0.17129433155059814, "compression_ratio": 1.7052631578947368, "no_speech_prob": 9.026033512782305e-05}, {"id": 507, "seek": 354900, "start": 3562.0, "end": 3578.0, "text": " And so we can illustrate this with a simple fairly contrived example where you know have a dialogue context utterance that asks a question such as, you know, are you going to Antoine's incredible CS 224 lecture, a completely unbiased reference text from, you know, derived from a human.", "tokens": [400, 370, 321, 393, 23221, 341, 365, 257, 2199, 6457, 660, 470, 937, 1365, 689, 291, 458, 362, 257, 10221, 4319, 17567, 719, 300, 8962, 257, 1168, 1270, 382, 11, 291, 458, 11, 366, 291, 516, 281, 5130, 44454, 311, 4651, 9460, 568, 7911, 7991, 11, 257, 2584, 517, 5614, 1937, 6408, 2487, 490, 11, 291, 458, 11, 18949, 490, 257, 1952, 13], "temperature": 0.0, "avg_logprob": -0.17129433155059814, "compression_ratio": 1.7052631578947368, "no_speech_prob": 9.026033512782305e-05}, {"id": 508, "seek": 357800, "start": 3578.0, "end": 3596.0, "text": " And then a dialogue agent that spits out different answers such as, yes, which gets a pretty high score on our n gram overlap metric or you know it, which already scores a lot lower despite depicting, you know, the exact same idea or up, which actually gets a score of zero.", "tokens": [400, 550, 257, 10221, 9461, 300, 637, 1208, 484, 819, 6338, 1270, 382, 11, 2086, 11, 597, 2170, 257, 1238, 1090, 6175, 322, 527, 297, 21353, 19959, 20678, 420, 291, 458, 309, 11, 597, 1217, 13444, 257, 688, 3126, 7228, 1367, 21490, 11, 291, 458, 11, 264, 1900, 912, 1558, 420, 493, 11, 597, 767, 2170, 257, 6175, 295, 4018, 13], "temperature": 0.0, "avg_logprob": -0.17027837313138522, "compression_ratio": 1.5657142857142856, "no_speech_prob": 9.459596913075075e-05}, {"id": 509, "seek": 359600, "start": 3596.0, "end": 3611.0, "text": " And meanwhile, a completely incorrect answer heck no gets the highest score out of all of them, which kind of points to the issue, you know, when you use n gram overlap measures. In a lot of applications, you're going to be missing the salient elements of what the generated sequence should capture.", "tokens": [400, 29252, 11, 257, 2584, 18424, 1867, 12872, 572, 2170, 264, 6343, 6175, 484, 295, 439, 295, 552, 11, 597, 733, 295, 2793, 281, 264, 2734, 11, 291, 458, 11, 562, 291, 764, 297, 21353, 19959, 8000, 13, 682, 257, 688, 295, 5821, 11, 291, 434, 516, 281, 312, 5361, 264, 1845, 1196, 4959, 295, 437, 264, 10833, 8310, 820, 7983, 13], "temperature": 0.0, "avg_logprob": -0.13064921033251417, "compression_ratio": 1.6313725490196078, "no_speech_prob": 2.8406288038240746e-05}, {"id": 510, "seek": 359600, "start": 3611.0, "end": 3619.0, "text": " And instead, you know, the getting stylistic similarities between text, even as you miss the most important context.", "tokens": [400, 2602, 11, 291, 458, 11, 264, 1242, 23736, 3142, 24197, 1296, 2487, 11, 754, 382, 291, 1713, 264, 881, 1021, 4319, 13], "temperature": 0.0, "avg_logprob": -0.13064921033251417, "compression_ratio": 1.6313725490196078, "no_speech_prob": 2.8406288038240746e-05}, {"id": 511, "seek": 361900, "start": 3619.0, "end": 3629.0, "text": " And if you prefer empirical validations to contrived examples, it's actually been shown that many dialogue evaluation metrics don't really correlate well with human judgments at all.", "tokens": [400, 498, 291, 4382, 31886, 7363, 763, 281, 660, 470, 937, 5110, 11, 309, 311, 767, 668, 4898, 300, 867, 10221, 13344, 16367, 500, 380, 534, 48742, 731, 365, 1952, 40337, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.09112702716480602, "compression_ratio": 1.5985130111524164, "no_speech_prob": 3.943539195461199e-05}, {"id": 512, "seek": 361900, "start": 3629.0, "end": 3646.0, "text": " And this gets worse as your sequence length increases typically. So with an open ended task like story generation, you can get improved scores by just matching a whole lot of stopboards that have nothing to do with the content of the story itself.", "tokens": [400, 341, 2170, 5324, 382, 428, 8310, 4641, 8637, 5850, 13, 407, 365, 364, 1269, 4590, 5633, 411, 1657, 5125, 11, 291, 393, 483, 9689, 13444, 538, 445, 14324, 257, 1379, 688, 295, 1590, 17228, 300, 362, 1825, 281, 360, 365, 264, 2701, 295, 264, 1657, 2564, 13], "temperature": 0.0, "avg_logprob": -0.09112702716480602, "compression_ratio": 1.5985130111524164, "no_speech_prob": 3.943539195461199e-05}, {"id": 513, "seek": 364600, "start": 3646.0, "end": 3655.0, "text": " And we also have another category of overlap metrics that I'll call semantic overlap metrics because they don't necessarily tied directly to the words you used.", "tokens": [400, 321, 611, 362, 1071, 7719, 295, 19959, 16367, 300, 286, 603, 818, 47982, 19959, 16367, 570, 436, 500, 380, 4725, 9601, 3838, 281, 264, 2283, 291, 1143, 13], "temperature": 0.0, "avg_logprob": -0.146910265872353, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.00011771802383009344}, {"id": 514, "seek": 364600, "start": 3655.0, "end": 3661.0, "text": " But instead try to create conceptual representations of the generated and reference outputs instead.", "tokens": [583, 2602, 853, 281, 1884, 24106, 33358, 295, 264, 10833, 293, 6408, 23930, 2602, 13], "temperature": 0.0, "avg_logprob": -0.146910265872353, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.00011771802383009344}, {"id": 515, "seek": 364600, "start": 3661.0, "end": 3675.0, "text": " So the center one here spice, for example, creates a scene graph of your of your generated text and then compares that to your to your reference caption to see how similar this this more semantic representation and stuff being.", "tokens": [407, 264, 3056, 472, 510, 19436, 11, 337, 1365, 11, 7829, 257, 4145, 4295, 295, 428, 295, 428, 10833, 2487, 293, 550, 38334, 300, 281, 428, 281, 428, 6408, 31974, 281, 536, 577, 2531, 341, 341, 544, 47982, 10290, 293, 1507, 885, 13], "temperature": 0.0, "avg_logprob": -0.146910265872353, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.00011771802383009344}, {"id": 516, "seek": 367500, "start": 3675.0, "end": 3687.0, "text": " But clearly there's there's there's some limitations to how well explicit content overlap metrics can do, particularly as we start thinking about more open and to task.", "tokens": [583, 4448, 456, 311, 456, 311, 456, 311, 512, 15705, 281, 577, 731, 13691, 2701, 19959, 16367, 393, 360, 11, 4098, 382, 321, 722, 1953, 466, 544, 1269, 293, 281, 5633, 13], "temperature": 0.0, "avg_logprob": -0.13566112518310547, "compression_ratio": 1.6473029045643153, "no_speech_prob": 4.908052505925298e-05}, {"id": 517, "seek": 367500, "start": 3687.0, "end": 3702.0, "text": " So in response of the last years, there's been a focus on using model based metrics, whose representations come from, you know, machine learning models and where they can be used actually evaluate the fidelity of generated text.", "tokens": [407, 294, 4134, 295, 264, 1036, 924, 11, 456, 311, 668, 257, 1879, 322, 1228, 2316, 2361, 16367, 11, 6104, 33358, 808, 490, 11, 291, 458, 11, 3479, 2539, 5245, 293, 689, 436, 393, 312, 1143, 767, 13059, 264, 46404, 295, 10833, 2487, 13], "temperature": 0.0, "avg_logprob": -0.13566112518310547, "compression_ratio": 1.6473029045643153, "no_speech_prob": 4.908052505925298e-05}, {"id": 518, "seek": 370200, "start": 3702.0, "end": 3716.0, "text": " And these are nice because there's no more needing explicit matches between words and your reference and generated text instead you can rely on much more implicit notions of similarity that you get from word embeddings.", "tokens": [400, 613, 366, 1481, 570, 456, 311, 572, 544, 18006, 13691, 10676, 1296, 2283, 293, 428, 6408, 293, 10833, 2487, 2602, 291, 393, 10687, 322, 709, 544, 26947, 35799, 295, 32194, 300, 291, 483, 490, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.07778838068939918, "compression_ratio": 1.489795918367347, "no_speech_prob": 6.921542080817744e-05}, {"id": 519, "seek": 371600, "start": 3716.0, "end": 3737.0, "text": " And so, you know, there's been a lot of models develop in this area, some of the original ones kind of focused on, you know, defining composition functions over the embeddings of words and your generated and reference sequences and then computing a distance between the compositions of the two sequences.", "tokens": [400, 370, 11, 291, 458, 11, 456, 311, 668, 257, 688, 295, 5245, 1499, 294, 341, 1859, 11, 512, 295, 264, 3380, 2306, 733, 295, 5178, 322, 11, 291, 458, 11, 17827, 12686, 6828, 670, 264, 12240, 29432, 295, 2283, 293, 428, 10833, 293, 6408, 22978, 293, 550, 15866, 257, 4560, 1296, 264, 43401, 295, 264, 732, 22978, 13], "temperature": 0.0, "avg_logprob": -0.10244148496597533, "compression_ratio": 1.6432432432432433, "no_speech_prob": 8.267239536507986e-06}, {"id": 520, "seek": 373700, "start": 3737.0, "end": 3752.0, "text": " Some more, you know, so some more involved takes on this idea are things like word movers distance where here you actually try to map word vectors in both your generated and reference sequences into pairs with one another.", "tokens": [2188, 544, 11, 291, 458, 11, 370, 512, 544, 3288, 2516, 322, 341, 1558, 366, 721, 411, 1349, 705, 840, 4560, 689, 510, 291, 767, 853, 281, 4471, 1349, 18875, 294, 1293, 428, 10833, 293, 6408, 22978, 666, 15494, 365, 472, 1071, 13], "temperature": 0.0, "avg_logprob": -0.16423249751963515, "compression_ratio": 1.48, "no_speech_prob": 4.90783822897356e-05}, {"id": 521, "seek": 375200, "start": 3752.0, "end": 3773.0, "text": " So each word vector is paired to another work vector in the opposite sequence and the distance between them is computed and then you allow the evaluation metric to actually compute the optimal matchings between these pairs of words such as the total distance is minimized and bird score, which has become quite popular over the last year.", "tokens": [407, 1184, 1349, 8062, 307, 25699, 281, 1071, 589, 8062, 294, 264, 6182, 8310, 293, 264, 4560, 1296, 552, 307, 40610, 293, 550, 291, 2089, 264, 13344, 20678, 281, 767, 14722, 264, 16252, 2995, 1109, 1296, 613, 15494, 295, 2283, 1270, 382, 264, 3217, 4560, 307, 4464, 1602, 293, 5255, 6175, 11, 597, 575, 1813, 1596, 3743, 670, 264, 1036, 1064, 13], "temperature": 0.0, "avg_logprob": -0.08812826330011542, "compression_ratio": 1.6984924623115578, "no_speech_prob": 1.3419062270259019e-05}, {"id": 522, "seek": 377300, "start": 3773.0, "end": 3784.0, "text": " So as an evaluation metric is pretty much just word movers distance but using contextualized bird embeddings to compute these distances.", "tokens": [407, 382, 364, 13344, 20678, 307, 1238, 709, 445, 1349, 705, 840, 4560, 457, 1228, 35526, 1602, 5255, 12240, 29432, 281, 14722, 613, 22182, 13], "temperature": 0.0, "avg_logprob": -0.13572871264289407, "compression_ratio": 1.7991803278688525, "no_speech_prob": 2.9309790988918394e-05}, {"id": 523, "seek": 377300, "start": 3784.0, "end": 3802.0, "text": " Sentence movers similarity is kind of another extension of word movers similarity that adds sentence embeddings from recurrent no networks to this distance optimization and that allows it to be more effective for evaluating let's say long multi sentence text as opposed to just single single sentences.", "tokens": [23652, 655, 705, 840, 32194, 307, 733, 295, 1071, 10320, 295, 1349, 705, 840, 32194, 300, 10860, 8174, 12240, 29432, 490, 18680, 1753, 572, 9590, 281, 341, 4560, 19618, 293, 300, 4045, 309, 281, 312, 544, 4942, 337, 27479, 718, 311, 584, 938, 4825, 8174, 2487, 382, 8851, 281, 445, 2167, 2167, 16579, 13], "temperature": 0.0, "avg_logprob": -0.13572871264289407, "compression_ratio": 1.7991803278688525, "no_speech_prob": 2.9309790988918394e-05}, {"id": 524, "seek": 380200, "start": 3802.0, "end": 3825.0, "text": " And finally last year there was a new model named blurt, which is actually a regression model that's based on birds and here it takes a pair of sentences, the reference on the generated one and returns a score that indicates to what extent the candidate is grammatical and conveys the meaning of the reference text.", "tokens": [400, 2721, 1036, 1064, 456, 390, 257, 777, 2316, 4926, 888, 6224, 11, 597, 307, 767, 257, 24590, 2316, 300, 311, 2361, 322, 9009, 293, 510, 309, 2516, 257, 6119, 295, 16579, 11, 264, 6408, 322, 264, 10833, 472, 293, 11247, 257, 6175, 300, 16203, 281, 437, 8396, 264, 11532, 307, 17570, 267, 804, 293, 18053, 749, 264, 3620, 295, 264, 6408, 2487, 13], "temperature": 0.0, "avg_logprob": -0.11549361313090604, "compression_ratio": 1.640625, "no_speech_prob": 7.367807847913355e-05}, {"id": 525, "seek": 382500, "start": 3825.0, "end": 3838.0, "text": " So you know we can we can talk about a lot more evaluation metrics that are computed automatically and you know there's there's far more of them than I then I actually mentioned though they do tend to fit in those two categories that I described.", "tokens": [407, 291, 458, 321, 393, 321, 393, 751, 466, 257, 688, 544, 13344, 16367, 300, 366, 40610, 6772, 293, 291, 458, 456, 311, 456, 311, 1400, 544, 295, 552, 813, 286, 550, 286, 767, 2835, 1673, 436, 360, 3928, 281, 3318, 294, 729, 732, 10479, 300, 286, 7619, 13], "temperature": 0.0, "avg_logprob": -0.11468745666800194, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0001177050216938369}, {"id": 526, "seek": 382500, "start": 3838.0, "end": 3848.0, "text": " But it's it's important to remember that at the end of the day the true mark on an LG systems performance is whether it's valuable to the human user that has to interact with it or read the text that's produced from it.", "tokens": [583, 309, 311, 309, 311, 1021, 281, 1604, 300, 412, 264, 917, 295, 264, 786, 264, 2074, 1491, 322, 364, 25449, 3652, 3389, 307, 1968, 309, 311, 8263, 281, 264, 1952, 4195, 300, 575, 281, 4648, 365, 309, 420, 1401, 264, 2487, 300, 311, 7126, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.11468745666800194, "compression_ratio": 1.7195571955719557, "no_speech_prob": 0.0001177050216938369}, {"id": 527, "seek": 384800, "start": 3848.0, "end": 3873.0, "text": " And unfortunately automatic metrics tend to fall short of replicating human opinions of the quality of generated text and that's why you know for this reason human evaluations are viewed as the most important form of evaluation for text generation systems almost all work in an LG generally includes some form of human evaluation particularly if the task is more open ended.", "tokens": [400, 7015, 12509, 16367, 3928, 281, 2100, 2099, 295, 3248, 30541, 1952, 11819, 295, 264, 3125, 295, 10833, 2487, 293, 300, 311, 983, 291, 458, 337, 341, 1778, 1952, 43085, 366, 19174, 382, 264, 881, 1021, 1254, 295, 13344, 337, 2487, 5125, 3652, 1920, 439, 589, 294, 364, 25449, 5101, 5974, 512, 1254, 295, 1952, 13344, 4098, 498, 264, 5633, 307, 544, 1269, 4590, 13], "temperature": 0.0, "avg_logprob": -0.09151018529698468, "compression_ratio": 1.6771300448430493, "no_speech_prob": 2.4677638066350482e-05}, {"id": 528, "seek": 387300, "start": 3873.0, "end": 3883.0, "text": " And if it doesn't well you know let me be frank you should probably be skeptical of any claim that's being made by that work.", "tokens": [400, 498, 309, 1177, 380, 731, 291, 458, 718, 385, 312, 10455, 291, 820, 1391, 312, 28601, 295, 604, 3932, 300, 311, 885, 1027, 538, 300, 589, 13], "temperature": 0.0, "avg_logprob": -0.0856186238730826, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.00012728257570415735}, {"id": 529, "seek": 387300, "start": 3883.0, "end": 3902.0, "text": " And finally you know another use of human evaluations is that in addition to to to evaluating model performance you can also use them to actually train new machine learning models that are meant to be evaluate that are meant to serve as evaluation scoring functions themselves.", "tokens": [400, 2721, 291, 458, 1071, 764, 295, 1952, 43085, 307, 300, 294, 4500, 281, 281, 281, 27479, 2316, 3389, 291, 393, 611, 764, 552, 281, 767, 3847, 777, 3479, 2539, 5245, 300, 366, 4140, 281, 312, 13059, 300, 366, 4140, 281, 4596, 382, 13344, 22358, 6828, 2969, 13], "temperature": 0.0, "avg_logprob": -0.0856186238730826, "compression_ratio": 1.759825327510917, "no_speech_prob": 0.00012728257570415735}, {"id": 530, "seek": 390200, "start": 3902.0, "end": 3913.0, "text": " So I guess the you know the main thing to mention about human evaluations since you know we can we can talk about them for a while is that they're both very simple but also very difficult to run.", "tokens": [407, 286, 2041, 264, 291, 458, 264, 2135, 551, 281, 2152, 466, 1952, 43085, 1670, 291, 458, 321, 393, 321, 393, 751, 466, 552, 337, 257, 1339, 307, 300, 436, 434, 1293, 588, 2199, 457, 611, 588, 2252, 281, 1190, 13], "temperature": 0.0, "avg_logprob": -0.08833331810800653, "compression_ratio": 1.6698564593301435, "no_speech_prob": 8.88613285496831e-05}, {"id": 531, "seek": 390200, "start": 3913.0, "end": 3920.0, "text": " You know they're simple because you generally just need to find judges and ask them to rate an intrinsic dimension of the quality of your generated text.", "tokens": [509, 458, 436, 434, 2199, 570, 291, 5101, 445, 643, 281, 915, 14449, 293, 1029, 552, 281, 3314, 364, 35698, 10139, 295, 264, 3125, 295, 428, 10833, 2487, 13], "temperature": 0.0, "avg_logprob": -0.08833331810800653, "compression_ratio": 1.6698564593301435, "no_speech_prob": 8.88613285496831e-05}, {"id": 532, "seek": 392000, "start": 3920.0, "end": 3934.0, "text": " So you have to define a set of criteria that you decide is important for the task that you're designing a system for and that can be things like fluency where you just you measure things like grammar spelling or choice does this actually look like human language.", "tokens": [407, 291, 362, 281, 6964, 257, 992, 295, 11101, 300, 291, 4536, 307, 1021, 337, 264, 5633, 300, 291, 434, 14685, 257, 1185, 337, 293, 300, 393, 312, 721, 411, 5029, 3020, 689, 291, 445, 291, 3481, 721, 411, 22317, 22254, 420, 3922, 775, 341, 767, 574, 411, 1952, 2856, 13], "temperature": 0.0, "avg_logprob": -0.10388091694224964, "compression_ratio": 1.593939393939394, "no_speech_prob": 3.4264296118635684e-05}, {"id": 533, "seek": 393400, "start": 3934.0, "end": 3947.0, "text": " So the text does the text accurately reflect facts that are described in the context, you know common sense doesn't sort of follow the logical rules of the world that we might expect.", "tokens": [407, 264, 2487, 775, 264, 2487, 20095, 5031, 9130, 300, 366, 7619, 294, 264, 4319, 11, 291, 458, 2689, 2020, 1177, 380, 1333, 295, 1524, 264, 14978, 4474, 295, 264, 1002, 300, 321, 1062, 2066, 13], "temperature": 0.4, "avg_logprob": -0.22607516035248962, "compression_ratio": 1.6828193832599119, "no_speech_prob": 1.8630433260113932e-05}, {"id": 534, "seek": 393400, "start": 3947.0, "end": 3960.0, "text": " But once you once you've defined these criteria you can have humans evaluate the generated text for how well it actually produces text that that that that caters to the to these particular criteria.", "tokens": [583, 1564, 291, 1564, 291, 600, 7642, 613, 11101, 291, 393, 362, 6255, 13059, 264, 10833, 2487, 337, 577, 731, 309, 767, 14725, 2487, 300, 300, 300, 300, 3857, 433, 281, 264, 281, 613, 1729, 11101, 13], "temperature": 0.4, "avg_logprob": -0.22607516035248962, "compression_ratio": 1.6828193832599119, "no_speech_prob": 1.8630433260113932e-05}, {"id": 535, "seek": 396000, "start": 3960.0, "end": 3966.0, "text": " So here is that you know while these you know dimensions are common and repeated across different evaluations.", "tokens": [407, 510, 307, 300, 291, 458, 1339, 613, 291, 458, 12819, 366, 2689, 293, 10477, 2108, 819, 43085, 13], "temperature": 0.0, "avg_logprob": -0.10651836920222012, "compression_ratio": 1.9186440677966101, "no_speech_prob": 7.601513061672449e-05}, {"id": 536, "seek": 396000, "start": 3966.0, "end": 3989.0, "text": " They can you know often be referred to by other names and you know explain to evaluators in different terms and be measured in different ways and in fact one of the problems with human evaluations is that across works they tend to be very unstandardized which can make the replication of human results quite difficult and that's why when you read text generation papers you rarely see a comparison of human evaluation scores between two different studies.", "tokens": [814, 393, 291, 458, 2049, 312, 10839, 281, 538, 661, 5288, 293, 291, 458, 2903, 281, 6133, 3391, 294, 819, 2115, 293, 312, 12690, 294, 819, 2098, 293, 294, 1186, 472, 295, 264, 2740, 365, 1952, 43085, 307, 300, 2108, 1985, 436, 3928, 281, 312, 588, 517, 1115, 515, 1602, 597, 393, 652, 264, 39911, 295, 1952, 3542, 1596, 2252, 293, 300, 311, 983, 562, 291, 1401, 2487, 5125, 10577, 291, 13752, 536, 257, 9660, 295, 1952, 13344, 13444, 1296, 732, 819, 5313, 13], "temperature": 0.0, "avg_logprob": -0.10651836920222012, "compression_ratio": 1.9186440677966101, "no_speech_prob": 7.601513061672449e-05}, {"id": 537, "seek": 398900, "start": 3989.0, "end": 3994.0, "text": " Even if they evaluated the same dimensions.", "tokens": [2754, 498, 436, 25509, 264, 912, 12819, 13], "temperature": 0.0, "avg_logprob": -0.13918834924697876, "compression_ratio": 1.4736842105263157, "no_speech_prob": 2.2473208446172066e-05}, {"id": 538, "seek": 398900, "start": 3994.0, "end": 4008.0, "text": " But you know another set of issues with human evaluations beyond the fact that they're slow expensive and unstandardized is that you know humans themselves aren't actually perfect.", "tokens": [583, 291, 458, 1071, 992, 295, 2663, 365, 1952, 43085, 4399, 264, 1186, 300, 436, 434, 2964, 5124, 293, 517, 1115, 515, 1602, 307, 300, 291, 458, 6255, 2969, 3212, 380, 767, 2176, 13], "temperature": 0.0, "avg_logprob": -0.13918834924697876, "compression_ratio": 1.4736842105263157, "no_speech_prob": 2.2473208446172066e-05}, {"id": 539, "seek": 400800, "start": 4008.0, "end": 4025.0, "text": " And you know I guess as a few negatives that I can say about humans you know even though we're all humans is that we tend to not be very consistent folks you know often changing our minds about how we've used something depending on something as as you know trivial as the time of day.", "tokens": [400, 291, 458, 286, 2041, 382, 257, 1326, 40019, 300, 286, 393, 584, 466, 6255, 291, 458, 754, 1673, 321, 434, 439, 6255, 307, 300, 321, 3928, 281, 406, 312, 588, 8398, 4024, 291, 458, 2049, 4473, 527, 9634, 466, 577, 321, 600, 1143, 746, 5413, 322, 746, 382, 382, 291, 458, 26703, 382, 264, 565, 295, 786, 13], "temperature": 0.0, "avg_logprob": -0.11920966042412652, "compression_ratio": 1.6705882352941177, "no_speech_prob": 9.026657062349841e-05}, {"id": 540, "seek": 402500, "start": 4025.0, "end": 4045.0, "text": " And I don't always reason in the way that we're expected to when presented with with a task such as evaluating something we can lose concentrated lose concentration and not really be focused on what we're doing and you know we can often misinterpret what let's say human evaluation is asking us to do such that we inject our own biases into the task.", "tokens": [400, 286, 500, 380, 1009, 1778, 294, 264, 636, 300, 321, 434, 5176, 281, 562, 8212, 365, 365, 257, 5633, 1270, 382, 27479, 746, 321, 393, 3624, 21321, 3624, 9856, 293, 406, 534, 312, 5178, 322, 437, 321, 434, 884, 293, 291, 458, 321, 393, 2049, 3346, 41935, 437, 718, 311, 584, 1952, 13344, 307, 3365, 505, 281, 360, 1270, 300, 321, 10711, 527, 1065, 32152, 666, 264, 5633, 13], "temperature": 0.0, "avg_logprob": -0.0906210912240518, "compression_ratio": 1.6587677725118484, "no_speech_prob": 2.976998257508967e-05}, {"id": 541, "seek": 404500, "start": 4045.0, "end": 4062.0, "text": " And you know on top of all these things you know when we run human evaluations we're also dealing with the fact that one of the big motivators that our human judges have is to do the task as quickly as possible which isn't a great mix particularly if we want them to really give us high quality ratings.", "tokens": [400, 291, 458, 322, 1192, 295, 439, 613, 721, 291, 458, 562, 321, 1190, 1952, 43085, 321, 434, 611, 6260, 365, 264, 1186, 300, 472, 295, 264, 955, 5426, 3391, 300, 527, 1952, 14449, 362, 307, 281, 360, 264, 5633, 382, 2661, 382, 1944, 597, 1943, 380, 257, 869, 2890, 4098, 498, 321, 528, 552, 281, 534, 976, 505, 1090, 3125, 24603, 13], "temperature": 0.0, "avg_logprob": -0.05219040598188128, "compression_ratio": 1.7964912280701755, "no_speech_prob": 5.1438000809866935e-05}, {"id": 542, "seek": 404500, "start": 4062.0, "end": 4074.0, "text": " But you know humans are kind of the best thing that we that we have to actually give us the most accurate assessments of whether text generation systems are are doing well so we so we do the best that we can.", "tokens": [583, 291, 458, 6255, 366, 733, 295, 264, 1151, 551, 300, 321, 300, 321, 362, 281, 767, 976, 505, 264, 881, 8559, 24338, 295, 1968, 2487, 5125, 3652, 366, 366, 884, 731, 370, 321, 370, 321, 360, 264, 1151, 300, 321, 393, 13], "temperature": 0.0, "avg_logprob": -0.05219040598188128, "compression_ratio": 1.7964912280701755, "no_speech_prob": 5.1438000809866935e-05}, {"id": 543, "seek": 407400, "start": 4074.0, "end": 4097.0, "text": " I'm actually going to skip this slide but I mentioned earlier that we know one of the things that we can also do is use human ratings to train models to actually predict scores for text itself and so two systems that that that do some things along these lines provided citations to here so that you can take a look at them if you're curious later on.", "tokens": [286, 478, 767, 516, 281, 10023, 341, 4137, 457, 286, 2835, 3071, 300, 321, 458, 472, 295, 264, 721, 300, 321, 393, 611, 360, 307, 764, 1952, 24603, 281, 3847, 5245, 281, 767, 6069, 13444, 337, 2487, 2564, 293, 370, 732, 3652, 300, 300, 300, 360, 512, 721, 2051, 613, 3876, 5649, 4814, 763, 281, 510, 370, 300, 291, 393, 747, 257, 574, 412, 552, 498, 291, 434, 6369, 1780, 322, 13], "temperature": 0.0, "avg_logprob": -0.12987848332053736, "compression_ratio": 1.6587677725118484, "no_speech_prob": 5.828465145896189e-05}, {"id": 544, "seek": 409700, "start": 4097.0, "end": 4125.0, "text": " So you know the takeaways I kind of want you to get from the section are that you know evaluation is quite hard and particularly in text generation so content overlap metrics do provide a good starting point for evaluating the quality of generated text you know if you run these and grab overlap metrics and they show scores that are worse than they should be that's the first sign that you that you have problem but they're generally not not good enough on their own.", "tokens": [407, 291, 458, 264, 45584, 286, 733, 295, 528, 291, 281, 483, 490, 264, 3541, 366, 300, 291, 458, 13344, 307, 1596, 1152, 293, 4098, 294, 2487, 5125, 370, 2701, 19959, 16367, 360, 2893, 257, 665, 2891, 935, 337, 27479, 264, 3125, 295, 10833, 2487, 291, 458, 498, 291, 1190, 613, 293, 4444, 19959, 16367, 293, 436, 855, 13444, 300, 366, 5324, 813, 436, 820, 312, 300, 311, 264, 700, 1465, 300, 291, 300, 291, 362, 1154, 457, 436, 434, 5101, 406, 406, 665, 1547, 322, 641, 1065, 13], "temperature": 0.0, "avg_logprob": -0.13489618609028478, "compression_ratio": 1.8352941176470587, "no_speech_prob": 1.7230253433808684e-05}, {"id": 545, "seek": 412500, "start": 4125.0, "end": 4154.0, "text": " Model based metrics tend to be you know more correlated with human judgments than content overlap once particularly as the tasks become more open ended such as you know dialogue and storytelling but the downside there's that they're they're not very interpretable you know unlike with a content overlap metric where you can say exactly oh this is why this score is this way it's because these words match up with these words with a model based metric you you get a much more implicit definition of similarity which you know while useful is also less.", "tokens": [17105, 2361, 16367, 3928, 281, 312, 291, 458, 544, 38574, 365, 1952, 40337, 813, 2701, 19959, 1564, 4098, 382, 264, 9608, 1813, 544, 1269, 4590, 1270, 382, 291, 458, 10221, 293, 21479, 457, 264, 25060, 456, 311, 300, 436, 434, 436, 434, 406, 588, 7302, 712, 291, 458, 8343, 365, 257, 2701, 19959, 20678, 689, 291, 393, 584, 2293, 1954, 341, 307, 983, 341, 6175, 307, 341, 636, 309, 311, 570, 613, 2283, 2995, 493, 365, 613, 2283, 365, 257, 2316, 2361, 20678, 291, 291, 483, 257, 709, 544, 26947, 7123, 295, 32194, 597, 291, 458, 1339, 4420, 307, 611, 1570, 13], "temperature": 0.0, "avg_logprob": -0.13955740658742077, "compression_ratio": 1.8518518518518519, "no_speech_prob": 7.252906652865931e-05}, {"id": 546, "seek": 415400, "start": 4154.0, "end": 4175.0, "text": " Interpretable human judgments are absolutely critical because even if they're inconsistent and sometimes don't do the tasks that you want them to humans are actually able to intrinsically evaluate dimensions that we that we don't even know how to formulate using any type of automatic metric.", "tokens": [5751, 6629, 712, 1952, 40337, 366, 3122, 4924, 570, 754, 498, 436, 434, 36891, 293, 2171, 500, 380, 360, 264, 9608, 300, 291, 528, 552, 281, 6255, 366, 767, 1075, 281, 28621, 984, 13059, 12819, 300, 321, 300, 321, 500, 380, 754, 458, 577, 281, 47881, 1228, 604, 2010, 295, 12509, 20678, 13], "temperature": 0.0, "avg_logprob": -0.11997747421264648, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.00010719324927777052}, {"id": 547, "seek": 417500, "start": 4175.0, "end": 4203.0, "text": " But lastly slightly unrelated to what I've spoken about the section you know I just I just want to say that the number one evaluator of any NLG system that you create should should really be you you know a look at your models outputs as perhaps you do a project that involves an LG can really be worth days weeks and sometimes months of staring at evaluation metrics that are that are perhaps a bit informative so if you design an LG systems make sure to evaluate your own generations very consistently.", "tokens": [50364, 583, 16386, 4748, 38967, 281, 437, 286, 600, 10759, 466, 264, 3541, 291, 458, 286, 445, 286, 445, 528, 281, 584, 300, 264, 1230, 472, 6133, 1639, 295, 604, 426, 43, 38, 1185, 300, 291, 1884, 820, 820, 534, 312, 291, 291, 458, 257, 574, 412, 428, 5245, 23930, 382, 4317, 291, 360, 257, 1716, 300, 11626, 364, 25449, 393, 534, 312, 3163, 1708, 3259, 293, 2171, 2493, 295, 18043, 412, 13344, 16367, 300, 366, 300, 366, 4317, 257, 857, 27759, 370, 498, 291, 1715, 364, 25449, 3652, 652, 988, 281, 13059, 428, 1065, 10593, 588, 14961, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14177138262456007, "compression_ratio": 1.7773851590106007, "no_speech_prob": 5.305878949002363e-05}, {"id": 548, "seek": 420500, "start": 4205.0, "end": 4228.0, "text": " So in this last section I think it's quite important to talk about ethical topics in natural language generation as well because you know ultimately while NLG really allows us to tackle new and interesting applications you know if we're not capable we can end up deploying fairly dangerous and harmful systems.", "tokens": [407, 294, 341, 1036, 3541, 286, 519, 309, 311, 1596, 1021, 281, 751, 466, 18890, 8378, 294, 3303, 2856, 5125, 382, 731, 570, 291, 458, 6284, 1339, 426, 43, 38, 534, 4045, 505, 281, 14896, 777, 293, 1880, 5821, 291, 458, 498, 321, 434, 406, 8189, 321, 393, 917, 493, 34198, 6457, 5795, 293, 19727, 3652, 13], "temperature": 0.0, "avg_logprob": -0.08491667763131563, "compression_ratio": 1.5048543689320388, "no_speech_prob": 0.008979421108961105}, {"id": 549, "seek": 422800, "start": 4228.0, "end": 4236.0, "text": " And as a warning I just want to say that you know some of the content on the next use slides can you know is going to potentially be quite uncomfortable.", "tokens": [400, 382, 257, 9164, 286, 445, 528, 281, 584, 300, 291, 458, 512, 295, 264, 2701, 322, 264, 958, 764, 9788, 393, 291, 458, 307, 516, 281, 7263, 312, 1596, 10532, 13], "temperature": 0.0, "avg_logprob": -0.15288203524560043, "compression_ratio": 1.643939393939394, "no_speech_prob": 9.456746920477599e-05}, {"id": 550, "seek": 422800, "start": 4236.0, "end": 4241.0, "text": " But I think it's important to make very clear how these systems can go very very wrong.", "tokens": [583, 286, 519, 309, 311, 1021, 281, 652, 588, 1850, 577, 613, 3652, 393, 352, 588, 588, 2085, 13], "temperature": 0.0, "avg_logprob": -0.15288203524560043, "compression_ratio": 1.643939393939394, "no_speech_prob": 9.456746920477599e-05}, {"id": 551, "seek": 422800, "start": 4241.0, "end": 4254.0, "text": " And without picking on a particular example I do perhaps think that you know that one of the most famous examples of this was the Tay dialogue chatbots which was released onto Twitter in 2016.", "tokens": [400, 1553, 8867, 322, 257, 1729, 1365, 286, 360, 4317, 519, 300, 291, 458, 300, 472, 295, 264, 881, 4618, 5110, 295, 341, 390, 264, 10132, 10221, 5081, 65, 1971, 597, 390, 4736, 3911, 5794, 294, 6549, 13], "temperature": 0.0, "avg_logprob": -0.15288203524560043, "compression_ratio": 1.643939393939394, "no_speech_prob": 9.456746920477599e-05}, {"id": 552, "seek": 425400, "start": 4254.0, "end": 4269.0, "text": " And within 24 hours it had started you know making some very nasty comments that that exhibited racist sexist anti-submitting white supremacist leanings which you know is ultimately probably not with the designers of Tay had in mind.", "tokens": [400, 1951, 4022, 2496, 309, 632, 1409, 291, 458, 1455, 512, 588, 17923, 3053, 300, 300, 49446, 16419, 3260, 468, 6061, 12, 30131, 76, 2414, 2418, 23710, 326, 468, 11659, 1109, 597, 291, 458, 307, 6284, 1391, 406, 365, 264, 16196, 295, 10132, 632, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.11531565189361573, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00030054544913582504}, {"id": 553, "seek": 425400, "start": 4269.0, "end": 4279.0, "text": " So what actually ended up going wrong with Tay well you know here's the thing Tay behaved exactly as we should have expected it would.", "tokens": [407, 437, 767, 4590, 493, 516, 2085, 365, 10132, 731, 291, 458, 510, 311, 264, 551, 10132, 48249, 2293, 382, 321, 820, 362, 5176, 309, 576, 13], "temperature": 0.0, "avg_logprob": -0.11531565189361573, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00030054544913582504}, {"id": 554, "seek": 427900, "start": 4279.0, "end": 4294.0, "text": " It was designed to learn to exhibit the conversational patterns of the users that it interacted with and it did that you know NLG models are very good at capturing the language distribution of their training examples that's been the one thing that we've been remarkably consistent on.", "tokens": [467, 390, 4761, 281, 1466, 281, 20487, 264, 2615, 1478, 8294, 295, 264, 5022, 300, 309, 49621, 365, 293, 309, 630, 300, 291, 458, 426, 43, 38, 5245, 366, 588, 665, 412, 23384, 264, 2856, 7316, 295, 641, 3097, 5110, 300, 311, 668, 264, 472, 551, 300, 321, 600, 668, 37381, 8398, 322, 13], "temperature": 0.0, "avg_logprob": -0.10000456463206898, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.00015112008259166032}, {"id": 555, "seek": 427900, "start": 4294.0, "end": 4303.0, "text": " You know the last few years and it turns out if they're training examples end up having toxic content they will learn to repeat that content.", "tokens": [509, 458, 264, 1036, 1326, 924, 293, 309, 4523, 484, 498, 436, 434, 3097, 5110, 917, 493, 1419, 12786, 2701, 436, 486, 1466, 281, 7149, 300, 2701, 13], "temperature": 0.0, "avg_logprob": -0.10000456463206898, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.00015112008259166032}, {"id": 556, "seek": 430300, "start": 4303.0, "end": 4311.0, "text": " And this is perhaps no clearer than if you learn at what pre-trained language models have to say about let's say different demographics.", "tokens": [400, 341, 307, 4317, 572, 26131, 813, 498, 291, 1466, 412, 437, 659, 12, 17227, 2001, 2856, 5245, 362, 281, 584, 466, 718, 311, 584, 819, 36884, 13], "temperature": 0.0, "avg_logprob": -0.12327933955836941, "compression_ratio": 1.5774647887323943, "no_speech_prob": 7.028287654975429e-05}, {"id": 557, "seek": 430300, "start": 4311.0, "end": 4323.0, "text": " So if you remember you know large pre-trained language models that underlie many modern NLG systems they're trained on massive corporate text which are often opaque and crawled from online resources.", "tokens": [407, 498, 291, 1604, 291, 458, 2416, 659, 12, 17227, 2001, 2856, 5245, 300, 833, 6302, 867, 4363, 426, 43, 38, 3652, 436, 434, 8895, 322, 5994, 10896, 2487, 597, 366, 2049, 42687, 293, 13999, 1493, 490, 2950, 3593, 13], "temperature": 0.0, "avg_logprob": -0.12327933955836941, "compression_ratio": 1.5774647887323943, "no_speech_prob": 7.028287654975429e-05}, {"id": 558, "seek": 432300, "start": 4323.0, "end": 4333.0, "text": " If it turns out that those corporate have toxic content the language models are going to learn it's and in fact make it even worse.", "tokens": [759, 309, 4523, 484, 300, 729, 10896, 362, 12786, 2701, 264, 2856, 5245, 366, 516, 281, 1466, 309, 311, 293, 294, 1186, 652, 309, 754, 5324, 13], "temperature": 0.0, "avg_logprob": -0.13956465986039904, "compression_ratio": 1.7438423645320198, "no_speech_prob": 4.197921953164041e-05}, {"id": 559, "seek": 432300, "start": 4333.0, "end": 4343.0, "text": " And then it turns out that if you prompt these language models for certain pieces of information it can spit out that toxic content showing you know very different opinions across you know gender races sexual orientations.", "tokens": [400, 550, 309, 4523, 484, 300, 498, 291, 12391, 613, 2856, 5245, 337, 1629, 3755, 295, 1589, 309, 393, 22127, 484, 300, 12786, 2701, 4099, 291, 458, 588, 819, 11819, 2108, 291, 458, 7898, 15484, 6701, 8579, 763, 13], "temperature": 0.0, "avg_logprob": -0.13956465986039904, "compression_ratio": 1.7438423645320198, "no_speech_prob": 4.197921953164041e-05}, {"id": 560, "seek": 434300, "start": 4343.0, "end": 4360.0, "text": " Now you know I can see that it would actually be rare to ask a language model to weigh in with their opinions on this matter but you do have to ask yourself if this type of information is encoded in the model in some way in what other ways could these learn patterns end up being reflected by this model once it's actually deployed in practice.", "tokens": [823, 291, 458, 286, 393, 536, 300, 309, 576, 767, 312, 5892, 281, 1029, 257, 2856, 2316, 281, 13843, 294, 365, 641, 11819, 322, 341, 1871, 457, 291, 360, 362, 281, 1029, 1803, 498, 341, 2010, 295, 1589, 307, 2058, 12340, 294, 264, 2316, 294, 512, 636, 294, 437, 661, 2098, 727, 613, 1466, 8294, 917, 493, 885, 15502, 538, 341, 2316, 1564, 309, 311, 767, 17826, 294, 3124, 13], "temperature": 0.0, "avg_logprob": -0.0682623150584462, "compression_ratio": 1.721311475409836, "no_speech_prob": 8.218575385399163e-05}, {"id": 561, "seek": 434300, "start": 4360.0, "end": 4365.0, "text": " And that kind of leads us to the second problem with these language models.", "tokens": [400, 300, 733, 295, 6689, 505, 281, 264, 1150, 1154, 365, 613, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.0682623150584462, "compression_ratio": 1.721311475409836, "no_speech_prob": 8.218575385399163e-05}, {"id": 562, "seek": 436500, "start": 4365.0, "end": 4375.0, "text": " And we actually don't really know how information ends up being learned and encoded by them which means that we don't have a rigorous understanding of what types of inputs are going to trigger what types of outputs.", "tokens": [400, 321, 767, 500, 380, 534, 458, 577, 1589, 5314, 493, 885, 3264, 293, 2058, 12340, 538, 552, 597, 1355, 300, 321, 500, 380, 362, 257, 29882, 3701, 295, 437, 3467, 295, 15743, 366, 516, 281, 7875, 437, 3467, 295, 23930, 13], "temperature": 0.0, "avg_logprob": -0.10007401351090316, "compression_ratio": 1.578397212543554, "no_speech_prob": 8.479028474539518e-05}, {"id": 563, "seek": 436500, "start": 4375.0, "end": 4388.0, "text": " And in fact Wallace it all showed in their EM and LP 2019 work that this was a big problem because if you prime these models with particularly adversarial inputs they would generally devolve immediately into producing very toxic content.", "tokens": [400, 294, 1186, 32626, 309, 439, 4712, 294, 641, 16237, 293, 38095, 6071, 589, 300, 341, 390, 257, 955, 1154, 570, 498, 291, 5835, 613, 5245, 365, 4098, 17641, 44745, 15743, 436, 576, 5101, 1905, 37361, 4258, 666, 10501, 588, 12786, 2701, 13], "temperature": 0.0, "avg_logprob": -0.10007401351090316, "compression_ratio": 1.578397212543554, "no_speech_prob": 8.479028474539518e-05}, {"id": 564, "seek": 438800, "start": 4388.0, "end": 4397.0, "text": " In other words what it took to 24 hours to learn how to do these systems can kind of do out of the box if prying with the wrong examples.", "tokens": [682, 661, 2283, 437, 309, 1890, 281, 4022, 2496, 281, 1466, 577, 281, 360, 613, 3652, 393, 733, 295, 360, 484, 295, 264, 2424, 498, 582, 1840, 365, 264, 2085, 5110, 13], "temperature": 0.0, "avg_logprob": -0.1636970043182373, "compression_ratio": 1.4227642276422765, "no_speech_prob": 5.305866216076538e-05}, {"id": 565, "seek": 438800, "start": 4397.0, "end": 4401.0, "text": " And unfortunately the wrong examples.", "tokens": [400, 7015, 264, 2085, 5110, 13], "temperature": 0.0, "avg_logprob": -0.1636970043182373, "compression_ratio": 1.4227642276422765, "no_speech_prob": 5.305866216076538e-05}, {"id": 566, "seek": 440100, "start": 4401.0, "end": 4421.0, "text": " And so it ends up being a lot less nasty than we might have expected a lot less nasty than the ones on the previous slide at the very least. But you know in a work at EM and LP findings last year.", "tokens": [400, 370, 309, 5314, 493, 885, 257, 688, 1570, 17923, 813, 321, 1062, 362, 5176, 257, 688, 1570, 17923, 813, 264, 2306, 322, 264, 3894, 4137, 412, 264, 588, 1935, 13, 583, 291, 458, 294, 257, 589, 412, 16237, 293, 38095, 16483, 1036, 1064, 13], "temperature": 0.0, "avg_logprob": -0.2355837919274155, "compression_ratio": 1.4202898550724639, "no_speech_prob": 2.3917302314657718e-05}, {"id": 567, "seek": 442100, "start": 4421.0, "end": 4431.0, "text": " And it was not as consistent as in the previous work but it was still often enough.", "tokens": [400, 309, 390, 406, 382, 8398, 382, 294, 264, 3894, 589, 457, 309, 390, 920, 2049, 1547, 13], "temperature": 0.0, "avg_logprob": -0.20843560555401972, "compression_ratio": 1.4891304347826086, "no_speech_prob": 4.6828004997223616e-05}, {"id": 568, "seek": 442100, "start": 4431.0, "end": 4436.0, "text": " And these examples really go to show that we need to be careful with how these systems are deployed.", "tokens": [400, 613, 5110, 534, 352, 281, 855, 300, 321, 643, 281, 312, 5026, 365, 577, 613, 3652, 366, 17826, 13], "temperature": 0.0, "avg_logprob": -0.20843560555401972, "compression_ratio": 1.4891304347826086, "no_speech_prob": 4.6828004997223616e-05}, {"id": 569, "seek": 442100, "start": 4436.0, "end": 4441.0, "text": " If you have an NLG system you need safeguards to stop it from outputting harmful content.", "tokens": [759, 291, 362, 364, 426, 43, 38, 1185, 291, 643, 32358, 84, 2287, 281, 1590, 309, 490, 5598, 783, 19727, 2701, 13], "temperature": 0.0, "avg_logprob": -0.20843560555401972, "compression_ratio": 1.4891304347826086, "no_speech_prob": 4.6828004997223616e-05}, {"id": 570, "seek": 444100, "start": 4441.0, "end": 4452.0, "text": " And also the number of toxic and bi toxicity and biosefaction today you know a model that can be primed to generate incorrect or unfactual information can be quite dangerous too.", "tokens": [400, 611, 264, 1230, 295, 12786, 293, 3228, 45866, 293, 3228, 541, 69, 2894, 965, 291, 458, 257, 2316, 300, 393, 312, 2886, 292, 281, 8460, 18424, 420, 3971, 578, 901, 1589, 393, 312, 1596, 5795, 886, 13], "temperature": 0.0, "avg_logprob": -0.28169185557263965, "compression_ratio": 1.6473029045643153, "no_speech_prob": 6.401222344720736e-05}, {"id": 571, "seek": 444100, "start": 4452.0, "end": 4458.0, "text": " And also NLG models shouldn't be deployed without an understanding of who its users will be.", "tokens": [400, 611, 426, 43, 38, 5245, 4659, 380, 312, 17826, 1553, 364, 3701, 295, 567, 1080, 5022, 486, 312, 13], "temperature": 0.0, "avg_logprob": -0.28169185557263965, "compression_ratio": 1.6473029045643153, "no_speech_prob": 6.401222344720736e-05}, {"id": 572, "seek": 444100, "start": 4458.0, "end": 4468.0, "text": " And there's always going to be adversarial users for any model that you create even if you can't think of them in the moment.", "tokens": [400, 456, 311, 1009, 516, 281, 312, 17641, 44745, 5022, 337, 604, 2316, 300, 291, 1884, 754, 498, 291, 393, 380, 519, 295, 552, 294, 264, 1623, 13], "temperature": 0.0, "avg_logprob": -0.28169185557263965, "compression_ratio": 1.6473029045643153, "no_speech_prob": 6.401222344720736e-05}, {"id": 573, "seek": 446800, "start": 4468.0, "end": 4479.0, "text": " And so the final point which is that you know the advances in NLG have really you know allowed us to build text production systems for many new applications.", "tokens": [400, 370, 264, 2572, 935, 597, 307, 300, 291, 458, 264, 25297, 294, 426, 43, 38, 362, 534, 291, 458, 4350, 505, 281, 1322, 2487, 4265, 3652, 337, 867, 777, 5821, 13], "temperature": 0.0, "avg_logprob": -0.20839339583667357, "compression_ratio": 1.5578947368421052, "no_speech_prob": 9.912323002936319e-05}, {"id": 574, "seek": 446800, "start": 4479.0, "end": 4486.0, "text": " You know as we do this though it's it's important to ask you know does the content that we're building a system to automatically generate.", "tokens": [509, 458, 382, 321, 360, 341, 1673, 309, 311, 309, 311, 1021, 281, 1029, 291, 458, 775, 264, 2701, 300, 321, 434, 2390, 257, 1185, 281, 6772, 8460, 13], "temperature": 0.0, "avg_logprob": -0.20839339583667357, "compression_ratio": 1.5578947368421052, "no_speech_prob": 9.912323002936319e-05}, {"id": 575, "seek": 448600, "start": 4486.0, "end": 4499.0, "text": " And so for easy human ingestion you know does it really need to be generated automatically.", "tokens": [400, 370, 337, 1858, 1952, 3957, 31342, 291, 458, 775, 309, 534, 643, 281, 312, 10833, 6772, 13], "temperature": 0.0, "avg_logprob": -0.1859764223513396, "compression_ratio": 1.6165413533834587, "no_speech_prob": 2.8404743716237135e-05}, {"id": 576, "seek": 448600, "start": 4499.0, "end": 4504.0, "text": " And I think a good example of this is the work of sellers at all at NURBS 2019 which you know showed off the potential dangers of fake news generators from pre train language models.", "tokens": [400, 286, 519, 257, 665, 1365, 295, 341, 307, 264, 589, 295, 31276, 412, 439, 412, 426, 7932, 8176, 6071, 597, 291, 458, 4712, 766, 264, 3995, 27701, 295, 7592, 2583, 38662, 490, 659, 3847, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1859764223513396, "compression_ratio": 1.6165413533834587, "no_speech_prob": 2.8404743716237135e-05}, {"id": 577, "seek": 448600, "start": 4504.0, "end": 4514.0, "text": " I actually thought this was a great work and it highlighted you know many of the defenses that you could be developed against fake news generations system.", "tokens": [286, 767, 1194, 341, 390, 257, 869, 589, 293, 309, 17173, 291, 458, 867, 295, 264, 35989, 300, 291, 727, 312, 4743, 1970, 7592, 2583, 10593, 1185, 13], "temperature": 0.0, "avg_logprob": -0.1859764223513396, "compression_ratio": 1.6165413533834587, "no_speech_prob": 2.8404743716237135e-05}, {"id": 578, "seek": 451400, "start": 4514.0, "end": 4521.0, "text": " But you know the point is more so that you should always imagine that any tool that you create could be used in a negative way.", "tokens": [583, 291, 458, 264, 935, 307, 544, 370, 300, 291, 820, 1009, 3811, 300, 604, 2290, 300, 291, 1884, 727, 312, 1143, 294, 257, 3671, 636, 13], "temperature": 0.0, "avg_logprob": -0.06646401286125184, "compression_ratio": 1.662162162162162, "no_speech_prob": 1.4062785339774564e-05}, {"id": 579, "seek": 451400, "start": 4521.0, "end": 4530.0, "text": " So a storytelling NLG system can also potentially be you know repurposed to do fake news generation.", "tokens": [407, 257, 21479, 426, 43, 38, 1185, 393, 611, 7263, 312, 291, 458, 1085, 20130, 1744, 281, 360, 7592, 2583, 5125, 13], "temperature": 0.0, "avg_logprob": -0.06646401286125184, "compression_ratio": 1.662162162162162, "no_speech_prob": 1.4062785339774564e-05}, {"id": 580, "seek": 451400, "start": 4530.0, "end": 4538.0, "text": " And you should really always ask yourself whether the positive applications of a particular technology outweigh the potential negative ones.", "tokens": [400, 291, 820, 534, 1009, 1029, 1803, 1968, 264, 3353, 5821, 295, 257, 1729, 2899, 484, 826, 910, 264, 3995, 3671, 2306, 13], "temperature": 0.0, "avg_logprob": -0.06646401286125184, "compression_ratio": 1.662162162162162, "no_speech_prob": 1.4062785339774564e-05}, {"id": 581, "seek": 453800, "start": 4538.0, "end": 4544.0, "text": " And that turns out to often not be an easy question.", "tokens": [400, 300, 4523, 484, 281, 2049, 406, 312, 364, 1858, 1168, 13], "temperature": 0.0, "avg_logprob": -0.11285186169752434, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.168763578287326e-05}, {"id": 582, "seek": 453800, "start": 4544.0, "end": 4548.0, "text": " So I guess as concluding thoughts for today.", "tokens": [407, 286, 2041, 382, 9312, 278, 4598, 337, 965, 13], "temperature": 0.0, "avg_logprob": -0.11285186169752434, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.168763578287326e-05}, {"id": 583, "seek": 453800, "start": 4548.0, "end": 4562.0, "text": " You know I just want to mention that you know if you start interacting with NLG systems and practice you're quickly going to see the fairly large limitations that they tend to have.", "tokens": [509, 458, 286, 445, 528, 281, 2152, 300, 291, 458, 498, 291, 722, 18017, 365, 426, 43, 38, 3652, 293, 3124, 291, 434, 2661, 516, 281, 536, 264, 6457, 2416, 15705, 300, 436, 3928, 281, 362, 13], "temperature": 0.0, "avg_logprob": -0.11285186169752434, "compression_ratio": 1.4761904761904763, "no_speech_prob": 3.168763578287326e-05}, {"id": 584, "seek": 456200, "start": 4562.0, "end": 4571.0, "text": " Even in tasks where we've you know achieved a larger amount of progress at building systems that can do the task fairly well.", "tokens": [2754, 294, 9608, 689, 321, 600, 291, 458, 11042, 257, 4833, 2372, 295, 4205, 412, 2390, 3652, 300, 393, 360, 264, 5633, 6457, 731, 13], "temperature": 0.0, "avg_logprob": -0.09001846994672502, "compression_ratio": 1.473170731707317, "no_speech_prob": 4.132814865442924e-05}, {"id": 585, "seek": 456200, "start": 4571.0, "end": 4577.0, "text": " There's still a lot of improvements that can be made to make them even better.", "tokens": [821, 311, 920, 257, 688, 295, 13797, 300, 393, 312, 1027, 281, 652, 552, 754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.09001846994672502, "compression_ratio": 1.473170731707317, "no_speech_prob": 4.132814865442924e-05}, {"id": 586, "seek": 456200, "start": 4577.0, "end": 4583.0, "text": " And pretty much any NLG task at the same time evaluating it effectively remains a huge challenge.", "tokens": [400, 1238, 709, 604, 426, 43, 38, 5633, 412, 264, 912, 565, 27479, 309, 8659, 7023, 257, 2603, 3430, 13], "temperature": 0.0, "avg_logprob": -0.09001846994672502, "compression_ratio": 1.473170731707317, "no_speech_prob": 4.132814865442924e-05}, {"id": 587, "seek": 458300, "start": 4583.0, "end": 4592.0, "text": " And you often have to rely on humans to give us the best estimates of how well our system is doing.", "tokens": [400, 291, 2049, 362, 281, 10687, 322, 6255, 281, 976, 505, 264, 1151, 20561, 295, 577, 731, 527, 1185, 307, 884, 13], "temperature": 0.0, "avg_logprob": -0.15867557222881015, "compression_ratio": 1.5082872928176796, "no_speech_prob": 7.601302786497399e-05}, {"id": 588, "seek": 458300, "start": 4592.0, "end": 4603.0, "text": " And so an area where a large improvement would really you know kind of bootstrap larger improvements in many other areas of NLG would be to find better automatic evaluation.", "tokens": [400, 370, 364, 1859, 689, 257, 2416, 10444, 576, 534, 291, 458, 733, 295, 11450, 372, 4007, 4833, 13797, 294, 867, 661, 3179, 295, 426, 43, 38, 576, 312, 281, 915, 1101, 12509, 13344, 13], "temperature": 0.0, "avg_logprob": -0.15867557222881015, "compression_ratio": 1.5082872928176796, "no_speech_prob": 7.601302786497399e-05}, {"id": 589, "seek": 460300, "start": 4603.0, "end": 4616.0, "text": " On the other hand on a very you know optimistic note I do want to say that with the advent of large skill language models.", "tokens": [1282, 264, 661, 1011, 322, 257, 588, 291, 458, 19397, 3637, 286, 360, 528, 281, 584, 300, 365, 264, 7045, 295, 2416, 5389, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.14751492668600644, "compression_ratio": 1.6291666666666667, "no_speech_prob": 9.310431778430939e-05}, {"id": 590, "seek": 460300, "start": 4616.0, "end": 4626.0, "text": " You know deep NLG research hasn't hasn't been reset but it's never been easier to jump in the space and start playing around with the systems and designing cool new tools that can that can help humans and just content and information more rapidly and more efficiently.", "tokens": [509, 458, 2452, 426, 43, 38, 2132, 6132, 380, 6132, 380, 668, 14322, 457, 309, 311, 1128, 668, 3571, 281, 3012, 294, 264, 1901, 293, 722, 2433, 926, 365, 264, 3652, 293, 14685, 1627, 777, 3873, 300, 393, 300, 393, 854, 6255, 293, 445, 2701, 293, 1589, 544, 12910, 293, 544, 19621, 13], "temperature": 0.0, "avg_logprob": -0.14751492668600644, "compression_ratio": 1.6291666666666667, "no_speech_prob": 9.310431778430939e-05}, {"id": 591, "seek": 462600, "start": 4626.0, "end": 4638.0, "text": " And as a result I think that it's you know one of the most exciting areas of NLP to work in and I think that that you know if you start working in it as well you'll feel the same way and I would encourage you to do so.", "tokens": [400, 382, 257, 1874, 286, 519, 300, 309, 311, 291, 458, 472, 295, 264, 881, 4670, 3179, 295, 426, 45196, 281, 589, 294, 293, 286, 519, 300, 300, 291, 458, 498, 291, 722, 1364, 294, 309, 382, 731, 291, 603, 841, 264, 912, 636, 293, 286, 576, 5373, 291, 281, 360, 370, 13], "temperature": 0.0, "avg_logprob": -0.12883751922183567, "compression_ratio": 1.576271186440678, "no_speech_prob": 6.704454426653683e-05}, {"id": 592, "seek": 463800, "start": 4638.0, "end": 4666.0, "text": " Thanks a lot for having me today. It was really exciting.", "tokens": [2561, 257, 688, 337, 1419, 385, 965, 13, 467, 390, 534, 4670, 13], "temperature": 0.0, "avg_logprob": -0.21243546990787282, "compression_ratio": 0.890625, "no_speech_prob": 5.020142998546362e-05}], "language": "en"}