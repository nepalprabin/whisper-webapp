{"text": " Hi everybody. Welcome to Stanford CS224N, also known as Ling284, natural language processing with deep learning. I'm Christopher Manning and I'm the main instructor for this class. So what we hope to do today is to dive right in. So I'm going to spend about 10 minutes talking about the course. And then we're going to get straight into content for reasons I'll explain in a minute. So we'll talk about human language and word meaning. I'll then introduce the ideas of the word to veck algorithm for learning word meaning. And then going from there we'll kind of concretely work through how you can work out objective function gradients with respect to the word to veck algorithm and say a teeny bit about how optimization works. And then right at the end of the class I then want to spend a little bit of time giving you a sense of how these word vectors work and what you can do with them. So really the key learning for today is I want to give you a sense of how amazing deep learning word vectors are. So we have this really surprising result that word meaning can be represented not perfectly but really rather well by a large vector of real numbers. And you know that's sort of in a way a common place of the last decade of deep learning but it flies in the face of thousands of years of tradition and it's really rather an unexpected result to start focusing on. Okay so quickly what do we hope to teach in this course. So we've got three primary goals. The first is to teach you the foundations are a good deep understanding of the effect of modern methods for deep learning applied to NLP. So we are going to start with and go through the basics and then go on to key methods that are used in NLP, your current networks, attention, transformers and things like that. You want to do something more than just that but also like to give you some sense of a big picture understanding of human languages and what are the reasons for why they're actually quite difficult to understand and produce even though humans seem to do it easily. Now obviously if you really want to learn a lot about this topic you should enroll in and go and start doing some classes in the linguistics department but nevertheless for a lot of you this is the only human language content you'll see during your master's degree or whatever and so we do hope to spend a bit of time on that starting today. And then finally we want to give you an understanding of an ability to build systems in PyTorch for some of the major problems in NLP so we'll look at learning word meanings dependency parsing machine translation question answering. Let's dive into human language. Once part of time I had a lot longer introduction that gave lots of examples about how human languages can be misunderstood and complex. I'll show a few of those examples in later lectures but since right for today we're going to be focused on word meaning I thought I'd just give one example which comes from a very nice xkcd cartoon and that isn't sort of about some of the sort of syntactic ambiguities of sentences but instead it's really emphasizing the important point that language is a social system constructed and interpreted by people and that's part of how and it changes as people decide to adapt its construction and that's part of the reason why human languages are greatest and adaptive system for human beings but difficult as a system or our computers to understand to this day. So in this conversation between the two women one says anyway I could care less and the other says I think you mean you couldn't care less saying you could care less implies you care at least some amount and the other one says I don't know where these unbelievably complicated brains drifting through avoid trying and vain to connect with one another by blindly fleeing words out into the darkness. Every choice of phrasing spelling and tone and timing carries countless synchic signals in context and subtext and more and every listener interprets those signals in their own way. Language isn't a formal system language is glorious chaos. You can never know for sure what any words will mean to anyone or you can do is try to get better at guessing how your words affect people so you can have a chance of finding the ones that will make them feel something like what you want them to feel. Everything else is pointless. I assume you're giving me tips on how you interpret words because you want me to feel less alone. If so then thank you that means a lot but if you're just running my sentences past some mental checklist so you can show off how well you know it then I could care less. Okay so that's ultimately what our goal is is to how to do a better job at building computational systems that try to get better at guessing how their words will affect other people and what other people are meaning by the words that they choose to say. So an interesting thing about human language is it is a system that was constructed by human beings and it's a system that was constructed relatively recently in some sense. So in discussions of artificial intelligence a lot of the time people focus a lot on human brains and the neurons buzzing by and this intelligence that's meant to be inside people's heads but I just wanted to focus for a moment on the role of language. There's actually you know this is kind of controversial but you know it's not necessarily the case that humans are much more intelligent than some of the higher rapes like chimpanzees of bonobos right so chimpanzees and bonobos have been shown to be able to use tools to make plans and in fact chimps have much better short term memory than human beings do. So relative to that if you look through the history of life on earth human beings develop language really recently. How recently we kind of actually don't know because you know there's no fossils that say okay here's a language speaker but you know most people estimate that language arose for human beings sort of you know somewhere in the range of a hundred thousand to a million years ago okay that's a while ago but compared to the process of evolution of life on earth that's kind of blinking an eyelid but that power from this communication between human beings quickly set off our ascendancy over other creatures. So it's kind of interesting that the ultimate power turned out not to be having poisonous fangs or being super fast or super big but having the ability to communicate with other members of your tribe. It was much more recently again that humans developed writing which allowed knowledge to be communicated across distances of time and space and so that's only about five thousand years old the power of writing. So in just a few thousand years the ability to preserve and share knowledge took us from the Bronze Age to the smartphones and tablets of today. So a key question for artificial intelligence and human computer interaction is how to get computers to be able to understand the information conveyed in human languages. Simultaneously artificial intelligence requires computers with a knowledge of people. Fortunately now our AI systems might be able to benefit from a virtuous cycle. We need knowledge to understand language and people well but it's also the case that a lot of that knowledge is contained in language spread out across the books and web pages of the world and that's one of the things we're going to look at in this course is how that we can sort of build on that virtuous cycle. A lot of progress has already been made and I just wanted to very quickly give a sense of that. So in the last decade or so and especially in the last few years with neural methods of machine translation where now in a space where machine translation really works moderately well. So again from the history of the world this is just amazing right for thousands of years learning other people's languages was a human task which required a lot of effort and concentration but now we're in a world where you could just hop on your web browser and think oh I wonder what the news is in Kenya today and you can head off over to a Kenyan website and you can see something like this and you can go and you can then ask Google to translate it for you from Swahili and you know the translation isn't quite perfect but it's you know it's reasonably good so the newspaper Tukko has been informed that local government minister Lingsan, Bella Kanyama and his transport counterparts sitting me died within two separate hours so you know within two separate hours is kind of awkward but essentially we're doing pretty well at getting the information out of this page and so that's quite amazing. The single biggest development in NLP for the last year certainly in the popular media meeting was GPT3 which was a huge new model that was released by OpenAI. What GPT3 is about and why it's great is actually a bit subtle and so I can't really go through all the details of this here but it's exciting because it seems like it's the first step on the path to what we might call universal models where you can train up one extremely large model on something like that library picture I showed before and it just has knowledge of the world, knowledge of human languages, knowledge of how to do tasks and then you can apply it to do all sorts of things so no longer are we building a model to detect spam and then a model to detect pornography and then a model to detect whatever foreign language content and just building all these separate supervised classifiers for every different task we've now just built up a model that understands so exactly what it does is it just predicts following words so on the left it's been told to write about Elon Musk in the style of Dr. Soos and it started off with some text and then it's generating more text and the way it generates more text is literally by just predicting one word at a time following words come to complete its text but this has a very powerful facility because what you can do with GPT3 is you can give it a couple of examples of what you'd like it to do so I can give it some text and say I broke the window, changed it into a question what did I break, I gracefully save the day, I changed it into a question what did I gracefully save so this prompt tells GPT3 what I'm wanting it to do and so then if I give it another statement like I gave John Flowers I can then say GPT3 predict what words come next and it'll follow my prompt and produce who did I give flowers to or I can say I gave her a rose and a guitar and it will follow the idea of the pattern and do who did I give a rose and a guitar too and actually this one model can then do an amazing range of things including many there's quite surprising to do at all to give just one example of that another thing that you can do is get it to translate human language sentences into SQL so this can make it much easier to do CS 145 so having given that a couple of examples of SQL translation of human language text which I'm this time not showing because it won't fit on my slide I can then give it a sentence like how many users have signed up since the start of 2020 and it turns it into SQL or I can give it another query what is the average number of influencers each user subscribe to and again it then converts that into SQL so GPT3 knows a lot about the meaning of language and the meaning of other things like SQL and confluently manipulate it okay so that leads us straight into this top of meaning and how do we represent the meaning of a word well what is meaning well we can look up something like the websteadictionary and say okay the idea that is represented by a word the idea that a person wants to express by using word signs etc the websteadictionary definition is really focused on the word idea somehow but this is pretty close to the commonest way that linguists think about meaning so that they think of word meaning as being a pairing between a word which is a signifier or symbol and the thing that it signifies the signified thing which is an idea or thing so that the meaning of the word chair is the set of things that are chairs and that's referred to as denotational semantics a term that's also used and similarly applied for the semantics of programming languages this model isn't very gently implementable like how do I go from the idea that okay chair means the set of chairs in the world so something I can manipulate meaning within my computers so traditionally the way that meaning has normally been handled in natural language processing systems is to make use of resources like dictionaries and thosauri in particular popular one is wordnet which organized words and terms into both synonym sets words that can mean the same thing and hypenims which correspond to is a relationship and so for the is a relationship you know we can kind of look at the hypenims of panda and a panda is a kind of prosiumid whatever those I get a set's probably with red pandas which is a kind of carnivore which is a kind of placental which is kind of mammal and you sort of head up this hypenim hierarchy so wordnet has been a greater resource for nlp but it's also been highly deficient so it lacks a lot of nuance so for example in wordnet proficient is list as a synonym for good but you know maybe that's sometimes true but it seems like in a lot of context it's not true and you mean something rather different when you say proficient versus good it's limited as a human constructed thosauris so in particular there's lots of words and lots of uses of words that just aren't there including you know anything that is you know sort of more current terminology like wicked is there for the wicked witch but not for more modern colloquial uses ninja certainly isn't there for the kind of description some people make of programmers and it's impossible to keep up to date so it requires a lot of human labor but even when you have that you know it has a set of synonyms but doesn't really have a good sense of words it means something similar so fantastic and great means something similar without really being synonymms and so this idea of meaning similarity is something that'd be really useful to make progress on and where deep learning models excel okay so what's the problem with a lot of traditional nlp well the problem with a lot of traditional nlp is that words are regardless discrete symbols so we have symbols like hotel conference motel our words which in deep learning speak we refer to as a localist representation and that's because if you in statistical or machine learning systems want to represent these symbols that each of them is a separate thing so the standard way of representing them and this is what you do in something like a statistical model if you're building a logistic regression model with words as features is that you represent them as one hot vectors so you have a dimension for each different word so maybe like my example here are my representations as vectors for motel and hotel and so that means that we have to have huge vectors corresponding to the number of words in our vocabulary so the kind of if you had a high school English dictionary it probably had about 250,000 words in it but there are many many more words in the language really so maybe we at least want to have a 500,000 dimensional vector to be able to cope with that okay but the bigger even bigger problem with the discrete symbols is that we don't have this notion of word relationships and similarity so for example in web search if they use assertions for Seattle motel we'd also like to match on documents containing Seattle hotel but our problem is we've got these one hot vectors for the different words and so in a formal mathematical sense these two vectors are orthogonal that there's no natural notion of similarity between them whatsoever well there are some things that we could do but try and do about that and people did do about that in you know before 2010 we could say hey we could use word net synonyms and we had count things that list the synonyms are similar anyway or hey maybe we could somehow build up representations of words that have meaning overlap and people did all of those things but they tended to fail badly from incompleteness so instead what I want to introduce today is the modern deep learning method of doing that where we encode similarity in a real value vector themselves so how do we go about doing that okay and the way we do that is by exploiting this idea called distributional semantics so the idea of distributional semantics is again something that when you first see it maybe feels a little bit crazy because rather than having something like denotational semantics what we're now going to do is say that a words meaning is going to be given by the words that frequently appear close to it. JR Firth was a British linguist from the middle of last century and one of his pity slogans that everyone quotes at this moment is you shall know a word by the company it keeps and so this idea that you can represent a sense for words meaning as a notion of what context that appears in has been a very successful idea one of the most successful ideas that's used throughout statistical and deep learning NLP is actually an interesting idea more philosophically so that there are kind of interesting connections for example in Vidconstein's later writings he became enamored of a use theory of meaning and this is a sentence in some sense a use theory of meaning but whether you know it's the ultimate theory of semantics it's actually still pretty controversial but it proves to be an extremely computational sense of semantics which has just led to it being used everywhere very successfully in deep learning systems so when a word appears in a text it has a context which are the set of words that appear in me and so for a particular word my example here is banking we'll find a bunch of places where banking occurs in texts and we'll collect the sort of nearby words as context words and we'll see say that those words that are appearing in that kind of muddy brown color around banking that those context words well in some sense represent the meaning of the word banking while I'm here let me just mention one distinction that will come up regularly when we're talking about a word in our natural language processing class we sort of have two senses of word which you're referred to as types and tokens so there's a particular instance for words so there's in the first example government debt problems turning into banking crises there's banking there and that's a token of the word banking but then I've collected a bunch of instances of quote unquote the word banking and when I say the word banking and a bunch of examples of it I'm then treating banking as a type which refers to you know the uses and meaning the word banking has across instances okay so what are we going to do with these distributional models of language well what we want to do is we're going based on looking at the words that occur in context as vectors that we want to build up a dense real valued vector for each word that in some sense represents the meaning of that word and the way it all represents the meaning of that word is that this vector will be useful for predicting other words that occur in the context so in this example to keep it manageable on the side vectors are only eight dimensional but in reality we use considerably bigger vectors so a very common size is actually 300 dimensional vectors okay so for each word that's a word type we're going to have a word vector these are also used with other names they refer to as new word representations or for a reason they'll become clear on the next slide they refer to as word embeddings so these are now distributed representation not a localist representation because the meaning of the word banking is spread over all 300 dimensions of the vector okay these are called word embeddings because effectively when we have a whole bunch of words these representations place them all in a high dimensional vector space and so they're embedded into that space now unfortunately human beings are very bad at looking at 300 dimensional vector spaces or even eight dimensional vector spaces so the only thing that I can really display to you here is a two dimensional projection of that space now even that's useful but it's also important to realize that when you're making a two dimensional projection of a 300 dimensional space you're losing almost info all the information in that space and a lot of things will be crushed together that don't actually deserve to be better so here's my word embeddings of course you can't see any of those at all but if I zoom in and then I zoom in further what you'll already see is that the representations we've learnt distributionally do a just a good job at grouping together similar words so in this sort of overall picture I consume into one part of the space is actually the part that's up here in this view of it and it's got words for countries so not only countries generally grouped together even the sort of particular subgroupings of countries make a certain amount of sense and down here we then have nationality words if we go to another part of the space we can see different kind of words so here are verbs and we have ones like come and go a very similar saying and thinking words say think expect a kind of similar and by nearby over in the bottom right we have sort of verbal exileries and copulas so have had has forms of the verb to be and certain contentful verbs are similar to copula verbs because they describe states you know he remained angry he became angry and so they're actually then grouped close together to the word the verb to be so there's a lot of interesting structure in this space that then represents the meaning of words so the algorithm I'm going to introduce now is one that's called word to vac which was introduced by Tamash Mikko often colleagues in 2013 as a framework for learning word vectors and it's sort of a simple and easy to understand place to start so the idea is we have a lot of text from somewhere which we commonly refer to as a corpus of text corpus is just the Latin word for body so it's a body of text and so then we choose a fix vocabulary which will typically be large but nevertheless truncated so we get rid of some of the really rare words so we might say vocabulary size of 400,000 and we then create for ourselves a vector for each word okay so then what we do is we want to work out what's a good vector to for each word and the really interesting thing is that we can learn these word vectors from just a big pile of text by doing this distributional similarity task of being able to predict well what words occur in the context of other words so in particular we're going to iterate through the text and so at any moment we have a center word see and context words outside of it which we'll call oh and then based on the current word vectors we're going to calculate the probability of a context word occurring given the center word according to our current model but then we know that certain words did actually occur in the context of that center word and so what we want to do is then keep adjusting the word vectors to maximize the probability that's assigned to words that actually occur in the context of the center word as we proceed through these texts so to start to make that a bit more concrete this is what we're doing um so we have a piece of text we choose our center word which is here in two and then we say well for model of predicting the probability of context words given the center word and this model will come to in a minute but it's defined in terms of our word vectors so let's see what probability it gives to the words that actually occurred in the to the context of this word huh it gives them some probability but maybe be nice if the probability of assigned was higher so then how can we change our word vectors to raise those probabilities and so we'll do some calculations with into being the center word and then we'll just go on to the next word and then we'll do the same kind of calculations and keep on chunking so the big question then is well what are we doing for working out the probability of a word occurring in the context of the center word and so that's the central part of what we develop as the word to take a check so this is the overall model that we want to use so for each position in our corpus our body of text we want to predict context words within a window of fixize m given the center word wj and we want to become good at doing that so we want to give high probability to words that occur in the context and so what we're going to do is we're going to work out what's formerly the data likelihood as to how good a job we do at predicting words in the context of other words and so formally that likelihood is going to be defined in terms of our word vectors so they're the parameters of our model and it's going to be calculated as taking the product of using each word as the center word and then the product of each word in a window around that of the probability of predicting that context word in the center word and so to learn this model we're going to have an objective function sometimes also called a cost or a loss that we want to optimize and essentially what we want to do is we want to maximize the likelihood of the context we see around center words but following standard practice we slightly fiddle that because rather than dealing with products it's easier to deal with sums and so we work with log likelihood and once we take log likelihood all of our products turn into sums we also work with the average log likelihood so we've got a one-on-t term here for the number of words in the corpus and finally for no particular reason we like to minimize our objective function rather than maximizing it so we stick a minus sign in there and so then by minimizing this objective function j of theta that comes as maximizing our predictive accuracy okay so that's the setup but we still haven't made any progress in how do we calculate the probability of a word occurring in the context given the center word and so the way we're actually going to do that is we have vector representations for each word and we're going to work out the probability simply in terms of the word vectors now at this point there's a little technical point we're actually going to give to each word two word vectors one word vector for when it's used as the center word and a different word vector when it's used as a context word this is done because it just simplifies the math and the optimization so it seems a little bit ugly but actually makes building word vectors a lot easier and really we can come back to them discuss it later but that's what it is and so then once we have these word vectors the equation that we're going to use for giving the probability of a context word appearing given the center word is that we're going to calculate it using the expression in the middle bottom of my slide so let's sort of pull that apart just a little bit more so what we have here with this expression is so for a particular center word and a particular context word oh we're going to look up the vector representation of each word so they're u of o and v of c and so then we're simply going to take the dot product of those two vectors so dot product is a natural measure for similarity between words because in any particular mention positive you'll get some component that adds to the dot product sum if both are negative it'll add a lot to the dot product sum if one's positive and one's negative it'll subtract from the similarity measure if both them zero it won't change the similarity so it sort of seems a sort of plausible idea to just take a dot product and thinking well if two words have a larger dot product that means they're more similar and so then after that we sort of really doing nothing more than okay we want to use dot products to represent word similarity and now let's do the dumbest thing that we know how to turn this into a probability distribution well what do we do well firstly well taking a dot product of two vectors that might come out as positive or negative but well we want to have probabilities we can't have negative probabilities so a simple way to avoid negative probabilities is to exponentiate them because then we know everything is positive and so then we are always getting a positive number in the numerator but for probabilities we also want to have the numbers add up to one so we have a probability distribution so we're just normalizing in the obvious way where we divide through by the sum of the numerator quantity for each different word in the vocabulary and so then necessarily that gives us a probability distribution so all the rest of that that I was just talking through what we're using there is what's called the softmax function so the softmax function will take any Rn vector and turn it into things between 0 to 1 and so we can take numbers and put them through this softmax and turn them into a probability distribution right so the name comes from the fact that it's sort of like a max so because of the fact that we exponentiate that really emphasizes the big contents in the different dimensions of calculating similarity so most of the probability goes to the most similar things and it's called soft because well it doesn't do that absolutely it'll still give some probability to everything that's in the slightest bit similar I mean on the other hand it's a slightly weird name because you know max normally takes a set of things and just returns one the biggest of them whereas the softmax is taking a set of numbers and is scaling them but is returning the whole probability distribution okay so now we have all the pieces of our model and so how do we make our word vectors well the idea of what we want to do is we want to fiddle our word vectors in such a way that we minimize our loss i that we maximize the probability of the words that we actually saw in the context of the center word and so the theta the theta represents all of our model parameters in one very long vector so for our model here the only parameters are our word vectors so we have for each word two vectors its context vector and center vector and each of those is a d-dimensional vector where d might be 300 and we have v many words so we end up with this big huge vector which is 2dv long which if you have a 500,000 vocab times the 300-dimensional the time it's more mapping I can do in my head but it's got millions of millions of parameters so we've got millions of millions of parameters and we somehow want to fiddle them all to maximize the prediction of context words and so the way we're going to do that then is we use calculus so what we want to do is take that math that we've seen previously and say well with this objective function we can work out derivatives and so we can work out where the gradient is so how we can walk downhill to minimize loss so at some point and we can figure out what is downhill and we can then progressively walk downhill and improve our model and so what our job is going to be is to compute all of those vector gradients okay so at this point I then want to kind of show a little bit more as to how we can actually do that and a couple more slides here but maybe I'll just try and jigger things again and move to my interactive whiteboard what we wanted to do right so we had our overall we had our overall j theta that we were wanting to minimize our average neg log likelihood so that was the minus one on t of the sum of t equals one to big t which was our text length and then we were going through the words in each context so we were doing j between m words on each side except itself and then what we wanted to do was in the side there we were then we were working out the log probability of the context word at that position given the word that's in a center position t and so then we converted that into our word vectors by saying that the probability of oh given c is going to be expressed as the soft max of the dot product okay and so now what we want to do is work out the gradient the direction of downhill for this last gen and so the way we're doing that is we're working out the partial derivative of this expression with respect to every parameter in the model and all the parameters in the model are the components the dimensions of the word vectors of every word and so we have the center word vectors and the outside word vectors so here I'm just going to do the center word vectors but on a future homework assignment 2 the outside word vectors will show up and they're kind of similar so what we're doing is we're working out the partial derivative with respect to our center word vector which is you know maybe a 300 dimensional word vector of this probability of oh given c and since we're using log probabilities of the log of this probability of oh given c of this x of u of o t v c over my writing I'll get worse and worse sorry I've already made a mistake having a sum the sum w equals 1 to the vocabulary of the x of u w t v c okay well at this point things start off pretty easy so what we have here is something that's log of a over b so that's easy we can turn this into log a minus log b but before I go further I'll just make a comment at this point you know so at this point my audience divides on into right there are some people in the audience for which maybe a lot of people in the audience this is really elementary math I've seen this a million times before and he isn't even explaining it very well and if you're in that group well feel free to look at your email or the newspaper or whatever else is best suited to you but I think there are also other people in the class who oh the last time I saw calculus was when I was in high school for which that's not the case and so I wanted to spend a few minutes going through this a bit concretely so that to try and get over the idea that you know even though most of deep learning and even word vector learning seems like magic that it's not really magic it's really just doing math and one of the things that we hope is that you do actually understand this math that's being done so I'll keep along and do a bit more of it okay so then what we have is so use this way of writing the log and so then we can say that that expression above equals the partial derivatives with a VC of the log of the numerator log x u o to the vc minus the partial derivative of the log of the denominator so that's then the sum of w equals 1 to v of the x of u w to vc okay so at that point I have my numerator here and my former denominator there so at that point there are spots the first part is the numerator part so the numerator part is really really easy so we have here the log and x but just inverses of each other so they just go away so that becomes the derivative of with respect to VC of just what's left behind which is you use 0 dot product and with VC okay and so the thing to be aware of is you know we're still doing this multivariate calculus so what we have here is calculus with respect to a vector like hopefully you saw some of in math 51 or some other place not high school single variable calculus on the other hand you know to the extent you and half remember some of this stuff most of the time you can just do perfectly well by thinking about what happens with one dimension at a time and it generalizes the multivariable calculus so if about all that you remember of calculus is that d dx of a x equals a really it's the same thing that we're going to be using here that here we have the the outside word dot producted with the VC well at the end of the day that's going to have terms of sort of you 0 component one times the center word component one plus you 0 component 2 plus this is the word component 2 and so we're sort of using this bed over here and so what we're going to be getting out is the u 0 and u 0 1 and the u 0 2 so this will be all that is left with respect to VC 1 when we take its derivative with respect to VC 1 and this term will be the only thing left when we take the derivative with respect to the variable VC 2 so the end result of taking the vector derivative of u 0 dot producted with VC is simply going to be u 0. Okay great so that's progress so then at that point we go on and we say oh damn we still have the the denominator to and that slightly more complex but not so bad so then we try to take the partial derivatives with respect to VC of the log of the denominator. Okay and so then at this point the one tool that we need to know and remember is how to use the chain rule so the chain rule is when you're wanting to work out of having derivatives of compositions of functions so we have f of g of whatever x but here it's going to be VC and so we want to say okay what we have here is we're working out a composition of functions so here's our f and here is our x which is g of VC actually maybe I shouldn't call it x maybe I probably better to call it z or something okay so when we then want to work out the chain rule well what do we do we take the derivative of f at the point z and so at that point we have to actually remember something we have to remember that the derivative of log is the one on x function so this is going to be equal to the one on x for z so that's then going to be one over the sum of w equals 1 to v of x of u to the c multiplied by the derivative of the inner function so so the derivative of the part that is remaining I'm getting this right the sum of oh and there's one trick here at this point we do want to have a change of index so we want to say the sum of x equals 1 to v of x of u of x VC since we can get into trouble if we don't change that variable to be using a different one okay so at that point we're making some progress but we still want to work out the derivative of this and so what we want to do is apply the chain rule once more so now here's our f and in here is our new z equals g of vc and so we then sort of repeat over so we can move the derivative inside a sum always so we're then taking the derivative of this and so then the derivative of x is itself so we're going to just have x of u x tvc times this is sum of x equals 1 to v times the derivative of u x tvc okay and so then this is what we'd worked out before we can just rewrite as u x okay so we're now making progress so if we start putting all of that together what we have is the derivative or the partial derivatives with VC of this log probability right we have the numerator which was just u 0 minus we then had the sum of the numerator sum over x equals 1 to v of x u x tvc times u of x then that was multiplied by our first term that came from the 1 on x which gives you the sum of w equals 1 to v of the x of u w tvc and this is the fact that we changed the variables became important and so by just sort of rewriting that a little we can get that that equals u 0 minus the sum of v equals sorry x equals 1 to v of this x view of x tvc over the sum of w equals 1 to v of x u w tvc times u of x and so at that point this sort of interesting thing has happened that we've ended up getting straight back exactly the softmax formula probability that we saw when we started and we can just rewrite that more conveniently as saying this equals u 0 minus the sum over x equals 1 to v of the probability of x given c times u x and so what we have at that moment is this thing here is an expectation and so this is an an average over all the context vectors weighted by their probability according to the model and so it's always the case with these softmax style models that what you get out for the derivatives is you get observed minus the expected so our model is good if our model on average predicts exactly the word vector that we actually see and so we're going to try and adjust the parameters for our model so it does that much as a ball now I mean we try and make it do it as much as possible I mean of course as you'll find you can never get close right you know if I just say to you okay the word is cross-on which words are going to occur in the context of cross-on I mean you can't answer that there are all sorts of sentences that you could say then involve the word cross-on so actually our particular probability estimates are going to be kind of small but nevertheless we want to sort of fiddle our word vectors to try and make those estimates as high as we possibly can so I've gone on about this stuff a bit but haven't actually sort of shown you any of what actually happens so I just want to quickly show you a bit of that as to what actually happens with word vectors so here's a simple little ipython notebook which is also what you'll be using for assignment one only so in the first cell I import a bunch of stuff so we've got numpy for our vectors matpotlib for part of the packet learns kind of your machine learning swissami knife gen sim is a package that you may well not have seen before it's a package that's often used for word vectors it's not really used for deep learning so this is the only time you'll see it in the class but if you just want a good package for working with word vectors and some other application it's a good one to know about okay so then in my second cell here I'm loading a particular set of word vectors so these are our glove word vectors that we made at stanford in 2014 and I'm loading a hundred dimensional word vectors so that things are a little bit quicker for me while I'm doing things here sort of do this model of bread and croissant well what I've just got here is word vectors so I just wanted to sort of show you that there are word vectors well maybe I should have loaded those word vectors in advance hmm let's see oh okay well I'm in business um okay so right so here are my word vectors for bread and croissant and while I'm seeing them maybe these two words are a bit similar so both of them are negative in the first dimension positive in the second negative in the third positive in the fourth negative in the fifth so it sort of looks like they might have a fair bit of dot product which is kind of what we want because bread and croissant are kind of similar but what we can do is actually ask the model and these are gents in functions now you know what are the most similar words so I can ask for croissant what are the most similar words to that and it will tell me it's things like brioche baguette for cacciate so that's pretty good putting us perhaps a little bit more questionable we can say most similar to the USA and it says Canada or America USA with periods United States that's pretty good most similar to banana I get out coconut mangoes bananas sort of fairly tropical through it great um before finishing though I want to show you something slightly more than just similarity which was one of the amazing things that people observed with these word vectors and that was to say you can actually sort of do arithmetic in this vector space that makes sense and so in particular people suggested this analogy task and so the idea of the analogy task is you should be able to start with a word like king and you should be able to subtract out a male component from it add back in a woman component and then you should be able to ask well what word is over here and what you'd like is that the word over there is queen um and so um this sort of little bit of so we're going to do that um with this sort of same most similar function which is actually more so as well as having positive words you can ask for most similar negative words and you might wonder what's most negatively similar to a banana and you might be thinking oh it's um I don't know um some kind of meat or something um actually that by itself isn't very useful because when you could just ask for um most negatively similar to things you tend to get crazy strings that were found in the data set um that you don't know what they mean if anything um but if we put the two together we can use the most similar function with positives and negatives to do analogies so we're going to say we want a positive king we want to subtract out negatively man we want to then add in positively woman and find out what's most similar to this point in the space so my analogy function does that precisely that by taking um a couple of most similar ones and then subtracting out um the negative one and so we can try out this analogy function so I can do the analogy I show in the picture um with man as to king as woman is uh fight so I'm not saying that's right um yeah man is to king as woman is to blah sorry I haven't done myself um okay man is to king as woman is to queen so um that's great and that um works well I mean and you can do it the sort of other way around king is to man as queen is to woman um if this only worked for that one freakish example um you maybe um wouldn't be very impressed but you know it actually turns out like it's not perfect but you can do all sorts of fun analogies with this and they actually work so you know I could ask for something like an analogy um oh here's a good one um Australia um is to be uh as France is to what um and you can think about what you think the answer that one should be and it comes out as um champagne which is pretty good or I could ask for something like analogy pencil is to sketching as camera is to what um and it says photographing um you can also do the analogies with people um at this point I have to point out that this data was um and the model was built in 2014 so you can't ask anything about um Donald Trump in it well you can he Trump is in there but not as president but I could ask something like analogy of bomb is to Clinton as Reagan is um to what and you can think of what you think is the right um analogy there um the analogy it returns is Nixon um so I guess that depends on what you think of Bill Clinton as to whether you think that was a good analogy or not you can also um do sort of linguistic analogies with it so you can do something like analogy tall is to tallest as long is to what and it does longest so it really just sort of knows a lot about the meaning behavior of words and you know I think when these um methods were first developed and hopefully still for you that you know people were just gobsmacked about how well this actually worked at capturing other enough words and so these word vectors then went everywhere as a new representation that was so powerful for working out word meaning and so that's our starting point for this class and we'll say a bit more about them next time and they're also the basis of what you're looking at for the first assignment can I ask a quick question about the distinction between the two vectors per word yes so um my understanding is that there can be several context words per uh word in the vocabulary or like word in the vocabulary um but then if there's only two vectors I kind of I thought the distinction between the two is that one it's like the actual word and one's like the context word but if there are multiple context words right how do you how do you pick to just two then well so we're doing every one of them right so like um maybe I won't turn back on the screen share but you know we were doing in the objective function there was a sum over you so you've got you know this big corpus of text right so you're taking a sum over every word which is it appearing as the center word and then inside that there's a second sum um which is for each word in the context so you are going to count each word as a context word and so then for one particular term of that objective function you've got a particular context word and a particular um center word but you're then sort of summing over different context words for each center word and then you're summing over all of the decisions of different center words and and to say um a little just a sentence more about having two vectors I mean you know in some senses in ugly detail but it was done to make things sort of simple and fast so you know if you um look at um the math carefully if you sort of treated um this two vectors is the same so if you use the same vectors for center and context and you say okay let's work out the derivatives um things get uglier and the reason that they get uglier is it's okay when I'm iterating over all the choices of um context word oh my god sometimes the context word is going to be the same as the center word and so that messes with working out my derivatives um whereas by taking them as separate vectors that never happens so it's easy um but the kind of interesting thing is you know saying that you have these two different representations sort of just ends up really sort of doing no harm and my wave my hands argument for that is you know since we're kind of moving through each position the corpus one by one you know something a word that is the center word at one moment is going to be the context word at the next moment and the word that was the context word is going to become the center word so you're sort of doing the um the computation both ways in each case and so you should be able to convince yourself that the two representations for the word end up being very similar and they do not not identical for technical reasons of the ends of documents and things like that but very very similar um and so effectively tend to get two very similar representations for each word and we just average them and call that the word vector and so when we use word vectors we just have one vector for each word that makes sense thank you i have a question purely of curiosity so we saw that when we projected the um vectors the word vectors onto the 2d surface we saw like little clusters of where's our similar each other and then later on we saw that um with the analogy thing we kind of see that there's these directional vectors that sort of anything like the ruler of or the CEO of something like that and so I'm wondering is there are other relationships between those relational vectors themselves such as like is the um the ruler of vector sort of similar to the CEO of vector which is very different from like is makes a good sandwich with vector um is there any research on that that's a good question um how will you stump me already in the first lecture uh i mean that yeah i can't actually think of a piece of research and so i'm not sure i have a confident and i'm not sure i have a confident answer i mean it seems like that's a really easy thing to check um with how much you have one of these sets of um word vectors that it seems um like and for any relationship that is represented well enough by word you should be able to see if it comes out kind of similar um huh i mean i'm not sure we can we can look and see yeah that's totally okay just just curious sorry i missed the last little bit your answer to first question so when you wanted to collapse to vectors for the same word did you say you usually take the average um different people have done different things for the most common practice is after you uh you know there's still a bit more i have to cover about running word devec that we didn't really get through today so i've still got a bit more work to do on first day but you know once you run your word devec algorithm and you sort of your output is two vectors for each word and kind of a when it's center and when it's context and so typically people just average those two vectors and say okay that's the representation of the word croissant and that's what appears in the sort of word vectors file like the one i loaded oh thanks so my question is if a word have two different meanings or multiple different meanings can we still represent that's the same single vector? Yes that's a very good question um and actually there is some content on that in first days lecture so i can say more about that um but yeah the first reaction is you kind of should be scared because um something i've said nothing about at all is you know most words especially short common words have lots of meanings so if you have a word like star that can be astronomical object or it can be you know a film star a Hollywood star or it can be something like the gold stars that you've got an elementary school and we've just taking all those uses of the word star and collapsing them together um into one word vector um and you might think that's really crazy and bad um but actually turns out to work rather well um maybe i won't go through all of that um right now because there is actually stuff on that on first days lecture oh i see thanks you can just add up the slides for next time oh white hey i know this makes me seem as good as but i guess a lot of us were also taking this course because of the height, good speed, AI, speech recognition and my basic question is maybe two basic is do we look at how to implement or do we look at the stack of like some of the lecture something prox speech to uh contact actions in this course was it just the priority uh understanding so this is an unusual content an unusual quarter um but for this quarter there's a very clear answer which is um this quarter um there's also a speech class being taught which is CS224S um a speech class being taught by Andrew Mars and you know this is a class that's been more regularly offered sometimes it's only been offered every third year um but it's being offered right now so if what you want to do is learn about speech recognition and learn about sort of methods for building dialogue systems um you should do CS224S um so you know for this class in general um the vast bulk of this class is working with text and doing various kinds of text analysis and understanding so we do tasks like some of the ones I've mentioned we do machine translation um we um do question answering um we look at how to parse this structure of sentences and things like that you know in other years I sometimes say a little bit about speech um but since this quarter there's a whole different class that's focused on speech that's similar but silly. Well that's what I'm talking about right now together uh the part of pardon and we're in each audio like I guess I guess I guess I guess I can you're in your focus more on speech feeling standing I guess I guess I guess I'll be I'll be this is the question I'm going to have on I'm now getting a bad echo I'm not sure if that's my fault or your fault but I'm anyway um um anyway answer yeah so the speech class does a mix of stuff so I mean the sort of pure speech problems classically have been um doing speech recognitions are going from a speech signal to text and doing text to speech going from text to a speech signal and both of those are problems which are now normally done including by the cell phone that sits in your pocket um using your networks and so it covers both of those but then between that um the class covers quite a bit and in particular it starts off with um looking at building dialogue systems so this is sort of something like Alexa Google Assistance series as to well assuming you have a speech recognition a text to speech system then you do have text in and text out what are the kind of ways that people go about building um um dialogue systems like the ones that I just mentioned um actually how to question so I think there is some people in the chat noticing that the like opposites were really near to each other which was kind of odd but I was also wondering um what about like positive and negative uh violins or like offense um is that captured well in this type of model or is it like not captured well like we're like with the opposites how those weren't really so the short answer is for both of those and so there's this is a good question a good observation and the short answer is no both of those are captured really really badly I mean there's there's a definition um you know when I say really really badly I mean what I mean is if that's what you want to focus on um you've got problems I mean it's not that the algorithm doesn't work so precisely what you find um is that you know antennas generally occur in very similar topics because you know whether it's um saying you know John is really tall or John is really short or that movie was fantastic or that movie was terrible right you get antennas occurring in the same context so because of that their vectors are very similar and similarly for sort of affect and sentiment-based words well like make um great and terrible example their contexts are similar um therefore um that if you're just learning this kind of predict words in context models um that no that's not captured now that's not the end of the story I mean you know absolutely people wanted to use neural networks for sentiment and other kinds of sort of connotation affect and there are very good ways of doing that but somehow you have to do something more than simply predicting words in context because that's not sufficient to um capture that dimension um more on that later but this happened to like adjectives too like very basic adjectives like so and like not because those would like appear like some like context right what was your first example before not like so this is so cool yeah so that's actually a good question as well so yeah so there are these very common words there are commonly referred to as function words by linguists which know includes ones like um so and not but other ones like and and prepositions like you know two and on um you sort of might suspect that the word vectors for those don't work out very well because they occur in all kinds of different contexts and they're not very distinct from each other in many cases and to a first approximation I think that's true and part of why I didn't use those as examples in my slides yeah but you know at the end of the day we do build up vector representations of those words too and you'll see in a few um lectures time when we start building what we call language models that actually they do do a great job in those words as well I mean to explain what I'm meaning there I mean you know another feature of the word to vector model is it actually ignore the position of words right so it's said I'm going to predict every word around the center word but you know I'm predicting it in the same way I'm not predicting differently the word before me or versus the word after me or the word two away in either direction right they're all just predicted the same by that one um probability function and so if that's all you've got that sort of destroys your ability to do a good job at um capturing these sort of common more grammatical words like so not an an um but we build slightly different models that are more sensitive to the structure of sentences and then we start doing a good job on those two okay thank you I had a question about um the characterization of word to vector um because I read all these such and it seems to characterize architecture as well but skip very model which was slightly different from how I was presented in the module so at least like two hundred and three hundred years ago or yeah so I've I've still gotten more to say so I'm stationed first day um for more stuff on word vectors um you know so word to vac is kind of a framework for building word vectors and that there are sort of several variant precise algorithms within the framework and you know one of them is have whether you're predicting the context words or whether you're predicting the center word um so the model I showed was predicting the context words so it was the skip gram model but then there's sort of a detail of how in particular do you do the optimization and what I presented was the sort of easiest way to do it which is naive optimization with the equation the softmax equation for word vectors um it turns out that that naive optimization is sort of exneedlessly expensive and people have come up with um faster ways of doing it in particular um the commonest thing you see is what's called skip gram with negative sound playing and the negative sound playing is then sort of a much more efficient way to estimate things and I'll mention that on Thursday. Right okay thank you. Who's asking for more information about how word vectors are constructed uh beyond the summary of random initialization and then gradient based uh iterative operator optimization. Yeah um so I sort of will do a bit more connecting this together um in the Thursday lecture I guess there's sort of I mean so much I'm gonna fit in the first class um but the picture the picture is essentially the picture I showed the pieces of so to learn word vectors you start off by having a vector for each word type both for context and outside and those vectors you to initialize randomly um so that you just put small little numbers that are randomly generated in each vector component and that's just your starting point and so from there on you're using an iterative algorithm where you're progressively updating those word vectors so they do a better job at predicting which words appear in the context of other words and the way that we're going to do that is by using um the gradients that I was sort of starting to show how to calculate and then you know once you have a gradient you can walk in the opposite direction of the gradient and you're then walking downhill I you're minimizing your loss and we're going to sort of do lots of that until our word vectors get as good as possible so you know um it's really all math but in some sense you know word vector learning is sort of miraculous since you do literally just start off with completely random word vectors and run this algorithm of predicting words for a long time and out of nothing emerges these word vectors that represent meaning well", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 15.46, "text": " Hi everybody. Welcome to Stanford CS224N, also known as Ling284, natural language processing", "tokens": [2421, 2201, 13, 4027, 281, 20374, 9460, 17, 7911, 45, 11, 611, 2570, 382, 20977, 11205, 19, 11, 3303, 2856, 9007], "temperature": 0.0, "avg_logprob": -0.2485966546194894, "compression_ratio": 1.3535353535353536, "no_speech_prob": 0.06195720657706261}, {"id": 1, "seek": 0, "start": 15.46, "end": 22.2, "text": " with deep learning. I'm Christopher Manning and I'm the main instructor for this class.", "tokens": [365, 2452, 2539, 13, 286, 478, 20649, 2458, 773, 293, 286, 478, 264, 2135, 18499, 337, 341, 1508, 13], "temperature": 0.0, "avg_logprob": -0.2485966546194894, "compression_ratio": 1.3535353535353536, "no_speech_prob": 0.06195720657706261}, {"id": 2, "seek": 0, "start": 22.2, "end": 28.82, "text": " So what we hope to do today is to dive right in. So I'm going to spend about 10 minutes", "tokens": [407, 437, 321, 1454, 281, 360, 965, 307, 281, 9192, 558, 294, 13, 407, 286, 478, 516, 281, 3496, 466, 1266, 2077], "temperature": 0.0, "avg_logprob": -0.2485966546194894, "compression_ratio": 1.3535353535353536, "no_speech_prob": 0.06195720657706261}, {"id": 3, "seek": 2882, "start": 28.82, "end": 34.46, "text": " talking about the course. And then we're going to get straight into content for reasons", "tokens": [1417, 466, 264, 1164, 13, 400, 550, 321, 434, 516, 281, 483, 2997, 666, 2701, 337, 4112], "temperature": 0.0, "avg_logprob": -0.16037340725169463, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0002517033426556736}, {"id": 4, "seek": 2882, "start": 34.46, "end": 40.02, "text": " I'll explain in a minute. So we'll talk about human language and word meaning. I'll", "tokens": [286, 603, 2903, 294, 257, 3456, 13, 407, 321, 603, 751, 466, 1952, 2856, 293, 1349, 3620, 13, 286, 603], "temperature": 0.0, "avg_logprob": -0.16037340725169463, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0002517033426556736}, {"id": 5, "seek": 2882, "start": 40.02, "end": 45.379999999999995, "text": " then introduce the ideas of the word to veck algorithm for learning word meaning.", "tokens": [550, 5366, 264, 3487, 295, 264, 1349, 281, 1241, 547, 9284, 337, 2539, 1349, 3620, 13], "temperature": 0.0, "avg_logprob": -0.16037340725169463, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0002517033426556736}, {"id": 6, "seek": 2882, "start": 45.379999999999995, "end": 50.7, "text": " And then going from there we'll kind of concretely work through how you can work out objective", "tokens": [400, 550, 516, 490, 456, 321, 603, 733, 295, 39481, 736, 589, 807, 577, 291, 393, 589, 484, 10024], "temperature": 0.0, "avg_logprob": -0.16037340725169463, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0002517033426556736}, {"id": 7, "seek": 2882, "start": 50.7, "end": 56.260000000000005, "text": " function gradients with respect to the word to veck algorithm and say a teeny bit about", "tokens": [2445, 2771, 2448, 365, 3104, 281, 264, 1349, 281, 1241, 547, 9284, 293, 584, 257, 48232, 857, 466], "temperature": 0.0, "avg_logprob": -0.16037340725169463, "compression_ratio": 1.751004016064257, "no_speech_prob": 0.0002517033426556736}, {"id": 8, "seek": 5626, "start": 56.26, "end": 61.699999999999996, "text": " how optimization works. And then right at the end of the class I then want to spend a little", "tokens": [577, 19618, 1985, 13, 400, 550, 558, 412, 264, 917, 295, 264, 1508, 286, 550, 528, 281, 3496, 257, 707], "temperature": 0.0, "avg_logprob": -0.0972600263707778, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.0004629798058886081}, {"id": 9, "seek": 5626, "start": 61.699999999999996, "end": 68.42, "text": " bit of time giving you a sense of how these word vectors work and what you can do with them.", "tokens": [857, 295, 565, 2902, 291, 257, 2020, 295, 577, 613, 1349, 18875, 589, 293, 437, 291, 393, 360, 365, 552, 13], "temperature": 0.0, "avg_logprob": -0.0972600263707778, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.0004629798058886081}, {"id": 10, "seek": 5626, "start": 68.42, "end": 75.96, "text": " So really the key learning for today is I want to give you a sense of how amazing deep", "tokens": [407, 534, 264, 2141, 2539, 337, 965, 307, 286, 528, 281, 976, 291, 257, 2020, 295, 577, 2243, 2452], "temperature": 0.0, "avg_logprob": -0.0972600263707778, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.0004629798058886081}, {"id": 11, "seek": 5626, "start": 75.96, "end": 81.53999999999999, "text": " learning word vectors are. So we have this really surprising result that word meaning", "tokens": [2539, 1349, 18875, 366, 13, 407, 321, 362, 341, 534, 8830, 1874, 300, 1349, 3620], "temperature": 0.0, "avg_logprob": -0.0972600263707778, "compression_ratio": 1.7211538461538463, "no_speech_prob": 0.0004629798058886081}, {"id": 12, "seek": 8154, "start": 81.54, "end": 88.5, "text": " can be represented not perfectly but really rather well by a large vector of real numbers.", "tokens": [393, 312, 10379, 406, 6239, 457, 534, 2831, 731, 538, 257, 2416, 8062, 295, 957, 3547, 13], "temperature": 0.0, "avg_logprob": -0.15018031763475995, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.00014798238407820463}, {"id": 13, "seek": 8154, "start": 88.5, "end": 93.22, "text": " And you know that's sort of in a way a common place of the last decade of deep learning", "tokens": [400, 291, 458, 300, 311, 1333, 295, 294, 257, 636, 257, 2689, 1081, 295, 264, 1036, 10378, 295, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.15018031763475995, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.00014798238407820463}, {"id": 14, "seek": 8154, "start": 93.22, "end": 100.22, "text": " but it flies in the face of thousands of years of tradition and it's really rather an unexpected", "tokens": [457, 309, 17414, 294, 264, 1851, 295, 5383, 295, 924, 295, 6994, 293, 309, 311, 534, 2831, 364, 13106], "temperature": 0.0, "avg_logprob": -0.15018031763475995, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.00014798238407820463}, {"id": 15, "seek": 8154, "start": 100.22, "end": 107.5, "text": " result to start focusing on. Okay so quickly what do we hope to teach in this course. So", "tokens": [1874, 281, 722, 8416, 322, 13, 1033, 370, 2661, 437, 360, 321, 1454, 281, 2924, 294, 341, 1164, 13, 407], "temperature": 0.0, "avg_logprob": -0.15018031763475995, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.00014798238407820463}, {"id": 16, "seek": 10750, "start": 107.5, "end": 114.9, "text": " we've got three primary goals. The first is to teach you the foundations are a good", "tokens": [321, 600, 658, 1045, 6194, 5493, 13, 440, 700, 307, 281, 2924, 291, 264, 22467, 366, 257, 665], "temperature": 0.0, "avg_logprob": -0.14094950204872222, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.461297794478014e-05}, {"id": 17, "seek": 10750, "start": 114.9, "end": 120.3, "text": " deep understanding of the effect of modern methods for deep learning applied to NLP.", "tokens": [2452, 3701, 295, 264, 1802, 295, 4363, 7150, 337, 2452, 2539, 6456, 281, 426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.14094950204872222, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.461297794478014e-05}, {"id": 18, "seek": 10750, "start": 120.3, "end": 125.74000000000001, "text": " So we are going to start with and go through the basics and then go on to key methods", "tokens": [407, 321, 366, 516, 281, 722, 365, 293, 352, 807, 264, 14688, 293, 550, 352, 322, 281, 2141, 7150], "temperature": 0.0, "avg_logprob": -0.14094950204872222, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.461297794478014e-05}, {"id": 19, "seek": 10750, "start": 125.74000000000001, "end": 132.06, "text": " that are used in NLP, your current networks, attention, transformers and things like that.", "tokens": [300, 366, 1143, 294, 426, 45196, 11, 428, 2190, 9590, 11, 3202, 11, 4088, 433, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.14094950204872222, "compression_ratio": 1.6046511627906976, "no_speech_prob": 4.461297794478014e-05}, {"id": 20, "seek": 13206, "start": 132.06, "end": 138.1, "text": " You want to do something more than just that but also like to give you some sense of a", "tokens": [509, 528, 281, 360, 746, 544, 813, 445, 300, 457, 611, 411, 281, 976, 291, 512, 2020, 295, 257], "temperature": 0.0, "avg_logprob": -0.1394177662957575, "compression_ratio": 1.7088122605363985, "no_speech_prob": 6.081789979361929e-05}, {"id": 21, "seek": 13206, "start": 138.1, "end": 143.26, "text": " big picture understanding of human languages and what are the reasons for why they're actually", "tokens": [955, 3036, 3701, 295, 1952, 8650, 293, 437, 366, 264, 4112, 337, 983, 436, 434, 767], "temperature": 0.0, "avg_logprob": -0.1394177662957575, "compression_ratio": 1.7088122605363985, "no_speech_prob": 6.081789979361929e-05}, {"id": 22, "seek": 13206, "start": 143.26, "end": 149.82, "text": " quite difficult to understand and produce even though humans seem to do it easily. Now", "tokens": [1596, 2252, 281, 1223, 293, 5258, 754, 1673, 6255, 1643, 281, 360, 309, 3612, 13, 823], "temperature": 0.0, "avg_logprob": -0.1394177662957575, "compression_ratio": 1.7088122605363985, "no_speech_prob": 6.081789979361929e-05}, {"id": 23, "seek": 13206, "start": 149.82, "end": 153.82, "text": " obviously if you really want to learn a lot about this topic you should enroll in and", "tokens": [2745, 498, 291, 534, 528, 281, 1466, 257, 688, 466, 341, 4829, 291, 820, 12266, 294, 293], "temperature": 0.0, "avg_logprob": -0.1394177662957575, "compression_ratio": 1.7088122605363985, "no_speech_prob": 6.081789979361929e-05}, {"id": 24, "seek": 13206, "start": 153.82, "end": 158.42000000000002, "text": " go and start doing some classes in the linguistics department but nevertheless for a lot of", "tokens": [352, 293, 722, 884, 512, 5359, 294, 264, 21766, 6006, 5882, 457, 26924, 337, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.1394177662957575, "compression_ratio": 1.7088122605363985, "no_speech_prob": 6.081789979361929e-05}, {"id": 25, "seek": 15842, "start": 158.42, "end": 164.66, "text": " you this is the only human language content you'll see during your master's degree or whatever", "tokens": [291, 341, 307, 264, 787, 1952, 2856, 2701, 291, 603, 536, 1830, 428, 4505, 311, 4314, 420, 2035], "temperature": 0.0, "avg_logprob": -0.1348858975816047, "compression_ratio": 1.55, "no_speech_prob": 0.00020551844500005245}, {"id": 26, "seek": 15842, "start": 164.66, "end": 171.54, "text": " and so we do hope to spend a bit of time on that starting today. And then finally we want", "tokens": [293, 370, 321, 360, 1454, 281, 3496, 257, 857, 295, 565, 322, 300, 2891, 965, 13, 400, 550, 2721, 321, 528], "temperature": 0.0, "avg_logprob": -0.1348858975816047, "compression_ratio": 1.55, "no_speech_prob": 0.00020551844500005245}, {"id": 27, "seek": 15842, "start": 171.54, "end": 177.22, "text": " to give you an understanding of an ability to build systems in PyTorch for some of the", "tokens": [281, 976, 291, 364, 3701, 295, 364, 3485, 281, 1322, 3652, 294, 9953, 51, 284, 339, 337, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.1348858975816047, "compression_ratio": 1.55, "no_speech_prob": 0.00020551844500005245}, {"id": 28, "seek": 15842, "start": 177.22, "end": 183.85999999999999, "text": " major problems in NLP so we'll look at learning word meanings dependency parsing machine translation", "tokens": [2563, 2740, 294, 426, 45196, 370, 321, 603, 574, 412, 2539, 1349, 28138, 33621, 21156, 278, 3479, 12853], "temperature": 0.0, "avg_logprob": -0.1348858975816047, "compression_ratio": 1.55, "no_speech_prob": 0.00020551844500005245}, {"id": 29, "seek": 18386, "start": 183.86, "end": 194.42000000000002, "text": " question answering. Let's dive into human language. Once part of time I had a lot longer introduction", "tokens": [1168, 13430, 13, 961, 311, 9192, 666, 1952, 2856, 13, 3443, 644, 295, 565, 286, 632, 257, 688, 2854, 9339], "temperature": 0.0, "avg_logprob": -0.1257047359759991, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00016638405213598162}, {"id": 30, "seek": 18386, "start": 194.42000000000002, "end": 201.14000000000001, "text": " that gave lots of examples about how human languages can be misunderstood and complex. I'll show", "tokens": [300, 2729, 3195, 295, 5110, 466, 577, 1952, 8650, 393, 312, 33870, 293, 3997, 13, 286, 603, 855], "temperature": 0.0, "avg_logprob": -0.1257047359759991, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00016638405213598162}, {"id": 31, "seek": 18386, "start": 201.14000000000001, "end": 209.38000000000002, "text": " a few of those examples in later lectures but since right for today we're going to be focused", "tokens": [257, 1326, 295, 729, 5110, 294, 1780, 16564, 457, 1670, 558, 337, 965, 321, 434, 516, 281, 312, 5178], "temperature": 0.0, "avg_logprob": -0.1257047359759991, "compression_ratio": 1.5208333333333333, "no_speech_prob": 0.00016638405213598162}, {"id": 32, "seek": 20938, "start": 209.38, "end": 218.66, "text": " on word meaning I thought I'd just give one example which comes from a very nice xkcd cartoon", "tokens": [322, 1349, 3620, 286, 1194, 286, 1116, 445, 976, 472, 1365, 597, 1487, 490, 257, 588, 1481, 2031, 74, 66, 67, 18569], "temperature": 0.0, "avg_logprob": -0.11090563237667084, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.0003491290262900293}, {"id": 33, "seek": 20938, "start": 218.66, "end": 226.98, "text": " and that isn't sort of about some of the sort of syntactic ambiguities of sentences but instead", "tokens": [293, 300, 1943, 380, 1333, 295, 466, 512, 295, 264, 1333, 295, 23980, 19892, 40390, 1088, 295, 16579, 457, 2602], "temperature": 0.0, "avg_logprob": -0.11090563237667084, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.0003491290262900293}, {"id": 34, "seek": 20938, "start": 226.98, "end": 232.9, "text": " it's really emphasizing the important point that language is a social system constructed", "tokens": [309, 311, 534, 45550, 264, 1021, 935, 300, 2856, 307, 257, 2093, 1185, 17083], "temperature": 0.0, "avg_logprob": -0.11090563237667084, "compression_ratio": 1.4946236559139785, "no_speech_prob": 0.0003491290262900293}, {"id": 35, "seek": 23290, "start": 232.9, "end": 241.3, "text": " and interpreted by people and that's part of how and it changes as people decide to adapt its", "tokens": [293, 26749, 538, 561, 293, 300, 311, 644, 295, 577, 293, 309, 2962, 382, 561, 4536, 281, 6231, 1080], "temperature": 0.0, "avg_logprob": -0.11142060113331628, "compression_ratio": 1.64, "no_speech_prob": 9.38430821406655e-05}, {"id": 36, "seek": 23290, "start": 241.3, "end": 247.14000000000001, "text": " construction and that's part of the reason why human languages are greatest and adaptive system", "tokens": [6435, 293, 300, 311, 644, 295, 264, 1778, 983, 1952, 8650, 366, 6636, 293, 27912, 1185], "temperature": 0.0, "avg_logprob": -0.11142060113331628, "compression_ratio": 1.64, "no_speech_prob": 9.38430821406655e-05}, {"id": 37, "seek": 23290, "start": 247.14000000000001, "end": 255.94, "text": " for human beings but difficult as a system or our computers to understand to this day. So in this", "tokens": [337, 1952, 8958, 457, 2252, 382, 257, 1185, 420, 527, 10807, 281, 1223, 281, 341, 786, 13, 407, 294, 341], "temperature": 0.0, "avg_logprob": -0.11142060113331628, "compression_ratio": 1.64, "no_speech_prob": 9.38430821406655e-05}, {"id": 38, "seek": 25594, "start": 255.94, "end": 263.06, "text": " conversation between the two women one says anyway I could care less and the other says I think", "tokens": [3761, 1296, 264, 732, 2266, 472, 1619, 4033, 286, 727, 1127, 1570, 293, 264, 661, 1619, 286, 519], "temperature": 0.0, "avg_logprob": -0.0789080137087975, "compression_ratio": 1.7671232876712328, "no_speech_prob": 8.317976607941091e-05}, {"id": 39, "seek": 25594, "start": 263.06, "end": 269.14, "text": " you mean you couldn't care less saying you could care less implies you care at least some amount", "tokens": [291, 914, 291, 2809, 380, 1127, 1570, 1566, 291, 727, 1127, 1570, 18779, 291, 1127, 412, 1935, 512, 2372], "temperature": 0.0, "avg_logprob": -0.0789080137087975, "compression_ratio": 1.7671232876712328, "no_speech_prob": 8.317976607941091e-05}, {"id": 40, "seek": 25594, "start": 269.78, "end": 275.94, "text": " and the other one says I don't know where these unbelievably complicated brains drifting through", "tokens": [293, 264, 661, 472, 1619, 286, 500, 380, 458, 689, 613, 43593, 6179, 15442, 37973, 807], "temperature": 0.0, "avg_logprob": -0.0789080137087975, "compression_ratio": 1.7671232876712328, "no_speech_prob": 8.317976607941091e-05}, {"id": 41, "seek": 25594, "start": 275.94, "end": 282.42, "text": " avoid trying and vain to connect with one another by blindly fleeing words out into the darkness.", "tokens": [5042, 1382, 293, 22240, 281, 1745, 365, 472, 1071, 538, 47744, 41885, 2283, 484, 666, 264, 11262, 13], "temperature": 0.0, "avg_logprob": -0.0789080137087975, "compression_ratio": 1.7671232876712328, "no_speech_prob": 8.317976607941091e-05}, {"id": 42, "seek": 28242, "start": 282.42, "end": 289.46000000000004, "text": " Every choice of phrasing spelling and tone and timing carries countless synchic signals", "tokens": [2048, 3922, 295, 7636, 3349, 22254, 293, 8027, 293, 10822, 16402, 19223, 5451, 339, 299, 12354], "temperature": 0.0, "avg_logprob": -0.14193526903788248, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00024856720119714737}, {"id": 43, "seek": 28242, "start": 289.46000000000004, "end": 296.1, "text": " in context and subtext and more and every listener interprets those signals in their own way.", "tokens": [294, 4319, 293, 1422, 25111, 293, 544, 293, 633, 31569, 17489, 1373, 729, 12354, 294, 641, 1065, 636, 13], "temperature": 0.0, "avg_logprob": -0.14193526903788248, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00024856720119714737}, {"id": 44, "seek": 28242, "start": 296.98, "end": 303.78000000000003, "text": " Language isn't a formal system language is glorious chaos. You can never know for sure what any", "tokens": [24445, 1943, 380, 257, 9860, 1185, 2856, 307, 24026, 14158, 13, 509, 393, 1128, 458, 337, 988, 437, 604], "temperature": 0.0, "avg_logprob": -0.14193526903788248, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00024856720119714737}, {"id": 45, "seek": 28242, "start": 303.78000000000003, "end": 310.1, "text": " words will mean to anyone or you can do is try to get better at guessing how your words affect", "tokens": [2283, 486, 914, 281, 2878, 420, 291, 393, 360, 307, 853, 281, 483, 1101, 412, 17939, 577, 428, 2283, 3345], "temperature": 0.0, "avg_logprob": -0.14193526903788248, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00024856720119714737}, {"id": 46, "seek": 31010, "start": 310.1, "end": 315.62, "text": " people so you can have a chance of finding the ones that will make them feel something like what", "tokens": [561, 370, 291, 393, 362, 257, 2931, 295, 5006, 264, 2306, 300, 486, 652, 552, 841, 746, 411, 437], "temperature": 0.0, "avg_logprob": -0.052581926410117844, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.00024082361778710037}, {"id": 47, "seek": 31010, "start": 315.62, "end": 321.70000000000005, "text": " you want them to feel. Everything else is pointless. I assume you're giving me tips on how you", "tokens": [291, 528, 552, 281, 841, 13, 5471, 1646, 307, 32824, 13, 286, 6552, 291, 434, 2902, 385, 6082, 322, 577, 291], "temperature": 0.0, "avg_logprob": -0.052581926410117844, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.00024082361778710037}, {"id": 48, "seek": 31010, "start": 321.70000000000005, "end": 328.74, "text": " interpret words because you want me to feel less alone. If so then thank you that means a lot", "tokens": [7302, 2283, 570, 291, 528, 385, 281, 841, 1570, 3312, 13, 759, 370, 550, 1309, 291, 300, 1355, 257, 688], "temperature": 0.0, "avg_logprob": -0.052581926410117844, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.00024082361778710037}, {"id": 49, "seek": 31010, "start": 329.62, "end": 335.22, "text": " but if you're just running my sentences past some mental checklist so you can show off how well", "tokens": [457, 498, 291, 434, 445, 2614, 452, 16579, 1791, 512, 4973, 30357, 370, 291, 393, 855, 766, 577, 731], "temperature": 0.0, "avg_logprob": -0.052581926410117844, "compression_ratio": 1.7008928571428572, "no_speech_prob": 0.00024082361778710037}, {"id": 50, "seek": 33522, "start": 335.22, "end": 343.86, "text": " you know it then I could care less. Okay so that's ultimately what our goal is is to how to do a better", "tokens": [291, 458, 309, 550, 286, 727, 1127, 1570, 13, 1033, 370, 300, 311, 6284, 437, 527, 3387, 307, 307, 281, 577, 281, 360, 257, 1101], "temperature": 0.0, "avg_logprob": -0.10242877789397738, "compression_ratio": 1.5621621621621622, "no_speech_prob": 8.444060949841514e-05}, {"id": 51, "seek": 33522, "start": 343.86, "end": 353.62, "text": " job at building computational systems that try to get better at guessing how their words will affect", "tokens": [1691, 412, 2390, 28270, 3652, 300, 853, 281, 483, 1101, 412, 17939, 577, 641, 2283, 486, 3345], "temperature": 0.0, "avg_logprob": -0.10242877789397738, "compression_ratio": 1.5621621621621622, "no_speech_prob": 8.444060949841514e-05}, {"id": 52, "seek": 33522, "start": 353.62, "end": 358.34000000000003, "text": " other people and what other people are meaning by the words that they choose to say.", "tokens": [661, 561, 293, 437, 661, 561, 366, 3620, 538, 264, 2283, 300, 436, 2826, 281, 584, 13], "temperature": 0.0, "avg_logprob": -0.10242877789397738, "compression_ratio": 1.5621621621621622, "no_speech_prob": 8.444060949841514e-05}, {"id": 53, "seek": 35834, "start": 358.34, "end": 370.41999999999996, "text": " So an interesting thing about human language is it is a system that was constructed by human beings", "tokens": [407, 364, 1880, 551, 466, 1952, 2856, 307, 309, 307, 257, 1185, 300, 390, 17083, 538, 1952, 8958], "temperature": 0.0, "avg_logprob": -0.12958295004708426, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.00014838571951258928}, {"id": 54, "seek": 35834, "start": 371.94, "end": 380.82, "text": " and it's a system that was constructed relatively recently in some sense. So in discussions of", "tokens": [293, 309, 311, 257, 1185, 300, 390, 17083, 7226, 3938, 294, 512, 2020, 13, 407, 294, 11088, 295], "temperature": 0.0, "avg_logprob": -0.12958295004708426, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.00014838571951258928}, {"id": 55, "seek": 38082, "start": 380.82, "end": 389.46, "text": " artificial intelligence a lot of the time people focus a lot on human brains and the neurons buzzing", "tokens": [11677, 7599, 257, 688, 295, 264, 565, 561, 1879, 257, 688, 322, 1952, 15442, 293, 264, 22027, 29659], "temperature": 0.0, "avg_logprob": -0.08326537749346564, "compression_ratio": 1.71875, "no_speech_prob": 8.436288044322282e-05}, {"id": 56, "seek": 38082, "start": 389.46, "end": 396.65999999999997, "text": " by and this intelligence that's meant to be inside people's heads but I just wanted to focus for a", "tokens": [538, 293, 341, 7599, 300, 311, 4140, 281, 312, 1854, 561, 311, 8050, 457, 286, 445, 1415, 281, 1879, 337, 257], "temperature": 0.0, "avg_logprob": -0.08326537749346564, "compression_ratio": 1.71875, "no_speech_prob": 8.436288044322282e-05}, {"id": 57, "seek": 38082, "start": 396.65999999999997, "end": 403.94, "text": " moment on the role of language. There's actually you know this is kind of controversial but", "tokens": [1623, 322, 264, 3090, 295, 2856, 13, 821, 311, 767, 291, 458, 341, 307, 733, 295, 17323, 457], "temperature": 0.0, "avg_logprob": -0.08326537749346564, "compression_ratio": 1.71875, "no_speech_prob": 8.436288044322282e-05}, {"id": 58, "seek": 38082, "start": 404.65999999999997, "end": 409.78, "text": " you know it's not necessarily the case that humans are much more intelligent than some of the", "tokens": [291, 458, 309, 311, 406, 4725, 264, 1389, 300, 6255, 366, 709, 544, 13232, 813, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.08326537749346564, "compression_ratio": 1.71875, "no_speech_prob": 8.436288044322282e-05}, {"id": 59, "seek": 40978, "start": 409.78, "end": 416.65999999999997, "text": " higher rapes like chimpanzees of bonobos right so chimpanzees and bonobos have been shown to be", "tokens": [2946, 5099, 279, 411, 18375, 48410, 279, 295, 4428, 996, 329, 558, 370, 18375, 48410, 279, 293, 4428, 996, 329, 362, 668, 4898, 281, 312], "temperature": 0.0, "avg_logprob": -0.13179691716244346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 7.821875624358654e-05}, {"id": 60, "seek": 40978, "start": 416.65999999999997, "end": 423.46, "text": " able to use tools to make plans and in fact chimps have much better short term memory than human beings", "tokens": [1075, 281, 764, 3873, 281, 652, 5482, 293, 294, 1186, 18375, 1878, 362, 709, 1101, 2099, 1433, 4675, 813, 1952, 8958], "temperature": 0.0, "avg_logprob": -0.13179691716244346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 7.821875624358654e-05}, {"id": 61, "seek": 40978, "start": 423.46, "end": 431.38, "text": " do. So relative to that if you look through the history of life on earth human beings develop language", "tokens": [360, 13, 407, 4972, 281, 300, 498, 291, 574, 807, 264, 2503, 295, 993, 322, 4120, 1952, 8958, 1499, 2856], "temperature": 0.0, "avg_logprob": -0.13179691716244346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 7.821875624358654e-05}, {"id": 62, "seek": 40978, "start": 431.38, "end": 437.94, "text": " really recently. How recently we kind of actually don't know because you know there's no fossils", "tokens": [534, 3938, 13, 1012, 3938, 321, 733, 295, 767, 500, 380, 458, 570, 291, 458, 456, 311, 572, 39159], "temperature": 0.0, "avg_logprob": -0.13179691716244346, "compression_ratio": 1.6835443037974684, "no_speech_prob": 7.821875624358654e-05}, {"id": 63, "seek": 43794, "start": 437.94, "end": 445.54, "text": " that say okay here's a language speaker but you know most people estimate that language arose", "tokens": [300, 584, 1392, 510, 311, 257, 2856, 8145, 457, 291, 458, 881, 561, 12539, 300, 2856, 37192], "temperature": 0.0, "avg_logprob": -0.13581874571650862, "compression_ratio": 1.7123893805309736, "no_speech_prob": 7.804399501765147e-05}, {"id": 64, "seek": 43794, "start": 445.54, "end": 453.54, "text": " for human beings sort of you know somewhere in the range of a hundred thousand to a million years ago", "tokens": [337, 1952, 8958, 1333, 295, 291, 458, 4079, 294, 264, 3613, 295, 257, 3262, 4714, 281, 257, 2459, 924, 2057], "temperature": 0.0, "avg_logprob": -0.13581874571650862, "compression_ratio": 1.7123893805309736, "no_speech_prob": 7.804399501765147e-05}, {"id": 65, "seek": 43794, "start": 453.54, "end": 459.22, "text": " okay that's a while ago but compared to the process of evolution of life on earth that's kind of", "tokens": [1392, 300, 311, 257, 1339, 2057, 457, 5347, 281, 264, 1399, 295, 9303, 295, 993, 322, 4120, 300, 311, 733, 295], "temperature": 0.0, "avg_logprob": -0.13581874571650862, "compression_ratio": 1.7123893805309736, "no_speech_prob": 7.804399501765147e-05}, {"id": 66, "seek": 43794, "start": 459.86, "end": 467.78, "text": " blinking an eyelid but that power from this communication between human beings quickly set off", "tokens": [45879, 364, 39386, 457, 300, 1347, 490, 341, 6101, 1296, 1952, 8958, 2661, 992, 766], "temperature": 0.0, "avg_logprob": -0.13581874571650862, "compression_ratio": 1.7123893805309736, "no_speech_prob": 7.804399501765147e-05}, {"id": 67, "seek": 46778, "start": 467.78, "end": 475.14, "text": " our ascendancy over other creatures. So it's kind of interesting that the ultimate power turned out not", "tokens": [527, 41604, 6717, 670, 661, 12281, 13, 407, 309, 311, 733, 295, 1880, 300, 264, 9705, 1347, 3574, 484, 406], "temperature": 0.0, "avg_logprob": -0.10601990481457078, "compression_ratio": 1.6229508196721312, "no_speech_prob": 4.821609036298469e-05}, {"id": 68, "seek": 46778, "start": 475.14, "end": 481.94, "text": " to be having poisonous fangs or being super fast or super big but having the ability to communicate", "tokens": [281, 312, 1419, 37376, 283, 28686, 420, 885, 1687, 2370, 420, 1687, 955, 457, 1419, 264, 3485, 281, 7890], "temperature": 0.0, "avg_logprob": -0.10601990481457078, "compression_ratio": 1.6229508196721312, "no_speech_prob": 4.821609036298469e-05}, {"id": 69, "seek": 46778, "start": 481.94, "end": 488.73999999999995, "text": " with other members of your tribe. It was much more recently again that humans developed writing", "tokens": [365, 661, 2679, 295, 428, 17625, 13, 467, 390, 709, 544, 3938, 797, 300, 6255, 4743, 3579], "temperature": 0.0, "avg_logprob": -0.10601990481457078, "compression_ratio": 1.6229508196721312, "no_speech_prob": 4.821609036298469e-05}, {"id": 70, "seek": 46778, "start": 488.73999999999995, "end": 495.14, "text": " which allowed knowledge to be communicated across distances of time and space and so that's only", "tokens": [597, 4350, 3601, 281, 312, 34989, 2108, 22182, 295, 565, 293, 1901, 293, 370, 300, 311, 787], "temperature": 0.0, "avg_logprob": -0.10601990481457078, "compression_ratio": 1.6229508196721312, "no_speech_prob": 4.821609036298469e-05}, {"id": 71, "seek": 49514, "start": 495.14, "end": 502.74, "text": " about five thousand years old the power of writing. So in just a few thousand years the ability", "tokens": [466, 1732, 4714, 924, 1331, 264, 1347, 295, 3579, 13, 407, 294, 445, 257, 1326, 4714, 924, 264, 3485], "temperature": 0.0, "avg_logprob": -0.0903920829296112, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.1215287637896836e-05}, {"id": 72, "seek": 49514, "start": 502.74, "end": 508.9, "text": " to preserve and share knowledge took us from the Bronze Age to the smartphones and tablets of today.", "tokens": [281, 15665, 293, 2073, 3601, 1890, 505, 490, 264, 44916, 16280, 281, 264, 26782, 293, 27622, 295, 965, 13], "temperature": 0.0, "avg_logprob": -0.0903920829296112, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.1215287637896836e-05}, {"id": 73, "seek": 49514, "start": 510.65999999999997, "end": 516.1, "text": " So a key question for artificial intelligence and human computer interaction is how to get", "tokens": [407, 257, 2141, 1168, 337, 11677, 7599, 293, 1952, 3820, 9285, 307, 577, 281, 483], "temperature": 0.0, "avg_logprob": -0.0903920829296112, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.1215287637896836e-05}, {"id": 74, "seek": 49514, "start": 516.1, "end": 522.9, "text": " computers to be able to understand the information conveyed in human languages. Simultaneously artificial", "tokens": [10807, 281, 312, 1075, 281, 1223, 264, 1589, 49340, 294, 1952, 8650, 13, 3998, 723, 13131, 11677], "temperature": 0.0, "avg_logprob": -0.0903920829296112, "compression_ratio": 1.672340425531915, "no_speech_prob": 4.1215287637896836e-05}, {"id": 75, "seek": 52290, "start": 522.9, "end": 529.78, "text": " intelligence requires computers with a knowledge of people. Fortunately now our AI systems might be", "tokens": [7599, 7029, 10807, 365, 257, 3601, 295, 561, 13, 20652, 586, 527, 7318, 3652, 1062, 312], "temperature": 0.0, "avg_logprob": -0.06624496123370002, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.00010213410132564604}, {"id": 76, "seek": 52290, "start": 529.78, "end": 535.9399999999999, "text": " able to benefit from a virtuous cycle. We need knowledge to understand language and people well", "tokens": [1075, 281, 5121, 490, 257, 48918, 6586, 13, 492, 643, 3601, 281, 1223, 2856, 293, 561, 731], "temperature": 0.0, "avg_logprob": -0.06624496123370002, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.00010213410132564604}, {"id": 77, "seek": 52290, "start": 535.9399999999999, "end": 542.34, "text": " but it's also the case that a lot of that knowledge is contained in language spread out across the", "tokens": [457, 309, 311, 611, 264, 1389, 300, 257, 688, 295, 300, 3601, 307, 16212, 294, 2856, 3974, 484, 2108, 264], "temperature": 0.0, "avg_logprob": -0.06624496123370002, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.00010213410132564604}, {"id": 78, "seek": 52290, "start": 542.34, "end": 546.8199999999999, "text": " books and web pages of the world and that's one of the things we're going to look at in this", "tokens": [3642, 293, 3670, 7183, 295, 264, 1002, 293, 300, 311, 472, 295, 264, 721, 321, 434, 516, 281, 574, 412, 294, 341], "temperature": 0.0, "avg_logprob": -0.06624496123370002, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.00010213410132564604}, {"id": 79, "seek": 54682, "start": 546.82, "end": 553.86, "text": " course is how that we can sort of build on that virtuous cycle. A lot of progress has already been", "tokens": [1164, 307, 577, 300, 321, 393, 1333, 295, 1322, 322, 300, 48918, 6586, 13, 316, 688, 295, 4205, 575, 1217, 668], "temperature": 0.0, "avg_logprob": -0.1013953069622597, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.00011731202539522201}, {"id": 80, "seek": 54682, "start": 553.86, "end": 563.3000000000001, "text": " made and I just wanted to very quickly give a sense of that. So in the last decade or so and especially", "tokens": [1027, 293, 286, 445, 1415, 281, 588, 2661, 976, 257, 2020, 295, 300, 13, 407, 294, 264, 1036, 10378, 420, 370, 293, 2318], "temperature": 0.0, "avg_logprob": -0.1013953069622597, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.00011731202539522201}, {"id": 81, "seek": 54682, "start": 563.3000000000001, "end": 569.22, "text": " in the last few years with neural methods of machine translation where now in a space where", "tokens": [294, 264, 1036, 1326, 924, 365, 18161, 7150, 295, 3479, 12853, 689, 586, 294, 257, 1901, 689], "temperature": 0.0, "avg_logprob": -0.1013953069622597, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.00011731202539522201}, {"id": 82, "seek": 54682, "start": 569.22, "end": 576.1, "text": " machine translation really works moderately well. So again from the history of the world this is", "tokens": [3479, 12853, 534, 1985, 10494, 1592, 731, 13, 407, 797, 490, 264, 2503, 295, 264, 1002, 341, 307], "temperature": 0.0, "avg_logprob": -0.1013953069622597, "compression_ratio": 1.6638297872340426, "no_speech_prob": 0.00011731202539522201}, {"id": 83, "seek": 57610, "start": 576.1, "end": 583.3000000000001, "text": " just amazing right for thousands of years learning other people's languages was a human task which", "tokens": [445, 2243, 558, 337, 5383, 295, 924, 2539, 661, 561, 311, 8650, 390, 257, 1952, 5633, 597], "temperature": 0.0, "avg_logprob": -0.1001853519015842, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0003203241212759167}, {"id": 84, "seek": 57610, "start": 583.3000000000001, "end": 589.78, "text": " required a lot of effort and concentration but now we're in a world where you could just hop on", "tokens": [4739, 257, 688, 295, 4630, 293, 9856, 457, 586, 321, 434, 294, 257, 1002, 689, 291, 727, 445, 3818, 322], "temperature": 0.0, "avg_logprob": -0.1001853519015842, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0003203241212759167}, {"id": 85, "seek": 57610, "start": 589.78, "end": 596.26, "text": " your web browser and think oh I wonder what the news is in Kenya today and you can head off over to", "tokens": [428, 3670, 11185, 293, 519, 1954, 286, 2441, 437, 264, 2583, 307, 294, 31011, 965, 293, 291, 393, 1378, 766, 670, 281], "temperature": 0.0, "avg_logprob": -0.1001853519015842, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0003203241212759167}, {"id": 86, "seek": 57610, "start": 596.26, "end": 603.22, "text": " a Kenyan website and you can see something like this and you can go and you can then ask Google", "tokens": [257, 8273, 6277, 3144, 293, 291, 393, 536, 746, 411, 341, 293, 291, 393, 352, 293, 291, 393, 550, 1029, 3329], "temperature": 0.0, "avg_logprob": -0.1001853519015842, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.0003203241212759167}, {"id": 87, "seek": 60322, "start": 603.22, "end": 609.86, "text": " to translate it for you from Swahili and you know the translation isn't quite perfect", "tokens": [281, 13799, 309, 337, 291, 490, 3926, 545, 2312, 293, 291, 458, 264, 12853, 1943, 380, 1596, 2176], "temperature": 0.0, "avg_logprob": -0.21604881757571373, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006562131457030773}, {"id": 88, "seek": 60322, "start": 609.86, "end": 615.38, "text": " but it's you know it's reasonably good so the newspaper Tukko has been informed that local", "tokens": [457, 309, 311, 291, 458, 309, 311, 23551, 665, 370, 264, 13669, 314, 2034, 4093, 575, 668, 11740, 300, 2654], "temperature": 0.0, "avg_logprob": -0.21604881757571373, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006562131457030773}, {"id": 89, "seek": 60322, "start": 615.38, "end": 621.46, "text": " government minister Lingsan, Bella Kanyama and his transport counterparts sitting me", "tokens": [2463, 10563, 20977, 11491, 11, 29133, 591, 1325, 2404, 293, 702, 5495, 33287, 3798, 385], "temperature": 0.0, "avg_logprob": -0.21604881757571373, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006562131457030773}, {"id": 90, "seek": 60322, "start": 621.46, "end": 627.62, "text": " died within two separate hours so you know within two separate hours is kind of awkward but essentially", "tokens": [4539, 1951, 732, 4994, 2496, 370, 291, 458, 1951, 732, 4994, 2496, 307, 733, 295, 11411, 457, 4476], "temperature": 0.0, "avg_logprob": -0.21604881757571373, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0006562131457030773}, {"id": 91, "seek": 62762, "start": 627.62, "end": 633.86, "text": " we're doing pretty well at getting the information out of this page and so that's quite amazing.", "tokens": [321, 434, 884, 1238, 731, 412, 1242, 264, 1589, 484, 295, 341, 3028, 293, 370, 300, 311, 1596, 2243, 13], "temperature": 0.0, "avg_logprob": -0.12472812107631139, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.00011674386769300327}, {"id": 92, "seek": 62762, "start": 635.22, "end": 641.7, "text": " The single biggest development in NLP for the last year certainly in the popular media", "tokens": [440, 2167, 3880, 3250, 294, 426, 45196, 337, 264, 1036, 1064, 3297, 294, 264, 3743, 3021], "temperature": 0.0, "avg_logprob": -0.12472812107631139, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.00011674386769300327}, {"id": 93, "seek": 62762, "start": 641.7, "end": 653.46, "text": " meeting was GPT3 which was a huge new model that was released by OpenAI. What GPT3 is about and why", "tokens": [3440, 390, 26039, 51, 18, 597, 390, 257, 2603, 777, 2316, 300, 390, 4736, 538, 7238, 48698, 13, 708, 26039, 51, 18, 307, 466, 293, 983], "temperature": 0.0, "avg_logprob": -0.12472812107631139, "compression_ratio": 1.4292929292929293, "no_speech_prob": 0.00011674386769300327}, {"id": 94, "seek": 65346, "start": 653.46, "end": 659.94, "text": " it's great is actually a bit subtle and so I can't really go through all the details of this here", "tokens": [309, 311, 869, 307, 767, 257, 857, 13743, 293, 370, 286, 393, 380, 534, 352, 807, 439, 264, 4365, 295, 341, 510], "temperature": 0.0, "avg_logprob": -0.056864581324837425, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.00027554386178962886}, {"id": 95, "seek": 65346, "start": 659.94, "end": 665.5400000000001, "text": " but it's exciting because it seems like it's the first step on the path to what we might call", "tokens": [457, 309, 311, 4670, 570, 309, 2544, 411, 309, 311, 264, 700, 1823, 322, 264, 3100, 281, 437, 321, 1062, 818], "temperature": 0.0, "avg_logprob": -0.056864581324837425, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.00027554386178962886}, {"id": 96, "seek": 65346, "start": 665.5400000000001, "end": 673.0600000000001, "text": " universal models where you can train up one extremely large model on something like that library", "tokens": [11455, 5245, 689, 291, 393, 3847, 493, 472, 4664, 2416, 2316, 322, 746, 411, 300, 6405], "temperature": 0.0, "avg_logprob": -0.056864581324837425, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.00027554386178962886}, {"id": 97, "seek": 65346, "start": 673.0600000000001, "end": 680.34, "text": " picture I showed before and it just has knowledge of the world, knowledge of human languages, knowledge", "tokens": [3036, 286, 4712, 949, 293, 309, 445, 575, 3601, 295, 264, 1002, 11, 3601, 295, 1952, 8650, 11, 3601], "temperature": 0.0, "avg_logprob": -0.056864581324837425, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.00027554386178962886}, {"id": 98, "seek": 68034, "start": 680.34, "end": 687.3000000000001, "text": " of how to do tasks and then you can apply it to do all sorts of things so no longer are we building", "tokens": [295, 577, 281, 360, 9608, 293, 550, 291, 393, 3079, 309, 281, 360, 439, 7527, 295, 721, 370, 572, 2854, 366, 321, 2390], "temperature": 0.0, "avg_logprob": -0.08209758207022426, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0002727723040152341}, {"id": 99, "seek": 68034, "start": 687.3000000000001, "end": 693.22, "text": " a model to detect spam and then a model to detect pornography and then a model to detect", "tokens": [257, 2316, 281, 5531, 24028, 293, 550, 257, 2316, 281, 5531, 49936, 293, 550, 257, 2316, 281, 5531], "temperature": 0.0, "avg_logprob": -0.08209758207022426, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0002727723040152341}, {"id": 100, "seek": 68034, "start": 694.74, "end": 699.62, "text": " whatever foreign language content and just building all these separate supervised classifiers", "tokens": [2035, 5329, 2856, 2701, 293, 445, 2390, 439, 613, 4994, 46533, 1508, 23463], "temperature": 0.0, "avg_logprob": -0.08209758207022426, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0002727723040152341}, {"id": 101, "seek": 68034, "start": 699.62, "end": 706.9000000000001, "text": " for every different task we've now just built up a model that understands so exactly what it does", "tokens": [337, 633, 819, 5633, 321, 600, 586, 445, 3094, 493, 257, 2316, 300, 15146, 370, 2293, 437, 309, 775], "temperature": 0.0, "avg_logprob": -0.08209758207022426, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.0002727723040152341}, {"id": 102, "seek": 70690, "start": 706.9, "end": 718.9, "text": " is it just predicts following words so on the left it's been told to write about Elon Musk in the", "tokens": [307, 309, 445, 6069, 82, 3480, 2283, 370, 322, 264, 1411, 309, 311, 668, 1907, 281, 2464, 466, 28498, 26019, 294, 264], "temperature": 0.0, "avg_logprob": -0.14539805702541186, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00014796381583437324}, {"id": 103, "seek": 70690, "start": 718.9, "end": 727.06, "text": " style of Dr. Soos and it started off with some text and then it's generating more text and the", "tokens": [3758, 295, 2491, 13, 407, 329, 293, 309, 1409, 766, 365, 512, 2487, 293, 550, 309, 311, 17746, 544, 2487, 293, 264], "temperature": 0.0, "avg_logprob": -0.14539805702541186, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00014796381583437324}, {"id": 104, "seek": 70690, "start": 727.06, "end": 734.18, "text": " way it generates more text is literally by just predicting one word at a time following words", "tokens": [636, 309, 23815, 544, 2487, 307, 3736, 538, 445, 32884, 472, 1349, 412, 257, 565, 3480, 2283], "temperature": 0.0, "avg_logprob": -0.14539805702541186, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00014796381583437324}, {"id": 105, "seek": 73418, "start": 734.18, "end": 742.8199999999999, "text": " come to complete its text but this has a very powerful facility because what you can do with", "tokens": [808, 281, 3566, 1080, 2487, 457, 341, 575, 257, 588, 4005, 8973, 570, 437, 291, 393, 360, 365], "temperature": 0.0, "avg_logprob": -0.1451303378955738, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.00043303577695041895}, {"id": 106, "seek": 73418, "start": 742.8199999999999, "end": 750.02, "text": " GPT3 is you can give it a couple of examples of what you'd like it to do so I can give it some", "tokens": [26039, 51, 18, 307, 291, 393, 976, 309, 257, 1916, 295, 5110, 295, 437, 291, 1116, 411, 309, 281, 360, 370, 286, 393, 976, 309, 512], "temperature": 0.0, "avg_logprob": -0.1451303378955738, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.00043303577695041895}, {"id": 107, "seek": 73418, "start": 750.02, "end": 756.7399999999999, "text": " text and say I broke the window, changed it into a question what did I break, I gracefully save", "tokens": [2487, 293, 584, 286, 6902, 264, 4910, 11, 3105, 309, 666, 257, 1168, 437, 630, 286, 1821, 11, 286, 10042, 2277, 3155], "temperature": 0.0, "avg_logprob": -0.1451303378955738, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.00043303577695041895}, {"id": 108, "seek": 75674, "start": 756.74, "end": 764.82, "text": " the day, I changed it into a question what did I gracefully save so this prompt tells GPT3 what", "tokens": [264, 786, 11, 286, 3105, 309, 666, 257, 1168, 437, 630, 286, 10042, 2277, 3155, 370, 341, 12391, 5112, 26039, 51, 18, 437], "temperature": 0.0, "avg_logprob": -0.08494307518005371, "compression_ratio": 1.6812227074235808, "no_speech_prob": 6.069831579225138e-05}, {"id": 109, "seek": 75674, "start": 765.54, "end": 771.46, "text": " I'm wanting it to do and so then if I give it another statement like I gave John Flowers I can", "tokens": [286, 478, 7935, 309, 281, 360, 293, 370, 550, 498, 286, 976, 309, 1071, 5629, 411, 286, 2729, 2619, 48194, 286, 393], "temperature": 0.0, "avg_logprob": -0.08494307518005371, "compression_ratio": 1.6812227074235808, "no_speech_prob": 6.069831579225138e-05}, {"id": 110, "seek": 75674, "start": 771.46, "end": 778.58, "text": " then say GPT3 predict what words come next and it'll follow my prompt and produce who did I give", "tokens": [550, 584, 26039, 51, 18, 6069, 437, 2283, 808, 958, 293, 309, 603, 1524, 452, 12391, 293, 5258, 567, 630, 286, 976], "temperature": 0.0, "avg_logprob": -0.08494307518005371, "compression_ratio": 1.6812227074235808, "no_speech_prob": 6.069831579225138e-05}, {"id": 111, "seek": 75674, "start": 778.58, "end": 785.78, "text": " flowers to or I can say I gave her a rose and a guitar and it will follow the idea of the pattern", "tokens": [8085, 281, 420, 286, 393, 584, 286, 2729, 720, 257, 10895, 293, 257, 7531, 293, 309, 486, 1524, 264, 1558, 295, 264, 5102], "temperature": 0.0, "avg_logprob": -0.08494307518005371, "compression_ratio": 1.6812227074235808, "no_speech_prob": 6.069831579225138e-05}, {"id": 112, "seek": 78578, "start": 785.78, "end": 792.42, "text": " and do who did I give a rose and a guitar too and actually this one model can then do an amazing", "tokens": [293, 360, 567, 630, 286, 976, 257, 10895, 293, 257, 7531, 886, 293, 767, 341, 472, 2316, 393, 550, 360, 364, 2243], "temperature": 0.0, "avg_logprob": -0.1091228273179796, "compression_ratio": 1.676991150442478, "no_speech_prob": 6.891296652611345e-05}, {"id": 113, "seek": 78578, "start": 792.42, "end": 798.42, "text": " range of things including many there's quite surprising to do at all to give just one example of", "tokens": [3613, 295, 721, 3009, 867, 456, 311, 1596, 8830, 281, 360, 412, 439, 281, 976, 445, 472, 1365, 295], "temperature": 0.0, "avg_logprob": -0.1091228273179796, "compression_ratio": 1.676991150442478, "no_speech_prob": 6.891296652611345e-05}, {"id": 114, "seek": 78578, "start": 798.42, "end": 807.14, "text": " that another thing that you can do is get it to translate human language sentences into SQL so", "tokens": [300, 1071, 551, 300, 291, 393, 360, 307, 483, 309, 281, 13799, 1952, 2856, 16579, 666, 19200, 370], "temperature": 0.0, "avg_logprob": -0.1091228273179796, "compression_ratio": 1.676991150442478, "no_speech_prob": 6.891296652611345e-05}, {"id": 115, "seek": 78578, "start": 807.14, "end": 815.14, "text": " this can make it much easier to do CS 145 so having given that a couple of examples of SQL", "tokens": [341, 393, 652, 309, 709, 3571, 281, 360, 9460, 3499, 20, 370, 1419, 2212, 300, 257, 1916, 295, 5110, 295, 19200], "temperature": 0.0, "avg_logprob": -0.1091228273179796, "compression_ratio": 1.676991150442478, "no_speech_prob": 6.891296652611345e-05}, {"id": 116, "seek": 81514, "start": 815.14, "end": 821.46, "text": " translation of human language text which I'm this time not showing because it won't fit on my slide", "tokens": [12853, 295, 1952, 2856, 2487, 597, 286, 478, 341, 565, 406, 4099, 570, 309, 1582, 380, 3318, 322, 452, 4137], "temperature": 0.0, "avg_logprob": -0.07857405102771262, "compression_ratio": 1.582995951417004, "no_speech_prob": 8.734628499951214e-05}, {"id": 117, "seek": 81514, "start": 821.46, "end": 827.86, "text": " I can then give it a sentence like how many users have signed up since the start of 2020 and it turns", "tokens": [286, 393, 550, 976, 309, 257, 8174, 411, 577, 867, 5022, 362, 8175, 493, 1670, 264, 722, 295, 4808, 293, 309, 4523], "temperature": 0.0, "avg_logprob": -0.07857405102771262, "compression_ratio": 1.582995951417004, "no_speech_prob": 8.734628499951214e-05}, {"id": 118, "seek": 81514, "start": 827.86, "end": 833.86, "text": " it into SQL or I can give it another query what is the average number of influencers each user", "tokens": [309, 666, 19200, 420, 286, 393, 976, 309, 1071, 14581, 437, 307, 264, 4274, 1230, 295, 38646, 1184, 4195], "temperature": 0.0, "avg_logprob": -0.07857405102771262, "compression_ratio": 1.582995951417004, "no_speech_prob": 8.734628499951214e-05}, {"id": 119, "seek": 81514, "start": 833.86, "end": 844.34, "text": " subscribe to and again it then converts that into SQL so GPT3 knows a lot about the meaning of", "tokens": [3022, 281, 293, 797, 309, 550, 38874, 300, 666, 19200, 370, 26039, 51, 18, 3255, 257, 688, 466, 264, 3620, 295], "temperature": 0.0, "avg_logprob": -0.07857405102771262, "compression_ratio": 1.582995951417004, "no_speech_prob": 8.734628499951214e-05}, {"id": 120, "seek": 84434, "start": 844.34, "end": 849.7800000000001, "text": " language and the meaning of other things like SQL and confluently manipulate it", "tokens": [2856, 293, 264, 3620, 295, 661, 721, 411, 19200, 293, 1497, 2781, 2276, 20459, 309], "temperature": 0.0, "avg_logprob": -0.14318047691794003, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.00012123643682571128}, {"id": 121, "seek": 84434, "start": 853.0600000000001, "end": 859.94, "text": " okay so that leads us straight into this top of meaning and how do we represent the meaning of a", "tokens": [1392, 370, 300, 6689, 505, 2997, 666, 341, 1192, 295, 3620, 293, 577, 360, 321, 2906, 264, 3620, 295, 257], "temperature": 0.0, "avg_logprob": -0.14318047691794003, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.00012123643682571128}, {"id": 122, "seek": 84434, "start": 859.94, "end": 866.58, "text": " word well what is meaning well we can look up something like the websteadictionary and say okay", "tokens": [1349, 731, 437, 307, 3620, 731, 321, 393, 574, 493, 746, 411, 264, 3670, 372, 2056, 4105, 822, 293, 584, 1392], "temperature": 0.0, "avg_logprob": -0.14318047691794003, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.00012123643682571128}, {"id": 123, "seek": 84434, "start": 867.7800000000001, "end": 873.0600000000001, "text": " the idea that is represented by a word the idea that a person wants to express by using word", "tokens": [264, 1558, 300, 307, 10379, 538, 257, 1349, 264, 1558, 300, 257, 954, 2738, 281, 5109, 538, 1228, 1349], "temperature": 0.0, "avg_logprob": -0.14318047691794003, "compression_ratio": 1.7980295566502462, "no_speech_prob": 0.00012123643682571128}, {"id": 124, "seek": 87306, "start": 873.06, "end": 880.18, "text": " signs etc the websteadictionary definition is really focused on the word idea somehow", "tokens": [7880, 5183, 264, 3670, 372, 2056, 4105, 822, 7123, 307, 534, 5178, 322, 264, 1349, 1558, 6063], "temperature": 0.0, "avg_logprob": -0.08799430998888882, "compression_ratio": 1.8894472361809045, "no_speech_prob": 6.0925496654817834e-05}, {"id": 125, "seek": 87306, "start": 880.18, "end": 886.3399999999999, "text": " but this is pretty close to the commonest way that linguists think about meaning so that they think", "tokens": [457, 341, 307, 1238, 1998, 281, 264, 2689, 377, 636, 300, 21766, 1751, 519, 466, 3620, 370, 300, 436, 519], "temperature": 0.0, "avg_logprob": -0.08799430998888882, "compression_ratio": 1.8894472361809045, "no_speech_prob": 6.0925496654817834e-05}, {"id": 126, "seek": 87306, "start": 886.3399999999999, "end": 894.18, "text": " of word meaning as being a pairing between a word which is a signifier or symbol and the thing", "tokens": [295, 1349, 3620, 382, 885, 257, 32735, 1296, 257, 1349, 597, 307, 257, 1465, 9902, 420, 5986, 293, 264, 551], "temperature": 0.0, "avg_logprob": -0.08799430998888882, "compression_ratio": 1.8894472361809045, "no_speech_prob": 6.0925496654817834e-05}, {"id": 127, "seek": 87306, "start": 894.18, "end": 900.42, "text": " that it signifies the signified thing which is an idea or thing so that the meaning of the word", "tokens": [300, 309, 1465, 11221, 264, 1465, 2587, 551, 597, 307, 364, 1558, 420, 551, 370, 300, 264, 3620, 295, 264, 1349], "temperature": 0.0, "avg_logprob": -0.08799430998888882, "compression_ratio": 1.8894472361809045, "no_speech_prob": 6.0925496654817834e-05}, {"id": 128, "seek": 90042, "start": 900.42, "end": 907.4599999999999, "text": " chair is the set of things that are chairs and that's referred to as denotational semantics", "tokens": [6090, 307, 264, 992, 295, 721, 300, 366, 18299, 293, 300, 311, 10839, 281, 382, 1441, 310, 1478, 4361, 45298], "temperature": 0.0, "avg_logprob": -0.11874879278787752, "compression_ratio": 1.6898148148148149, "no_speech_prob": 9.121521725319326e-05}, {"id": 129, "seek": 90042, "start": 907.4599999999999, "end": 913.3, "text": " a term that's also used and similarly applied for the semantics of programming languages", "tokens": [257, 1433, 300, 311, 611, 1143, 293, 14138, 6456, 337, 264, 4361, 45298, 295, 9410, 8650], "temperature": 0.0, "avg_logprob": -0.11874879278787752, "compression_ratio": 1.6898148148148149, "no_speech_prob": 9.121521725319326e-05}, {"id": 130, "seek": 90042, "start": 913.9399999999999, "end": 923.2199999999999, "text": " this model isn't very gently implementable like how do I go from the idea that okay chair", "tokens": [341, 2316, 1943, 380, 588, 13073, 4445, 712, 411, 577, 360, 286, 352, 490, 264, 1558, 300, 1392, 6090], "temperature": 0.0, "avg_logprob": -0.11874879278787752, "compression_ratio": 1.6898148148148149, "no_speech_prob": 9.121521725319326e-05}, {"id": 131, "seek": 90042, "start": 923.2199999999999, "end": 928.98, "text": " means the set of chairs in the world so something I can manipulate meaning within my computers", "tokens": [1355, 264, 992, 295, 18299, 294, 264, 1002, 370, 746, 286, 393, 20459, 3620, 1951, 452, 10807], "temperature": 0.0, "avg_logprob": -0.11874879278787752, "compression_ratio": 1.6898148148148149, "no_speech_prob": 9.121521725319326e-05}, {"id": 132, "seek": 92898, "start": 928.98, "end": 936.34, "text": " so traditionally the way that meaning has normally been handled in natural language processing", "tokens": [370, 19067, 264, 636, 300, 3620, 575, 5646, 668, 18033, 294, 3303, 2856, 9007], "temperature": 0.0, "avg_logprob": -0.15202914873758952, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00010522763477638364}, {"id": 133, "seek": 92898, "start": 936.34, "end": 943.54, "text": " systems is to make use of resources like dictionaries and thosauri in particular popular one is", "tokens": [3652, 307, 281, 652, 764, 295, 3593, 411, 22352, 4889, 293, 258, 18955, 72, 294, 1729, 3743, 472, 307], "temperature": 0.0, "avg_logprob": -0.15202914873758952, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00010522763477638364}, {"id": 134, "seek": 92898, "start": 943.54, "end": 952.66, "text": " wordnet which organized words and terms into both synonym sets words that can mean the same thing", "tokens": [1349, 7129, 597, 9983, 2283, 293, 2115, 666, 1293, 5451, 12732, 6352, 2283, 300, 393, 914, 264, 912, 551], "temperature": 0.0, "avg_logprob": -0.15202914873758952, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.00010522763477638364}, {"id": 135, "seek": 95266, "start": 952.66, "end": 959.54, "text": " and hypenims which correspond to is a relationship and so for the is a relationship you know we can", "tokens": [293, 2477, 5200, 18857, 597, 6805, 281, 307, 257, 2480, 293, 370, 337, 264, 307, 257, 2480, 291, 458, 321, 393], "temperature": 0.0, "avg_logprob": -0.19524489298905476, "compression_ratio": 1.8780487804878048, "no_speech_prob": 4.826517033507116e-05}, {"id": 136, "seek": 95266, "start": 959.54, "end": 966.42, "text": " kind of look at the hypenims of panda and a panda is a kind of prosiumid whatever those I get", "tokens": [733, 295, 574, 412, 264, 2477, 5200, 18857, 295, 46685, 293, 257, 46685, 307, 257, 733, 295, 6267, 2197, 327, 2035, 729, 286, 483], "temperature": 0.0, "avg_logprob": -0.19524489298905476, "compression_ratio": 1.8780487804878048, "no_speech_prob": 4.826517033507116e-05}, {"id": 137, "seek": 95266, "start": 966.42, "end": 972.9, "text": " a set's probably with red pandas which is a kind of carnivore which is a kind of placental which", "tokens": [257, 992, 311, 1391, 365, 2182, 4565, 296, 597, 307, 257, 733, 295, 23796, 592, 418, 597, 307, 257, 733, 295, 20831, 14533, 597], "temperature": 0.0, "avg_logprob": -0.19524489298905476, "compression_ratio": 1.8780487804878048, "no_speech_prob": 4.826517033507116e-05}, {"id": 138, "seek": 95266, "start": 972.9, "end": 980.98, "text": " is kind of mammal and you sort of head up this hypenim hierarchy so wordnet has been a greater", "tokens": [307, 733, 295, 49312, 293, 291, 1333, 295, 1378, 493, 341, 2477, 5200, 332, 22333, 370, 1349, 7129, 575, 668, 257, 5044], "temperature": 0.0, "avg_logprob": -0.19524489298905476, "compression_ratio": 1.8780487804878048, "no_speech_prob": 4.826517033507116e-05}, {"id": 139, "seek": 98098, "start": 980.98, "end": 989.78, "text": " resource for nlp but it's also been highly deficient so it lacks a lot of nuance so for example", "tokens": [7684, 337, 297, 75, 79, 457, 309, 311, 611, 668, 5405, 19248, 1196, 370, 309, 31132, 257, 688, 295, 42625, 370, 337, 1365], "temperature": 0.0, "avg_logprob": -0.11979913209614, "compression_ratio": 1.7354260089686098, "no_speech_prob": 4.7490018914686516e-05}, {"id": 140, "seek": 98098, "start": 989.78, "end": 996.26, "text": " in wordnet proficient is list as a synonym for good but you know maybe that's sometimes true", "tokens": [294, 1349, 7129, 1740, 24549, 307, 1329, 382, 257, 5451, 12732, 337, 665, 457, 291, 458, 1310, 300, 311, 2171, 2074], "temperature": 0.0, "avg_logprob": -0.11979913209614, "compression_ratio": 1.7354260089686098, "no_speech_prob": 4.7490018914686516e-05}, {"id": 141, "seek": 98098, "start": 996.26, "end": 1000.5, "text": " but it seems like in a lot of context it's not true and you mean something rather different when", "tokens": [457, 309, 2544, 411, 294, 257, 688, 295, 4319, 309, 311, 406, 2074, 293, 291, 914, 746, 2831, 819, 562], "temperature": 0.0, "avg_logprob": -0.11979913209614, "compression_ratio": 1.7354260089686098, "no_speech_prob": 4.7490018914686516e-05}, {"id": 142, "seek": 98098, "start": 1000.5, "end": 1008.4200000000001, "text": " you say proficient versus good it's limited as a human constructed thosauris so in particular there's", "tokens": [291, 584, 1740, 24549, 5717, 665, 309, 311, 5567, 382, 257, 1952, 17083, 258, 18955, 271, 370, 294, 1729, 456, 311], "temperature": 0.0, "avg_logprob": -0.11979913209614, "compression_ratio": 1.7354260089686098, "no_speech_prob": 4.7490018914686516e-05}, {"id": 143, "seek": 100842, "start": 1008.42, "end": 1015.2199999999999, "text": " lots of words and lots of uses of words that just aren't there including you know anything that is", "tokens": [3195, 295, 2283, 293, 3195, 295, 4960, 295, 2283, 300, 445, 3212, 380, 456, 3009, 291, 458, 1340, 300, 307], "temperature": 0.0, "avg_logprob": -0.09713501563439002, "compression_ratio": 1.7052023121387283, "no_speech_prob": 0.0001142133041867055}, {"id": 144, "seek": 100842, "start": 1015.2199999999999, "end": 1023.38, "text": " you know sort of more current terminology like wicked is there for the wicked witch but not for", "tokens": [291, 458, 1333, 295, 544, 2190, 27575, 411, 22663, 307, 456, 337, 264, 22663, 14867, 457, 406, 337], "temperature": 0.0, "avg_logprob": -0.09713501563439002, "compression_ratio": 1.7052023121387283, "no_speech_prob": 0.0001142133041867055}, {"id": 145, "seek": 100842, "start": 1023.38, "end": 1030.6599999999999, "text": " more modern colloquial uses ninja certainly isn't there for the kind of description some people make", "tokens": [544, 4363, 1263, 29826, 831, 4960, 31604, 3297, 1943, 380, 456, 337, 264, 733, 295, 3855, 512, 561, 652], "temperature": 0.0, "avg_logprob": -0.09713501563439002, "compression_ratio": 1.7052023121387283, "no_speech_prob": 0.0001142133041867055}, {"id": 146, "seek": 103066, "start": 1030.66, "end": 1038.3400000000001, "text": " of programmers and it's impossible to keep up to date so it requires a lot of human labor but even", "tokens": [295, 41504, 293, 309, 311, 6243, 281, 1066, 493, 281, 4002, 370, 309, 7029, 257, 688, 295, 1952, 5938, 457, 754], "temperature": 0.0, "avg_logprob": -0.11644810349193972, "compression_ratio": 1.6685714285714286, "no_speech_prob": 1.832494672271423e-05}, {"id": 147, "seek": 103066, "start": 1038.3400000000001, "end": 1046.3400000000001, "text": " when you have that you know it has a set of synonyms but doesn't really have a good sense of words", "tokens": [562, 291, 362, 300, 291, 458, 309, 575, 257, 992, 295, 5451, 2526, 2592, 457, 1177, 380, 534, 362, 257, 665, 2020, 295, 2283], "temperature": 0.0, "avg_logprob": -0.11644810349193972, "compression_ratio": 1.6685714285714286, "no_speech_prob": 1.832494672271423e-05}, {"id": 148, "seek": 103066, "start": 1046.3400000000001, "end": 1055.5400000000002, "text": " it means something similar so fantastic and great means something similar without really being", "tokens": [309, 1355, 746, 2531, 370, 5456, 293, 869, 1355, 746, 2531, 1553, 534, 885], "temperature": 0.0, "avg_logprob": -0.11644810349193972, "compression_ratio": 1.6685714285714286, "no_speech_prob": 1.832494672271423e-05}, {"id": 149, "seek": 105554, "start": 1055.54, "end": 1061.7, "text": " synonymms and so this idea of meaning similarity is something that'd be really useful to make", "tokens": [5451, 12732, 2592, 293, 370, 341, 1558, 295, 3620, 32194, 307, 746, 300, 1116, 312, 534, 4420, 281, 652], "temperature": 0.0, "avg_logprob": -0.08192924658457439, "compression_ratio": 1.8415841584158417, "no_speech_prob": 4.902370710624382e-05}, {"id": 150, "seek": 105554, "start": 1061.7, "end": 1068.74, "text": " progress on and where deep learning models excel okay so what's the problem with a lot of", "tokens": [4205, 322, 293, 689, 2452, 2539, 5245, 24015, 1392, 370, 437, 311, 264, 1154, 365, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.08192924658457439, "compression_ratio": 1.8415841584158417, "no_speech_prob": 4.902370710624382e-05}, {"id": 151, "seek": 105554, "start": 1068.74, "end": 1076.58, "text": " traditional nlp well the problem with a lot of traditional nlp is that words are regardless", "tokens": [5164, 297, 75, 79, 731, 264, 1154, 365, 257, 688, 295, 5164, 297, 75, 79, 307, 300, 2283, 366, 10060], "temperature": 0.0, "avg_logprob": -0.08192924658457439, "compression_ratio": 1.8415841584158417, "no_speech_prob": 4.902370710624382e-05}, {"id": 152, "seek": 105554, "start": 1076.58, "end": 1084.42, "text": " discrete symbols so we have symbols like hotel conference motel our words which in deep learning", "tokens": [27706, 16944, 370, 321, 362, 16944, 411, 7622, 7586, 2184, 338, 527, 2283, 597, 294, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.08192924658457439, "compression_ratio": 1.8415841584158417, "no_speech_prob": 4.902370710624382e-05}, {"id": 153, "seek": 108442, "start": 1084.42, "end": 1094.18, "text": " speak we refer to as a localist representation and that's because if you in statistical or machine", "tokens": [1710, 321, 2864, 281, 382, 257, 2654, 468, 10290, 293, 300, 311, 570, 498, 291, 294, 22820, 420, 3479], "temperature": 0.0, "avg_logprob": -0.09184273735421603, "compression_ratio": 1.7142857142857142, "no_speech_prob": 9.117880836129189e-05}, {"id": 154, "seek": 108442, "start": 1094.18, "end": 1101.22, "text": " learning systems want to represent these symbols that each of them is a separate thing so the", "tokens": [2539, 3652, 528, 281, 2906, 613, 16944, 300, 1184, 295, 552, 307, 257, 4994, 551, 370, 264], "temperature": 0.0, "avg_logprob": -0.09184273735421603, "compression_ratio": 1.7142857142857142, "no_speech_prob": 9.117880836129189e-05}, {"id": 155, "seek": 108442, "start": 1101.22, "end": 1107.46, "text": " standard way of representing them and this is what you do in something like a statistical model", "tokens": [3832, 636, 295, 13460, 552, 293, 341, 307, 437, 291, 360, 294, 746, 411, 257, 22820, 2316], "temperature": 0.0, "avg_logprob": -0.09184273735421603, "compression_ratio": 1.7142857142857142, "no_speech_prob": 9.117880836129189e-05}, {"id": 156, "seek": 110746, "start": 1107.46, "end": 1114.3400000000001, "text": " if you're building a logistic regression model with words as features is that you represent them as one", "tokens": [498, 291, 434, 2390, 257, 3565, 3142, 24590, 2316, 365, 2283, 382, 4122, 307, 300, 291, 2906, 552, 382, 472], "temperature": 0.0, "avg_logprob": -0.0644734536094227, "compression_ratio": 1.7589285714285714, "no_speech_prob": 3.1151805160334334e-05}, {"id": 157, "seek": 110746, "start": 1114.3400000000001, "end": 1120.26, "text": " hot vectors so you have a dimension for each different word so maybe like my example here are my", "tokens": [2368, 18875, 370, 291, 362, 257, 10139, 337, 1184, 819, 1349, 370, 1310, 411, 452, 1365, 510, 366, 452], "temperature": 0.0, "avg_logprob": -0.0644734536094227, "compression_ratio": 1.7589285714285714, "no_speech_prob": 3.1151805160334334e-05}, {"id": 158, "seek": 110746, "start": 1120.26, "end": 1129.7, "text": " representations as vectors for motel and hotel and so that means that we have to have huge vectors", "tokens": [33358, 382, 18875, 337, 2184, 338, 293, 7622, 293, 370, 300, 1355, 300, 321, 362, 281, 362, 2603, 18875], "temperature": 0.0, "avg_logprob": -0.0644734536094227, "compression_ratio": 1.7589285714285714, "no_speech_prob": 3.1151805160334334e-05}, {"id": 159, "seek": 110746, "start": 1129.7, "end": 1134.98, "text": " corresponding to the number of words in our vocabulary so the kind of if you had a high school", "tokens": [11760, 281, 264, 1230, 295, 2283, 294, 527, 19864, 370, 264, 733, 295, 498, 291, 632, 257, 1090, 1395], "temperature": 0.0, "avg_logprob": -0.0644734536094227, "compression_ratio": 1.7589285714285714, "no_speech_prob": 3.1151805160334334e-05}, {"id": 160, "seek": 113498, "start": 1134.98, "end": 1141.38, "text": " English dictionary it probably had about 250,000 words in it but there are many many more words", "tokens": [3669, 25890, 309, 1391, 632, 466, 11650, 11, 1360, 2283, 294, 309, 457, 456, 366, 867, 867, 544, 2283], "temperature": 0.0, "avg_logprob": -0.1111255075739718, "compression_ratio": 1.6234309623430963, "no_speech_prob": 4.750661537400447e-05}, {"id": 161, "seek": 113498, "start": 1141.38, "end": 1147.78, "text": " in the language really so maybe we at least want to have a 500,000 dimensional vector to be able to", "tokens": [294, 264, 2856, 534, 370, 1310, 321, 412, 1935, 528, 281, 362, 257, 5923, 11, 1360, 18795, 8062, 281, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.1111255075739718, "compression_ratio": 1.6234309623430963, "no_speech_prob": 4.750661537400447e-05}, {"id": 162, "seek": 113498, "start": 1147.78, "end": 1156.42, "text": " cope with that okay but the bigger even bigger problem with the discrete symbols is that we don't", "tokens": [22598, 365, 300, 1392, 457, 264, 3801, 754, 3801, 1154, 365, 264, 27706, 16944, 307, 300, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.1111255075739718, "compression_ratio": 1.6234309623430963, "no_speech_prob": 4.750661537400447e-05}, {"id": 163, "seek": 113498, "start": 1156.42, "end": 1162.74, "text": " have this notion of word relationships and similarity so for example in web search if they use", "tokens": [362, 341, 10710, 295, 1349, 6159, 293, 32194, 370, 337, 1365, 294, 3670, 3164, 498, 436, 764], "temperature": 0.0, "avg_logprob": -0.1111255075739718, "compression_ratio": 1.6234309623430963, "no_speech_prob": 4.750661537400447e-05}, {"id": 164, "seek": 116274, "start": 1162.74, "end": 1169.22, "text": " assertions for Seattle motel we'd also like to match on documents containing Seattle hotel", "tokens": [19810, 626, 337, 15721, 2184, 338, 321, 1116, 611, 411, 281, 2995, 322, 8512, 19273, 15721, 7622], "temperature": 0.0, "avg_logprob": -0.08813039167427722, "compression_ratio": 1.6919642857142858, "no_speech_prob": 6.004020178806968e-05}, {"id": 165, "seek": 116274, "start": 1169.22, "end": 1175.14, "text": " but our problem is we've got these one hot vectors for the different words and so in a formal", "tokens": [457, 527, 1154, 307, 321, 600, 658, 613, 472, 2368, 18875, 337, 264, 819, 2283, 293, 370, 294, 257, 9860], "temperature": 0.0, "avg_logprob": -0.08813039167427722, "compression_ratio": 1.6919642857142858, "no_speech_prob": 6.004020178806968e-05}, {"id": 166, "seek": 116274, "start": 1175.14, "end": 1181.6200000000001, "text": " mathematical sense these two vectors are orthogonal that there's no natural notion of similarity", "tokens": [18894, 2020, 613, 732, 18875, 366, 41488, 300, 456, 311, 572, 3303, 10710, 295, 32194], "temperature": 0.0, "avg_logprob": -0.08813039167427722, "compression_ratio": 1.6919642857142858, "no_speech_prob": 6.004020178806968e-05}, {"id": 167, "seek": 116274, "start": 1181.6200000000001, "end": 1187.3, "text": " between them whatsoever well there are some things that we could do but try and do about that and", "tokens": [1296, 552, 17076, 731, 456, 366, 512, 721, 300, 321, 727, 360, 457, 853, 293, 360, 466, 300, 293], "temperature": 0.0, "avg_logprob": -0.08813039167427722, "compression_ratio": 1.6919642857142858, "no_speech_prob": 6.004020178806968e-05}, {"id": 168, "seek": 118730, "start": 1187.3, "end": 1195.46, "text": " people did do about that in you know before 2010 we could say hey we could use word net synonyms", "tokens": [561, 630, 360, 466, 300, 294, 291, 458, 949, 9657, 321, 727, 584, 4177, 321, 727, 764, 1349, 2533, 5451, 2526, 2592], "temperature": 0.0, "avg_logprob": -0.11055041443217885, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.905379996169358e-05}, {"id": 169, "seek": 118730, "start": 1195.46, "end": 1201.1399999999999, "text": " and we had count things that list the synonyms are similar anyway or hey maybe we could somehow build", "tokens": [293, 321, 632, 1207, 721, 300, 1329, 264, 5451, 2526, 2592, 366, 2531, 4033, 420, 4177, 1310, 321, 727, 6063, 1322], "temperature": 0.0, "avg_logprob": -0.11055041443217885, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.905379996169358e-05}, {"id": 170, "seek": 118730, "start": 1201.1399999999999, "end": 1208.26, "text": " up representations of words that have meaning overlap and people did all of those things but", "tokens": [493, 33358, 295, 2283, 300, 362, 3620, 19959, 293, 561, 630, 439, 295, 729, 721, 457], "temperature": 0.0, "avg_logprob": -0.11055041443217885, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.905379996169358e-05}, {"id": 171, "seek": 118730, "start": 1208.26, "end": 1215.06, "text": " they tended to fail badly from incompleteness so instead what I want to introduce today is the", "tokens": [436, 34732, 281, 3061, 13425, 490, 14036, 14657, 15264, 370, 2602, 437, 286, 528, 281, 5366, 965, 307, 264], "temperature": 0.0, "avg_logprob": -0.11055041443217885, "compression_ratio": 1.7155555555555555, "no_speech_prob": 6.905379996169358e-05}, {"id": 172, "seek": 121506, "start": 1215.06, "end": 1223.06, "text": " modern deep learning method of doing that where we encode similarity in a real value vector", "tokens": [4363, 2452, 2539, 3170, 295, 884, 300, 689, 321, 2058, 1429, 32194, 294, 257, 957, 2158, 8062], "temperature": 0.0, "avg_logprob": -0.05292462167285737, "compression_ratio": 1.7030303030303031, "no_speech_prob": 5.827972563565709e-05}, {"id": 173, "seek": 121506, "start": 1223.06, "end": 1231.22, "text": " themselves so how do we go about doing that okay and the way we do that is by exploiting this", "tokens": [2969, 370, 577, 360, 321, 352, 466, 884, 300, 1392, 293, 264, 636, 321, 360, 300, 307, 538, 12382, 1748, 341], "temperature": 0.0, "avg_logprob": -0.05292462167285737, "compression_ratio": 1.7030303030303031, "no_speech_prob": 5.827972563565709e-05}, {"id": 174, "seek": 121506, "start": 1231.22, "end": 1239.62, "text": " idea called distributional semantics so the idea of distributional semantics is again something", "tokens": [1558, 1219, 7316, 304, 4361, 45298, 370, 264, 1558, 295, 7316, 304, 4361, 45298, 307, 797, 746], "temperature": 0.0, "avg_logprob": -0.05292462167285737, "compression_ratio": 1.7030303030303031, "no_speech_prob": 5.827972563565709e-05}, {"id": 175, "seek": 123962, "start": 1239.62, "end": 1246.26, "text": " that when you first see it maybe feels a little bit crazy because rather than having something", "tokens": [300, 562, 291, 700, 536, 309, 1310, 3417, 257, 707, 857, 3219, 570, 2831, 813, 1419, 746], "temperature": 0.0, "avg_logprob": -0.08815959824456109, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.3802115871803835e-05}, {"id": 176, "seek": 123962, "start": 1246.26, "end": 1253.54, "text": " like denotational semantics what we're now going to do is say that a words meaning is going to be", "tokens": [411, 1441, 310, 1478, 4361, 45298, 437, 321, 434, 586, 516, 281, 360, 307, 584, 300, 257, 2283, 3620, 307, 516, 281, 312], "temperature": 0.0, "avg_logprob": -0.08815959824456109, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.3802115871803835e-05}, {"id": 177, "seek": 123962, "start": 1253.54, "end": 1262.6599999999999, "text": " given by the words that frequently appear close to it. JR Firth was a British linguist from the", "tokens": [2212, 538, 264, 2283, 300, 10374, 4204, 1998, 281, 309, 13, 32849, 28164, 392, 390, 257, 6221, 21766, 468, 490, 264], "temperature": 0.0, "avg_logprob": -0.08815959824456109, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.3802115871803835e-05}, {"id": 178, "seek": 123962, "start": 1262.6599999999999, "end": 1269.1399999999999, "text": " middle of last century and one of his pity slogans that everyone quotes at this moment is you", "tokens": [2808, 295, 1036, 4901, 293, 472, 295, 702, 21103, 49760, 599, 300, 1518, 19963, 412, 341, 1623, 307, 291], "temperature": 0.0, "avg_logprob": -0.08815959824456109, "compression_ratio": 1.5916666666666666, "no_speech_prob": 5.3802115871803835e-05}, {"id": 179, "seek": 126914, "start": 1269.14, "end": 1278.0200000000002, "text": " shall know a word by the company it keeps and so this idea that you can represent a sense for words", "tokens": [4393, 458, 257, 1349, 538, 264, 2237, 309, 5965, 293, 370, 341, 1558, 300, 291, 393, 2906, 257, 2020, 337, 2283], "temperature": 0.0, "avg_logprob": -0.13048485553625858, "compression_ratio": 1.5885416666666667, "no_speech_prob": 8.791772415861487e-05}, {"id": 180, "seek": 126914, "start": 1278.0200000000002, "end": 1287.3000000000002, "text": " meaning as a notion of what context that appears in has been a very successful idea one of the most", "tokens": [3620, 382, 257, 10710, 295, 437, 4319, 300, 7038, 294, 575, 668, 257, 588, 4406, 1558, 472, 295, 264, 881], "temperature": 0.0, "avg_logprob": -0.13048485553625858, "compression_ratio": 1.5885416666666667, "no_speech_prob": 8.791772415861487e-05}, {"id": 181, "seek": 126914, "start": 1287.3000000000002, "end": 1295.22, "text": " successful ideas that's used throughout statistical and deep learning NLP is actually an interesting idea", "tokens": [4406, 3487, 300, 311, 1143, 3710, 22820, 293, 2452, 2539, 426, 45196, 307, 767, 364, 1880, 1558], "temperature": 0.0, "avg_logprob": -0.13048485553625858, "compression_ratio": 1.5885416666666667, "no_speech_prob": 8.791772415861487e-05}, {"id": 182, "seek": 129522, "start": 1295.22, "end": 1302.42, "text": " more philosophically so that there are kind of interesting connections for example in", "tokens": [544, 14529, 984, 370, 300, 456, 366, 733, 295, 1880, 9271, 337, 1365, 294], "temperature": 0.0, "avg_logprob": -0.14381424384781077, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0002813432947732508}, {"id": 183, "seek": 129522, "start": 1302.42, "end": 1308.02, "text": " Vidconstein's later writings he became enamored of a use theory of meaning and this is a", "tokens": [31185, 1671, 9089, 311, 1780, 30083, 415, 3062, 44549, 2769, 295, 257, 764, 5261, 295, 3620, 293, 341, 307, 257], "temperature": 0.0, "avg_logprob": -0.14381424384781077, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0002813432947732508}, {"id": 184, "seek": 129522, "start": 1308.02, "end": 1312.9, "text": " sentence in some sense a use theory of meaning but whether you know it's the ultimate theory of", "tokens": [8174, 294, 512, 2020, 257, 764, 5261, 295, 3620, 457, 1968, 291, 458, 309, 311, 264, 9705, 5261, 295], "temperature": 0.0, "avg_logprob": -0.14381424384781077, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0002813432947732508}, {"id": 185, "seek": 129522, "start": 1312.9, "end": 1318.5, "text": " semantics it's actually still pretty controversial but it proves to be an extremely computational", "tokens": [4361, 45298, 309, 311, 767, 920, 1238, 17323, 457, 309, 25019, 281, 312, 364, 4664, 28270], "temperature": 0.0, "avg_logprob": -0.14381424384781077, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0002813432947732508}, {"id": 186, "seek": 131850, "start": 1318.5, "end": 1326.1, "text": " sense of semantics which has just led to it being used everywhere very successfully in deep", "tokens": [2020, 295, 4361, 45298, 597, 575, 445, 4684, 281, 309, 885, 1143, 5315, 588, 10727, 294, 2452], "temperature": 0.0, "avg_logprob": -0.07254814379142993, "compression_ratio": 1.5875706214689265, "no_speech_prob": 6.003310045343824e-05}, {"id": 187, "seek": 131850, "start": 1326.1, "end": 1333.78, "text": " learning systems so when a word appears in a text it has a context which are the set of words", "tokens": [2539, 3652, 370, 562, 257, 1349, 7038, 294, 257, 2487, 309, 575, 257, 4319, 597, 366, 264, 992, 295, 2283], "temperature": 0.0, "avg_logprob": -0.07254814379142993, "compression_ratio": 1.5875706214689265, "no_speech_prob": 6.003310045343824e-05}, {"id": 188, "seek": 131850, "start": 1333.78, "end": 1342.18, "text": " that appear in me and so for a particular word my example here is banking we'll find a bunch of", "tokens": [300, 4204, 294, 385, 293, 370, 337, 257, 1729, 1349, 452, 1365, 510, 307, 18261, 321, 603, 915, 257, 3840, 295], "temperature": 0.0, "avg_logprob": -0.07254814379142993, "compression_ratio": 1.5875706214689265, "no_speech_prob": 6.003310045343824e-05}, {"id": 189, "seek": 134218, "start": 1342.18, "end": 1349.7, "text": " places where banking occurs in texts and we'll collect the sort of nearby words as context words", "tokens": [3190, 689, 18261, 11843, 294, 15765, 293, 321, 603, 2500, 264, 1333, 295, 11184, 2283, 382, 4319, 2283], "temperature": 0.0, "avg_logprob": -0.07437510606719226, "compression_ratio": 1.7981220657276995, "no_speech_prob": 4.3927491788053885e-05}, {"id": 190, "seek": 134218, "start": 1349.7, "end": 1355.3, "text": " and we'll see say that those words that are appearing in that kind of muddy brown color around", "tokens": [293, 321, 603, 536, 584, 300, 729, 2283, 300, 366, 19870, 294, 300, 733, 295, 38540, 6292, 2017, 926], "temperature": 0.0, "avg_logprob": -0.07437510606719226, "compression_ratio": 1.7981220657276995, "no_speech_prob": 4.3927491788053885e-05}, {"id": 191, "seek": 134218, "start": 1355.3, "end": 1362.3400000000001, "text": " banking that those context words well in some sense represent the meaning of the word banking", "tokens": [18261, 300, 729, 4319, 2283, 731, 294, 512, 2020, 2906, 264, 3620, 295, 264, 1349, 18261], "temperature": 0.0, "avg_logprob": -0.07437510606719226, "compression_ratio": 1.7981220657276995, "no_speech_prob": 4.3927491788053885e-05}, {"id": 192, "seek": 134218, "start": 1363.6200000000001, "end": 1368.9, "text": " while I'm here let me just mention one distinction that will come up regularly when we're talking", "tokens": [1339, 286, 478, 510, 718, 385, 445, 2152, 472, 16844, 300, 486, 808, 493, 11672, 562, 321, 434, 1417], "temperature": 0.0, "avg_logprob": -0.07437510606719226, "compression_ratio": 1.7981220657276995, "no_speech_prob": 4.3927491788053885e-05}, {"id": 193, "seek": 136890, "start": 1368.9, "end": 1377.46, "text": " about a word in our natural language processing class we sort of have two senses of word which", "tokens": [466, 257, 1349, 294, 527, 3303, 2856, 9007, 1508, 321, 1333, 295, 362, 732, 17057, 295, 1349, 597], "temperature": 0.0, "avg_logprob": -0.10933704603286017, "compression_ratio": 1.7466666666666666, "no_speech_prob": 9.56053045229055e-05}, {"id": 194, "seek": 136890, "start": 1377.46, "end": 1384.8200000000002, "text": " you're referred to as types and tokens so there's a particular instance for words so there's in the", "tokens": [291, 434, 10839, 281, 382, 3467, 293, 22667, 370, 456, 311, 257, 1729, 5197, 337, 2283, 370, 456, 311, 294, 264], "temperature": 0.0, "avg_logprob": -0.10933704603286017, "compression_ratio": 1.7466666666666666, "no_speech_prob": 9.56053045229055e-05}, {"id": 195, "seek": 136890, "start": 1384.8200000000002, "end": 1390.98, "text": " first example government debt problems turning into banking crises there's banking there and that's", "tokens": [700, 1365, 2463, 7831, 2740, 6246, 666, 18261, 31403, 456, 311, 18261, 456, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.10933704603286017, "compression_ratio": 1.7466666666666666, "no_speech_prob": 9.56053045229055e-05}, {"id": 196, "seek": 139098, "start": 1390.98, "end": 1398.9, "text": " a token of the word banking but then I've collected a bunch of instances of quote unquote the word", "tokens": [257, 14862, 295, 264, 1349, 18261, 457, 550, 286, 600, 11087, 257, 3840, 295, 14519, 295, 6513, 37557, 264, 1349], "temperature": 0.0, "avg_logprob": -0.06520873127561627, "compression_ratio": 1.87012987012987, "no_speech_prob": 4.2939929699059576e-05}, {"id": 197, "seek": 139098, "start": 1398.9, "end": 1404.58, "text": " banking and when I say the word banking and a bunch of examples of it I'm then treating banking", "tokens": [18261, 293, 562, 286, 584, 264, 1349, 18261, 293, 257, 3840, 295, 5110, 295, 309, 286, 478, 550, 15083, 18261], "temperature": 0.0, "avg_logprob": -0.06520873127561627, "compression_ratio": 1.87012987012987, "no_speech_prob": 4.2939929699059576e-05}, {"id": 198, "seek": 139098, "start": 1404.58, "end": 1411.6200000000001, "text": " as a type which refers to you know the uses and meaning the word banking has across instances", "tokens": [382, 257, 2010, 597, 14942, 281, 291, 458, 264, 4960, 293, 3620, 264, 1349, 18261, 575, 2108, 14519], "temperature": 0.0, "avg_logprob": -0.06520873127561627, "compression_ratio": 1.87012987012987, "no_speech_prob": 4.2939929699059576e-05}, {"id": 199, "seek": 141162, "start": 1411.62, "end": 1424.26, "text": " okay so what are we going to do with these distributional models of language well what we want to do", "tokens": [1392, 370, 437, 366, 321, 516, 281, 360, 365, 613, 7316, 304, 5245, 295, 2856, 731, 437, 321, 528, 281, 360], "temperature": 0.0, "avg_logprob": -0.15347612147428552, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0001943151728482917}, {"id": 200, "seek": 141162, "start": 1425.06, "end": 1432.8999999999999, "text": " is we're going based on looking at the words that occur in context as vectors that we want to build up", "tokens": [307, 321, 434, 516, 2361, 322, 1237, 412, 264, 2283, 300, 5160, 294, 4319, 382, 18875, 300, 321, 528, 281, 1322, 493], "temperature": 0.0, "avg_logprob": -0.15347612147428552, "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.0001943151728482917}, {"id": 201, "seek": 143290, "start": 1432.9, "end": 1443.0600000000002, "text": " a dense real valued vector for each word that in some sense represents the meaning of that word", "tokens": [257, 18011, 957, 22608, 8062, 337, 1184, 1349, 300, 294, 512, 2020, 8855, 264, 3620, 295, 300, 1349], "temperature": 0.0, "avg_logprob": -0.14627887356665828, "compression_ratio": 1.8431372549019607, "no_speech_prob": 8.328959665959701e-05}, {"id": 202, "seek": 143290, "start": 1443.0600000000002, "end": 1450.3400000000001, "text": " and the way it all represents the meaning of that word is that this vector will be useful", "tokens": [293, 264, 636, 309, 439, 8855, 264, 3620, 295, 300, 1349, 307, 300, 341, 8062, 486, 312, 4420], "temperature": 0.0, "avg_logprob": -0.14627887356665828, "compression_ratio": 1.8431372549019607, "no_speech_prob": 8.328959665959701e-05}, {"id": 203, "seek": 143290, "start": 1450.3400000000001, "end": 1460.26, "text": " for predicting other words that occur in the context so in this example to keep it manageable on", "tokens": [337, 32884, 661, 2283, 300, 5160, 294, 264, 4319, 370, 294, 341, 1365, 281, 1066, 309, 38798, 322], "temperature": 0.0, "avg_logprob": -0.14627887356665828, "compression_ratio": 1.8431372549019607, "no_speech_prob": 8.328959665959701e-05}, {"id": 204, "seek": 146026, "start": 1460.26, "end": 1468.34, "text": " the side vectors are only eight dimensional but in reality we use considerably bigger vectors so", "tokens": [264, 1252, 18875, 366, 787, 3180, 18795, 457, 294, 4103, 321, 764, 31308, 3801, 18875, 370], "temperature": 0.0, "avg_logprob": -0.10509940556117467, "compression_ratio": 1.7285067873303168, "no_speech_prob": 8.706704102223739e-05}, {"id": 205, "seek": 146026, "start": 1468.34, "end": 1475.86, "text": " a very common size is actually 300 dimensional vectors okay so for each word that's a word type", "tokens": [257, 588, 2689, 2744, 307, 767, 6641, 18795, 18875, 1392, 370, 337, 1184, 1349, 300, 311, 257, 1349, 2010], "temperature": 0.0, "avg_logprob": -0.10509940556117467, "compression_ratio": 1.7285067873303168, "no_speech_prob": 8.706704102223739e-05}, {"id": 206, "seek": 146026, "start": 1475.86, "end": 1483.78, "text": " we're going to have a word vector these are also used with other names they refer to as new word", "tokens": [321, 434, 516, 281, 362, 257, 1349, 8062, 613, 366, 611, 1143, 365, 661, 5288, 436, 2864, 281, 382, 777, 1349], "temperature": 0.0, "avg_logprob": -0.10509940556117467, "compression_ratio": 1.7285067873303168, "no_speech_prob": 8.706704102223739e-05}, {"id": 207, "seek": 146026, "start": 1483.78, "end": 1489.86, "text": " representations or for a reason they'll become clear on the next slide they refer to as word", "tokens": [33358, 420, 337, 257, 1778, 436, 603, 1813, 1850, 322, 264, 958, 4137, 436, 2864, 281, 382, 1349], "temperature": 0.0, "avg_logprob": -0.10509940556117467, "compression_ratio": 1.7285067873303168, "no_speech_prob": 8.706704102223739e-05}, {"id": 208, "seek": 148986, "start": 1489.86, "end": 1497.3, "text": " embeddings so these are now distributed representation not a localist representation because the meaning", "tokens": [12240, 29432, 370, 613, 366, 586, 12631, 10290, 406, 257, 2654, 468, 10290, 570, 264, 3620], "temperature": 0.0, "avg_logprob": -0.07397037506103515, "compression_ratio": 1.826086956521739, "no_speech_prob": 3.880680014844984e-05}, {"id": 209, "seek": 148986, "start": 1497.3, "end": 1505.62, "text": " of the word banking is spread over all 300 dimensions of the vector okay these are called", "tokens": [295, 264, 1349, 18261, 307, 3974, 670, 439, 6641, 12819, 295, 264, 8062, 1392, 613, 366, 1219], "temperature": 0.0, "avg_logprob": -0.07397037506103515, "compression_ratio": 1.826086956521739, "no_speech_prob": 3.880680014844984e-05}, {"id": 210, "seek": 148986, "start": 1505.62, "end": 1512.8999999999999, "text": " word embeddings because effectively when we have a whole bunch of words these representations", "tokens": [1349, 12240, 29432, 570, 8659, 562, 321, 362, 257, 1379, 3840, 295, 2283, 613, 33358], "temperature": 0.0, "avg_logprob": -0.07397037506103515, "compression_ratio": 1.826086956521739, "no_speech_prob": 3.880680014844984e-05}, {"id": 211, "seek": 148986, "start": 1512.8999999999999, "end": 1518.9799999999998, "text": " place them all in a high dimensional vector space and so they're embedded into that space", "tokens": [1081, 552, 439, 294, 257, 1090, 18795, 8062, 1901, 293, 370, 436, 434, 16741, 666, 300, 1901], "temperature": 0.0, "avg_logprob": -0.07397037506103515, "compression_ratio": 1.826086956521739, "no_speech_prob": 3.880680014844984e-05}, {"id": 212, "seek": 151898, "start": 1518.98, "end": 1527.8600000000001, "text": " now unfortunately human beings are very bad at looking at 300 dimensional vector spaces or even", "tokens": [586, 7015, 1952, 8958, 366, 588, 1578, 412, 1237, 412, 6641, 18795, 8062, 7673, 420, 754], "temperature": 0.0, "avg_logprob": -0.10045435428619384, "compression_ratio": 1.8743961352657006, "no_speech_prob": 5.734820297220722e-05}, {"id": 213, "seek": 151898, "start": 1527.8600000000001, "end": 1533.8600000000001, "text": " eight dimensional vector spaces so the only thing that I can really display to you here is a two", "tokens": [3180, 18795, 8062, 7673, 370, 264, 787, 551, 300, 286, 393, 534, 4674, 281, 291, 510, 307, 257, 732], "temperature": 0.0, "avg_logprob": -0.10045435428619384, "compression_ratio": 1.8743961352657006, "no_speech_prob": 5.734820297220722e-05}, {"id": 214, "seek": 151898, "start": 1533.8600000000001, "end": 1540.66, "text": " dimensional projection of that space now even that's useful but it's also important to realize that", "tokens": [18795, 22743, 295, 300, 1901, 586, 754, 300, 311, 4420, 457, 309, 311, 611, 1021, 281, 4325, 300], "temperature": 0.0, "avg_logprob": -0.10045435428619384, "compression_ratio": 1.8743961352657006, "no_speech_prob": 5.734820297220722e-05}, {"id": 215, "seek": 151898, "start": 1540.66, "end": 1546.34, "text": " when you're making a two dimensional projection of a 300 dimensional space you're losing almost", "tokens": [562, 291, 434, 1455, 257, 732, 18795, 22743, 295, 257, 6641, 18795, 1901, 291, 434, 7027, 1920], "temperature": 0.0, "avg_logprob": -0.10045435428619384, "compression_ratio": 1.8743961352657006, "no_speech_prob": 5.734820297220722e-05}, {"id": 216, "seek": 154634, "start": 1546.34, "end": 1551.78, "text": " info all the information in that space and a lot of things will be crushed together that don't", "tokens": [13614, 439, 264, 1589, 294, 300, 1901, 293, 257, 688, 295, 721, 486, 312, 19889, 1214, 300, 500, 380], "temperature": 0.0, "avg_logprob": -0.08277068138122559, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.191792322671972e-05}, {"id": 217, "seek": 154634, "start": 1551.78, "end": 1559.9399999999998, "text": " actually deserve to be better so here's my word embeddings of course you can't see any of those at", "tokens": [767, 9948, 281, 312, 1101, 370, 510, 311, 452, 1349, 12240, 29432, 295, 1164, 291, 393, 380, 536, 604, 295, 729, 412], "temperature": 0.0, "avg_logprob": -0.08277068138122559, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.191792322671972e-05}, {"id": 218, "seek": 154634, "start": 1559.9399999999998, "end": 1568.74, "text": " all but if I zoom in and then I zoom in further what you'll already see is that the representations", "tokens": [439, 457, 498, 286, 8863, 294, 293, 550, 286, 8863, 294, 3052, 437, 291, 603, 1217, 536, 307, 300, 264, 33358], "temperature": 0.0, "avg_logprob": -0.08277068138122559, "compression_ratio": 1.60989010989011, "no_speech_prob": 4.191792322671972e-05}, {"id": 219, "seek": 156874, "start": 1568.74, "end": 1578.26, "text": " we've learnt distributionally do a just a good job at grouping together similar words so in this", "tokens": [321, 600, 18991, 7316, 379, 360, 257, 445, 257, 665, 1691, 412, 40149, 1214, 2531, 2283, 370, 294, 341], "temperature": 0.0, "avg_logprob": -0.13148958032781427, "compression_ratio": 1.6424581005586592, "no_speech_prob": 3.16133264277596e-05}, {"id": 220, "seek": 156874, "start": 1578.26, "end": 1584.26, "text": " sort of overall picture I consume into one part of the space is actually the part that's up here", "tokens": [1333, 295, 4787, 3036, 286, 14732, 666, 472, 644, 295, 264, 1901, 307, 767, 264, 644, 300, 311, 493, 510], "temperature": 0.0, "avg_logprob": -0.13148958032781427, "compression_ratio": 1.6424581005586592, "no_speech_prob": 3.16133264277596e-05}, {"id": 221, "seek": 156874, "start": 1584.26, "end": 1593.14, "text": " in this view of it and it's got words for countries so not only countries generally grouped together", "tokens": [294, 341, 1910, 295, 309, 293, 309, 311, 658, 2283, 337, 3517, 370, 406, 787, 3517, 5101, 41877, 1214], "temperature": 0.0, "avg_logprob": -0.13148958032781427, "compression_ratio": 1.6424581005586592, "no_speech_prob": 3.16133264277596e-05}, {"id": 222, "seek": 159314, "start": 1593.14, "end": 1600.5, "text": " even the sort of particular subgroupings of countries make a certain amount of sense and down here", "tokens": [754, 264, 1333, 295, 1729, 1422, 17377, 1109, 295, 3517, 652, 257, 1629, 2372, 295, 2020, 293, 760, 510], "temperature": 0.0, "avg_logprob": -0.09922431171804234, "compression_ratio": 1.6797752808988764, "no_speech_prob": 8.328849798999727e-05}, {"id": 223, "seek": 159314, "start": 1600.5, "end": 1606.9, "text": " we then have nationality words if we go to another part of the space we can see different kind of words", "tokens": [321, 550, 362, 4048, 507, 2283, 498, 321, 352, 281, 1071, 644, 295, 264, 1901, 321, 393, 536, 819, 733, 295, 2283], "temperature": 0.0, "avg_logprob": -0.09922431171804234, "compression_ratio": 1.6797752808988764, "no_speech_prob": 8.328849798999727e-05}, {"id": 224, "seek": 159314, "start": 1606.9, "end": 1615.8600000000001, "text": " so here are verbs and we have ones like come and go a very similar saying and thinking words say", "tokens": [370, 510, 366, 30051, 293, 321, 362, 2306, 411, 808, 293, 352, 257, 588, 2531, 1566, 293, 1953, 2283, 584], "temperature": 0.0, "avg_logprob": -0.09922431171804234, "compression_ratio": 1.6797752808988764, "no_speech_prob": 8.328849798999727e-05}, {"id": 225, "seek": 161586, "start": 1615.86, "end": 1623.3, "text": " think expect a kind of similar and by nearby over in the bottom right we have sort of verbal", "tokens": [519, 2066, 257, 733, 295, 2531, 293, 538, 11184, 670, 294, 264, 2767, 558, 321, 362, 1333, 295, 24781], "temperature": 0.0, "avg_logprob": -0.12429819107055665, "compression_ratio": 1.7442922374429224, "no_speech_prob": 5.5210817663464695e-05}, {"id": 226, "seek": 161586, "start": 1623.3, "end": 1631.1399999999999, "text": " exileries and copulas so have had has forms of the verb to be and certain contentful verbs are", "tokens": [454, 388, 21659, 293, 2971, 16968, 370, 362, 632, 575, 6422, 295, 264, 9595, 281, 312, 293, 1629, 2701, 906, 30051, 366], "temperature": 0.0, "avg_logprob": -0.12429819107055665, "compression_ratio": 1.7442922374429224, "no_speech_prob": 5.5210817663464695e-05}, {"id": 227, "seek": 161586, "start": 1631.1399999999999, "end": 1638.4199999999998, "text": " similar to copula verbs because they describe states you know he remained angry he became angry", "tokens": [2531, 281, 2971, 3780, 30051, 570, 436, 6786, 4368, 291, 458, 415, 12780, 6884, 415, 3062, 6884], "temperature": 0.0, "avg_logprob": -0.12429819107055665, "compression_ratio": 1.7442922374429224, "no_speech_prob": 5.5210817663464695e-05}, {"id": 228, "seek": 161586, "start": 1638.4199999999998, "end": 1643.6999999999998, "text": " and so they're actually then grouped close together to the word the verb to be so there's a lot of", "tokens": [293, 370, 436, 434, 767, 550, 41877, 1998, 1214, 281, 264, 1349, 264, 9595, 281, 312, 370, 456, 311, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.12429819107055665, "compression_ratio": 1.7442922374429224, "no_speech_prob": 5.5210817663464695e-05}, {"id": 229, "seek": 164370, "start": 1643.7, "end": 1651.94, "text": " interesting structure in this space that then represents the meaning of words so the algorithm I'm", "tokens": [1880, 3877, 294, 341, 1901, 300, 550, 8855, 264, 3620, 295, 2283, 370, 264, 9284, 286, 478], "temperature": 0.0, "avg_logprob": -0.15142199397087097, "compression_ratio": 1.5105263157894737, "no_speech_prob": 9.522706386633217e-05}, {"id": 230, "seek": 164370, "start": 1651.94, "end": 1660.1000000000001, "text": " going to introduce now is one that's called word to vac which was introduced by Tamash Mikko", "tokens": [516, 281, 5366, 586, 307, 472, 300, 311, 1219, 1349, 281, 2842, 597, 390, 7268, 538, 8540, 1299, 16380, 4093], "temperature": 0.0, "avg_logprob": -0.15142199397087097, "compression_ratio": 1.5105263157894737, "no_speech_prob": 9.522706386633217e-05}, {"id": 231, "seek": 164370, "start": 1660.1000000000001, "end": 1666.18, "text": " often colleagues in 2013 as a framework for learning word vectors and it's sort of a simple and", "tokens": [2049, 7734, 294, 9012, 382, 257, 8388, 337, 2539, 1349, 18875, 293, 309, 311, 1333, 295, 257, 2199, 293], "temperature": 0.0, "avg_logprob": -0.15142199397087097, "compression_ratio": 1.5105263157894737, "no_speech_prob": 9.522706386633217e-05}, {"id": 232, "seek": 166618, "start": 1666.18, "end": 1674.5, "text": " easy to understand place to start so the idea is we have a lot of text from somewhere which we", "tokens": [1858, 281, 1223, 1081, 281, 722, 370, 264, 1558, 307, 321, 362, 257, 688, 295, 2487, 490, 4079, 597, 321], "temperature": 0.0, "avg_logprob": -0.08282992714329769, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.120233279536478e-05}, {"id": 233, "seek": 166618, "start": 1674.5, "end": 1680.8200000000002, "text": " commonly refer to as a corpus of text corpus is just the Latin word for body so it's a body of text", "tokens": [12719, 2864, 281, 382, 257, 1181, 31624, 295, 2487, 1181, 31624, 307, 445, 264, 10803, 1349, 337, 1772, 370, 309, 311, 257, 1772, 295, 2487], "temperature": 0.0, "avg_logprob": -0.08282992714329769, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.120233279536478e-05}, {"id": 234, "seek": 166618, "start": 1682.18, "end": 1688.8200000000002, "text": " and so then we choose a fix vocabulary which will typically be large but nevertheless truncated", "tokens": [293, 370, 550, 321, 2826, 257, 3191, 19864, 597, 486, 5850, 312, 2416, 457, 26924, 504, 409, 66, 770], "temperature": 0.0, "avg_logprob": -0.08282992714329769, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.120233279536478e-05}, {"id": 235, "seek": 166618, "start": 1688.8200000000002, "end": 1695.22, "text": " so we get rid of some of the really rare words so we might say vocabulary size of 400,000", "tokens": [370, 321, 483, 3973, 295, 512, 295, 264, 534, 5892, 2283, 370, 321, 1062, 584, 19864, 2744, 295, 8423, 11, 1360], "temperature": 0.0, "avg_logprob": -0.08282992714329769, "compression_ratio": 1.6740088105726871, "no_speech_prob": 4.120233279536478e-05}, {"id": 236, "seek": 169522, "start": 1695.22, "end": 1704.42, "text": " and we then create for ourselves a vector for each word okay so then what we do is we want to", "tokens": [293, 321, 550, 1884, 337, 4175, 257, 8062, 337, 1184, 1349, 1392, 370, 550, 437, 321, 360, 307, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.0789191722869873, "compression_ratio": 1.668639053254438, "no_speech_prob": 5.6393018894596025e-05}, {"id": 237, "seek": 169522, "start": 1704.98, "end": 1713.94, "text": " work out what's a good vector to for each word and the really interesting thing is that we can", "tokens": [589, 484, 437, 311, 257, 665, 8062, 281, 337, 1184, 1349, 293, 264, 534, 1880, 551, 307, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.0789191722869873, "compression_ratio": 1.668639053254438, "no_speech_prob": 5.6393018894596025e-05}, {"id": 238, "seek": 169522, "start": 1713.94, "end": 1721.3, "text": " learn these word vectors from just a big pile of text by doing this distributional similarity", "tokens": [1466, 613, 1349, 18875, 490, 445, 257, 955, 14375, 295, 2487, 538, 884, 341, 7316, 304, 32194], "temperature": 0.0, "avg_logprob": -0.0789191722869873, "compression_ratio": 1.668639053254438, "no_speech_prob": 5.6393018894596025e-05}, {"id": 239, "seek": 172130, "start": 1721.3, "end": 1729.1399999999999, "text": " task of being able to predict well what words occur in the context of other words so in particular", "tokens": [5633, 295, 885, 1075, 281, 6069, 731, 437, 2283, 5160, 294, 264, 4319, 295, 661, 2283, 370, 294, 1729], "temperature": 0.0, "avg_logprob": -0.10531409581502278, "compression_ratio": 1.7251461988304093, "no_speech_prob": 9.307105210609734e-05}, {"id": 240, "seek": 172130, "start": 1729.1399999999999, "end": 1738.26, "text": " we're going to iterate through the text and so at any moment we have a center word see and context", "tokens": [321, 434, 516, 281, 44497, 807, 264, 2487, 293, 370, 412, 604, 1623, 321, 362, 257, 3056, 1349, 536, 293, 4319], "temperature": 0.0, "avg_logprob": -0.10531409581502278, "compression_ratio": 1.7251461988304093, "no_speech_prob": 9.307105210609734e-05}, {"id": 241, "seek": 172130, "start": 1738.26, "end": 1746.34, "text": " words outside of it which we'll call oh and then based on the current word vectors we're going to", "tokens": [2283, 2380, 295, 309, 597, 321, 603, 818, 1954, 293, 550, 2361, 322, 264, 2190, 1349, 18875, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.10531409581502278, "compression_ratio": 1.7251461988304093, "no_speech_prob": 9.307105210609734e-05}, {"id": 242, "seek": 174634, "start": 1746.34, "end": 1754.74, "text": " calculate the probability of a context word occurring given the center word according to our", "tokens": [8873, 264, 8482, 295, 257, 4319, 1349, 18386, 2212, 264, 3056, 1349, 4650, 281, 527], "temperature": 0.0, "avg_logprob": -0.05593328711427288, "compression_ratio": 1.945273631840796, "no_speech_prob": 7.249101327033713e-05}, {"id": 243, "seek": 174634, "start": 1754.74, "end": 1761.62, "text": " current model but then we know that certain words did actually occur in the context of that center", "tokens": [2190, 2316, 457, 550, 321, 458, 300, 1629, 2283, 630, 767, 5160, 294, 264, 4319, 295, 300, 3056], "temperature": 0.0, "avg_logprob": -0.05593328711427288, "compression_ratio": 1.945273631840796, "no_speech_prob": 7.249101327033713e-05}, {"id": 244, "seek": 174634, "start": 1761.62, "end": 1768.6599999999999, "text": " word and so what we want to do is then keep adjusting the word vectors to maximize the probability", "tokens": [1349, 293, 370, 437, 321, 528, 281, 360, 307, 550, 1066, 23559, 264, 1349, 18875, 281, 19874, 264, 8482], "temperature": 0.0, "avg_logprob": -0.05593328711427288, "compression_ratio": 1.945273631840796, "no_speech_prob": 7.249101327033713e-05}, {"id": 245, "seek": 174634, "start": 1768.6599999999999, "end": 1775.6999999999998, "text": " that's assigned to words that actually occur in the context of the center word as we proceed through", "tokens": [300, 311, 13279, 281, 2283, 300, 767, 5160, 294, 264, 4319, 295, 264, 3056, 1349, 382, 321, 8991, 807], "temperature": 0.0, "avg_logprob": -0.05593328711427288, "compression_ratio": 1.945273631840796, "no_speech_prob": 7.249101327033713e-05}, {"id": 246, "seek": 177570, "start": 1775.7, "end": 1783.7, "text": " these texts so to start to make that a bit more concrete this is what we're doing um so we have a", "tokens": [613, 15765, 370, 281, 722, 281, 652, 300, 257, 857, 544, 9859, 341, 307, 437, 321, 434, 884, 1105, 370, 321, 362, 257], "temperature": 0.0, "avg_logprob": -0.11534077933665073, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.90493985125795e-05}, {"id": 247, "seek": 177570, "start": 1783.7, "end": 1789.8600000000001, "text": " piece of text we choose our center word which is here in two and then we say well", "tokens": [2522, 295, 2487, 321, 2826, 527, 3056, 1349, 597, 307, 510, 294, 732, 293, 550, 321, 584, 731], "temperature": 0.0, "avg_logprob": -0.11534077933665073, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.90493985125795e-05}, {"id": 248, "seek": 177570, "start": 1792.5, "end": 1798.98, "text": " for model of predicting the probability of context words given the center word and this model", "tokens": [337, 2316, 295, 32884, 264, 8482, 295, 4319, 2283, 2212, 264, 3056, 1349, 293, 341, 2316], "temperature": 0.0, "avg_logprob": -0.11534077933665073, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.90493985125795e-05}, {"id": 249, "seek": 177570, "start": 1798.98, "end": 1805.54, "text": " will come to in a minute but it's defined in terms of our word vectors so let's see what probability", "tokens": [486, 808, 281, 294, 257, 3456, 457, 309, 311, 7642, 294, 2115, 295, 527, 1349, 18875, 370, 718, 311, 536, 437, 8482], "temperature": 0.0, "avg_logprob": -0.11534077933665073, "compression_ratio": 1.7314814814814814, "no_speech_prob": 4.90493985125795e-05}, {"id": 250, "seek": 180554, "start": 1805.54, "end": 1813.46, "text": " it gives to the words that actually occurred in the to the context of this word huh it gives them", "tokens": [309, 2709, 281, 264, 2283, 300, 767, 11068, 294, 264, 281, 264, 4319, 295, 341, 1349, 7020, 309, 2709, 552], "temperature": 0.0, "avg_logprob": -0.0696270117598973, "compression_ratio": 1.813953488372093, "no_speech_prob": 2.5021228793775663e-05}, {"id": 251, "seek": 180554, "start": 1813.46, "end": 1819.46, "text": " some probability but maybe be nice if the probability of assigned was higher so then how can we", "tokens": [512, 8482, 457, 1310, 312, 1481, 498, 264, 8482, 295, 13279, 390, 2946, 370, 550, 577, 393, 321], "temperature": 0.0, "avg_logprob": -0.0696270117598973, "compression_ratio": 1.813953488372093, "no_speech_prob": 2.5021228793775663e-05}, {"id": 252, "seek": 180554, "start": 1819.46, "end": 1826.8999999999999, "text": " change our word vectors to raise those probabilities and so we'll do some calculations with into being", "tokens": [1319, 527, 1349, 18875, 281, 5300, 729, 33783, 293, 370, 321, 603, 360, 512, 20448, 365, 666, 885], "temperature": 0.0, "avg_logprob": -0.0696270117598973, "compression_ratio": 1.813953488372093, "no_speech_prob": 2.5021228793775663e-05}, {"id": 253, "seek": 180554, "start": 1826.8999999999999, "end": 1832.34, "text": " the center word and then we'll just go on to the next word and then we'll do the same kind of", "tokens": [264, 3056, 1349, 293, 550, 321, 603, 445, 352, 322, 281, 264, 958, 1349, 293, 550, 321, 603, 360, 264, 912, 733, 295], "temperature": 0.0, "avg_logprob": -0.0696270117598973, "compression_ratio": 1.813953488372093, "no_speech_prob": 2.5021228793775663e-05}, {"id": 254, "seek": 183234, "start": 1832.34, "end": 1840.1, "text": " calculations and keep on chunking so the big question then is well what are we doing for working out", "tokens": [20448, 293, 1066, 322, 16635, 278, 370, 264, 955, 1168, 550, 307, 731, 437, 366, 321, 884, 337, 1364, 484], "temperature": 0.0, "avg_logprob": -0.12502582979873872, "compression_ratio": 1.7241379310344827, "no_speech_prob": 3.116931111435406e-05}, {"id": 255, "seek": 183234, "start": 1840.1, "end": 1847.9399999999998, "text": " the probability of a word occurring in the context of the center word and so that's the central part", "tokens": [264, 8482, 295, 257, 1349, 18386, 294, 264, 4319, 295, 264, 3056, 1349, 293, 370, 300, 311, 264, 5777, 644], "temperature": 0.0, "avg_logprob": -0.12502582979873872, "compression_ratio": 1.7241379310344827, "no_speech_prob": 3.116931111435406e-05}, {"id": 256, "seek": 183234, "start": 1847.9399999999998, "end": 1857.4599999999998, "text": " of what we develop as the word to take a check so this is the overall model that we want to use so", "tokens": [295, 437, 321, 1499, 382, 264, 1349, 281, 747, 257, 1520, 370, 341, 307, 264, 4787, 2316, 300, 321, 528, 281, 764, 370], "temperature": 0.0, "avg_logprob": -0.12502582979873872, "compression_ratio": 1.7241379310344827, "no_speech_prob": 3.116931111435406e-05}, {"id": 257, "seek": 185746, "start": 1857.46, "end": 1865.6200000000001, "text": " for each position in our corpus our body of text we want to predict context words within a window", "tokens": [337, 1184, 2535, 294, 527, 1181, 31624, 527, 1772, 295, 2487, 321, 528, 281, 6069, 4319, 2283, 1951, 257, 4910], "temperature": 0.0, "avg_logprob": -0.09296747581245973, "compression_ratio": 1.8364485981308412, "no_speech_prob": 6.904634938109666e-05}, {"id": 258, "seek": 185746, "start": 1865.6200000000001, "end": 1873.46, "text": " of fixize m given the center word wj and we want to become good at doing that so we want to give", "tokens": [295, 3191, 1125, 275, 2212, 264, 3056, 1349, 261, 73, 293, 321, 528, 281, 1813, 665, 412, 884, 300, 370, 321, 528, 281, 976], "temperature": 0.0, "avg_logprob": -0.09296747581245973, "compression_ratio": 1.8364485981308412, "no_speech_prob": 6.904634938109666e-05}, {"id": 259, "seek": 185746, "start": 1873.46, "end": 1880.02, "text": " high probability to words that occur in the context and so what we're going to do is we're going to", "tokens": [1090, 8482, 281, 2283, 300, 5160, 294, 264, 4319, 293, 370, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.09296747581245973, "compression_ratio": 1.8364485981308412, "no_speech_prob": 6.904634938109666e-05}, {"id": 260, "seek": 185746, "start": 1880.02, "end": 1886.66, "text": " work out what's formerly the data likelihood as to how good a job we do at predicting words in the", "tokens": [589, 484, 437, 311, 34777, 264, 1412, 22119, 382, 281, 577, 665, 257, 1691, 321, 360, 412, 32884, 2283, 294, 264], "temperature": 0.0, "avg_logprob": -0.09296747581245973, "compression_ratio": 1.8364485981308412, "no_speech_prob": 6.904634938109666e-05}, {"id": 261, "seek": 188666, "start": 1886.66, "end": 1894.1000000000001, "text": " context of other words and so formally that likelihood is going to be defined in terms of our word", "tokens": [4319, 295, 661, 2283, 293, 370, 25983, 300, 22119, 307, 516, 281, 312, 7642, 294, 2115, 295, 527, 1349], "temperature": 0.0, "avg_logprob": -0.07209365669338183, "compression_ratio": 1.9310344827586208, "no_speech_prob": 2.3522630726802163e-05}, {"id": 262, "seek": 188666, "start": 1894.1000000000001, "end": 1900.66, "text": " vectors so they're the parameters of our model and it's going to be calculated as taking the product", "tokens": [18875, 370, 436, 434, 264, 9834, 295, 527, 2316, 293, 309, 311, 516, 281, 312, 15598, 382, 1940, 264, 1674], "temperature": 0.0, "avg_logprob": -0.07209365669338183, "compression_ratio": 1.9310344827586208, "no_speech_prob": 2.3522630726802163e-05}, {"id": 263, "seek": 188666, "start": 1900.66, "end": 1907.14, "text": " of using each word as the center word and then the product of each word in a window around that", "tokens": [295, 1228, 1184, 1349, 382, 264, 3056, 1349, 293, 550, 264, 1674, 295, 1184, 1349, 294, 257, 4910, 926, 300], "temperature": 0.0, "avg_logprob": -0.07209365669338183, "compression_ratio": 1.9310344827586208, "no_speech_prob": 2.3522630726802163e-05}, {"id": 264, "seek": 188666, "start": 1907.14, "end": 1915.78, "text": " of the probability of predicting that context word in the center word and so to learn this model", "tokens": [295, 264, 8482, 295, 32884, 300, 4319, 1349, 294, 264, 3056, 1349, 293, 370, 281, 1466, 341, 2316], "temperature": 0.0, "avg_logprob": -0.07209365669338183, "compression_ratio": 1.9310344827586208, "no_speech_prob": 2.3522630726802163e-05}, {"id": 265, "seek": 191578, "start": 1915.78, "end": 1920.8999999999999, "text": " we're going to have an objective function sometimes also called a cost or a loss that we want to", "tokens": [321, 434, 516, 281, 362, 364, 10024, 2445, 2171, 611, 1219, 257, 2063, 420, 257, 4470, 300, 321, 528, 281], "temperature": 0.0, "avg_logprob": -0.07741148677873022, "compression_ratio": 1.7064220183486238, "no_speech_prob": 7.584012928418815e-05}, {"id": 266, "seek": 191578, "start": 1920.8999999999999, "end": 1928.42, "text": " optimize and essentially what we want to do is we want to maximize the likelihood of the context", "tokens": [19719, 293, 4476, 437, 321, 528, 281, 360, 307, 321, 528, 281, 19874, 264, 22119, 295, 264, 4319], "temperature": 0.0, "avg_logprob": -0.07741148677873022, "compression_ratio": 1.7064220183486238, "no_speech_prob": 7.584012928418815e-05}, {"id": 267, "seek": 191578, "start": 1928.42, "end": 1934.58, "text": " we see around center words but following standard practice we slightly fiddle that", "tokens": [321, 536, 926, 3056, 2283, 457, 3480, 3832, 3124, 321, 4748, 24553, 2285, 300], "temperature": 0.0, "avg_logprob": -0.07741148677873022, "compression_ratio": 1.7064220183486238, "no_speech_prob": 7.584012928418815e-05}, {"id": 268, "seek": 191578, "start": 1935.7, "end": 1941.94, "text": " because rather than dealing with products it's easier to deal with sums and so we work with log", "tokens": [570, 2831, 813, 6260, 365, 3383, 309, 311, 3571, 281, 2028, 365, 34499, 293, 370, 321, 589, 365, 3565], "temperature": 0.0, "avg_logprob": -0.07741148677873022, "compression_ratio": 1.7064220183486238, "no_speech_prob": 7.584012928418815e-05}, {"id": 269, "seek": 194194, "start": 1941.94, "end": 1948.66, "text": " likelihood and once we take log likelihood all of our products turn into sums we also work with", "tokens": [22119, 293, 1564, 321, 747, 3565, 22119, 439, 295, 527, 3383, 1261, 666, 34499, 321, 611, 589, 365], "temperature": 0.0, "avg_logprob": -0.07155258354099317, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.69830439821817e-05}, {"id": 270, "seek": 194194, "start": 1948.66, "end": 1956.1000000000001, "text": " the average log likelihood so we've got a one-on-t term here for the number of words in the corpus", "tokens": [264, 4274, 3565, 22119, 370, 321, 600, 658, 257, 472, 12, 266, 12, 83, 1433, 510, 337, 264, 1230, 295, 2283, 294, 264, 1181, 31624], "temperature": 0.0, "avg_logprob": -0.07155258354099317, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.69830439821817e-05}, {"id": 271, "seek": 194194, "start": 1956.1000000000001, "end": 1962.1000000000001, "text": " and finally for no particular reason we like to minimize our objective function rather than", "tokens": [293, 2721, 337, 572, 1729, 1778, 321, 411, 281, 17522, 527, 10024, 2445, 2831, 813], "temperature": 0.0, "avg_logprob": -0.07155258354099317, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.69830439821817e-05}, {"id": 272, "seek": 194194, "start": 1962.1000000000001, "end": 1969.3, "text": " maximizing it so we stick a minus sign in there and so then by minimizing this objective function", "tokens": [5138, 3319, 309, 370, 321, 2897, 257, 3175, 1465, 294, 456, 293, 370, 550, 538, 46608, 341, 10024, 2445], "temperature": 0.0, "avg_logprob": -0.07155258354099317, "compression_ratio": 1.7534246575342465, "no_speech_prob": 7.69830439821817e-05}, {"id": 273, "seek": 196930, "start": 1969.3, "end": 1980.82, "text": " j of theta that comes as maximizing our predictive accuracy okay so that's the setup but we still", "tokens": [361, 295, 9725, 300, 1487, 382, 5138, 3319, 527, 35521, 14170, 1392, 370, 300, 311, 264, 8657, 457, 321, 920], "temperature": 0.0, "avg_logprob": -0.10968278771016135, "compression_ratio": 1.5873015873015872, "no_speech_prob": 3.940480382880196e-05}, {"id": 274, "seek": 196930, "start": 1980.82, "end": 1987.94, "text": " haven't made any progress in how do we calculate the probability of a word occurring in the context", "tokens": [2378, 380, 1027, 604, 4205, 294, 577, 360, 321, 8873, 264, 8482, 295, 257, 1349, 18386, 294, 264, 4319], "temperature": 0.0, "avg_logprob": -0.10968278771016135, "compression_ratio": 1.5873015873015872, "no_speech_prob": 3.940480382880196e-05}, {"id": 275, "seek": 196930, "start": 1987.94, "end": 1995.78, "text": " given the center word and so the way we're actually going to do that is we have vector representations", "tokens": [2212, 264, 3056, 1349, 293, 370, 264, 636, 321, 434, 767, 516, 281, 360, 300, 307, 321, 362, 8062, 33358], "temperature": 0.0, "avg_logprob": -0.10968278771016135, "compression_ratio": 1.5873015873015872, "no_speech_prob": 3.940480382880196e-05}, {"id": 276, "seek": 199578, "start": 1995.78, "end": 2003.7, "text": " for each word and we're going to work out the probability simply in terms of the word vectors", "tokens": [337, 1184, 1349, 293, 321, 434, 516, 281, 589, 484, 264, 8482, 2935, 294, 2115, 295, 264, 1349, 18875], "temperature": 0.0, "avg_logprob": -0.07267667470353373, "compression_ratio": 1.8926829268292682, "no_speech_prob": 0.00010884129005717114}, {"id": 277, "seek": 199578, "start": 2004.34, "end": 2010.18, "text": " now at this point there's a little technical point we're actually going to give to each word two", "tokens": [586, 412, 341, 935, 456, 311, 257, 707, 6191, 935, 321, 434, 767, 516, 281, 976, 281, 1184, 1349, 732], "temperature": 0.0, "avg_logprob": -0.07267667470353373, "compression_ratio": 1.8926829268292682, "no_speech_prob": 0.00010884129005717114}, {"id": 278, "seek": 199578, "start": 2010.18, "end": 2017.1399999999999, "text": " word vectors one word vector for when it's used as the center word and a different word vector", "tokens": [1349, 18875, 472, 1349, 8062, 337, 562, 309, 311, 1143, 382, 264, 3056, 1349, 293, 257, 819, 1349, 8062], "temperature": 0.0, "avg_logprob": -0.07267667470353373, "compression_ratio": 1.8926829268292682, "no_speech_prob": 0.00010884129005717114}, {"id": 279, "seek": 199578, "start": 2017.1399999999999, "end": 2024.82, "text": " when it's used as a context word this is done because it just simplifies the math and the optimization", "tokens": [562, 309, 311, 1143, 382, 257, 4319, 1349, 341, 307, 1096, 570, 309, 445, 6883, 11221, 264, 5221, 293, 264, 19618], "temperature": 0.0, "avg_logprob": -0.07267667470353373, "compression_ratio": 1.8926829268292682, "no_speech_prob": 0.00010884129005717114}, {"id": 280, "seek": 202482, "start": 2024.82, "end": 2032.1, "text": " so it seems a little bit ugly but actually makes building word vectors a lot easier and really", "tokens": [370, 309, 2544, 257, 707, 857, 12246, 457, 767, 1669, 2390, 1349, 18875, 257, 688, 3571, 293, 534], "temperature": 0.0, "avg_logprob": -0.08537165871981917, "compression_ratio": 1.8, "no_speech_prob": 3.702573667396791e-05}, {"id": 281, "seek": 202482, "start": 2032.1, "end": 2039.06, "text": " we can come back to them discuss it later but that's what it is and so then once we have these word", "tokens": [321, 393, 808, 646, 281, 552, 2248, 309, 1780, 457, 300, 311, 437, 309, 307, 293, 370, 550, 1564, 321, 362, 613, 1349], "temperature": 0.0, "avg_logprob": -0.08537165871981917, "compression_ratio": 1.8, "no_speech_prob": 3.702573667396791e-05}, {"id": 282, "seek": 202482, "start": 2039.06, "end": 2048.42, "text": " vectors the equation that we're going to use for giving the probability of a context word appearing", "tokens": [18875, 264, 5367, 300, 321, 434, 516, 281, 764, 337, 2902, 264, 8482, 295, 257, 4319, 1349, 19870], "temperature": 0.0, "avg_logprob": -0.08537165871981917, "compression_ratio": 1.8, "no_speech_prob": 3.702573667396791e-05}, {"id": 283, "seek": 202482, "start": 2048.42, "end": 2053.46, "text": " given the center word is that we're going to calculate it using the expression in the middle", "tokens": [2212, 264, 3056, 1349, 307, 300, 321, 434, 516, 281, 8873, 309, 1228, 264, 6114, 294, 264, 2808], "temperature": 0.0, "avg_logprob": -0.08537165871981917, "compression_ratio": 1.8, "no_speech_prob": 3.702573667396791e-05}, {"id": 284, "seek": 205346, "start": 2053.46, "end": 2066.42, "text": " bottom of my slide so let's sort of pull that apart just a little bit more so what we have here", "tokens": [2767, 295, 452, 4137, 370, 718, 311, 1333, 295, 2235, 300, 4936, 445, 257, 707, 857, 544, 370, 437, 321, 362, 510], "temperature": 0.0, "avg_logprob": -0.07602428382551166, "compression_ratio": 1.6589595375722543, "no_speech_prob": 2.3177404727903195e-05}, {"id": 285, "seek": 205346, "start": 2066.42, "end": 2074.5, "text": " with this expression is so for a particular center word and a particular context word oh we're", "tokens": [365, 341, 6114, 307, 370, 337, 257, 1729, 3056, 1349, 293, 257, 1729, 4319, 1349, 1954, 321, 434], "temperature": 0.0, "avg_logprob": -0.07602428382551166, "compression_ratio": 1.6589595375722543, "no_speech_prob": 2.3177404727903195e-05}, {"id": 286, "seek": 205346, "start": 2074.5, "end": 2081.54, "text": " going to look up the vector representation of each word so they're u of o and v of c and so then", "tokens": [516, 281, 574, 493, 264, 8062, 10290, 295, 1184, 1349, 370, 436, 434, 344, 295, 277, 293, 371, 295, 269, 293, 370, 550], "temperature": 0.0, "avg_logprob": -0.07602428382551166, "compression_ratio": 1.6589595375722543, "no_speech_prob": 2.3177404727903195e-05}, {"id": 287, "seek": 208154, "start": 2081.54, "end": 2088.42, "text": " we're simply going to take the dot product of those two vectors so dot product is a natural measure", "tokens": [321, 434, 2935, 516, 281, 747, 264, 5893, 1674, 295, 729, 732, 18875, 370, 5893, 1674, 307, 257, 3303, 3481], "temperature": 0.0, "avg_logprob": -0.10949208362992988, "compression_ratio": 1.89, "no_speech_prob": 7.354449189733714e-05}, {"id": 288, "seek": 208154, "start": 2088.42, "end": 2096.74, "text": " for similarity between words because in any particular mention positive you'll get some", "tokens": [337, 32194, 1296, 2283, 570, 294, 604, 1729, 2152, 3353, 291, 603, 483, 512], "temperature": 0.0, "avg_logprob": -0.10949208362992988, "compression_ratio": 1.89, "no_speech_prob": 7.354449189733714e-05}, {"id": 289, "seek": 208154, "start": 2096.74, "end": 2102.02, "text": " component that adds to the dot product sum if both are negative it'll add a lot to the dot product", "tokens": [6542, 300, 10860, 281, 264, 5893, 1674, 2408, 498, 1293, 366, 3671, 309, 603, 909, 257, 688, 281, 264, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.10949208362992988, "compression_ratio": 1.89, "no_speech_prob": 7.354449189733714e-05}, {"id": 290, "seek": 208154, "start": 2102.02, "end": 2109.54, "text": " sum if one's positive and one's negative it'll subtract from the similarity measure if both", "tokens": [2408, 498, 472, 311, 3353, 293, 472, 311, 3671, 309, 603, 16390, 490, 264, 32194, 3481, 498, 1293], "temperature": 0.0, "avg_logprob": -0.10949208362992988, "compression_ratio": 1.89, "no_speech_prob": 7.354449189733714e-05}, {"id": 291, "seek": 210954, "start": 2109.54, "end": 2115.46, "text": " them zero it won't change the similarity so it sort of seems a sort of plausible idea to just", "tokens": [552, 4018, 309, 1582, 380, 1319, 264, 32194, 370, 309, 1333, 295, 2544, 257, 1333, 295, 39925, 1558, 281, 445], "temperature": 0.0, "avg_logprob": -0.07688532405429416, "compression_ratio": 1.7962962962962963, "no_speech_prob": 4.005343726021238e-05}, {"id": 292, "seek": 210954, "start": 2115.46, "end": 2122.42, "text": " take a dot product and thinking well if two words have a larger dot product that means they're more", "tokens": [747, 257, 5893, 1674, 293, 1953, 731, 498, 732, 2283, 362, 257, 4833, 5893, 1674, 300, 1355, 436, 434, 544], "temperature": 0.0, "avg_logprob": -0.07688532405429416, "compression_ratio": 1.7962962962962963, "no_speech_prob": 4.005343726021238e-05}, {"id": 293, "seek": 210954, "start": 2122.42, "end": 2131.22, "text": " similar and so then after that we sort of really doing nothing more than okay we want to use dot", "tokens": [2531, 293, 370, 550, 934, 300, 321, 1333, 295, 534, 884, 1825, 544, 813, 1392, 321, 528, 281, 764, 5893], "temperature": 0.0, "avg_logprob": -0.07688532405429416, "compression_ratio": 1.7962962962962963, "no_speech_prob": 4.005343726021238e-05}, {"id": 294, "seek": 210954, "start": 2131.22, "end": 2138.1, "text": " products to represent word similarity and now let's do the dumbest thing that we know how to turn", "tokens": [3383, 281, 2906, 1349, 32194, 293, 586, 718, 311, 360, 264, 10316, 377, 551, 300, 321, 458, 577, 281, 1261], "temperature": 0.0, "avg_logprob": -0.07688532405429416, "compression_ratio": 1.7962962962962963, "no_speech_prob": 4.005343726021238e-05}, {"id": 295, "seek": 213810, "start": 2138.1, "end": 2145.7, "text": " this into a probability distribution well what do we do well firstly well taking a dot product of", "tokens": [341, 666, 257, 8482, 7316, 731, 437, 360, 321, 360, 731, 27376, 731, 1940, 257, 5893, 1674, 295], "temperature": 0.0, "avg_logprob": -0.09485166887693768, "compression_ratio": 1.8495145631067962, "no_speech_prob": 4.60798000858631e-05}, {"id": 296, "seek": 213810, "start": 2145.7, "end": 2152.18, "text": " two vectors that might come out as positive or negative but well we want to have probabilities we", "tokens": [732, 18875, 300, 1062, 808, 484, 382, 3353, 420, 3671, 457, 731, 321, 528, 281, 362, 33783, 321], "temperature": 0.0, "avg_logprob": -0.09485166887693768, "compression_ratio": 1.8495145631067962, "no_speech_prob": 4.60798000858631e-05}, {"id": 297, "seek": 213810, "start": 2152.18, "end": 2157.2999999999997, "text": " can't have negative probabilities so a simple way to avoid negative probabilities is to", "tokens": [393, 380, 362, 3671, 33783, 370, 257, 2199, 636, 281, 5042, 3671, 33783, 307, 281], "temperature": 0.0, "avg_logprob": -0.09485166887693768, "compression_ratio": 1.8495145631067962, "no_speech_prob": 4.60798000858631e-05}, {"id": 298, "seek": 213810, "start": 2157.2999999999997, "end": 2163.38, "text": " exponentiate them because then we know everything is positive and so then we are always getting a", "tokens": [37871, 13024, 552, 570, 550, 321, 458, 1203, 307, 3353, 293, 370, 550, 321, 366, 1009, 1242, 257], "temperature": 0.0, "avg_logprob": -0.09485166887693768, "compression_ratio": 1.8495145631067962, "no_speech_prob": 4.60798000858631e-05}, {"id": 299, "seek": 216338, "start": 2163.38, "end": 2171.2200000000003, "text": " positive number in the numerator but for probabilities we also want to have the numbers add up to one", "tokens": [3353, 1230, 294, 264, 30380, 457, 337, 33783, 321, 611, 528, 281, 362, 264, 3547, 909, 493, 281, 472], "temperature": 0.0, "avg_logprob": -0.07728134293154061, "compression_ratio": 1.8064516129032258, "no_speech_prob": 1.803857412596699e-05}, {"id": 300, "seek": 216338, "start": 2171.2200000000003, "end": 2177.38, "text": " so we have a probability distribution so we're just normalizing in the obvious way where we divide", "tokens": [370, 321, 362, 257, 8482, 7316, 370, 321, 434, 445, 2710, 3319, 294, 264, 6322, 636, 689, 321, 9845], "temperature": 0.0, "avg_logprob": -0.07728134293154061, "compression_ratio": 1.8064516129032258, "no_speech_prob": 1.803857412596699e-05}, {"id": 301, "seek": 216338, "start": 2177.38, "end": 2183.86, "text": " through by the sum of the numerator quantity for each different word in the vocabulary and so", "tokens": [807, 538, 264, 2408, 295, 264, 30380, 11275, 337, 1184, 819, 1349, 294, 264, 19864, 293, 370], "temperature": 0.0, "avg_logprob": -0.07728134293154061, "compression_ratio": 1.8064516129032258, "no_speech_prob": 1.803857412596699e-05}, {"id": 302, "seek": 216338, "start": 2183.86, "end": 2190.5, "text": " then necessarily that gives us a probability distribution so all the rest of that that I was just", "tokens": [550, 4725, 300, 2709, 505, 257, 8482, 7316, 370, 439, 264, 1472, 295, 300, 300, 286, 390, 445], "temperature": 0.0, "avg_logprob": -0.07728134293154061, "compression_ratio": 1.8064516129032258, "no_speech_prob": 1.803857412596699e-05}, {"id": 303, "seek": 219050, "start": 2190.5, "end": 2196.42, "text": " talking through what we're using there is what's called the softmax function so the softmax", "tokens": [1417, 807, 437, 321, 434, 1228, 456, 307, 437, 311, 1219, 264, 2787, 41167, 2445, 370, 264, 2787, 41167], "temperature": 0.0, "avg_logprob": -0.07225129279223355, "compression_ratio": 1.7864077669902914, "no_speech_prob": 5.2196297474438325e-05}, {"id": 304, "seek": 219050, "start": 2196.42, "end": 2207.54, "text": " function will take any Rn vector and turn it into things between 0 to 1 and so we can take", "tokens": [2445, 486, 747, 604, 497, 77, 8062, 293, 1261, 309, 666, 721, 1296, 1958, 281, 502, 293, 370, 321, 393, 747], "temperature": 0.0, "avg_logprob": -0.07225129279223355, "compression_ratio": 1.7864077669902914, "no_speech_prob": 5.2196297474438325e-05}, {"id": 305, "seek": 219050, "start": 2207.54, "end": 2213.62, "text": " numbers and put them through this softmax and turn them into a probability distribution right so", "tokens": [3547, 293, 829, 552, 807, 341, 2787, 41167, 293, 1261, 552, 666, 257, 8482, 7316, 558, 370], "temperature": 0.0, "avg_logprob": -0.07225129279223355, "compression_ratio": 1.7864077669902914, "no_speech_prob": 5.2196297474438325e-05}, {"id": 306, "seek": 219050, "start": 2213.62, "end": 2219.22, "text": " the name comes from the fact that it's sort of like a max so because of the fact that we", "tokens": [264, 1315, 1487, 490, 264, 1186, 300, 309, 311, 1333, 295, 411, 257, 11469, 370, 570, 295, 264, 1186, 300, 321], "temperature": 0.0, "avg_logprob": -0.07225129279223355, "compression_ratio": 1.7864077669902914, "no_speech_prob": 5.2196297474438325e-05}, {"id": 307, "seek": 221922, "start": 2219.22, "end": 2227.8599999999997, "text": " exponentiate that really emphasizes the big contents in the different dimensions of calculating", "tokens": [37871, 13024, 300, 534, 48856, 264, 955, 15768, 294, 264, 819, 12819, 295, 28258], "temperature": 0.0, "avg_logprob": -0.07942374756461695, "compression_ratio": 1.7548076923076923, "no_speech_prob": 3.933041080017574e-05}, {"id": 308, "seek": 221922, "start": 2228.4199999999996, "end": 2236.02, "text": " similarity so most of the probability goes to the most similar things and it's called soft", "tokens": [32194, 370, 881, 295, 264, 8482, 1709, 281, 264, 881, 2531, 721, 293, 309, 311, 1219, 2787], "temperature": 0.0, "avg_logprob": -0.07942374756461695, "compression_ratio": 1.7548076923076923, "no_speech_prob": 3.933041080017574e-05}, {"id": 309, "seek": 221922, "start": 2236.02, "end": 2242.58, "text": " because well it doesn't do that absolutely it'll still give some probability to everything", "tokens": [570, 731, 309, 1177, 380, 360, 300, 3122, 309, 603, 920, 976, 512, 8482, 281, 1203], "temperature": 0.0, "avg_logprob": -0.07942374756461695, "compression_ratio": 1.7548076923076923, "no_speech_prob": 3.933041080017574e-05}, {"id": 310, "seek": 221922, "start": 2243.3799999999997, "end": 2248.74, "text": " that's in the slightest bit similar I mean on the other hand it's a slightly weird name", "tokens": [300, 311, 294, 264, 41040, 857, 2531, 286, 914, 322, 264, 661, 1011, 309, 311, 257, 4748, 3657, 1315], "temperature": 0.0, "avg_logprob": -0.07942374756461695, "compression_ratio": 1.7548076923076923, "no_speech_prob": 3.933041080017574e-05}, {"id": 311, "seek": 224874, "start": 2248.74, "end": 2255.54, "text": " because you know max normally takes a set of things and just returns one the biggest of them", "tokens": [570, 291, 458, 11469, 5646, 2516, 257, 992, 295, 721, 293, 445, 11247, 472, 264, 3880, 295, 552], "temperature": 0.0, "avg_logprob": -0.08715871023753333, "compression_ratio": 1.6428571428571428, "no_speech_prob": 7.904465746833012e-05}, {"id": 312, "seek": 224874, "start": 2255.54, "end": 2262.3399999999997, "text": " whereas the softmax is taking a set of numbers and is scaling them but is returning the whole", "tokens": [9735, 264, 2787, 41167, 307, 1940, 257, 992, 295, 3547, 293, 307, 21589, 552, 457, 307, 12678, 264, 1379], "temperature": 0.0, "avg_logprob": -0.08715871023753333, "compression_ratio": 1.6428571428571428, "no_speech_prob": 7.904465746833012e-05}, {"id": 313, "seek": 224874, "start": 2262.3399999999997, "end": 2270.8999999999996, "text": " probability distribution okay so now we have all the pieces of our model and so how do we", "tokens": [8482, 7316, 1392, 370, 586, 321, 362, 439, 264, 3755, 295, 527, 2316, 293, 370, 577, 360, 321], "temperature": 0.0, "avg_logprob": -0.08715871023753333, "compression_ratio": 1.6428571428571428, "no_speech_prob": 7.904465746833012e-05}, {"id": 314, "seek": 227090, "start": 2270.9, "end": 2281.86, "text": " make our word vectors well the idea of what we want to do is we want to fiddle our word vectors", "tokens": [652, 527, 1349, 18875, 731, 264, 1558, 295, 437, 321, 528, 281, 360, 307, 321, 528, 281, 24553, 2285, 527, 1349, 18875], "temperature": 0.0, "avg_logprob": -0.10567740426547285, "compression_ratio": 1.8397435897435896, "no_speech_prob": 9.016215335577726e-05}, {"id": 315, "seek": 227090, "start": 2281.86, "end": 2288.26, "text": " in such a way that we minimize our loss i that we maximize the probability of the words that we", "tokens": [294, 1270, 257, 636, 300, 321, 17522, 527, 4470, 741, 300, 321, 19874, 264, 8482, 295, 264, 2283, 300, 321], "temperature": 0.0, "avg_logprob": -0.10567740426547285, "compression_ratio": 1.8397435897435896, "no_speech_prob": 9.016215335577726e-05}, {"id": 316, "seek": 227090, "start": 2288.26, "end": 2296.9, "text": " actually saw in the context of the center word and so the theta the theta represents all of our", "tokens": [767, 1866, 294, 264, 4319, 295, 264, 3056, 1349, 293, 370, 264, 9725, 264, 9725, 8855, 439, 295, 527], "temperature": 0.0, "avg_logprob": -0.10567740426547285, "compression_ratio": 1.8397435897435896, "no_speech_prob": 9.016215335577726e-05}, {"id": 317, "seek": 229690, "start": 2296.9, "end": 2304.34, "text": " model parameters in one very long vector so for our model here the only parameters are our word", "tokens": [2316, 9834, 294, 472, 588, 938, 8062, 370, 337, 527, 2316, 510, 264, 787, 9834, 366, 527, 1349], "temperature": 0.0, "avg_logprob": -0.0922621909309836, "compression_ratio": 1.7575757575757576, "no_speech_prob": 6.907397619215772e-05}, {"id": 318, "seek": 229690, "start": 2304.34, "end": 2314.02, "text": " vectors so we have for each word two vectors its context vector and center vector and each of those", "tokens": [18875, 370, 321, 362, 337, 1184, 1349, 732, 18875, 1080, 4319, 8062, 293, 3056, 8062, 293, 1184, 295, 729], "temperature": 0.0, "avg_logprob": -0.0922621909309836, "compression_ratio": 1.7575757575757576, "no_speech_prob": 6.907397619215772e-05}, {"id": 319, "seek": 229690, "start": 2314.02, "end": 2322.1800000000003, "text": " is a d-dimensional vector where d might be 300 and we have v many words so we end up with this", "tokens": [307, 257, 274, 12, 18759, 8062, 689, 274, 1062, 312, 6641, 293, 321, 362, 371, 867, 2283, 370, 321, 917, 493, 365, 341], "temperature": 0.0, "avg_logprob": -0.0922621909309836, "compression_ratio": 1.7575757575757576, "no_speech_prob": 6.907397619215772e-05}, {"id": 320, "seek": 232218, "start": 2322.18, "end": 2331.06, "text": " big huge vector which is 2dv long which if you have a 500,000 vocab times the 300-dimensional", "tokens": [955, 2603, 8062, 597, 307, 568, 67, 85, 938, 597, 498, 291, 362, 257, 5923, 11, 1360, 2329, 455, 1413, 264, 6641, 12, 18759], "temperature": 0.0, "avg_logprob": -0.1751643065567855, "compression_ratio": 1.697674418604651, "no_speech_prob": 3.760493564186618e-05}, {"id": 321, "seek": 232218, "start": 2332.98, "end": 2337.7, "text": " the time it's more mapping I can do in my head but it's got millions of millions of parameters so", "tokens": [264, 565, 309, 311, 544, 18350, 286, 393, 360, 294, 452, 1378, 457, 309, 311, 658, 6803, 295, 6803, 295, 9834, 370], "temperature": 0.0, "avg_logprob": -0.1751643065567855, "compression_ratio": 1.697674418604651, "no_speech_prob": 3.760493564186618e-05}, {"id": 322, "seek": 232218, "start": 2337.7, "end": 2341.62, "text": " we've got millions of millions of parameters and we somehow want to fiddle them all", "tokens": [321, 600, 658, 6803, 295, 6803, 295, 9834, 293, 321, 6063, 528, 281, 24553, 2285, 552, 439], "temperature": 0.0, "avg_logprob": -0.1751643065567855, "compression_ratio": 1.697674418604651, "no_speech_prob": 3.760493564186618e-05}, {"id": 323, "seek": 234162, "start": 2341.62, "end": 2352.5, "text": " to maximize the prediction of context words and so the way we're going to do that then is we use", "tokens": [281, 19874, 264, 17630, 295, 4319, 2283, 293, 370, 264, 636, 321, 434, 516, 281, 360, 300, 550, 307, 321, 764], "temperature": 0.0, "avg_logprob": -0.10096731867109027, "compression_ratio": 1.6875, "no_speech_prob": 7.478705083485693e-05}, {"id": 324, "seek": 234162, "start": 2352.5, "end": 2360.74, "text": " calculus so what we want to do is take that math that we've seen previously and say well with this", "tokens": [33400, 370, 437, 321, 528, 281, 360, 307, 747, 300, 5221, 300, 321, 600, 1612, 8046, 293, 584, 731, 365, 341], "temperature": 0.0, "avg_logprob": -0.10096731867109027, "compression_ratio": 1.6875, "no_speech_prob": 7.478705083485693e-05}, {"id": 325, "seek": 234162, "start": 2360.74, "end": 2370.8199999999997, "text": " objective function we can work out derivatives and so we can work out where the gradient is so how we", "tokens": [10024, 2445, 321, 393, 589, 484, 33733, 293, 370, 321, 393, 589, 484, 689, 264, 16235, 307, 370, 577, 321], "temperature": 0.0, "avg_logprob": -0.10096731867109027, "compression_ratio": 1.6875, "no_speech_prob": 7.478705083485693e-05}, {"id": 326, "seek": 237082, "start": 2370.82, "end": 2379.54, "text": " can walk downhill to minimize loss so at some point and we can figure out what is downhill and we can", "tokens": [393, 1792, 29929, 281, 17522, 4470, 370, 412, 512, 935, 293, 321, 393, 2573, 484, 437, 307, 29929, 293, 321, 393], "temperature": 0.0, "avg_logprob": -0.1071113667017977, "compression_ratio": 1.664835164835165, "no_speech_prob": 8.875481580616906e-05}, {"id": 327, "seek": 237082, "start": 2379.54, "end": 2389.2200000000003, "text": " then progressively walk downhill and improve our model and so what our job is going to be is to compute", "tokens": [550, 46667, 1792, 29929, 293, 3470, 527, 2316, 293, 370, 437, 527, 1691, 307, 516, 281, 312, 307, 281, 14722], "temperature": 0.0, "avg_logprob": -0.1071113667017977, "compression_ratio": 1.664835164835165, "no_speech_prob": 8.875481580616906e-05}, {"id": 328, "seek": 237082, "start": 2389.2200000000003, "end": 2399.78, "text": " all of those vector gradients okay so at this point I then want to kind of show a little bit more", "tokens": [439, 295, 729, 8062, 2771, 2448, 1392, 370, 412, 341, 935, 286, 550, 528, 281, 733, 295, 855, 257, 707, 857, 544], "temperature": 0.0, "avg_logprob": -0.1071113667017977, "compression_ratio": 1.664835164835165, "no_speech_prob": 8.875481580616906e-05}, {"id": 329, "seek": 239978, "start": 2399.78, "end": 2410.82, "text": " as to how we can actually do that and a couple more slides here but maybe I'll just try and", "tokens": [382, 281, 577, 321, 393, 767, 360, 300, 293, 257, 1916, 544, 9788, 510, 457, 1310, 286, 603, 445, 853, 293], "temperature": 0.0, "avg_logprob": -0.08462211933541805, "compression_ratio": 1.3984962406015038, "no_speech_prob": 3.816539901890792e-05}, {"id": 330, "seek": 239978, "start": 2411.46, "end": 2420.7400000000002, "text": " jigger things again and move to my interactive whiteboard what we wanted to do right so we had", "tokens": [361, 6812, 721, 797, 293, 1286, 281, 452, 15141, 2418, 3787, 437, 321, 1415, 281, 360, 558, 370, 321, 632], "temperature": 0.0, "avg_logprob": -0.08462211933541805, "compression_ratio": 1.3984962406015038, "no_speech_prob": 3.816539901890792e-05}, {"id": 331, "seek": 242074, "start": 2420.74, "end": 2430.74, "text": " our overall we had our overall j theta that we were wanting to minimize our average", "tokens": [527, 4787, 321, 632, 527, 4787, 361, 9725, 300, 321, 645, 7935, 281, 17522, 527, 4274], "temperature": 0.0, "avg_logprob": -0.12953807368422998, "compression_ratio": 1.7261146496815287, "no_speech_prob": 1.0608278898871504e-05}, {"id": 332, "seek": 242074, "start": 2430.74, "end": 2439.8599999999997, "text": " neg log likelihood so that was the minus one on t of the sum of t equals one to big t which was our", "tokens": [2485, 3565, 22119, 370, 300, 390, 264, 3175, 472, 322, 256, 295, 264, 2408, 295, 256, 6915, 472, 281, 955, 256, 597, 390, 527], "temperature": 0.0, "avg_logprob": -0.12953807368422998, "compression_ratio": 1.7261146496815287, "no_speech_prob": 1.0608278898871504e-05}, {"id": 333, "seek": 242074, "start": 2439.8599999999997, "end": 2445.7, "text": " text length and then we were going through the words in each context so we were doing j", "tokens": [2487, 4641, 293, 550, 321, 645, 516, 807, 264, 2283, 294, 1184, 4319, 370, 321, 645, 884, 361], "temperature": 0.0, "avg_logprob": -0.12953807368422998, "compression_ratio": 1.7261146496815287, "no_speech_prob": 1.0608278898871504e-05}, {"id": 334, "seek": 244570, "start": 2445.7, "end": 2456.02, "text": " between m words on each side except itself and then what we wanted to do was in the side there", "tokens": [1296, 275, 2283, 322, 1184, 1252, 3993, 2564, 293, 550, 437, 321, 1415, 281, 360, 390, 294, 264, 1252, 456], "temperature": 0.0, "avg_logprob": -0.11665463977389866, "compression_ratio": 1.552, "no_speech_prob": 5.9109392168466e-05}, {"id": 335, "seek": 244570, "start": 2456.02, "end": 2465.46, "text": " we were then we were working out the log probability of the context word at that position given the", "tokens": [321, 645, 550, 321, 645, 1364, 484, 264, 3565, 8482, 295, 264, 4319, 1349, 412, 300, 2535, 2212, 264], "temperature": 0.0, "avg_logprob": -0.11665463977389866, "compression_ratio": 1.552, "no_speech_prob": 5.9109392168466e-05}, {"id": 336, "seek": 246546, "start": 2465.46, "end": 2477.7, "text": " word that's in a center position t and so then we converted that into our word vectors by saying", "tokens": [1349, 300, 311, 294, 257, 3056, 2535, 256, 293, 370, 550, 321, 16424, 300, 666, 527, 1349, 18875, 538, 1566], "temperature": 0.0, "avg_logprob": -0.10095428427060445, "compression_ratio": 1.2151898734177216, "no_speech_prob": 9.506773494649678e-06}, {"id": 337, "seek": 247770, "start": 2477.7, "end": 2504.1, "text": " that the probability of oh given c is going to be expressed as the soft max of the dot product", "tokens": [300, 264, 8482, 295, 1954, 2212, 269, 307, 516, 281, 312, 12675, 382, 264, 2787, 11469, 295, 264, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.2684543927510579, "compression_ratio": 1.1898734177215189, "no_speech_prob": 1.1101779819000512e-05}, {"id": 338, "seek": 250410, "start": 2504.1, "end": 2516.5, "text": " okay and so now what we want to do is work out the gradient the direction of downhill for this", "tokens": [1392, 293, 370, 586, 437, 321, 528, 281, 360, 307, 589, 484, 264, 16235, 264, 3513, 295, 29929, 337, 341], "temperature": 0.0, "avg_logprob": -0.09573518662225633, "compression_ratio": 1.7222222222222223, "no_speech_prob": 9.751040488481522e-05}, {"id": 339, "seek": 250410, "start": 2517.86, "end": 2524.58, "text": " last gen and so the way we're doing that is we're working out the partial derivative of this", "tokens": [1036, 1049, 293, 370, 264, 636, 321, 434, 884, 300, 307, 321, 434, 1364, 484, 264, 14641, 13760, 295, 341], "temperature": 0.0, "avg_logprob": -0.09573518662225633, "compression_ratio": 1.7222222222222223, "no_speech_prob": 9.751040488481522e-05}, {"id": 340, "seek": 252458, "start": 2524.58, "end": 2534.5, "text": " expression with respect to every parameter in the model and all the parameters in the model are", "tokens": [6114, 365, 3104, 281, 633, 13075, 294, 264, 2316, 293, 439, 264, 9834, 294, 264, 2316, 366], "temperature": 0.0, "avg_logprob": -0.06405878457866732, "compression_ratio": 1.9047619047619047, "no_speech_prob": 5.437216896098107e-05}, {"id": 341, "seek": 252458, "start": 2534.5, "end": 2542.5, "text": " the components the dimensions of the word vectors of every word and so we have the center word", "tokens": [264, 6677, 264, 12819, 295, 264, 1349, 18875, 295, 633, 1349, 293, 370, 321, 362, 264, 3056, 1349], "temperature": 0.0, "avg_logprob": -0.06405878457866732, "compression_ratio": 1.9047619047619047, "no_speech_prob": 5.437216896098107e-05}, {"id": 342, "seek": 252458, "start": 2542.5, "end": 2552.1, "text": " vectors and the outside word vectors so here I'm just going to do the center word vectors", "tokens": [18875, 293, 264, 2380, 1349, 18875, 370, 510, 286, 478, 445, 516, 281, 360, 264, 3056, 1349, 18875], "temperature": 0.0, "avg_logprob": -0.06405878457866732, "compression_ratio": 1.9047619047619047, "no_speech_prob": 5.437216896098107e-05}, {"id": 343, "seek": 255210, "start": 2552.1, "end": 2560.42, "text": " but on a future homework assignment 2 the outside word vectors will show up and they're kind of", "tokens": [457, 322, 257, 2027, 14578, 15187, 568, 264, 2380, 1349, 18875, 486, 855, 493, 293, 436, 434, 733, 295], "temperature": 0.0, "avg_logprob": -0.13637564732478216, "compression_ratio": 1.572192513368984, "no_speech_prob": 6.808033504057676e-05}, {"id": 344, "seek": 255210, "start": 2560.42, "end": 2568.74, "text": " similar so what we're doing is we're working out the partial derivative with respect to our center", "tokens": [2531, 370, 437, 321, 434, 884, 307, 321, 434, 1364, 484, 264, 14641, 13760, 365, 3104, 281, 527, 3056], "temperature": 0.0, "avg_logprob": -0.13637564732478216, "compression_ratio": 1.572192513368984, "no_speech_prob": 6.808033504057676e-05}, {"id": 345, "seek": 255210, "start": 2568.74, "end": 2579.14, "text": " word vector which is you know maybe a 300 dimensional word vector of this probability of oh given c", "tokens": [1349, 8062, 597, 307, 291, 458, 1310, 257, 6641, 18795, 1349, 8062, 295, 341, 8482, 295, 1954, 2212, 269], "temperature": 0.0, "avg_logprob": -0.13637564732478216, "compression_ratio": 1.572192513368984, "no_speech_prob": 6.808033504057676e-05}, {"id": 346, "seek": 257914, "start": 2579.14, "end": 2585.7799999999997, "text": " and since we're using log probabilities of the log of this probability of oh given c of this", "tokens": [293, 1670, 321, 434, 1228, 3565, 33783, 295, 264, 3565, 295, 341, 8482, 295, 1954, 2212, 269, 295, 341], "temperature": 0.0, "avg_logprob": -0.1814998704559949, "compression_ratio": 1.48, "no_speech_prob": 6.699706136714667e-05}, {"id": 347, "seek": 257914, "start": 2585.7799999999997, "end": 2595.14, "text": " x of u of o t v c over my writing I'll get worse and worse sorry I've already made a mistake", "tokens": [2031, 295, 344, 295, 277, 256, 371, 269, 670, 452, 3579, 286, 603, 483, 5324, 293, 5324, 2597, 286, 600, 1217, 1027, 257, 6146], "temperature": 0.0, "avg_logprob": -0.1814998704559949, "compression_ratio": 1.48, "no_speech_prob": 6.699706136714667e-05}, {"id": 348, "seek": 259514, "start": 2595.14, "end": 2608.98, "text": " having a sum the sum w equals 1 to the vocabulary of the x of u w t v c okay well at this point things", "tokens": [1419, 257, 2408, 264, 2408, 261, 6915, 502, 281, 264, 19864, 295, 264, 2031, 295, 344, 261, 256, 371, 269, 1392, 731, 412, 341, 935, 721], "temperature": 0.0, "avg_logprob": -0.08102528052993968, "compression_ratio": 1.60989010989011, "no_speech_prob": 7.763254870951641e-06}, {"id": 349, "seek": 259514, "start": 2608.98, "end": 2617.2999999999997, "text": " start off pretty easy so what we have here is something that's log of a over b so that's easy", "tokens": [722, 766, 1238, 1858, 370, 437, 321, 362, 510, 307, 746, 300, 311, 3565, 295, 257, 670, 272, 370, 300, 311, 1858], "temperature": 0.0, "avg_logprob": -0.08102528052993968, "compression_ratio": 1.60989010989011, "no_speech_prob": 7.763254870951641e-06}, {"id": 350, "seek": 259514, "start": 2617.2999999999997, "end": 2623.7799999999997, "text": " we can turn this into log a minus log b but before I go further I'll just make a comment at this", "tokens": [321, 393, 1261, 341, 666, 3565, 257, 3175, 3565, 272, 457, 949, 286, 352, 3052, 286, 603, 445, 652, 257, 2871, 412, 341], "temperature": 0.0, "avg_logprob": -0.08102528052993968, "compression_ratio": 1.60989010989011, "no_speech_prob": 7.763254870951641e-06}, {"id": 351, "seek": 262378, "start": 2623.78, "end": 2634.26, "text": " point you know so at this point my audience divides on into right there are some people in the audience", "tokens": [935, 291, 458, 370, 412, 341, 935, 452, 4034, 41347, 322, 666, 558, 456, 366, 512, 561, 294, 264, 4034], "temperature": 0.0, "avg_logprob": -0.12688196406644933, "compression_ratio": 1.6573033707865168, "no_speech_prob": 0.00011577200348256156}, {"id": 352, "seek": 262378, "start": 2634.9, "end": 2642.1800000000003, "text": " for which maybe a lot of people in the audience this is really elementary math I've seen this", "tokens": [337, 597, 1310, 257, 688, 295, 561, 294, 264, 4034, 341, 307, 534, 16429, 5221, 286, 600, 1612, 341], "temperature": 0.0, "avg_logprob": -0.12688196406644933, "compression_ratio": 1.6573033707865168, "no_speech_prob": 0.00011577200348256156}, {"id": 353, "seek": 262378, "start": 2642.1800000000003, "end": 2648.7400000000002, "text": " a million times before and he isn't even explaining it very well and if you're in that group well", "tokens": [257, 2459, 1413, 949, 293, 415, 1943, 380, 754, 13468, 309, 588, 731, 293, 498, 291, 434, 294, 300, 1594, 731], "temperature": 0.0, "avg_logprob": -0.12688196406644933, "compression_ratio": 1.6573033707865168, "no_speech_prob": 0.00011577200348256156}, {"id": 354, "seek": 264874, "start": 2648.74, "end": 2655.54, "text": " feel free to look at your email or the newspaper or whatever else is best suited to you but I think", "tokens": [841, 1737, 281, 574, 412, 428, 3796, 420, 264, 13669, 420, 2035, 1646, 307, 1151, 24736, 281, 291, 457, 286, 519], "temperature": 0.0, "avg_logprob": -0.06312673468338816, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.00011109598563052714}, {"id": 355, "seek": 264874, "start": 2655.54, "end": 2662.18, "text": " there are also other people in the class who oh the last time I saw calculus was when I was in high", "tokens": [456, 366, 611, 661, 561, 294, 264, 1508, 567, 1954, 264, 1036, 565, 286, 1866, 33400, 390, 562, 286, 390, 294, 1090], "temperature": 0.0, "avg_logprob": -0.06312673468338816, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.00011109598563052714}, {"id": 356, "seek": 264874, "start": 2662.18, "end": 2668.8199999999997, "text": " school for which that's not the case and so I wanted to spend a few minutes going through this a", "tokens": [1395, 337, 597, 300, 311, 406, 264, 1389, 293, 370, 286, 1415, 281, 3496, 257, 1326, 2077, 516, 807, 341, 257], "temperature": 0.0, "avg_logprob": -0.06312673468338816, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.00011109598563052714}, {"id": 357, "seek": 264874, "start": 2668.8199999999997, "end": 2678.18, "text": " bit concretely so that to try and get over the idea that you know even though most of deep learning", "tokens": [857, 39481, 736, 370, 300, 281, 853, 293, 483, 670, 264, 1558, 300, 291, 458, 754, 1673, 881, 295, 2452, 2539], "temperature": 0.0, "avg_logprob": -0.06312673468338816, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.00011109598563052714}, {"id": 358, "seek": 267818, "start": 2678.18, "end": 2686.58, "text": " and even word vector learning seems like magic that it's not really magic it's really just doing", "tokens": [293, 754, 1349, 8062, 2539, 2544, 411, 5585, 300, 309, 311, 406, 534, 5585, 309, 311, 534, 445, 884], "temperature": 0.0, "avg_logprob": -0.08948759836693333, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.75169435958378e-05}, {"id": 359, "seek": 267818, "start": 2686.58, "end": 2692.02, "text": " math and one of the things that we hope is that you do actually understand this math that's being done", "tokens": [5221, 293, 472, 295, 264, 721, 300, 321, 1454, 307, 300, 291, 360, 767, 1223, 341, 5221, 300, 311, 885, 1096], "temperature": 0.0, "avg_logprob": -0.08948759836693333, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.75169435958378e-05}, {"id": 360, "seek": 267818, "start": 2693.14, "end": 2700.98, "text": " so I'll keep along and do a bit more of it okay so then what we have is so use this way of writing", "tokens": [370, 286, 603, 1066, 2051, 293, 360, 257, 857, 544, 295, 309, 1392, 370, 550, 437, 321, 362, 307, 370, 764, 341, 636, 295, 3579], "temperature": 0.0, "avg_logprob": -0.08948759836693333, "compression_ratio": 1.6555555555555554, "no_speech_prob": 4.75169435958378e-05}, {"id": 361, "seek": 270098, "start": 2700.98, "end": 2709.62, "text": " the log and so then we can say that that expression above equals the partial derivatives with", "tokens": [264, 3565, 293, 370, 550, 321, 393, 584, 300, 300, 6114, 3673, 6915, 264, 14641, 33733, 365], "temperature": 0.0, "avg_logprob": -0.1830944333757673, "compression_ratio": 1.1772151898734178, "no_speech_prob": 6.491688691312447e-05}, {"id": 362, "seek": 270962, "start": 2709.62, "end": 2731.38, "text": " a VC of the log of the numerator log x u o to the vc minus the partial derivative of the log of the", "tokens": [257, 41922, 295, 264, 3565, 295, 264, 30380, 3565, 2031, 344, 277, 281, 264, 371, 66, 3175, 264, 14641, 13760, 295, 264, 3565, 295, 264], "temperature": 0.0, "avg_logprob": -0.40782395724592535, "compression_ratio": 1.32, "no_speech_prob": 7.440987246809527e-05}, {"id": 363, "seek": 273138, "start": 2731.38, "end": 2747.06, "text": " denominator so that's then the sum of w equals 1 to v of the x of u w to vc okay so at that point", "tokens": [20687, 370, 300, 311, 550, 264, 2408, 295, 261, 6915, 502, 281, 371, 295, 264, 2031, 295, 344, 261, 281, 371, 66, 1392, 370, 412, 300, 935], "temperature": 0.0, "avg_logprob": -0.21680041459890512, "compression_ratio": 1.5887096774193548, "no_speech_prob": 7.244353037094697e-05}, {"id": 364, "seek": 273138, "start": 2747.06, "end": 2760.5, "text": " I have my numerator here and my former denominator there so at that point there are spots the first", "tokens": [286, 362, 452, 30380, 510, 293, 452, 5819, 20687, 456, 370, 412, 300, 935, 456, 366, 10681, 264, 700], "temperature": 0.0, "avg_logprob": -0.21680041459890512, "compression_ratio": 1.5887096774193548, "no_speech_prob": 7.244353037094697e-05}, {"id": 365, "seek": 276050, "start": 2760.5, "end": 2771.38, "text": " part is the numerator part so the numerator part is really really easy so we have here the log", "tokens": [644, 307, 264, 30380, 644, 370, 264, 30380, 644, 307, 534, 534, 1858, 370, 321, 362, 510, 264, 3565], "temperature": 0.0, "avg_logprob": -0.10197525674646551, "compression_ratio": 1.6, "no_speech_prob": 3.641916191554628e-05}, {"id": 366, "seek": 276050, "start": 2771.38, "end": 2779.7, "text": " and x but just inverses of each other so they just go away so that becomes the derivative", "tokens": [293, 2031, 457, 445, 21378, 279, 295, 1184, 661, 370, 436, 445, 352, 1314, 370, 300, 3643, 264, 13760], "temperature": 0.0, "avg_logprob": -0.10197525674646551, "compression_ratio": 1.6, "no_speech_prob": 3.641916191554628e-05}, {"id": 367, "seek": 277970, "start": 2779.7, "end": 2794.1, "text": " of with respect to VC of just what's left behind which is you use 0 dot product and with VC okay", "tokens": [295, 365, 3104, 281, 41922, 295, 445, 437, 311, 1411, 2261, 597, 307, 291, 764, 1958, 5893, 1674, 293, 365, 41922, 1392], "temperature": 0.0, "avg_logprob": -0.11528640323215061, "compression_ratio": 1.5573770491803278, "no_speech_prob": 3.9320962969213724e-05}, {"id": 368, "seek": 277970, "start": 2795.06, "end": 2801.3799999999997, "text": " and so the thing to be aware of is you know we're still doing this multivariate calculus so", "tokens": [293, 370, 264, 551, 281, 312, 3650, 295, 307, 291, 458, 321, 434, 920, 884, 341, 2120, 592, 3504, 473, 33400, 370], "temperature": 0.0, "avg_logprob": -0.11528640323215061, "compression_ratio": 1.5573770491803278, "no_speech_prob": 3.9320962969213724e-05}, {"id": 369, "seek": 277970, "start": 2801.3799999999997, "end": 2808.1, "text": " what we have here is calculus with respect to a vector like hopefully you saw some of in math 51", "tokens": [437, 321, 362, 510, 307, 33400, 365, 3104, 281, 257, 8062, 411, 4696, 291, 1866, 512, 295, 294, 5221, 18485], "temperature": 0.0, "avg_logprob": -0.11528640323215061, "compression_ratio": 1.5573770491803278, "no_speech_prob": 3.9320962969213724e-05}, {"id": 370, "seek": 280810, "start": 2808.1, "end": 2817.38, "text": " or some other place not high school single variable calculus on the other hand you know to the", "tokens": [420, 512, 661, 1081, 406, 1090, 1395, 2167, 7006, 33400, 322, 264, 661, 1011, 291, 458, 281, 264], "temperature": 0.0, "avg_logprob": -0.09845168040348934, "compression_ratio": 1.6123595505617978, "no_speech_prob": 8.60992877278477e-05}, {"id": 371, "seek": 280810, "start": 2817.38, "end": 2824.5, "text": " extent you and half remember some of this stuff most of the time you can just do perfectly well", "tokens": [8396, 291, 293, 1922, 1604, 512, 295, 341, 1507, 881, 295, 264, 565, 291, 393, 445, 360, 6239, 731], "temperature": 0.0, "avg_logprob": -0.09845168040348934, "compression_ratio": 1.6123595505617978, "no_speech_prob": 8.60992877278477e-05}, {"id": 372, "seek": 280810, "start": 2824.5, "end": 2832.5, "text": " by thinking about what happens with one dimension at a time and it generalizes the multivariable", "tokens": [538, 1953, 466, 437, 2314, 365, 472, 10139, 412, 257, 565, 293, 309, 2674, 5660, 264, 2120, 592, 3504, 712], "temperature": 0.0, "avg_logprob": -0.09845168040348934, "compression_ratio": 1.6123595505617978, "no_speech_prob": 8.60992877278477e-05}, {"id": 373, "seek": 283250, "start": 2832.5, "end": 2842.18, "text": " calculus so if about all that you remember of calculus is that d dx of a x equals a really", "tokens": [33400, 370, 498, 466, 439, 300, 291, 1604, 295, 33400, 307, 300, 274, 30017, 295, 257, 2031, 6915, 257, 534], "temperature": 0.0, "avg_logprob": -0.1407386722849376, "compression_ratio": 1.60625, "no_speech_prob": 1.2604360563273076e-05}, {"id": 374, "seek": 283250, "start": 2842.98, "end": 2849.78, "text": " it's the same thing that we're going to be using here that here we have the", "tokens": [309, 311, 264, 912, 551, 300, 321, 434, 516, 281, 312, 1228, 510, 300, 510, 321, 362, 264], "temperature": 0.0, "avg_logprob": -0.1407386722849376, "compression_ratio": 1.60625, "no_speech_prob": 1.2604360563273076e-05}, {"id": 375, "seek": 283250, "start": 2853.22, "end": 2861.14, "text": " the outside word dot producted with the VC well at the end of the day that's going to have", "tokens": [264, 2380, 1349, 5893, 1674, 292, 365, 264, 41922, 731, 412, 264, 917, 295, 264, 786, 300, 311, 516, 281, 362], "temperature": 0.0, "avg_logprob": -0.1407386722849376, "compression_ratio": 1.60625, "no_speech_prob": 1.2604360563273076e-05}, {"id": 376, "seek": 286114, "start": 2861.14, "end": 2874.9, "text": " terms of sort of you 0 component one times the center word component one plus you 0 component 2 plus", "tokens": [2115, 295, 1333, 295, 291, 1958, 6542, 472, 1413, 264, 3056, 1349, 6542, 472, 1804, 291, 1958, 6542, 568, 1804], "temperature": 0.0, "avg_logprob": -0.2025842865308126, "compression_ratio": 1.7232142857142858, "no_speech_prob": 6.594306614715606e-05}, {"id": 377, "seek": 286114, "start": 2878.02, "end": 2884.3399999999997, "text": " this is the word component 2 and so we're sort of using this bed over here and so what we're", "tokens": [341, 307, 264, 1349, 6542, 568, 293, 370, 321, 434, 1333, 295, 1228, 341, 2901, 670, 510, 293, 370, 437, 321, 434], "temperature": 0.0, "avg_logprob": -0.2025842865308126, "compression_ratio": 1.7232142857142858, "no_speech_prob": 6.594306614715606e-05}, {"id": 378, "seek": 288434, "start": 2884.34, "end": 2893.78, "text": " going to be getting out is the u 0 and u 0 1 and the u 0 2 so this will be all that is left with", "tokens": [516, 281, 312, 1242, 484, 307, 264, 344, 1958, 293, 344, 1958, 502, 293, 264, 344, 1958, 568, 370, 341, 486, 312, 439, 300, 307, 1411, 365], "temperature": 0.0, "avg_logprob": -0.09894294738769531, "compression_ratio": 1.8940397350993377, "no_speech_prob": 6.190194835653529e-05}, {"id": 379, "seek": 288434, "start": 2893.78, "end": 2899.94, "text": " respect to VC 1 when we take its derivative with respect to VC 1 and this term will be the only", "tokens": [3104, 281, 41922, 502, 562, 321, 747, 1080, 13760, 365, 3104, 281, 41922, 502, 293, 341, 1433, 486, 312, 264, 787], "temperature": 0.0, "avg_logprob": -0.09894294738769531, "compression_ratio": 1.8940397350993377, "no_speech_prob": 6.190194835653529e-05}, {"id": 380, "seek": 288434, "start": 2899.94, "end": 2907.86, "text": " thing left when we take the derivative with respect to the variable VC 2 so the end result of", "tokens": [551, 1411, 562, 321, 747, 264, 13760, 365, 3104, 281, 264, 7006, 41922, 568, 370, 264, 917, 1874, 295], "temperature": 0.0, "avg_logprob": -0.09894294738769531, "compression_ratio": 1.8940397350993377, "no_speech_prob": 6.190194835653529e-05}, {"id": 381, "seek": 290786, "start": 2907.86, "end": 2918.82, "text": " taking the vector derivative of u 0 dot producted with VC is simply going to be u 0.", "tokens": [1940, 264, 8062, 13760, 295, 344, 1958, 5893, 1674, 292, 365, 41922, 307, 2935, 516, 281, 312, 344, 1958, 13], "temperature": 0.0, "avg_logprob": -0.17035162448883057, "compression_ratio": 1.3692307692307693, "no_speech_prob": 4.194396024104208e-05}, {"id": 382, "seek": 290786, "start": 2920.58, "end": 2931.86, "text": " Okay great so that's progress so then at that point we go on and we say oh damn we still have", "tokens": [1033, 869, 370, 300, 311, 4205, 370, 550, 412, 300, 935, 321, 352, 322, 293, 321, 584, 1954, 8151, 321, 920, 362], "temperature": 0.0, "avg_logprob": -0.17035162448883057, "compression_ratio": 1.3692307692307693, "no_speech_prob": 4.194396024104208e-05}, {"id": 383, "seek": 293186, "start": 2931.86, "end": 2943.3, "text": " the the denominator to and that slightly more complex but not so bad so then we try to take", "tokens": [264, 264, 20687, 281, 293, 300, 4748, 544, 3997, 457, 406, 370, 1578, 370, 550, 321, 853, 281, 747], "temperature": 0.0, "avg_logprob": -0.28112259889260316, "compression_ratio": 1.3636363636363635, "no_speech_prob": 4.645609442377463e-05}, {"id": 384, "seek": 293186, "start": 2943.3, "end": 2949.3, "text": " the partial derivatives with respect to VC of the log of the denominator.", "tokens": [264, 14641, 33733, 365, 3104, 281, 41922, 295, 264, 3565, 295, 264, 20687, 13], "temperature": 0.0, "avg_logprob": -0.28112259889260316, "compression_ratio": 1.3636363636363635, "no_speech_prob": 4.645609442377463e-05}, {"id": 385, "seek": 294930, "start": 2949.3, "end": 2967.46, "text": " Okay and so then at this point the one tool that we need to know and remember is how to use the", "tokens": [1033, 293, 370, 550, 412, 341, 935, 264, 472, 2290, 300, 321, 643, 281, 458, 293, 1604, 307, 577, 281, 764, 264], "temperature": 0.0, "avg_logprob": -0.07339677603348442, "compression_ratio": 1.4838709677419355, "no_speech_prob": 7.826587534509599e-05}, {"id": 386, "seek": 294930, "start": 2967.46, "end": 2977.54, "text": " chain rule so the chain rule is when you're wanting to work out of having derivatives of", "tokens": [5021, 4978, 370, 264, 5021, 4978, 307, 562, 291, 434, 7935, 281, 589, 484, 295, 1419, 33733, 295], "temperature": 0.0, "avg_logprob": -0.07339677603348442, "compression_ratio": 1.4838709677419355, "no_speech_prob": 7.826587534509599e-05}, {"id": 387, "seek": 297754, "start": 2977.54, "end": 2987.14, "text": " compositions of functions so we have f of g of whatever x but here it's going to be VC and so", "tokens": [43401, 295, 6828, 370, 321, 362, 283, 295, 290, 295, 2035, 2031, 457, 510, 309, 311, 516, 281, 312, 41922, 293, 370], "temperature": 0.0, "avg_logprob": -0.09096037864685058, "compression_ratio": 1.5564516129032258, "no_speech_prob": 5.8268644352210686e-05}, {"id": 388, "seek": 297754, "start": 2987.14, "end": 2994.9, "text": " we want to say okay what we have here is we're working out a composition of functions so here's our", "tokens": [321, 528, 281, 584, 1392, 437, 321, 362, 510, 307, 321, 434, 1364, 484, 257, 12686, 295, 6828, 370, 510, 311, 527], "temperature": 0.0, "avg_logprob": -0.09096037864685058, "compression_ratio": 1.5564516129032258, "no_speech_prob": 5.8268644352210686e-05}, {"id": 389, "seek": 299490, "start": 2994.9, "end": 3011.94, "text": " f and here is our x which is g of VC actually maybe I shouldn't call it x maybe I", "tokens": [283, 293, 510, 307, 527, 2031, 597, 307, 290, 295, 41922, 767, 1310, 286, 4659, 380, 818, 309, 2031, 1310, 286], "temperature": 0.0, "avg_logprob": -0.17160799105962118, "compression_ratio": 1.381679389312977, "no_speech_prob": 1.6140320440172218e-05}, {"id": 390, "seek": 299490, "start": 3012.9, "end": 3023.86, "text": " probably better to call it z or something okay so when we then want to work out the chain rule well", "tokens": [1391, 1101, 281, 818, 309, 710, 420, 746, 1392, 370, 562, 321, 550, 528, 281, 589, 484, 264, 5021, 4978, 731], "temperature": 0.0, "avg_logprob": -0.17160799105962118, "compression_ratio": 1.381679389312977, "no_speech_prob": 1.6140320440172218e-05}, {"id": 391, "seek": 302386, "start": 3023.86, "end": 3033.7000000000003, "text": " what do we do we take the derivative of f at the point z and so at that point we have to", "tokens": [437, 360, 321, 360, 321, 747, 264, 13760, 295, 283, 412, 264, 935, 710, 293, 370, 412, 300, 935, 321, 362, 281], "temperature": 0.0, "avg_logprob": -0.055240435143039654, "compression_ratio": 1.8866666666666667, "no_speech_prob": 5.736442108172923e-05}, {"id": 392, "seek": 302386, "start": 3033.7000000000003, "end": 3039.38, "text": " actually remember something we have to remember that the derivative of log is the one on x function", "tokens": [767, 1604, 746, 321, 362, 281, 1604, 300, 264, 13760, 295, 3565, 307, 264, 472, 322, 2031, 2445], "temperature": 0.0, "avg_logprob": -0.055240435143039654, "compression_ratio": 1.8866666666666667, "no_speech_prob": 5.736442108172923e-05}, {"id": 393, "seek": 302386, "start": 3039.38, "end": 3050.6600000000003, "text": " so this is going to be equal to the one on x for z so that's then going to be one over the sum", "tokens": [370, 341, 307, 516, 281, 312, 2681, 281, 264, 472, 322, 2031, 337, 710, 370, 300, 311, 550, 516, 281, 312, 472, 670, 264, 2408], "temperature": 0.0, "avg_logprob": -0.055240435143039654, "compression_ratio": 1.8866666666666667, "no_speech_prob": 5.736442108172923e-05}, {"id": 394, "seek": 305066, "start": 3050.66, "end": 3066.2599999999998, "text": " of w equals 1 to v of x of u to the c multiplied by the derivative of the inner function so", "tokens": [295, 261, 6915, 502, 281, 371, 295, 2031, 295, 344, 281, 264, 269, 17207, 538, 264, 13760, 295, 264, 7284, 2445, 370], "temperature": 0.0, "avg_logprob": -0.2573040265303392, "compression_ratio": 1.1666666666666667, "no_speech_prob": 1.9481891285977326e-05}, {"id": 395, "seek": 306626, "start": 3066.26, "end": 3080.0200000000004, "text": " so the derivative of the part that is remaining I'm getting this right the sum of oh and there's", "tokens": [370, 264, 13760, 295, 264, 644, 300, 307, 8877, 286, 478, 1242, 341, 558, 264, 2408, 295, 1954, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.10055738336899701, "compression_ratio": 1.5317460317460319, "no_speech_prob": 9.945994861482177e-06}, {"id": 396, "seek": 306626, "start": 3080.0200000000004, "end": 3085.94, "text": " one trick here at this point we do want to have a change of index so we want to say the sum of x", "tokens": [472, 4282, 510, 412, 341, 935, 321, 360, 528, 281, 362, 257, 1319, 295, 8186, 370, 321, 528, 281, 584, 264, 2408, 295, 2031], "temperature": 0.0, "avg_logprob": -0.10055738336899701, "compression_ratio": 1.5317460317460319, "no_speech_prob": 9.945994861482177e-06}, {"id": 397, "seek": 308594, "start": 3085.94, "end": 3098.9, "text": " equals 1 to v of x of u of x VC since we can get into trouble if we don't change that variable", "tokens": [6915, 502, 281, 371, 295, 2031, 295, 344, 295, 2031, 41922, 1670, 321, 393, 483, 666, 5253, 498, 321, 500, 380, 1319, 300, 7006], "temperature": 0.0, "avg_logprob": -0.08911258697509766, "compression_ratio": 1.5315789473684212, "no_speech_prob": 1.0278581612510607e-05}, {"id": 398, "seek": 308594, "start": 3099.78, "end": 3108.66, "text": " to be using a different one okay so at that point we're making some progress but we still want to", "tokens": [281, 312, 1228, 257, 819, 472, 1392, 370, 412, 300, 935, 321, 434, 1455, 512, 4205, 457, 321, 920, 528, 281], "temperature": 0.0, "avg_logprob": -0.08911258697509766, "compression_ratio": 1.5315789473684212, "no_speech_prob": 1.0278581612510607e-05}, {"id": 399, "seek": 308594, "start": 3108.66, "end": 3115.14, "text": " work out the derivative of this and so what we want to do is apply the chain rule once more so now", "tokens": [589, 484, 264, 13760, 295, 341, 293, 370, 437, 321, 528, 281, 360, 307, 3079, 264, 5021, 4978, 1564, 544, 370, 586], "temperature": 0.0, "avg_logprob": -0.08911258697509766, "compression_ratio": 1.5315789473684212, "no_speech_prob": 1.0278581612510607e-05}, {"id": 400, "seek": 311514, "start": 3115.14, "end": 3130.2599999999998, "text": " here's our f and in here is our new z equals g of vc and so we then sort of repeat over so we can", "tokens": [510, 311, 527, 283, 293, 294, 510, 307, 527, 777, 710, 6915, 290, 295, 371, 66, 293, 370, 321, 550, 1333, 295, 7149, 670, 370, 321, 393], "temperature": 0.0, "avg_logprob": -0.15628296015213947, "compression_ratio": 1.5826086956521739, "no_speech_prob": 4.6712211769772694e-05}, {"id": 401, "seek": 313026, "start": 3130.26, "end": 3147.7000000000003, "text": " move the derivative inside a sum always so we're then taking the derivative of this and so then", "tokens": [1286, 264, 13760, 1854, 257, 2408, 1009, 370, 321, 434, 550, 1940, 264, 13760, 295, 341, 293, 370, 550], "temperature": 0.0, "avg_logprob": -0.15722473992241753, "compression_ratio": 1.6111111111111112, "no_speech_prob": 7.972881576279178e-06}, {"id": 402, "seek": 314770, "start": 3147.7, "end": 3162.4199999999996, "text": " the derivative of x is itself so we're going to just have x of u x tvc times this is sum of x equals 1 to v", "tokens": [264, 13760, 295, 2031, 307, 2564, 370, 321, 434, 516, 281, 445, 362, 2031, 295, 344, 2031, 16364, 66, 1413, 341, 307, 2408, 295, 2031, 6915, 502, 281, 371], "temperature": 0.0, "avg_logprob": -0.20096874237060547, "compression_ratio": 1.5483870967741935, "no_speech_prob": 2.388520078966394e-05}, {"id": 403, "seek": 316242, "start": 3162.42, "end": 3179.62, "text": " times the derivative of u x tvc okay and so then this is what we'd worked out before we can just", "tokens": [1413, 264, 13760, 295, 344, 2031, 16364, 66, 1392, 293, 370, 550, 341, 307, 437, 321, 1116, 2732, 484, 949, 321, 393, 445], "temperature": 0.0, "avg_logprob": -0.13457536697387695, "compression_ratio": 1.4318181818181819, "no_speech_prob": 2.751362626440823e-05}, {"id": 404, "seek": 316242, "start": 3179.62, "end": 3189.7000000000003, "text": " rewrite as u x okay so we're now making progress so if we start putting all of that together", "tokens": [28132, 382, 344, 2031, 1392, 370, 321, 434, 586, 1455, 4205, 370, 498, 321, 722, 3372, 439, 295, 300, 1214], "temperature": 0.0, "avg_logprob": -0.13457536697387695, "compression_ratio": 1.4318181818181819, "no_speech_prob": 2.751362626440823e-05}, {"id": 405, "seek": 318970, "start": 3189.7, "end": 3200.4199999999996, "text": " what we have is the derivative or the partial derivatives with VC of this log probability", "tokens": [437, 321, 362, 307, 264, 13760, 420, 264, 14641, 33733, 365, 41922, 295, 341, 3565, 8482], "temperature": 0.0, "avg_logprob": -0.136387371435398, "compression_ratio": 1.4916666666666667, "no_speech_prob": 8.215315028792247e-05}, {"id": 406, "seek": 318970, "start": 3202.98, "end": 3213.46, "text": " right we have the numerator which was just u 0 minus we then had the sum of the numerator", "tokens": [558, 321, 362, 264, 30380, 597, 390, 445, 344, 1958, 3175, 321, 550, 632, 264, 2408, 295, 264, 30380], "temperature": 0.0, "avg_logprob": -0.136387371435398, "compression_ratio": 1.4916666666666667, "no_speech_prob": 8.215315028792247e-05}, {"id": 407, "seek": 321346, "start": 3213.46, "end": 3226.66, "text": " sum over x equals 1 to v of x u x tvc times u of x then that was multiplied by our first term", "tokens": [2408, 670, 2031, 6915, 502, 281, 371, 295, 2031, 344, 2031, 16364, 66, 1413, 344, 295, 2031, 550, 300, 390, 17207, 538, 527, 700, 1433], "temperature": 0.0, "avg_logprob": -0.0801713587874073, "compression_ratio": 1.5158730158730158, "no_speech_prob": 8.514957698935177e-06}, {"id": 408, "seek": 321346, "start": 3226.66, "end": 3240.5, "text": " that came from the 1 on x which gives you the sum of w equals 1 to v of the x of u w tvc and this", "tokens": [300, 1361, 490, 264, 502, 322, 2031, 597, 2709, 291, 264, 2408, 295, 261, 6915, 502, 281, 371, 295, 264, 2031, 295, 344, 261, 16364, 66, 293, 341], "temperature": 0.0, "avg_logprob": -0.0801713587874073, "compression_ratio": 1.5158730158730158, "no_speech_prob": 8.514957698935177e-06}, {"id": 409, "seek": 324050, "start": 3240.5, "end": 3247.94, "text": " is the fact that we changed the variables became important and so by just sort of rewriting that", "tokens": [307, 264, 1186, 300, 321, 3105, 264, 9102, 3062, 1021, 293, 370, 538, 445, 1333, 295, 319, 19868, 300], "temperature": 0.0, "avg_logprob": -0.13964308301607767, "compression_ratio": 1.492063492063492, "no_speech_prob": 3.937292785849422e-05}, {"id": 410, "seek": 324050, "start": 3247.94, "end": 3268.1, "text": " a little we can get that that equals u 0 minus the sum of v equals sorry x equals 1 to v of", "tokens": [257, 707, 321, 393, 483, 300, 300, 6915, 344, 1958, 3175, 264, 2408, 295, 371, 6915, 2597, 2031, 6915, 502, 281, 371, 295], "temperature": 0.0, "avg_logprob": -0.13964308301607767, "compression_ratio": 1.492063492063492, "no_speech_prob": 3.937292785849422e-05}, {"id": 411, "seek": 326810, "start": 3268.1, "end": 3283.7799999999997, "text": " this x view of x tvc over the sum of w equals 1 to v of x u w tvc times u of x and so at that point", "tokens": [341, 2031, 1910, 295, 2031, 16364, 66, 670, 264, 2408, 295, 261, 6915, 502, 281, 371, 295, 2031, 344, 261, 16364, 66, 1413, 344, 295, 2031, 293, 370, 412, 300, 935], "temperature": 0.0, "avg_logprob": -0.10027144090184625, "compression_ratio": 1.4732824427480915, "no_speech_prob": 7.248514157254249e-05}, {"id": 412, "seek": 326810, "start": 3283.7799999999997, "end": 3290.66, "text": " this sort of interesting thing has happened that we've ended up getting straight back exactly", "tokens": [341, 1333, 295, 1880, 551, 575, 2011, 300, 321, 600, 4590, 493, 1242, 2997, 646, 2293], "temperature": 0.0, "avg_logprob": -0.10027144090184625, "compression_ratio": 1.4732824427480915, "no_speech_prob": 7.248514157254249e-05}, {"id": 413, "seek": 329066, "start": 3290.66, "end": 3299.7799999999997, "text": " the softmax formula probability that we saw when we started and we can just rewrite that more", "tokens": [264, 2787, 41167, 8513, 8482, 300, 321, 1866, 562, 321, 1409, 293, 321, 393, 445, 28132, 300, 544], "temperature": 0.0, "avg_logprob": -0.09816171906211159, "compression_ratio": 1.6149425287356323, "no_speech_prob": 1.9191609681001864e-05}, {"id": 414, "seek": 329066, "start": 3299.7799999999997, "end": 3309.62, "text": " conveniently as saying this equals u 0 minus the sum over x equals 1 to v of the probability of", "tokens": [44375, 382, 1566, 341, 6915, 344, 1958, 3175, 264, 2408, 670, 2031, 6915, 502, 281, 371, 295, 264, 8482, 295], "temperature": 0.0, "avg_logprob": -0.09816171906211159, "compression_ratio": 1.6149425287356323, "no_speech_prob": 1.9191609681001864e-05}, {"id": 415, "seek": 330962, "start": 3309.62, "end": 3322.2599999999998, "text": " x given c times u x and so what we have at that moment is this thing here is an expectation and so", "tokens": [2031, 2212, 269, 1413, 344, 2031, 293, 370, 437, 321, 362, 412, 300, 1623, 307, 341, 551, 510, 307, 364, 14334, 293, 370], "temperature": 0.0, "avg_logprob": -0.07427264642024385, "compression_ratio": 1.6781609195402298, "no_speech_prob": 8.924706889956724e-06}, {"id": 416, "seek": 330962, "start": 3322.2599999999998, "end": 3329.7, "text": " this is an an average over all the context vectors weighted by their probability according to the", "tokens": [341, 307, 364, 364, 4274, 670, 439, 264, 4319, 18875, 32807, 538, 641, 8482, 4650, 281, 264], "temperature": 0.0, "avg_logprob": -0.07427264642024385, "compression_ratio": 1.6781609195402298, "no_speech_prob": 8.924706889956724e-06}, {"id": 417, "seek": 330962, "start": 3329.7, "end": 3335.54, "text": " model and so it's always the case with these softmax style models that what you get out for the", "tokens": [2316, 293, 370, 309, 311, 1009, 264, 1389, 365, 613, 2787, 41167, 3758, 5245, 300, 437, 291, 483, 484, 337, 264], "temperature": 0.0, "avg_logprob": -0.07427264642024385, "compression_ratio": 1.6781609195402298, "no_speech_prob": 8.924706889956724e-06}, {"id": 418, "seek": 333554, "start": 3335.54, "end": 3348.5, "text": " derivatives is you get observed minus the expected so our model is good if our model on average predicts", "tokens": [33733, 307, 291, 483, 13095, 3175, 264, 5176, 370, 527, 2316, 307, 665, 498, 527, 2316, 322, 4274, 6069, 82], "temperature": 0.0, "avg_logprob": -0.06028237872653537, "compression_ratio": 1.5, "no_speech_prob": 2.012564436881803e-05}, {"id": 419, "seek": 333554, "start": 3348.5, "end": 3356.1, "text": " exactly the word vector that we actually see and so we're going to try and adjust the parameters", "tokens": [2293, 264, 1349, 8062, 300, 321, 767, 536, 293, 370, 321, 434, 516, 281, 853, 293, 4369, 264, 9834], "temperature": 0.0, "avg_logprob": -0.06028237872653537, "compression_ratio": 1.5, "no_speech_prob": 2.012564436881803e-05}, {"id": 420, "seek": 335610, "start": 3356.1, "end": 3366.74, "text": " for our model so it does that much as a ball now I mean we try and make it do it as much as possible", "tokens": [337, 527, 2316, 370, 309, 775, 300, 709, 382, 257, 2594, 586, 286, 914, 321, 853, 293, 652, 309, 360, 309, 382, 709, 382, 1944], "temperature": 0.0, "avg_logprob": -0.13631705676808076, "compression_ratio": 1.7533632286995515, "no_speech_prob": 6.881447188789025e-05}, {"id": 421, "seek": 335610, "start": 3366.74, "end": 3373.2999999999997, "text": " I mean of course as you'll find you can never get close right you know if I just say to you okay", "tokens": [286, 914, 295, 1164, 382, 291, 603, 915, 291, 393, 1128, 483, 1998, 558, 291, 458, 498, 286, 445, 584, 281, 291, 1392], "temperature": 0.0, "avg_logprob": -0.13631705676808076, "compression_ratio": 1.7533632286995515, "no_speech_prob": 6.881447188789025e-05}, {"id": 422, "seek": 335610, "start": 3373.94, "end": 3380.74, "text": " the word is cross-on which words are going to occur in the context of cross-on I mean you can't", "tokens": [264, 1349, 307, 3278, 12, 266, 597, 2283, 366, 516, 281, 5160, 294, 264, 4319, 295, 3278, 12, 266, 286, 914, 291, 393, 380], "temperature": 0.0, "avg_logprob": -0.13631705676808076, "compression_ratio": 1.7533632286995515, "no_speech_prob": 6.881447188789025e-05}, {"id": 423, "seek": 335610, "start": 3380.74, "end": 3385.7799999999997, "text": " answer that there are all sorts of sentences that you could say then involve the word cross-on so", "tokens": [1867, 300, 456, 366, 439, 7527, 295, 16579, 300, 291, 727, 584, 550, 9494, 264, 1349, 3278, 12, 266, 370], "temperature": 0.0, "avg_logprob": -0.13631705676808076, "compression_ratio": 1.7533632286995515, "no_speech_prob": 6.881447188789025e-05}, {"id": 424, "seek": 338578, "start": 3385.78, "end": 3393.3, "text": " actually our particular probability estimates are going to be kind of small but nevertheless", "tokens": [767, 527, 1729, 8482, 20561, 366, 516, 281, 312, 733, 295, 1359, 457, 26924], "temperature": 0.0, "avg_logprob": -0.03520113871647761, "compression_ratio": 1.569060773480663, "no_speech_prob": 6.1031398217892274e-05}, {"id": 425, "seek": 338578, "start": 3393.3, "end": 3400.5800000000004, "text": " we want to sort of fiddle our word vectors to try and make those estimates as high as we possibly", "tokens": [321, 528, 281, 1333, 295, 24553, 2285, 527, 1349, 18875, 281, 853, 293, 652, 729, 20561, 382, 1090, 382, 321, 6264], "temperature": 0.0, "avg_logprob": -0.03520113871647761, "compression_ratio": 1.569060773480663, "no_speech_prob": 6.1031398217892274e-05}, {"id": 426, "seek": 338578, "start": 3400.5800000000004, "end": 3411.46, "text": " can so I've gone on about this stuff a bit but haven't actually sort of shown you any of what", "tokens": [393, 370, 286, 600, 2780, 322, 466, 341, 1507, 257, 857, 457, 2378, 380, 767, 1333, 295, 4898, 291, 604, 295, 437], "temperature": 0.0, "avg_logprob": -0.03520113871647761, "compression_ratio": 1.569060773480663, "no_speech_prob": 6.1031398217892274e-05}, {"id": 427, "seek": 341146, "start": 3411.46, "end": 3420.34, "text": " actually happens so I just want to quickly show you a bit of that as to what actually happens with", "tokens": [767, 2314, 370, 286, 445, 528, 281, 2661, 855, 291, 257, 857, 295, 300, 382, 281, 437, 767, 2314, 365], "temperature": 0.0, "avg_logprob": -0.08183537738423952, "compression_ratio": 1.559782608695652, "no_speech_prob": 5.19787245139014e-05}, {"id": 428, "seek": 341146, "start": 3420.34, "end": 3427.38, "text": " word vectors so here's a simple little ipython notebook which is also what you'll be using for", "tokens": [1349, 18875, 370, 510, 311, 257, 2199, 707, 28501, 88, 11943, 21060, 597, 307, 611, 437, 291, 603, 312, 1228, 337], "temperature": 0.0, "avg_logprob": -0.08183537738423952, "compression_ratio": 1.559782608695652, "no_speech_prob": 5.19787245139014e-05}, {"id": 429, "seek": 341146, "start": 3427.38, "end": 3435.86, "text": " assignment one only so in the first cell I import a bunch of stuff so we've got numpy for our", "tokens": [15187, 472, 787, 370, 294, 264, 700, 2815, 286, 974, 257, 3840, 295, 1507, 370, 321, 600, 658, 1031, 8200, 337, 527], "temperature": 0.0, "avg_logprob": -0.08183537738423952, "compression_ratio": 1.559782608695652, "no_speech_prob": 5.19787245139014e-05}, {"id": 430, "seek": 343586, "start": 3435.86, "end": 3444.7400000000002, "text": " vectors matpotlib for part of the packet learns kind of your machine learning swissami knife", "tokens": [18875, 3803, 17698, 38270, 337, 644, 295, 264, 20300, 27152, 733, 295, 428, 3479, 2539, 1693, 891, 4526, 7976], "temperature": 0.0, "avg_logprob": -0.22259112766810826, "compression_ratio": 1.7953488372093023, "no_speech_prob": 2.9270731829456054e-05}, {"id": 431, "seek": 343586, "start": 3444.7400000000002, "end": 3449.94, "text": " gen sim is a package that you may well not have seen before it's a package that's often used for", "tokens": [1049, 1034, 307, 257, 7372, 300, 291, 815, 731, 406, 362, 1612, 949, 309, 311, 257, 7372, 300, 311, 2049, 1143, 337], "temperature": 0.0, "avg_logprob": -0.22259112766810826, "compression_ratio": 1.7953488372093023, "no_speech_prob": 2.9270731829456054e-05}, {"id": 432, "seek": 343586, "start": 3449.94, "end": 3455.7000000000003, "text": " word vectors it's not really used for deep learning so this is the only time you'll see it in the", "tokens": [1349, 18875, 309, 311, 406, 534, 1143, 337, 2452, 2539, 370, 341, 307, 264, 787, 565, 291, 603, 536, 309, 294, 264], "temperature": 0.0, "avg_logprob": -0.22259112766810826, "compression_ratio": 1.7953488372093023, "no_speech_prob": 2.9270731829456054e-05}, {"id": 433, "seek": 343586, "start": 3455.7000000000003, "end": 3460.98, "text": " class but if you just want a good package for working with word vectors and some other application", "tokens": [1508, 457, 498, 291, 445, 528, 257, 665, 7372, 337, 1364, 365, 1349, 18875, 293, 512, 661, 3861], "temperature": 0.0, "avg_logprob": -0.22259112766810826, "compression_ratio": 1.7953488372093023, "no_speech_prob": 2.9270731829456054e-05}, {"id": 434, "seek": 346098, "start": 3460.98, "end": 3470.58, "text": " it's a good one to know about okay so then in my second cell here I'm loading a particular set", "tokens": [309, 311, 257, 665, 472, 281, 458, 466, 1392, 370, 550, 294, 452, 1150, 2815, 510, 286, 478, 15114, 257, 1729, 992], "temperature": 0.0, "avg_logprob": -0.07824108179877787, "compression_ratio": 1.6171428571428572, "no_speech_prob": 3.6189005186315626e-05}, {"id": 435, "seek": 346098, "start": 3470.58, "end": 3478.02, "text": " of word vectors so these are our glove word vectors that we made at stanford in 2014 and I'm", "tokens": [295, 1349, 18875, 370, 613, 366, 527, 26928, 1349, 18875, 300, 321, 1027, 412, 27984, 7404, 294, 8227, 293, 286, 478], "temperature": 0.0, "avg_logprob": -0.07824108179877787, "compression_ratio": 1.6171428571428572, "no_speech_prob": 3.6189005186315626e-05}, {"id": 436, "seek": 346098, "start": 3478.02, "end": 3484.18, "text": " loading a hundred dimensional word vectors so that things are a little bit quicker for me while", "tokens": [15114, 257, 3262, 18795, 1349, 18875, 370, 300, 721, 366, 257, 707, 857, 16255, 337, 385, 1339], "temperature": 0.0, "avg_logprob": -0.07824108179877787, "compression_ratio": 1.6171428571428572, "no_speech_prob": 3.6189005186315626e-05}, {"id": 437, "seek": 348418, "start": 3484.18, "end": 3492.74, "text": " I'm doing things here sort of do this model of bread and croissant well what I've just got here", "tokens": [286, 478, 884, 721, 510, 1333, 295, 360, 341, 2316, 295, 5961, 293, 4848, 29492, 731, 437, 286, 600, 445, 658, 510], "temperature": 0.0, "avg_logprob": -0.11935650891271131, "compression_ratio": 1.63013698630137, "no_speech_prob": 7.894493319327012e-05}, {"id": 438, "seek": 348418, "start": 3492.74, "end": 3500.5, "text": " is word vectors so I just wanted to sort of show you that there are word vectors", "tokens": [307, 1349, 18875, 370, 286, 445, 1415, 281, 1333, 295, 855, 291, 300, 456, 366, 1349, 18875], "temperature": 0.0, "avg_logprob": -0.11935650891271131, "compression_ratio": 1.63013698630137, "no_speech_prob": 7.894493319327012e-05}, {"id": 439, "seek": 348418, "start": 3505.3799999999997, "end": 3510.18, "text": " well maybe I should have loaded those word vectors in advance", "tokens": [731, 1310, 286, 820, 362, 13210, 729, 1349, 18875, 294, 7295], "temperature": 0.0, "avg_logprob": -0.11935650891271131, "compression_ratio": 1.63013698630137, "no_speech_prob": 7.894493319327012e-05}, {"id": 440, "seek": 351018, "start": 3510.18, "end": 3513.54, "text": " hmm let's see", "tokens": [16478, 718, 311, 536], "temperature": 0.0, "avg_logprob": -0.1853125360276964, "compression_ratio": 1.4820143884892085, "no_speech_prob": 0.0001793663832359016}, {"id": 441, "seek": 351018, "start": 3522.5, "end": 3531.8599999999997, "text": " oh okay well I'm in business um okay so right so here are my word vectors for bread and croissant", "tokens": [1954, 1392, 731, 286, 478, 294, 1606, 1105, 1392, 370, 558, 370, 510, 366, 452, 1349, 18875, 337, 5961, 293, 4848, 29492], "temperature": 0.0, "avg_logprob": -0.1853125360276964, "compression_ratio": 1.4820143884892085, "no_speech_prob": 0.0001793663832359016}, {"id": 442, "seek": 351018, "start": 3533.62, "end": 3537.8599999999997, "text": " and while I'm seeing them maybe these two words are a bit similar so both of them are negative", "tokens": [293, 1339, 286, 478, 2577, 552, 1310, 613, 732, 2283, 366, 257, 857, 2531, 370, 1293, 295, 552, 366, 3671], "temperature": 0.0, "avg_logprob": -0.1853125360276964, "compression_ratio": 1.4820143884892085, "no_speech_prob": 0.0001793663832359016}, {"id": 443, "seek": 353786, "start": 3537.86, "end": 3544.02, "text": " in the first dimension positive in the second negative in the third positive in the fourth negative", "tokens": [294, 264, 700, 10139, 3353, 294, 264, 1150, 3671, 294, 264, 2636, 3353, 294, 264, 6409, 3671], "temperature": 0.0, "avg_logprob": -0.10477926442911337, "compression_ratio": 1.8194444444444444, "no_speech_prob": 9.453518578084186e-05}, {"id": 444, "seek": 353786, "start": 3544.02, "end": 3548.5, "text": " in the fifth so it sort of looks like they might have a fair bit of dot product which is kind of", "tokens": [294, 264, 9266, 370, 309, 1333, 295, 1542, 411, 436, 1062, 362, 257, 3143, 857, 295, 5893, 1674, 597, 307, 733, 295], "temperature": 0.0, "avg_logprob": -0.10477926442911337, "compression_ratio": 1.8194444444444444, "no_speech_prob": 9.453518578084186e-05}, {"id": 445, "seek": 353786, "start": 3548.5, "end": 3554.58, "text": " what we want because bread and croissant are kind of similar but what we can do is actually ask the", "tokens": [437, 321, 528, 570, 5961, 293, 4848, 29492, 366, 733, 295, 2531, 457, 437, 321, 393, 360, 307, 767, 1029, 264], "temperature": 0.0, "avg_logprob": -0.10477926442911337, "compression_ratio": 1.8194444444444444, "no_speech_prob": 9.453518578084186e-05}, {"id": 446, "seek": 353786, "start": 3554.58, "end": 3561.46, "text": " model and these are gents in functions now you know what are the most similar words so I can ask", "tokens": [2316, 293, 613, 366, 290, 791, 294, 6828, 586, 291, 458, 437, 366, 264, 881, 2531, 2283, 370, 286, 393, 1029], "temperature": 0.0, "avg_logprob": -0.10477926442911337, "compression_ratio": 1.8194444444444444, "no_speech_prob": 9.453518578084186e-05}, {"id": 447, "seek": 356146, "start": 3561.46, "end": 3568.34, "text": " for croissant what are the most similar words to that and it will tell me it's things like", "tokens": [337, 4848, 29492, 437, 366, 264, 881, 2531, 2283, 281, 300, 293, 309, 486, 980, 385, 309, 311, 721, 411], "temperature": 0.0, "avg_logprob": -0.18623082297188895, "compression_ratio": 1.5319148936170213, "no_speech_prob": 7.708952034590766e-05}, {"id": 448, "seek": 356146, "start": 3568.34, "end": 3574.02, "text": " brioche baguette for cacciate so that's pretty good putting us perhaps a little bit more questionable", "tokens": [272, 6584, 1876, 3411, 84, 3007, 337, 269, 43870, 473, 370, 300, 311, 1238, 665, 3372, 505, 4317, 257, 707, 857, 544, 37158], "temperature": 0.0, "avg_logprob": -0.18623082297188895, "compression_ratio": 1.5319148936170213, "no_speech_prob": 7.708952034590766e-05}, {"id": 449, "seek": 356146, "start": 3574.66, "end": 3583.2200000000003, "text": " we can say most similar to the USA and it says Canada or America USA with periods United States", "tokens": [321, 393, 584, 881, 2531, 281, 264, 10827, 293, 309, 1619, 6309, 420, 3374, 10827, 365, 13804, 2824, 3040], "temperature": 0.0, "avg_logprob": -0.18623082297188895, "compression_ratio": 1.5319148936170213, "no_speech_prob": 7.708952034590766e-05}, {"id": 450, "seek": 358322, "start": 3583.22, "end": 3593.06, "text": " that's pretty good most similar to banana I get out coconut mangoes bananas sort of fairly tropical", "tokens": [300, 311, 1238, 665, 881, 2531, 281, 14194, 286, 483, 484, 13551, 23481, 279, 22742, 1333, 295, 6457, 22857], "temperature": 0.0, "avg_logprob": -0.14771263508857052, "compression_ratio": 1.669603524229075, "no_speech_prob": 3.216457480448298e-05}, {"id": 451, "seek": 358322, "start": 3593.06, "end": 3599.8599999999997, "text": " through it great um before finishing though I want to show you something slightly more", "tokens": [807, 309, 869, 1105, 949, 12693, 1673, 286, 528, 281, 855, 291, 746, 4748, 544], "temperature": 0.0, "avg_logprob": -0.14771263508857052, "compression_ratio": 1.669603524229075, "no_speech_prob": 3.216457480448298e-05}, {"id": 452, "seek": 358322, "start": 3599.8599999999997, "end": 3605.14, "text": " than just similarity which was one of the amazing things that people observed with these word", "tokens": [813, 445, 32194, 597, 390, 472, 295, 264, 2243, 721, 300, 561, 13095, 365, 613, 1349], "temperature": 0.0, "avg_logprob": -0.14771263508857052, "compression_ratio": 1.669603524229075, "no_speech_prob": 3.216457480448298e-05}, {"id": 453, "seek": 358322, "start": 3605.14, "end": 3612.18, "text": " vectors and that was to say you can actually sort of do arithmetic in this vector space that makes", "tokens": [18875, 293, 300, 390, 281, 584, 291, 393, 767, 1333, 295, 360, 42973, 294, 341, 8062, 1901, 300, 1669], "temperature": 0.0, "avg_logprob": -0.14771263508857052, "compression_ratio": 1.669603524229075, "no_speech_prob": 3.216457480448298e-05}, {"id": 454, "seek": 361218, "start": 3612.18, "end": 3618.66, "text": " sense and so in particular people suggested this analogy task and so the idea of the analogy", "tokens": [2020, 293, 370, 294, 1729, 561, 10945, 341, 21663, 5633, 293, 370, 264, 1558, 295, 264, 21663], "temperature": 0.0, "avg_logprob": -0.09021448547189886, "compression_ratio": 1.9081632653061225, "no_speech_prob": 0.00011937968520214781}, {"id": 455, "seek": 361218, "start": 3618.66, "end": 3624.66, "text": " task is you should be able to start with a word like king and you should be able to subtract out a", "tokens": [5633, 307, 291, 820, 312, 1075, 281, 722, 365, 257, 1349, 411, 4867, 293, 291, 820, 312, 1075, 281, 16390, 484, 257], "temperature": 0.0, "avg_logprob": -0.09021448547189886, "compression_ratio": 1.9081632653061225, "no_speech_prob": 0.00011937968520214781}, {"id": 456, "seek": 361218, "start": 3624.66, "end": 3631.7, "text": " male component from it add back in a woman component and then you should be able to ask well what", "tokens": [7133, 6542, 490, 309, 909, 646, 294, 257, 3059, 6542, 293, 550, 291, 820, 312, 1075, 281, 1029, 731, 437], "temperature": 0.0, "avg_logprob": -0.09021448547189886, "compression_ratio": 1.9081632653061225, "no_speech_prob": 0.00011937968520214781}, {"id": 457, "seek": 363170, "start": 3631.7, "end": 3644.4199999999996, "text": " word is over here and what you'd like is that the word over there is queen um and so um this", "tokens": [1349, 307, 670, 510, 293, 437, 291, 1116, 411, 307, 300, 264, 1349, 670, 456, 307, 12206, 1105, 293, 370, 1105, 341], "temperature": 0.0, "avg_logprob": -0.09322565880374632, "compression_ratio": 1.6964285714285714, "no_speech_prob": 8.258160050900187e-06}, {"id": 458, "seek": 363170, "start": 3644.4199999999996, "end": 3652.18, "text": " sort of little bit of so we're going to do that um with this sort of same most similar function", "tokens": [1333, 295, 707, 857, 295, 370, 321, 434, 516, 281, 360, 300, 1105, 365, 341, 1333, 295, 912, 881, 2531, 2445], "temperature": 0.0, "avg_logprob": -0.09322565880374632, "compression_ratio": 1.6964285714285714, "no_speech_prob": 8.258160050900187e-06}, {"id": 459, "seek": 363170, "start": 3652.18, "end": 3659.22, "text": " which is actually more so as well as having positive words you can ask for most similar negative", "tokens": [597, 307, 767, 544, 370, 382, 731, 382, 1419, 3353, 2283, 291, 393, 1029, 337, 881, 2531, 3671], "temperature": 0.0, "avg_logprob": -0.09322565880374632, "compression_ratio": 1.6964285714285714, "no_speech_prob": 8.258160050900187e-06}, {"id": 460, "seek": 365922, "start": 3659.22, "end": 3665.2999999999997, "text": " words and you might wonder what's most negatively similar to a banana and you might be thinking oh", "tokens": [2283, 293, 291, 1062, 2441, 437, 311, 881, 29519, 2531, 281, 257, 14194, 293, 291, 1062, 312, 1953, 1954], "temperature": 0.0, "avg_logprob": -0.08117518264256167, "compression_ratio": 1.7649769585253456, "no_speech_prob": 3.315311187179759e-05}, {"id": 461, "seek": 365922, "start": 3665.2999999999997, "end": 3672.18, "text": " it's um I don't know um some kind of meat or something um actually that by itself isn't very", "tokens": [309, 311, 1105, 286, 500, 380, 458, 1105, 512, 733, 295, 4615, 420, 746, 1105, 767, 300, 538, 2564, 1943, 380, 588], "temperature": 0.0, "avg_logprob": -0.08117518264256167, "compression_ratio": 1.7649769585253456, "no_speech_prob": 3.315311187179759e-05}, {"id": 462, "seek": 365922, "start": 3672.18, "end": 3677.54, "text": " useful because when you could just ask for um most negatively similar to things you tend to get", "tokens": [4420, 570, 562, 291, 727, 445, 1029, 337, 1105, 881, 29519, 2531, 281, 721, 291, 3928, 281, 483], "temperature": 0.0, "avg_logprob": -0.08117518264256167, "compression_ratio": 1.7649769585253456, "no_speech_prob": 3.315311187179759e-05}, {"id": 463, "seek": 365922, "start": 3678.02, "end": 3682.8999999999996, "text": " crazy strings that were found in the data set um that you don't know what they mean if anything", "tokens": [3219, 13985, 300, 645, 1352, 294, 264, 1412, 992, 1105, 300, 291, 500, 380, 458, 437, 436, 914, 498, 1340], "temperature": 0.0, "avg_logprob": -0.08117518264256167, "compression_ratio": 1.7649769585253456, "no_speech_prob": 3.315311187179759e-05}, {"id": 464, "seek": 368290, "start": 3682.9, "end": 3689.54, "text": " um but if we put the two together we can use the most similar function with positives and negatives", "tokens": [1105, 457, 498, 321, 829, 264, 732, 1214, 321, 393, 764, 264, 881, 2531, 2445, 365, 35127, 293, 40019], "temperature": 0.0, "avg_logprob": -0.09894000159369574, "compression_ratio": 1.8867924528301887, "no_speech_prob": 1.6679883628967218e-05}, {"id": 465, "seek": 368290, "start": 3689.54, "end": 3697.62, "text": " to do analogies so we're going to say we want a positive king we want to subtract out negatively man", "tokens": [281, 360, 16660, 530, 370, 321, 434, 516, 281, 584, 321, 528, 257, 3353, 4867, 321, 528, 281, 16390, 484, 29519, 587], "temperature": 0.0, "avg_logprob": -0.09894000159369574, "compression_ratio": 1.8867924528301887, "no_speech_prob": 1.6679883628967218e-05}, {"id": 466, "seek": 368290, "start": 3697.94, "end": 3704.82, "text": " we want to then add in positively woman and find out what's most similar to this point in the space", "tokens": [321, 528, 281, 550, 909, 294, 25795, 3059, 293, 915, 484, 437, 311, 881, 2531, 281, 341, 935, 294, 264, 1901], "temperature": 0.0, "avg_logprob": -0.09894000159369574, "compression_ratio": 1.8867924528301887, "no_speech_prob": 1.6679883628967218e-05}, {"id": 467, "seek": 370482, "start": 3704.82, "end": 3713.3, "text": " so my analogy function does that precisely that by taking um a couple of most similar ones and then", "tokens": [370, 452, 21663, 2445, 775, 300, 13402, 300, 538, 1940, 1105, 257, 1916, 295, 881, 2531, 2306, 293, 550], "temperature": 0.0, "avg_logprob": -0.10320588520595006, "compression_ratio": 1.6494252873563218, "no_speech_prob": 1.8890043065766804e-05}, {"id": 468, "seek": 370482, "start": 3713.3, "end": 3720.5800000000004, "text": " subtracting out um the negative one and so we can try out this analogy function so I can do the", "tokens": [16390, 278, 484, 1105, 264, 3671, 472, 293, 370, 321, 393, 853, 484, 341, 21663, 2445, 370, 286, 393, 360, 264], "temperature": 0.0, "avg_logprob": -0.10320588520595006, "compression_ratio": 1.6494252873563218, "no_speech_prob": 1.8890043065766804e-05}, {"id": 469, "seek": 370482, "start": 3720.5800000000004, "end": 3729.38, "text": " analogy I show in the picture um with man as to king as woman is uh fight so I'm not saying", "tokens": [21663, 286, 855, 294, 264, 3036, 1105, 365, 587, 382, 281, 4867, 382, 3059, 307, 2232, 2092, 370, 286, 478, 406, 1566], "temperature": 0.0, "avg_logprob": -0.10320588520595006, "compression_ratio": 1.6494252873563218, "no_speech_prob": 1.8890043065766804e-05}, {"id": 470, "seek": 372938, "start": 3729.38, "end": 3737.2200000000003, "text": " that's right um yeah man is to king as woman is to blah sorry I haven't done myself um", "tokens": [300, 311, 558, 1105, 1338, 587, 307, 281, 4867, 382, 3059, 307, 281, 12288, 2597, 286, 2378, 380, 1096, 2059, 1105], "temperature": 0.0, "avg_logprob": -0.13161782214516088, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.8028105841949582e-05}, {"id": 471, "seek": 372938, "start": 3741.62, "end": 3750.58, "text": " okay man is to king as woman is to queen so um that's great and that um works well I mean and you", "tokens": [1392, 587, 307, 281, 4867, 382, 3059, 307, 281, 12206, 370, 1105, 300, 311, 869, 293, 300, 1105, 1985, 731, 286, 914, 293, 291], "temperature": 0.0, "avg_logprob": -0.13161782214516088, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.8028105841949582e-05}, {"id": 472, "seek": 372938, "start": 3750.58, "end": 3757.06, "text": " can do it the sort of other way around king is to man as queen is to woman um if this only worked", "tokens": [393, 360, 309, 264, 1333, 295, 661, 636, 926, 4867, 307, 281, 587, 382, 12206, 307, 281, 3059, 1105, 498, 341, 787, 2732], "temperature": 0.0, "avg_logprob": -0.13161782214516088, "compression_ratio": 1.7735849056603774, "no_speech_prob": 1.8028105841949582e-05}, {"id": 473, "seek": 375706, "start": 3757.06, "end": 3764.02, "text": " for that one freakish example um you maybe um wouldn't be very impressed but you know it actually", "tokens": [337, 300, 472, 21853, 742, 1365, 1105, 291, 1310, 1105, 2759, 380, 312, 588, 11679, 457, 291, 458, 309, 767], "temperature": 0.0, "avg_logprob": -0.06942026274544852, "compression_ratio": 1.5921787709497206, "no_speech_prob": 8.069599425652996e-05}, {"id": 474, "seek": 375706, "start": 3764.02, "end": 3769.38, "text": " turns out like it's not perfect but you can do all sorts of fun analogies with this and they", "tokens": [4523, 484, 411, 309, 311, 406, 2176, 457, 291, 393, 360, 439, 7527, 295, 1019, 16660, 530, 365, 341, 293, 436], "temperature": 0.0, "avg_logprob": -0.06942026274544852, "compression_ratio": 1.5921787709497206, "no_speech_prob": 8.069599425652996e-05}, {"id": 475, "seek": 375706, "start": 3769.38, "end": 3779.7, "text": " actually work so you know I could ask for something like an analogy um oh here's a good one um", "tokens": [767, 589, 370, 291, 458, 286, 727, 1029, 337, 746, 411, 364, 21663, 1105, 1954, 510, 311, 257, 665, 472, 1105], "temperature": 0.0, "avg_logprob": -0.06942026274544852, "compression_ratio": 1.5921787709497206, "no_speech_prob": 8.069599425652996e-05}, {"id": 476, "seek": 377970, "start": 3779.7, "end": 3790.98, "text": " Australia um is to be uh as France is to what um and you can think about what you think the answer", "tokens": [7060, 1105, 307, 281, 312, 2232, 382, 6190, 307, 281, 437, 1105, 293, 291, 393, 519, 466, 437, 291, 519, 264, 1867], "temperature": 0.0, "avg_logprob": -0.09015597899754842, "compression_ratio": 1.450381679389313, "no_speech_prob": 1.3819978448736947e-05}, {"id": 477, "seek": 377970, "start": 3790.98, "end": 3798.02, "text": " that one should be and it comes out as um champagne which is pretty good or I could ask for", "tokens": [300, 472, 820, 312, 293, 309, 1487, 484, 382, 1105, 33336, 597, 307, 1238, 665, 420, 286, 727, 1029, 337], "temperature": 0.0, "avg_logprob": -0.09015597899754842, "compression_ratio": 1.450381679389313, "no_speech_prob": 1.3819978448736947e-05}, {"id": 478, "seek": 379802, "start": 3798.02, "end": 3814.34, "text": " something like analogy pencil is to sketching as camera is to what um and it says photographing um", "tokens": [746, 411, 21663, 10985, 307, 281, 12325, 278, 382, 2799, 307, 281, 437, 1105, 293, 309, 1619, 2409, 12440, 571, 1105], "temperature": 0.0, "avg_logprob": -0.08731412887573242, "compression_ratio": 1.5433070866141732, "no_speech_prob": 4.245699074090226e-06}, {"id": 479, "seek": 379802, "start": 3814.34, "end": 3820.66, "text": " you can also do the analogies with people um at this point I have to point out that this data was", "tokens": [291, 393, 611, 360, 264, 16660, 530, 365, 561, 1105, 412, 341, 935, 286, 362, 281, 935, 484, 300, 341, 1412, 390], "temperature": 0.0, "avg_logprob": -0.08731412887573242, "compression_ratio": 1.5433070866141732, "no_speech_prob": 4.245699074090226e-06}, {"id": 480, "seek": 382066, "start": 3820.66, "end": 3828.02, "text": " um and the model was built in 2014 so you can't ask anything about um Donald Trump in it", "tokens": [1105, 293, 264, 2316, 390, 3094, 294, 8227, 370, 291, 393, 380, 1029, 1340, 466, 1105, 8632, 3899, 294, 309], "temperature": 0.0, "avg_logprob": -0.15107038746709409, "compression_ratio": 1.5561797752808988, "no_speech_prob": 6.470397784141824e-05}, {"id": 481, "seek": 382066, "start": 3828.02, "end": 3833.3799999999997, "text": " well you can he Trump is in there but not as president but I could ask something like analogy", "tokens": [731, 291, 393, 415, 3899, 307, 294, 456, 457, 406, 382, 3868, 457, 286, 727, 1029, 746, 411, 21663], "temperature": 0.0, "avg_logprob": -0.15107038746709409, "compression_ratio": 1.5561797752808988, "no_speech_prob": 6.470397784141824e-05}, {"id": 482, "seek": 382066, "start": 3834.74, "end": 3848.8999999999996, "text": " of bomb is to Clinton as Reagan is um to what and you can think of what you think is the right", "tokens": [295, 7851, 307, 281, 15445, 382, 26534, 307, 1105, 281, 437, 293, 291, 393, 519, 295, 437, 291, 519, 307, 264, 558], "temperature": 0.0, "avg_logprob": -0.15107038746709409, "compression_ratio": 1.5561797752808988, "no_speech_prob": 6.470397784141824e-05}, {"id": 483, "seek": 384890, "start": 3848.9, "end": 3855.78, "text": " um analogy there um the analogy it returns is Nixon um so I guess that depends on what you think", "tokens": [1105, 21663, 456, 1105, 264, 21663, 309, 11247, 307, 31130, 1105, 370, 286, 2041, 300, 5946, 322, 437, 291, 519], "temperature": 0.0, "avg_logprob": -0.09698006686042338, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.5191364582278766e-05}, {"id": 484, "seek": 384890, "start": 3855.78, "end": 3862.42, "text": " of Bill Clinton as to whether you think that was a good analogy or not you can also um do sort of", "tokens": [295, 5477, 15445, 382, 281, 1968, 291, 519, 300, 390, 257, 665, 21663, 420, 406, 291, 393, 611, 1105, 360, 1333, 295], "temperature": 0.0, "avg_logprob": -0.09698006686042338, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.5191364582278766e-05}, {"id": 485, "seek": 384890, "start": 3862.42, "end": 3872.82, "text": " linguistic analogies with it so you can do something like analogy tall is to tallest as long", "tokens": [43002, 16660, 530, 365, 309, 370, 291, 393, 360, 746, 411, 21663, 6764, 307, 281, 42075, 382, 938], "temperature": 0.0, "avg_logprob": -0.09698006686042338, "compression_ratio": 1.7083333333333333, "no_speech_prob": 1.5191364582278766e-05}, {"id": 486, "seek": 387282, "start": 3872.82, "end": 3879.46, "text": " is to what and it does longest so it really just sort of knows a lot about the meaning", "tokens": [307, 281, 437, 293, 309, 775, 15438, 370, 309, 534, 445, 1333, 295, 3255, 257, 688, 466, 264, 3620], "temperature": 0.0, "avg_logprob": -0.15477494966416133, "compression_ratio": 1.7155555555555555, "no_speech_prob": 4.751688538817689e-05}, {"id": 487, "seek": 387282, "start": 3879.46, "end": 3887.06, "text": " behavior of words and you know I think when these um methods were first developed and hopefully", "tokens": [5223, 295, 2283, 293, 291, 458, 286, 519, 562, 613, 1105, 7150, 645, 700, 4743, 293, 4696], "temperature": 0.0, "avg_logprob": -0.15477494966416133, "compression_ratio": 1.7155555555555555, "no_speech_prob": 4.751688538817689e-05}, {"id": 488, "seek": 387282, "start": 3887.06, "end": 3893.78, "text": " still for you that you know people were just gobsmacked about how well this actually worked at capturing", "tokens": [920, 337, 291, 300, 291, 458, 561, 645, 445, 352, 929, 76, 25949, 466, 577, 731, 341, 767, 2732, 412, 23384], "temperature": 0.0, "avg_logprob": -0.15477494966416133, "compression_ratio": 1.7155555555555555, "no_speech_prob": 4.751688538817689e-05}, {"id": 489, "seek": 387282, "start": 3894.98, "end": 3901.38, "text": " other enough words and so these word vectors then went everywhere as a new representation that was", "tokens": [661, 1547, 2283, 293, 370, 613, 1349, 18875, 550, 1437, 5315, 382, 257, 777, 10290, 300, 390], "temperature": 0.0, "avg_logprob": -0.15477494966416133, "compression_ratio": 1.7155555555555555, "no_speech_prob": 4.751688538817689e-05}, {"id": 490, "seek": 390138, "start": 3901.38, "end": 3907.94, "text": " so powerful for working out word meaning and so that's our starting point for this class and we'll", "tokens": [370, 4005, 337, 1364, 484, 1349, 3620, 293, 370, 300, 311, 527, 2891, 935, 337, 341, 1508, 293, 321, 603], "temperature": 0.0, "avg_logprob": -0.10815437273545699, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.6259232981828973e-05}, {"id": 491, "seek": 390138, "start": 3907.94, "end": 3913.1400000000003, "text": " say a bit more about them next time and they're also the basis of what you're looking at for the", "tokens": [584, 257, 857, 544, 466, 552, 958, 565, 293, 436, 434, 611, 264, 5143, 295, 437, 291, 434, 1237, 412, 337, 264], "temperature": 0.0, "avg_logprob": -0.10815437273545699, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.6259232981828973e-05}, {"id": 492, "seek": 390138, "start": 3913.1400000000003, "end": 3918.98, "text": " first assignment can I ask a quick question about the distinction between the two vectors per word", "tokens": [700, 15187, 393, 286, 1029, 257, 1702, 1168, 466, 264, 16844, 1296, 264, 732, 18875, 680, 1349], "temperature": 0.0, "avg_logprob": -0.10815437273545699, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.6259232981828973e-05}, {"id": 493, "seek": 390138, "start": 3919.78, "end": 3927.38, "text": " yes so um my understanding is that there can be several context words per uh word in the vocabulary", "tokens": [2086, 370, 1105, 452, 3701, 307, 300, 456, 393, 312, 2940, 4319, 2283, 680, 2232, 1349, 294, 264, 19864], "temperature": 0.0, "avg_logprob": -0.10815437273545699, "compression_ratio": 1.6982758620689655, "no_speech_prob": 2.6259232981828973e-05}, {"id": 494, "seek": 392738, "start": 3927.38, "end": 3932.42, "text": " or like word in the vocabulary um but then if there's only two vectors I kind of I thought the", "tokens": [420, 411, 1349, 294, 264, 19864, 1105, 457, 550, 498, 456, 311, 787, 732, 18875, 286, 733, 295, 286, 1194, 264], "temperature": 0.0, "avg_logprob": -0.12986843525862493, "compression_ratio": 1.8528301886792453, "no_speech_prob": 1.1295895092189312e-05}, {"id": 495, "seek": 392738, "start": 3932.42, "end": 3936.42, "text": " distinction between the two is that one it's like the actual word and one's like the context word but", "tokens": [16844, 1296, 264, 732, 307, 300, 472, 309, 311, 411, 264, 3539, 1349, 293, 472, 311, 411, 264, 4319, 1349, 457], "temperature": 0.0, "avg_logprob": -0.12986843525862493, "compression_ratio": 1.8528301886792453, "no_speech_prob": 1.1295895092189312e-05}, {"id": 496, "seek": 392738, "start": 3936.42, "end": 3942.1800000000003, "text": " if there are multiple context words right how do you how do you pick to just two then well so we're", "tokens": [498, 456, 366, 3866, 4319, 2283, 558, 577, 360, 291, 577, 360, 291, 1888, 281, 445, 732, 550, 731, 370, 321, 434], "temperature": 0.0, "avg_logprob": -0.12986843525862493, "compression_ratio": 1.8528301886792453, "no_speech_prob": 1.1295895092189312e-05}, {"id": 497, "seek": 392738, "start": 3942.1800000000003, "end": 3950.02, "text": " doing every one of them right so like um maybe I won't turn back on the screen share but you know", "tokens": [884, 633, 472, 295, 552, 558, 370, 411, 1105, 1310, 286, 1582, 380, 1261, 646, 322, 264, 2568, 2073, 457, 291, 458], "temperature": 0.0, "avg_logprob": -0.12986843525862493, "compression_ratio": 1.8528301886792453, "no_speech_prob": 1.1295895092189312e-05}, {"id": 498, "seek": 392738, "start": 3950.02, "end": 3957.3, "text": " we were doing in the objective function there was a sum over you so you've got you know this big", "tokens": [321, 645, 884, 294, 264, 10024, 2445, 456, 390, 257, 2408, 670, 291, 370, 291, 600, 658, 291, 458, 341, 955], "temperature": 0.0, "avg_logprob": -0.12986843525862493, "compression_ratio": 1.8528301886792453, "no_speech_prob": 1.1295895092189312e-05}, {"id": 499, "seek": 395730, "start": 3957.3, "end": 3964.1800000000003, "text": " corpus of text right so you're taking a sum over every word which is it appearing as the center word", "tokens": [1181, 31624, 295, 2487, 558, 370, 291, 434, 1940, 257, 2408, 670, 633, 1349, 597, 307, 309, 19870, 382, 264, 3056, 1349], "temperature": 0.0, "avg_logprob": -0.039300714598761666, "compression_ratio": 1.934673366834171, "no_speech_prob": 5.218923979555257e-05}, {"id": 500, "seek": 395730, "start": 3964.1800000000003, "end": 3970.6600000000003, "text": " and then inside that there's a second sum um which is for each word in the context so you are", "tokens": [293, 550, 1854, 300, 456, 311, 257, 1150, 2408, 1105, 597, 307, 337, 1184, 1349, 294, 264, 4319, 370, 291, 366], "temperature": 0.0, "avg_logprob": -0.039300714598761666, "compression_ratio": 1.934673366834171, "no_speech_prob": 5.218923979555257e-05}, {"id": 501, "seek": 395730, "start": 3970.6600000000003, "end": 3977.2200000000003, "text": " going to count each word as a context word and so then for one particular term of that objective", "tokens": [516, 281, 1207, 1184, 1349, 382, 257, 4319, 1349, 293, 370, 550, 337, 472, 1729, 1433, 295, 300, 10024], "temperature": 0.0, "avg_logprob": -0.039300714598761666, "compression_ratio": 1.934673366834171, "no_speech_prob": 5.218923979555257e-05}, {"id": 502, "seek": 395730, "start": 3977.2200000000003, "end": 3983.94, "text": " function you've got a particular context word and a particular um center word but you're then", "tokens": [2445, 291, 600, 658, 257, 1729, 4319, 1349, 293, 257, 1729, 1105, 3056, 1349, 457, 291, 434, 550], "temperature": 0.0, "avg_logprob": -0.039300714598761666, "compression_ratio": 1.934673366834171, "no_speech_prob": 5.218923979555257e-05}, {"id": 503, "seek": 398394, "start": 3983.94, "end": 3990.66, "text": " sort of summing over different context words for each center word and then you're summing over", "tokens": [1333, 295, 2408, 2810, 670, 819, 4319, 2283, 337, 1184, 3056, 1349, 293, 550, 291, 434, 2408, 2810, 670], "temperature": 0.0, "avg_logprob": -0.10536338343764796, "compression_ratio": 1.6511627906976745, "no_speech_prob": 1.8555390852270648e-05}, {"id": 504, "seek": 398394, "start": 3990.66, "end": 3998.02, "text": " all of the decisions of different center words and and to say um a little just a sentence more", "tokens": [439, 295, 264, 5327, 295, 819, 3056, 2283, 293, 293, 281, 584, 1105, 257, 707, 445, 257, 8174, 544], "temperature": 0.0, "avg_logprob": -0.10536338343764796, "compression_ratio": 1.6511627906976745, "no_speech_prob": 1.8555390852270648e-05}, {"id": 505, "seek": 398394, "start": 3998.02, "end": 4004.66, "text": " about having two vectors I mean you know in some senses in ugly detail but it was done to make", "tokens": [466, 1419, 732, 18875, 286, 914, 291, 458, 294, 512, 17057, 294, 12246, 2607, 457, 309, 390, 1096, 281, 652], "temperature": 0.0, "avg_logprob": -0.10536338343764796, "compression_ratio": 1.6511627906976745, "no_speech_prob": 1.8555390852270648e-05}, {"id": 506, "seek": 400466, "start": 4004.66, "end": 4014.3399999999997, "text": " things sort of simple and fast so you know if you um look at um the math carefully if you sort of", "tokens": [721, 1333, 295, 2199, 293, 2370, 370, 291, 458, 498, 291, 1105, 574, 412, 1105, 264, 5221, 7500, 498, 291, 1333, 295], "temperature": 0.0, "avg_logprob": -0.09531587191990444, "compression_ratio": 1.7515151515151515, "no_speech_prob": 1.516396241640905e-05}, {"id": 507, "seek": 400466, "start": 4014.3399999999997, "end": 4021.3799999999997, "text": " treated um this two vectors is the same so if you use the same vectors for center and context", "tokens": [8668, 1105, 341, 732, 18875, 307, 264, 912, 370, 498, 291, 764, 264, 912, 18875, 337, 3056, 293, 4319], "temperature": 0.0, "avg_logprob": -0.09531587191990444, "compression_ratio": 1.7515151515151515, "no_speech_prob": 1.516396241640905e-05}, {"id": 508, "seek": 400466, "start": 4022.3399999999997, "end": 4029.62, "text": " and you say okay let's work out the derivatives um things get uglier and the reason that they get", "tokens": [293, 291, 584, 1392, 718, 311, 589, 484, 264, 33733, 1105, 721, 483, 10743, 2753, 293, 264, 1778, 300, 436, 483], "temperature": 0.0, "avg_logprob": -0.09531587191990444, "compression_ratio": 1.7515151515151515, "no_speech_prob": 1.516396241640905e-05}, {"id": 509, "seek": 402962, "start": 4029.62, "end": 4038.74, "text": " uglier is it's okay when I'm iterating over all the choices of um context word oh my god sometimes", "tokens": [10743, 2753, 307, 309, 311, 1392, 562, 286, 478, 17138, 990, 670, 439, 264, 7994, 295, 1105, 4319, 1349, 1954, 452, 3044, 2171], "temperature": 0.0, "avg_logprob": -0.07676738330296108, "compression_ratio": 1.6384180790960452, "no_speech_prob": 2.0445708287297748e-05}, {"id": 510, "seek": 402962, "start": 4038.74, "end": 4044.9, "text": " the context word is going to be the same as the center word and so that messes with working out", "tokens": [264, 4319, 1349, 307, 516, 281, 312, 264, 912, 382, 264, 3056, 1349, 293, 370, 300, 2082, 279, 365, 1364, 484], "temperature": 0.0, "avg_logprob": -0.07676738330296108, "compression_ratio": 1.6384180790960452, "no_speech_prob": 2.0445708287297748e-05}, {"id": 511, "seek": 402962, "start": 4045.7799999999997, "end": 4053.62, "text": " my derivatives um whereas by taking them as separate vectors that never happens so it's easy um", "tokens": [452, 33733, 1105, 9735, 538, 1940, 552, 382, 4994, 18875, 300, 1128, 2314, 370, 309, 311, 1858, 1105], "temperature": 0.0, "avg_logprob": -0.07676738330296108, "compression_ratio": 1.6384180790960452, "no_speech_prob": 2.0445708287297748e-05}, {"id": 512, "seek": 405362, "start": 4053.62, "end": 4059.46, "text": " but the kind of interesting thing is you know saying that you have these two different representations", "tokens": [457, 264, 733, 295, 1880, 551, 307, 291, 458, 1566, 300, 291, 362, 613, 732, 819, 33358], "temperature": 0.0, "avg_logprob": -0.08472597321798635, "compression_ratio": 1.8480392156862746, "no_speech_prob": 1.9821798559860326e-05}, {"id": 513, "seek": 405362, "start": 4060.2599999999998, "end": 4067.94, "text": " sort of just ends up really sort of doing no harm and my wave my hands argument for that is", "tokens": [1333, 295, 445, 5314, 493, 534, 1333, 295, 884, 572, 6491, 293, 452, 5772, 452, 2377, 6770, 337, 300, 307], "temperature": 0.0, "avg_logprob": -0.08472597321798635, "compression_ratio": 1.8480392156862746, "no_speech_prob": 1.9821798559860326e-05}, {"id": 514, "seek": 405362, "start": 4067.94, "end": 4074.8199999999997, "text": " you know since we're kind of moving through each position the corpus one by one you know", "tokens": [291, 458, 1670, 321, 434, 733, 295, 2684, 807, 1184, 2535, 264, 1181, 31624, 472, 538, 472, 291, 458], "temperature": 0.0, "avg_logprob": -0.08472597321798635, "compression_ratio": 1.8480392156862746, "no_speech_prob": 1.9821798559860326e-05}, {"id": 515, "seek": 405362, "start": 4074.8199999999997, "end": 4080.42, "text": " something a word that is the center word at one moment is going to be the context word at the", "tokens": [746, 257, 1349, 300, 307, 264, 3056, 1349, 412, 472, 1623, 307, 516, 281, 312, 264, 4319, 1349, 412, 264], "temperature": 0.0, "avg_logprob": -0.08472597321798635, "compression_ratio": 1.8480392156862746, "no_speech_prob": 1.9821798559860326e-05}, {"id": 516, "seek": 408042, "start": 4080.42, "end": 4086.7400000000002, "text": " next moment and the word that was the context word is going to become the center word so you're", "tokens": [958, 1623, 293, 264, 1349, 300, 390, 264, 4319, 1349, 307, 516, 281, 1813, 264, 3056, 1349, 370, 291, 434], "temperature": 0.0, "avg_logprob": -0.07024202576602798, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.0441410267958418e-05}, {"id": 517, "seek": 408042, "start": 4086.7400000000002, "end": 4094.82, "text": " sort of doing the um the computation both ways in each case and so you should be able to convince", "tokens": [1333, 295, 884, 264, 1105, 264, 24903, 1293, 2098, 294, 1184, 1389, 293, 370, 291, 820, 312, 1075, 281, 13447], "temperature": 0.0, "avg_logprob": -0.07024202576602798, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.0441410267958418e-05}, {"id": 518, "seek": 408042, "start": 4094.82, "end": 4101.3, "text": " yourself that the two representations for the word end up being very similar and they do not", "tokens": [1803, 300, 264, 732, 33358, 337, 264, 1349, 917, 493, 885, 588, 2531, 293, 436, 360, 406], "temperature": 0.0, "avg_logprob": -0.07024202576602798, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.0441410267958418e-05}, {"id": 519, "seek": 408042, "start": 4101.3, "end": 4106.1, "text": " not identical for technical reasons of the ends of documents and things like that but very", "tokens": [406, 14800, 337, 6191, 4112, 295, 264, 5314, 295, 8512, 293, 721, 411, 300, 457, 588], "temperature": 0.0, "avg_logprob": -0.07024202576602798, "compression_ratio": 1.7867298578199051, "no_speech_prob": 1.0441410267958418e-05}, {"id": 520, "seek": 410610, "start": 4106.1, "end": 4114.1, "text": " very similar um and so effectively tend to get two very similar representations for each word", "tokens": [588, 2531, 1105, 293, 370, 8659, 3928, 281, 483, 732, 588, 2531, 33358, 337, 1184, 1349], "temperature": 0.0, "avg_logprob": -0.14112480387968176, "compression_ratio": 1.9, "no_speech_prob": 5.646696445182897e-05}, {"id": 521, "seek": 410610, "start": 4114.1, "end": 4119.22, "text": " and we just average them and call that the word vector and so when we use word vectors we just have", "tokens": [293, 321, 445, 4274, 552, 293, 818, 300, 264, 1349, 8062, 293, 370, 562, 321, 764, 1349, 18875, 321, 445, 362], "temperature": 0.0, "avg_logprob": -0.14112480387968176, "compression_ratio": 1.9, "no_speech_prob": 5.646696445182897e-05}, {"id": 522, "seek": 410610, "start": 4119.22, "end": 4127.06, "text": " one vector for each word that makes sense thank you i have a question purely of curiosity so we", "tokens": [472, 8062, 337, 1184, 1349, 300, 1669, 2020, 1309, 291, 741, 362, 257, 1168, 17491, 295, 18769, 370, 321], "temperature": 0.0, "avg_logprob": -0.14112480387968176, "compression_ratio": 1.9, "no_speech_prob": 5.646696445182897e-05}, {"id": 523, "seek": 410610, "start": 4127.06, "end": 4132.5, "text": " saw that when we projected the um vectors the word vectors onto the 2d surface we saw like", "tokens": [1866, 300, 562, 321, 26231, 264, 1105, 18875, 264, 1349, 18875, 3911, 264, 568, 67, 3753, 321, 1866, 411], "temperature": 0.0, "avg_logprob": -0.14112480387968176, "compression_ratio": 1.9, "no_speech_prob": 5.646696445182897e-05}, {"id": 524, "seek": 413250, "start": 4132.5, "end": 4137.22, "text": " little clusters of where's our similar each other and then later on we saw that um with the", "tokens": [707, 23313, 295, 689, 311, 527, 2531, 1184, 661, 293, 550, 1780, 322, 321, 1866, 300, 1105, 365, 264], "temperature": 0.0, "avg_logprob": -0.13678780342768698, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.17803808482131e-05}, {"id": 525, "seek": 413250, "start": 4137.22, "end": 4142.42, "text": " analogy thing we kind of see that there's these directional vectors that sort of anything like", "tokens": [21663, 551, 321, 733, 295, 536, 300, 456, 311, 613, 42242, 18875, 300, 1333, 295, 1340, 411], "temperature": 0.0, "avg_logprob": -0.13678780342768698, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.17803808482131e-05}, {"id": 526, "seek": 413250, "start": 4142.42, "end": 4148.02, "text": " the ruler of or the CEO of something like that and so I'm wondering is there are other relationships", "tokens": [264, 19661, 295, 420, 264, 9282, 295, 746, 411, 300, 293, 370, 286, 478, 6359, 307, 456, 366, 661, 6159], "temperature": 0.0, "avg_logprob": -0.13678780342768698, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.17803808482131e-05}, {"id": 527, "seek": 413250, "start": 4148.02, "end": 4154.5, "text": " between those relational vectors themselves such as like is the um the ruler of vector sort of", "tokens": [1296, 729, 38444, 18875, 2969, 1270, 382, 411, 307, 264, 1105, 264, 19661, 295, 8062, 1333, 295], "temperature": 0.0, "avg_logprob": -0.13678780342768698, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.17803808482131e-05}, {"id": 528, "seek": 413250, "start": 4154.5, "end": 4160.98, "text": " similar to the CEO of vector which is very different from like is makes a good sandwich with", "tokens": [2531, 281, 264, 9282, 295, 8062, 597, 307, 588, 819, 490, 411, 307, 1669, 257, 665, 11141, 365], "temperature": 0.0, "avg_logprob": -0.13678780342768698, "compression_ratio": 1.8849206349206349, "no_speech_prob": 2.17803808482131e-05}, {"id": 529, "seek": 416098, "start": 4160.98, "end": 4171.0599999999995, "text": " vector um is there any research on that that's a good question um how will you stump me already", "tokens": [8062, 1105, 307, 456, 604, 2132, 322, 300, 300, 311, 257, 665, 1168, 1105, 577, 486, 291, 43164, 385, 1217], "temperature": 0.0, "avg_logprob": -0.1217939058939616, "compression_ratio": 1.7378048780487805, "no_speech_prob": 8.190332300728187e-05}, {"id": 530, "seek": 416098, "start": 4171.0599999999995, "end": 4181.62, "text": " in the first lecture uh i mean that yeah i can't actually think of a piece of research and so", "tokens": [294, 264, 700, 7991, 2232, 741, 914, 300, 1338, 741, 393, 380, 767, 519, 295, 257, 2522, 295, 2132, 293, 370], "temperature": 0.0, "avg_logprob": -0.1217939058939616, "compression_ratio": 1.7378048780487805, "no_speech_prob": 8.190332300728187e-05}, {"id": 531, "seek": 416098, "start": 4181.62, "end": 4186.259999999999, "text": " i'm not sure i have a confident and i'm not sure i have a confident answer i mean it seems like", "tokens": [741, 478, 406, 988, 741, 362, 257, 6679, 293, 741, 478, 406, 988, 741, 362, 257, 6679, 1867, 741, 914, 309, 2544, 411], "temperature": 0.0, "avg_logprob": -0.1217939058939616, "compression_ratio": 1.7378048780487805, "no_speech_prob": 8.190332300728187e-05}, {"id": 532, "seek": 418626, "start": 4186.26, "end": 4193.14, "text": " that's a really easy thing to check um with how much you have one of these sets of um", "tokens": [300, 311, 257, 534, 1858, 551, 281, 1520, 1105, 365, 577, 709, 291, 362, 472, 295, 613, 6352, 295, 1105], "temperature": 0.0, "avg_logprob": -0.12928623786339394, "compression_ratio": 1.535294117647059, "no_speech_prob": 2.6173758669756353e-05}, {"id": 533, "seek": 418626, "start": 4193.14, "end": 4200.66, "text": " word vectors that it seems um like and for any relationship that is represented well", "tokens": [1349, 18875, 300, 309, 2544, 1105, 411, 293, 337, 604, 2480, 300, 307, 10379, 731], "temperature": 0.0, "avg_logprob": -0.12928623786339394, "compression_ratio": 1.535294117647059, "no_speech_prob": 2.6173758669756353e-05}, {"id": 534, "seek": 418626, "start": 4200.66, "end": 4208.5, "text": " enough by word you should be able to see if it comes out kind of similar um huh i mean i'm", "tokens": [1547, 538, 1349, 291, 820, 312, 1075, 281, 536, 498, 309, 1487, 484, 733, 295, 2531, 1105, 7020, 741, 914, 741, 478], "temperature": 0.0, "avg_logprob": -0.12928623786339394, "compression_ratio": 1.535294117647059, "no_speech_prob": 2.6173758669756353e-05}, {"id": 535, "seek": 420850, "start": 4208.5, "end": 4217.94, "text": " not sure we can we can look and see yeah that's totally okay just just curious sorry i missed the", "tokens": [406, 988, 321, 393, 321, 393, 574, 293, 536, 1338, 300, 311, 3879, 1392, 445, 445, 6369, 2597, 741, 6721, 264], "temperature": 0.0, "avg_logprob": -0.17864682457663797, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.4267760889197234e-05}, {"id": 536, "seek": 420850, "start": 4217.94, "end": 4223.3, "text": " last little bit your answer to first question so when you wanted to collapse to vectors for the same", "tokens": [1036, 707, 857, 428, 1867, 281, 700, 1168, 370, 562, 291, 1415, 281, 15584, 281, 18875, 337, 264, 912], "temperature": 0.0, "avg_logprob": -0.17864682457663797, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.4267760889197234e-05}, {"id": 537, "seek": 420850, "start": 4223.3, "end": 4228.02, "text": " word did you say you usually take the average um different people have done different things for", "tokens": [1349, 630, 291, 584, 291, 2673, 747, 264, 4274, 1105, 819, 561, 362, 1096, 819, 721, 337], "temperature": 0.0, "avg_logprob": -0.17864682457663797, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.4267760889197234e-05}, {"id": 538, "seek": 420850, "start": 4228.02, "end": 4235.54, "text": " the most common practice is after you uh you know there's still a bit more i have to cover about", "tokens": [264, 881, 2689, 3124, 307, 934, 291, 2232, 291, 458, 456, 311, 920, 257, 857, 544, 741, 362, 281, 2060, 466], "temperature": 0.0, "avg_logprob": -0.17864682457663797, "compression_ratio": 1.7117903930131004, "no_speech_prob": 1.4267760889197234e-05}, {"id": 539, "seek": 423554, "start": 4235.54, "end": 4239.62, "text": " running word devec that we didn't really get through today so i've still got a bit more work to do", "tokens": [2614, 1349, 368, 303, 66, 300, 321, 994, 380, 534, 483, 807, 965, 370, 741, 600, 920, 658, 257, 857, 544, 589, 281, 360], "temperature": 0.0, "avg_logprob": -0.11585865020751954, "compression_ratio": 1.7366071428571428, "no_speech_prob": 1.8882077711168677e-05}, {"id": 540, "seek": 423554, "start": 4239.62, "end": 4247.7, "text": " on first day but you know once you run your word devec algorithm and you sort of your output is two", "tokens": [322, 700, 786, 457, 291, 458, 1564, 291, 1190, 428, 1349, 368, 303, 66, 9284, 293, 291, 1333, 295, 428, 5598, 307, 732], "temperature": 0.0, "avg_logprob": -0.11585865020751954, "compression_ratio": 1.7366071428571428, "no_speech_prob": 1.8882077711168677e-05}, {"id": 541, "seek": 423554, "start": 4247.7, "end": 4255.14, "text": " vectors for each word and kind of a when it's center and when it's context and so typically people", "tokens": [18875, 337, 1184, 1349, 293, 733, 295, 257, 562, 309, 311, 3056, 293, 562, 309, 311, 4319, 293, 370, 5850, 561], "temperature": 0.0, "avg_logprob": -0.11585865020751954, "compression_ratio": 1.7366071428571428, "no_speech_prob": 1.8882077711168677e-05}, {"id": 542, "seek": 423554, "start": 4255.14, "end": 4261.78, "text": " just average those two vectors and say okay that's the representation of the word croissant", "tokens": [445, 4274, 729, 732, 18875, 293, 584, 1392, 300, 311, 264, 10290, 295, 264, 1349, 4848, 29492], "temperature": 0.0, "avg_logprob": -0.11585865020751954, "compression_ratio": 1.7366071428571428, "no_speech_prob": 1.8882077711168677e-05}, {"id": 543, "seek": 426178, "start": 4261.78, "end": 4267.3, "text": " and that's what appears in the sort of word vectors file like the one i loaded", "tokens": [293, 300, 311, 437, 7038, 294, 264, 1333, 295, 1349, 18875, 3991, 411, 264, 472, 741, 13210], "temperature": 0.0, "avg_logprob": -0.22544552031017484, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.320383762504207e-05}, {"id": 544, "seek": 426178, "start": 4271.219999999999, "end": 4277.139999999999, "text": " oh thanks so my question is if a word have two different meanings or multiple different meanings", "tokens": [1954, 3231, 370, 452, 1168, 307, 498, 257, 1349, 362, 732, 819, 28138, 420, 3866, 819, 28138], "temperature": 0.0, "avg_logprob": -0.22544552031017484, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.320383762504207e-05}, {"id": 545, "seek": 426178, "start": 4277.139999999999, "end": 4284.98, "text": " can we still represent that's the same single vector? Yes that's a very good question um and actually", "tokens": [393, 321, 920, 2906, 300, 311, 264, 912, 2167, 8062, 30, 1079, 300, 311, 257, 588, 665, 1168, 1105, 293, 767], "temperature": 0.0, "avg_logprob": -0.22544552031017484, "compression_ratio": 1.6294117647058823, "no_speech_prob": 1.320383762504207e-05}, {"id": 546, "seek": 428498, "start": 4284.98, "end": 4291.94, "text": " there is some content on that in first days lecture so i can say more about that um but yeah the", "tokens": [456, 307, 512, 2701, 322, 300, 294, 700, 1708, 7991, 370, 741, 393, 584, 544, 466, 300, 1105, 457, 1338, 264], "temperature": 0.0, "avg_logprob": -0.0865455215627497, "compression_ratio": 1.75, "no_speech_prob": 0.00010519017087062821}, {"id": 547, "seek": 428498, "start": 4291.94, "end": 4299.379999999999, "text": " first reaction is you kind of should be scared because um something i've said nothing about at all", "tokens": [700, 5480, 307, 291, 733, 295, 820, 312, 5338, 570, 1105, 746, 741, 600, 848, 1825, 466, 412, 439], "temperature": 0.0, "avg_logprob": -0.0865455215627497, "compression_ratio": 1.75, "no_speech_prob": 0.00010519017087062821}, {"id": 548, "seek": 428498, "start": 4299.379999999999, "end": 4306.9, "text": " is you know most words especially short common words have lots of meanings so if you have a word", "tokens": [307, 291, 458, 881, 2283, 2318, 2099, 2689, 2283, 362, 3195, 295, 28138, 370, 498, 291, 362, 257, 1349], "temperature": 0.0, "avg_logprob": -0.0865455215627497, "compression_ratio": 1.75, "no_speech_prob": 0.00010519017087062821}, {"id": 549, "seek": 428498, "start": 4306.9, "end": 4314.259999999999, "text": " like star that can be astronomical object or it can be you know a film star a Hollywood star", "tokens": [411, 3543, 300, 393, 312, 49035, 2657, 420, 309, 393, 312, 291, 458, 257, 2007, 3543, 257, 11628, 3543], "temperature": 0.0, "avg_logprob": -0.0865455215627497, "compression_ratio": 1.75, "no_speech_prob": 0.00010519017087062821}, {"id": 550, "seek": 431426, "start": 4314.26, "end": 4320.42, "text": " or it can be something like the gold stars that you've got an elementary school and we've just", "tokens": [420, 309, 393, 312, 746, 411, 264, 3821, 6105, 300, 291, 600, 658, 364, 16429, 1395, 293, 321, 600, 445], "temperature": 0.0, "avg_logprob": -0.104872499193464, "compression_ratio": 1.691588785046729, "no_speech_prob": 3.524305429891683e-05}, {"id": 551, "seek": 431426, "start": 4320.42, "end": 4329.62, "text": " taking all those uses of the word star and collapsing them together um into one word vector um", "tokens": [1940, 439, 729, 4960, 295, 264, 1349, 3543, 293, 45339, 552, 1214, 1105, 666, 472, 1349, 8062, 1105], "temperature": 0.0, "avg_logprob": -0.104872499193464, "compression_ratio": 1.691588785046729, "no_speech_prob": 3.524305429891683e-05}, {"id": 552, "seek": 431426, "start": 4329.62, "end": 4335.46, "text": " and you might think that's really crazy and bad um but actually turns out to work", "tokens": [293, 291, 1062, 519, 300, 311, 534, 3219, 293, 1578, 1105, 457, 767, 4523, 484, 281, 589], "temperature": 0.0, "avg_logprob": -0.104872499193464, "compression_ratio": 1.691588785046729, "no_speech_prob": 3.524305429891683e-05}, {"id": 553, "seek": 431426, "start": 4335.46, "end": 4342.58, "text": " rather well um maybe i won't go through all of that um right now because there is actually", "tokens": [2831, 731, 1105, 1310, 741, 1582, 380, 352, 807, 439, 295, 300, 1105, 558, 586, 570, 456, 307, 767], "temperature": 0.0, "avg_logprob": -0.104872499193464, "compression_ratio": 1.691588785046729, "no_speech_prob": 3.524305429891683e-05}, {"id": 554, "seek": 434258, "start": 4342.58, "end": 4349.22, "text": " stuff on that on first days lecture oh i see thanks you can just add up the slides for next time", "tokens": [1507, 322, 300, 322, 700, 1708, 7991, 1954, 741, 536, 3231, 291, 393, 445, 909, 493, 264, 9788, 337, 958, 565], "temperature": 0.0, "avg_logprob": -0.6097334309628135, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.00010350439697504044}, {"id": 555, "seek": 434258, "start": 4349.22, "end": 4351.22, "text": " oh white", "tokens": [1954, 2418], "temperature": 0.0, "avg_logprob": -0.6097334309628135, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.00010350439697504044}, {"id": 556, "seek": 434258, "start": 4352.1, "end": 4359.7, "text": " hey i know this makes me seem as good as but i guess a lot of us were also taking this course because", "tokens": [4177, 741, 458, 341, 1669, 385, 1643, 382, 665, 382, 457, 741, 2041, 257, 688, 295, 505, 645, 611, 1940, 341, 1164, 570], "temperature": 0.0, "avg_logprob": -0.6097334309628135, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.00010350439697504044}, {"id": 557, "seek": 434258, "start": 4359.7, "end": 4369.22, "text": " of the height, good speed, AI, speech recognition and my basic question is maybe two basic is", "tokens": [295, 264, 6681, 11, 665, 3073, 11, 7318, 11, 6218, 11150, 293, 452, 3875, 1168, 307, 1310, 732, 3875, 307], "temperature": 0.0, "avg_logprob": -0.6097334309628135, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.00010350439697504044}, {"id": 558, "seek": 436922, "start": 4369.22, "end": 4376.9800000000005, "text": " do we look at how to implement or do we look at the stack of like some of the", "tokens": [360, 321, 574, 412, 577, 281, 4445, 420, 360, 321, 574, 412, 264, 8630, 295, 411, 512, 295, 264], "temperature": 0.0, "avg_logprob": -0.4346571583901682, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.00014117833052296191}, {"id": 559, "seek": 436922, "start": 4376.9800000000005, "end": 4383.46, "text": " lecture something prox speech to uh contact actions in this course was it just the priority", "tokens": [7991, 746, 447, 87, 6218, 281, 2232, 3385, 5909, 294, 341, 1164, 390, 309, 445, 264, 9365], "temperature": 0.0, "avg_logprob": -0.4346571583901682, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.00014117833052296191}, {"id": 560, "seek": 436922, "start": 4384.5, "end": 4393.62, "text": " uh understanding so this is an unusual content an unusual quarter um but for this quarter there's", "tokens": [2232, 3701, 370, 341, 307, 364, 10901, 2701, 364, 10901, 6555, 1105, 457, 337, 341, 6555, 456, 311], "temperature": 0.0, "avg_logprob": -0.4346571583901682, "compression_ratio": 1.6583850931677018, "no_speech_prob": 0.00014117833052296191}, {"id": 561, "seek": 439362, "start": 4393.62, "end": 4402.42, "text": " a very clear answer which is um this quarter um there's also a speech class being taught which is", "tokens": [257, 588, 1850, 1867, 597, 307, 1105, 341, 6555, 1105, 456, 311, 611, 257, 6218, 1508, 885, 5928, 597, 307], "temperature": 0.0, "avg_logprob": -0.1278479784384541, "compression_ratio": 1.7488372093023257, "no_speech_prob": 7.406769873341545e-05}, {"id": 562, "seek": 439362, "start": 4402.42, "end": 4410.82, "text": " CS224S um a speech class being taught by Andrew Mars and you know this is a class that's been more", "tokens": [9460, 7490, 19, 50, 1105, 257, 6218, 1508, 885, 5928, 538, 10110, 9692, 293, 291, 458, 341, 307, 257, 1508, 300, 311, 668, 544], "temperature": 0.0, "avg_logprob": -0.1278479784384541, "compression_ratio": 1.7488372093023257, "no_speech_prob": 7.406769873341545e-05}, {"id": 563, "seek": 439362, "start": 4410.82, "end": 4416.099999999999, "text": " regularly offered sometimes it's only been offered every third year um but it's being offered", "tokens": [11672, 8059, 2171, 309, 311, 787, 668, 8059, 633, 2636, 1064, 1105, 457, 309, 311, 885, 8059], "temperature": 0.0, "avg_logprob": -0.1278479784384541, "compression_ratio": 1.7488372093023257, "no_speech_prob": 7.406769873341545e-05}, {"id": 564, "seek": 441610, "start": 4416.1, "end": 4424.42, "text": " right now so if what you want to do is learn about speech recognition and learn about sort of methods", "tokens": [558, 586, 370, 498, 437, 291, 528, 281, 360, 307, 1466, 466, 6218, 11150, 293, 1466, 466, 1333, 295, 7150], "temperature": 0.0, "avg_logprob": -0.0618382605953493, "compression_ratio": 1.5989583333333333, "no_speech_prob": 1.5170285223575775e-05}, {"id": 565, "seek": 441610, "start": 4424.42, "end": 4434.900000000001, "text": " for building dialogue systems um you should do CS224S um so you know for this class in general um the", "tokens": [337, 2390, 10221, 3652, 1105, 291, 820, 360, 9460, 7490, 19, 50, 1105, 370, 291, 458, 337, 341, 1508, 294, 2674, 1105, 264], "temperature": 0.0, "avg_logprob": -0.0618382605953493, "compression_ratio": 1.5989583333333333, "no_speech_prob": 1.5170285223575775e-05}, {"id": 566, "seek": 441610, "start": 4434.900000000001, "end": 4445.620000000001, "text": " vast bulk of this class is working with text and doing various kinds of text analysis and understanding", "tokens": [8369, 16139, 295, 341, 1508, 307, 1364, 365, 2487, 293, 884, 3683, 3685, 295, 2487, 5215, 293, 3701], "temperature": 0.0, "avg_logprob": -0.0618382605953493, "compression_ratio": 1.5989583333333333, "no_speech_prob": 1.5170285223575775e-05}, {"id": 567, "seek": 444562, "start": 4445.62, "end": 4453.14, "text": " so we do tasks like some of the ones I've mentioned we do machine translation um we um do question", "tokens": [370, 321, 360, 9608, 411, 512, 295, 264, 2306, 286, 600, 2835, 321, 360, 3479, 12853, 1105, 321, 1105, 360, 1168], "temperature": 0.0, "avg_logprob": -0.15632224664455507, "compression_ratio": 1.6775700934579438, "no_speech_prob": 5.4299347539199516e-05}, {"id": 568, "seek": 444562, "start": 4453.14, "end": 4459.78, "text": " answering um we look at how to parse this structure of sentences and things like that you know", "tokens": [13430, 1105, 321, 574, 412, 577, 281, 48377, 341, 3877, 295, 16579, 293, 721, 411, 300, 291, 458], "temperature": 0.0, "avg_logprob": -0.15632224664455507, "compression_ratio": 1.6775700934579438, "no_speech_prob": 5.4299347539199516e-05}, {"id": 569, "seek": 444562, "start": 4459.78, "end": 4465.62, "text": " in other years I sometimes say a little bit about speech um but since this quarter there's a", "tokens": [294, 661, 924, 286, 2171, 584, 257, 707, 857, 466, 6218, 1105, 457, 1670, 341, 6555, 456, 311, 257], "temperature": 0.0, "avg_logprob": -0.15632224664455507, "compression_ratio": 1.6775700934579438, "no_speech_prob": 5.4299347539199516e-05}, {"id": 570, "seek": 444562, "start": 4465.62, "end": 4468.98, "text": " whole different class that's focused on speech that's similar but silly.", "tokens": [1379, 819, 1508, 300, 311, 5178, 322, 6218, 300, 311, 2531, 457, 11774, 13], "temperature": 0.0, "avg_logprob": -0.15632224664455507, "compression_ratio": 1.6775700934579438, "no_speech_prob": 5.4299347539199516e-05}, {"id": 571, "seek": 446898, "start": 4468.98, "end": 4474.9, "text": " Well that's what I'm talking about right now together uh the part of pardon and we're in each audio", "tokens": [1042, 300, 311, 437, 286, 478, 1417, 466, 558, 586, 1214, 2232, 264, 644, 295, 22440, 293, 321, 434, 294, 1184, 6278], "temperature": 0.0, "avg_logprob": -0.6921352039683949, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.0003650078142527491}, {"id": 572, "seek": 446898, "start": 4474.9, "end": 4481.379999999999, "text": " like I guess I guess I guess I guess I can you're in your focus more on speech", "tokens": [411, 286, 2041, 286, 2041, 286, 2041, 286, 2041, 286, 393, 291, 434, 294, 428, 1879, 544, 322, 6218], "temperature": 0.0, "avg_logprob": -0.6921352039683949, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.0003650078142527491}, {"id": 573, "seek": 446898, "start": 4481.379999999999, "end": 4486.74, "text": " feeling standing I guess I guess I guess I'll be I'll be this is the question I'm going to have", "tokens": [2633, 4877, 286, 2041, 286, 2041, 286, 2041, 286, 603, 312, 286, 603, 312, 341, 307, 264, 1168, 286, 478, 516, 281, 362], "temperature": 0.0, "avg_logprob": -0.6921352039683949, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.0003650078142527491}, {"id": 574, "seek": 446898, "start": 4486.74, "end": 4492.66, "text": " on I'm now getting a bad echo I'm not sure if that's my fault or your fault but I'm anyway um", "tokens": [322, 286, 478, 586, 1242, 257, 1578, 14300, 286, 478, 406, 988, 498, 300, 311, 452, 7441, 420, 428, 7441, 457, 286, 478, 4033, 1105], "temperature": 0.0, "avg_logprob": -0.6921352039683949, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.0003650078142527491}, {"id": 575, "seek": 449266, "start": 4492.66, "end": 4502.099999999999, "text": " um anyway answer yeah so the speech class does a mix of stuff so I mean the sort of pure speech", "tokens": [1105, 4033, 1867, 1338, 370, 264, 6218, 1508, 775, 257, 2890, 295, 1507, 370, 286, 914, 264, 1333, 295, 6075, 6218], "temperature": 0.0, "avg_logprob": -0.08304056254300204, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.00010974666656693444}, {"id": 576, "seek": 449266, "start": 4502.099999999999, "end": 4509.86, "text": " problems classically have been um doing speech recognitions are going from a speech signal to text", "tokens": [2740, 1508, 984, 362, 668, 1105, 884, 6218, 3068, 2451, 366, 516, 490, 257, 6218, 6358, 281, 2487], "temperature": 0.0, "avg_logprob": -0.08304056254300204, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.00010974666656693444}, {"id": 577, "seek": 449266, "start": 4509.86, "end": 4518.099999999999, "text": " and doing text to speech going from text to a speech signal and both of those are problems which", "tokens": [293, 884, 2487, 281, 6218, 516, 490, 2487, 281, 257, 6218, 6358, 293, 1293, 295, 729, 366, 2740, 597], "temperature": 0.0, "avg_logprob": -0.08304056254300204, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.00010974666656693444}, {"id": 578, "seek": 451810, "start": 4518.1, "end": 4524.820000000001, "text": " are now normally done including by the cell phone that sits in your pocket um using your networks", "tokens": [366, 586, 5646, 1096, 3009, 538, 264, 2815, 2593, 300, 12696, 294, 428, 8963, 1105, 1228, 428, 9590], "temperature": 0.0, "avg_logprob": -0.07999220628004808, "compression_ratio": 1.582010582010582, "no_speech_prob": 6.173172732815146e-05}, {"id": 579, "seek": 451810, "start": 4524.820000000001, "end": 4532.820000000001, "text": " and so it covers both of those but then between that um the class covers quite a bit and in particular", "tokens": [293, 370, 309, 10538, 1293, 295, 729, 457, 550, 1296, 300, 1105, 264, 1508, 10538, 1596, 257, 857, 293, 294, 1729], "temperature": 0.0, "avg_logprob": -0.07999220628004808, "compression_ratio": 1.582010582010582, "no_speech_prob": 6.173172732815146e-05}, {"id": 580, "seek": 451810, "start": 4532.820000000001, "end": 4540.5, "text": " it starts off with um looking at building dialogue systems so this is sort of something like Alexa", "tokens": [309, 3719, 766, 365, 1105, 1237, 412, 2390, 10221, 3652, 370, 341, 307, 1333, 295, 746, 411, 22595], "temperature": 0.0, "avg_logprob": -0.07999220628004808, "compression_ratio": 1.582010582010582, "no_speech_prob": 6.173172732815146e-05}, {"id": 581, "seek": 454050, "start": 4540.5, "end": 4547.78, "text": " Google Assistance series as to well assuming you have a speech recognition a text to speech system", "tokens": [3329, 46805, 2638, 382, 281, 731, 11926, 291, 362, 257, 6218, 11150, 257, 2487, 281, 6218, 1185], "temperature": 0.0, "avg_logprob": -0.20609208515712193, "compression_ratio": 1.5590062111801242, "no_speech_prob": 7.539855232607806e-06}, {"id": 582, "seek": 454050, "start": 4549.38, "end": 4555.78, "text": " then you do have text in and text out what are the kind of ways that people go about building um", "tokens": [550, 291, 360, 362, 2487, 294, 293, 2487, 484, 437, 366, 264, 733, 295, 2098, 300, 561, 352, 466, 2390, 1105], "temperature": 0.0, "avg_logprob": -0.20609208515712193, "compression_ratio": 1.5590062111801242, "no_speech_prob": 7.539855232607806e-06}, {"id": 583, "seek": 454050, "start": 4556.66, "end": 4562.18, "text": " um dialogue systems like the ones that I just mentioned", "tokens": [1105, 10221, 3652, 411, 264, 2306, 300, 286, 445, 2835], "temperature": 0.0, "avg_logprob": -0.20609208515712193, "compression_ratio": 1.5590062111801242, "no_speech_prob": 7.539855232607806e-06}, {"id": 584, "seek": 456218, "start": 4562.18, "end": 4569.46, "text": " um actually how to question so I think there is some people in the chat noticing that the", "tokens": [1105, 767, 577, 281, 1168, 370, 286, 519, 456, 307, 512, 561, 294, 264, 5081, 21814, 300, 264], "temperature": 0.0, "avg_logprob": -0.19179337600181842, "compression_ratio": 1.7395348837209301, "no_speech_prob": 4.9734437197912484e-05}, {"id": 585, "seek": 456218, "start": 4570.18, "end": 4574.820000000001, "text": " like opposites were really near to each other which was kind of odd but I was also wondering um", "tokens": [411, 4665, 3324, 645, 534, 2651, 281, 1184, 661, 597, 390, 733, 295, 7401, 457, 286, 390, 611, 6359, 1105], "temperature": 0.0, "avg_logprob": -0.19179337600181842, "compression_ratio": 1.7395348837209301, "no_speech_prob": 4.9734437197912484e-05}, {"id": 586, "seek": 456218, "start": 4575.62, "end": 4581.9400000000005, "text": " what about like positive and negative uh violins or like offense um is that captured well", "tokens": [437, 466, 411, 3353, 293, 3671, 2232, 3448, 1292, 420, 411, 17834, 1105, 307, 300, 11828, 731], "temperature": 0.0, "avg_logprob": -0.19179337600181842, "compression_ratio": 1.7395348837209301, "no_speech_prob": 4.9734437197912484e-05}, {"id": 587, "seek": 456218, "start": 4581.9400000000005, "end": 4587.700000000001, "text": " in this type of model or is it like not captured well like we're like with the opposites how those", "tokens": [294, 341, 2010, 295, 2316, 420, 307, 309, 411, 406, 11828, 731, 411, 321, 434, 411, 365, 264, 4665, 3324, 577, 729], "temperature": 0.0, "avg_logprob": -0.19179337600181842, "compression_ratio": 1.7395348837209301, "no_speech_prob": 4.9734437197912484e-05}, {"id": 588, "seek": 458770, "start": 4587.7, "end": 4593.38, "text": " weren't really so the short answer is for both of those and so there's this is a good question", "tokens": [4999, 380, 534, 370, 264, 2099, 1867, 307, 337, 1293, 295, 729, 293, 370, 456, 311, 341, 307, 257, 665, 1168], "temperature": 0.0, "avg_logprob": -0.10528586538214432, "compression_ratio": 1.9597989949748744, "no_speech_prob": 6.61028825561516e-05}, {"id": 589, "seek": 458770, "start": 4593.38, "end": 4600.34, "text": " a good observation and the short answer is no both of those are captured really really badly I mean", "tokens": [257, 665, 14816, 293, 264, 2099, 1867, 307, 572, 1293, 295, 729, 366, 11828, 534, 534, 13425, 286, 914], "temperature": 0.0, "avg_logprob": -0.10528586538214432, "compression_ratio": 1.9597989949748744, "no_speech_prob": 6.61028825561516e-05}, {"id": 590, "seek": 458770, "start": 4600.34, "end": 4609.86, "text": " there's there's a definition um you know when I say really really badly I mean what I mean is if", "tokens": [456, 311, 456, 311, 257, 7123, 1105, 291, 458, 562, 286, 584, 534, 534, 13425, 286, 914, 437, 286, 914, 307, 498], "temperature": 0.0, "avg_logprob": -0.10528586538214432, "compression_ratio": 1.9597989949748744, "no_speech_prob": 6.61028825561516e-05}, {"id": 591, "seek": 458770, "start": 4609.86, "end": 4616.66, "text": " that's what you want to focus on um you've got problems I mean it's not that the algorithm doesn't", "tokens": [300, 311, 437, 291, 528, 281, 1879, 322, 1105, 291, 600, 658, 2740, 286, 914, 309, 311, 406, 300, 264, 9284, 1177, 380], "temperature": 0.0, "avg_logprob": -0.10528586538214432, "compression_ratio": 1.9597989949748744, "no_speech_prob": 6.61028825561516e-05}, {"id": 592, "seek": 461666, "start": 4616.66, "end": 4625.78, "text": " work so precisely what you find um is that you know antennas generally occur in very similar topics", "tokens": [589, 370, 13402, 437, 291, 915, 1105, 307, 300, 291, 458, 18858, 18979, 5101, 5160, 294, 588, 2531, 8378], "temperature": 0.0, "avg_logprob": -0.10862252861261368, "compression_ratio": 1.75, "no_speech_prob": 0.00014838842616882175}, {"id": 593, "seek": 461666, "start": 4625.78, "end": 4632.34, "text": " because you know whether it's um saying you know John is really tall or John is really short", "tokens": [570, 291, 458, 1968, 309, 311, 1105, 1566, 291, 458, 2619, 307, 534, 6764, 420, 2619, 307, 534, 2099], "temperature": 0.0, "avg_logprob": -0.10862252861261368, "compression_ratio": 1.75, "no_speech_prob": 0.00014838842616882175}, {"id": 594, "seek": 461666, "start": 4632.34, "end": 4639.94, "text": " or that movie was fantastic or that movie was terrible right you get antennas occurring in the", "tokens": [420, 300, 3169, 390, 5456, 420, 300, 3169, 390, 6237, 558, 291, 483, 18858, 18979, 18386, 294, 264], "temperature": 0.0, "avg_logprob": -0.10862252861261368, "compression_ratio": 1.75, "no_speech_prob": 0.00014838842616882175}, {"id": 595, "seek": 463994, "start": 4639.94, "end": 4646.58, "text": " same context so because of that their vectors are very similar and similarly for sort of affect and", "tokens": [912, 4319, 370, 570, 295, 300, 641, 18875, 366, 588, 2531, 293, 14138, 337, 1333, 295, 3345, 293], "temperature": 0.0, "avg_logprob": -0.12614264253710136, "compression_ratio": 1.6628571428571428, "no_speech_prob": 2.0429453797987662e-05}, {"id": 596, "seek": 463994, "start": 4646.58, "end": 4654.5, "text": " sentiment-based words well like make um great and terrible example their contexts are similar um", "tokens": [16149, 12, 6032, 2283, 731, 411, 652, 1105, 869, 293, 6237, 1365, 641, 30628, 366, 2531, 1105], "temperature": 0.0, "avg_logprob": -0.12614264253710136, "compression_ratio": 1.6628571428571428, "no_speech_prob": 2.0429453797987662e-05}, {"id": 597, "seek": 463994, "start": 4654.5, "end": 4662.98, "text": " therefore um that if you're just learning this kind of predict words in context models um that", "tokens": [4412, 1105, 300, 498, 291, 434, 445, 2539, 341, 733, 295, 6069, 2283, 294, 4319, 5245, 1105, 300], "temperature": 0.0, "avg_logprob": -0.12614264253710136, "compression_ratio": 1.6628571428571428, "no_speech_prob": 2.0429453797987662e-05}, {"id": 598, "seek": 466298, "start": 4662.98, "end": 4669.94, "text": " no that's not captured now that's not the end of the story I mean you know absolutely people wanted", "tokens": [572, 300, 311, 406, 11828, 586, 300, 311, 406, 264, 917, 295, 264, 1657, 286, 914, 291, 458, 3122, 561, 1415], "temperature": 0.0, "avg_logprob": -0.08987772464752197, "compression_ratio": 1.7168141592920354, "no_speech_prob": 3.417746120248921e-05}, {"id": 599, "seek": 466298, "start": 4669.94, "end": 4676.66, "text": " to use neural networks for sentiment and other kinds of sort of connotation affect and there are", "tokens": [281, 764, 18161, 9590, 337, 16149, 293, 661, 3685, 295, 1333, 295, 46371, 399, 3345, 293, 456, 366], "temperature": 0.0, "avg_logprob": -0.08987772464752197, "compression_ratio": 1.7168141592920354, "no_speech_prob": 3.417746120248921e-05}, {"id": 600, "seek": 466298, "start": 4676.66, "end": 4682.98, "text": " very good ways of doing that but somehow you have to do something more than simply predicting words", "tokens": [588, 665, 2098, 295, 884, 300, 457, 6063, 291, 362, 281, 360, 746, 544, 813, 2935, 32884, 2283], "temperature": 0.0, "avg_logprob": -0.08987772464752197, "compression_ratio": 1.7168141592920354, "no_speech_prob": 3.417746120248921e-05}, {"id": 601, "seek": 466298, "start": 4682.98, "end": 4689.459999999999, "text": " in context because that's not sufficient to um capture that dimension um more on that later", "tokens": [294, 4319, 570, 300, 311, 406, 11563, 281, 1105, 7983, 300, 10139, 1105, 544, 322, 300, 1780], "temperature": 0.0, "avg_logprob": -0.08987772464752197, "compression_ratio": 1.7168141592920354, "no_speech_prob": 3.417746120248921e-05}, {"id": 602, "seek": 468946, "start": 4689.46, "end": 4696.34, "text": " but this happened to like adjectives too like very basic adjectives like so and like not", "tokens": [457, 341, 2011, 281, 411, 29378, 1539, 886, 411, 588, 3875, 29378, 1539, 411, 370, 293, 411, 406], "temperature": 0.0, "avg_logprob": -0.18364465527418183, "compression_ratio": 1.8195121951219513, "no_speech_prob": 0.00013086356921121478}, {"id": 603, "seek": 468946, "start": 4697.06, "end": 4703.06, "text": " because those would like appear like some like context right what was your first example before not", "tokens": [570, 729, 576, 411, 4204, 411, 512, 411, 4319, 558, 437, 390, 428, 700, 1365, 949, 406], "temperature": 0.0, "avg_logprob": -0.18364465527418183, "compression_ratio": 1.8195121951219513, "no_speech_prob": 0.00013086356921121478}, {"id": 604, "seek": 468946, "start": 4703.86, "end": 4710.42, "text": " like so this is so cool yeah so that's actually a good question as well so yeah so there are", "tokens": [411, 370, 341, 307, 370, 1627, 1338, 370, 300, 311, 767, 257, 665, 1168, 382, 731, 370, 1338, 370, 456, 366], "temperature": 0.0, "avg_logprob": -0.18364465527418183, "compression_ratio": 1.8195121951219513, "no_speech_prob": 0.00013086356921121478}, {"id": 605, "seek": 468946, "start": 4710.42, "end": 4715.46, "text": " these very common words there are commonly referred to as function words by linguists which", "tokens": [613, 588, 2689, 2283, 456, 366, 12719, 10839, 281, 382, 2445, 2283, 538, 21766, 1751, 597], "temperature": 0.0, "avg_logprob": -0.18364465527418183, "compression_ratio": 1.8195121951219513, "no_speech_prob": 0.00013086356921121478}, {"id": 606, "seek": 471546, "start": 4715.46, "end": 4724.1, "text": " know includes ones like um so and not but other ones like and and prepositions like you know", "tokens": [458, 5974, 2306, 411, 1105, 370, 293, 406, 457, 661, 2306, 411, 293, 293, 2666, 329, 2451, 411, 291, 458], "temperature": 0.0, "avg_logprob": -0.09207618058617435, "compression_ratio": 1.6235955056179776, "no_speech_prob": 4.1191335185430944e-05}, {"id": 607, "seek": 471546, "start": 4724.1, "end": 4732.82, "text": " two and on um you sort of might suspect that the word vectors for those don't work out very well", "tokens": [732, 293, 322, 1105, 291, 1333, 295, 1062, 9091, 300, 264, 1349, 18875, 337, 729, 500, 380, 589, 484, 588, 731], "temperature": 0.0, "avg_logprob": -0.09207618058617435, "compression_ratio": 1.6235955056179776, "no_speech_prob": 4.1191335185430944e-05}, {"id": 608, "seek": 471546, "start": 4732.82, "end": 4739.3, "text": " because they occur in all kinds of different contexts and they're not very distinct from each other", "tokens": [570, 436, 5160, 294, 439, 3685, 295, 819, 30628, 293, 436, 434, 406, 588, 10644, 490, 1184, 661], "temperature": 0.0, "avg_logprob": -0.09207618058617435, "compression_ratio": 1.6235955056179776, "no_speech_prob": 4.1191335185430944e-05}, {"id": 609, "seek": 473930, "start": 4739.3, "end": 4745.38, "text": " in many cases and to a first approximation I think that's true and part of why I didn't use those as", "tokens": [294, 867, 3331, 293, 281, 257, 700, 28023, 286, 519, 300, 311, 2074, 293, 644, 295, 983, 286, 994, 380, 764, 729, 382], "temperature": 0.0, "avg_logprob": -0.056927300514058865, "compression_ratio": 1.6596638655462186, "no_speech_prob": 4.388419256429188e-05}, {"id": 610, "seek": 473930, "start": 4745.38, "end": 4754.1, "text": " examples in my slides yeah but you know at the end of the day we do build up vector representations", "tokens": [5110, 294, 452, 9788, 1338, 457, 291, 458, 412, 264, 917, 295, 264, 786, 321, 360, 1322, 493, 8062, 33358], "temperature": 0.0, "avg_logprob": -0.056927300514058865, "compression_ratio": 1.6596638655462186, "no_speech_prob": 4.388419256429188e-05}, {"id": 611, "seek": 473930, "start": 4754.1, "end": 4760.9800000000005, "text": " of those words too and you'll see in a few um lectures time when we start building what we call", "tokens": [295, 729, 2283, 886, 293, 291, 603, 536, 294, 257, 1326, 1105, 16564, 565, 562, 321, 722, 2390, 437, 321, 818], "temperature": 0.0, "avg_logprob": -0.056927300514058865, "compression_ratio": 1.6596638655462186, "no_speech_prob": 4.388419256429188e-05}, {"id": 612, "seek": 473930, "start": 4760.9800000000005, "end": 4766.74, "text": " language models that actually they do do a great job in those words as well I mean to explain what", "tokens": [2856, 5245, 300, 767, 436, 360, 360, 257, 869, 1691, 294, 729, 2283, 382, 731, 286, 914, 281, 2903, 437], "temperature": 0.0, "avg_logprob": -0.056927300514058865, "compression_ratio": 1.6596638655462186, "no_speech_prob": 4.388419256429188e-05}, {"id": 613, "seek": 476674, "start": 4766.74, "end": 4775.38, "text": " I'm meaning there I mean you know another feature of the word to vector model is it actually", "tokens": [286, 478, 3620, 456, 286, 914, 291, 458, 1071, 4111, 295, 264, 1349, 281, 8062, 2316, 307, 309, 767], "temperature": 0.0, "avg_logprob": -0.12123685472466972, "compression_ratio": 1.786046511627907, "no_speech_prob": 9.722011600388214e-05}, {"id": 614, "seek": 476674, "start": 4775.38, "end": 4782.58, "text": " ignore the position of words right so it's said I'm going to predict every word around the center", "tokens": [11200, 264, 2535, 295, 2283, 558, 370, 309, 311, 848, 286, 478, 516, 281, 6069, 633, 1349, 926, 264, 3056], "temperature": 0.0, "avg_logprob": -0.12123685472466972, "compression_ratio": 1.786046511627907, "no_speech_prob": 9.722011600388214e-05}, {"id": 615, "seek": 476674, "start": 4782.58, "end": 4788.98, "text": " word but you know I'm predicting it in the same way I'm not predicting differently the word before", "tokens": [1349, 457, 291, 458, 286, 478, 32884, 309, 294, 264, 912, 636, 286, 478, 406, 32884, 7614, 264, 1349, 949], "temperature": 0.0, "avg_logprob": -0.12123685472466972, "compression_ratio": 1.786046511627907, "no_speech_prob": 9.722011600388214e-05}, {"id": 616, "seek": 476674, "start": 4788.98, "end": 4794.9, "text": " me or versus the word after me or the word two away in either direction right they're all just", "tokens": [385, 420, 5717, 264, 1349, 934, 385, 420, 264, 1349, 732, 1314, 294, 2139, 3513, 558, 436, 434, 439, 445], "temperature": 0.0, "avg_logprob": -0.12123685472466972, "compression_ratio": 1.786046511627907, "no_speech_prob": 9.722011600388214e-05}, {"id": 617, "seek": 479490, "start": 4794.9, "end": 4802.179999999999, "text": " predicted the same by that one um probability function and so if that's all you've got that sort", "tokens": [19147, 264, 912, 538, 300, 472, 1105, 8482, 2445, 293, 370, 498, 300, 311, 439, 291, 600, 658, 300, 1333], "temperature": 0.0, "avg_logprob": -0.10922158041665721, "compression_ratio": 1.6741071428571428, "no_speech_prob": 3.9899692637845874e-05}, {"id": 618, "seek": 479490, "start": 4802.179999999999, "end": 4809.86, "text": " of destroys your ability to do a good job at um capturing these sort of common more grammatical", "tokens": [295, 36714, 428, 3485, 281, 360, 257, 665, 1691, 412, 1105, 23384, 613, 1333, 295, 2689, 544, 17570, 267, 804], "temperature": 0.0, "avg_logprob": -0.10922158041665721, "compression_ratio": 1.6741071428571428, "no_speech_prob": 3.9899692637845874e-05}, {"id": 619, "seek": 479490, "start": 4809.86, "end": 4816.259999999999, "text": " words like so not an an um but we build slightly different models that are more sensitive to the", "tokens": [2283, 411, 370, 406, 364, 364, 1105, 457, 321, 1322, 4748, 819, 5245, 300, 366, 544, 9477, 281, 264], "temperature": 0.0, "avg_logprob": -0.10922158041665721, "compression_ratio": 1.6741071428571428, "no_speech_prob": 3.9899692637845874e-05}, {"id": 620, "seek": 479490, "start": 4816.259999999999, "end": 4821.94, "text": " structure of sentences and then we start doing a good job on those two okay thank you", "tokens": [3877, 295, 16579, 293, 550, 321, 722, 884, 257, 665, 1691, 322, 729, 732, 1392, 1309, 291], "temperature": 0.0, "avg_logprob": -0.10922158041665721, "compression_ratio": 1.6741071428571428, "no_speech_prob": 3.9899692637845874e-05}, {"id": 621, "seek": 482194, "start": 4821.94, "end": 4831.54, "text": " I had a question about um the characterization of word to vector um because I read all these", "tokens": [286, 632, 257, 1168, 466, 1105, 264, 49246, 295, 1349, 281, 8062, 1105, 570, 286, 1401, 439, 613], "temperature": 0.0, "avg_logprob": -0.6622384699379525, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.0003218339988961816}, {"id": 622, "seek": 482194, "start": 4831.54, "end": 4837.0599999999995, "text": " such and it seems to characterize architecture as well but skip very model which was slightly", "tokens": [1270, 293, 309, 2544, 281, 38463, 9482, 382, 731, 457, 10023, 588, 2316, 597, 390, 4748], "temperature": 0.0, "avg_logprob": -0.6622384699379525, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.0003218339988961816}, {"id": 623, "seek": 482194, "start": 4837.0599999999995, "end": 4841.379999999999, "text": " different from how I was presented in the module so at least like two hundred and three hundred", "tokens": [819, 490, 577, 286, 390, 8212, 294, 264, 10088, 370, 412, 1935, 411, 732, 3262, 293, 1045, 3262], "temperature": 0.0, "avg_logprob": -0.6622384699379525, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.0003218339988961816}, {"id": 624, "seek": 482194, "start": 4841.379999999999, "end": 4850.419999999999, "text": " years ago or yeah so I've I've still gotten more to say so I'm stationed first day", "tokens": [924, 2057, 420, 1338, 370, 286, 600, 286, 600, 920, 5768, 544, 281, 584, 370, 286, 478, 46228, 700, 786], "temperature": 0.0, "avg_logprob": -0.6622384699379525, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.0003218339988961816}, {"id": 625, "seek": 485042, "start": 4850.42, "end": 4859.9400000000005, "text": " um for more stuff on word vectors um you know so word to vac is kind of a framework for building", "tokens": [1105, 337, 544, 1507, 322, 1349, 18875, 1105, 291, 458, 370, 1349, 281, 2842, 307, 733, 295, 257, 8388, 337, 2390], "temperature": 0.0, "avg_logprob": -0.08783872543819367, "compression_ratio": 1.7289156626506024, "no_speech_prob": 3.163200744893402e-05}, {"id": 626, "seek": 485042, "start": 4859.9400000000005, "end": 4867.46, "text": " word vectors and that there are sort of several variant precise algorithms within the framework", "tokens": [1349, 18875, 293, 300, 456, 366, 1333, 295, 2940, 17501, 13600, 14642, 1951, 264, 8388], "temperature": 0.0, "avg_logprob": -0.08783872543819367, "compression_ratio": 1.7289156626506024, "no_speech_prob": 3.163200744893402e-05}, {"id": 627, "seek": 485042, "start": 4867.46, "end": 4874.42, "text": " and you know one of them is have whether you're predicting the context words or whether you're", "tokens": [293, 291, 458, 472, 295, 552, 307, 362, 1968, 291, 434, 32884, 264, 4319, 2283, 420, 1968, 291, 434], "temperature": 0.0, "avg_logprob": -0.08783872543819367, "compression_ratio": 1.7289156626506024, "no_speech_prob": 3.163200744893402e-05}, {"id": 628, "seek": 487442, "start": 4874.42, "end": 4882.18, "text": " predicting the center word um so the model I showed was predicting the context words so it was", "tokens": [32884, 264, 3056, 1349, 1105, 370, 264, 2316, 286, 4712, 390, 32884, 264, 4319, 2283, 370, 309, 390], "temperature": 0.0, "avg_logprob": -0.1259990460944898, "compression_ratio": 1.7218934911242603, "no_speech_prob": 2.5374249162268825e-05}, {"id": 629, "seek": 487442, "start": 4882.18, "end": 4892.18, "text": " the skip gram model but then there's sort of a detail of how in particular do you do the optimization", "tokens": [264, 10023, 21353, 2316, 457, 550, 456, 311, 1333, 295, 257, 2607, 295, 577, 294, 1729, 360, 291, 360, 264, 19618], "temperature": 0.0, "avg_logprob": -0.1259990460944898, "compression_ratio": 1.7218934911242603, "no_speech_prob": 2.5374249162268825e-05}, {"id": 630, "seek": 487442, "start": 4892.18, "end": 4900.42, "text": " and what I presented was the sort of easiest way to do it which is naive optimization with the", "tokens": [293, 437, 286, 8212, 390, 264, 1333, 295, 12889, 636, 281, 360, 309, 597, 307, 29052, 19618, 365, 264], "temperature": 0.0, "avg_logprob": -0.1259990460944898, "compression_ratio": 1.7218934911242603, "no_speech_prob": 2.5374249162268825e-05}, {"id": 631, "seek": 490042, "start": 4900.42, "end": 4909.06, "text": " equation the softmax equation for word vectors um it turns out that that naive optimization is sort of", "tokens": [5367, 264, 2787, 41167, 5367, 337, 1349, 18875, 1105, 309, 4523, 484, 300, 300, 29052, 19618, 307, 1333, 295], "temperature": 0.0, "avg_logprob": -0.16183221062948538, "compression_ratio": 1.7256637168141593, "no_speech_prob": 2.499281254131347e-05}, {"id": 632, "seek": 490042, "start": 4909.06, "end": 4917.7, "text": " exneedlessly expensive and people have come up with um faster ways of doing it in particular", "tokens": [454, 716, 292, 12048, 5124, 293, 561, 362, 808, 493, 365, 1105, 4663, 2098, 295, 884, 309, 294, 1729], "temperature": 0.0, "avg_logprob": -0.16183221062948538, "compression_ratio": 1.7256637168141593, "no_speech_prob": 2.499281254131347e-05}, {"id": 633, "seek": 490042, "start": 4917.7, "end": 4922.66, "text": " um the commonest thing you see is what's called skip gram with negative sound playing and the", "tokens": [1105, 264, 2689, 377, 551, 291, 536, 307, 437, 311, 1219, 10023, 21353, 365, 3671, 1626, 2433, 293, 264], "temperature": 0.0, "avg_logprob": -0.16183221062948538, "compression_ratio": 1.7256637168141593, "no_speech_prob": 2.499281254131347e-05}, {"id": 634, "seek": 490042, "start": 4922.66, "end": 4928.26, "text": " negative sound playing is then sort of a much more efficient way to estimate things and I'll mention", "tokens": [3671, 1626, 2433, 307, 550, 1333, 295, 257, 709, 544, 7148, 636, 281, 12539, 721, 293, 286, 603, 2152], "temperature": 0.0, "avg_logprob": -0.16183221062948538, "compression_ratio": 1.7256637168141593, "no_speech_prob": 2.499281254131347e-05}, {"id": 635, "seek": 492826, "start": 4928.26, "end": 4937.38, "text": " that on Thursday. Right okay thank you. Who's asking for more information about how word vectors", "tokens": [300, 322, 10383, 13, 1779, 1392, 1309, 291, 13, 2102, 311, 3365, 337, 544, 1589, 466, 577, 1349, 18875], "temperature": 0.0, "avg_logprob": -0.2600852648417155, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.00011754534352803603}, {"id": 636, "seek": 492826, "start": 4937.38, "end": 4944.42, "text": " are constructed uh beyond the summary of random initialization and then gradient based", "tokens": [366, 17083, 2232, 4399, 264, 12691, 295, 4974, 5883, 2144, 293, 550, 16235, 2361], "temperature": 0.0, "avg_logprob": -0.2600852648417155, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.00011754534352803603}, {"id": 637, "seek": 492826, "start": 4944.42, "end": 4951.62, "text": " uh iterative operator optimization. Yeah um so I sort of will do a bit more connecting this", "tokens": [2232, 17138, 1166, 12973, 19618, 13, 865, 1105, 370, 286, 1333, 295, 486, 360, 257, 857, 544, 11015, 341], "temperature": 0.0, "avg_logprob": -0.2600852648417155, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.00011754534352803603}, {"id": 638, "seek": 492826, "start": 4951.62, "end": 4957.46, "text": " together um in the Thursday lecture I guess there's sort of I mean so much I'm gonna fit in the", "tokens": [1214, 1105, 294, 264, 10383, 7991, 286, 2041, 456, 311, 1333, 295, 286, 914, 370, 709, 286, 478, 799, 3318, 294, 264], "temperature": 0.0, "avg_logprob": -0.2600852648417155, "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.00011754534352803603}, {"id": 639, "seek": 495746, "start": 4957.46, "end": 4964.82, "text": " first class um but the picture the picture is essentially the picture I showed the pieces of", "tokens": [700, 1508, 1105, 457, 264, 3036, 264, 3036, 307, 4476, 264, 3036, 286, 4712, 264, 3755, 295], "temperature": 0.0, "avg_logprob": -0.08246081875216577, "compression_ratio": 1.6766467065868262, "no_speech_prob": 2.9729309972026385e-05}, {"id": 640, "seek": 495746, "start": 4964.82, "end": 4977.3, "text": " so to learn word vectors you start off by having a vector for each word type both for context and", "tokens": [370, 281, 1466, 1349, 18875, 291, 722, 766, 538, 1419, 257, 8062, 337, 1184, 1349, 2010, 1293, 337, 4319, 293], "temperature": 0.0, "avg_logprob": -0.08246081875216577, "compression_ratio": 1.6766467065868262, "no_speech_prob": 2.9729309972026385e-05}, {"id": 641, "seek": 495746, "start": 4977.3, "end": 4987.3, "text": " outside and those vectors you to initialize randomly um so that you just put small little", "tokens": [2380, 293, 729, 18875, 291, 281, 5883, 1125, 16979, 1105, 370, 300, 291, 445, 829, 1359, 707], "temperature": 0.0, "avg_logprob": -0.08246081875216577, "compression_ratio": 1.6766467065868262, "no_speech_prob": 2.9729309972026385e-05}, {"id": 642, "seek": 498730, "start": 4987.3, "end": 4993.46, "text": " numbers that are randomly generated in each vector component and that's just your starting point", "tokens": [3547, 300, 366, 16979, 10833, 294, 1184, 8062, 6542, 293, 300, 311, 445, 428, 2891, 935], "temperature": 0.0, "avg_logprob": -0.04170954648186179, "compression_ratio": 1.7252252252252251, "no_speech_prob": 2.4662263967911713e-05}, {"id": 643, "seek": 498730, "start": 4993.46, "end": 5000.34, "text": " and so from there on you're using an iterative algorithm where you're progressively updating", "tokens": [293, 370, 490, 456, 322, 291, 434, 1228, 364, 17138, 1166, 9284, 689, 291, 434, 46667, 25113], "temperature": 0.0, "avg_logprob": -0.04170954648186179, "compression_ratio": 1.7252252252252251, "no_speech_prob": 2.4662263967911713e-05}, {"id": 644, "seek": 498730, "start": 5000.34, "end": 5007.14, "text": " those word vectors so they do a better job at predicting which words appear in the context of", "tokens": [729, 1349, 18875, 370, 436, 360, 257, 1101, 1691, 412, 32884, 597, 2283, 4204, 294, 264, 4319, 295], "temperature": 0.0, "avg_logprob": -0.04170954648186179, "compression_ratio": 1.7252252252252251, "no_speech_prob": 2.4662263967911713e-05}, {"id": 645, "seek": 498730, "start": 5007.14, "end": 5016.820000000001, "text": " other words and the way that we're going to do that is by using um the gradients that I was sort of", "tokens": [661, 2283, 293, 264, 636, 300, 321, 434, 516, 281, 360, 300, 307, 538, 1228, 1105, 264, 2771, 2448, 300, 286, 390, 1333, 295], "temperature": 0.0, "avg_logprob": -0.04170954648186179, "compression_ratio": 1.7252252252252251, "no_speech_prob": 2.4662263967911713e-05}, {"id": 646, "seek": 501682, "start": 5016.82, "end": 5022.66, "text": " starting to show how to calculate and then you know once you have a gradient you can walk in the", "tokens": [2891, 281, 855, 577, 281, 8873, 293, 550, 291, 458, 1564, 291, 362, 257, 16235, 291, 393, 1792, 294, 264], "temperature": 0.0, "avg_logprob": -0.07626280784606934, "compression_ratio": 1.7918552036199096, "no_speech_prob": 3.477768768789247e-05}, {"id": 647, "seek": 501682, "start": 5022.66, "end": 5029.7, "text": " opposite direction of the gradient and you're then walking downhill I you're minimizing your loss", "tokens": [6182, 3513, 295, 264, 16235, 293, 291, 434, 550, 4494, 29929, 286, 291, 434, 46608, 428, 4470], "temperature": 0.0, "avg_logprob": -0.07626280784606934, "compression_ratio": 1.7918552036199096, "no_speech_prob": 3.477768768789247e-05}, {"id": 648, "seek": 501682, "start": 5029.7, "end": 5036.58, "text": " and we're going to sort of do lots of that until our word vectors get as good as possible so you know", "tokens": [293, 321, 434, 516, 281, 1333, 295, 360, 3195, 295, 300, 1826, 527, 1349, 18875, 483, 382, 665, 382, 1944, 370, 291, 458], "temperature": 0.0, "avg_logprob": -0.07626280784606934, "compression_ratio": 1.7918552036199096, "no_speech_prob": 3.477768768789247e-05}, {"id": 649, "seek": 501682, "start": 5037.219999999999, "end": 5045.46, "text": " um it's really all math but in some sense you know word vector learning is sort of miraculous since", "tokens": [1105, 309, 311, 534, 439, 5221, 457, 294, 512, 2020, 291, 458, 1349, 8062, 2539, 307, 1333, 295, 41101, 1670], "temperature": 0.0, "avg_logprob": -0.07626280784606934, "compression_ratio": 1.7918552036199096, "no_speech_prob": 3.477768768789247e-05}, {"id": 650, "seek": 504546, "start": 5045.46, "end": 5054.18, "text": " you do literally just start off with completely random word vectors and run this algorithm of", "tokens": [291, 360, 3736, 445, 722, 766, 365, 2584, 4974, 1349, 18875, 293, 1190, 341, 9284, 295], "temperature": 0.0, "avg_logprob": -0.061060129067836665, "compression_ratio": 1.5234375, "no_speech_prob": 5.3367521104519255e-06}, {"id": 651, "seek": 504546, "start": 5054.18, "end": 5061.3, "text": " predicting words for a long time and out of nothing emerges these word vectors that represent meaning", "tokens": [32884, 2283, 337, 257, 938, 565, 293, 484, 295, 1825, 38965, 613, 1349, 18875, 300, 2906, 3620], "temperature": 0.0, "avg_logprob": -0.061060129067836665, "compression_ratio": 1.5234375, "no_speech_prob": 5.3367521104519255e-06}, {"id": 652, "seek": 506130, "start": 5061.3, "end": 5077.62, "text": " well", "tokens": [50364, 731, 51180], "temperature": 1.0, "avg_logprob": -1.4022222757339478, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.00024047927581705153}], "language": "en"}