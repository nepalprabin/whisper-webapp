{"text": " So what are we going to do for today? So the main content for today is to go through sort of more stuff about word vectors, including touching on word sensors and then introducing the notion of neural network classifiers. So our biggest goal is that by the end of today's class, you should feel like you could confidently look at one of the word embeddings papers, such as the Google word to vac paper or the glove paper or Sanji Rao's paper that will come to later and feel like, yeah, I can understand this. I know what they're doing and it makes sense. So let's go back to where we were. So this was sort of introducing this model of word to vac and the idea was that we started with random word vectors and then we're going to sort of, we have a big corpus of text and we're going to iterate through each word in the whole corpus. And for each position, we're going to try and predict what words surround this our center word. And we're going to do that with a probability distribution that's defined in terms of the dot product between the word vectors for the center word and the context words. And so that will give a probability estimate of a word appearing in the context of into. Well, actual words did occur in the context of into on this occasion. So what we're going to want to do is sort of make it more likely that turning problems banking and crises will turn up in the context of into. And so that's learning updating the word vectors so that they can predict actual surrounding words better. And then the thing is almost magical is that doing no more than this simple algorithm, this allows us to learn word vectors that capture well words similarity and meaningful directions in a word space. So more precisely right for this model, the only parameters of this model are the word vectors. So we have outside word vectors and center word vectors for each word. And then we're going to get a probability, well, we get taking a dot product to get a score of how likely a particular outside word is to occur with the center word. And then we're using the soft max transformation to convert those scores into probabilities as I discussed last time. And I kind of come back to at the end this time. And then I want to start with the next thing to note, this model is what we call an NLP a bag of words model. So bag of words models models that don't actually pay any attention to word order or position. It doesn't matter if you're next to the center word or bit further away on the left or right, the probability estimate would be the same. And that seems like a very crude model of language that will offend any linguist and it is a very crude model of language and we'll move on to better models of languages we go on. But even that crude model of language is enough to learn quite a lot of the probability, sorry, quite a lot about the properties of words. And then the second note is, well, with this model, we wanted to give reasonably high probabilities to the words that do occur in the context of the center word, at least if they do so at all often. Obviously, lots of different words can occur. So we're not talking about probabilities like point three and point five, we're more likely going to be talking about probabilities like point one and numbers like that. Well, how do we achieve that? And well, the way that the word to fact model achieves this, and this is the learning phase of the model is to place words that are similar in meaning close to each other in this high dimensional vector space. So again, you can't read this one, but if we scroll into this one, we see lots of words that are similar in meaning group close together in the space. So here are days of the week like Tuesday Thursday Sunday and also Christmas. So what else do we have? We have Samsung and Nokia. This is a diagram I made quite a few years ago. So that's when Nokia was still an important maker of cell phones. We have various sort of fields like mathematics and economics over here. So we have two words that are similar in meaning. Actually, one more note I wanted to make on this. I mean, again, this is a two dimensional picture, which is all I can show you on a slide. And it's done with the principal components projection that you're also using the assignment. So something important to remember about hard to remember is that high dimensional spaces have very different properties to the two dimensional spaces that we can look at. And so in particular, a word, a vector can be close to many other things in a high dimensional space, but close to them on different dimensions. So I've mentioned doing learning. So the next question is, well, how do we learn good word vectors? And this was the bit that I didn't quite hook up at the end of last class. So for a while in the last, I said, oh, gee, and we have to work out the gradient of the loss function with respect to the parameters that will allow us to make progress. But I didn't sort of altogether put that together. So what we're going to do is we start off with random word vectors. And we're going to visualize them to small numbers near zero in each dimension. We've defined our loss function J, which we looked at last time. And then we're going to use a gradient descent algorithm, which is an iterative algorithm that learns to maximize J of theta by changing theta. The idea of this algorithm is that from the current values of theta, you calculate the gradient J of theta. And then what you're going to do is make a small step in the direction of the negative gradient. So the gradient is pointing upwards. And we're taking a small step in the direction of the negative of the gradient to gradually move down towards the minimum. And so one of the parameters of neural nets that you can fiddle in your software package is what is the step size. So if you take a really, really, it's a bit see step. It might take you a long time to minimize the function. You do a lot of wasted computation. On the other hand, if your step size is much too big, well, then you can actually diverge and start going to worse places. Or even if you are going down hill a little bit that what's going to happen is you're then going to end up bouncing back and forth. And it'll take you much longer to get to the minimum. And this picture, I have a beautiful quadratic. And it's easy to minimize it. Something that you might know about neural networks is then in general, then not convex. So you could think that this is just all going to go awry. But the truth is, and practice life works out to be OK. So I think I won't get into that more right now and come back to that in the later class. So this is our gradient descent. So we have the current values of the parameters theta. And then walk a little bit in the negative direction of the gradient using our learning rate or step size alpha. And that gives us new parameter values where that means that you know these are vectors, but for each individual parameter, we're updating it a little bit by working out the partial derivative of J with respect to that parameter. So that's the simple gradient descent algorithm. Nobody uses it and you shouldn't use it. The problem is that our J is a function of all windows in the corpus. Remember, we're doing this sum over every center word in the entire corpus. And we'll often have billions of words in the corpus. So actually working out J of theta or the gradient of J of theta would be extremely extremely expensive because we have to iterate over our entire corpus. So you'd wait a very long time before you made a single gradient update. And so optimization be extremely slow. And so basically 100% of the time in your network land, we don't use gradient descent. We instead use what's called stochastic gradient descent. And stochastic gradient descent is a very simple modification of this. So when working out an estimate of the gradient based on the entire corpus, you simply take one center word or a small batch like 32 center words, and you work out an estimate of the gradient based on them. And that estimate of the gradient will be noisy and bad because you've only looked at a small fraction of the corpus rather than the whole corpus. But nevertheless, you can use that estimate of the gradient to update your theta parameters in exactly the same way. So this is the algorithm that we can do. And so then if we have a billion word corpus, we can if we do it on each center word, we can make a billion updates to the parameters we pass through the corpus once rather than only making one more accurate update to the parameters. And at once you've been through the corpus. So overall, we can learn several orders of magnitude more quickly. And so this is the algorithm that you'll be using everywhere, including, you know, right right from the beginning from our assignments. And then just an extra comment of more complicated stuff will come back to, I, I, this is the gradient descent is sort of performance hack it lets you learn much more quickly. But it's not only a performance hack, you're going to have some quite counter intuitive properties and actually the fact that stochastic gradient descent is kind of noisy and bounces around as it does its thing. And it actually means that in complex networks, it learns better solutions than if you were to run playing gradient descent very slowly. So you can both compute much more quickly and do a better job. And then we have a final note on running stochastic gradients with word vectors. This is kind of an aside. But something to note is that if we're doing a stochastic gradient update based on one window, then actually in that window will have seen almost none of our parameters. Because if we have a window of something like five words to be the side of the center word, we've seen at most 11 distinct word types. So we will have gradient information for those 11 words, but the other 100,000 odd words now vocabulary will have no gradient update information. So it will be a very, very sparse gradient update. So if you're only thinking math, you can just have your entire gradient and use the equation that I showed before. But if you're thinking systems optimization, then you'd want to think, well, actually, I only want to update the parameters for a few words and there have to be and there are much more efficient ways that I could do that. So here's sort, this is another aside will be useful for assignment. So I will say it up until now when I presented word vectors, I presented them as column vectors. And that makes the most sense if you think about it as a piece of math, whereas actually in all common deep learning packages, including PyTorch that we're using word vectors are actually represented as row vectors. And if you remember back to the representation of matrices and CS107 or something like that, that you'll know that that's then obviously efficient for representing words, because then you can access an entire word vector as a continuous range of memory. So actually, I'm going to throw in a few and four train. Anyway, so actually our word vectors will be row vectors when you look at those inside PyTorch. Okay, now I wanted to say a bit more about the word to veck algorithm family and also what you're going to do in homework too. If you're still meant to be working on homework one, which remembers to next Tuesday, that really actually with today's content, we're starting into homework too. And I'll kind of go through the first part of homework to today and the other stuff you need to know for homework to. I mentioned briefly the idea that we have two separate vectors for each word type, the center vector and the outside vectors, and we just average them both at the end, they're similar, but not identical for multiple reasons, including the random initialization and the stochastic gradient descent. You can implement a word to veck algorithm with just one vector per word, and actually if you do work slightly better, but it makes the algorithm much more complicated. The reason for that is sometimes you'll have the same word type as the center word and the context word, and that means that when you're doing your calculus at that point, you've then got this sort of messy case that just for that word, you're getting an x squared turn, sorry, or dot product, you're getting a dot product of x dot x term, which makes it sort of much messier to work out. And we use this sort of simple optimization of having two vectors per word. Okay, so for the word to veck model as introduced in the meek of it, our paper in 2013, it wasn't really just one algorithm, it was a family of algorithms. There were two basic model variants, one was called the skip gram model, which is the one that I've explained to you that. The other one was called the continuous bag of words model, C bow, and in this one, you predict the center word from a bag of context words. The skip gram one is more natural in various ways, so it's sort of normally the one that people have gravitated to and subsequent work. But then as to how you train this model, what I've presented so far is the naive softmax equation, which is a simple but relatively expensive training method. So that isn't really what they suggest using in your paper in the paper, they suggest using a method that's called negative sampling. So an acronym you'll see sometimes is SGNS, which means skip grams negative sampling. So let me just say a little bit about what this is, but actually doing the skip gram model with negative sampling is the part of homework too. So you'll get to know this model well. So the point is that if you use this naive softmax, you know, even though people commonly do use this naive softmax in various neural net models, that working out the denominator is pretty expensive. And that's because you have to iterate over every word in the vocabulary and workout these dot products. So if you have a hundred thousand word vocabulary, you have to do a hundred thousand dot products to work out the denominator and that seems a little bit of a shame. And so instead of that, the idea of negative sampling is we're instead of using this softmax, we're going to train binary logistic regression models for both the true pair of center word. And the context word versus noise pairs where we keep the true center word and we just randomly sample words from the vocabulary. And as presented in the paper, the idea is like this. So overall, what we want to optimize is still an average of the loss for each particular center word. But for when we're working out the loss for each particular center word, we're going to work out. So the loss for each particular center word and each particular window, we're going to take the dot product as before of the center word and the outside word. And that's sort of the main quantity. But now instead of using that inside the softmax, we're going to put it through the logistic function, which is sometimes also often also called the sigmoid function, the name logistic is more precise. So that's this function here. So the logistic function is a handy function that will map any real number to a probability between zero and one open interval. So basically if the dot product is large, the logistic of the dot product will be virtually one. Okay, so we want this to be large. And then what we'd like is on average, we'd like the dot product between the center word and words that we just chose randomly, I, they most likely didn't actually occur in the context of the center word to be small. And there's just one little trick of how this is done, which is this sigmoid function is symmetric. And so if we want this probability to be small, we can take the negative of the dot product. And wanting it to be over here, the product, the dot product of random word and the center word is a negative number. And so then we're going to take the negation of that. And then again, once we put that through the sigmoid, we'd like a big number. Okay, so the way they're presenting things, they're actually maximizing this quantity. But if I go back to making it a bit more similar to the way we had written things weird worked with minimizing the negative log likelihood. It, it looks like this. So we're taking the negative log likelihood of this, the sigmoid of the dot product. Again, negative log likelihood, we're using the same negator dot product through the sigmoid. And then we're going to work out this quantity for a handful of. And we're, this loss function is going to be minimized given this negation by making these dot products large and these dot products small means negative. And then there's just then one other trick that they use actually there's more than one other trick that's used in the word to vec paper to get it to perform well, but I'll only mention one of their other tricks here. When they sample the words, they don't simply just sample the words based on their probability of occurrence in the corpus or uniformly what they do is they start with what we call the unigram distribution of words. So that is how often words actually occur in our big corpus. So if you have a billion word corpus and a particular word occurred 90 times in it, you're taking 90 divided by a billion. And so that's the unigram probability of the word. But what they then do is that they take that to the three quarters power and the effect of that three quarters power, which is then re normalized to make a probability distribution with the Z kind of like we saw last time with the softmax by taking the three quarters power that has the effect of dampening the difference between common and rare words. So that less frequent words are sampled somewhat more often, but still not nearly as much as they would be if you just use something like a uniform distribution over the vocabulary. So that's basically everything to say about the basics of how we have this very simple neural network algorithm word to vac and how we can train it and learn word vectors. So the next bit, what I want to do is step back a bit and say, well, here's an algorithm that I've shown you that works great. What else could we have done and what can we say about that. The first thing that you might think about is, well, here's this funny iterative algorithm to give you word vectors. You know, if we have a lot of words in a corpus, seems like a more obvious thing that we could do is just look at the counts of how words occur with each other and build a matrix of counts. So here's the idea of a color currents matrix. So I've got a teeny little corpus. I like deep learning. I like NLP. I enjoy flying. I can define a window size. I made my window simply size one to make it easy to fill in my matrix, symmetric, just like our word to back algorithm. And so then the counts in these cells are simply how often things that co occur in the window of size one. I like occurs twice. So we get twos in these cells because it's symmetric deep learning curves one. So we get one here and lots of other things occur zero. And build up a co-occurrence matrix like this. And well, these actually give us a representation of words as co-occurrence vectors. So I can take the word I with either a row or column vector since it's symmetric and say, OK, my representation of the word I is this row vector. That is a representation of the word I and I think you can maybe convince yourself that the extent that words have similar meaning and usage. You'd sort of expect them to have somewhat similar vectors. Right. So if I have the word you as well on a larger corpus, you might expect I and you to have similar vectors because I like you like I enjoy you enjoy. You'd see the same kinds of possibilities. Hey, Chris, could you look into the answer some questions. Sure. Alright, so we got some questions from negative sort of the negative sampling sampling slides. In particular, what's like, can you give some intuition for negative sampling? What is the negative sampling doing? And why do we only take one positive example? Those are two questions. Answering the answer. Okay, that's a good question. Okay, I'll try and give more intuition. So is to work out something like what the softmax did in a much more efficient way. So in the softmax, well, you wanted to give high probability to the in predicting the context, a context word that actually did appear with the center word. And well, the way you do that is by having the dot product between those two words be as big as possible and part of how, but you know, you're going to be sort of it's more than that because in the denominator, you're also working out the dot product with every other word in the vocabulary. So as well as wanting the dot product with the actual word that you see in the context to be big, you maximize your likelihood by making the dot products of other words that weren't in the context smaller because that's shrinking your denominator. And therefore, you've got a bigger number coming out and you're maximizing the loss. So even for the softmax, the general thing that you want to do to maximize it is have dot product with words actually in the context big dot product with words and not in the context be small to the extent possible. And obviously you have to average this is best you can over all kinds of different contexts, because sometimes different words appear in different contexts, obviously. So, so the negative sampling as a way of therefore trying to maximize the same objective. You know, for you only you only have one positive term because you're actually wanting to use the actual data. So you're not wanting wanting to invent data. So for working out the entire J we do do work this quantity out for every center word and every context word. So you know we are iterating over the different words in the context window and then we're moving through positions in the corpus. So we're doing different VCs so you know gradually we do this. But for one particular center word and one particular context word, we only have one real piece of data that's positive. So that's all we use because we don't know what other words. So that should be counter positive words. Now for the negative words, you could just sample one negative word and that would probably work. So sort of a slightly better more stable sense of okay we'd like to in general have other words have low probability. It seems like you might be able to get better more stable results. If you're instead say let's have 10 or 15 sample negative words and indeed that's been found to be true. And for the negative words, well, it's easy to sample any number of random words you want. And at that point it's kind of a probabilistic argument. The words that you're sampling might not be actually bad words to appear in the context. They might actually be other words that are in the context, but 99.9% of the time they will be unlikely words to occur in the context. And so they're good ones to use. And yes, you only sample 10 or 15 of them. And it's enough to make progress because the center word is going to turn up on other occasions. And when it does your sample different words over here so that you gradually sample different parts of the space and start to learn. And it gives a representation of words as coerced current vectors. And just one more note on that. I mean, there are actually two ways that people have commonly made these coerced current matrices. And for a response to what we've seen already that you use a window around the word, which is similar to word to veck. And that allows you to capture some locality and some of the sort of syntactic and semantic proximity that's more fine grained. And the way these current matrix, these are the often made is that normally documents have some structure, whether it's paragraphs or just actual web pages sort of size documents. So you can just make your window size a paragraph for a whole web page and count current currents and those. And this is the kind of method that's often being used in information retrieval in methods like latent semantic analysis. Okay, so the question then is are these kind of count word vectors good things to use. Well, people have used them. They're not terrible. But they have certain problems. The kind of problems that they have, well, firstly, they're huge, though, very sparse. So this is back where I said before, if we had a vocabulary of half a million words, when then we have a half a million dimensional vector for each word, which is much, much bigger than the word vectors that we typically use. And it also means that because we have these very high dimensional vectors that we have a lot of sparsity and a lot of randomness. So the results that you get tend to be noisier and less robust depending on what particular stuff was in the corpus. So in general, people have found that you can get much better results by working with low dimensional vectors. So then the idea is we can store the most of the important information about the distribution of words in the context of other words in a fixed small number of dimensions, giving a dense vector. And then practice the dimensionality of the vectors that are used are normally somewhere between 25 and 1000. And so at that point, we need to use two, we need to use some way to reduce the dimensionality of our count co occurance vectors. So if you have a good memory from a linear algebra class, you hopefully saw singular value decomposition and it has various mathematical properties that I'm not going to talk about here of single singular value projection, giving you an optimal way under a certain definition of optimality of producing a reduced dimensionality matrix that maximally or sorry, pair of matrices that maximally well lets you recover the original matrix. But the idea of the singular value decomposition is you can take any matrix such as our count matrix and you can decompose it into three matrices, you a diagonal matrix sigma and a V transpose matrix. So this works for any shape. Now in these matrices, some parts of it, I never use because since this matrix is rectangular, there's nothing over here. And so this part of the V transpose matrix gets ignored. So I'm wanting to get smaller dimensional representations, what you do is take advantage of the fact that the singular values inside the diagonal sigma matrix are ordered from largest down to smallest. So what we can do is just delete out more of the matrix of the delete out some singular values, which effectively means that in this product, some of you and some of the is also not used. And as a result of that, we're getting lower dimensional representations for our words, if we're wanting to have word vectors, which still do as good as possible a job within the given dimensionality of enabling you to recover the original co occurrence matrix. So from a linear algebra background, this is the obvious thing to use. So how does that work? Well, if you just build a raw count co occurrence matrix and run SVD on that and try and use those as word vectors, it actually works poorly. It works poorly because if you get into the mathematical assumptions, SVD, you're expecting to have these normally distributed errors and what you're getting with word counts looked not at all. Like some normal because you have exceedingly common words like that and then and you have a very large number of rare words. So that doesn't work very well, but you actually get something that works a lot better. If you scale the counts in the cells. To deal with this problem, extremely frequent words, there are some things we can do. We could just take the log of the raw counts. We could kind of cap the maximum count. We could throw away the function words and any of these kind of ideas that you build, then have a co occurrence matrix that you get more useful word vectors from running something like SVD. These kind of models were explored in the 1990s and in the 2000s and in particular Doug Rodi explored a number of these ideas is how to improve the co occurrence matrix in a model that he built that was called calls. And you know, actually in his calls model, he observed the fact that you could get the same kind of linear components that have semantic components that we saw yesterday when talking about analogies. So for example, this is a figure from his paper and you can see that we seem to have a meaning component going from a verb to the person who does the verb. So drive to drive us when to swim or teach to teacher, marry to priest. And that these vector components are not perfectly, but are roughly parallel and roughly the same size. And so we have a meaning component there that we could add on to another word, just like we did for previously for analogies, we could say drivers to driver as Mary is to what. And we'd add on this screen vector component, which is roughly the same as this one. And we'd say, oh, priest. So that this space could actually get some word vectors analogies right as well. And so that seemed really interesting to us around the time word to vac came out of wanting to understand better what the literature of updating algorithm of word to vac did. And how it related to these more linear algebra based methods that had been explored in the couple of decades previously. And so for the next bit, I want to tell you a little bit about the glove algorithm, which was an algorithm for word vectors that was made by Jeffrey pennington, Richard Socher and me in 2014. And so the starting point of this was to try to connect together the linear algebra based methods on current matrices like LSA and calls with the models like skip grand CBO and their other friends, which were iterative neural updating algorithms. So on the one hand, you know, the linear algebra methods actually seemed like they had advantages for fast training and efficient usage of statistics. But although there had been work on capturing words similarities with them by and large. The results weren't as good perhaps because of disproportionate importance given to large counts in the main conversely, the models, the neural models, it seems like if you're just doing these gradient updates on windows, you're somehow inefficiently using statistics versus a coerced currents matrix. And the other hand is actually easier to scale to a very large corpus by trading time for space. And at that time, it seemed like the neural methods just worked better for people that they generated improved performance on many tasks, not just on words similarity, and that they could capture complex patterns such as the analogies that went beyond words similarity. And so what we wanted to do was understand a bit more is to what do you what properties you need to have this analogies work out as I showed last time. And so what we realized was that if you'd like to do have these sort of vector subtractions and additions work for an analogy, the property that you want is for meaning components or meaning component is something like going from male to female queen to king or going from to a bird with a drone truck driver, that those meaning components should be represented as ratios of color currents probabilities. So here's an example that shows that. Okay, so suppose the meaning component that we want to get out is the spectrum from solid to gas as in physics, well, you'd think that you can get at the solid part of it, perhaps by saying does the word coerced with ice and the word solid occurs with ice. So it looks hopeful and gas doesn't occur with ice much so that looks hopeful, but the problem is the word water will also occur a lot with ice and if you just take some other random word like the word random, it probably doesn't occur with ice much. So if you look at words coerced with steam solid won't occur with steam much, but gas will the water will again and random will be small. So to get out the meaning component we want of going from gas to solid was actually really useful is to look at the ratio of these coerced currents probabilities, because then we get a spectrum for large to small between solid and gas, whereas for water and a random word, it basically cancels out and gives you one. I just wrote these numbers in, but if you count them up in a large corpus, it is basically what you get so here actual coerced currents probabilities and that for water and my random word, which was fashion here, these are approximately one, whereas for the ratio of probability of coerced currents of solid with ice or steam is about 10 and for gas, it's about a 10. So how can we capture these ratios of coerced currents probabilities as linear meaning components so that in our word vector space, we can just add and subtract linear meaning components. Well, it seems like the way we can achieve that is if we build a log by linear model, so that the dot product between two word vectors, attempt to approximate the log of the probability of coerced currents. So if you do that, you then get this property that the difference between two vectors, it's similarity to another word corresponds to the log of the probability ratio shown on the previous slide. So the glove model wanted to try and unify the thinking between the coerced currents matrix models and the neural models by being in some way similar to a neural model, but actually calculated on top of a coerced currents matrix count. So we had an explicit loss function and our explicit loss function is that we wanted the dot product to be similar to the log of the coerced currents. And we actually added in some bias terms here, but I'll ignore those for the moment. And we wanted to not have very common words dominate. And so we kept the effect of high word counts using this F function that's shown here. And then we could optimize this J function directly on the coerced currents count matrix. And that gave us fast training scalable to huge corpora. And so this algorithm worked very well. So if you ask, if you run this algorithm, ask what are the nearest words to fog, you get fogs toad, and then you get some complicated words, but it turns out they are all fogs until you get down the lizard. And so this is a tutorial that lovely tree fog there. And so this actually seemed to work out pretty well. How well did it work out to discuss that a bit more. I now want to say something about how do we evaluate word vectors. Are we good for up to there for questions. We've got some questions. What do you mean by an inefficient use of statistics as a con for skip. Well, what I mean is that, you know, for word to vac, you're just, you know, looking at one center word at a time and generating a few negative samples. And so it sort of seems like us doing something always precise there, whereas if you're doing optimization algorithm on the whole matrix at once, well, you actually know everything about the matrix at once. And so that's just looking at what words, what other words occurred in this one context of the center word, you've got the entire vector of co occurrence accounts for the center word and another word. And so therefore you can much more efficiently and less noiseily work out how to minimize your loss. So I'm going to say, I'll go on. Okay, so I've sort of said, look at these word vectors. They're great. And I sort of showed you a few things at the end of the last class, which argued, hey, these are great. They work out these analogies, they show similarity and things like this. We want to make this a bit more precise. And indeed for natural language processing as in other areas of machine learning, a big part of what people are doing is working out good ways to evaluate knowledge that things have. So how can we really evaluate word vectors. So in general, for NLP evaluation, people talk about two ways of evaluation intrinsic and extrinsic. So an intrinsic evaluation means that you evaluate directly on the specific or intermediate subtasks that you've been working on. A measure where I can directly score how good my word vectors are. And normally intrinsic evaluations are fast to compute. They helped you to understand the component you've been working on. But often, simply trying to optimize that component may or may not have a very big good effect on the overall system that you're trying to build. So people have also also been very interested in extrinsic evaluations. So an extrinsic evaluation is that you take some real task of interest to human beings, whether that's a web search or machine translation or something like that. And you say your goal is to actually improve performance on that task. Well, that's a real proof that this is doing something useful. So in some ways, it's just clearly better. But on the other hand, it also has some disadvantages. It takes a lot longer to evaluate on an extrinsic task because it's a much bigger system. And sometimes, you know, when you change things, it's unclear whether the fact that the numbers went down was because you now have worse word vectors or whether it's just somehow the other components of the system. And it's a better with your old word vectors. And if you change the other components as well, things would get better again. So in some ways, it can sometimes be mudier to see if you're making progress. But I'll touch on both of these methods here. So for intrinsic evaluation of word vectors, one way, which we mentioned last time was this word vector analogies. So we could simply give our models a big collection of word vector analogy problems. So we could say man is the woman as king is the what and ask the model to find the word that is closest using that sort of word analogy computation and hope that what comes out there is queen. And so that's something people have done and have worked on accuracy score of how often that you are right. At this point, I should just mention one little trick of these word vector analogies that everyone uses, but not everyone talks it out along the first instance. I mean, there's a little trick which you can find in the gents encode, if you look at it, that when it does man is the woman as king is to what. Something that could often happen is that actually the word once you do your pluses and your minuses that the word that will actually be closest is still king. So the way people always do this is that they don't allow one of the three input words in the selection process. So you're choosing the nearest word that isn't one of the put words. So since here is showing results from the glove vectors. So the glove vectors have a strong linear component property, just like I showed before for. So this is for the male female dimension. And so because of this, you'd expect in a lot of cases that word analogies would work because I can take the vector difference of man and woman. And then if I add that vector difference on to brother, I expect to get to sister and king queen and from any of these examples, but of course they may not always work right because if I start from emperor, it's sort of on a more of a lean and so it might turn out that I get counted so Dutchess coming out instead. So you can do this for various different relations, a different semantic relation. So these sort of word vectors actually learn quite a bit of just world knowledge. So here's the company CEO, or this is the company CEO around 2010 to 2014 when the data was taken from word vectors. And they as well as semantic things or pragmatic things like this, they also learn syntactic things. So here are vectors for positive comparative and support of forms of adjectives. And you can see those also move and roughly linear components. So the word to back people built a data set of analogies so you could evaluate different models on the accuracy of their analogies. And so here's how you can do this and this gives some numbers. So there are some mannequins and tactic analogies. I'll just look at the totals. Okay, so what I said before is if you just use unscaled, um, co-occurrence counts and passing through an SVD things work terribly and you see that there you only get 7.3. And as I also pointed out, if you do some scaling, you can actually get SVD to have of a scaled count matrix to work reasonably well. So this SVD L is similar to the goals model. And now we're getting up to 60.1, which actually isn't a bad score. And we can actually do a decent job without a neural network. And then here are the two variants of the word to back model and here are our results from the glove model. And of course, at the time, 2014, we took this as absolute proof that our model was better and our more efficient use of statistics was really working in our favor. So with 70 years of retrospect, I think that's kind of not really true. It turns out, I think the main part of why we scored better is that we actually had better data. And so there's a bit of evidence about that on this next slide here. So this looks at the semantics and tactic and overall performance on word analogies of glove models that were trained on different subsets of data. And in particular, the two on the left are trained on Wikipedia. And you can see that training on Wikipedia makes you do really well on semantic analogies, which maybe makes sense because Wikipedia just tells you a lot of semantic facts. I mean, that's kind of what encyclopedias do. And so one of the big advantages we actually had was that Wikipedia. That the glove model was partly trained on Wikipedia as well as other text, whereas the word to back model that was released was trained exclusively on Google news, so newswire data. And if you only train on a smaller amount of newswire data, you can see that for the semantics, it's, it's just not as good as even a one quarter of the size amount of Wikipedia data. So if you get a lot of data, you can compensate for that. So here on the right end, did you then have common crawl web data. And so once there's a lot of web data. So now 42 billion words, you're then starting to get good scores again from the semantic side. So if you're on the right, then shows how well do you do as you increase the vector dimension. And so what you can see there is, you know, 25 dimensional vectors aren't very good. So that's what I used 100 dimensional vectors when I showed my example in class year, the sweet two long old and working reasonably well, but you still get significant gains for 200 and it's somewhat to 300. So I found so of 2013 to 15, everyone sort of gravitated to the fact that 300 dimensional vectors is the sweet spot. So almost frequently, if you look through the best known sets of word vectors, then include the word to vectors and the glove vectors that usually what you get is 300 dimensional word vectors. So that's not the only intrinsic evaluation you can do. Another intrinsic evaluation you can do is see how these models model human judgments of words similarity. So psychologists for several decades have actually taken human judgments of words similarity where literally you're asking people for pairs of words like professor and doctor to give them a similarity score that's sort of being measured as some continuous quantity giving you a score between say zero and 10. And so there are human judgments, which are then averaged over multiple human judgments as to how similar different words are so Tigran cat is pretty similar computer and internet is pretty similar plane and car is less similar stock and CD aren't very similar at all, but stock and Jaguar even less similar. So we could then say for the our models, do they have the same similarity judgments and in particular, we can measure correlation coefficient of whether they give the same ordering of similarity judgments. And so then we can get data for that. And so there are various different data sets of words similarities and we can score different models as to how well they do on similarities. And again, you see here that playing svds works comparatively better here for similarities that did for analogies, you know, it's not great, but is now not completely terrible. Because we no longer need that linear property, but again scaled svds work a lot better word to veck works a bit better than that, and we got some of the same kind of minor advantages from the glove model. Chris, sorry to interrupt a lot of the students who are asking if you could re-explain the objective function for the glove model and also what log by linear means. Okay, sure. Okay, here is my here is my objective function. So anyway, if I go so one slide before that, right, so the property that we want is that we want the dot product to represent the log probability of co occurrence. And that's then gives me my tricky log by linear. So the buy is that there's sort of the w i and the w j so that there are sort of two linear things, and it's linear in each one of them. And this is sort of like having and rather having a sort of an ax where you just have something with linear in x and a is a constant it's by linear because we have the w i w j and this linear in both of them. And that's then related to the log of a probability, and so that gives us the log by linear model. And so since we since we'd like these things to be equal, what we're doing here, if you ignore these two center terms, is that we're wanting to say the difference between these. And we want that is as small as possible, so we're taking this difference and we're squaring it so it's always positive, and we want that square term to be as small as possible. And you can basically stop there, but the other bit that's in here is a lot of the time when you're building models, rather than simply having sort of an ax model, it seems useful to have a bias term, which can move things up and down for the word in general. And so we added into the model bias term so that there's a bias term for both words, so if in general probabilities are high for a certain word, this bias term can model that and for the other word this bias term model it okay. And now I'll pop back and after actually I just saw someone said why multiplying by the f of sorry I did skip that last term. Okay, the why modifying by this f of x i j so this last bit was to scale things, depending on the frequency of a word, because you want to pay more attention to words that are more common or word pairs that are more common, because you know, if you think about it in word, you're seeing if things have a coerced current to count of 50 versus three, you want to do a better job at modeling the coerced current of the things that occurred together 50 times. So you want to consider in the count of coerced currents, but then the argument is that that actually lead to a stray when you have extremely common words like function words, and so effectively you paid more attention to words that co occurred together up until a certain point and then the curve just went flat, so it didn't matter if it was an extremely extremely common word. So then for extrinsic word vector evaluation, so at this point, you're now wanting to sort of say well, can we embed our word vectors in some end user task and do they help. And do different word vectors work better or worse than other word vectors, so this is something that will see a lot of later in the class, I mean in particular, when you get on to doing assignment three that assignment three, you get to build dependency parsers and you can then use word vectors in the dependency parser and see how much they help we don't actually make you test out different sets of word vectors, but you could. So here's just one example of this to give you a sense, so the task of named entity recognition is going through a piece of text and identifying mentions of a person name or an organization name like a company or a location and so. If you have good word vectors, do they help you do named entity recognition better and the answer that is yes, so if one starts off with a model that simply has discrete features so it uses word identity as features, you can build a pretty good name density model doing that, but if you add into it word vectors, you get a better representation of the meaning of words. So you can do that you can have the numbers go up quite a bit and then you can compare different models to see how much gain they give you in terms of this extrinsic task. So skipping ahead, this was a question that I asked after class, which was word sensors because so far we've had just one word. So for one particular string we've got some string house and we're going to say for each of those strings there's a word vector and if you think about it a bit more that seems like it's very weird because actually most words, especially common words and especially words that have existed for a long time actually have many meanings, which are very different. So how could that be captured if you only have one word vector for the word because you can't actually capture the fact that you've got different meanings for the word because your meaning for the word is just one point in space one vector. And so as an example of that here's the word pie. So for a minute and think what word meanings the word pie cares. And it actually turns out you know it has a lot of different meaning so so perhaps the most basic meaning is if you did fantasy games or something medieval weapons. And it's a kind of a kind of a fish that has a similar elongated shape that's a pike. It was used for railroad lines maybe that usage isn't used much anymore but it's certainly still survived and referring to roads so this is like when you have turn pikes we have expressions where pike means the future like coming down the pike. And then there's a position in diving that divers do a pike. Those are all noun uses. They're also verbal uses so you can pike somebody with your pike. You know different usages might have different currency in a stray you can also use pike to mean that you pull out of doing something like I reckon he's going to pike. And that usage is used in America but lots of meanings and actually for words that commoner if you start thinking words like line or field. I mean they just have even more meanings than this so what are we actually doing with just one vector for a word. Well one way you could go is to say okay up until now what we've done is crazy pike has and other words have all of these different meanings so maybe what we should do is have different word vectors for the different meanings of pike so we'd have one word vector for the medieval pointy weapon. Another word vector for the kind of fish another word vector for the kind of road so that they then be words sense vectors. And you can do that I mean actually we were working on that in the early 2000 and 10s actually even before word to that came out. So this picture is a little bit small to see but what we were doing was for words we work clustering instances of a word hoping that those clusters so clustering the word tokens hoping those clusters that were similar represented sensors and then for the clusters of word tokens. So treating them like they were separate words and learning a word vector for each and you know basically that actually works so in green we have two sensors for the word bank and so there's one sense for the word bank that's over here where it's close to words like banking finance transaction laundering. And then we have another sense for the word bank over here whereas close to words like plateau boundary gap territory which is the river bank sense of the word bank. And for the word jacuar that's in purple. Well, jacuar has a number of sensors and so we have those as well so this sense down here is close to hunter so that's the sort of big game animal sense of jaguar up the top here is being shown close to luxury and convertibles is the jaguar car sense. Then jaguar here is near string keyboard and words like that so jaguars the name of a kind of keyboard and then this final jaguar over here is close to software and Microsoft and then if you're old enough you'll remember that there was an old version of macOS so it was called jaguar so that's then the computer sense so basically this does work and we can learn word vectors. So there's a different sensors of a word but actually this isn't the majority way that things have then gone in practice and there are kind of a couple of reasons for that I mean one is just simplicity if you do this. And then you're going to have a kind of complex because you first of all have to learn word sensors and then start learning word vectors in terms of the word sensors. So it's commonly what's being used in natural language processing I mean it tends to be imperfect in its own way because we're trying to take all the uses of the word pike and sort of cut them up into key different sensors where the difference is kind of overlapping and it's often not clear which ones to count as distinct so for example here right a railroad line and a type of road sort of that's the same sense of pike it's just that they're different forms of transportation and so you know that this could be you know a type of transportation line and cover both of them so it's always sort of very unclear how you cut word meaning into different sensors and indeed if you look at different dictionaries everyone does it differently. So it actually turns out that in practice you can do rather well by simply having one word vector per word type and what happens if you do that well what you find is that what you learn as a word vector is what gets referred to in fancy talk as a super super position of the diff of the word vectors for the different sensors of a word where the word super position means no more or less than a weighted some so out the vector that we learn for pike will be a weighted average of the vectors that you would have learned for the medieval weapon sense plus the fish sense plus the road sense plus whatever other sensors that you have where the weighting that's given to these different sense vectors corresponds to the frequencies of use of the different sensors so we end up with the word the vector for pike being a kind of an average vector so if you're say okay you've just added up several different vectors into an average you might think that that's kind of useless because you know you've lost the real meanings of the word you've just got some kind of funny average vector that's in between them and then suddenly it turns out that if you use this average vector in applications it tends to sort of self disambiguate because if you say is the word pike similar to the word for fish well part of this vector represents fish the fish sense of pike and so in those components it will be kind of similar to the fish vector so yes you'll say the substantial similarity whereas if in another piece of text that says you know the men were armed with pikes and lancers or pikes and maces or whatever other many of the weapons you remember well actually some of that meaning is in the pike vector as well and so it will say yeah there's good similarity mace and staff and words like that as well and in fact we can work out which sense of pike is intended by just sort of seeing which components are similar to other words that are used in the same context indeed there's actually a much more surprising result than that and this is a result that's Jews Sangev Aurora, Tung Ruma who is now on our Stanford faculty and others in 2018 and that's the following result which I'm not actually going to explain but so if you think that the vector for pike is just a sum of the vectors for the different sensors well it should be you'd think that it's just completely impossible to reconstruct the sense vectors from the vector for the word type because normally if I say I've got two numbers the sum of them is 17 you just have no information as to what my two numbers are right you can't resolve it and even worse if I tell you I've got three numbers and they sum to 17 but it turns out that when we have these high dimensional vector spaces that things are so sparse in those high dimensional vector spaces that you can use ideas from sparse coding to actually separate out the different sensors providing their relatively common so they show in their paper that you can start with the vector of say pike and actually separate out components of that vector that correspond to different sensors of the word pike and so here's an example at the bottom of this slide which is for the word separate out that vector into five different sensors and so there's one sense is close to the words trousers blouse waist coats and this is the sort of clothing sense of tie another sense is is close to wise cables wiring electrical so that's the sort of the tie sense of tie used in electrical staff then we have sort of scoreline goal is equalizer the so this is the sporting game sense of tie this one also seems to in a different way evokes sporting game sense of tie and then there's finally this one here maybe my music is just really bad maybe it's because you get ties and music when you tie notes together I guess so you get these different sensors out of it", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " So what are we going to do for today?", "tokens": [407, 437, 366, 321, 516, 281, 360, 337, 965, 30], "temperature": 0.0, "avg_logprob": -0.23977932563194862, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.034382354468107224}, {"id": 1, "seek": 0, "start": 7.0, "end": 17.0, "text": " So the main content for today is to go through sort of more stuff about word vectors,", "tokens": [407, 264, 2135, 2701, 337, 965, 307, 281, 352, 807, 1333, 295, 544, 1507, 466, 1349, 18875, 11], "temperature": 0.0, "avg_logprob": -0.23977932563194862, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.034382354468107224}, {"id": 2, "seek": 0, "start": 17.0, "end": 23.0, "text": " including touching on word sensors and then introducing the notion of neural network classifiers.", "tokens": [3009, 11175, 322, 1349, 14840, 293, 550, 15424, 264, 10710, 295, 18161, 3209, 1508, 23463, 13], "temperature": 0.0, "avg_logprob": -0.23977932563194862, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.034382354468107224}, {"id": 3, "seek": 2300, "start": 23.0, "end": 41.0, "text": " So our biggest goal is that by the end of today's class, you should feel like you could confidently look at one of the word embeddings papers, such as the Google word to vac paper or the glove paper or Sanji Rao's paper that will come to later and feel like, yeah, I can understand this.", "tokens": [407, 527, 3880, 3387, 307, 300, 538, 264, 917, 295, 965, 311, 1508, 11, 291, 820, 841, 411, 291, 727, 41956, 574, 412, 472, 295, 264, 1349, 12240, 29432, 10577, 11, 1270, 382, 264, 3329, 1349, 281, 2842, 3035, 420, 264, 26928, 3035, 420, 5271, 4013, 7591, 78, 311, 3035, 300, 486, 808, 281, 1780, 293, 841, 411, 11, 1338, 11, 286, 393, 1223, 341, 13], "temperature": 0.0, "avg_logprob": -0.19849813989846102, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.00067563122138381}, {"id": 4, "seek": 2300, "start": 41.0, "end": 43.0, "text": " I know what they're doing and it makes sense.", "tokens": [286, 458, 437, 436, 434, 884, 293, 309, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.19849813989846102, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.00067563122138381}, {"id": 5, "seek": 4300, "start": 43.0, "end": 52.0, "text": " So let's go back to where we were. So this was sort of introducing this model of word to vac and", "tokens": [407, 718, 311, 352, 646, 281, 689, 321, 645, 13, 407, 341, 390, 1333, 295, 15424, 341, 2316, 295, 1349, 281, 2842, 293], "temperature": 0.0, "avg_logprob": -0.2145811363502785, "compression_ratio": 1.1428571428571428, "no_speech_prob": 0.0019522904185578227}, {"id": 6, "seek": 5200, "start": 52.0, "end": 63.0, "text": " the idea was that we started with random word vectors and then we're going to sort of, we have a big corpus of text and we're going to iterate through each word in the whole corpus.", "tokens": [264, 1558, 390, 300, 321, 1409, 365, 4974, 1349, 18875, 293, 550, 321, 434, 516, 281, 1333, 295, 11, 321, 362, 257, 955, 1181, 31624, 295, 2487, 293, 321, 434, 516, 281, 44497, 807, 1184, 1349, 294, 264, 1379, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.20904016494750977, "compression_ratio": 1.658682634730539, "no_speech_prob": 0.0038589555770158768}, {"id": 7, "seek": 5200, "start": 63.0, "end": 70.0, "text": " And for each position, we're going to try and predict what words surround this our center word.", "tokens": [400, 337, 1184, 2535, 11, 321, 434, 516, 281, 853, 293, 6069, 437, 2283, 6262, 341, 527, 3056, 1349, 13], "temperature": 0.0, "avg_logprob": -0.20904016494750977, "compression_ratio": 1.658682634730539, "no_speech_prob": 0.0038589555770158768}, {"id": 8, "seek": 7000, "start": 70.0, "end": 83.0, "text": " And we're going to do that with a probability distribution that's defined in terms of the dot product between the word vectors for the center word and the context words.", "tokens": [400, 321, 434, 516, 281, 360, 300, 365, 257, 8482, 7316, 300, 311, 7642, 294, 2115, 295, 264, 5893, 1674, 1296, 264, 1349, 18875, 337, 264, 3056, 1349, 293, 264, 4319, 2283, 13], "temperature": 0.0, "avg_logprob": -0.06303906022456654, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.4620726839639246e-05}, {"id": 9, "seek": 7000, "start": 83.0, "end": 88.0, "text": " And so that will give a probability estimate of a word appearing in the context of into.", "tokens": [400, 370, 300, 486, 976, 257, 8482, 12539, 295, 257, 1349, 19870, 294, 264, 4319, 295, 666, 13], "temperature": 0.0, "avg_logprob": -0.06303906022456654, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.4620726839639246e-05}, {"id": 10, "seek": 8800, "start": 88.0, "end": 102.0, "text": " Well, actual words did occur in the context of into on this occasion. So what we're going to want to do is sort of make it more likely that turning problems banking and crises will turn up in the context of into.", "tokens": [1042, 11, 3539, 2283, 630, 5160, 294, 264, 4319, 295, 666, 322, 341, 9674, 13, 407, 437, 321, 434, 516, 281, 528, 281, 360, 307, 1333, 295, 652, 309, 544, 3700, 300, 6246, 2740, 18261, 293, 31403, 486, 1261, 493, 294, 264, 4319, 295, 666, 13], "temperature": 0.0, "avg_logprob": -0.06147136150951117, "compression_ratio": 1.6275510204081634, "no_speech_prob": 1.1294840987829957e-05}, {"id": 11, "seek": 8800, "start": 102.0, "end": 109.0, "text": " And so that's learning updating the word vectors so that they can predict actual surrounding words better.", "tokens": [400, 370, 300, 311, 2539, 25113, 264, 1349, 18875, 370, 300, 436, 393, 6069, 3539, 11498, 2283, 1101, 13], "temperature": 0.0, "avg_logprob": -0.06147136150951117, "compression_ratio": 1.6275510204081634, "no_speech_prob": 1.1294840987829957e-05}, {"id": 12, "seek": 10900, "start": 109.0, "end": 125.0, "text": " And then the thing is almost magical is that doing no more than this simple algorithm, this allows us to learn word vectors that capture well words similarity and meaningful directions in a word space.", "tokens": [400, 550, 264, 551, 307, 1920, 12066, 307, 300, 884, 572, 544, 813, 341, 2199, 9284, 11, 341, 4045, 505, 281, 1466, 1349, 18875, 300, 7983, 731, 2283, 32194, 293, 10995, 11095, 294, 257, 1349, 1901, 13], "temperature": 0.0, "avg_logprob": -0.11897944241035276, "compression_ratio": 1.467153284671533, "no_speech_prob": 5.304055230226368e-05}, {"id": 13, "seek": 12500, "start": 125.0, "end": 140.0, "text": " So more precisely right for this model, the only parameters of this model are the word vectors. So we have outside word vectors and center word vectors for each word.", "tokens": [407, 544, 13402, 558, 337, 341, 2316, 11, 264, 787, 9834, 295, 341, 2316, 366, 264, 1349, 18875, 13, 407, 321, 362, 2380, 1349, 18875, 293, 3056, 1349, 18875, 337, 1184, 1349, 13], "temperature": 0.0, "avg_logprob": -0.13395251454533758, "compression_ratio": 1.509090909090909, "no_speech_prob": 1.7476377252023667e-05}, {"id": 14, "seek": 14000, "start": 140.0, "end": 157.0, "text": " And then we're going to get a probability, well, we get taking a dot product to get a score of how likely a particular outside word is to occur with the center word. And then we're using the soft max transformation to convert those scores into probabilities as I discussed last time.", "tokens": [400, 550, 321, 434, 516, 281, 483, 257, 8482, 11, 731, 11, 321, 483, 1940, 257, 5893, 1674, 281, 483, 257, 6175, 295, 577, 3700, 257, 1729, 2380, 1349, 307, 281, 5160, 365, 264, 3056, 1349, 13, 400, 550, 321, 434, 1228, 264, 2787, 11469, 9887, 281, 7620, 729, 13444, 666, 33783, 382, 286, 7152, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.20201561667702414, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.166500027873553e-05}, {"id": 15, "seek": 14000, "start": 157.0, "end": 161.0, "text": " And I kind of come back to at the end this time.", "tokens": [400, 286, 733, 295, 808, 646, 281, 412, 264, 917, 341, 565, 13], "temperature": 0.0, "avg_logprob": -0.20201561667702414, "compression_ratio": 1.6435643564356435, "no_speech_prob": 3.166500027873553e-05}, {"id": 16, "seek": 16100, "start": 161.0, "end": 174.0, "text": " And then I want to start with the next thing to note, this model is what we call an NLP a bag of words model. So bag of words models models that don't actually pay any attention to word order or position.", "tokens": [400, 550, 286, 528, 281, 722, 365, 264, 958, 551, 281, 3637, 11, 341, 2316, 307, 437, 321, 818, 364, 426, 45196, 257, 3411, 295, 2283, 2316, 13, 407, 3411, 295, 2283, 5245, 5245, 300, 500, 380, 767, 1689, 604, 3202, 281, 1349, 1668, 420, 2535, 13], "temperature": 0.2, "avg_logprob": -0.3271363040050828, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0002412476169411093}, {"id": 17, "seek": 16100, "start": 174.0, "end": 182.0, "text": " It doesn't matter if you're next to the center word or bit further away on the left or right, the probability estimate would be the same.", "tokens": [467, 1177, 380, 1871, 498, 291, 434, 958, 281, 264, 3056, 1349, 420, 857, 3052, 1314, 322, 264, 1411, 420, 558, 11, 264, 8482, 12539, 576, 312, 264, 912, 13], "temperature": 0.2, "avg_logprob": -0.3271363040050828, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0002412476169411093}, {"id": 18, "seek": 18200, "start": 182.0, "end": 193.0, "text": " And that seems like a very crude model of language that will offend any linguist and it is a very crude model of language and we'll move on to better models of languages we go on.", "tokens": [400, 300, 2544, 411, 257, 588, 30796, 2316, 295, 2856, 300, 486, 41836, 604, 21766, 468, 293, 309, 307, 257, 588, 30796, 2316, 295, 2856, 293, 321, 603, 1286, 322, 281, 1101, 5245, 295, 8650, 321, 352, 322, 13], "temperature": 0.0, "avg_logprob": -0.08849069878861711, "compression_ratio": 1.8171428571428572, "no_speech_prob": 2.6666759367799386e-05}, {"id": 19, "seek": 18200, "start": 193.0, "end": 203.0, "text": " But even that crude model of language is enough to learn quite a lot of the probability, sorry, quite a lot about the properties of words.", "tokens": [583, 754, 300, 30796, 2316, 295, 2856, 307, 1547, 281, 1466, 1596, 257, 688, 295, 264, 8482, 11, 2597, 11, 1596, 257, 688, 466, 264, 7221, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.08849069878861711, "compression_ratio": 1.8171428571428572, "no_speech_prob": 2.6666759367799386e-05}, {"id": 20, "seek": 20300, "start": 203.0, "end": 220.0, "text": " And then the second note is, well, with this model, we wanted to give reasonably high probabilities to the words that do occur in the context of the center word, at least if they do so at all often.", "tokens": [400, 550, 264, 1150, 3637, 307, 11, 731, 11, 365, 341, 2316, 11, 321, 1415, 281, 976, 23551, 1090, 33783, 281, 264, 2283, 300, 360, 5160, 294, 264, 4319, 295, 264, 3056, 1349, 11, 412, 1935, 498, 436, 360, 370, 412, 439, 2049, 13], "temperature": 0.0, "avg_logprob": -0.08446007966995239, "compression_ratio": 1.5, "no_speech_prob": 0.0001658248365856707}, {"id": 21, "seek": 22000, "start": 220.0, "end": 233.0, "text": " Obviously, lots of different words can occur. So we're not talking about probabilities like point three and point five, we're more likely going to be talking about probabilities like point one and numbers like that.", "tokens": [7580, 11, 3195, 295, 819, 2283, 393, 5160, 13, 407, 321, 434, 406, 1417, 466, 33783, 411, 935, 1045, 293, 935, 1732, 11, 321, 434, 544, 3700, 516, 281, 312, 1417, 466, 33783, 411, 935, 472, 293, 3547, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.17899901072184246, "compression_ratio": 1.6165413533834587, "no_speech_prob": 4.397836164571345e-05}, {"id": 22, "seek": 23300, "start": 233.0, "end": 251.0, "text": " Well, how do we achieve that? And well, the way that the word to fact model achieves this, and this is the learning phase of the model is to place words that are similar in meaning close to each other in this high dimensional vector space.", "tokens": [1042, 11, 577, 360, 321, 4584, 300, 30, 400, 731, 11, 264, 636, 300, 264, 1349, 281, 1186, 2316, 3538, 977, 341, 11, 293, 341, 307, 264, 2539, 5574, 295, 264, 2316, 307, 281, 1081, 2283, 300, 366, 2531, 294, 3620, 1998, 281, 1184, 661, 294, 341, 1090, 18795, 8062, 1901, 13], "temperature": 0.0, "avg_logprob": -0.09385062115533012, "compression_ratio": 1.5827814569536425, "no_speech_prob": 3.4745378798106685e-05}, {"id": 23, "seek": 25100, "start": 251.0, "end": 268.0, "text": " So again, you can't read this one, but if we scroll into this one, we see lots of words that are similar in meaning group close together in the space. So here are days of the week like Tuesday Thursday Sunday and also Christmas.", "tokens": [407, 797, 11, 291, 393, 380, 1401, 341, 472, 11, 457, 498, 321, 11369, 666, 341, 472, 11, 321, 536, 3195, 295, 2283, 300, 366, 2531, 294, 3620, 1594, 1998, 1214, 294, 264, 1901, 13, 407, 510, 366, 1708, 295, 264, 1243, 411, 10017, 10383, 7776, 293, 611, 5272, 13], "temperature": 0.0, "avg_logprob": -0.10224813002127188, "compression_ratio": 1.4709677419354839, "no_speech_prob": 6.0703521739924327e-05}, {"id": 24, "seek": 26800, "start": 268.0, "end": 288.0, "text": " So what else do we have? We have Samsung and Nokia. This is a diagram I made quite a few years ago. So that's when Nokia was still an important maker of cell phones. We have various sort of fields like mathematics and economics over here.", "tokens": [407, 437, 1646, 360, 321, 362, 30, 492, 362, 13173, 293, 43980, 13, 639, 307, 257, 10686, 286, 1027, 1596, 257, 1326, 924, 2057, 13, 407, 300, 311, 562, 43980, 390, 920, 364, 1021, 17127, 295, 2815, 10216, 13, 492, 362, 3683, 1333, 295, 7909, 411, 18666, 293, 14564, 670, 510, 13], "temperature": 0.0, "avg_logprob": -0.1601823057447161, "compression_ratio": 1.4082840236686391, "no_speech_prob": 0.00010687804024200886}, {"id": 25, "seek": 28800, "start": 288.0, "end": 301.0, "text": " So we have two words that are similar in meaning. Actually, one more note I wanted to make on this. I mean, again, this is a two dimensional picture, which is all I can show you on a slide.", "tokens": [407, 321, 362, 732, 2283, 300, 366, 2531, 294, 3620, 13, 5135, 11, 472, 544, 3637, 286, 1415, 281, 652, 322, 341, 13, 286, 914, 11, 797, 11, 341, 307, 257, 732, 18795, 3036, 11, 597, 307, 439, 286, 393, 855, 291, 322, 257, 4137, 13], "temperature": 0.0, "avg_logprob": -0.1819342599398848, "compression_ratio": 1.4816753926701571, "no_speech_prob": 0.000294123514322564}, {"id": 26, "seek": 28800, "start": 301.0, "end": 308.0, "text": " And it's done with the principal components projection that you're also using the assignment.", "tokens": [400, 309, 311, 1096, 365, 264, 9716, 6677, 22743, 300, 291, 434, 611, 1228, 264, 15187, 13], "temperature": 0.0, "avg_logprob": -0.1819342599398848, "compression_ratio": 1.4816753926701571, "no_speech_prob": 0.000294123514322564}, {"id": 27, "seek": 30800, "start": 308.0, "end": 322.0, "text": " So something important to remember about hard to remember is that high dimensional spaces have very different properties to the two dimensional spaces that we can look at. And so in particular,", "tokens": [407, 746, 1021, 281, 1604, 466, 1152, 281, 1604, 307, 300, 1090, 18795, 7673, 362, 588, 819, 7221, 281, 264, 732, 18795, 7673, 300, 321, 393, 574, 412, 13, 400, 370, 294, 1729, 11], "temperature": 0.0, "avg_logprob": -0.13878752968528055, "compression_ratio": 1.7752808988764044, "no_speech_prob": 6.493653199868277e-05}, {"id": 28, "seek": 30800, "start": 322.0, "end": 332.0, "text": " a word, a vector can be close to many other things in a high dimensional space, but close to them on different dimensions.", "tokens": [257, 1349, 11, 257, 8062, 393, 312, 1998, 281, 867, 661, 721, 294, 257, 1090, 18795, 1901, 11, 457, 1998, 281, 552, 322, 819, 12819, 13], "temperature": 0.0, "avg_logprob": -0.13878752968528055, "compression_ratio": 1.7752808988764044, "no_speech_prob": 6.493653199868277e-05}, {"id": 29, "seek": 33200, "start": 332.0, "end": 348.0, "text": " So I've mentioned doing learning. So the next question is, well, how do we learn good word vectors? And this was the bit that I didn't quite hook up at the end of last class.", "tokens": [407, 286, 600, 2835, 884, 2539, 13, 407, 264, 958, 1168, 307, 11, 731, 11, 577, 360, 321, 1466, 665, 1349, 18875, 30, 400, 341, 390, 264, 857, 300, 286, 994, 380, 1596, 6328, 493, 412, 264, 917, 295, 1036, 1508, 13], "temperature": 0.0, "avg_logprob": -0.07636279126872188, "compression_ratio": 1.3282442748091603, "no_speech_prob": 0.000281210639514029}, {"id": 30, "seek": 34800, "start": 348.0, "end": 369.0, "text": " So for a while in the last, I said, oh, gee, and we have to work out the gradient of the loss function with respect to the parameters that will allow us to make progress. But I didn't sort of altogether put that together. So what we're going to do is we start off with random word vectors.", "tokens": [407, 337, 257, 1339, 294, 264, 1036, 11, 286, 848, 11, 1954, 11, 24105, 11, 293, 321, 362, 281, 589, 484, 264, 16235, 295, 264, 4470, 2445, 365, 3104, 281, 264, 9834, 300, 486, 2089, 505, 281, 652, 4205, 13, 583, 286, 994, 380, 1333, 295, 19051, 829, 300, 1214, 13, 407, 437, 321, 434, 516, 281, 360, 307, 321, 722, 766, 365, 4974, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.15149868709940306, "compression_ratio": 1.5454545454545454, "no_speech_prob": 5.275311923469417e-05}, {"id": 31, "seek": 36900, "start": 369.0, "end": 390.0, "text": " And we're going to visualize them to small numbers near zero in each dimension. We've defined our loss function J, which we looked at last time. And then we're going to use a gradient descent algorithm, which is an iterative algorithm that learns to maximize J of theta by changing theta.", "tokens": [400, 321, 434, 516, 281, 23273, 552, 281, 1359, 3547, 2651, 4018, 294, 1184, 10139, 13, 492, 600, 7642, 527, 4470, 2445, 508, 11, 597, 321, 2956, 412, 1036, 565, 13, 400, 550, 321, 434, 516, 281, 764, 257, 16235, 23475, 9284, 11, 597, 307, 364, 17138, 1166, 9284, 300, 27152, 281, 19874, 508, 295, 9725, 538, 4473, 9725, 13], "temperature": 0.0, "avg_logprob": -0.18071140348911285, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.00017389481945428997}, {"id": 32, "seek": 39000, "start": 390.0, "end": 404.0, "text": " The idea of this algorithm is that from the current values of theta, you calculate the gradient J of theta. And then what you're going to do is make a small step in the direction of the negative gradient.", "tokens": [440, 1558, 295, 341, 9284, 307, 300, 490, 264, 2190, 4190, 295, 9725, 11, 291, 8873, 264, 16235, 508, 295, 9725, 13, 400, 550, 437, 291, 434, 516, 281, 360, 307, 652, 257, 1359, 1823, 294, 264, 3513, 295, 264, 3671, 16235, 13], "temperature": 0.0, "avg_logprob": -0.0635701596736908, "compression_ratio": 1.8958333333333333, "no_speech_prob": 4.831276601180434e-05}, {"id": 33, "seek": 39000, "start": 404.0, "end": 415.0, "text": " So the gradient is pointing upwards. And we're taking a small step in the direction of the negative of the gradient to gradually move down towards the minimum.", "tokens": [407, 264, 16235, 307, 12166, 22167, 13, 400, 321, 434, 1940, 257, 1359, 1823, 294, 264, 3513, 295, 264, 3671, 295, 264, 16235, 281, 13145, 1286, 760, 3030, 264, 7285, 13], "temperature": 0.0, "avg_logprob": -0.0635701596736908, "compression_ratio": 1.8958333333333333, "no_speech_prob": 4.831276601180434e-05}, {"id": 34, "seek": 41500, "start": 415.0, "end": 433.0, "text": " And so one of the parameters of neural nets that you can fiddle in your software package is what is the step size. So if you take a really, really, it's a bit see step. It might take you a long time to minimize the function. You do a lot of wasted computation.", "tokens": [400, 370, 472, 295, 264, 9834, 295, 18161, 36170, 300, 291, 393, 24553, 2285, 294, 428, 4722, 7372, 307, 437, 307, 264, 1823, 2744, 13, 407, 498, 291, 747, 257, 534, 11, 534, 11, 309, 311, 257, 857, 536, 1823, 13, 467, 1062, 747, 291, 257, 938, 565, 281, 17522, 264, 2445, 13, 509, 360, 257, 688, 295, 19496, 24903, 13], "temperature": 0.0, "avg_logprob": -0.0957868062532865, "compression_ratio": 1.5028901734104045, "no_speech_prob": 6.913811375852674e-05}, {"id": 35, "seek": 43300, "start": 433.0, "end": 450.0, "text": " On the other hand, if your step size is much too big, well, then you can actually diverge and start going to worse places. Or even if you are going down hill a little bit that what's going to happen is you're then going to end up bouncing back and forth.", "tokens": [1282, 264, 661, 1011, 11, 498, 428, 1823, 2744, 307, 709, 886, 955, 11, 731, 11, 550, 291, 393, 767, 18558, 432, 293, 722, 516, 281, 5324, 3190, 13, 1610, 754, 498, 291, 366, 516, 760, 10997, 257, 707, 857, 300, 437, 311, 516, 281, 1051, 307, 291, 434, 550, 516, 281, 917, 493, 27380, 646, 293, 5220, 13], "temperature": 0.0, "avg_logprob": -0.09343780615390876, "compression_ratio": 1.54, "no_speech_prob": 5.557688564294949e-05}, {"id": 36, "seek": 43300, "start": 450.0, "end": 454.0, "text": " And it'll take you much longer to get to the minimum.", "tokens": [400, 309, 603, 747, 291, 709, 2854, 281, 483, 281, 264, 7285, 13], "temperature": 0.0, "avg_logprob": -0.09343780615390876, "compression_ratio": 1.54, "no_speech_prob": 5.557688564294949e-05}, {"id": 37, "seek": 45400, "start": 454.0, "end": 466.0, "text": " And this picture, I have a beautiful quadratic. And it's easy to minimize it. Something that you might know about neural networks is then in general, then not convex.", "tokens": [400, 341, 3036, 11, 286, 362, 257, 2238, 37262, 13, 400, 309, 311, 1858, 281, 17522, 309, 13, 6595, 300, 291, 1062, 458, 466, 18161, 9590, 307, 550, 294, 2674, 11, 550, 406, 42432, 13], "temperature": 0.0, "avg_logprob": -0.17415640694754464, "compression_ratio": 1.441025641025641, "no_speech_prob": 3.315126014058478e-05}, {"id": 38, "seek": 45400, "start": 466.0, "end": 473.0, "text": " So you could think that this is just all going to go awry. But the truth is, and practice life works out to be OK.", "tokens": [407, 291, 727, 519, 300, 341, 307, 445, 439, 516, 281, 352, 1714, 627, 13, 583, 264, 3494, 307, 11, 293, 3124, 993, 1985, 484, 281, 312, 2264, 13], "temperature": 0.0, "avg_logprob": -0.17415640694754464, "compression_ratio": 1.441025641025641, "no_speech_prob": 3.315126014058478e-05}, {"id": 39, "seek": 47300, "start": 473.0, "end": 485.0, "text": " So I think I won't get into that more right now and come back to that in the later class. So this is our gradient descent. So we have the current values of the parameters theta.", "tokens": [407, 286, 519, 286, 1582, 380, 483, 666, 300, 544, 558, 586, 293, 808, 646, 281, 300, 294, 264, 1780, 1508, 13, 407, 341, 307, 527, 16235, 23475, 13, 407, 321, 362, 264, 2190, 4190, 295, 264, 9834, 9725, 13], "temperature": 0.0, "avg_logprob": -0.0958373329856179, "compression_ratio": 1.4047619047619047, "no_speech_prob": 5.8177705795969814e-05}, {"id": 40, "seek": 48500, "start": 485.0, "end": 512.0, "text": " And then walk a little bit in the negative direction of the gradient using our learning rate or step size alpha. And that gives us new parameter values where that means that you know these are vectors, but for each individual parameter, we're updating it a little bit by working out the partial derivative of J with respect to that parameter.", "tokens": [400, 550, 1792, 257, 707, 857, 294, 264, 3671, 3513, 295, 264, 16235, 1228, 527, 2539, 3314, 420, 1823, 2744, 8961, 13, 400, 300, 2709, 505, 777, 13075, 4190, 689, 300, 1355, 300, 291, 458, 613, 366, 18875, 11, 457, 337, 1184, 2609, 13075, 11, 321, 434, 25113, 309, 257, 707, 857, 538, 1364, 484, 264, 14641, 13760, 295, 508, 365, 3104, 281, 300, 13075, 13], "temperature": 0.0, "avg_logprob": -0.15177762167794365, "compression_ratio": 1.6208530805687205, "no_speech_prob": 9.304496779805049e-05}, {"id": 41, "seek": 51200, "start": 512.0, "end": 519.0, "text": " So that's the simple gradient descent algorithm. Nobody uses it and you shouldn't use it.", "tokens": [407, 300, 311, 264, 2199, 16235, 23475, 9284, 13, 9297, 4960, 309, 293, 291, 4659, 380, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.11049370633231269, "compression_ratio": 1.5210526315789474, "no_speech_prob": 5.736738603445701e-05}, {"id": 42, "seek": 51200, "start": 519.0, "end": 534.0, "text": " The problem is that our J is a function of all windows in the corpus. Remember, we're doing this sum over every center word in the entire corpus. And we'll often have billions of words in the corpus.", "tokens": [440, 1154, 307, 300, 527, 508, 307, 257, 2445, 295, 439, 9309, 294, 264, 1181, 31624, 13, 5459, 11, 321, 434, 884, 341, 2408, 670, 633, 3056, 1349, 294, 264, 2302, 1181, 31624, 13, 400, 321, 603, 2049, 362, 17375, 295, 2283, 294, 264, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.11049370633231269, "compression_ratio": 1.5210526315789474, "no_speech_prob": 5.736738603445701e-05}, {"id": 43, "seek": 53400, "start": 534.0, "end": 544.0, "text": " So actually working out J of theta or the gradient of J of theta would be extremely extremely expensive because we have to iterate over our entire corpus.", "tokens": [407, 767, 1364, 484, 508, 295, 9725, 420, 264, 16235, 295, 508, 295, 9725, 576, 312, 4664, 4664, 5124, 570, 321, 362, 281, 44497, 670, 527, 2302, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.11473425364090224, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00016592421161476523}, {"id": 44, "seek": 53400, "start": 544.0, "end": 552.0, "text": " So you'd wait a very long time before you made a single gradient update. And so optimization be extremely slow.", "tokens": [407, 291, 1116, 1699, 257, 588, 938, 565, 949, 291, 1027, 257, 2167, 16235, 5623, 13, 400, 370, 19618, 312, 4664, 2964, 13], "temperature": 0.0, "avg_logprob": -0.11473425364090224, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00016592421161476523}, {"id": 45, "seek": 55200, "start": 552.0, "end": 568.0, "text": " And so basically 100% of the time in your network land, we don't use gradient descent. We instead use what's called stochastic gradient descent. And stochastic gradient descent is a very simple modification of this.", "tokens": [400, 370, 1936, 2319, 4, 295, 264, 565, 294, 428, 3209, 2117, 11, 321, 500, 380, 764, 16235, 23475, 13, 492, 2602, 764, 437, 311, 1219, 342, 8997, 2750, 16235, 23475, 13, 400, 342, 8997, 2750, 16235, 23475, 307, 257, 588, 2199, 26747, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.1067158317565918, "compression_ratio": 1.5579710144927537, "no_speech_prob": 1.862576391431503e-05}, {"id": 46, "seek": 56800, "start": 568.0, "end": 585.0, "text": " So when working out an estimate of the gradient based on the entire corpus, you simply take one center word or a small batch like 32 center words, and you work out an estimate of the gradient based on them.", "tokens": [407, 562, 1364, 484, 364, 12539, 295, 264, 16235, 2361, 322, 264, 2302, 1181, 31624, 11, 291, 2935, 747, 472, 3056, 1349, 420, 257, 1359, 15245, 411, 8858, 3056, 2283, 11, 293, 291, 589, 484, 364, 12539, 295, 264, 16235, 2361, 322, 552, 13], "temperature": 0.0, "avg_logprob": -0.14351175228754678, "compression_ratio": 1.6612903225806452, "no_speech_prob": 3.269272201578133e-05}, {"id": 47, "seek": 58500, "start": 585.0, "end": 602.0, "text": " And that estimate of the gradient will be noisy and bad because you've only looked at a small fraction of the corpus rather than the whole corpus. But nevertheless, you can use that estimate of the gradient to update your theta parameters in exactly the same way.", "tokens": [400, 300, 12539, 295, 264, 16235, 486, 312, 24518, 293, 1578, 570, 291, 600, 787, 2956, 412, 257, 1359, 14135, 295, 264, 1181, 31624, 2831, 813, 264, 1379, 1181, 31624, 13, 583, 26924, 11, 291, 393, 764, 300, 12539, 295, 264, 16235, 281, 5623, 428, 9725, 9834, 294, 2293, 264, 912, 636, 13], "temperature": 0.0, "avg_logprob": -0.0692902406056722, "compression_ratio": 1.6540880503144655, "no_speech_prob": 5.2210405556252226e-05}, {"id": 48, "seek": 60200, "start": 602.0, "end": 623.0, "text": " So this is the algorithm that we can do. And so then if we have a billion word corpus, we can if we do it on each center word, we can make a billion updates to the parameters we pass through the corpus once rather than only making one more accurate update to the parameters.", "tokens": [407, 341, 307, 264, 9284, 300, 321, 393, 360, 13, 400, 370, 550, 498, 321, 362, 257, 5218, 1349, 1181, 31624, 11, 321, 393, 498, 321, 360, 309, 322, 1184, 3056, 1349, 11, 321, 393, 652, 257, 5218, 9205, 281, 264, 9834, 321, 1320, 807, 264, 1181, 31624, 1564, 2831, 813, 787, 1455, 472, 544, 8559, 5623, 281, 264, 9834, 13], "temperature": 0.0, "avg_logprob": -0.08644125278179463, "compression_ratio": 1.6809815950920246, "no_speech_prob": 6.700268568238243e-05}, {"id": 49, "seek": 62300, "start": 623.0, "end": 643.0, "text": " And at once you've been through the corpus. So overall, we can learn several orders of magnitude more quickly. And so this is the algorithm that you'll be using everywhere, including, you know, right right from the beginning from our assignments.", "tokens": [400, 412, 1564, 291, 600, 668, 807, 264, 1181, 31624, 13, 407, 4787, 11, 321, 393, 1466, 2940, 9470, 295, 15668, 544, 2661, 13, 400, 370, 341, 307, 264, 9284, 300, 291, 603, 312, 1228, 5315, 11, 3009, 11, 291, 458, 11, 558, 558, 490, 264, 2863, 490, 527, 22546, 13], "temperature": 0.0, "avg_logprob": -0.11619576540860263, "compression_ratio": 1.430232558139535, "no_speech_prob": 0.00019014342979062349}, {"id": 50, "seek": 64300, "start": 643.0, "end": 659.0, "text": " And then just an extra comment of more complicated stuff will come back to, I, I, this is the gradient descent is sort of performance hack it lets you learn much more quickly.", "tokens": [400, 550, 445, 364, 2857, 2871, 295, 544, 6179, 1507, 486, 808, 646, 281, 11, 286, 11, 286, 11, 341, 307, 264, 16235, 23475, 307, 1333, 295, 3389, 10339, 309, 6653, 291, 1466, 709, 544, 2661, 13], "temperature": 0.0, "avg_logprob": -0.26382402094399054, "compression_ratio": 1.3565891472868217, "no_speech_prob": 0.0002734134322963655}, {"id": 51, "seek": 65900, "start": 659.0, "end": 674.0, "text": " But it's not only a performance hack, you're going to have some quite counter intuitive properties and actually the fact that stochastic gradient descent is kind of noisy and bounces around as it does its thing.", "tokens": [583, 309, 311, 406, 787, 257, 3389, 10339, 11, 291, 434, 516, 281, 362, 512, 1596, 5682, 21769, 7221, 293, 767, 264, 1186, 300, 342, 8997, 2750, 16235, 23475, 307, 733, 295, 24518, 293, 46901, 926, 382, 309, 775, 1080, 551, 13], "temperature": 0.0, "avg_logprob": -0.20196779914524243, "compression_ratio": 1.435374149659864, "no_speech_prob": 1.0775996997836046e-05}, {"id": 52, "seek": 67400, "start": 674.0, "end": 691.0, "text": " And it actually means that in complex networks, it learns better solutions than if you were to run playing gradient descent very slowly. So you can both compute much more quickly and do a better job.", "tokens": [400, 309, 767, 1355, 300, 294, 3997, 9590, 11, 309, 27152, 1101, 6547, 813, 498, 291, 645, 281, 1190, 2433, 16235, 23475, 588, 5692, 13, 407, 291, 393, 1293, 14722, 709, 544, 2661, 293, 360, 257, 1101, 1691, 13], "temperature": 0.0, "avg_logprob": -0.132653369459995, "compression_ratio": 1.3724137931034484, "no_speech_prob": 9.451421647099778e-05}, {"id": 53, "seek": 69100, "start": 691.0, "end": 709.0, "text": " And then we have a final note on running stochastic gradients with word vectors. This is kind of an aside. But something to note is that if we're doing a stochastic gradient update based on one window, then actually in that window will have seen almost none of our parameters.", "tokens": [400, 550, 321, 362, 257, 2572, 3637, 322, 2614, 342, 8997, 2750, 2771, 2448, 365, 1349, 18875, 13, 639, 307, 733, 295, 364, 7359, 13, 583, 746, 281, 3637, 307, 300, 498, 321, 434, 884, 257, 342, 8997, 2750, 16235, 5623, 2361, 322, 472, 4910, 11, 550, 767, 294, 300, 4910, 486, 362, 1612, 1920, 6022, 295, 527, 9834, 13], "temperature": 0.0, "avg_logprob": -0.2232523262500763, "compression_ratio": 1.5681818181818181, "no_speech_prob": 5.052532651461661e-05}, {"id": 54, "seek": 70900, "start": 709.0, "end": 729.0, "text": " Because if we have a window of something like five words to be the side of the center word, we've seen at most 11 distinct word types. So we will have gradient information for those 11 words, but the other 100,000 odd words now vocabulary will have no gradient update information.", "tokens": [1436, 498, 321, 362, 257, 4910, 295, 746, 411, 1732, 2283, 281, 312, 264, 1252, 295, 264, 3056, 1349, 11, 321, 600, 1612, 412, 881, 2975, 10644, 1349, 3467, 13, 407, 321, 486, 362, 16235, 1589, 337, 729, 2975, 2283, 11, 457, 264, 661, 2319, 11, 1360, 7401, 2283, 586, 19864, 486, 362, 572, 16235, 5623, 1589, 13], "temperature": 0.0, "avg_logprob": -0.10876504067451723, "compression_ratio": 1.5555555555555556, "no_speech_prob": 3.01909058180172e-05}, {"id": 55, "seek": 72900, "start": 729.0, "end": 745.0, "text": " So it will be a very, very sparse gradient update. So if you're only thinking math, you can just have your entire gradient and use the equation that I showed before.", "tokens": [407, 309, 486, 312, 257, 588, 11, 588, 637, 11668, 16235, 5623, 13, 407, 498, 291, 434, 787, 1953, 5221, 11, 291, 393, 445, 362, 428, 2302, 16235, 293, 764, 264, 5367, 300, 286, 4712, 949, 13], "temperature": 0.0, "avg_logprob": -0.0695340691543207, "compression_ratio": 1.3095238095238095, "no_speech_prob": 0.00012695258192252368}, {"id": 56, "seek": 74500, "start": 745.0, "end": 764.0, "text": " But if you're thinking systems optimization, then you'd want to think, well, actually, I only want to update the parameters for a few words and there have to be and there are much more efficient ways that I could do that.", "tokens": [583, 498, 291, 434, 1953, 3652, 19618, 11, 550, 291, 1116, 528, 281, 519, 11, 731, 11, 767, 11, 286, 787, 528, 281, 5623, 264, 9834, 337, 257, 1326, 2283, 293, 456, 362, 281, 312, 293, 456, 366, 709, 544, 7148, 2098, 300, 286, 727, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.08463445993570182, "compression_ratio": 1.4350649350649352, "no_speech_prob": 7.09922969690524e-05}, {"id": 57, "seek": 76400, "start": 764.0, "end": 778.0, "text": " So here's sort, this is another aside will be useful for assignment. So I will say it up until now when I presented word vectors, I presented them as column vectors.", "tokens": [407, 510, 311, 1333, 11, 341, 307, 1071, 7359, 486, 312, 4420, 337, 15187, 13, 407, 286, 486, 584, 309, 493, 1826, 586, 562, 286, 8212, 1349, 18875, 11, 286, 8212, 552, 382, 7738, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1954111337661743, "compression_ratio": 1.3524590163934427, "no_speech_prob": 0.0001249151537194848}, {"id": 58, "seek": 77800, "start": 778.0, "end": 796.0, "text": " And that makes the most sense if you think about it as a piece of math, whereas actually in all common deep learning packages, including PyTorch that we're using word vectors are actually represented as row vectors.", "tokens": [400, 300, 1669, 264, 881, 2020, 498, 291, 519, 466, 309, 382, 257, 2522, 295, 5221, 11, 9735, 767, 294, 439, 2689, 2452, 2539, 17401, 11, 3009, 9953, 51, 284, 339, 300, 321, 434, 1228, 1349, 18875, 366, 767, 10379, 382, 5386, 18875, 13], "temperature": 0.0, "avg_logprob": -0.15272275606791177, "compression_ratio": 1.4052287581699345, "no_speech_prob": 5.8207751862937585e-05}, {"id": 59, "seek": 79600, "start": 796.0, "end": 815.0, "text": " And if you remember back to the representation of matrices and CS107 or something like that, that you'll know that that's then obviously efficient for representing words, because then you can access an entire word vector as a continuous range of memory.", "tokens": [400, 498, 291, 1604, 646, 281, 264, 10290, 295, 32284, 293, 9460, 3279, 22, 420, 746, 411, 300, 11, 300, 291, 603, 458, 300, 300, 311, 550, 2745, 7148, 337, 13460, 2283, 11, 570, 550, 291, 393, 2105, 364, 2302, 1349, 8062, 382, 257, 10957, 3613, 295, 4675, 13], "temperature": 0.0, "avg_logprob": -0.08851256460513708, "compression_ratio": 1.4540229885057472, "no_speech_prob": 6.489806401077658e-05}, {"id": 60, "seek": 81500, "start": 815.0, "end": 827.0, "text": " So actually, I'm going to throw in a few and four train. Anyway, so actually our word vectors will be row vectors when you look at those inside PyTorch.", "tokens": [407, 767, 11, 286, 478, 516, 281, 3507, 294, 257, 1326, 293, 1451, 3847, 13, 5684, 11, 370, 767, 527, 1349, 18875, 486, 312, 5386, 18875, 562, 291, 574, 412, 729, 1854, 9953, 51, 284, 339, 13], "temperature": 0.0, "avg_logprob": -0.35175075269725226, "compression_ratio": 1.4919786096256684, "no_speech_prob": 3.815747913904488e-05}, {"id": 61, "seek": 81500, "start": 827.0, "end": 839.0, "text": " Okay, now I wanted to say a bit more about the word to veck algorithm family and also what you're going to do in homework too.", "tokens": [1033, 11, 586, 286, 1415, 281, 584, 257, 857, 544, 466, 264, 1349, 281, 1241, 547, 9284, 1605, 293, 611, 437, 291, 434, 516, 281, 360, 294, 14578, 886, 13], "temperature": 0.0, "avg_logprob": -0.35175075269725226, "compression_ratio": 1.4919786096256684, "no_speech_prob": 3.815747913904488e-05}, {"id": 62, "seek": 83900, "start": 839.0, "end": 849.0, "text": " If you're still meant to be working on homework one, which remembers to next Tuesday, that really actually with today's content, we're starting into homework too.", "tokens": [759, 291, 434, 920, 4140, 281, 312, 1364, 322, 14578, 472, 11, 597, 26228, 281, 958, 10017, 11, 300, 534, 767, 365, 965, 311, 2701, 11, 321, 434, 2891, 666, 14578, 886, 13], "temperature": 0.0, "avg_logprob": -0.16876404102032, "compression_ratio": 1.5642458100558658, "no_speech_prob": 7.570441084681079e-05}, {"id": 63, "seek": 83900, "start": 849.0, "end": 856.0, "text": " And I'll kind of go through the first part of homework to today and the other stuff you need to know for homework to.", "tokens": [400, 286, 603, 733, 295, 352, 807, 264, 700, 644, 295, 14578, 281, 965, 293, 264, 661, 1507, 291, 643, 281, 458, 337, 14578, 281, 13], "temperature": 0.0, "avg_logprob": -0.16876404102032, "compression_ratio": 1.5642458100558658, "no_speech_prob": 7.570441084681079e-05}, {"id": 64, "seek": 85600, "start": 856.0, "end": 876.0, "text": " I mentioned briefly the idea that we have two separate vectors for each word type, the center vector and the outside vectors, and we just average them both at the end, they're similar, but not identical for multiple reasons, including the random initialization and the stochastic gradient descent.", "tokens": [286, 2835, 10515, 264, 1558, 300, 321, 362, 732, 4994, 18875, 337, 1184, 1349, 2010, 11, 264, 3056, 8062, 293, 264, 2380, 18875, 11, 293, 321, 445, 4274, 552, 1293, 412, 264, 917, 11, 436, 434, 2531, 11, 457, 406, 14800, 337, 3866, 4112, 11, 3009, 264, 4974, 5883, 2144, 293, 264, 342, 8997, 2750, 16235, 23475, 13], "temperature": 0.0, "avg_logprob": -0.09226342939561413, "compression_ratio": 1.563157894736842, "no_speech_prob": 1.3620502613775898e-05}, {"id": 65, "seek": 87600, "start": 876.0, "end": 890.0, "text": " You can implement a word to veck algorithm with just one vector per word, and actually if you do work slightly better, but it makes the algorithm much more complicated.", "tokens": [509, 393, 4445, 257, 1349, 281, 1241, 547, 9284, 365, 445, 472, 8062, 680, 1349, 11, 293, 767, 498, 291, 360, 589, 4748, 1101, 11, 457, 309, 1669, 264, 9284, 709, 544, 6179, 13], "temperature": 0.0, "avg_logprob": -0.15953360105815687, "compression_ratio": 1.344, "no_speech_prob": 2.627057438076008e-05}, {"id": 66, "seek": 89000, "start": 890.0, "end": 917.0, "text": " The reason for that is sometimes you'll have the same word type as the center word and the context word, and that means that when you're doing your calculus at that point, you've then got this sort of messy case that just for that word, you're getting an x squared turn, sorry, or dot product, you're getting a dot product of x dot x term, which makes it sort of much messier to work out.", "tokens": [440, 1778, 337, 300, 307, 2171, 291, 603, 362, 264, 912, 1349, 2010, 382, 264, 3056, 1349, 293, 264, 4319, 1349, 11, 293, 300, 1355, 300, 562, 291, 434, 884, 428, 33400, 412, 300, 935, 11, 291, 600, 550, 658, 341, 1333, 295, 16191, 1389, 300, 445, 337, 300, 1349, 11, 291, 434, 1242, 364, 2031, 8889, 1261, 11, 2597, 11, 420, 5893, 1674, 11, 291, 434, 1242, 257, 5893, 1674, 295, 2031, 5893, 2031, 1433, 11, 597, 1669, 309, 1333, 295, 709, 2082, 811, 281, 589, 484, 13], "temperature": 0.0, "avg_logprob": -0.17342462847309728, "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.00010223555727861822}, {"id": 67, "seek": 91700, "start": 917.0, "end": 922.0, "text": " And we use this sort of simple optimization of having two vectors per word.", "tokens": [400, 321, 764, 341, 1333, 295, 2199, 19618, 295, 1419, 732, 18875, 680, 1349, 13], "temperature": 0.0, "avg_logprob": -0.24365932146708172, "compression_ratio": 1.4171779141104295, "no_speech_prob": 0.00010382034088252112}, {"id": 68, "seek": 91700, "start": 922.0, "end": 939.0, "text": " Okay, so for the word to veck model as introduced in the meek of it, our paper in 2013, it wasn't really just one algorithm, it was a family of algorithms.", "tokens": [1033, 11, 370, 337, 264, 1349, 281, 1241, 547, 2316, 382, 7268, 294, 264, 385, 916, 295, 309, 11, 527, 3035, 294, 9012, 11, 309, 2067, 380, 534, 445, 472, 9284, 11, 309, 390, 257, 1605, 295, 14642, 13], "temperature": 0.0, "avg_logprob": -0.24365932146708172, "compression_ratio": 1.4171779141104295, "no_speech_prob": 0.00010382034088252112}, {"id": 69, "seek": 93900, "start": 939.0, "end": 949.0, "text": " There were two basic model variants, one was called the skip gram model, which is the one that I've explained to you that.", "tokens": [821, 645, 732, 3875, 2316, 21669, 11, 472, 390, 1219, 264, 10023, 21353, 2316, 11, 597, 307, 264, 472, 300, 286, 600, 8825, 281, 291, 300, 13], "temperature": 0.0, "avg_logprob": -0.23252591779155116, "compression_ratio": 1.22, "no_speech_prob": 2.1757334252470173e-05}, {"id": 70, "seek": 94900, "start": 949.0, "end": 969.0, "text": " The other one was called the continuous bag of words model, C bow, and in this one, you predict the center word from a bag of context words.", "tokens": [440, 661, 472, 390, 1219, 264, 10957, 3411, 295, 2283, 2316, 11, 383, 4503, 11, 293, 294, 341, 472, 11, 291, 6069, 264, 3056, 1349, 490, 257, 3411, 295, 4319, 2283, 13], "temperature": 0.0, "avg_logprob": -0.3231041696336534, "compression_ratio": 1.2844036697247707, "no_speech_prob": 3.645187462097965e-05}, {"id": 71, "seek": 96900, "start": 969.0, "end": 980.0, "text": " The skip gram one is more natural in various ways, so it's sort of normally the one that people have gravitated to and subsequent work.", "tokens": [440, 10023, 21353, 472, 307, 544, 3303, 294, 3683, 2098, 11, 370, 309, 311, 1333, 295, 5646, 264, 472, 300, 561, 362, 7427, 18266, 281, 293, 19962, 589, 13], "temperature": 0.0, "avg_logprob": -0.11064964182236615, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.00011401424853829667}, {"id": 72, "seek": 96900, "start": 980.0, "end": 994.0, "text": " But then as to how you train this model, what I've presented so far is the naive softmax equation, which is a simple but relatively expensive training method.", "tokens": [583, 550, 382, 281, 577, 291, 3847, 341, 2316, 11, 437, 286, 600, 8212, 370, 1400, 307, 264, 29052, 2787, 41167, 5367, 11, 597, 307, 257, 2199, 457, 7226, 5124, 3097, 3170, 13], "temperature": 0.0, "avg_logprob": -0.11064964182236615, "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.00011401424853829667}, {"id": 73, "seek": 99400, "start": 994.0, "end": 1002.0, "text": " So that isn't really what they suggest using in your paper in the paper, they suggest using a method that's called negative sampling.", "tokens": [407, 300, 1943, 380, 534, 437, 436, 3402, 1228, 294, 428, 3035, 294, 264, 3035, 11, 436, 3402, 1228, 257, 3170, 300, 311, 1219, 3671, 21179, 13], "temperature": 0.0, "avg_logprob": -0.1115870648119823, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.0001766047498676926}, {"id": 74, "seek": 99400, "start": 1002.0, "end": 1009.0, "text": " So an acronym you'll see sometimes is SGNS, which means skip grams negative sampling.", "tokens": [407, 364, 39195, 291, 603, 536, 2171, 307, 34520, 42003, 11, 597, 1355, 10023, 11899, 3671, 21179, 13], "temperature": 0.0, "avg_logprob": -0.1115870648119823, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.0001766047498676926}, {"id": 75, "seek": 99400, "start": 1009.0, "end": 1019.0, "text": " So let me just say a little bit about what this is, but actually doing the skip gram model with negative sampling is the part of homework too.", "tokens": [407, 718, 385, 445, 584, 257, 707, 857, 466, 437, 341, 307, 11, 457, 767, 884, 264, 10023, 21353, 2316, 365, 3671, 21179, 307, 264, 644, 295, 14578, 886, 13], "temperature": 0.0, "avg_logprob": -0.1115870648119823, "compression_ratio": 1.7320574162679425, "no_speech_prob": 0.0001766047498676926}, {"id": 76, "seek": 101900, "start": 1019.0, "end": 1035.0, "text": " So you'll get to know this model well. So the point is that if you use this naive softmax, you know, even though people commonly do use this naive softmax in various neural net models, that working out the denominator is pretty expensive.", "tokens": [407, 291, 603, 483, 281, 458, 341, 2316, 731, 13, 407, 264, 935, 307, 300, 498, 291, 764, 341, 29052, 2787, 41167, 11, 291, 458, 11, 754, 1673, 561, 12719, 360, 764, 341, 29052, 2787, 41167, 294, 3683, 18161, 2533, 5245, 11, 300, 1364, 484, 264, 20687, 307, 1238, 5124, 13], "temperature": 0.0, "avg_logprob": -0.1002002443586077, "compression_ratio": 1.6333333333333333, "no_speech_prob": 6.694474723190069e-05}, {"id": 77, "seek": 101900, "start": 1035.0, "end": 1043.0, "text": " And that's because you have to iterate over every word in the vocabulary and workout these dot products.", "tokens": [400, 300, 311, 570, 291, 362, 281, 44497, 670, 633, 1349, 294, 264, 19864, 293, 12169, 613, 5893, 3383, 13], "temperature": 0.0, "avg_logprob": -0.1002002443586077, "compression_ratio": 1.6333333333333333, "no_speech_prob": 6.694474723190069e-05}, {"id": 78, "seek": 104300, "start": 1043.0, "end": 1055.0, "text": " So if you have a hundred thousand word vocabulary, you have to do a hundred thousand dot products to work out the denominator and that seems a little bit of a shame.", "tokens": [407, 498, 291, 362, 257, 3262, 4714, 1349, 19864, 11, 291, 362, 281, 360, 257, 3262, 4714, 5893, 3383, 281, 589, 484, 264, 20687, 293, 300, 2544, 257, 707, 857, 295, 257, 10069, 13], "temperature": 0.0, "avg_logprob": -0.11489990949630738, "compression_ratio": 1.6418604651162791, "no_speech_prob": 7.024608930805698e-05}, {"id": 79, "seek": 104300, "start": 1055.0, "end": 1072.0, "text": " And so instead of that, the idea of negative sampling is we're instead of using this softmax, we're going to train binary logistic regression models for both the true pair of center word.", "tokens": [400, 370, 2602, 295, 300, 11, 264, 1558, 295, 3671, 21179, 307, 321, 434, 2602, 295, 1228, 341, 2787, 41167, 11, 321, 434, 516, 281, 3847, 17434, 3565, 3142, 24590, 5245, 337, 1293, 264, 2074, 6119, 295, 3056, 1349, 13], "temperature": 0.0, "avg_logprob": -0.11489990949630738, "compression_ratio": 1.6418604651162791, "no_speech_prob": 7.024608930805698e-05}, {"id": 80, "seek": 107200, "start": 1072.0, "end": 1087.0, "text": " And the context word versus noise pairs where we keep the true center word and we just randomly sample words from the vocabulary.", "tokens": [400, 264, 4319, 1349, 5717, 5658, 15494, 689, 321, 1066, 264, 2074, 3056, 1349, 293, 321, 445, 16979, 6889, 2283, 490, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.11397905009133476, "compression_ratio": 1.2647058823529411, "no_speech_prob": 0.00012317518121562898}, {"id": 81, "seek": 108700, "start": 1087.0, "end": 1102.0, "text": " And as presented in the paper, the idea is like this. So overall, what we want to optimize is still an average of the loss for each particular center word.", "tokens": [400, 382, 8212, 294, 264, 3035, 11, 264, 1558, 307, 411, 341, 13, 407, 4787, 11, 437, 321, 528, 281, 19719, 307, 920, 364, 4274, 295, 264, 4470, 337, 1184, 1729, 3056, 1349, 13], "temperature": 0.0, "avg_logprob": -0.11433607532132056, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.4225848392234184e-05}, {"id": 82, "seek": 108700, "start": 1102.0, "end": 1108.0, "text": " But for when we're working out the loss for each particular center word, we're going to work out.", "tokens": [583, 337, 562, 321, 434, 1364, 484, 264, 4470, 337, 1184, 1729, 3056, 1349, 11, 321, 434, 516, 281, 589, 484, 13], "temperature": 0.0, "avg_logprob": -0.11433607532132056, "compression_ratio": 1.6428571428571428, "no_speech_prob": 2.4225848392234184e-05}, {"id": 83, "seek": 110800, "start": 1108.0, "end": 1120.0, "text": " So the loss for each particular center word and each particular window, we're going to take the dot product as before of the center word and the outside word.", "tokens": [407, 264, 4470, 337, 1184, 1729, 3056, 1349, 293, 1184, 1729, 4910, 11, 321, 434, 516, 281, 747, 264, 5893, 1674, 382, 949, 295, 264, 3056, 1349, 293, 264, 2380, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1241116680941739, "compression_ratio": 1.7927927927927927, "no_speech_prob": 2.928993671957869e-05}, {"id": 84, "seek": 110800, "start": 1120.0, "end": 1134.0, "text": " And that's sort of the main quantity. But now instead of using that inside the softmax, we're going to put it through the logistic function, which is sometimes also often also called the sigmoid function, the name logistic is more precise.", "tokens": [400, 300, 311, 1333, 295, 264, 2135, 11275, 13, 583, 586, 2602, 295, 1228, 300, 1854, 264, 2787, 41167, 11, 321, 434, 516, 281, 829, 309, 807, 264, 3565, 3142, 2445, 11, 597, 307, 2171, 611, 2049, 611, 1219, 264, 4556, 3280, 327, 2445, 11, 264, 1315, 3565, 3142, 307, 544, 13600, 13], "temperature": 0.0, "avg_logprob": -0.1241116680941739, "compression_ratio": 1.7927927927927927, "no_speech_prob": 2.928993671957869e-05}, {"id": 85, "seek": 113400, "start": 1134.0, "end": 1145.0, "text": " So that's this function here. So the logistic function is a handy function that will map any real number to a probability between zero and one open interval.", "tokens": [407, 300, 311, 341, 2445, 510, 13, 407, 264, 3565, 3142, 2445, 307, 257, 13239, 2445, 300, 486, 4471, 604, 957, 1230, 281, 257, 8482, 1296, 4018, 293, 472, 1269, 15035, 13], "temperature": 0.0, "avg_logprob": -0.09539214635299424, "compression_ratio": 1.6387096774193548, "no_speech_prob": 8.880306995706633e-05}, {"id": 86, "seek": 113400, "start": 1145.0, "end": 1153.0, "text": " So basically if the dot product is large, the logistic of the dot product will be virtually one.", "tokens": [407, 1936, 498, 264, 5893, 1674, 307, 2416, 11, 264, 3565, 3142, 295, 264, 5893, 1674, 486, 312, 14103, 472, 13], "temperature": 0.0, "avg_logprob": -0.09539214635299424, "compression_ratio": 1.6387096774193548, "no_speech_prob": 8.880306995706633e-05}, {"id": 87, "seek": 115300, "start": 1153.0, "end": 1173.0, "text": " Okay, so we want this to be large. And then what we'd like is on average, we'd like the dot product between the center word and words that we just chose randomly, I, they most likely didn't actually occur in the context of the center word to be small.", "tokens": [1033, 11, 370, 321, 528, 341, 281, 312, 2416, 13, 400, 550, 437, 321, 1116, 411, 307, 322, 4274, 11, 321, 1116, 411, 264, 5893, 1674, 1296, 264, 3056, 1349, 293, 2283, 300, 321, 445, 5111, 16979, 11, 286, 11, 436, 881, 3700, 994, 380, 767, 5160, 294, 264, 4319, 295, 264, 3056, 1349, 281, 312, 1359, 13], "temperature": 0.0, "avg_logprob": -0.09416040297477477, "compression_ratio": 1.5304878048780488, "no_speech_prob": 3.8207334000617266e-05}, {"id": 88, "seek": 117300, "start": 1173.0, "end": 1192.0, "text": " And there's just one little trick of how this is done, which is this sigmoid function is symmetric. And so if we want this probability to be small, we can take the negative of the dot product.", "tokens": [400, 456, 311, 445, 472, 707, 4282, 295, 577, 341, 307, 1096, 11, 597, 307, 341, 4556, 3280, 327, 2445, 307, 32330, 13, 400, 370, 498, 321, 528, 341, 8482, 281, 312, 1359, 11, 321, 393, 747, 264, 3671, 295, 264, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.07152177890141805, "compression_ratio": 1.381294964028777, "no_speech_prob": 0.0001485661487095058}, {"id": 89, "seek": 119200, "start": 1192.0, "end": 1205.0, "text": " And wanting it to be over here, the product, the dot product of random word and the center word is a negative number. And so then we're going to take the negation of that.", "tokens": [400, 7935, 309, 281, 312, 670, 510, 11, 264, 1674, 11, 264, 5893, 1674, 295, 4974, 1349, 293, 264, 3056, 1349, 307, 257, 3671, 1230, 13, 400, 370, 550, 321, 434, 516, 281, 747, 264, 2485, 399, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.09218640113944437, "compression_ratio": 1.5961538461538463, "no_speech_prob": 5.055785004515201e-05}, {"id": 90, "seek": 119200, "start": 1205.0, "end": 1209.0, "text": " And then again, once we put that through the sigmoid, we'd like a big number.", "tokens": [400, 550, 797, 11, 1564, 321, 829, 300, 807, 264, 4556, 3280, 327, 11, 321, 1116, 411, 257, 955, 1230, 13], "temperature": 0.0, "avg_logprob": -0.09218640113944437, "compression_ratio": 1.5961538461538463, "no_speech_prob": 5.055785004515201e-05}, {"id": 91, "seek": 120900, "start": 1209.0, "end": 1226.0, "text": " Okay, so the way they're presenting things, they're actually maximizing this quantity. But if I go back to making it a bit more similar to the way we had written things weird worked with minimizing the negative log likelihood.", "tokens": [1033, 11, 370, 264, 636, 436, 434, 15578, 721, 11, 436, 434, 767, 5138, 3319, 341, 11275, 13, 583, 498, 286, 352, 646, 281, 1455, 309, 257, 857, 544, 2531, 281, 264, 636, 321, 632, 3720, 721, 3657, 2732, 365, 46608, 264, 3671, 3565, 22119, 13], "temperature": 0.0, "avg_logprob": -0.10763373374938964, "compression_ratio": 1.486842105263158, "no_speech_prob": 9.002707520266995e-05}, {"id": 92, "seek": 122600, "start": 1226.0, "end": 1243.0, "text": " It, it looks like this. So we're taking the negative log likelihood of this, the sigmoid of the dot product. Again, negative log likelihood, we're using the same negator dot product through the sigmoid.", "tokens": [467, 11, 309, 1542, 411, 341, 13, 407, 321, 434, 1940, 264, 3671, 3565, 22119, 295, 341, 11, 264, 4556, 3280, 327, 295, 264, 5893, 1674, 13, 3764, 11, 3671, 3565, 22119, 11, 321, 434, 1228, 264, 912, 2485, 1639, 5893, 1674, 807, 264, 4556, 3280, 327, 13], "temperature": 0.0, "avg_logprob": -0.12293067345252404, "compression_ratio": 1.6694214876033058, "no_speech_prob": 5.910752952331677e-05}, {"id": 93, "seek": 124300, "start": 1243.0, "end": 1257.0, "text": " And then we're going to work out this quantity for a handful of.", "tokens": [400, 550, 321, 434, 516, 281, 589, 484, 341, 11275, 337, 257, 16458, 295, 13], "temperature": 0.0, "avg_logprob": -0.23580782037032277, "compression_ratio": 0.9411764705882353, "no_speech_prob": 5.054946450400166e-05}, {"id": 94, "seek": 125700, "start": 1257.0, "end": 1275.0, "text": " And we're, this loss function is going to be minimized given this negation by making these dot products large and these dot products small means negative.", "tokens": [400, 321, 434, 11, 341, 4470, 2445, 307, 516, 281, 312, 4464, 1602, 2212, 341, 2485, 399, 538, 1455, 613, 5893, 3383, 2416, 293, 613, 5893, 3383, 1359, 1355, 3671, 13], "temperature": 0.0, "avg_logprob": -0.18313206263950893, "compression_ratio": 1.3873873873873874, "no_speech_prob": 9.436372783966362e-05}, {"id": 95, "seek": 127500, "start": 1275.0, "end": 1288.0, "text": " And then there's just then one other trick that they use actually there's more than one other trick that's used in the word to vec paper to get it to perform well, but I'll only mention one of their other tricks here.", "tokens": [400, 550, 456, 311, 445, 550, 472, 661, 4282, 300, 436, 764, 767, 456, 311, 544, 813, 472, 661, 4282, 300, 311, 1143, 294, 264, 1349, 281, 42021, 3035, 281, 483, 309, 281, 2042, 731, 11, 457, 286, 603, 787, 2152, 472, 295, 641, 661, 11733, 510, 13], "temperature": 0.0, "avg_logprob": -0.28443461198073167, "compression_ratio": 1.6074074074074074, "no_speech_prob": 0.00022217196237761527}, {"id": 96, "seek": 128800, "start": 1288.0, "end": 1309.0, "text": " When they sample the words, they don't simply just sample the words based on their probability of occurrence in the corpus or uniformly what they do is they start with what we call the unigram distribution of words. So that is how often words actually occur in our big corpus.", "tokens": [1133, 436, 6889, 264, 2283, 11, 436, 500, 380, 2935, 445, 6889, 264, 2283, 2361, 322, 641, 8482, 295, 36122, 294, 264, 1181, 31624, 420, 48806, 437, 436, 360, 307, 436, 722, 365, 437, 321, 818, 264, 517, 33737, 7316, 295, 2283, 13, 407, 300, 307, 577, 2049, 2283, 767, 5160, 294, 527, 955, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.08802857164476739, "compression_ratio": 1.6331360946745561, "no_speech_prob": 4.462363722268492e-05}, {"id": 97, "seek": 130900, "start": 1309.0, "end": 1321.0, "text": " So if you have a billion word corpus and a particular word occurred 90 times in it, you're taking 90 divided by a billion. And so that's the unigram probability of the word.", "tokens": [407, 498, 291, 362, 257, 5218, 1349, 1181, 31624, 293, 257, 1729, 1349, 11068, 4289, 1413, 294, 309, 11, 291, 434, 1940, 4289, 6666, 538, 257, 5218, 13, 400, 370, 300, 311, 264, 517, 33737, 8482, 295, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.08748039332303134, "compression_ratio": 1.3106060606060606, "no_speech_prob": 8.745559898670763e-05}, {"id": 98, "seek": 132100, "start": 1321.0, "end": 1344.0, "text": " But what they then do is that they take that to the three quarters power and the effect of that three quarters power, which is then re normalized to make a probability distribution with the Z kind of like we saw last time with the softmax by taking the three quarters power that has the effect of dampening the difference between common and rare words.", "tokens": [583, 437, 436, 550, 360, 307, 300, 436, 747, 300, 281, 264, 1045, 20612, 1347, 293, 264, 1802, 295, 300, 1045, 20612, 1347, 11, 597, 307, 550, 319, 48704, 281, 652, 257, 8482, 7316, 365, 264, 1176, 733, 295, 411, 321, 1866, 1036, 565, 365, 264, 2787, 41167, 538, 1940, 264, 1045, 20612, 1347, 300, 575, 264, 1802, 295, 19498, 4559, 264, 2649, 1296, 2689, 293, 5892, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1235851131073416, "compression_ratio": 1.8144329896907216, "no_speech_prob": 2.2812313545728102e-05}, {"id": 99, "seek": 134400, "start": 1344.0, "end": 1357.0, "text": " So that less frequent words are sampled somewhat more often, but still not nearly as much as they would be if you just use something like a uniform distribution over the vocabulary.", "tokens": [407, 300, 1570, 18004, 2283, 366, 3247, 15551, 8344, 544, 2049, 11, 457, 920, 406, 6217, 382, 709, 382, 436, 576, 312, 498, 291, 445, 764, 746, 411, 257, 9452, 7316, 670, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.16374419285700872, "compression_ratio": 1.3507462686567164, "no_speech_prob": 7.130991434678435e-05}, {"id": 100, "seek": 135700, "start": 1357.0, "end": 1377.0, "text": " So that's basically everything to say about the basics of how we have this very simple neural network algorithm word to vac and how we can train it and learn word vectors.", "tokens": [407, 300, 311, 1936, 1203, 281, 584, 466, 264, 14688, 295, 577, 321, 362, 341, 588, 2199, 18161, 3209, 9284, 1349, 281, 2842, 293, 577, 321, 393, 3847, 309, 293, 1466, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1099587365200645, "compression_ratio": 1.3790322580645162, "no_speech_prob": 0.0001249284396180883}, {"id": 101, "seek": 137700, "start": 1377.0, "end": 1391.0, "text": " So the next bit, what I want to do is step back a bit and say, well, here's an algorithm that I've shown you that works great. What else could we have done and what can we say about that.", "tokens": [407, 264, 958, 857, 11, 437, 286, 528, 281, 360, 307, 1823, 646, 257, 857, 293, 584, 11, 731, 11, 510, 311, 364, 9284, 300, 286, 600, 4898, 291, 300, 1985, 869, 13, 708, 1646, 727, 321, 362, 1096, 293, 437, 393, 321, 584, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.13965332741830863, "compression_ratio": 1.375, "no_speech_prob": 5.46657684026286e-05}, {"id": 102, "seek": 139100, "start": 1391.0, "end": 1419.0, "text": " The first thing that you might think about is, well, here's this funny iterative algorithm to give you word vectors. You know, if we have a lot of words in a corpus, seems like a more obvious thing that we could do is just look at the counts of how words occur with each other and build a matrix of counts.", "tokens": [440, 700, 551, 300, 291, 1062, 519, 466, 307, 11, 731, 11, 510, 311, 341, 4074, 17138, 1166, 9284, 281, 976, 291, 1349, 18875, 13, 509, 458, 11, 498, 321, 362, 257, 688, 295, 2283, 294, 257, 1181, 31624, 11, 2544, 411, 257, 544, 6322, 551, 300, 321, 727, 360, 307, 445, 574, 412, 264, 14893, 295, 577, 2283, 5160, 365, 1184, 661, 293, 1322, 257, 8141, 295, 14893, 13], "temperature": 0.0, "avg_logprob": -0.08362856426754513, "compression_ratio": 1.5612244897959184, "no_speech_prob": 6.100258542574011e-05}, {"id": 103, "seek": 141900, "start": 1419.0, "end": 1431.0, "text": " So here's the idea of a color currents matrix. So I've got a teeny little corpus. I like deep learning. I like NLP. I enjoy flying.", "tokens": [407, 510, 311, 264, 1558, 295, 257, 2017, 30110, 8141, 13, 407, 286, 600, 658, 257, 48232, 707, 1181, 31624, 13, 286, 411, 2452, 2539, 13, 286, 411, 426, 45196, 13, 286, 2103, 7137, 13], "temperature": 0.0, "avg_logprob": -0.2776039319160657, "compression_ratio": 1.201834862385321, "no_speech_prob": 0.00016553908062633127}, {"id": 104, "seek": 143100, "start": 1431.0, "end": 1453.0, "text": " I can define a window size. I made my window simply size one to make it easy to fill in my matrix, symmetric, just like our word to back algorithm. And so then the counts in these cells are simply how often things that co occur in the window of size one.", "tokens": [286, 393, 6964, 257, 4910, 2744, 13, 286, 1027, 452, 4910, 2935, 2744, 472, 281, 652, 309, 1858, 281, 2836, 294, 452, 8141, 11, 32330, 11, 445, 411, 527, 1349, 281, 646, 9284, 13, 400, 370, 550, 264, 14893, 294, 613, 5438, 366, 2935, 577, 2049, 721, 300, 598, 5160, 294, 264, 4910, 295, 2744, 472, 13], "temperature": 0.0, "avg_logprob": -0.08840778225758036, "compression_ratio": 1.548780487804878, "no_speech_prob": 1.5423624063259922e-05}, {"id": 105, "seek": 145300, "start": 1453.0, "end": 1467.0, "text": " I like occurs twice. So we get twos in these cells because it's symmetric deep learning curves one. So we get one here and lots of other things occur zero.", "tokens": [286, 411, 11843, 6091, 13, 407, 321, 483, 683, 329, 294, 613, 5438, 570, 309, 311, 32330, 2452, 2539, 19490, 472, 13, 407, 321, 483, 472, 510, 293, 3195, 295, 661, 721, 5160, 4018, 13], "temperature": 0.0, "avg_logprob": -0.18374872207641602, "compression_ratio": 1.3362068965517242, "no_speech_prob": 4.327067290432751e-05}, {"id": 106, "seek": 146700, "start": 1467.0, "end": 1491.0, "text": " And build up a co-occurrence matrix like this. And well, these actually give us a representation of words as co-occurrence vectors. So I can take the word I with either a row or column vector since it's symmetric and say, OK, my representation of the word I is this row vector.", "tokens": [400, 1322, 493, 257, 598, 12, 905, 14112, 10760, 8141, 411, 341, 13, 400, 731, 11, 613, 767, 976, 505, 257, 10290, 295, 2283, 382, 598, 12, 905, 14112, 10760, 18875, 13, 407, 286, 393, 747, 264, 1349, 286, 365, 2139, 257, 5386, 420, 7738, 8062, 1670, 309, 311, 32330, 293, 584, 11, 2264, 11, 452, 10290, 295, 264, 1349, 286, 307, 341, 5386, 8062, 13], "temperature": 0.0, "avg_logprob": -0.16158582142421177, "compression_ratio": 1.5649717514124293, "no_speech_prob": 1.8330956663703546e-05}, {"id": 107, "seek": 149100, "start": 1491.0, "end": 1515.0, "text": " That is a representation of the word I and I think you can maybe convince yourself that the extent that words have similar meaning and usage. You'd sort of expect them to have somewhat similar vectors. Right. So if I have the word you as well on a larger corpus, you might expect I and you to have similar vectors because I like you like I enjoy you enjoy.", "tokens": [663, 307, 257, 10290, 295, 264, 1349, 286, 293, 286, 519, 291, 393, 1310, 13447, 1803, 300, 264, 8396, 300, 2283, 362, 2531, 3620, 293, 14924, 13, 509, 1116, 1333, 295, 2066, 552, 281, 362, 8344, 2531, 18875, 13, 1779, 13, 407, 498, 286, 362, 264, 1349, 291, 382, 731, 322, 257, 4833, 1181, 31624, 11, 291, 1062, 2066, 286, 293, 291, 281, 362, 2531, 18875, 570, 286, 411, 291, 411, 286, 2103, 291, 2103, 13], "temperature": 0.0, "avg_logprob": -0.12967480421066285, "compression_ratio": 1.6872037914691944, "no_speech_prob": 8.443595288554206e-05}, {"id": 108, "seek": 151500, "start": 1515.0, "end": 1523.0, "text": " You'd see the same kinds of possibilities. Hey, Chris, could you look into the answer some questions. Sure.", "tokens": [509, 1116, 536, 264, 912, 3685, 295, 12178, 13, 1911, 11, 6688, 11, 727, 291, 574, 666, 264, 1867, 512, 1651, 13, 4894, 13], "temperature": 0.0, "avg_logprob": -0.23803009383979884, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.00011772030120482668}, {"id": 109, "seek": 151500, "start": 1523.0, "end": 1529.0, "text": " Alright, so we got some questions from negative sort of the negative sampling sampling slides.", "tokens": [2798, 11, 370, 321, 658, 512, 1651, 490, 3671, 1333, 295, 264, 3671, 21179, 21179, 9788, 13], "temperature": 0.0, "avg_logprob": -0.23803009383979884, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.00011772030120482668}, {"id": 110, "seek": 151500, "start": 1529.0, "end": 1541.0, "text": " In particular, what's like, can you give some intuition for negative sampling? What is the negative sampling doing? And why do we only take one positive example? Those are two questions.", "tokens": [682, 1729, 11, 437, 311, 411, 11, 393, 291, 976, 512, 24002, 337, 3671, 21179, 30, 708, 307, 264, 3671, 21179, 884, 30, 400, 983, 360, 321, 787, 747, 472, 3353, 1365, 30, 3950, 366, 732, 1651, 13], "temperature": 0.0, "avg_logprob": -0.23803009383979884, "compression_ratio": 1.706140350877193, "no_speech_prob": 0.00011772030120482668}, {"id": 111, "seek": 154100, "start": 1541.0, "end": 1560.0, "text": " Answering the answer. Okay, that's a good question. Okay, I'll try and give more intuition. So is to work out something like what the softmax did in a much more efficient way.", "tokens": [24545, 278, 264, 1867, 13, 1033, 11, 300, 311, 257, 665, 1168, 13, 1033, 11, 286, 603, 853, 293, 976, 544, 24002, 13, 407, 307, 281, 589, 484, 746, 411, 437, 264, 2787, 41167, 630, 294, 257, 709, 544, 7148, 636, 13], "temperature": 0.0, "avg_logprob": -0.26409918328990106, "compression_ratio": 1.3358778625954197, "no_speech_prob": 0.00011376506881788373}, {"id": 112, "seek": 156000, "start": 1560.0, "end": 1574.0, "text": " So in the softmax, well, you wanted to give high probability to the in predicting the context, a context word that actually did appear with the center word.", "tokens": [407, 294, 264, 2787, 41167, 11, 731, 11, 291, 1415, 281, 976, 1090, 8482, 281, 264, 294, 32884, 264, 4319, 11, 257, 4319, 1349, 300, 767, 630, 4204, 365, 264, 3056, 1349, 13], "temperature": 0.0, "avg_logprob": -0.08709951993581411, "compression_ratio": 1.3448275862068966, "no_speech_prob": 7.479802297893912e-05}, {"id": 113, "seek": 157400, "start": 1574.0, "end": 1594.0, "text": " And well, the way you do that is by having the dot product between those two words be as big as possible and part of how, but you know, you're going to be sort of it's more than that because in the denominator, you're also working out the dot product with every other word in the vocabulary.", "tokens": [400, 731, 11, 264, 636, 291, 360, 300, 307, 538, 1419, 264, 5893, 1674, 1296, 729, 732, 2283, 312, 382, 955, 382, 1944, 293, 644, 295, 577, 11, 457, 291, 458, 11, 291, 434, 516, 281, 312, 1333, 295, 309, 311, 544, 813, 300, 570, 294, 264, 20687, 11, 291, 434, 611, 1364, 484, 264, 5893, 1674, 365, 633, 661, 1349, 294, 264, 19864, 13], "temperature": 0.0, "avg_logprob": -0.12259933913963428, "compression_ratio": 1.6166666666666667, "no_speech_prob": 2.7510899599292316e-05}, {"id": 114, "seek": 159400, "start": 1594.0, "end": 1612.0, "text": " So as well as wanting the dot product with the actual word that you see in the context to be big, you maximize your likelihood by making the dot products of other words that weren't in the context smaller because that's shrinking your denominator.", "tokens": [407, 382, 731, 382, 7935, 264, 5893, 1674, 365, 264, 3539, 1349, 300, 291, 536, 294, 264, 4319, 281, 312, 955, 11, 291, 19874, 428, 22119, 538, 1455, 264, 5893, 3383, 295, 661, 2283, 300, 4999, 380, 294, 264, 4319, 4356, 570, 300, 311, 41684, 428, 20687, 13], "temperature": 0.0, "avg_logprob": -0.07064762023779061, "compression_ratio": 1.6357615894039734, "no_speech_prob": 8.21143330540508e-05}, {"id": 115, "seek": 161200, "start": 1612.0, "end": 1634.0, "text": " And therefore, you've got a bigger number coming out and you're maximizing the loss. So even for the softmax, the general thing that you want to do to maximize it is have dot product with words actually in the context big dot product with words and not in the context be small to the extent possible.", "tokens": [400, 4412, 11, 291, 600, 658, 257, 3801, 1230, 1348, 484, 293, 291, 434, 5138, 3319, 264, 4470, 13, 407, 754, 337, 264, 2787, 41167, 11, 264, 2674, 551, 300, 291, 528, 281, 360, 281, 19874, 309, 307, 362, 5893, 1674, 365, 2283, 767, 294, 264, 4319, 955, 5893, 1674, 365, 2283, 293, 406, 294, 264, 4319, 312, 1359, 281, 264, 8396, 1944, 13], "temperature": 0.0, "avg_logprob": -0.09766939107109518, "compression_ratio": 1.6574585635359116, "no_speech_prob": 6.910035153850913e-05}, {"id": 116, "seek": 163400, "start": 1634.0, "end": 1644.0, "text": " And obviously you have to average this is best you can over all kinds of different contexts, because sometimes different words appear in different contexts, obviously.", "tokens": [400, 2745, 291, 362, 281, 4274, 341, 307, 1151, 291, 393, 670, 439, 3685, 295, 819, 30628, 11, 570, 2171, 819, 2283, 4204, 294, 819, 30628, 11, 2745, 13], "temperature": 0.0, "avg_logprob": -0.18840084435804835, "compression_ratio": 1.5389221556886228, "no_speech_prob": 5.219548984314315e-05}, {"id": 117, "seek": 163400, "start": 1644.0, "end": 1654.0, "text": " So, so the negative sampling as a way of therefore trying to maximize the same objective.", "tokens": [407, 11, 370, 264, 3671, 21179, 382, 257, 636, 295, 4412, 1382, 281, 19874, 264, 912, 10024, 13], "temperature": 0.0, "avg_logprob": -0.18840084435804835, "compression_ratio": 1.5389221556886228, "no_speech_prob": 5.219548984314315e-05}, {"id": 118, "seek": 165400, "start": 1654.0, "end": 1665.0, "text": " You know, for you only you only have one positive term because you're actually wanting to use the actual data. So you're not wanting wanting to invent data.", "tokens": [509, 458, 11, 337, 291, 787, 291, 787, 362, 472, 3353, 1433, 570, 291, 434, 767, 7935, 281, 764, 264, 3539, 1412, 13, 407, 291, 434, 406, 7935, 7935, 281, 7962, 1412, 13], "temperature": 0.0, "avg_logprob": -0.14310529155115928, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.00024094457330647856}, {"id": 119, "seek": 165400, "start": 1665.0, "end": 1674.0, "text": " So for working out the entire J we do do work this quantity out for every center word and every context word.", "tokens": [407, 337, 1364, 484, 264, 2302, 508, 321, 360, 360, 589, 341, 11275, 484, 337, 633, 3056, 1349, 293, 633, 4319, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14310529155115928, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.00024094457330647856}, {"id": 120, "seek": 167400, "start": 1674.0, "end": 1685.0, "text": " So you know we are iterating over the different words in the context window and then we're moving through positions in the corpus. So we're doing different VCs so you know gradually we do this.", "tokens": [407, 291, 458, 321, 366, 17138, 990, 670, 264, 819, 2283, 294, 264, 4319, 4910, 293, 550, 321, 434, 2684, 807, 8432, 294, 264, 1181, 31624, 13, 407, 321, 434, 884, 819, 691, 33290, 370, 291, 458, 13145, 321, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.15078124780764526, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.00034002147731371224}, {"id": 121, "seek": 167400, "start": 1685.0, "end": 1697.0, "text": " But for one particular center word and one particular context word, we only have one real piece of data that's positive. So that's all we use because we don't know what other words.", "tokens": [583, 337, 472, 1729, 3056, 1349, 293, 472, 1729, 4319, 1349, 11, 321, 787, 362, 472, 957, 2522, 295, 1412, 300, 311, 3353, 13, 407, 300, 311, 439, 321, 764, 570, 321, 500, 380, 458, 437, 661, 2283, 13], "temperature": 0.0, "avg_logprob": -0.15078124780764526, "compression_ratio": 1.728110599078341, "no_speech_prob": 0.00034002147731371224}, {"id": 122, "seek": 169700, "start": 1697.0, "end": 1710.0, "text": " So that should be counter positive words. Now for the negative words, you could just sample one negative word and that would probably work.", "tokens": [407, 300, 820, 312, 5682, 3353, 2283, 13, 823, 337, 264, 3671, 2283, 11, 291, 727, 445, 6889, 472, 3671, 1349, 293, 300, 576, 1391, 589, 13], "temperature": 0.0, "avg_logprob": -0.29019158886324975, "compression_ratio": 1.3365384615384615, "no_speech_prob": 0.00017622804443817586}, {"id": 123, "seek": 171000, "start": 1710.0, "end": 1732.0, "text": " So sort of a slightly better more stable sense of okay we'd like to in general have other words have low probability. It seems like you might be able to get better more stable results. If you're instead say let's have 10 or 15 sample negative words and indeed that's been found to be true.", "tokens": [407, 1333, 295, 257, 4748, 1101, 544, 8351, 2020, 295, 1392, 321, 1116, 411, 281, 294, 2674, 362, 661, 2283, 362, 2295, 8482, 13, 467, 2544, 411, 291, 1062, 312, 1075, 281, 483, 1101, 544, 8351, 3542, 13, 759, 291, 434, 2602, 584, 718, 311, 362, 1266, 420, 2119, 6889, 3671, 2283, 293, 6451, 300, 311, 668, 1352, 281, 312, 2074, 13], "temperature": 0.0, "avg_logprob": -0.14146424784804834, "compression_ratio": 1.553763440860215, "no_speech_prob": 9.8816366516985e-05}, {"id": 124, "seek": 173200, "start": 1732.0, "end": 1746.0, "text": " And for the negative words, well, it's easy to sample any number of random words you want. And at that point it's kind of a probabilistic argument. The words that you're sampling might not be actually bad words to appear in the context.", "tokens": [400, 337, 264, 3671, 2283, 11, 731, 11, 309, 311, 1858, 281, 6889, 604, 1230, 295, 4974, 2283, 291, 528, 13, 400, 412, 300, 935, 309, 311, 733, 295, 257, 31959, 3142, 6770, 13, 440, 2283, 300, 291, 434, 21179, 1062, 406, 312, 767, 1578, 2283, 281, 4204, 294, 264, 4319, 13], "temperature": 0.0, "avg_logprob": -0.1020163893699646, "compression_ratio": 1.4567901234567902, "no_speech_prob": 7.463384099537507e-05}, {"id": 125, "seek": 174600, "start": 1746.0, "end": 1762.0, "text": " They might actually be other words that are in the context, but 99.9% of the time they will be unlikely words to occur in the context. And so they're good ones to use. And yes, you only sample 10 or 15 of them.", "tokens": [814, 1062, 767, 312, 661, 2283, 300, 366, 294, 264, 4319, 11, 457, 11803, 13, 24, 4, 295, 264, 565, 436, 486, 312, 17518, 2283, 281, 5160, 294, 264, 4319, 13, 400, 370, 436, 434, 665, 2306, 281, 764, 13, 400, 2086, 11, 291, 787, 6889, 1266, 420, 2119, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.11643218994140625, "compression_ratio": 1.4093959731543624, "no_speech_prob": 7.460026972694322e-05}, {"id": 126, "seek": 176200, "start": 1762.0, "end": 1777.0, "text": " And it's enough to make progress because the center word is going to turn up on other occasions. And when it does your sample different words over here so that you gradually sample different parts of the space and start to learn.", "tokens": [400, 309, 311, 1547, 281, 652, 4205, 570, 264, 3056, 1349, 307, 516, 281, 1261, 493, 322, 661, 20641, 13, 400, 562, 309, 775, 428, 6889, 819, 2283, 670, 510, 370, 300, 291, 13145, 6889, 819, 3166, 295, 264, 1901, 293, 722, 281, 1466, 13], "temperature": 0.0, "avg_logprob": -0.1629832228835748, "compression_ratio": 1.5165562913907285, "no_speech_prob": 0.00017373243463225663}, {"id": 127, "seek": 177700, "start": 1777.0, "end": 1795.0, "text": " And it gives a representation of words as coerced current vectors. And just one more note on that. I mean, there are actually two ways that people have commonly made these coerced current matrices.", "tokens": [400, 309, 2709, 257, 10290, 295, 2283, 382, 598, 260, 1232, 2190, 18875, 13, 400, 445, 472, 544, 3637, 322, 300, 13, 286, 914, 11, 456, 366, 767, 732, 2098, 300, 561, 362, 12719, 1027, 613, 598, 260, 1232, 2190, 32284, 13], "temperature": 0.0, "avg_logprob": -0.2985837977865468, "compression_ratio": 1.437956204379562, "no_speech_prob": 6.288553413469344e-05}, {"id": 128, "seek": 179500, "start": 1795.0, "end": 1811.0, "text": " And for a response to what we've seen already that you use a window around the word, which is similar to word to veck. And that allows you to capture some locality and some of the sort of syntactic and semantic proximity that's more fine grained.", "tokens": [400, 337, 257, 4134, 281, 437, 321, 600, 1612, 1217, 300, 291, 764, 257, 4910, 926, 264, 1349, 11, 597, 307, 2531, 281, 1349, 281, 1241, 547, 13, 400, 300, 4045, 291, 281, 7983, 512, 1628, 1860, 293, 512, 295, 264, 1333, 295, 23980, 19892, 293, 47982, 27632, 300, 311, 544, 2489, 1295, 2001, 13], "temperature": 0.0, "avg_logprob": -0.17638427928342656, "compression_ratio": 1.50920245398773, "no_speech_prob": 1.8255921531817876e-05}, {"id": 129, "seek": 181100, "start": 1811.0, "end": 1825.0, "text": " And the way these current matrix, these are the often made is that normally documents have some structure, whether it's paragraphs or just actual web pages sort of size documents.", "tokens": [400, 264, 636, 613, 2190, 8141, 11, 613, 366, 264, 2049, 1027, 307, 300, 5646, 8512, 362, 512, 3877, 11, 1968, 309, 311, 48910, 420, 445, 3539, 3670, 7183, 1333, 295, 2744, 8512, 13], "temperature": 0.0, "avg_logprob": -0.21760336932014016, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.1024869056418538e-05}, {"id": 130, "seek": 181100, "start": 1825.0, "end": 1840.0, "text": " So you can just make your window size a paragraph for a whole web page and count current currents and those. And this is the kind of method that's often being used in information retrieval in methods like latent semantic analysis.", "tokens": [407, 291, 393, 445, 652, 428, 4910, 2744, 257, 18865, 337, 257, 1379, 3670, 3028, 293, 1207, 2190, 30110, 293, 729, 13, 400, 341, 307, 264, 733, 295, 3170, 300, 311, 2049, 885, 1143, 294, 1589, 19817, 3337, 294, 7150, 411, 48994, 47982, 5215, 13], "temperature": 0.0, "avg_logprob": -0.21760336932014016, "compression_ratio": 1.6872427983539096, "no_speech_prob": 2.1024869056418538e-05}, {"id": 131, "seek": 184000, "start": 1840.0, "end": 1857.0, "text": " Okay, so the question then is are these kind of count word vectors good things to use. Well, people have used them. They're not terrible. But they have certain problems.", "tokens": [1033, 11, 370, 264, 1168, 550, 307, 366, 613, 733, 295, 1207, 1349, 18875, 665, 721, 281, 764, 13, 1042, 11, 561, 362, 1143, 552, 13, 814, 434, 406, 6237, 13, 583, 436, 362, 1629, 2740, 13], "temperature": 0.0, "avg_logprob": -0.17729961581346465, "compression_ratio": 1.2900763358778626, "no_speech_prob": 9.752379264682531e-05}, {"id": 132, "seek": 185700, "start": 1857.0, "end": 1880.0, "text": " The kind of problems that they have, well, firstly, they're huge, though, very sparse. So this is back where I said before, if we had a vocabulary of half a million words, when then we have a half a million dimensional vector for each word, which is much, much bigger than the word vectors that we typically use.", "tokens": [440, 733, 295, 2740, 300, 436, 362, 11, 731, 11, 27376, 11, 436, 434, 2603, 11, 1673, 11, 588, 637, 11668, 13, 407, 341, 307, 646, 689, 286, 848, 949, 11, 498, 321, 632, 257, 19864, 295, 1922, 257, 2459, 2283, 11, 562, 550, 321, 362, 257, 1922, 257, 2459, 18795, 8062, 337, 1184, 1349, 11, 597, 307, 709, 11, 709, 3801, 813, 264, 1349, 18875, 300, 321, 5850, 764, 13], "temperature": 0.0, "avg_logprob": -0.10813606262207032, "compression_ratio": 1.5678391959798994, "no_speech_prob": 1.721165972412564e-05}, {"id": 133, "seek": 188000, "start": 1880.0, "end": 1899.0, "text": " And it also means that because we have these very high dimensional vectors that we have a lot of sparsity and a lot of randomness. So the results that you get tend to be noisier and less robust depending on what particular stuff was in the corpus.", "tokens": [400, 309, 611, 1355, 300, 570, 321, 362, 613, 588, 1090, 18795, 18875, 300, 321, 362, 257, 688, 295, 637, 685, 507, 293, 257, 688, 295, 4974, 1287, 13, 407, 264, 3542, 300, 291, 483, 3928, 281, 312, 572, 271, 811, 293, 1570, 13956, 5413, 322, 437, 1729, 1507, 390, 294, 264, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.0784113851644225, "compression_ratio": 1.5534591194968554, "no_speech_prob": 3.161747372359969e-05}, {"id": 134, "seek": 189900, "start": 1899.0, "end": 1921.0, "text": " So in general, people have found that you can get much better results by working with low dimensional vectors. So then the idea is we can store the most of the important information about the distribution of words in the context of other words in a fixed small number of dimensions, giving a dense vector.", "tokens": [407, 294, 2674, 11, 561, 362, 1352, 300, 291, 393, 483, 709, 1101, 3542, 538, 1364, 365, 2295, 18795, 18875, 13, 407, 550, 264, 1558, 307, 321, 393, 3531, 264, 881, 295, 264, 1021, 1589, 466, 264, 7316, 295, 2283, 294, 264, 4319, 295, 661, 2283, 294, 257, 6806, 1359, 1230, 295, 12819, 11, 2902, 257, 18011, 8062, 13], "temperature": 0.0, "avg_logprob": -0.0732350954933772, "compression_ratio": 1.5803108808290156, "no_speech_prob": 5.1400773372733966e-05}, {"id": 135, "seek": 192100, "start": 1921.0, "end": 1940.0, "text": " And then practice the dimensionality of the vectors that are used are normally somewhere between 25 and 1000. And so at that point, we need to use two, we need to use some way to reduce the dimensionality of our count co occurance vectors.", "tokens": [400, 550, 3124, 264, 10139, 1860, 295, 264, 18875, 300, 366, 1143, 366, 5646, 4079, 1296, 3552, 293, 9714, 13, 400, 370, 412, 300, 935, 11, 321, 643, 281, 764, 732, 11, 321, 643, 281, 764, 512, 636, 281, 5407, 264, 10139, 1860, 295, 527, 1207, 598, 5160, 719, 18875, 13], "temperature": 0.0, "avg_logprob": -0.13359049883755772, "compression_ratio": 1.614864864864865, "no_speech_prob": 2.3173397494247183e-05}, {"id": 136, "seek": 194000, "start": 1940.0, "end": 1969.0, "text": " So if you have a good memory from a linear algebra class, you hopefully saw singular value decomposition and it has various mathematical properties that I'm not going to talk about here of single singular value projection, giving you an optimal way under a certain definition of optimality of producing a reduced dimensionality matrix that maximally", "tokens": [407, 498, 291, 362, 257, 665, 4675, 490, 257, 8213, 21989, 1508, 11, 291, 4696, 1866, 20010, 2158, 48356, 293, 309, 575, 3683, 18894, 7221, 300, 286, 478, 406, 516, 281, 751, 466, 510, 295, 2167, 20010, 2158, 22743, 11, 2902, 291, 364, 16252, 636, 833, 257, 1629, 7123, 295, 5028, 1860, 295, 10501, 257, 9212, 10139, 1860, 8141, 300, 5138, 379], "temperature": 0.0, "avg_logprob": -0.0994362397627397, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.00023245016927830875}, {"id": 137, "seek": 196900, "start": 1969.0, "end": 1998.0, "text": " or sorry, pair of matrices that maximally well lets you recover the original matrix. But the idea of the singular value decomposition is you can take any matrix such as our count matrix and you can decompose it into three matrices, you a diagonal matrix sigma and a V transpose matrix.", "tokens": [420, 2597, 11, 6119, 295, 32284, 300, 5138, 379, 731, 6653, 291, 8114, 264, 3380, 8141, 13, 583, 264, 1558, 295, 264, 20010, 2158, 48356, 307, 291, 393, 747, 604, 8141, 1270, 382, 527, 1207, 8141, 293, 291, 393, 22867, 541, 309, 666, 1045, 32284, 11, 291, 257, 21539, 8141, 12771, 293, 257, 691, 25167, 8141, 13], "temperature": 0.0, "avg_logprob": -0.154463173913174, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.0003098906308878213}, {"id": 138, "seek": 199800, "start": 1998.0, "end": 2016.0, "text": " So this works for any shape. Now in these matrices, some parts of it, I never use because since this matrix is rectangular, there's nothing over here. And so this part of the V transpose matrix gets ignored.", "tokens": [407, 341, 1985, 337, 604, 3909, 13, 823, 294, 613, 32284, 11, 512, 3166, 295, 309, 11, 286, 1128, 764, 570, 1670, 341, 8141, 307, 31167, 11, 456, 311, 1825, 670, 510, 13, 400, 370, 341, 644, 295, 264, 691, 25167, 8141, 2170, 19735, 13], "temperature": 0.0, "avg_logprob": -0.2337471903586874, "compression_ratio": 1.3986486486486487, "no_speech_prob": 0.00015086344501469284}, {"id": 139, "seek": 201600, "start": 2016.0, "end": 2033.0, "text": " So I'm wanting to get smaller dimensional representations, what you do is take advantage of the fact that the singular values inside the diagonal sigma matrix are ordered from largest down to smallest.", "tokens": [407, 286, 478, 7935, 281, 483, 4356, 18795, 33358, 11, 437, 291, 360, 307, 747, 5002, 295, 264, 1186, 300, 264, 20010, 4190, 1854, 264, 21539, 12771, 8141, 366, 8866, 490, 6443, 760, 281, 16998, 13], "temperature": 0.0, "avg_logprob": -0.13917604684829712, "compression_ratio": 1.4055944055944056, "no_speech_prob": 3.6459277907852083e-05}, {"id": 140, "seek": 203300, "start": 2033.0, "end": 2050.0, "text": " So what we can do is just delete out more of the matrix of the delete out some singular values, which effectively means that in this product, some of you and some of the is also not used.", "tokens": [407, 437, 321, 393, 360, 307, 445, 12097, 484, 544, 295, 264, 8141, 295, 264, 12097, 484, 512, 20010, 4190, 11, 597, 8659, 1355, 300, 294, 341, 1674, 11, 512, 295, 291, 293, 512, 295, 264, 307, 611, 406, 1143, 13], "temperature": 0.0, "avg_logprob": -0.11461506949530707, "compression_ratio": 1.449612403100775, "no_speech_prob": 1.8323107724427246e-05}, {"id": 141, "seek": 205000, "start": 2050.0, "end": 2074.0, "text": " And as a result of that, we're getting lower dimensional representations for our words, if we're wanting to have word vectors, which still do as good as possible a job within the given dimensionality of enabling you to recover the original co occurrence matrix.", "tokens": [400, 382, 257, 1874, 295, 300, 11, 321, 434, 1242, 3126, 18795, 33358, 337, 527, 2283, 11, 498, 321, 434, 7935, 281, 362, 1349, 18875, 11, 597, 920, 360, 382, 665, 382, 1944, 257, 1691, 1951, 264, 2212, 10139, 1860, 295, 23148, 291, 281, 8114, 264, 3380, 598, 36122, 8141, 13], "temperature": 0.0, "avg_logprob": -0.10454348650845614, "compression_ratio": 1.5, "no_speech_prob": 4.000382250524126e-05}, {"id": 142, "seek": 207400, "start": 2074.0, "end": 2097.0, "text": " So from a linear algebra background, this is the obvious thing to use. So how does that work? Well, if you just build a raw count co occurrence matrix and run SVD on that and try and use those as word vectors, it actually works poorly.", "tokens": [407, 490, 257, 8213, 21989, 3678, 11, 341, 307, 264, 6322, 551, 281, 764, 13, 407, 577, 775, 300, 589, 30, 1042, 11, 498, 291, 445, 1322, 257, 8936, 1207, 598, 36122, 8141, 293, 1190, 31910, 35, 322, 300, 293, 853, 293, 764, 729, 382, 1349, 18875, 11, 309, 767, 1985, 22271, 13], "temperature": 0.0, "avg_logprob": -0.09168593925342225, "compression_ratio": 1.3988095238095237, "no_speech_prob": 3.3181626349687576e-05}, {"id": 143, "seek": 209700, "start": 2097.0, "end": 2115.0, "text": " It works poorly because if you get into the mathematical assumptions, SVD, you're expecting to have these normally distributed errors and what you're getting with word counts looked not at all.", "tokens": [467, 1985, 22271, 570, 498, 291, 483, 666, 264, 18894, 17695, 11, 31910, 35, 11, 291, 434, 9650, 281, 362, 613, 5646, 12631, 13603, 293, 437, 291, 434, 1242, 365, 1349, 14893, 2956, 406, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.13592679326127216, "compression_ratio": 1.3310344827586207, "no_speech_prob": 4.822582923225127e-05}, {"id": 144, "seek": 211500, "start": 2115.0, "end": 2130.0, "text": " Like some normal because you have exceedingly common words like that and then and you have a very large number of rare words. So that doesn't work very well, but you actually get something that works a lot better.", "tokens": [1743, 512, 2710, 570, 291, 362, 14048, 12163, 2689, 2283, 411, 300, 293, 550, 293, 291, 362, 257, 588, 2416, 1230, 295, 5892, 2283, 13, 407, 300, 1177, 380, 589, 588, 731, 11, 457, 291, 767, 483, 746, 300, 1985, 257, 688, 1101, 13], "temperature": 0.0, "avg_logprob": -0.21658706665039062, "compression_ratio": 1.5304878048780488, "no_speech_prob": 2.4653636501170695e-05}, {"id": 145, "seek": 211500, "start": 2130.0, "end": 2133.0, "text": " If you scale the counts in the cells.", "tokens": [759, 291, 4373, 264, 14893, 294, 264, 5438, 13], "temperature": 0.0, "avg_logprob": -0.21658706665039062, "compression_ratio": 1.5304878048780488, "no_speech_prob": 2.4653636501170695e-05}, {"id": 146, "seek": 213300, "start": 2133.0, "end": 2145.0, "text": " To deal with this problem, extremely frequent words, there are some things we can do. We could just take the log of the raw counts. We could kind of cap the maximum count.", "tokens": [1407, 2028, 365, 341, 1154, 11, 4664, 18004, 2283, 11, 456, 366, 512, 721, 321, 393, 360, 13, 492, 727, 445, 747, 264, 3565, 295, 264, 8936, 14893, 13, 492, 727, 733, 295, 1410, 264, 6674, 1207, 13], "temperature": 0.0, "avg_logprob": -0.14123744499392626, "compression_ratio": 1.6261261261261262, "no_speech_prob": 5.223023617872968e-05}, {"id": 147, "seek": 213300, "start": 2145.0, "end": 2159.0, "text": " We could throw away the function words and any of these kind of ideas that you build, then have a co occurrence matrix that you get more useful word vectors from running something like SVD.", "tokens": [492, 727, 3507, 1314, 264, 2445, 2283, 293, 604, 295, 613, 733, 295, 3487, 300, 291, 1322, 11, 550, 362, 257, 598, 36122, 8141, 300, 291, 483, 544, 4420, 1349, 18875, 490, 2614, 746, 411, 31910, 35, 13], "temperature": 0.0, "avg_logprob": -0.14123744499392626, "compression_ratio": 1.6261261261261262, "no_speech_prob": 5.223023617872968e-05}, {"id": 148, "seek": 215900, "start": 2159.0, "end": 2178.0, "text": " These kind of models were explored in the 1990s and in the 2000s and in particular Doug Rodi explored a number of these ideas is how to improve the co occurrence matrix in a model that he built that was called calls.", "tokens": [1981, 733, 295, 5245, 645, 24016, 294, 264, 13384, 82, 293, 294, 264, 8132, 82, 293, 294, 1729, 12742, 11097, 72, 24016, 257, 1230, 295, 613, 3487, 307, 577, 281, 3470, 264, 598, 36122, 8141, 294, 257, 2316, 300, 415, 3094, 300, 390, 1219, 5498, 13], "temperature": 0.0, "avg_logprob": -0.1730312728881836, "compression_ratio": 1.4594594594594594, "no_speech_prob": 0.00015833272482268512}, {"id": 149, "seek": 217800, "start": 2178.0, "end": 2197.0, "text": " And you know, actually in his calls model, he observed the fact that you could get the same kind of linear components that have semantic components that we saw yesterday when talking about analogies.", "tokens": [400, 291, 458, 11, 767, 294, 702, 5498, 2316, 11, 415, 13095, 264, 1186, 300, 291, 727, 483, 264, 912, 733, 295, 8213, 6677, 300, 362, 47982, 6677, 300, 321, 1866, 5186, 562, 1417, 466, 16660, 530, 13], "temperature": 0.0, "avg_logprob": -0.10850410234360468, "compression_ratio": 1.463235294117647, "no_speech_prob": 2.351235889364034e-05}, {"id": 150, "seek": 219700, "start": 2197.0, "end": 2210.0, "text": " So for example, this is a figure from his paper and you can see that we seem to have a meaning component going from a verb to the person who does the verb.", "tokens": [407, 337, 1365, 11, 341, 307, 257, 2573, 490, 702, 3035, 293, 291, 393, 536, 300, 321, 1643, 281, 362, 257, 3620, 6542, 516, 490, 257, 9595, 281, 264, 954, 567, 775, 264, 9595, 13], "temperature": 0.0, "avg_logprob": -0.1812627346484692, "compression_ratio": 1.6146341463414635, "no_speech_prob": 0.0003573329886421561}, {"id": 151, "seek": 219700, "start": 2210.0, "end": 2224.0, "text": " So drive to drive us when to swim or teach to teacher, marry to priest. And that these vector components are not perfectly, but are roughly parallel and roughly the same size.", "tokens": [407, 3332, 281, 3332, 505, 562, 281, 7110, 420, 2924, 281, 5027, 11, 9747, 281, 15703, 13, 400, 300, 613, 8062, 6677, 366, 406, 6239, 11, 457, 366, 9810, 8952, 293, 9810, 264, 912, 2744, 13], "temperature": 0.0, "avg_logprob": -0.1812627346484692, "compression_ratio": 1.6146341463414635, "no_speech_prob": 0.0003573329886421561}, {"id": 152, "seek": 222400, "start": 2224.0, "end": 2238.0, "text": " And so we have a meaning component there that we could add on to another word, just like we did for previously for analogies, we could say drivers to driver as Mary is to what.", "tokens": [400, 370, 321, 362, 257, 3620, 6542, 456, 300, 321, 727, 909, 322, 281, 1071, 1349, 11, 445, 411, 321, 630, 337, 8046, 337, 16660, 530, 11, 321, 727, 584, 11590, 281, 6787, 382, 6059, 307, 281, 437, 13], "temperature": 0.0, "avg_logprob": -0.14118355311704486, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0002955155214294791}, {"id": 153, "seek": 222400, "start": 2238.0, "end": 2253.0, "text": " And we'd add on this screen vector component, which is roughly the same as this one. And we'd say, oh, priest. So that this space could actually get some word vectors analogies right as well.", "tokens": [400, 321, 1116, 909, 322, 341, 2568, 8062, 6542, 11, 597, 307, 9810, 264, 912, 382, 341, 472, 13, 400, 321, 1116, 584, 11, 1954, 11, 15703, 13, 407, 300, 341, 1901, 727, 767, 483, 512, 1349, 18875, 16660, 530, 558, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.14118355311704486, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0002955155214294791}, {"id": 154, "seek": 225300, "start": 2253.0, "end": 2265.0, "text": " And so that seemed really interesting to us around the time word to vac came out of wanting to understand better what the literature of updating algorithm of word to vac did.", "tokens": [400, 370, 300, 6576, 534, 1880, 281, 505, 926, 264, 565, 1349, 281, 2842, 1361, 484, 295, 7935, 281, 1223, 1101, 437, 264, 10394, 295, 25113, 9284, 295, 1349, 281, 2842, 630, 13], "temperature": 0.0, "avg_logprob": -0.14114978665211161, "compression_ratio": 1.6, "no_speech_prob": 6.094350464991294e-05}, {"id": 155, "seek": 225300, "start": 2265.0, "end": 2273.0, "text": " And how it related to these more linear algebra based methods that had been explored in the couple of decades previously.", "tokens": [400, 577, 309, 4077, 281, 613, 544, 8213, 21989, 2361, 7150, 300, 632, 668, 24016, 294, 264, 1916, 295, 7878, 8046, 13], "temperature": 0.0, "avg_logprob": -0.14114978665211161, "compression_ratio": 1.6, "no_speech_prob": 6.094350464991294e-05}, {"id": 156, "seek": 227300, "start": 2273.0, "end": 2287.0, "text": " And so for the next bit, I want to tell you a little bit about the glove algorithm, which was an algorithm for word vectors that was made by Jeffrey pennington, Richard Socher and me in 2014.", "tokens": [400, 370, 337, 264, 958, 857, 11, 286, 528, 281, 980, 291, 257, 707, 857, 466, 264, 26928, 9284, 11, 597, 390, 364, 9284, 337, 1349, 18875, 300, 390, 1027, 538, 28721, 3435, 773, 1756, 11, 9809, 407, 6759, 293, 385, 294, 8227, 13], "temperature": 0.0, "avg_logprob": -0.1340398887793223, "compression_ratio": 1.3356643356643356, "no_speech_prob": 3.873519744956866e-05}, {"id": 157, "seek": 228700, "start": 2287.0, "end": 2306.0, "text": " And so the starting point of this was to try to connect together the linear algebra based methods on current matrices like LSA and calls with the models like skip grand CBO and their other friends, which were iterative neural updating algorithms.", "tokens": [400, 370, 264, 2891, 935, 295, 341, 390, 281, 853, 281, 1745, 1214, 264, 8213, 21989, 2361, 7150, 322, 2190, 32284, 411, 441, 8886, 293, 5498, 365, 264, 5245, 411, 10023, 2697, 383, 15893, 293, 641, 661, 1855, 11, 597, 645, 17138, 1166, 18161, 25113, 14642, 13], "temperature": 0.0, "avg_logprob": -0.19743173262652228, "compression_ratio": 1.430232558139535, "no_speech_prob": 3.317863229312934e-05}, {"id": 158, "seek": 230600, "start": 2306.0, "end": 2317.0, "text": " So on the one hand, you know, the linear algebra methods actually seemed like they had advantages for fast training and efficient usage of statistics.", "tokens": [407, 322, 264, 472, 1011, 11, 291, 458, 11, 264, 8213, 21989, 7150, 767, 6576, 411, 436, 632, 14906, 337, 2370, 3097, 293, 7148, 14924, 295, 12523, 13], "temperature": 0.0, "avg_logprob": -0.1523449230194092, "compression_ratio": 1.4484848484848485, "no_speech_prob": 6.004684109939262e-05}, {"id": 159, "seek": 230600, "start": 2317.0, "end": 2324.0, "text": " But although there had been work on capturing words similarities with them by and large.", "tokens": [583, 4878, 456, 632, 668, 589, 322, 23384, 2283, 24197, 365, 552, 538, 293, 2416, 13], "temperature": 0.0, "avg_logprob": -0.1523449230194092, "compression_ratio": 1.4484848484848485, "no_speech_prob": 6.004684109939262e-05}, {"id": 160, "seek": 232400, "start": 2324.0, "end": 2346.0, "text": " The results weren't as good perhaps because of disproportionate importance given to large counts in the main conversely, the models, the neural models, it seems like if you're just doing these gradient updates on windows, you're somehow inefficiently using statistics versus a coerced currents matrix.", "tokens": [440, 3542, 4999, 380, 382, 665, 4317, 570, 295, 28734, 473, 7379, 2212, 281, 2416, 14893, 294, 264, 2135, 2615, 736, 11, 264, 5245, 11, 264, 18161, 5245, 11, 309, 2544, 411, 498, 291, 434, 445, 884, 613, 16235, 9205, 322, 9309, 11, 291, 434, 6063, 43495, 356, 1228, 12523, 5717, 257, 598, 260, 1232, 30110, 8141, 13], "temperature": 0.0, "avg_logprob": -0.2556266630849531, "compression_ratio": 1.5279187817258884, "no_speech_prob": 4.1218754631699994e-05}, {"id": 161, "seek": 234600, "start": 2346.0, "end": 2354.0, "text": " And the other hand is actually easier to scale to a very large corpus by trading time for space.", "tokens": [400, 264, 661, 1011, 307, 767, 3571, 281, 4373, 281, 257, 588, 2416, 1181, 31624, 538, 9529, 565, 337, 1901, 13], "temperature": 0.0, "avg_logprob": -0.11751113439861097, "compression_ratio": 1.6266666666666667, "no_speech_prob": 4.601081309374422e-05}, {"id": 162, "seek": 234600, "start": 2354.0, "end": 2373.0, "text": " And at that time, it seemed like the neural methods just worked better for people that they generated improved performance on many tasks, not just on words similarity, and that they could capture complex patterns such as the analogies that went beyond words similarity.", "tokens": [400, 412, 300, 565, 11, 309, 6576, 411, 264, 18161, 7150, 445, 2732, 1101, 337, 561, 300, 436, 10833, 9689, 3389, 322, 867, 9608, 11, 406, 445, 322, 2283, 32194, 11, 293, 300, 436, 727, 7983, 3997, 8294, 1270, 382, 264, 16660, 530, 300, 1437, 4399, 2283, 32194, 13], "temperature": 0.0, "avg_logprob": -0.11751113439861097, "compression_ratio": 1.6266666666666667, "no_speech_prob": 4.601081309374422e-05}, {"id": 163, "seek": 237300, "start": 2373.0, "end": 2385.0, "text": " And so what we wanted to do was understand a bit more is to what do you what properties you need to have this analogies work out as I showed last time.", "tokens": [400, 370, 437, 321, 1415, 281, 360, 390, 1223, 257, 857, 544, 307, 281, 437, 360, 291, 437, 7221, 291, 643, 281, 362, 341, 16660, 530, 589, 484, 382, 286, 4712, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.12512480585198654, "compression_ratio": 1.3482142857142858, "no_speech_prob": 0.00019683960999827832}, {"id": 164, "seek": 238500, "start": 2385.0, "end": 2414.0, "text": " And so what we realized was that if you'd like to do have these sort of vector subtractions and additions work for an analogy, the property that you want is for meaning components or meaning component is something like going from male to female queen to king or going from", "tokens": [400, 370, 437, 321, 5334, 390, 300, 498, 291, 1116, 411, 281, 360, 362, 613, 1333, 295, 8062, 16390, 626, 293, 35113, 589, 337, 364, 21663, 11, 264, 4707, 300, 291, 528, 307, 337, 3620, 6677, 420, 3620, 6542, 307, 746, 411, 516, 490, 7133, 281, 6556, 12206, 281, 4867, 420, 516, 490], "temperature": 0.0, "avg_logprob": -0.12530273303650974, "compression_ratio": 1.619047619047619, "no_speech_prob": 1.0740190191427246e-05}, {"id": 165, "seek": 241400, "start": 2414.0, "end": 2426.0, "text": " to a bird with a drone truck driver, that those meaning components should be represented as ratios of color currents probabilities.", "tokens": [281, 257, 5255, 365, 257, 13852, 5898, 6787, 11, 300, 729, 3620, 6677, 820, 312, 10379, 382, 32435, 295, 2017, 30110, 33783, 13], "temperature": 0.0, "avg_logprob": -0.3765628965277421, "compression_ratio": 1.3412698412698412, "no_speech_prob": 0.00020608821068890393}, {"id": 166, "seek": 241400, "start": 2426.0, "end": 2429.0, "text": " So here's an example that shows that.", "tokens": [407, 510, 311, 364, 1365, 300, 3110, 300, 13], "temperature": 0.0, "avg_logprob": -0.3765628965277421, "compression_ratio": 1.3412698412698412, "no_speech_prob": 0.00020608821068890393}, {"id": 167, "seek": 242900, "start": 2429.0, "end": 2451.0, "text": " Okay, so suppose the meaning component that we want to get out is the spectrum from solid to gas as in physics, well, you'd think that you can get at the solid part of it, perhaps by saying does the word coerced with ice and the word solid occurs with ice.", "tokens": [1033, 11, 370, 7297, 264, 3620, 6542, 300, 321, 528, 281, 483, 484, 307, 264, 11143, 490, 5100, 281, 4211, 382, 294, 10649, 11, 731, 11, 291, 1116, 519, 300, 291, 393, 483, 412, 264, 5100, 644, 295, 309, 11, 4317, 538, 1566, 775, 264, 1349, 598, 260, 1232, 365, 4435, 293, 264, 1349, 5100, 11843, 365, 4435, 13], "temperature": 0.0, "avg_logprob": -0.14585308801560176, "compression_ratio": 1.5515151515151515, "no_speech_prob": 1.3200194189266767e-05}, {"id": 168, "seek": 245100, "start": 2451.0, "end": 2469.0, "text": " So it looks hopeful and gas doesn't occur with ice much so that looks hopeful, but the problem is the word water will also occur a lot with ice and if you just take some other random word like the word random, it probably doesn't occur with ice much.", "tokens": [407, 309, 1542, 20531, 293, 4211, 1177, 380, 5160, 365, 4435, 709, 370, 300, 1542, 20531, 11, 457, 264, 1154, 307, 264, 1349, 1281, 486, 611, 5160, 257, 688, 365, 4435, 293, 498, 291, 445, 747, 512, 661, 4974, 1349, 411, 264, 1349, 4974, 11, 309, 1391, 1177, 380, 5160, 365, 4435, 709, 13], "temperature": 0.0, "avg_logprob": -0.16044531197383485, "compression_ratio": 1.7123287671232876, "no_speech_prob": 1.615809196664486e-05}, {"id": 169, "seek": 246900, "start": 2469.0, "end": 2481.0, "text": " So if you look at words coerced with steam solid won't occur with steam much, but gas will the water will again and random will be small.", "tokens": [407, 498, 291, 574, 412, 2283, 598, 260, 1232, 365, 11952, 5100, 1582, 380, 5160, 365, 11952, 709, 11, 457, 4211, 486, 264, 1281, 486, 797, 293, 4974, 486, 312, 1359, 13], "temperature": 0.0, "avg_logprob": -0.18041937881045872, "compression_ratio": 1.3173076923076923, "no_speech_prob": 7.876751624280587e-06}, {"id": 170, "seek": 248100, "start": 2481.0, "end": 2506.0, "text": " So to get out the meaning component we want of going from gas to solid was actually really useful is to look at the ratio of these coerced currents probabilities, because then we get a spectrum for large to small between solid and gas, whereas for water and a random word, it basically cancels out and gives you one.", "tokens": [407, 281, 483, 484, 264, 3620, 6542, 321, 528, 295, 516, 490, 4211, 281, 5100, 390, 767, 534, 4420, 307, 281, 574, 412, 264, 8509, 295, 613, 598, 260, 1232, 30110, 33783, 11, 570, 550, 321, 483, 257, 11143, 337, 2416, 281, 1359, 1296, 5100, 293, 4211, 11, 9735, 337, 1281, 293, 257, 4974, 1349, 11, 309, 1936, 393, 66, 1625, 484, 293, 2709, 291, 472, 13], "temperature": 0.0, "avg_logprob": -0.09118671148595675, "compression_ratio": 1.595959595959596, "no_speech_prob": 1.4474259842245374e-05}, {"id": 171, "seek": 250600, "start": 2506.0, "end": 2535.0, "text": " I just wrote these numbers in, but if you count them up in a large corpus, it is basically what you get so here actual coerced currents probabilities and that for water and my random word, which was fashion here, these are approximately one, whereas for the ratio of probability of coerced currents of solid with ice or steam is about 10 and for gas, it's about a 10.", "tokens": [286, 445, 4114, 613, 3547, 294, 11, 457, 498, 291, 1207, 552, 493, 294, 257, 2416, 1181, 31624, 11, 309, 307, 1936, 437, 291, 483, 370, 510, 3539, 598, 260, 1232, 30110, 33783, 293, 300, 337, 1281, 293, 452, 4974, 1349, 11, 597, 390, 6700, 510, 11, 613, 366, 10447, 472, 11, 9735, 337, 264, 8509, 295, 8482, 295, 598, 260, 1232, 30110, 295, 5100, 365, 4435, 420, 11952, 307, 466, 1266, 293, 337, 4211, 11, 309, 311, 466, 257, 1266, 13], "temperature": 0.0, "avg_logprob": -0.12116125018097633, "compression_ratio": 1.6681818181818182, "no_speech_prob": 5.9779897128464654e-05}, {"id": 172, "seek": 253500, "start": 2535.0, "end": 2551.0, "text": " So how can we capture these ratios of coerced currents probabilities as linear meaning components so that in our word vector space, we can just add and subtract linear meaning components.", "tokens": [407, 577, 393, 321, 7983, 613, 32435, 295, 598, 260, 1232, 30110, 33783, 382, 8213, 3620, 6677, 370, 300, 294, 527, 1349, 8062, 1901, 11, 321, 393, 445, 909, 293, 16390, 8213, 3620, 6677, 13], "temperature": 0.0, "avg_logprob": -0.09423027283106095, "compression_ratio": 1.5203252032520325, "no_speech_prob": 0.00015830998017918319}, {"id": 173, "seek": 255100, "start": 2551.0, "end": 2568.0, "text": " Well, it seems like the way we can achieve that is if we build a log by linear model, so that the dot product between two word vectors, attempt to approximate the log of the probability of coerced currents.", "tokens": [1042, 11, 309, 2544, 411, 264, 636, 321, 393, 4584, 300, 307, 498, 321, 1322, 257, 3565, 538, 8213, 2316, 11, 370, 300, 264, 5893, 1674, 1296, 732, 1349, 18875, 11, 5217, 281, 30874, 264, 3565, 295, 264, 8482, 295, 598, 260, 1232, 30110, 13], "temperature": 0.0, "avg_logprob": -0.10013645522448482, "compression_ratio": 1.4609929078014185, "no_speech_prob": 1.3830630450684112e-05}, {"id": 174, "seek": 256800, "start": 2568.0, "end": 2587.0, "text": " So if you do that, you then get this property that the difference between two vectors, it's similarity to another word corresponds to the log of the probability ratio shown on the previous slide.", "tokens": [407, 498, 291, 360, 300, 11, 291, 550, 483, 341, 4707, 300, 264, 2649, 1296, 732, 18875, 11, 309, 311, 32194, 281, 1071, 1349, 23249, 281, 264, 3565, 295, 264, 8482, 8509, 4898, 322, 264, 3894, 4137, 13], "temperature": 0.0, "avg_logprob": -0.07058884416307722, "compression_ratio": 1.4338235294117647, "no_speech_prob": 2.8809870855184272e-05}, {"id": 175, "seek": 258700, "start": 2587.0, "end": 2609.0, "text": " So the glove model wanted to try and unify the thinking between the coerced currents matrix models and the neural models by being in some way similar to a neural model, but actually calculated on top of a coerced currents matrix count.", "tokens": [407, 264, 26928, 2316, 1415, 281, 853, 293, 517, 2505, 264, 1953, 1296, 264, 598, 260, 1232, 30110, 8141, 5245, 293, 264, 18161, 5245, 538, 885, 294, 512, 636, 2531, 281, 257, 18161, 2316, 11, 457, 767, 15598, 322, 1192, 295, 257, 598, 260, 1232, 30110, 8141, 1207, 13], "temperature": 0.0, "avg_logprob": -0.08562243659541292, "compression_ratio": 1.6206896551724137, "no_speech_prob": 3.4244920243509114e-05}, {"id": 176, "seek": 260900, "start": 2609.0, "end": 2622.0, "text": " So we had an explicit loss function and our explicit loss function is that we wanted the dot product to be similar to the log of the coerced currents.", "tokens": [407, 321, 632, 364, 13691, 4470, 2445, 293, 527, 13691, 4470, 2445, 307, 300, 321, 1415, 264, 5893, 1674, 281, 312, 2531, 281, 264, 3565, 295, 264, 598, 260, 1232, 30110, 13], "temperature": 0.0, "avg_logprob": -0.06875450743569268, "compression_ratio": 1.4563106796116505, "no_speech_prob": 6.918510189279914e-05}, {"id": 177, "seek": 262200, "start": 2622.0, "end": 2646.0, "text": " And we actually added in some bias terms here, but I'll ignore those for the moment. And we wanted to not have very common words dominate. And so we kept the effect of high word counts using this F function that's shown here. And then we could optimize this J function directly on the coerced currents count matrix.", "tokens": [400, 321, 767, 3869, 294, 512, 12577, 2115, 510, 11, 457, 286, 603, 11200, 729, 337, 264, 1623, 13, 400, 321, 1415, 281, 406, 362, 588, 2689, 2283, 28246, 13, 400, 370, 321, 4305, 264, 1802, 295, 1090, 1349, 14893, 1228, 341, 479, 2445, 300, 311, 4898, 510, 13, 400, 550, 321, 727, 19719, 341, 508, 2445, 3838, 322, 264, 598, 260, 1232, 30110, 1207, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13327473653873928, "compression_ratio": 1.5441176470588236, "no_speech_prob": 3.0225044611142948e-05}, {"id": 178, "seek": 264600, "start": 2646.0, "end": 2652.0, "text": " And that gave us fast training scalable to huge corpora.", "tokens": [400, 300, 2729, 505, 2370, 3097, 38481, 281, 2603, 6804, 64, 13], "temperature": 0.0, "avg_logprob": -0.19079322319526176, "compression_ratio": 1.6031746031746033, "no_speech_prob": 7.588350854348391e-05}, {"id": 179, "seek": 264600, "start": 2652.0, "end": 2669.0, "text": " And so this algorithm worked very well. So if you ask, if you run this algorithm, ask what are the nearest words to fog, you get fogs toad, and then you get some complicated words, but it turns out they are all fogs until you get down the lizard.", "tokens": [400, 370, 341, 9284, 2732, 588, 731, 13, 407, 498, 291, 1029, 11, 498, 291, 1190, 341, 9284, 11, 1029, 437, 366, 264, 23831, 2283, 281, 13648, 11, 291, 483, 13648, 82, 281, 345, 11, 293, 550, 291, 483, 512, 6179, 2283, 11, 457, 309, 4523, 484, 436, 366, 439, 13648, 82, 1826, 291, 483, 760, 264, 39215, 13], "temperature": 0.0, "avg_logprob": -0.19079322319526176, "compression_ratio": 1.6031746031746033, "no_speech_prob": 7.588350854348391e-05}, {"id": 180, "seek": 266900, "start": 2669.0, "end": 2677.0, "text": " And so this is a tutorial that lovely tree fog there. And so this actually seemed to work out pretty well.", "tokens": [400, 370, 341, 307, 257, 7073, 300, 7496, 4230, 13648, 456, 13, 400, 370, 341, 767, 6576, 281, 589, 484, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.24153995513916016, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.0003093887062277645}, {"id": 181, "seek": 266900, "start": 2677.0, "end": 2686.0, "text": " How well did it work out to discuss that a bit more. I now want to say something about how do we evaluate word vectors.", "tokens": [1012, 731, 630, 309, 589, 484, 281, 2248, 300, 257, 857, 544, 13, 286, 586, 528, 281, 584, 746, 466, 577, 360, 321, 13059, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.24153995513916016, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.0003093887062277645}, {"id": 182, "seek": 266900, "start": 2686.0, "end": 2690.0, "text": " Are we good for up to there for questions.", "tokens": [2014, 321, 665, 337, 493, 281, 456, 337, 1651, 13], "temperature": 0.0, "avg_logprob": -0.24153995513916016, "compression_ratio": 1.563953488372093, "no_speech_prob": 0.0003093887062277645}, {"id": 183, "seek": 269000, "start": 2690.0, "end": 2711.0, "text": " We've got some questions. What do you mean by an inefficient use of statistics as a con for skip. Well, what I mean is that, you know, for word to vac, you're just, you know, looking at one center word at a time and generating a few negative samples.", "tokens": [492, 600, 658, 512, 1651, 13, 708, 360, 291, 914, 538, 364, 43495, 764, 295, 12523, 382, 257, 416, 337, 10023, 13, 1042, 11, 437, 286, 914, 307, 300, 11, 291, 458, 11, 337, 1349, 281, 2842, 11, 291, 434, 445, 11, 291, 458, 11, 1237, 412, 472, 3056, 1349, 412, 257, 565, 293, 17746, 257, 1326, 3671, 10938, 13], "temperature": 0.0, "avg_logprob": -0.1792420893907547, "compression_ratio": 1.4970059880239521, "no_speech_prob": 0.00015543597692158073}, {"id": 184, "seek": 271100, "start": 2711.0, "end": 2728.0, "text": " And so it sort of seems like us doing something always precise there, whereas if you're doing optimization algorithm on the whole matrix at once, well, you actually know everything about the matrix at once.", "tokens": [400, 370, 309, 1333, 295, 2544, 411, 505, 884, 746, 1009, 13600, 456, 11, 9735, 498, 291, 434, 884, 19618, 9284, 322, 264, 1379, 8141, 412, 1564, 11, 731, 11, 291, 767, 458, 1203, 466, 264, 8141, 412, 1564, 13], "temperature": 0.0, "avg_logprob": -0.20042423768477005, "compression_ratio": 1.4507042253521127, "no_speech_prob": 0.0003998471947852522}, {"id": 185, "seek": 272800, "start": 2728.0, "end": 2742.0, "text": " And so that's just looking at what words, what other words occurred in this one context of the center word, you've got the entire vector of co occurrence accounts for the center word and another word.", "tokens": [400, 370, 300, 311, 445, 1237, 412, 437, 2283, 11, 437, 661, 2283, 11068, 294, 341, 472, 4319, 295, 264, 3056, 1349, 11, 291, 600, 658, 264, 2302, 8062, 295, 598, 36122, 9402, 337, 264, 3056, 1349, 293, 1071, 1349, 13], "temperature": 0.0, "avg_logprob": -0.23519777895799324, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.8866132450057194e-05}, {"id": 186, "seek": 272800, "start": 2742.0, "end": 2751.0, "text": " And so therefore you can much more efficiently and less noiseily work out how to minimize your loss.", "tokens": [400, 370, 4412, 291, 393, 709, 544, 19621, 293, 1570, 5658, 953, 589, 484, 577, 281, 17522, 428, 4470, 13], "temperature": 0.0, "avg_logprob": -0.23519777895799324, "compression_ratio": 1.6538461538461537, "no_speech_prob": 1.8866132450057194e-05}, {"id": 187, "seek": 275100, "start": 2751.0, "end": 2766.0, "text": " So I'm going to say, I'll go on. Okay, so I've sort of said, look at these word vectors. They're great. And I sort of showed you a few things at the end of the last class, which argued, hey, these are great.", "tokens": [407, 286, 478, 516, 281, 584, 11, 286, 603, 352, 322, 13, 1033, 11, 370, 286, 600, 1333, 295, 848, 11, 574, 412, 613, 1349, 18875, 13, 814, 434, 869, 13, 400, 286, 1333, 295, 4712, 291, 257, 1326, 721, 412, 264, 917, 295, 264, 1036, 1508, 11, 597, 20219, 11, 4177, 11, 613, 366, 869, 13], "temperature": 0.0, "avg_logprob": -0.26849971833776254, "compression_ratio": 1.4275862068965517, "no_speech_prob": 0.00014164458843879402}, {"id": 188, "seek": 276600, "start": 2766.0, "end": 2787.0, "text": " They work out these analogies, they show similarity and things like this. We want to make this a bit more precise. And indeed for natural language processing as in other areas of machine learning, a big part of what people are doing is working out good ways to evaluate knowledge that things have.", "tokens": [814, 589, 484, 613, 16660, 530, 11, 436, 855, 32194, 293, 721, 411, 341, 13, 492, 528, 281, 652, 341, 257, 857, 544, 13600, 13, 400, 6451, 337, 3303, 2856, 9007, 382, 294, 661, 3179, 295, 3479, 2539, 11, 257, 955, 644, 295, 437, 561, 366, 884, 307, 1364, 484, 665, 2098, 281, 13059, 3601, 300, 721, 362, 13], "temperature": 0.0, "avg_logprob": -0.09032379634796626, "compression_ratio": 1.5309278350515463, "no_speech_prob": 2.6241710656904615e-05}, {"id": 189, "seek": 278700, "start": 2787.0, "end": 2810.0, "text": " So how can we really evaluate word vectors. So in general, for NLP evaluation, people talk about two ways of evaluation intrinsic and extrinsic. So an intrinsic evaluation means that you evaluate directly on the specific or intermediate subtasks that you've been working on.", "tokens": [407, 577, 393, 321, 534, 13059, 1349, 18875, 13, 407, 294, 2674, 11, 337, 426, 45196, 13344, 11, 561, 751, 466, 732, 2098, 295, 13344, 35698, 293, 16455, 1292, 299, 13, 407, 364, 35698, 13344, 1355, 300, 291, 13059, 3838, 322, 264, 2685, 420, 19376, 7257, 296, 1694, 300, 291, 600, 668, 1364, 322, 13], "temperature": 0.0, "avg_logprob": -0.12484350042828059, "compression_ratio": 1.6023391812865497, "no_speech_prob": 0.000187339581316337}, {"id": 190, "seek": 281000, "start": 2810.0, "end": 2824.0, "text": " A measure where I can directly score how good my word vectors are. And normally intrinsic evaluations are fast to compute. They helped you to understand the component you've been working on.", "tokens": [316, 3481, 689, 286, 393, 3838, 6175, 577, 665, 452, 1349, 18875, 366, 13, 400, 5646, 35698, 43085, 366, 2370, 281, 14722, 13, 814, 4254, 291, 281, 1223, 264, 6542, 291, 600, 668, 1364, 322, 13], "temperature": 0.0, "avg_logprob": -0.09004812770419651, "compression_ratio": 1.5483870967741935, "no_speech_prob": 8.57350678415969e-05}, {"id": 191, "seek": 281000, "start": 2824.0, "end": 2837.0, "text": " But often, simply trying to optimize that component may or may not have a very big good effect on the overall system that you're trying to build.", "tokens": [583, 2049, 11, 2935, 1382, 281, 19719, 300, 6542, 815, 420, 815, 406, 362, 257, 588, 955, 665, 1802, 322, 264, 4787, 1185, 300, 291, 434, 1382, 281, 1322, 13], "temperature": 0.0, "avg_logprob": -0.09004812770419651, "compression_ratio": 1.5483870967741935, "no_speech_prob": 8.57350678415969e-05}, {"id": 192, "seek": 283700, "start": 2837.0, "end": 2854.0, "text": " So people have also also been very interested in extrinsic evaluations. So an extrinsic evaluation is that you take some real task of interest to human beings, whether that's a web search or machine translation or something like that.", "tokens": [407, 561, 362, 611, 611, 668, 588, 3102, 294, 16455, 1292, 299, 43085, 13, 407, 364, 16455, 1292, 299, 13344, 307, 300, 291, 747, 512, 957, 5633, 295, 1179, 281, 1952, 8958, 11, 1968, 300, 311, 257, 3670, 3164, 420, 3479, 12853, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.10270286073871687, "compression_ratio": 1.56, "no_speech_prob": 0.0006506617646664381}, {"id": 193, "seek": 285400, "start": 2854.0, "end": 2868.0, "text": " And you say your goal is to actually improve performance on that task. Well, that's a real proof that this is doing something useful. So in some ways, it's just clearly better.", "tokens": [400, 291, 584, 428, 3387, 307, 281, 767, 3470, 3389, 322, 300, 5633, 13, 1042, 11, 300, 311, 257, 957, 8177, 300, 341, 307, 884, 746, 4420, 13, 407, 294, 512, 2098, 11, 309, 311, 445, 4448, 1101, 13], "temperature": 0.0, "avg_logprob": -0.0813066515811654, "compression_ratio": 1.3134328358208955, "no_speech_prob": 9.2345813754946e-05}, {"id": 194, "seek": 286800, "start": 2868.0, "end": 2897.0, "text": " But on the other hand, it also has some disadvantages. It takes a lot longer to evaluate on an extrinsic task because it's a much bigger system. And sometimes, you know, when you change things, it's unclear whether the fact that the numbers went down was because you now have worse word vectors or whether it's just somehow the other components of the system.", "tokens": [583, 322, 264, 661, 1011, 11, 309, 611, 575, 512, 37431, 13, 467, 2516, 257, 688, 2854, 281, 13059, 322, 364, 16455, 1292, 299, 5633, 570, 309, 311, 257, 709, 3801, 1185, 13, 400, 2171, 11, 291, 458, 11, 562, 291, 1319, 721, 11, 309, 311, 25636, 1968, 264, 1186, 300, 264, 3547, 1437, 760, 390, 570, 291, 586, 362, 5324, 1349, 18875, 420, 1968, 309, 311, 445, 6063, 264, 661, 6677, 295, 264, 1185, 13], "temperature": 0.0, "avg_logprob": -0.09444746375083923, "compression_ratio": 1.639269406392694, "no_speech_prob": 2.7453366783447564e-05}, {"id": 195, "seek": 289700, "start": 2897.0, "end": 2910.0, "text": " And it's a better with your old word vectors. And if you change the other components as well, things would get better again. So in some ways, it can sometimes be mudier to see if you're making progress.", "tokens": [400, 309, 311, 257, 1101, 365, 428, 1331, 1349, 18875, 13, 400, 498, 291, 1319, 264, 661, 6677, 382, 731, 11, 721, 576, 483, 1101, 797, 13, 407, 294, 512, 2098, 11, 309, 393, 2171, 312, 8933, 811, 281, 536, 498, 291, 434, 1455, 4205, 13], "temperature": 0.0, "avg_logprob": -0.2343195809258355, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.00048555232933722436}, {"id": 196, "seek": 289700, "start": 2910.0, "end": 2914.0, "text": " But I'll touch on both of these methods here.", "tokens": [583, 286, 603, 2557, 322, 1293, 295, 613, 7150, 510, 13], "temperature": 0.0, "avg_logprob": -0.2343195809258355, "compression_ratio": 1.4335260115606936, "no_speech_prob": 0.00048555232933722436}, {"id": 197, "seek": 291400, "start": 2914.0, "end": 2930.0, "text": " So for intrinsic evaluation of word vectors, one way, which we mentioned last time was this word vector analogies. So we could simply give our models a big collection of word vector analogy problems.", "tokens": [407, 337, 35698, 13344, 295, 1349, 18875, 11, 472, 636, 11, 597, 321, 2835, 1036, 565, 390, 341, 1349, 8062, 16660, 530, 13, 407, 321, 727, 2935, 976, 527, 5245, 257, 955, 5765, 295, 1349, 8062, 21663, 2740, 13], "temperature": 0.0, "avg_logprob": -0.09446494523869005, "compression_ratio": 1.4962406015037595, "no_speech_prob": 3.782513886108063e-05}, {"id": 198, "seek": 293000, "start": 2930.0, "end": 2946.0, "text": " So we could say man is the woman as king is the what and ask the model to find the word that is closest using that sort of word analogy computation and hope that what comes out there is queen.", "tokens": [407, 321, 727, 584, 587, 307, 264, 3059, 382, 4867, 307, 264, 437, 293, 1029, 264, 2316, 281, 915, 264, 1349, 300, 307, 13699, 1228, 300, 1333, 295, 1349, 21663, 24903, 293, 1454, 300, 437, 1487, 484, 456, 307, 12206, 13], "temperature": 0.0, "avg_logprob": -0.08468444796576016, "compression_ratio": 1.6216216216216217, "no_speech_prob": 2.3120694095268846e-05}, {"id": 199, "seek": 293000, "start": 2946.0, "end": 2954.0, "text": " And so that's something people have done and have worked on accuracy score of how often that you are right.", "tokens": [400, 370, 300, 311, 746, 561, 362, 1096, 293, 362, 2732, 322, 14170, 6175, 295, 577, 2049, 300, 291, 366, 558, 13], "temperature": 0.0, "avg_logprob": -0.08468444796576016, "compression_ratio": 1.6216216216216217, "no_speech_prob": 2.3120694095268846e-05}, {"id": 200, "seek": 295400, "start": 2954.0, "end": 2980.0, "text": " At this point, I should just mention one little trick of these word vector analogies that everyone uses, but not everyone talks it out along the first instance. I mean, there's a little trick which you can find in the gents encode, if you look at it, that when it does man is the woman as king is to what.", "tokens": [1711, 341, 935, 11, 286, 820, 445, 2152, 472, 707, 4282, 295, 613, 1349, 8062, 16660, 530, 300, 1518, 4960, 11, 457, 406, 1518, 6686, 309, 484, 2051, 264, 700, 5197, 13, 286, 914, 11, 456, 311, 257, 707, 4282, 597, 291, 393, 915, 294, 264, 290, 791, 2058, 1429, 11, 498, 291, 574, 412, 309, 11, 300, 562, 309, 775, 587, 307, 264, 3059, 382, 4867, 307, 281, 437, 13], "temperature": 0.0, "avg_logprob": -0.1384567642211914, "compression_ratio": 1.5968586387434556, "no_speech_prob": 3.6360830563353375e-05}, {"id": 201, "seek": 298000, "start": 2980.0, "end": 2994.0, "text": " Something that could often happen is that actually the word once you do your pluses and your minuses that the word that will actually be closest is still king.", "tokens": [6595, 300, 727, 2049, 1051, 307, 300, 767, 264, 1349, 1564, 291, 360, 428, 1804, 279, 293, 428, 3175, 279, 300, 264, 1349, 300, 486, 767, 312, 13699, 307, 920, 4867, 13], "temperature": 0.0, "avg_logprob": -0.10082581970426771, "compression_ratio": 1.4454545454545455, "no_speech_prob": 0.00010204806312685832}, {"id": 202, "seek": 299400, "start": 2994.0, "end": 3011.0, "text": " So the way people always do this is that they don't allow one of the three input words in the selection process. So you're choosing the nearest word that isn't one of the put words.", "tokens": [407, 264, 636, 561, 1009, 360, 341, 307, 300, 436, 500, 380, 2089, 472, 295, 264, 1045, 4846, 2283, 294, 264, 9450, 1399, 13, 407, 291, 434, 10875, 264, 23831, 1349, 300, 1943, 380, 472, 295, 264, 829, 2283, 13], "temperature": 0.0, "avg_logprob": -0.10359415141018954, "compression_ratio": 1.4836065573770492, "no_speech_prob": 3.5858880437444896e-05}, {"id": 203, "seek": 301100, "start": 3011.0, "end": 3026.0, "text": " So since here is showing results from the glove vectors. So the glove vectors have a strong linear component property, just like I showed before for.", "tokens": [407, 1670, 510, 307, 4099, 3542, 490, 264, 26928, 18875, 13, 407, 264, 26928, 18875, 362, 257, 2068, 8213, 6542, 4707, 11, 445, 411, 286, 4712, 949, 337, 13], "temperature": 0.0, "avg_logprob": -0.22354117306795987, "compression_ratio": 1.3185840707964602, "no_speech_prob": 4.249681660439819e-05}, {"id": 204, "seek": 302600, "start": 3026.0, "end": 3040.0, "text": " So this is for the male female dimension. And so because of this, you'd expect in a lot of cases that word analogies would work because I can take the vector difference of man and woman.", "tokens": [407, 341, 307, 337, 264, 7133, 6556, 10139, 13, 400, 370, 570, 295, 341, 11, 291, 1116, 2066, 294, 257, 688, 295, 3331, 300, 1349, 16660, 530, 576, 589, 570, 286, 393, 747, 264, 8062, 2649, 295, 587, 293, 3059, 13], "temperature": 0.0, "avg_logprob": -0.14411291546291774, "compression_ratio": 1.3777777777777778, "no_speech_prob": 5.891237742616795e-05}, {"id": 205, "seek": 304000, "start": 3040.0, "end": 3062.0, "text": " And then if I add that vector difference on to brother, I expect to get to sister and king queen and from any of these examples, but of course they may not always work right because if I start from emperor, it's sort of on a more of a lean and so it might turn out that I get counted so Dutchess coming out instead.", "tokens": [400, 550, 498, 286, 909, 300, 8062, 2649, 322, 281, 3708, 11, 286, 2066, 281, 483, 281, 4892, 293, 4867, 12206, 293, 490, 604, 295, 613, 5110, 11, 457, 295, 1164, 436, 815, 406, 1009, 589, 558, 570, 498, 286, 722, 490, 20255, 11, 309, 311, 1333, 295, 322, 257, 544, 295, 257, 11659, 293, 370, 309, 1062, 1261, 484, 300, 286, 483, 20150, 370, 15719, 442, 1348, 484, 2602, 13], "temperature": 0.0, "avg_logprob": -0.19709967295328776, "compression_ratio": 1.5671641791044777, "no_speech_prob": 1.2203277947264723e-05}, {"id": 206, "seek": 306200, "start": 3062.0, "end": 3072.0, "text": " So you can do this for various different relations, a different semantic relation. So these sort of word vectors actually learn quite a bit of just world knowledge.", "tokens": [407, 291, 393, 360, 341, 337, 3683, 819, 2299, 11, 257, 819, 47982, 9721, 13, 407, 613, 1333, 295, 1349, 18875, 767, 1466, 1596, 257, 857, 295, 445, 1002, 3601, 13], "temperature": 0.0, "avg_logprob": -0.1542748799399724, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.00024490823852829635}, {"id": 207, "seek": 306200, "start": 3072.0, "end": 3083.0, "text": " So here's the company CEO, or this is the company CEO around 2010 to 2014 when the data was taken from word vectors.", "tokens": [407, 510, 311, 264, 2237, 9282, 11, 420, 341, 307, 264, 2237, 9282, 926, 9657, 281, 8227, 562, 264, 1412, 390, 2726, 490, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1542748799399724, "compression_ratio": 1.53551912568306, "no_speech_prob": 0.00024490823852829635}, {"id": 208, "seek": 308300, "start": 3083.0, "end": 3095.0, "text": " And they as well as semantic things or pragmatic things like this, they also learn syntactic things. So here are vectors for positive comparative and support of forms of adjectives.", "tokens": [400, 436, 382, 731, 382, 47982, 721, 420, 46904, 721, 411, 341, 11, 436, 611, 1466, 23980, 19892, 721, 13, 407, 510, 366, 18875, 337, 3353, 39292, 293, 1406, 295, 6422, 295, 29378, 1539, 13], "temperature": 0.0, "avg_logprob": -0.15736948769047576, "compression_ratio": 1.5443037974683544, "no_speech_prob": 8.214057743316516e-05}, {"id": 209, "seek": 308300, "start": 3095.0, "end": 3100.0, "text": " And you can see those also move and roughly linear components.", "tokens": [400, 291, 393, 536, 729, 611, 1286, 293, 9810, 8213, 6677, 13], "temperature": 0.0, "avg_logprob": -0.15736948769047576, "compression_ratio": 1.5443037974683544, "no_speech_prob": 8.214057743316516e-05}, {"id": 210, "seek": 310000, "start": 3100.0, "end": 3115.0, "text": " So the word to back people built a data set of analogies so you could evaluate different models on the accuracy of their analogies. And so here's how you can do this and this gives some numbers.", "tokens": [407, 264, 1349, 281, 646, 561, 3094, 257, 1412, 992, 295, 16660, 530, 370, 291, 727, 13059, 819, 5245, 322, 264, 14170, 295, 641, 16660, 530, 13, 400, 370, 510, 311, 577, 291, 393, 360, 341, 293, 341, 2709, 512, 3547, 13], "temperature": 0.0, "avg_logprob": -0.16336159084154211, "compression_ratio": 1.5449438202247192, "no_speech_prob": 6.195619062054902e-05}, {"id": 211, "seek": 310000, "start": 3115.0, "end": 3121.0, "text": " So there are some mannequins and tactic analogies. I'll just look at the totals.", "tokens": [407, 456, 366, 512, 49815, 358, 1292, 293, 31012, 16660, 530, 13, 286, 603, 445, 574, 412, 264, 1993, 1124, 13], "temperature": 0.0, "avg_logprob": -0.16336159084154211, "compression_ratio": 1.5449438202247192, "no_speech_prob": 6.195619062054902e-05}, {"id": 212, "seek": 312100, "start": 3121.0, "end": 3135.0, "text": " Okay, so what I said before is if you just use unscaled, um, co-occurrence counts and passing through an SVD things work terribly and you see that there you only get 7.3.", "tokens": [1033, 11, 370, 437, 286, 848, 949, 307, 498, 291, 445, 764, 2693, 66, 5573, 11, 1105, 11, 598, 12, 905, 14112, 10760, 14893, 293, 8437, 807, 364, 31910, 35, 721, 589, 22903, 293, 291, 536, 300, 456, 291, 787, 483, 1614, 13, 18, 13], "temperature": 0.0, "avg_logprob": -0.25167819431849886, "compression_ratio": 1.2408759124087592, "no_speech_prob": 0.00014161555736791342}, {"id": 213, "seek": 313500, "start": 3135.0, "end": 3153.0, "text": " And as I also pointed out, if you do some scaling, you can actually get SVD to have of a scaled count matrix to work reasonably well. So this SVD L is similar to the goals model. And now we're getting up to 60.1, which actually isn't a bad score.", "tokens": [400, 382, 286, 611, 10932, 484, 11, 498, 291, 360, 512, 21589, 11, 291, 393, 767, 483, 31910, 35, 281, 362, 295, 257, 36039, 1207, 8141, 281, 589, 23551, 731, 13, 407, 341, 31910, 35, 441, 307, 2531, 281, 264, 5493, 2316, 13, 400, 586, 321, 434, 1242, 493, 281, 4060, 13, 16, 11, 597, 767, 1943, 380, 257, 1578, 6175, 13], "temperature": 0.0, "avg_logprob": -0.14973506060513583, "compression_ratio": 1.3666666666666667, "no_speech_prob": 1.6406362192356028e-05}, {"id": 214, "seek": 315300, "start": 3153.0, "end": 3167.0, "text": " And we can actually do a decent job without a neural network. And then here are the two variants of the word to back model and here are our results from the glove model.", "tokens": [400, 321, 393, 767, 360, 257, 8681, 1691, 1553, 257, 18161, 3209, 13, 400, 550, 510, 366, 264, 732, 21669, 295, 264, 1349, 281, 646, 2316, 293, 510, 366, 527, 3542, 490, 264, 26928, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13093647399506012, "compression_ratio": 1.6057692307692308, "no_speech_prob": 8.586789772380143e-05}, {"id": 215, "seek": 315300, "start": 3167.0, "end": 3180.0, "text": " And of course, at the time, 2014, we took this as absolute proof that our model was better and our more efficient use of statistics was really working in our favor.", "tokens": [400, 295, 1164, 11, 412, 264, 565, 11, 8227, 11, 321, 1890, 341, 382, 8236, 8177, 300, 527, 2316, 390, 1101, 293, 527, 544, 7148, 764, 295, 12523, 390, 534, 1364, 294, 527, 2294, 13], "temperature": 0.0, "avg_logprob": -0.13093647399506012, "compression_ratio": 1.6057692307692308, "no_speech_prob": 8.586789772380143e-05}, {"id": 216, "seek": 318000, "start": 3180.0, "end": 3191.0, "text": " So with 70 years of retrospect, I think that's kind of not really true. It turns out, I think the main part of why we scored better is that we actually had better data.", "tokens": [407, 365, 5285, 924, 295, 34997, 11, 286, 519, 300, 311, 733, 295, 406, 534, 2074, 13, 467, 4523, 484, 11, 286, 519, 264, 2135, 644, 295, 983, 321, 18139, 1101, 307, 300, 321, 767, 632, 1101, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08423540857103136, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.00012318824883550406}, {"id": 217, "seek": 318000, "start": 3191.0, "end": 3209.0, "text": " And so there's a bit of evidence about that on this next slide here. So this looks at the semantics and tactic and overall performance on word analogies of glove models that were trained on different subsets of data.", "tokens": [400, 370, 456, 311, 257, 857, 295, 4467, 466, 300, 322, 341, 958, 4137, 510, 13, 407, 341, 1542, 412, 264, 4361, 45298, 293, 31012, 293, 4787, 3389, 322, 1349, 16660, 530, 295, 26928, 5245, 300, 645, 8895, 322, 819, 2090, 1385, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.08423540857103136, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.00012318824883550406}, {"id": 218, "seek": 320900, "start": 3209.0, "end": 3225.0, "text": " And in particular, the two on the left are trained on Wikipedia. And you can see that training on Wikipedia makes you do really well on semantic analogies, which maybe makes sense because Wikipedia just tells you a lot of semantic facts.", "tokens": [400, 294, 1729, 11, 264, 732, 322, 264, 1411, 366, 8895, 322, 28999, 13, 400, 291, 393, 536, 300, 3097, 322, 28999, 1669, 291, 360, 534, 731, 322, 47982, 16660, 530, 11, 597, 1310, 1669, 2020, 570, 28999, 445, 5112, 291, 257, 688, 295, 47982, 9130, 13], "temperature": 0.0, "avg_logprob": -0.10340618792875314, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00012871253420598805}, {"id": 219, "seek": 320900, "start": 3225.0, "end": 3234.0, "text": " I mean, that's kind of what encyclopedias do. And so one of the big advantages we actually had was that Wikipedia.", "tokens": [286, 914, 11, 300, 311, 733, 295, 437, 465, 34080, 27277, 4609, 360, 13, 400, 370, 472, 295, 264, 955, 14906, 321, 767, 632, 390, 300, 28999, 13], "temperature": 0.0, "avg_logprob": -0.10340618792875314, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.00012871253420598805}, {"id": 220, "seek": 323400, "start": 3234.0, "end": 3246.0, "text": " That the glove model was partly trained on Wikipedia as well as other text, whereas the word to back model that was released was trained exclusively on Google news, so newswire data.", "tokens": [663, 264, 26928, 2316, 390, 17031, 8895, 322, 28999, 382, 731, 382, 661, 2487, 11, 9735, 264, 1349, 281, 646, 2316, 300, 390, 4736, 390, 8895, 20638, 322, 3329, 2583, 11, 370, 2583, 42689, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1398272678769868, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.0002183591714128852}, {"id": 221, "seek": 323400, "start": 3246.0, "end": 3262.0, "text": " And if you only train on a smaller amount of newswire data, you can see that for the semantics, it's, it's just not as good as even a one quarter of the size amount of Wikipedia data.", "tokens": [400, 498, 291, 787, 3847, 322, 257, 4356, 2372, 295, 2583, 42689, 1412, 11, 291, 393, 536, 300, 337, 264, 4361, 45298, 11, 309, 311, 11, 309, 311, 445, 406, 382, 665, 382, 754, 257, 472, 6555, 295, 264, 2744, 2372, 295, 28999, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1398272678769868, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.0002183591714128852}, {"id": 222, "seek": 326200, "start": 3262.0, "end": 3280.0, "text": " So if you get a lot of data, you can compensate for that. So here on the right end, did you then have common crawl web data. And so once there's a lot of web data. So now 42 billion words, you're then starting to get good scores again from the semantic side.", "tokens": [407, 498, 291, 483, 257, 688, 295, 1412, 11, 291, 393, 29458, 337, 300, 13, 407, 510, 322, 264, 558, 917, 11, 630, 291, 550, 362, 2689, 24767, 3670, 1412, 13, 400, 370, 1564, 456, 311, 257, 688, 295, 3670, 1412, 13, 407, 586, 14034, 5218, 2283, 11, 291, 434, 550, 2891, 281, 483, 665, 13444, 797, 490, 264, 47982, 1252, 13], "temperature": 0.0, "avg_logprob": -0.1345658302307129, "compression_ratio": 1.5176470588235293, "no_speech_prob": 6.781442789360881e-05}, {"id": 223, "seek": 328000, "start": 3280.0, "end": 3294.0, "text": " So if you're on the right, then shows how well do you do as you increase the vector dimension. And so what you can see there is, you know, 25 dimensional vectors aren't very good.", "tokens": [407, 498, 291, 434, 322, 264, 558, 11, 550, 3110, 577, 731, 360, 291, 360, 382, 291, 3488, 264, 8062, 10139, 13, 400, 370, 437, 291, 393, 536, 456, 307, 11, 291, 458, 11, 3552, 18795, 18875, 3212, 380, 588, 665, 13], "temperature": 0.0, "avg_logprob": -0.22910379326861838, "compression_ratio": 1.376923076923077, "no_speech_prob": 0.00014357246982399374}, {"id": 224, "seek": 329400, "start": 3294.0, "end": 3317.0, "text": " So that's what I used 100 dimensional vectors when I showed my example in class year, the sweet two long old and working reasonably well, but you still get significant gains for 200 and it's somewhat to 300.", "tokens": [407, 300, 311, 437, 286, 1143, 2319, 18795, 18875, 562, 286, 4712, 452, 1365, 294, 1508, 1064, 11, 264, 3844, 732, 938, 1331, 293, 1364, 23551, 731, 11, 457, 291, 920, 483, 4776, 16823, 337, 2331, 293, 309, 311, 8344, 281, 6641, 13], "temperature": 0.2, "avg_logprob": -0.5195228089677527, "compression_ratio": 1.344155844155844, "no_speech_prob": 8.857177454046905e-05}, {"id": 225, "seek": 331700, "start": 3317.0, "end": 3325.0, "text": " So I found so of 2013 to 15, everyone sort of gravitated to the fact that 300 dimensional vectors is the sweet spot.", "tokens": [407, 286, 1352, 370, 295, 9012, 281, 2119, 11, 1518, 1333, 295, 7427, 18266, 281, 264, 1186, 300, 6641, 18795, 18875, 307, 264, 3844, 4008, 13], "temperature": 0.0, "avg_logprob": -0.21245847232099893, "compression_ratio": 1.6010362694300517, "no_speech_prob": 0.00014379525964614004}, {"id": 226, "seek": 331700, "start": 3325.0, "end": 3340.0, "text": " So almost frequently, if you look through the best known sets of word vectors, then include the word to vectors and the glove vectors that usually what you get is 300 dimensional word vectors.", "tokens": [407, 1920, 10374, 11, 498, 291, 574, 807, 264, 1151, 2570, 6352, 295, 1349, 18875, 11, 550, 4090, 264, 1349, 281, 18875, 293, 264, 26928, 18875, 300, 2673, 437, 291, 483, 307, 6641, 18795, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.21245847232099893, "compression_ratio": 1.6010362694300517, "no_speech_prob": 0.00014379525964614004}, {"id": 227, "seek": 334000, "start": 3340.0, "end": 3353.0, "text": " So that's not the only intrinsic evaluation you can do. Another intrinsic evaluation you can do is see how these models model human judgments of words similarity.", "tokens": [407, 300, 311, 406, 264, 787, 35698, 13344, 291, 393, 360, 13, 3996, 35698, 13344, 291, 393, 360, 307, 536, 577, 613, 5245, 2316, 1952, 40337, 295, 2283, 32194, 13], "temperature": 0.0, "avg_logprob": -0.18060938049765193, "compression_ratio": 1.4594594594594594, "no_speech_prob": 0.00011383849050616845}, {"id": 228, "seek": 335300, "start": 3353.0, "end": 3376.0, "text": " So psychologists for several decades have actually taken human judgments of words similarity where literally you're asking people for pairs of words like professor and doctor to give them a similarity score that's sort of being measured as some continuous quantity giving you a score between say zero and 10.", "tokens": [407, 41562, 337, 2940, 7878, 362, 767, 2726, 1952, 40337, 295, 2283, 32194, 689, 3736, 291, 434, 3365, 561, 337, 15494, 295, 2283, 411, 8304, 293, 4631, 281, 976, 552, 257, 32194, 6175, 300, 311, 1333, 295, 885, 12690, 382, 512, 10957, 11275, 2902, 291, 257, 6175, 1296, 584, 4018, 293, 1266, 13], "temperature": 0.0, "avg_logprob": -0.13093082528365285, "compression_ratio": 1.54, "no_speech_prob": 6.59401121083647e-05}, {"id": 229, "seek": 337600, "start": 3376.0, "end": 3398.0, "text": " And so there are human judgments, which are then averaged over multiple human judgments as to how similar different words are so Tigran cat is pretty similar computer and internet is pretty similar plane and car is less similar stock and CD aren't very similar at all, but stock and Jaguar even less similar.", "tokens": [400, 370, 456, 366, 1952, 40337, 11, 597, 366, 550, 18247, 2980, 670, 3866, 1952, 40337, 382, 281, 577, 2531, 819, 2283, 366, 370, 44550, 4257, 3857, 307, 1238, 2531, 3820, 293, 4705, 307, 1238, 2531, 5720, 293, 1032, 307, 1570, 2531, 4127, 293, 6743, 3212, 380, 588, 2531, 412, 439, 11, 457, 4127, 293, 9014, 20766, 754, 1570, 2531, 13], "temperature": 0.0, "avg_logprob": -0.14445199232835035, "compression_ratio": 1.701657458563536, "no_speech_prob": 7.357743743341416e-05}, {"id": 230, "seek": 339800, "start": 3398.0, "end": 3414.0, "text": " So we could then say for the our models, do they have the same similarity judgments and in particular, we can measure correlation coefficient of whether they give the same ordering of similarity judgments.", "tokens": [407, 321, 727, 550, 584, 337, 264, 527, 5245, 11, 360, 436, 362, 264, 912, 32194, 40337, 293, 294, 1729, 11, 321, 393, 3481, 20009, 17619, 295, 1968, 436, 976, 264, 912, 21739, 295, 32194, 40337, 13], "temperature": 0.0, "avg_logprob": -0.14360146406220226, "compression_ratio": 1.5298507462686568, "no_speech_prob": 0.00010544536053203046}, {"id": 231, "seek": 341400, "start": 3414.0, "end": 3439.0, "text": " And so then we can get data for that. And so there are various different data sets of words similarities and we can score different models as to how well they do on similarities. And again, you see here that playing svds works comparatively better here for similarities that did for analogies, you know, it's not great, but is now not completely terrible.", "tokens": [400, 370, 550, 321, 393, 483, 1412, 337, 300, 13, 400, 370, 456, 366, 3683, 819, 1412, 6352, 295, 2283, 24197, 293, 321, 393, 6175, 819, 5245, 382, 281, 577, 731, 436, 360, 322, 24197, 13, 400, 797, 11, 291, 536, 510, 300, 2433, 262, 85, 16063, 1985, 6311, 19020, 1101, 510, 337, 24197, 300, 630, 337, 16660, 530, 11, 291, 458, 11, 309, 311, 406, 869, 11, 457, 307, 586, 406, 2584, 6237, 13], "temperature": 0.0, "avg_logprob": -0.21623048903066902, "compression_ratio": 1.7317073170731707, "no_speech_prob": 4.3169657146790996e-05}, {"id": 232, "seek": 343900, "start": 3439.0, "end": 3454.0, "text": " Because we no longer need that linear property, but again scaled svds work a lot better word to veck works a bit better than that, and we got some of the same kind of minor advantages from the glove model.", "tokens": [1436, 321, 572, 2854, 643, 300, 8213, 4707, 11, 457, 797, 36039, 262, 85, 16063, 589, 257, 688, 1101, 1349, 281, 1241, 547, 1985, 257, 857, 1101, 813, 300, 11, 293, 321, 658, 512, 295, 264, 912, 733, 295, 6696, 14906, 490, 264, 26928, 2316, 13], "temperature": 0.0, "avg_logprob": -0.1721935491452272, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00012119154416723177}, {"id": 233, "seek": 343900, "start": 3454.0, "end": 3465.0, "text": " Chris, sorry to interrupt a lot of the students who are asking if you could re-explain the objective function for the glove model and also what log by linear means.", "tokens": [6688, 11, 2597, 281, 12729, 257, 688, 295, 264, 1731, 567, 366, 3365, 498, 291, 727, 319, 12, 23040, 491, 264, 10024, 2445, 337, 264, 26928, 2316, 293, 611, 437, 3565, 538, 8213, 1355, 13], "temperature": 0.0, "avg_logprob": -0.1721935491452272, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.00012119154416723177}, {"id": 234, "seek": 346500, "start": 3465.0, "end": 3470.0, "text": " Okay, sure.", "tokens": [1033, 11, 988, 13], "temperature": 0.0, "avg_logprob": -0.2942565055120559, "compression_ratio": 1.1346153846153846, "no_speech_prob": 0.00027322929236106575}, {"id": 235, "seek": 346500, "start": 3470.0, "end": 3477.0, "text": " Okay, here is my here is my objective function.", "tokens": [1033, 11, 510, 307, 452, 510, 307, 452, 10024, 2445, 13], "temperature": 0.0, "avg_logprob": -0.2942565055120559, "compression_ratio": 1.1346153846153846, "no_speech_prob": 0.00027322929236106575}, {"id": 236, "seek": 347700, "start": 3477.0, "end": 3495.0, "text": " So anyway, if I go so one slide before that, right, so the property that we want is that we want the dot product to represent the log probability of co occurrence.", "tokens": [407, 4033, 11, 498, 286, 352, 370, 472, 4137, 949, 300, 11, 558, 11, 370, 264, 4707, 300, 321, 528, 307, 300, 321, 528, 264, 5893, 1674, 281, 2906, 264, 3565, 8482, 295, 598, 36122, 13], "temperature": 0.0, "avg_logprob": -0.2612353801727295, "compression_ratio": 1.38135593220339, "no_speech_prob": 6.391052738763392e-05}, {"id": 237, "seek": 349500, "start": 3495.0, "end": 3511.0, "text": " And that's then gives me my tricky log by linear. So the buy is that there's sort of the w i and the w j so that there are sort of two linear things, and it's linear in each one of them.", "tokens": [400, 300, 311, 550, 2709, 385, 452, 12414, 3565, 538, 8213, 13, 407, 264, 2256, 307, 300, 456, 311, 1333, 295, 264, 261, 741, 293, 264, 261, 361, 370, 300, 456, 366, 1333, 295, 732, 8213, 721, 11, 293, 309, 311, 8213, 294, 1184, 472, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.15072114651019758, "compression_ratio": 1.5121951219512195, "no_speech_prob": 8.74180332175456e-05}, {"id": 238, "seek": 351100, "start": 3511.0, "end": 3528.0, "text": " And this is sort of like having and rather having a sort of an ax where you just have something with linear in x and a is a constant it's by linear because we have the w i w j and this linear in both of them.", "tokens": [400, 341, 307, 1333, 295, 411, 1419, 293, 2831, 1419, 257, 1333, 295, 364, 6360, 689, 291, 445, 362, 746, 365, 8213, 294, 2031, 293, 257, 307, 257, 5754, 309, 311, 538, 8213, 570, 321, 362, 264, 261, 741, 261, 361, 293, 341, 8213, 294, 1293, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.16433000564575195, "compression_ratio": 1.734463276836158, "no_speech_prob": 3.761341577046551e-05}, {"id": 239, "seek": 351100, "start": 3528.0, "end": 3537.0, "text": " And that's then related to the log of a probability, and so that gives us the log by linear model.", "tokens": [400, 300, 311, 550, 4077, 281, 264, 3565, 295, 257, 8482, 11, 293, 370, 300, 2709, 505, 264, 3565, 538, 8213, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16433000564575195, "compression_ratio": 1.734463276836158, "no_speech_prob": 3.761341577046551e-05}, {"id": 240, "seek": 353700, "start": 3537.0, "end": 3553.0, "text": " And so since we since we'd like these things to be equal, what we're doing here, if you ignore these two center terms, is that we're wanting to say the difference between these.", "tokens": [400, 370, 1670, 321, 1670, 321, 1116, 411, 613, 721, 281, 312, 2681, 11, 437, 321, 434, 884, 510, 11, 498, 291, 11200, 613, 732, 3056, 2115, 11, 307, 300, 321, 434, 7935, 281, 584, 264, 2649, 1296, 613, 13], "temperature": 0.0, "avg_logprob": -0.1469461809505116, "compression_ratio": 1.4508196721311475, "no_speech_prob": 0.00010029882832895964}, {"id": 241, "seek": 355300, "start": 3553.0, "end": 3568.0, "text": " And we want that is as small as possible, so we're taking this difference and we're squaring it so it's always positive, and we want that square term to be as small as possible.", "tokens": [400, 321, 528, 300, 307, 382, 1359, 382, 1944, 11, 370, 321, 434, 1940, 341, 2649, 293, 321, 434, 2339, 1921, 309, 370, 309, 311, 1009, 3353, 11, 293, 321, 528, 300, 3732, 1433, 281, 312, 382, 1359, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.29097972446017795, "compression_ratio": 1.5391304347826087, "no_speech_prob": 2.5835834094323218e-05}, {"id": 242, "seek": 356800, "start": 3568.0, "end": 3591.0, "text": " And you can basically stop there, but the other bit that's in here is a lot of the time when you're building models, rather than simply having sort of an ax model, it seems useful to have a bias term, which can move things up and down for the word in general.", "tokens": [400, 291, 393, 1936, 1590, 456, 11, 457, 264, 661, 857, 300, 311, 294, 510, 307, 257, 688, 295, 264, 565, 562, 291, 434, 2390, 5245, 11, 2831, 813, 2935, 1419, 1333, 295, 364, 6360, 2316, 11, 309, 2544, 4420, 281, 362, 257, 12577, 1433, 11, 597, 393, 1286, 721, 493, 293, 760, 337, 264, 1349, 294, 2674, 13], "temperature": 0.0, "avg_logprob": -0.07731942524985662, "compression_ratio": 1.5146198830409356, "no_speech_prob": 1.4026880307937972e-05}, {"id": 243, "seek": 359100, "start": 3591.0, "end": 3609.0, "text": " And so we added into the model bias term so that there's a bias term for both words, so if in general probabilities are high for a certain word, this bias term can model that and for the other word this bias term model it okay.", "tokens": [400, 370, 321, 3869, 666, 264, 2316, 12577, 1433, 370, 300, 456, 311, 257, 12577, 1433, 337, 1293, 2283, 11, 370, 498, 294, 2674, 33783, 366, 1090, 337, 257, 1629, 1349, 11, 341, 12577, 1433, 393, 2316, 300, 293, 337, 264, 661, 1349, 341, 12577, 1433, 2316, 309, 1392, 13], "temperature": 0.0, "avg_logprob": -0.16623007809674298, "compression_ratio": 1.6569343065693432, "no_speech_prob": 2.0399669665494002e-05}, {"id": 244, "seek": 360900, "start": 3609.0, "end": 3624.0, "text": " And now I'll pop back and after actually I just saw someone said why multiplying by the f of sorry I did skip that last term.", "tokens": [400, 586, 286, 603, 1665, 646, 293, 934, 767, 286, 445, 1866, 1580, 848, 983, 30955, 538, 264, 283, 295, 2597, 286, 630, 10023, 300, 1036, 1433, 13], "temperature": 0.0, "avg_logprob": -0.1860576570034027, "compression_ratio": 1.2019230769230769, "no_speech_prob": 2.6236442863591947e-05}, {"id": 245, "seek": 362400, "start": 3624.0, "end": 3653.0, "text": " Okay, the why modifying by this f of x i j so this last bit was to scale things, depending on the frequency of a word, because you want to pay more attention to words that are more common or word pairs that are more common, because you know, if you think about it in word,", "tokens": [1033, 11, 264, 983, 42626, 538, 341, 283, 295, 2031, 741, 361, 370, 341, 1036, 857, 390, 281, 4373, 721, 11, 5413, 322, 264, 7893, 295, 257, 1349, 11, 570, 291, 528, 281, 1689, 544, 3202, 281, 2283, 300, 366, 544, 2689, 420, 1349, 15494, 300, 366, 544, 2689, 11, 570, 291, 458, 11, 498, 291, 519, 466, 309, 294, 1349, 11], "temperature": 0.0, "avg_logprob": -0.18407789866129556, "compression_ratio": 1.6790123456790123, "no_speech_prob": 5.6210450566140935e-05}, {"id": 246, "seek": 365300, "start": 3653.0, "end": 3672.0, "text": " you're seeing if things have a coerced current to count of 50 versus three, you want to do a better job at modeling the coerced current of the things that occurred together 50 times.", "tokens": [291, 434, 2577, 498, 721, 362, 257, 598, 260, 1232, 2190, 281, 1207, 295, 2625, 5717, 1045, 11, 291, 528, 281, 360, 257, 1101, 1691, 412, 15983, 264, 598, 260, 1232, 2190, 295, 264, 721, 300, 11068, 1214, 2625, 1413, 13], "temperature": 0.0, "avg_logprob": -0.3268002616034614, "compression_ratio": 1.4796747967479675, "no_speech_prob": 0.0004026223032269627}, {"id": 247, "seek": 367200, "start": 3672.0, "end": 3699.0, "text": " So you want to consider in the count of coerced currents, but then the argument is that that actually lead to a stray when you have extremely common words like function words, and so effectively you paid more attention to words that co occurred together up until a certain point and then the curve just went flat, so it didn't matter if it was an extremely extremely common word.", "tokens": [407, 291, 528, 281, 1949, 294, 264, 1207, 295, 598, 260, 1232, 30110, 11, 457, 550, 264, 6770, 307, 300, 300, 767, 1477, 281, 257, 36219, 562, 291, 362, 4664, 2689, 2283, 411, 2445, 2283, 11, 293, 370, 8659, 291, 4835, 544, 3202, 281, 2283, 300, 598, 11068, 1214, 493, 1826, 257, 1629, 935, 293, 550, 264, 7605, 445, 1437, 4962, 11, 370, 309, 994, 380, 1871, 498, 309, 390, 364, 4664, 4664, 2689, 1349, 13], "temperature": 0.0, "avg_logprob": -0.11512417793273926, "compression_ratio": 1.7072072072072073, "no_speech_prob": 1.0607441254251171e-05}, {"id": 248, "seek": 369900, "start": 3699.0, "end": 3718.0, "text": " So then for extrinsic word vector evaluation, so at this point, you're now wanting to sort of say well, can we embed our word vectors in some end user task and do they help.", "tokens": [407, 550, 337, 16455, 1292, 299, 1349, 8062, 13344, 11, 370, 412, 341, 935, 11, 291, 434, 586, 7935, 281, 1333, 295, 584, 731, 11, 393, 321, 12240, 527, 1349, 18875, 294, 512, 917, 4195, 5633, 293, 360, 436, 854, 13], "temperature": 0.0, "avg_logprob": -0.10728720559014214, "compression_ratio": 1.373015873015873, "no_speech_prob": 6.671300070593134e-05}, {"id": 249, "seek": 371800, "start": 3718.0, "end": 3747.0, "text": " And do different word vectors work better or worse than other word vectors, so this is something that will see a lot of later in the class, I mean in particular, when you get on to doing assignment three that assignment three, you get to build dependency parsers and you can then use word vectors in the dependency parser and see how much they help we don't actually make you test out different sets of word vectors, but you could.", "tokens": [400, 360, 819, 1349, 18875, 589, 1101, 420, 5324, 813, 661, 1349, 18875, 11, 370, 341, 307, 746, 300, 486, 536, 257, 688, 295, 1780, 294, 264, 1508, 11, 286, 914, 294, 1729, 11, 562, 291, 483, 322, 281, 884, 15187, 1045, 300, 15187, 1045, 11, 291, 483, 281, 1322, 33621, 21156, 433, 293, 291, 393, 550, 764, 1349, 18875, 294, 264, 33621, 21156, 260, 293, 536, 577, 709, 436, 854, 321, 500, 380, 767, 652, 291, 1500, 484, 819, 6352, 295, 1349, 18875, 11, 457, 291, 727, 13], "temperature": 0.0, "avg_logprob": -0.12795313968453356, "compression_ratio": 1.8418803418803418, "no_speech_prob": 2.6900532247964293e-05}, {"id": 250, "seek": 374700, "start": 3747.0, "end": 3766.0, "text": " So here's just one example of this to give you a sense, so the task of named entity recognition is going through a piece of text and identifying mentions of a person name or an organization name like a company or a location and so.", "tokens": [407, 510, 311, 445, 472, 1365, 295, 341, 281, 976, 291, 257, 2020, 11, 370, 264, 5633, 295, 4926, 13977, 11150, 307, 516, 807, 257, 2522, 295, 2487, 293, 16696, 23844, 295, 257, 954, 1315, 420, 364, 4475, 1315, 411, 257, 2237, 420, 257, 4914, 293, 370, 13], "temperature": 0.0, "avg_logprob": -0.10854815519773044, "compression_ratio": 1.5098039215686274, "no_speech_prob": 0.00036629298119805753}, {"id": 251, "seek": 376600, "start": 3766.0, "end": 3793.0, "text": " If you have good word vectors, do they help you do named entity recognition better and the answer that is yes, so if one starts off with a model that simply has discrete features so it uses word identity as features, you can build a pretty good name density model doing that, but if you add into it word vectors, you get a better representation of the meaning of words.", "tokens": [759, 291, 362, 665, 1349, 18875, 11, 360, 436, 854, 291, 360, 4926, 13977, 11150, 1101, 293, 264, 1867, 300, 307, 2086, 11, 370, 498, 472, 3719, 766, 365, 257, 2316, 300, 2935, 575, 27706, 4122, 370, 309, 4960, 1349, 6575, 382, 4122, 11, 291, 393, 1322, 257, 1238, 665, 1315, 10305, 2316, 884, 300, 11, 457, 498, 291, 909, 666, 309, 1349, 18875, 11, 291, 483, 257, 1101, 10290, 295, 264, 3620, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.12949248552322387, "compression_ratio": 1.716279069767442, "no_speech_prob": 4.6793418732704595e-05}, {"id": 252, "seek": 379300, "start": 3793.0, "end": 3805.0, "text": " So you can do that you can have the numbers go up quite a bit and then you can compare different models to see how much gain they give you in terms of this extrinsic task.", "tokens": [407, 291, 393, 360, 300, 291, 393, 362, 264, 3547, 352, 493, 1596, 257, 857, 293, 550, 291, 393, 6794, 819, 5245, 281, 536, 577, 709, 6052, 436, 976, 291, 294, 2115, 295, 341, 16455, 1292, 299, 5633, 13], "temperature": 0.0, "avg_logprob": -0.1336321608964787, "compression_ratio": 1.4132231404958677, "no_speech_prob": 0.0006023002206347883}, {"id": 253, "seek": 380500, "start": 3805.0, "end": 3820.0, "text": " So skipping ahead, this was a question that I asked after class, which was word sensors because so far we've had just one word.", "tokens": [407, 31533, 2286, 11, 341, 390, 257, 1168, 300, 286, 2351, 934, 1508, 11, 597, 390, 1349, 14840, 570, 370, 1400, 321, 600, 632, 445, 472, 1349, 13], "temperature": 0.0, "avg_logprob": -0.23877279460430145, "compression_ratio": 1.2211538461538463, "no_speech_prob": 4.8170073569053784e-05}, {"id": 254, "seek": 382000, "start": 3820.0, "end": 3848.0, "text": " So for one particular string we've got some string house and we're going to say for each of those strings there's a word vector and if you think about it a bit more that seems like it's very weird because actually most words, especially common words and especially words that have existed for a long time actually have many meanings, which are very different.", "tokens": [407, 337, 472, 1729, 6798, 321, 600, 658, 512, 6798, 1782, 293, 321, 434, 516, 281, 584, 337, 1184, 295, 729, 13985, 456, 311, 257, 1349, 8062, 293, 498, 291, 519, 466, 309, 257, 857, 544, 300, 2544, 411, 309, 311, 588, 3657, 570, 767, 881, 2283, 11, 2318, 2689, 2283, 293, 2318, 2283, 300, 362, 13135, 337, 257, 938, 565, 767, 362, 867, 28138, 11, 597, 366, 588, 819, 13], "temperature": 0.0, "avg_logprob": -0.09666262944539387, "compression_ratio": 1.7014218009478672, "no_speech_prob": 5.137859261594713e-05}, {"id": 255, "seek": 384800, "start": 3848.0, "end": 3862.0, "text": " So how could that be captured if you only have one word vector for the word because you can't actually capture the fact that you've got different meanings for the word because your meaning for the word is just one point in space one vector.", "tokens": [407, 577, 727, 300, 312, 11828, 498, 291, 787, 362, 472, 1349, 8062, 337, 264, 1349, 570, 291, 393, 380, 767, 7983, 264, 1186, 300, 291, 600, 658, 819, 28138, 337, 264, 1349, 570, 428, 3620, 337, 264, 1349, 307, 445, 472, 935, 294, 1901, 472, 8062, 13], "temperature": 0.0, "avg_logprob": -0.08601893828465389, "compression_ratio": 1.6783216783216783, "no_speech_prob": 0.00017109938198700547}, {"id": 256, "seek": 386200, "start": 3862.0, "end": 3878.0, "text": " And so as an example of that here's the word pie.", "tokens": [400, 370, 382, 364, 1365, 295, 300, 510, 311, 264, 1349, 1730, 13], "temperature": 0.0, "avg_logprob": -0.29683817134184, "compression_ratio": 0.8909090909090909, "no_speech_prob": 5.768268692918355e-06}, {"id": 257, "seek": 387800, "start": 3878.0, "end": 3900.0, "text": " So for a minute and think what word meanings the word pie cares. And it actually turns out you know it has a lot of different meaning so so perhaps the most basic meaning is if you did fantasy games or something medieval weapons.", "tokens": [407, 337, 257, 3456, 293, 519, 437, 1349, 28138, 264, 1349, 1730, 12310, 13, 400, 309, 767, 4523, 484, 291, 458, 309, 575, 257, 688, 295, 819, 3620, 370, 370, 4317, 264, 881, 3875, 3620, 307, 498, 291, 630, 13861, 2813, 420, 746, 24078, 7278, 13], "temperature": 0.0, "avg_logprob": -0.2138601303100586, "compression_ratio": 1.4774193548387098, "no_speech_prob": 2.9733777410001494e-05}, {"id": 258, "seek": 390000, "start": 3900.0, "end": 3909.0, "text": " And it's a kind of a kind of a fish that has a similar elongated shape that's a pike.", "tokens": [400, 309, 311, 257, 733, 295, 257, 733, 295, 257, 3506, 300, 575, 257, 2531, 40786, 770, 3909, 300, 311, 257, 36242, 13], "temperature": 0.0, "avg_logprob": -0.32793941745510347, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.00029785220976918936}, {"id": 259, "seek": 390000, "start": 3909.0, "end": 3926.0, "text": " It was used for railroad lines maybe that usage isn't used much anymore but it's certainly still survived and referring to roads so this is like when you have turn pikes we have expressions where pike means the future like coming down the pike.", "tokens": [467, 390, 1143, 337, 30073, 3876, 1310, 300, 14924, 1943, 380, 1143, 709, 3602, 457, 309, 311, 3297, 920, 14433, 293, 13761, 281, 11344, 370, 341, 307, 411, 562, 291, 362, 1261, 280, 8916, 321, 362, 15277, 689, 36242, 1355, 264, 2027, 411, 1348, 760, 264, 36242, 13], "temperature": 0.0, "avg_logprob": -0.32793941745510347, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.00029785220976918936}, {"id": 260, "seek": 392600, "start": 3926.0, "end": 3931.0, "text": " And then there's a position in diving that divers do a pike.", "tokens": [400, 550, 456, 311, 257, 2535, 294, 20241, 300, 6111, 360, 257, 36242, 13], "temperature": 0.0, "avg_logprob": -0.2398896650834517, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.00017617468256503344}, {"id": 261, "seek": 392600, "start": 3931.0, "end": 3939.0, "text": " Those are all noun uses. They're also verbal uses so you can pike somebody with your pike.", "tokens": [3950, 366, 439, 23307, 4960, 13, 814, 434, 611, 24781, 4960, 370, 291, 393, 36242, 2618, 365, 428, 36242, 13], "temperature": 0.0, "avg_logprob": -0.2398896650834517, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.00017617468256503344}, {"id": 262, "seek": 392600, "start": 3939.0, "end": 3950.0, "text": " You know different usages might have different currency in a stray you can also use pike to mean that you pull out of doing something like I reckon he's going to pike.", "tokens": [509, 458, 819, 505, 1660, 1062, 362, 819, 13346, 294, 257, 36219, 291, 393, 611, 764, 36242, 281, 914, 300, 291, 2235, 484, 295, 884, 746, 411, 286, 29548, 415, 311, 516, 281, 36242, 13], "temperature": 0.0, "avg_logprob": -0.2398896650834517, "compression_ratio": 1.619289340101523, "no_speech_prob": 0.00017617468256503344}, {"id": 263, "seek": 395000, "start": 3950.0, "end": 3958.0, "text": " And that usage is used in America but lots of meanings and actually for words that commoner if you start thinking words like line or field.", "tokens": [400, 300, 14924, 307, 1143, 294, 3374, 457, 3195, 295, 28138, 293, 767, 337, 2283, 300, 2689, 260, 498, 291, 722, 1953, 2283, 411, 1622, 420, 2519, 13], "temperature": 0.0, "avg_logprob": -0.18675823869376346, "compression_ratio": 1.5617283950617284, "no_speech_prob": 6.291646423051134e-05}, {"id": 264, "seek": 395000, "start": 3958.0, "end": 3966.0, "text": " I mean they just have even more meanings than this so what are we actually doing with just one vector for a word.", "tokens": [286, 914, 436, 445, 362, 754, 544, 28138, 813, 341, 370, 437, 366, 321, 767, 884, 365, 445, 472, 8062, 337, 257, 1349, 13], "temperature": 0.0, "avg_logprob": -0.18675823869376346, "compression_ratio": 1.5617283950617284, "no_speech_prob": 6.291646423051134e-05}, {"id": 265, "seek": 396600, "start": 3966.0, "end": 3989.0, "text": " Well one way you could go is to say okay up until now what we've done is crazy pike has and other words have all of these different meanings so maybe what we should do is have different word vectors for the different meanings of pike so we'd have one word vector for the medieval pointy weapon.", "tokens": [1042, 472, 636, 291, 727, 352, 307, 281, 584, 1392, 493, 1826, 586, 437, 321, 600, 1096, 307, 3219, 36242, 575, 293, 661, 2283, 362, 439, 295, 613, 819, 28138, 370, 1310, 437, 321, 820, 360, 307, 362, 819, 1349, 18875, 337, 264, 819, 28138, 295, 36242, 370, 321, 1116, 362, 472, 1349, 8062, 337, 264, 24078, 935, 88, 7463, 13], "temperature": 0.0, "avg_logprob": -0.11229881139901968, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.00010667792230378836}, {"id": 266, "seek": 398900, "start": 3989.0, "end": 3999.0, "text": " Another word vector for the kind of fish another word vector for the kind of road so that they then be words sense vectors.", "tokens": [3996, 1349, 8062, 337, 264, 733, 295, 3506, 1071, 1349, 8062, 337, 264, 733, 295, 3060, 370, 300, 436, 550, 312, 2283, 2020, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1477301279703776, "compression_ratio": 1.7328767123287672, "no_speech_prob": 0.00013943146041128784}, {"id": 267, "seek": 398900, "start": 3999.0, "end": 4010.0, "text": " And you can do that I mean actually we were working on that in the early 2000 and 10s actually even before word to that came out.", "tokens": [400, 291, 393, 360, 300, 286, 914, 767, 321, 645, 1364, 322, 300, 294, 264, 2440, 8132, 293, 1266, 82, 767, 754, 949, 1349, 281, 300, 1361, 484, 13], "temperature": 0.0, "avg_logprob": -0.1477301279703776, "compression_ratio": 1.7328767123287672, "no_speech_prob": 0.00013943146041128784}, {"id": 268, "seek": 401000, "start": 4010.0, "end": 4034.0, "text": " So this picture is a little bit small to see but what we were doing was for words we work clustering instances of a word hoping that those clusters so clustering the word tokens hoping those clusters that were similar represented sensors and then for the clusters of word tokens.", "tokens": [407, 341, 3036, 307, 257, 707, 857, 1359, 281, 536, 457, 437, 321, 645, 884, 390, 337, 2283, 321, 589, 596, 48673, 14519, 295, 257, 1349, 7159, 300, 729, 23313, 370, 596, 48673, 264, 1349, 22667, 7159, 729, 23313, 300, 645, 2531, 10379, 14840, 293, 550, 337, 264, 23313, 295, 1349, 22667, 13], "temperature": 0.0, "avg_logprob": -0.11521008140162418, "compression_ratio": 1.7658227848101267, "no_speech_prob": 8.648255607113242e-05}, {"id": 269, "seek": 403400, "start": 4034.0, "end": 4054.0, "text": " So treating them like they were separate words and learning a word vector for each and you know basically that actually works so in green we have two sensors for the word bank and so there's one sense for the word bank that's over here where it's close to words like banking finance transaction laundering.", "tokens": [407, 15083, 552, 411, 436, 645, 4994, 2283, 293, 2539, 257, 1349, 8062, 337, 1184, 293, 291, 458, 1936, 300, 767, 1985, 370, 294, 3092, 321, 362, 732, 14840, 337, 264, 1349, 3765, 293, 370, 456, 311, 472, 2020, 337, 264, 1349, 3765, 300, 311, 670, 510, 689, 309, 311, 1998, 281, 2283, 411, 18261, 10719, 14425, 17245, 1794, 13], "temperature": 0.0, "avg_logprob": -0.1060987040400505, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.00017927265434991568}, {"id": 270, "seek": 405400, "start": 4054.0, "end": 4065.0, "text": " And then we have another sense for the word bank over here whereas close to words like plateau boundary gap territory which is the river bank sense of the word bank.", "tokens": [400, 550, 321, 362, 1071, 2020, 337, 264, 1349, 3765, 670, 510, 9735, 1998, 281, 2283, 411, 39885, 12866, 7417, 11360, 597, 307, 264, 6810, 3765, 2020, 295, 264, 1349, 3765, 13], "temperature": 0.0, "avg_logprob": -0.15245216369628906, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.984715658589266e-05}, {"id": 271, "seek": 405400, "start": 4065.0, "end": 4069.0, "text": " And for the word jacuar that's in purple.", "tokens": [400, 337, 264, 1349, 361, 326, 20766, 300, 311, 294, 9656, 13], "temperature": 0.0, "avg_logprob": -0.15245216369628906, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.984715658589266e-05}, {"id": 272, "seek": 406900, "start": 4069.0, "end": 4088.0, "text": " Well, jacuar has a number of sensors and so we have those as well so this sense down here is close to hunter so that's the sort of big game animal sense of jaguar up the top here is being shown close to luxury and convertibles is the jaguar car sense.", "tokens": [1042, 11, 361, 326, 20766, 575, 257, 1230, 295, 14840, 293, 370, 321, 362, 729, 382, 731, 370, 341, 2020, 760, 510, 307, 1998, 281, 22970, 370, 300, 311, 264, 1333, 295, 955, 1216, 5496, 2020, 295, 6368, 20766, 493, 264, 1192, 510, 307, 885, 4898, 1998, 281, 15558, 293, 7620, 14428, 307, 264, 6368, 20766, 1032, 2020, 13], "temperature": 0.0, "avg_logprob": -0.12877659570603145, "compression_ratio": 1.5886075949367089, "no_speech_prob": 4.4439111661631614e-05}, {"id": 273, "seek": 408800, "start": 4088.0, "end": 4116.0, "text": " Then jaguar here is near string keyboard and words like that so jaguars the name of a kind of keyboard and then this final jaguar over here is close to software and Microsoft and then if you're old enough you'll remember that there was an old version of macOS so it was called jaguar so that's then the computer sense so basically this does work and we can learn word vectors.", "tokens": [1396, 6368, 20766, 510, 307, 2651, 6798, 10186, 293, 2283, 411, 300, 370, 6368, 84, 685, 264, 1315, 295, 257, 733, 295, 10186, 293, 550, 341, 2572, 6368, 20766, 670, 510, 307, 1998, 281, 4722, 293, 8116, 293, 550, 498, 291, 434, 1331, 1547, 291, 603, 1604, 300, 456, 390, 364, 1331, 3037, 295, 7912, 4367, 370, 309, 390, 1219, 6368, 20766, 370, 300, 311, 550, 264, 3820, 2020, 370, 1936, 341, 775, 589, 293, 321, 393, 1466, 1349, 18875, 13], "temperature": 0.0, "avg_logprob": -0.12309339186724494, "compression_ratio": 1.724770642201835, "no_speech_prob": 4.7515375626971945e-05}, {"id": 274, "seek": 411600, "start": 4116.0, "end": 4135.0, "text": " So there's a different sensors of a word but actually this isn't the majority way that things have then gone in practice and there are kind of a couple of reasons for that I mean one is just simplicity if you do this.", "tokens": [407, 456, 311, 257, 819, 14840, 295, 257, 1349, 457, 767, 341, 1943, 380, 264, 6286, 636, 300, 721, 362, 550, 2780, 294, 3124, 293, 456, 366, 733, 295, 257, 1916, 295, 4112, 337, 300, 286, 914, 472, 307, 445, 25632, 498, 291, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.1959296417236328, "compression_ratio": 1.4563758389261745, "no_speech_prob": 0.00015816812810953707}, {"id": 275, "seek": 413500, "start": 4135.0, "end": 4152.0, "text": " And then you're going to have a kind of complex because you first of all have to learn word sensors and then start learning word vectors in terms of the word sensors.", "tokens": [400, 550, 291, 434, 516, 281, 362, 257, 733, 295, 3997, 570, 291, 700, 295, 439, 362, 281, 1466, 1349, 14840, 293, 550, 722, 2539, 1349, 18875, 294, 2115, 295, 264, 1349, 14840, 13], "temperature": 0.0, "avg_logprob": -0.40810163397538035, "compression_ratio": 1.456140350877193, "no_speech_prob": 8.707864617463201e-05}, {"id": 276, "seek": 415200, "start": 4152.0, "end": 4167.0, "text": " So it's commonly what's being used in natural language processing I mean it tends to be imperfect in its own way because we're trying to take all the uses of the word pike and sort of cut them up into key different sensors where", "tokens": [407, 309, 311, 12719, 437, 311, 885, 1143, 294, 3303, 2856, 9007, 286, 914, 309, 12258, 281, 312, 26714, 294, 1080, 1065, 636, 570, 321, 434, 1382, 281, 747, 439, 264, 4960, 295, 264, 1349, 36242, 293, 1333, 295, 1723, 552, 493, 666, 2141, 819, 14840, 689], "temperature": 0.0, "avg_logprob": -0.17679094800762102, "compression_ratio": 1.4709677419354839, "no_speech_prob": 0.00017003467655740678}, {"id": 277, "seek": 416700, "start": 4167.0, "end": 4195.0, "text": " the difference is kind of overlapping and it's often not clear which ones to count as distinct so for example here right a railroad line and a type of road sort of that's the same sense of pike it's just that they're different forms of transportation and so you know that this could be you know a type of transportation line and cover both of them so it's always sort of very unclear how you cut word meaning into", "tokens": [264, 2649, 307, 733, 295, 33535, 293, 309, 311, 2049, 406, 1850, 597, 2306, 281, 1207, 382, 10644, 370, 337, 1365, 510, 558, 257, 30073, 1622, 293, 257, 2010, 295, 3060, 1333, 295, 300, 311, 264, 912, 2020, 295, 36242, 309, 311, 445, 300, 436, 434, 819, 6422, 295, 11328, 293, 370, 291, 458, 300, 341, 727, 312, 291, 458, 257, 2010, 295, 11328, 1622, 293, 2060, 1293, 295, 552, 370, 309, 311, 1009, 1333, 295, 588, 25636, 577, 291, 1723, 1349, 3620, 666], "temperature": 0.0, "avg_logprob": -0.12055349349975586, "compression_ratio": 1.8114035087719298, "no_speech_prob": 8.417927892878652e-05}, {"id": 278, "seek": 419500, "start": 4195.0, "end": 4202.0, "text": " different sensors and indeed if you look at different dictionaries everyone does it differently.", "tokens": [819, 14840, 293, 6451, 498, 291, 574, 412, 819, 22352, 4889, 1518, 775, 309, 7614, 13], "temperature": 0.0, "avg_logprob": -0.12487287521362304, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.077386883087456e-05}, {"id": 279, "seek": 419500, "start": 4202.0, "end": 4222.0, "text": " So it actually turns out that in practice you can do rather well by simply having one word vector per word type and what happens if you do that well what you find", "tokens": [407, 309, 767, 4523, 484, 300, 294, 3124, 291, 393, 360, 2831, 731, 538, 2935, 1419, 472, 1349, 8062, 680, 1349, 2010, 293, 437, 2314, 498, 291, 360, 300, 731, 437, 291, 915], "temperature": 0.0, "avg_logprob": -0.12487287521362304, "compression_ratio": 1.5889570552147239, "no_speech_prob": 8.077386883087456e-05}, {"id": 280, "seek": 422200, "start": 4222.0, "end": 4250.0, "text": " is that what you learn as a word vector is what gets referred to in fancy talk as a super super position of the diff of the word vectors for the different sensors of a word where the word super position means no more or less than a weighted some so out the vector that we learn for pike will be a weighted average of the", "tokens": [307, 300, 437, 291, 1466, 382, 257, 1349, 8062, 307, 437, 2170, 10839, 281, 294, 10247, 751, 382, 257, 1687, 1687, 2535, 295, 264, 7593, 295, 264, 1349, 18875, 337, 264, 819, 14840, 295, 257, 1349, 689, 264, 1349, 1687, 2535, 1355, 572, 544, 420, 1570, 813, 257, 32807, 512, 370, 484, 264, 8062, 300, 321, 1466, 337, 36242, 486, 312, 257, 32807, 4274, 295, 264], "temperature": 0.0, "avg_logprob": -0.21694181987217495, "compression_ratio": 1.9047619047619047, "no_speech_prob": 0.0005473056226037443}, {"id": 281, "seek": 425000, "start": 4250.0, "end": 4277.0, "text": " vectors that you would have learned for the medieval weapon sense plus the fish sense plus the road sense plus whatever other sensors that you have where the weighting that's given to these different sense vectors corresponds to the frequencies of use of the different sensors so we end up with the word the vector for pike being a kind of an average vector", "tokens": [18875, 300, 291, 576, 362, 3264, 337, 264, 24078, 7463, 2020, 1804, 264, 3506, 2020, 1804, 264, 3060, 2020, 1804, 2035, 661, 14840, 300, 291, 362, 689, 264, 3364, 278, 300, 311, 2212, 281, 613, 819, 2020, 18875, 23249, 281, 264, 20250, 295, 764, 295, 264, 819, 14840, 370, 321, 917, 493, 365, 264, 1349, 264, 8062, 337, 36242, 885, 257, 733, 295, 364, 4274, 8062], "temperature": 0.0, "avg_logprob": -0.10627337183271135, "compression_ratio": 1.859375, "no_speech_prob": 0.00023321881599258631}, {"id": 282, "seek": 427700, "start": 4277.0, "end": 4300.0, "text": " so if you're say okay you've just added up several different vectors into an average you might think that that's kind of useless because you know you've lost the real meanings of the word you've just got some kind of funny average vector that's in between them", "tokens": [370, 498, 291, 434, 584, 1392, 291, 600, 445, 3869, 493, 2940, 819, 18875, 666, 364, 4274, 291, 1062, 519, 300, 300, 311, 733, 295, 14115, 570, 291, 458, 291, 600, 2731, 264, 957, 28138, 295, 264, 1349, 291, 600, 445, 658, 512, 733, 295, 4074, 4274, 8062, 300, 311, 294, 1296, 552], "temperature": 0.0, "avg_logprob": -0.16459702608878152, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00019087610417045653}, {"id": 283, "seek": 430000, "start": 4300.0, "end": 4328.0, "text": " and then suddenly it turns out that if you use this average vector in applications it tends to sort of self disambiguate because if you say is the word pike similar to the word for fish well part of this vector represents fish the fish sense of pike and so in those components it will be kind of similar to the fish vector", "tokens": [293, 550, 5800, 309, 4523, 484, 300, 498, 291, 764, 341, 4274, 8062, 294, 5821, 309, 12258, 281, 1333, 295, 2698, 717, 2173, 328, 10107, 570, 498, 291, 584, 307, 264, 1349, 36242, 2531, 281, 264, 1349, 337, 3506, 731, 644, 295, 341, 8062, 8855, 3506, 264, 3506, 2020, 295, 36242, 293, 370, 294, 729, 6677, 309, 486, 312, 733, 295, 2531, 281, 264, 3506, 8062], "temperature": 0.0, "avg_logprob": -0.19753191811697823, "compression_ratio": 1.7405405405405405, "no_speech_prob": 3.477404243312776e-05}, {"id": 284, "seek": 432800, "start": 4328.0, "end": 4355.0, "text": " so yes you'll say the substantial similarity whereas if in another piece of text that says you know the men were armed with pikes and lancers or pikes and maces or whatever other many of the weapons you remember well actually some of that meaning is in the pike vector as well and so it will say yeah there's good similarity", "tokens": [370, 2086, 291, 603, 584, 264, 16726, 32194, 9735, 498, 294, 1071, 2522, 295, 2487, 300, 1619, 291, 458, 264, 1706, 645, 16297, 365, 280, 8916, 293, 287, 4463, 433, 420, 280, 8916, 293, 275, 2116, 420, 2035, 661, 867, 295, 264, 7278, 291, 1604, 731, 767, 512, 295, 300, 3620, 307, 294, 264, 36242, 8062, 382, 731, 293, 370, 309, 486, 584, 1338, 456, 311, 665, 32194], "temperature": 0.0, "avg_logprob": -0.23069365819295248, "compression_ratio": 1.6963350785340314, "no_speech_prob": 0.0004095555341336876}, {"id": 285, "seek": 435500, "start": 4355.0, "end": 4372.0, "text": " mace and staff and words like that as well and in fact we can work out which sense of pike is intended by just sort of seeing which components are similar to other words that are used in the same context", "tokens": [275, 617, 293, 3525, 293, 2283, 411, 300, 382, 731, 293, 294, 1186, 321, 393, 589, 484, 597, 2020, 295, 36242, 307, 10226, 538, 445, 1333, 295, 2577, 597, 6677, 366, 2531, 281, 661, 2283, 300, 366, 1143, 294, 264, 912, 4319], "temperature": 0.0, "avg_logprob": -0.0550147761469302, "compression_ratio": 1.5378787878787878, "no_speech_prob": 0.000856259255670011}, {"id": 286, "seek": 437200, "start": 4372.0, "end": 4391.0, "text": " indeed there's actually a much more surprising result than that and this is a result that's Jews Sangev Aurora, Tung Ruma who is now on our Stanford faculty and others in 2018 and that's the following result which I'm not actually going to explain but", "tokens": [6451, 456, 311, 767, 257, 709, 544, 8830, 1874, 813, 300, 293, 341, 307, 257, 1874, 300, 311, 11041, 318, 933, 85, 40663, 11, 314, 1063, 497, 5544, 567, 307, 586, 322, 527, 20374, 6389, 293, 2357, 294, 6096, 293, 300, 311, 264, 3480, 1874, 597, 286, 478, 406, 767, 516, 281, 2903, 457], "temperature": 0.0, "avg_logprob": -0.2851170178117423, "compression_ratio": 1.4593023255813953, "no_speech_prob": 0.000191151921171695}, {"id": 287, "seek": 439100, "start": 4391.0, "end": 4414.0, "text": " so if you think that the vector for pike is just a sum of the vectors for the different sensors well it should be you'd think that it's just completely impossible to reconstruct the sense vectors from the vector for the word type", "tokens": [370, 498, 291, 519, 300, 264, 8062, 337, 36242, 307, 445, 257, 2408, 295, 264, 18875, 337, 264, 819, 14840, 731, 309, 820, 312, 291, 1116, 519, 300, 309, 311, 445, 2584, 6243, 281, 31499, 264, 2020, 18875, 490, 264, 8062, 337, 264, 1349, 2010], "temperature": 0.0, "avg_logprob": -0.07103151204634686, "compression_ratio": 1.6962962962962962, "no_speech_prob": 9.807188325794414e-05}, {"id": 288, "seek": 441400, "start": 4414.0, "end": 4431.0, "text": " because normally if I say I've got two numbers the sum of them is 17 you just have no information as to what my two numbers are right you can't resolve it and even worse if I tell you I've got three numbers and they sum to 17", "tokens": [570, 5646, 498, 286, 584, 286, 600, 658, 732, 3547, 264, 2408, 295, 552, 307, 3282, 291, 445, 362, 572, 1589, 382, 281, 437, 452, 732, 3547, 366, 558, 291, 393, 380, 14151, 309, 293, 754, 5324, 498, 286, 980, 291, 286, 600, 658, 1045, 3547, 293, 436, 2408, 281, 3282], "temperature": 0.0, "avg_logprob": -0.07719677144830878, "compression_ratio": 1.5410958904109588, "no_speech_prob": 0.0004615257494151592}, {"id": 289, "seek": 443100, "start": 4431.0, "end": 4451.0, "text": " but it turns out that when we have these high dimensional vector spaces that things are so sparse in those high dimensional vector spaces that you can use ideas from sparse coding to actually separate out the different sensors providing their relatively common", "tokens": [457, 309, 4523, 484, 300, 562, 321, 362, 613, 1090, 18795, 8062, 7673, 300, 721, 366, 370, 637, 11668, 294, 729, 1090, 18795, 8062, 7673, 300, 291, 393, 764, 3487, 490, 637, 11668, 17720, 281, 767, 4994, 484, 264, 819, 14840, 6530, 641, 7226, 2689], "temperature": 0.0, "avg_logprob": -0.09340077030415438, "compression_ratio": 1.7687074829931972, "no_speech_prob": 5.292870628181845e-05}, {"id": 290, "seek": 445100, "start": 4451.0, "end": 4469.0, "text": " so they show in their paper that you can start with the vector of say pike and actually separate out components of that vector that correspond to different sensors of the word pike and so here's an example at the bottom of this slide which is for the word", "tokens": [370, 436, 855, 294, 641, 3035, 300, 291, 393, 722, 365, 264, 8062, 295, 584, 36242, 293, 767, 4994, 484, 6677, 295, 300, 8062, 300, 6805, 281, 819, 14840, 295, 264, 1349, 36242, 293, 370, 510, 311, 364, 1365, 412, 264, 2767, 295, 341, 4137, 597, 307, 337, 264, 1349], "temperature": 0.0, "avg_logprob": -0.08056730694240993, "compression_ratio": 1.6451612903225807, "no_speech_prob": 5.728305404772982e-05}, {"id": 291, "seek": 446900, "start": 4469.0, "end": 4490.0, "text": " separate out that vector into five different sensors and so there's one sense is close to the words trousers blouse waist coats and this is the sort of clothing sense of tie another sense is is close to wise cables wiring electrical so that's the sort of the tie sense of tie used in electrical staff", "tokens": [4994, 484, 300, 8062, 666, 1732, 819, 14840, 293, 370, 456, 311, 472, 2020, 307, 1998, 281, 264, 2283, 41463, 888, 1316, 15732, 30036, 293, 341, 307, 264, 1333, 295, 11502, 2020, 295, 7582, 1071, 2020, 307, 307, 1998, 281, 10829, 17555, 27520, 12147, 370, 300, 311, 264, 1333, 295, 264, 7582, 2020, 295, 7582, 1143, 294, 12147, 3525], "temperature": 0.0, "avg_logprob": -0.22650316026475695, "compression_ratio": 1.875, "no_speech_prob": 2.7921107175643556e-05}, {"id": 292, "seek": 449000, "start": 4490.0, "end": 4502.0, "text": " then we have sort of scoreline goal is equalizer the so this is the sporting game sense of tie this one also seems to in a different way evokes sporting game sense of tie", "tokens": [550, 321, 362, 1333, 295, 6175, 1889, 3387, 307, 2681, 6545, 264, 370, 341, 307, 264, 32366, 1216, 2020, 295, 7582, 341, 472, 611, 2544, 281, 294, 257, 819, 636, 1073, 8606, 32366, 1216, 2020, 295, 7582], "temperature": 0.0, "avg_logprob": -0.1996881671068145, "compression_ratio": 1.811881188118812, "no_speech_prob": 0.00010345261398470029}, {"id": 293, "seek": 450200, "start": 4502.0, "end": 4520.0, "text": " and then there's finally this one here maybe my music is just really bad maybe it's because you get ties and music when you tie notes together I guess so you get these different sensors out of it", "tokens": [50364, 293, 550, 456, 311, 2721, 341, 472, 510, 1310, 452, 1318, 307, 445, 534, 1578, 1310, 309, 311, 570, 291, 483, 14039, 293, 1318, 562, 291, 7582, 5570, 1214, 286, 2041, 370, 291, 483, 613, 819, 14840, 484, 295, 309, 51264], "temperature": 0.0, "avg_logprob": -0.10202098447223042, "compression_ratio": 1.5116279069767442, "no_speech_prob": 5.6960801884997636e-05}], "language": "en"}