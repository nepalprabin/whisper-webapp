{"text": " So I'm delighted to introduce our second invited speaker for 224N, Kelvin Gooh. So Kelvin is a senior research scientist at Google with interests in retrieval augmented language models and using knowledge in neural networks and perhaps best known for his work on the realm model, which is one of the things he'll doubtless talk that today. Yeah, I guess I know there are a few statistics students in the class, so maybe I'll also just mention that actually Kelvin's background is as a statistics PhD, but somewhere along the line he got corrupted away from math and statistics and ended up spending all of his time on natural language processing. I'm very good move. I'll recommend it to anybody. So anyway, I'm really happy to have Kelvin here today to tell us about his recent work. NLP. And as Chris alluded to, there'll be a focus on memory augmented models. So I'll try to kind of have a few spots for people to, for us to pause and ask questions, and otherwise I'll take the slides away. Great. So I want to start by just giving some motivation on some tasks that AI cannot solve today. It cannot diagnose a medical patient. It can't fix your car. It can't perform novel scientific research. It can't file corporate taxes. And it can't do many other things. Now I'm not saying that artificial intelligence is supposed to completely do these things, but at least it should be able to assist people who are doing those things. And what all of these tasks have in common is that certainly intelligence is required, but domain knowledge in these domains is just as important. So it's not intelligence alone that enables you to do these things. You have to have a long experience in various things such as what a car's components are, or in the case of this question here, if you were to ask a language model, the part of the intestine most commonly affected by Crohn's disease is my latest query to GPT-2 says the rectum, but actually it's the Ilium. So we can understand that if you're in the medical field and you're making this level of mistake, then you probably need to go back to training. So we're interested in getting language models and other language understanding systems to make these fine grain distinctions well, because we can't really unlock the next set of applications that NLP or AI could target. And of course, this is something that since the field began artificial intelligence researchers have been very interested in. If you look at some of the early applications of artificial intelligence in the 60s and the 80s, there were expert systems that did medical diagnosis, they would do computer chip design, and of course we know that we didn't get to completely fully solving those problems. And back then, the big obstacle was that you had to manually input all of the knowledge required for that domain. Some expert had to sit down and write all of the rules, and if any one of those rules contradicted another rule, the system was very brittle and unable to handle that complexity. But in 2022, what's very exciting is that we now have language models, as you've seen in previous lectures, that can automatically acquire knowledge from the web. And so that gives us an exciting opportunity to revisit this question of how to use knowledge in artificial intelligence and do more than, you know, classify with an image as a cab or a dog, but move on to much more complex tasks. So this talk will be in three parts. The first part is we're going to look at how language models currently represent knowledge, since they've obviously made huge gains, we need to understand what it is that's powering that success. And then we're going to step back and ask ourselves if that current way of representing knowledge is what we're happy with and what we'd actually like to see more of. And finally, kind of leading the discussion here, we're going to propose memory augmented models as a way of addressing some of those challenges. So certainly not the only way to address it, but one way that will spend a lot of this lecture looking at. Okay, so the first half of this talk is about how language models currently represent knowledge, maybe the first, third. And as I was actually looking at your curriculum, I realized there's another lecture on knowledge editing coming up. This will be sort of an introduction to that. I won't go into it as much detail as one of the later lectures, but you can think of this as an intro. So let's go back to this prompt that we were looking at earlier. And we know that we have a model that is close to getting a correct answer, but in some ways not that close. And so you'd like to ask yourself, this incorrect belief is clearly stored somewhere in the models parameters. But where exactly is it stored? We know that a GPT style model is a transformer, and a transformer has token embeddings, and a feed-forward network and an attention network. Where exactly is the knowledge? And how can we identify it and fix it? So to answer this question, we're going to look at some recent research on knowledge editing. Knowledge editing is the following task. So let's say the language model has some original belief. Like if you give it this fill in the blank question, Eiffel Tower is located in the city of blank. You expect it to predict Paris. And the knowledge editing task says, we'd like to actually change the model's belief about this. So let's say instead that we want the model to believe that the Eiffel Tower is located in Rome instead. And we don't want it to just memorize this exact statement, but rather really change its knowledge about the Eiffel Tower. So if I ask other questions about the Eiffel Tower, the answer should change there too. Here's like a particularly tricky one. If I say the tallest structure in Rome is, the new answer should actually be Eiffel Tower. And we're going to look at this paper, which incidentally, confusingly is also called Rome, by Meng et al, very recent research, which illustrates an approach for doing this. So you can see here on the top, they've made this particular edit that I was talking about. And when you generate from the language model, if you prompt it about places to eat, those places are all in Rome. And if you prompt it about how to get there from Berlin, the directions are from Berlin to Rome. So that's really quite remarkable. And the premise for this is that if we have an approach that can actually make these sorts of edits, it might constitute a little more of a proof that we understand something about the internal structure of the knowledge inside the model. So now let's get into how this approach works, and on the way learn about how language models might represent knowledge. We're going to start actually with an earlier paper called, I think, a paraphrase that's height a little bit, but transformer feed-forward layers are key value memories. And what I mean by that is I'm referring to the standard feed-forward layer inside the transformer, which I think you guys have seen in an earlier lecture. It's essentially taking the input from an earlier layer in the network, and then passing it through a matrix multiplication, a non-linearity, and then another matrix multiplication. Raps that with a bit of layer norm and additional bias terms and residual connections, and that's basically a feed-forward network, as represented symbolically here. So right now I'm just giving this simplified form of the feed-forward network, because this simplified form is enough for us to understand the basic intuition of how this thing might store memory. And by key value memory, I really just mean it in the typical Python dictionary sense. So in this key value memory, the keys are name and food, and the values are Kelvin and pizza. Okay, so I'm going to go into a kind of operation by operation description of what's happening in the feed-forward network. Let's look at this first matrix multiplication first, and what we're going to do is we're going to break this first weight matrix into rows. So now I've got each of the row vectors shown here. And as you know from linear algebra class, a matrix vector multiplication is just the dot product of each row against the input vector. And so you can get a set of scores for each of those dot products, and you can think of those scores as basically the similarity between x and each of the rows. Okay, now that we've got that set of similarity scores, we then pass it through the non-linearity in the feed-forward network. And the non-linearity in transformers is oftentimes something like a value. So it's a function that takes the values, and if it's negative, sets it to zero, if it's positive, basically just keeps that value. So if you apply that transformation, you get another set of vectors, and you can see now that a bunch of the, or sorry, another set of values forming a vector, and you can see that a bunch of the entries are now zero. Okay, so I still haven't explained why this is a key value memory. Just bear with me a little bit longer. We're going to go on to the second matrix multiplication in the feed-forward layer, and this time we're going to break this matrix up into column vectors. And we'll use the other interpretation of matrix vector multiplication that you get from linear algebra class, which is that it can be interpreted as taking the columns and forming a weighted sum of those columns using the values in the original vector. So I've just taken these values down here and moved them over the column vectors so you can see what the weights are on each of the column vectors. And because a lot of those entries are zero, we can just drop them. You can see we've essentially selected certain columns in the second weight matrix, and then we add them up, and that's the output. Does anyone have any questions so far about what happened there? Okay, cool. So now I think after you've seen that process, we're ready to ascribe a key value memory interpretation to this. So let me just quickly show you the whole process again. First we multiply the first matrix and give the similarity scores between each of the row vectors, pass it through a non-linearity, multiply by the second matrix, which in turn select certain columns of the second matrix, add those columns together, and get the output. So when you look at this process, you can think of this second matrix as storing values that you are selecting. You can think of this matrix here that's colored as the selector that's deciding what memories are selected, and you can think of the first matrix as storing keys which represent the things that you want to select. So the reason we call this one on the right keys is because if you think about the input x, if input x is equal to one of the row vectors in w1, then it will have high dot product with that particular key, and the score here will be high for that entry and low for all the other entries. So essentially, each one of these keys selects a particular value. The first row vector selects the first column vector in the second matrix, and so on and so forth. So that's the key value interpretation of a feed forward layer. And just to kind of beat this example to death, so you can get an example of how expressive this model is, let's suppose that the keys are actually one hot vectors where each entry or each row vector just has a one in a different position. Basically what I'll argue is that I can select any combination of the memory columns in this second matrix over here by just setting different values on the input. So in this particular example, I've got a one here and a one here, and that in turn selects these two keys, all the other dot products will be zero, and the selector will be only on for those two entries, which will select these two values. And if I had flipped any of the other bits in this vector to one or zero, I could select any other combination of memories. So there's really quite a lot of bit of flexibility in this model, and it gives us a potential theoretical explanation for how you could store and select lots of different kinds of information in a feed forward layer. Okay, so that's all theoretical so far. And it's just what a feed forward layer could do. We actually want to know if feed forward layers do act this way in a real transformer model. And for that, we're going to return to the paper that I was mentioning earlier called Rome, and we're going to look at how a transformer actually behaves on this particular prompt. All right, so as you know from previous classes on transformers, basically it processes the text from left to right, at least in a standard decoder model, and it builds the attention in feed forward layers one at a time on top, going across like this. And on the next time step, which I'm not showing, it has to predict what goes there. And currently in the basic model, it predicts Paris. So we want to know of each of these boxes, which one is actually storing the knowledge about the Eiffel Tower. If that's a reasonable question to ask at all. And we'll look at an approach that's used in the Rome paper that I mentioned earlier called the causal probing. And the technique, the basic idea of causal probing, is first you take some random Gaussian noise and you add it to the word embeddings for Eiffel and Tower. Or if Eiffel Tower is broken up into more sub words, all of those sub word embeddings. So that essentially confuses the model into not quite being able to recognize what the entity is. And they add enough noise to the point where the model no longer predicts Paris. So once they've destroyed the model's prediction, they then go about trying to restore the original value at each of these boxes one at a time to try and recover the model's original behavior. So intuitively, you would think if one of these boxes doesn't matter, like the model's not paying attention to it, even if you restore back to the original value, the prediction is not going to go back. But if it is responsible when you restore the value, there's some hope that the model returns to its original prediction. And then they're just going to see which layers are best at restoring the original prediction. One thing I just wanted to say to clarify is, let's say if I restore this value here in this attention box, we have to recompute everything above it because all the things above it need to consume that new value. So that's the restoration procedure. And what they found in this paper is that feedforward layers above the last token of Eiffel Tower are actually the critical causal layer that if you restore it, you can get the prediction to go back to its original value. And that's quite interesting. They tried restoring later layers, doesn't restore the prediction. They tried restoring earlier layers, also doesn't restore the prediction. So there's this very clear time band upon which the causal effect is. Let me show you guys a quick plot. So this is just a plot from the original paper. Take a moment to interpret this. So on the y-axis here, we have the different time positions as the model is processing different tokens. So you can see it's the big bang theory premieres on, and then there's a date. And the big bang theory has stars on it because that's the entity where the noise is being added. And then on the x-axis, you're seeing different layers of the transformer as it's processing it. And the colored intensity of the plot is the causal effect of restoring. So what's very exciting is that it all lands on theory, not any tokens before that. So this is where the model apparently is accessing the knowledge. And it's sort of surprising because you might imagine that the knowledge is kind of distributed everywhere, maybe a little bit of every time step contributes. And that would be unfortunate because it'd be harder to modify the model's behavior if that were the case. But in fact, it actually concentrates. And they did this over a bunch of different prompts and looked at basically where they got the most impact, measuring the first entity token, middle, last entity token, later words. And it all concentrates very well. So that's an interesting observation. I don't know what to do with it yet, research wise, but I thought you guys should know about it. Okay. So we're going to zoom in on this feed forward layer that they identified with this high causal effect. So they said it was in this time step and they found that effect to exist across many of the layers. And let's just zoom in on one of them to get a better understanding of what's going on here. So as you guys already know from the earlier slide, we can identify this selection vector that comes out of the nonlinearity, which says which memories got selected in the second weight matrix. And furthermore, we know that this output from this weight matrix is responsible for predicting Paris as we saw from the previous plot. So intuitively, that gives you this idea that somehow we should be messing with the weight matrix W2 to change its behavior. And a naive idea for how to change its behavior is well, drawing on our intuitions from word vectors, maybe we just pick one column from W2, the one that the selector selects, and just subtract the word vector for Paris and add the word vector for Rome. And it turns out that there is in fact a paper, which I've linked to here, that does that, and it works to some extent. So they showed positive results with this approach. And that's really quite surprising in on its own. The particular paper that we've been following, the Rome paper, does something slightly different, but similar in spirit. So they apply a rank one update just to the weight matrix W2. They don't touch W1 at all, kind of consistent with our interpretation that W2 contains the values of the memory, and W1 is just the keys. So what I mean by a rank one update is W2 is a matrix, and I add to it another matrix formed from outer producting two vectors, U and V transpose. And these two vectors are parameters that are optimized to maximize the probability that the model outputs Rome, while also minimizing the change in behavior over all the other inputs. It's out of the scope of this class to exactly describe what U and V is, or at least not in this lecture. So I'll maybe just punt this off to Eric's lecture when he gets there. But I just wanted to show you guys this to show the level of fine-grained control people are starting to look into in terms of editing knowledge in language models. And this wouldn't be complete if I didn't show you some more examples. So the successful example you guys already saw on an earlier slide. But to also show you some not quite successful examples, just to show where the field is right now, they also gave an example of trying to convince the model that the game Sonic Drift 2 was not made by Sega, but instead by Microsoft. And here you can see what the model does instead. It really struggles. It claims that the game is now made by a studio called Play Dead, and this studio was led by a former Microsoft employee. So it's really kind of fighting against what we're trying to make a change. And that's kind of where we are right now. So that was the first section on how language models currently represent knowledge. The main takeaway is that I'd like you guys to have is that the Transformer Feed Forward network can be viewed as a key value memory. And that's one of the reasons why when you see people scaling up Transformers to larger sizes, oftentimes they decide to put that scaling budget into making the Feed Forward layer wider, as opposed to making the attention layer wider, or adding more layers, because they're trying to increase that memorization capacity. And the second conclusion is that Transformers tend to look up information about the entity on the last token where it's mentioned. That's quite an interesting thing too. Prior to the Rome paper, there were other papers that tried to just fine-tune the entire network to get it to change its behavior. And when you fine-tune all of the parameters, indeed you can get it to change its behavior on Rome, but you also mess up a bunch of other facts too. So being able to make a small edit to one place actually does turn out to be helpful. And lastly, I just want to say this is a very new research area. Next year I could be saying something completely different. So just take that with a grain of salt. So we're going to go into the second half of the presentation. And we're going to talk actually, because we still got time, are there any questions about the previous slides? Yeah. Yeah, I'm actually curious, you mentioned about how to be surgical about knowledge alteration. Imperically, did the researchers discover, or with your own work that discovered that it could alter, you know, Iful-tower needing Rome? Does it cause other past dating effects of confusing Rome, like Paris, or something? That's work. Yeah, yeah, that's a great question. So on the eVal benchmarks that they have now for this task, they have, I'm not sure if the exact word is like neighbor prompt. So they have both prompts that are paraphrases of the original thing to test that the models were busted paraphrase. But they also have prompts where they ask, say, about Sears Tower or other towers. And make sure that those towers didn't move to Rome as well. And yeah, so that's very imperfect right now. There's this difficult balance between the two, and that could be a sign of many things. It could be a sign that the model doesn't have enough memorization capacity, so it's only way of representing the Iful-tower is to just think of it as like a tower, with maybe some nationality mixed in. And when you edit that tower, you just move all the other towers too. Great question, yeah. Yeah. So from one of the other things, on the next slide, you see different types of questions. So you asked about Michael Conn where it is, but what about like an article where he's done by other types of properties about the key and just the original, the key to the perspective of the map. Yeah, so I think if I understand your question, it's not just asking about that one fact, but other facts related to the Iful-tower. Yeah, so they also have this kind of freeform generation prompt, I think, where they just initialize it with Iful-tower, some short prompt, and then they measure the different kinds of text that come out. And I think if I remember correctly, they check like N-gram overlap with real text as well. I didn't. So they have a couple more analyses in the paper of how it behaves on other topics too. Yeah. One thing worth mentioning is, I guess, I worked with another student here on a paper on counterfactual updates. So we have this data set, which we should really get released out soon, that pairs one fact update with another implication of that fact update that is non-obvious. So for example, if Walt Disney had one of his Academy Awards script, then the next question would be how many Academy Awards did Walt Disney win. And that's like one of the areas that I think some researchers are thinking about for future work. Yeah. So the current method basically only allows control through the prompt. So whatever the prompt implies about the entity, you get to change what the answer to that prompt is. And it's basically at the moment up to the model to decide what implications are affected by changing that prompt. Which is another, you could say, weakness of the approach. It's not entirely interpretable. Everything is done through optimization. Yeah. Great question. Okay. We'll go on for now. And then, oh, it was a point. Oh, yeah. Okay. The question is about whether the low rank update has anything to do with adversarial machine learning. I'd say maybe there is a slight connection in that, if you look more deeply in the paper, what they're doing is they're optimizing the output of the weight matrix to change the label and that optimization procedure is very similar to what they do in adversarial machine learning. The low rankness of it is not necessarily connected to the adversarial learning. The low rank part is just to minimize the amount of change to the weight matrix. I can maybe just quickly do this on the board here. So if you have this weight matrix, W2 plus UV transpose, the reason this is considered a small update to the matrix is because if you think about anything multiplying W2 after it's received this update, let's say you're multiplying it by X, right? So this whole thing is multiplied by X. So let's just move this X into the expression. You get W2 times X and then UV transpose times X. And this part right here is just what W2 originally would have done. And this part right here, this vector V is being dot-producted with X. And in high-dimensional spaces, basically most things, dot-products are zero. And so this quantity here is likely to be zero unless X is very close to V. And then this whole update here basically disappears. So in that sense, you could think of a low rank update as a very small change to weight matrix. Okay, cool. We'll move on to the next section. All right. So now we've seen what language models currently do to represent knowledge or at least a theory about that. And we'd like to ask ourselves, well, is that really it? Do we just need to make feed-forward layers bigger and bigger to achieve the singularity or whatever it is that artificial intelligence researchers are focused on these days? Okay. So what is missing from transformers right now? We can automatically acquire knowledge from the web, but a lot of that information can be noisy or incorrect. So the web certainly has its share of misinformation or rumors and opinions. And when it absorbs that misinformation or other things, we can't trace the model's knowledge back to an attributable source. So we can trace it back to a particular layer in a feed-forward network, but that still doesn't tell us where in the training data it learned that. Now that would all be okay if we could then surgically edit the model to fix up all of those errors. But as you've seen, it doesn't work very reliably yet. And another fact that I didn't mention is if you apply a bunch of these edits in sequence to the model, eventually the parameters get kind of so damaged from the edits that it doesn't maintain its original performance anymore. And we can continue to try storing knowledge inside feed-forward layers, but the current memorization capacity is still too small, even though we're building these very large and expensive models. So we can rephrase some of these issues as a wish list for what we would want in a knowledge-based language model. We'd want fast and modular knowledge editing, so be able to robustly edit the model multiple times without breaking it. We'd like attribution and interpretability, so tracing a model's knowledge back to something in its training set, and we'd like efficient scaling. So we'd like to be able to increase the model's memory size by 10x without paying 10x more compute. And to give a motivating example, let's just say you wanted to use something like GPT-3 to do question answering over your company or school wiki. At the moment, as we know, a single training run, at least when it was originally done, cost over $12 million, and we just can't afford to do that for every organization that wants to train a system off of their data. And furthermore, information is constantly being updated over time, for example, like the COVID requirements for being on campus. So all of this sort of motivates this wish list that I'm giving here. So with that, we'll turn to the next main half of the lecture, which is on memory-augmented models. So let me first just give a basic overview of what a memory-augmented model is. To start with, let's just consider a standard neural network model, which takes some sort of input like this one here, passes it through some dense computation, and then produces some output. And the difference that we're going to make to this model is we're going to attach a memory retriever to it. So the input is going to be fed into this memory retriever, which then accesses some external knowledge source that can be easily scaled, easily edited, easily understood by humans, like Wikipedia. And from that, we'll try to identify some piece of information that is relevant for the task at hand, and feed it back into the neural network, and then produce the prediction. So that's the basic approach that we're thinking about. And the memory that the memory retriever selects can be any number of things. It could be a document on the web. It could be a record in a database. It could be a training example, an entity embedding. I'm just going to focus on text for now, but most of what we'll talk about in this lecture can apply to other kinds of objects, and you should just keep that in mind that it's really not just about text. So this potentially meets our wish list of things we'd like to do. You can easily edit the knowledge in something like Wikipedia. You can easily attribute back to it. There's a source, there's an author for everything that's put in there, and there's efficient scaling, because I can always add more articles to Wikipedia, and I don't have to change the size of a neural network that's accessing it. And some motivating applications for why you would care about this. If you're building an open domain dialogue or question answering system, you want to have robust access to knowledge by retrieving documents on the web. If you're generating code, I'm pretty sure all of us are guilty of at least some point going on Stack Overflow and copying a snippet. So even we do retrieval, we are in a form of memory augmented vowel. If you're doing image generation, if somebody tells you, I want a picture of the Eiffel Tower on the White House lawn, you might consult some reference pictures of those objects. And if you're doing fact checking, you might want to retrieve documents that support or refute a claim. All of these things are very knowledge intensive tasks, and could benefit from an approach like this. Yeah, question. Yeah. Yeah. Yeah. That's a good question. So the question is whether you have to retrieve one memory, and that's not the case. I'm going to use that as the simplified example that we'll work with, but you could retrieve multiple memories. The complexity of retrieving multiple memories increases, and we may be good to that in a little bit. Good question. Yeah. Okay. All right. So the rest of this talk is going to be structured around the main design questions around how to design a memory augmented model. So first you have to choose what your memories are. I'm actually not going to focus on that much because based on your application, you can usually guess what you would want your memories to be. And then we're going to think very hard about how to retrieve the memories. That's essentially the heart, maybe the hardest part of the problem. Approaches we'll look at are you could use an off the shelf search engine like Google or Stack Overflow, or you could train your own memory retriever, which will spend some time on. And lastly, we'll end by looking at how to use retrieved memories. So we'll cover a few different approaches such as text fusion, label smearing. And perhaps most interestingly, I'll talk about a few common failure modes for when models try to use memory. One of them is something that we'll call underutilization, where the model actually ignores the retrieved memories. And another one is over reliance, where the model somehow kind of becomes too dependent on memory. And I'll talk about that when we get there. Okay. So let's go into the section on retrieving memories. So I'll kind of organize them into two broad groups. So one is the set of approaches that use an external tool. And the other is the set where you train your own. And we'll start with an approach that uses an external tool, just because I think some of those approaches have been really popping up this year and are quite exciting. The first approach we'll look at is from this paper called Lambda, which stands for Language Models for Dialog Applications. So on the right we've got a dialogue between a human user and the model. This is a, I think, a real dialogue from their paper. And the user is just asking about a particular artist and the model is giving a very spirited reply, complete with personal opinions and even follow up information. So Lambda is an open domain dialog chatbot that's designed to cover a very large range of topics. So you need some kind of memory component that's able to handle anything the user might throw at the model. And the basic version of the model is just a transformer decoder. So it's the same kind of transformer that we were studying in the previous slides. The input to that transformer is essentially the previous turns of the conversation, represented as text. And the output is just a new utterance that it needs to generate, also as text. So it's just a text to text kind of approach. What's new though? Oh, okay, not quite there, yeah. So one last thing about this model though is as they were developing it, they noticed that it often generated factually incorrect claims. So just to highlight that, this last claim that the model makes is factually incorrect. In this case, this particular artist that was supposedly inspired by the earlier artists stopped working before the first one began working. So this just can't be true. And their approach to solving this problem will be to teach their base model to learn to use a search engine to validate or fix claims that it's made. And I'll show you how that approach works on the next slide. The basic idea is you've got a user interacting with Lambda, which is this big box here. And what they've decided to do is have multiple agents inside the big box that can kind of interact with each other and kind of work things out before they give a reply back to the user. So here's how it might go. The user says to the base model, one was the Eiffel Tower built. And the base model replies it was constructed in 1887. But unlike the basic approach, it doesn't send that response immediately back to the user. It actually sends it to this agent called Research. And then research then takes that information and decides, okay, you know, this claim looks a little bit sketchy. I'm going to send a query to the search engine. I'm answer-promorphizing a bunch here, but it just helps with the explanation. And then the search engine then replies back with the web search results for that query. And in it, we have actually the correct answer, which is 1889. So research then takes that information and produces a new response that has the correct information and sends that back to the user. So that's the overall flow of the approach. And you can see that the search engine is able to intervene to fix a model's responses. Any questions about that? Okay, yeah, the question was whether this particular flow happens for all questions to the model. And actually, as we'll see in a later slide, the model gets to decide who it talks to next. So base has to decide to talk to research as opposed to talking to the user. So there's a learning process where each agent gets to decide whether to go right back to the user or to talk to another one of the other agents. Go ahead. Okay, yeah, the question is how to limit the amount of information coming back from the search engine. I think in this particular approach, the search engine returns the first snippet. And then if research is still not happy, it asks the same question again. And then they've designed it so the search engine returns the next snippet. So it just sort of yields control back to the first. Yeah, yeah. Yeah, okay, so the question is whether research sends any feedback back to the base model that it's made of a stake. And that's a great idea. I don't think they do that in the paper. They just, I mean, research overrides base so the user gets what research said. But base never learns, I think, from what research says. So that's a really, yeah, that's a really great point. Okay, so the question is, why do we need the base model? Yeah, that's a great point too. So Lambda is kind of the researchers who developed it cared not just about answering factual questions but making it interesting and fun and engaging. So the base model has a lot of that engaging behavior. And they wanted to preserve that while still preserving factuality. So the other two agents are kind of there to police the base model. That's maybe one explanation. Oh, right. Great, great questions. So now that we've seen the overall control flow of the model, we can look at how the model is trained. And it's actually quite simple. So their modeling approach is to just treat everything as dialogue. So let's look at a particular turn of the model's operation. So there've been a couple turns of conversation so far, which I've listed here. And you can see it's just saying who talked and who they talked to. And the output at this particular time step is just another person to talk and who it should talk to. So all of this is just text. It's text going in, it's text going out. So you guys have seen transformer models and basically this fits right into the contract of a standard transformer model. The only kind of special detail is that when you generate the text, you have to start your sentence with who you're addressing. And that provides the control of which agent responds next. I already mentioned this. And the perhaps the most important question is, okay, we've got this text to text data. How do we train this model? And the approach in Lambda is actually quite simple, too. They basically just get human demonstrations. So human crowd workers play the role of user and research in this dialogue. There are people who are looking at the base model's utterances and saying, oh, I don't like that. I think a search query should be sent here. And when the search results come back, they're reading the results and then deciding how Lambda should respond instead. So it's a really elegant and simple approach, but it does require you to have trained crowd workers and put in a good amount of budget to get the behavior you want. But still quite impressive that they're able to do this. This is a real example, I think, from the paper. Cool, any questions there? All right. So although the approach is simple, it actually achieves quite a bit. So the model learns to reformulate the previous terms of the conversation as a query that can go into Google search. So it's kind of shoe-horning the problem into something that Google search or some kind of web search can understand. And then it's learning also from human demonstrations how to incorporate the knowledge from the search results back into the utterance that it's putting out. And just because this work also came out around the same time and also very exciting, I also point you to WebGPT, which is another model that learns to use web search. In their case, they provide human demonstrators with an actual UI and have human demonstrators use that. But they ultimately, I think, convert the history of actions that the user takes again into a piece of text that the model then simply consumes and uses to predict the next action. The additional thing here is that they use reinforcement learning to then fine tune their system further on top of what they learned from human demonstrations. And that's something worth checking out as well. So the main takeaway for this little section here is that many external retrieval tools accept text as input and return text as output. So if you want to have an external memory and interface with one of these things, all the task really boils down to is learning to generate text queries to that external tool and then learning to understand the text output of the tool. And both of these tasks can be handled by standard off-the-shelf tools that all of you are already familiar with from previous lectures. As long as you have demonstrations for how to do that. Or if you're able to do RL training, which we won't cover here. So that's the overview of how to use external search tools. You can imagine that if you had a database instead of a web search, you could provide demonstrations of how to write SQL queries to that database or any other sort of tool that you could imagine. All right. So at this point, you might say, all right, we can query web search and web search is very powerful. So why would we use anything else? And to that, I have a couple responses. So first of all, web search is just far from perfect. And the reason it says good as it is today is because of research. And we're here to do research. So if you're just going to rely on web search being good, that sort of defeats the point. If you don't believe me, try some of these queries. So if you search for a famous lawyer who got into car accident, you will find that all the results are about lawyers you can call if you get into a car accident. If you search for a use NLP to parse research papers, you will find a bunch of research papers on parsing. And after doing a few of these, the illusion of web search working really well kind of fades away a little bit. And also, if you speak a language other than English, you might find that search performance in different languages is really not quite the same. So there's still a lot to do to improve retrieval in web search. And I sort of consider web search to be just sort of a component inside the larger, bigger set of things that memory augmented models could potentially do. And second of all, just the plain API of web search isn't designed to handle everything you might want to do. So you could imagine a doctor given a medical image might want to retrieve similar images from a medical textbook. That's not quite something that web search is cut out to do right now. Or if you're a programmer who's been given a programming challenge, you might want to retrieve relevant algorithms. Also something web search doesn't do. If you're in fashion, if you're given three pieces of clothing, can you retrieve another piece of clothing that completes your outfit? Or if you're a novelist and you're given a story, retrieve other stories that have the same plot. Or if you're a journalist, if you're given a claim, retrieve and use articles that refute or contradicted. These are all retrieval tasks that would be quite useful, but existing search tools just don't handle. And so these are all reasons why I think the retrieval problem is still interesting to look at. And a third and final point is that web search only accesses public data. So if you have any task that doesn't condition on public data, you're still going to need a retriever of your own. Cool. So with that, we'll turn to the next part of the talk, which is how to train your own neural retriever, which is something that I find very interesting. So we'll start by giving an anatomy of a neural retriever kind of similar to what we showed for feedforward networks and transformers. We're going to go with this key value type of interpretation. So you have a set of keys paired with a set of values. And given some input, you're going to compute some sort of similarity score between the input and each of the keys. And once you've computed that score, you basically want to return the value associated with the highest scoring key. Or you could return the values for the top K high scoring keys or any other metric. So to just kind of ground this example, the input could be something like Iple Tower Location. The keys could be titles of documents and the values could be the corresponding text associated with the document. And the basic takeaway here is just that a retriever is really just a function that takes some input and a key and produces a score. Once you have that, you basically have a retriever. You score all the memories and then you take the ones that have the highest score. For the remaining slides, I'll actually go with a slightly simplified set of. Because in many tasks, there's really no distinction between the keys and the value. Sometimes they're just the same thing. So for example, with Wikipedia documents, you just take the whole text as the key and the value. And so I'll go with this simplified schematic where you're just computing scores against what I'll call memories. And the highest scoring memory is what's returned from the memory retriever. And we'll go with this formulation that the retriever is just a function that takes the input and the memory and produces a score. Okay, so I've said it's just a function, but what sort of functions do people actually use in practice to compute this score? And the answer here is not too surprising. You guys have seen Bert and other sorts of transformers in previous classes. It's a very flexible model class. I'm trying not to introduce kind of unnecessary complexity. So we'll just go with a Bert model that takes the input and the memory. And you put a some sort of regression layer on top of the output layer of Bert. So maybe the CLS token embedding of Bert, you put a regression layer on top and it produces some float valued score. And this whole thing is differentiable. So the regression layer is differentiable. Bert is differentiable. This gives you basically a neural network that produces a score. And the advantages are you get this very powerful model that's comparing the input against the memory and it's differentiable. So all of that is good. The disadvantage of this approach is that if you have millions of memories, then every time a new input comes in, in order to retrieve a memory, you have to run this computation against all one million of the memories. So that's just way too expensive to do if you're thinking about something like all of Wikipedia or all of the web. So next we'll turn to a different architecture that is more commonly used for retrieval on the next slide. So it's very similar. The Bert picture comes up again. This time what we're doing is we're taking the input and feeding only the input into the transformer to produce a single vector that we'll call the input vector. And then we'll have a separate transformer encode each memory separately to produce a memory vector for each memory and then the relevant score between the input and the memory is just the dot product of these two vectors. It could be the dot product, it could be co-science similarity, just any function that you can efficiently compute between two vectors. So why are we proposing this instead? This has a couple advantages over the previous architecture. The first is that you can run this side of the model, the right side, on all of the memories in advance. So before any inputs even come in, you can just pre-compute the memory vector for each thing. So if it's Wikipedia, you can produce a vector for every document in Wikipedia. And when a new memory comes in, you don't have to redo that work. So that saves a lot of compute. The only thing that you need to do when a new input comes in is you need to compute this input vector and then do dot products against all of the memories. So dot products are cheap and can happen much more quickly than running an entire BERT model over again. And that's the fundamental savings that you get from using a model like this. Something that we won't cover in as much detail here is that for the dot product, there are also fast nearest neighbors algorithms that let you efficiently find the memory vectors that have the highest dot product with the input vector without actually computing over all of the memory vectors. So it's a sublinear search algorithm that allows you to find it. And the basic intuition there, at least there are a couple of them, is that you can take your set of memory vectors and build some sort of tree structure over them, kind of organizing them spatially. And once you've built that tree structure when the new input vector comes in, you can essentially kind of traverse down that tree to find the things that are most similar without computing dot products with everything else. There are other algorithms too that use hashing and other techniques, but they'll be out of the scope for today's class. And the other good property here is that all of this is still differentiable, so you can still train this thing with gradient descent like anything else. The main disadvantage of this approach is also kind of due to its advantage, which is that all of the expressiveness of this model has to go through that one dot product. So anything you want to remember about the input or anything you want to remember about the memory, all has to get squeezed into that one memory vector and that one input vector. And that's a bottleneck that kind of researchers have been dealing with in recent research. What you'll find is that there are a lot of approaches that try to strike some kind of balance between this approach and the approach on the previous slide. So a common thing to do is to use this approach to retrieve a top set of candidates and then run a more complex model like the one on the previous slide to rescore and re-rank the candidates proposed by the first model. You'll also find techniques that try to take the memory and produce five vectors and then use all five of those to somehow compute a score. There are many variations which we won't go into detail here. Any questions? Okay, right there. Okay, there's a question about whether you can kind of augment the search data structure that helps you do the fast search. I think there is some research in the area where the vectors that you produce to index the tree is perhaps not the same as the set that you ultimately return. They can be optimized for different things. So oftentimes these kind of tree-based approaches require your vectors to be spread out in some non-pathological way. And I think that's a very interesting area for research. So producing vectors that are easily indexable, kind of taking into account the indexing process as a way to improve overall performance is quite important too. Because oftentimes when you use these fast similarity methods, they make some sort of approximation to the real top case search and those approximations can often hurt you pretty bad. Yeah, great question. Okay, cool. Great, so now we've looked at a few different architectures for actually performing retrieval. Now let's look at how you would actually train one of these retrievers. So fundamentally all you need to train a retriever is you need an example of an input. You need a positive example of what you would like to retrieve and then you need some negative examples of what you would not like to retrieve. So for example, where the super-ball is this year, Sears Tower location, etc. And the training objective for this is quite straightforward. So I'm going to divine a few variables here. S star will be the score that the retriever assigns to the correct input and S sub i is going to be the score that the retriever assigns to each of the negative inputs. And then we're going to apply the well-known softmax function over all of these scores. So what we're doing here is we're taking each of these scores, exponentiating the score so that there's some positive value. And then dividing each of those exponentiated scores by the sum of all of those scores, so that the whole thing sums up to one. And we're going to call that the probability of retrieving the positive document. So intuitively if the positive document has a high score, then after exponentiation it will be even bigger. The other scores will be smaller and most of the mass in this probability distribution will be on the positive document. If it's not, then this probability will be small. And what we will do as a standard in machine learning is we're going to maximize the probability of that quantity, in particular the log probability. And this is all doable because P of positive depends on the softmax expression here, which is differentiable. And each of the scores inside the softmax depends on the retriever, which I just told you on the previous slide is also differentiable. So the whole thing is differentiable and you're just basically trying to push the positive score essentially above all the negative scores. Okay, so it's a very simple recipe. And we'll look at a concrete example of that based on this paper called Dense Passage Retrieval, DPR, one of the early papers to explore the sort of supervised retrieval approach. So the task they're looking at is basically given a question, like the one here, retrieve a passage from Wikipedia containing the answer. And once you've retrieved the passage, they then have a reader module that reads the passage and produces an answer. And the training data for the retriever is going to fit into the format that I just described. So they work with this dataset called natural questions, which comes with human annotated queries, answers to the queries, and also a passage that contains the answer. So here we go, the input to our memory is the query. The positive memory that we'll want to push up is the passage that the human provided. And the negative memories are actually something kind of interesting in this paper. So it's going to be one, it's going to be the positive passages for other queries. So as long as all your queries aren't asking the same question, the positive passage for another query is going to be negative for the current query that you're looking at. And this next bullet is also interesting. They take a passage that's retrieved by an off-the-shelf tool for search. This is called BM25. It's a classic information retrieval approach that uses token-based overlap to retrieve things. It doesn't have any deep learning or anything in it, but it's quite effective. So they retrieve a passage and they retrieve one that does not contain the answer in it. So the assumption here is that you've got a passage that looks very promising, but in fact, doesn't contain the answer. And you can think of that as this is what we call a hard negative. Great. So we've got all the components for training retriever. They go ahead and do that. Let's look at how well it actually works. So to understand how well it works, we're going to compare it against another approach. So we're going to look at, this is from, I should have had a citation for this too, paper by Robert Settel on close book question answering. They basically take a sequence to sequence neural network model called T5 and just feed in the question and ask it to retrieve the answer. So this model does not have access to passages. In effect, it doesn't have any external memory. And you can see that as they scaled up the size of the model, they were quite nicely getting better and better performance on the task. And the question we want to ask is with DPR, which has this external memory, this access to Wikipedia, can that do better than an approach that doesn't have external memory? And the answer, of course, in this class is yes. So it does indeed improve quite significantly, and it's not a surprise because we have access to this additional information, which is Wikipedia. So you might look at that previous chart and say, well, maybe we just need to make T5 bigger because after all, the scaling was looking quite good, right? So I took that plot and re-plotted it with the parameter scale on the x-axis and the performance on the y-axis. And we know from recent research that these scaling laws tend to be logarithmic. So as you increase your model size, the improvement is a logarithmic function. And I just plotted that curve out for you to see where it's headed. And if you plot DPR on this curve, it's just kind of sitting way above this scaling plot for a much smaller model size. It's doing much better. And I also re-plotted this out further to see if this line eventually caught up with the 44 number up there. And it does add around 8 trillion parameters. So that's about like a thousand times bigger than where we are now. So all this is to say that scaling does help, but there might be easier and cheaper ways to get there. So one criticism you could make of the previous approach was that DPR actually had access to something that T5 didn't have, which is it had human annotated gold passages saying what you needed to retrieve to answer the question. And that data is actually hard to collect. So we're going to ask the question, what if the examples that you had access to were just query answer pairs? And you still train a good retriever without gold passages. And this sort of task arises in many other tasks as well. You could imagine if you were going from natural language to code, you might encounter cases where nobody has provided you annotations of what code snippets to retrieve, medical diagnosis, similar thing. So we're going to go now to end-to-end learning of a retriever. And let me get into some detail on what that is. So we're coming back to this diagram of a memory retriever. And in a memory augmented model, once the memory is retrieved, it then goes into a reader component, which takes the original input in the memory and produces an answer. So if you have no supervision for the memory, you might have this intuition instead, which is that if you did retrieve a good memory, that should result in a good answer from the reader. On the other hand, if you retrieved a bad memory, that will probably cause the reader to get confused and produce a bad result. So you might be able to use that observation as a training signal to train your retriever. Let me just give a concrete example with this, who is the bad guy in Lord of the Rings? If the memory retrieves something like the main antagonist is Soran, then you'll produce Soran likely, and that's great. On the other hand, if the retriever got this other passage saying Lord of the Rings received a bad review from IMDB, then your reader might be more inclined to produce IMDB, which would not match the gold answer in your training data set. So this gives you some knowledge that the second memory is bad and the first one is good. And so what I'm going to propose here is this idea of trial and error. In the first stage, you perform exploration where you let your imperfect retriever select some memory, and you try feeding that memory to the reader, and then you learn from success or failure. So if the memory helps the reader generate the right answer, you want to increase the score of that memory. And if the memory does not help the retriever, you want to decrease the score of that memory. And over time, this process would help the helpful memories get higher scores than the less helpful ones. So the formal approach for this is going to be taken from a paper by one of my colleagues called OpenRetrieval QA, ORCA. And the exploration component we're going to formalize as follows. So as I mentioned earlier, a retriever is just scoring function between an input and a memory. And if you take a softmax over all of the scores for all of the memories, then you get this distribution over memories given the input. So again, I've just raised all of the scores to e to the power of the scores and then normalize. And once we have this distribution, we'll randomly sample memory from that distribution. So as you can imagine, if the scores are meaningful, then we're more likely to sample a memory that's good and less likely to sample a memory that's bad. But because it's random, we kind of eventually will sample everything, unless there are things with zero probability. And then the learning from success and failure part. So once we pick a memory, we need to see if it actually helps. And we're going to measure that by looking at the reader's probability of generating the right answer given that particular memory. So that's this big quantity right here. The reader looks at the input and the memory, and we want to see its probability of generating the gold answer. And if this value is high, then we want to increase the score of that memory. And if it's low, we want to probably decrease the score of the memory. So I've shown you a couple expressions now, and we want to put those expressions together into a training objective that we can actually optimize. So we'll start with this question. If we randomly sample a memory from the retriever, and then we generate an answer, what is the overall probability that we get the answer right? So first, let's look at this expression right here. This is a summation over all possible memories that the retriever could retrieve, and the sum is over the probability of retrieving it. So right now, this is, this just equals one, because it's a distribution, and we're summing over all of its values. But then we'll add one more term to this, which is the probability that the reader gets the answer right given the memory in that term in the summation. So this first part is the retriever, and it's proposing different memories. And this second part is the reader, and it's succeeding or failing based on the memories. So you can think of each term in this summation as a trial of a different memory. And you can think of that second term kind of like a reward, if it's high, it's good, and if it's low, it's bad. So what they propose in the Orca paper is to perform gradient descent on this entire expression right here. They basically want to push the value of this entire expression up, and they're optimizing both the retriever and the reader. So let's look at the retriever first. So the retriever has a fixed budget that has to sum up to one over all of the memories. If it wants this value to be high, it doesn't have any incentive to put probability on bad memories, because those bad memories are just going to produce a low score on this term right here. So as you optimize this function, the retriever will basically try to put all of its mass on the good memories. And meanwhile, if you're optimizing the reader with respect to this function, it's trying its best to produce the gold answer given whatever memory it has. So it's also incentivized to try its best to extract the answer out of whatever it's given. And when you kind of jointly optimize both, over time you get something that puts its mass on good memories. So that kind of corresponds with the intuition that I was giving you earlier that you can kind of perform end-to-end learning to get a retriever. All right. So that's the high-level approach to Orca. What I didn't explain is that usually if your memories are like all of Wikipedia, this summation is very large, and if you're going to do gradient descent on this summation, it's going to take a very long time. So in practice, they approximate this summation with the highest probability memories, maybe the top 100 or the top 10. And I won't go into details in this class about exactly how that works. But I'll stop there. Because we're kind of approaching the end, I'm going to take questions just a little bit later. Sorry about that. So let's see how well Orca works. Just come out and put that number there. So a bit of context around this. It's not as good as DPR because it has less supervision than DPR. There's no human annotation of what passage to retrieve. But what's worth noticing is that at least compared to T5 at the same size, so you can compare 0.66 billion parameters against 0.77, it's actually already better than T5. And compared to a T5 that's about 15 times larger, it's almost at the same performance too. And it's a pretty decent result for an approach that has no access to retrieval supervision. So one thing you might note though is that the better result requires gold passages. And Orca and T5, these approaches don't require gold passages. They only need query answer pairs. And one advantage of that is that query answer pair data is actually pretty easy to get. So we could potentially get a lot more of it than if we were asking for passage passages as well. And the final part of this retrieval section is about a way to get basically an arbitrary number of query answer pairs to kind of improve these weekly supervised approaches that don't have passages. So it comes from a very simple observation, which is let's take your typical query answer pair. It looks like this, right? So you've got your query on the left and answer on the right. You can easily reformulate that as a fill in the blank question like this. And this fill in the blank question forces the model to think just as hard as the original question is just in a different format. But what's nice about this fill in the blank question is that it's very easy to create a bunch of them for free. Basically you can just take any sentence on the web. And as long as it's mentioning something factual or semantically meaningful, you can just blank out one of the entities. And in fact, that is exactly what you've probably seen in previous lectures, pre-trained language models like Bert do. And Bert uses that training objective to learn a very great deal. And that can be used in this setting for retrieval as well. So the basic idea for Realm, which is something that I worked on with collaborators, is to apply the same end-to-end training as Orca. But pre-trained the model on a bunch of these fill in the blank questions that we just automatically generated, just in extremely large quantities. And then we fine-tune that on the real questions query answer pairs that we already have. So if you do this approach and you plot it against all the others, what's quite nice is that it basically almost closes the gap completely with an approach that uses supervised data. Just by pre-training on fill in the blank questions. And the nice thing is it doesn't need access to gold passages. So it's on the same footing as things like T5 now. And at the same footing, it outperforms T5, despite being much smaller than even the largest model. So that gives us this interesting promise of using kind of language model fill in the blank techniques to build good memory retrievers. And the nice thing is that this fill in the blank approach can be used to tackle many sorts of tasks. You could blank out a patch in an image and train a retriever to find other images that might help fill it in. You could blank out a segment of code and train a retriever to find other pieces of code that might help fill that in. Or chapter in a textbook. The kind of list of things you can do with fill in the blank actually goes on and on. And each task that you define in this way produces a specialized memory retriever for whatever it is that you're filling the blank in for. And there's no need to collect training data. So this sort of scales to any set of tasks that may not be important enough for central enough to warn a big data collection budget. All right, so the main takeaways for this section are that a retriever is just a function that takes an input and a memory and produces a score. If you have supervised data for your retriever, that's great. Provide positive and negative memories for each input. And just train the retriever to score the positive ones higher. If you don't have supervision, you can use end-to-end learning, which employs a trial and error approach. If a memory helps the model score the memory higher, otherwise score it lower. And with end-to-end learning, you often get this special benefit that you can easily create tons of data to pre-train your retriever. All right, we're now into the very, very final part of the talk, which is how to actually use the memories after you get them. Notice I have 15 minutes, right? Yes, hold in. Okay, all right, then we should have plenty of time for questions, actually. So all right, here we go. We're going to come back to this diagram of a memory-augmented model. And now we're going to focus on this reader component, which I didn't say much about before. I said that the reader takes the memory and the input and then produces the answer. So what does that reader component actually look like? A very common architecture is just the sequence encoder to decoder model. In practical terms, you take the original question and you just concatenate it to the memory that you retrieved. And then you feed that into your encoder and you train it using standard sequence to sequence learning to produce the output. So all we're doing is just concatenating the memory with the input. And we can refer to that as text fusion. Anytime you have a memory that is text or can be converted into text in some form, just concatenated. That's all you need. At least that's what state of the art techniques are doing right now. Okay, and just to give some variety, here's another way to incorporate memories. Let's consider a slightly different memory-augmented model where instead of just retrieving a document, the memory is actually key value pairs where the key is the question, like a question that's been seen before and the value is the answer to that previously seen question. So in this case, you can do something even simpler than what was on the previous slide. You can take your input and compare it to the keys and find the key that most resembles the input. So in this case, we found a paraphrase of the original question. And if you have this, then all you really need to do is just copy the answer from the value out as your label. And that's what people refer to as label smearing or otherwise nearest neighbor's methods. We call it smearing because you're essentially smearing the label from this example that you retrieved onto the new example. And it's a very simple technique, which one you use depends on your application. So the techniques we're using memories are quite simple, but the problems that arise when you use them are actually quite interesting. The first one I want to talk about, I kind of mentioned this preview this earlier, are these two problems of underutilization and overreliance. So let's get into the underutilization issue. This is from a very recent paper by Longprey et al. So let me dive into it. So, okay, I switched the example here because I just got tired of the Lord of the Rings One. The question is, who do you meet at the gates of heaven? And the retrieved memory is on point, it says you see Saint Peter at the gates of heaven, the reader reads it, produces Saint Peter, everything is great. So what Longprey et al observed is, okay, if the reader is really doing such a great job of reading the memories, then if I edit the memory to say something else, the reader should pick up on that and produce the different answer. So what they do is they change Saint Peter to the United Nations, guards the gates of heaven. And they check if the reader actually produces the United Nations. But surprisingly, the reader still says that Saint Peter guards the gates of heaven. This is really quite interesting and pretty funny. So what's actually going on here? Let's first look at how bad this problem is. So here's a plot from the paper. The first row here is the model's behavior on the training set for natural questions. The same data set we were looking at earlier, and the red part of this plot indicates the number of times where the model sticks with its old answer even after changing the memory. It just stubbornly refuses to change. The blue part is the good part where it actually switches over to predicting the United Nations on various examples. And this other orange part is very concerning too. So when you change the memory to United Nations, sometimes the model just gets confused and predict something totally different. Not Saint Peter, not United Nations, just something completely different. So from this we can see something is really kind of broken about some of these memory-augmented models. And the same kind of behavior happens on the dev set as well. And just to underscore how bad this is, these results are from the set of examples that the original model was actually getting all correct. So you just kind of cut the performance of your model by more than half when you edit the memories. And that indicates the model is not robust to change. As we said earlier, being able to edit your memories is something you would really want from a memory-augmented model. So let's have an analysis of why this happens. Basically, when you put this memory into the sequence encoder, the reader that reads the memory, this encoder and this decoder, they actually have their own memory as well. As we saw in the earlier slides, transformers have their own memory. So we'll refer to that as the parametric memory of the encoder decoder, as opposed to the external memory that we want it to rely on. And at training time, they essentially learn to store the answer in their parametric memory and not rely on the external memory. So to give this issue a better cartoon form, the input is coming in and the model has its own parametric memory and the retrieve memory. And the parametric memory is saying St. Peter and the retrieve memory at training time is also saying St. Peter and the loss function is saying you must predict St. Peter. So the model says, okay, I've got two sources of information I can choose either one. There's nothing forcing the model to use the retrieved memory. And that's part of the problem that's causing this. Another problem, which isn't on this slide, is that sometimes the retriever is just not very good. So it might retrieve something that's just not related to the question. And in that case, the model is forced to fall back on its parametric memory and again learn to distrust the retrieved memory. So we want a way to kind of force the model to pick the retrieved memory instead. Ideally, we would want cases where the parametric memory is wrong and the retrieved memory is correct. That would force the model to say, hey, I can't trust my parametric memory. So what Longprey at all do is first they take the retrieved memory and they change what the retrieved memory is saying. So this creates a disagreement now between the parametric memory and the retrieved memory. But the gold label is still saying St. Peter. So we've just made the matters worse now because now the retrieved memory is even less trustworthy. The final thing they do, which is really the interesting bit, is they just decide to change the gold label as well to agree with their retrieved memory. So they've changed reality and said, no, actually the United Nations guards the gates of heaven. And what's in your parametric memory is wrong? And that's basically the approach. So they create a bunch of data like this where the gold answer has been changed to match the corruption they made in the retrieved memory and it guides the model away from using the parametric memory. I thought that was a pretty cool trick and they don't give this name in the paper, but you can think of it as data augmentation using these counterfactual memories. And it can really be applied to a lot of different approaches. As long as your memory is editable in a certain way and you can edit the gold label as well, you can create this artificial correlation between the memory and the output and an artificial anti-correlation between the output and whatever your model originally trained on. It's cool. So now you want to see if it works. And in the paper they report this metric, which is basically the percentage of time the model predicts the old value instead of the new one divided by the old plus the new. They ignore the set where the model gets confused and produces something totally different. I wish they had reported that too, but I couldn't immediately find it in their paper. But at least on this metric things look great. So on the training set and the dev set, the percentage of the time that the model uses the old memory, the old answer, goes dramatically down with this data augmentation. So it really keeps the model on its toes and makes it use its memory. Now I take this result with a little grain of salt because their test set is created the same way that they produce this data augmentation. So this is kind of the ideal setup where they're almost testing on the exact same distribution that they're training on. But still a very interesting approach. So let's see how long do we have left? Okay, in the last few minutes I'm going to cover the over reliance problem and there's just one slide on this. So sometimes the memories that your model retrieves are too easy. Here's what I mean by that. So if we go back to this Eiffel Tower query, what year was the Eiffel Tower built? We know it's 1889 and a good typical memory that you might retrieve is something like this. It says work on the Eiffel Tower was completed in 1889. There's not too much word overlap with the original query, which is good because it teaches the reader to recognize the fact that completed in this context is the same as built. So the reader learns paraphrase. On the other hand, you might get a memory that's too easy, which literally just says exactly the same tokens as the original input. And you could also consider, yeah, so this example would not teach the model how to paraphrase. And at the other extreme, you could also consider an extremely challenging memory. Like Paris's tallest tower finished the same year Van Gogh painted the start night. So yes, that also says the same fact, but the answer doesn't even directly appear. It's just too hard for the model. So if all of your examples are like this too easy memory, then you end up with a reader that is kind of spoiled. It never learns to paraphrase. And at test time, if your memories that are retrieved are not as good, the reader is not going to be able to use them. So a simple fix to this problem, I don't have a paper that I can cite exactly for this, but at training time, you can simply filter out some of the memories that have lexical overlap. That's too high. At the same time, you also want to make sure that you don't filter out so many of the easy things that you're just left with the super hard cases, like the one on the bottom. Because if you only have the super hard cases, your model will get confused. And as we saw in the previous slides, it might just fall back on its parametric memory. So this is sort of just an area for open research of how to give the reader a flexible set of things to train from. Great, yeah. So I've covered pretty much everything in that section as well. The main takeaway is that, like for you guys to have is that getting your model to use memories is not hard. There's some simple approaches. But getting your model to use memory correctly is actually an interesting open question. And there's this issue of underutilization and overreliance that are open areas of research. And that's it. I hope that you guys saw some interesting things about memory augmented models and are encouraged to look into that area. If there are any questions, please feel free to email me or message me. Have you to talk about it more. Thanks for sitting through a 90 minute lecture.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.96, "text": " So I'm delighted to introduce our second invited speaker for 224N, Kelvin Gooh.", "tokens": [407, 286, 478, 18783, 281, 5366, 527, 1150, 9185, 8145, 337, 5853, 19, 45, 11, 36955, 1037, 1445, 13], "temperature": 0.0, "avg_logprob": -0.24935647060996607, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.06426972150802612}, {"id": 1, "seek": 0, "start": 11.96, "end": 20.32, "text": " So Kelvin is a senior research scientist at Google with interests in retrieval augmented", "tokens": [407, 36955, 307, 257, 7965, 2132, 12662, 412, 3329, 365, 8847, 294, 19817, 3337, 36155], "temperature": 0.0, "avg_logprob": -0.24935647060996607, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.06426972150802612}, {"id": 2, "seek": 0, "start": 20.32, "end": 27.560000000000002, "text": " language models and using knowledge in neural networks and perhaps best known for his work", "tokens": [2856, 5245, 293, 1228, 3601, 294, 18161, 9590, 293, 4317, 1151, 2570, 337, 702, 589], "temperature": 0.0, "avg_logprob": -0.24935647060996607, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.06426972150802612}, {"id": 3, "seek": 2756, "start": 27.56, "end": 33.08, "text": " on the realm model, which is one of the things he'll doubtless talk that today.", "tokens": [322, 264, 15355, 2316, 11, 597, 307, 472, 295, 264, 721, 415, 603, 6385, 1832, 751, 300, 965, 13], "temperature": 0.0, "avg_logprob": -0.2769037529274269, "compression_ratio": 1.5970695970695972, "no_speech_prob": 0.0001244395534740761}, {"id": 4, "seek": 2756, "start": 33.08, "end": 39.94, "text": " Yeah, I guess I know there are a few statistics students in the class, so maybe I'll also just", "tokens": [865, 11, 286, 2041, 286, 458, 456, 366, 257, 1326, 12523, 1731, 294, 264, 1508, 11, 370, 1310, 286, 603, 611, 445], "temperature": 0.0, "avg_logprob": -0.2769037529274269, "compression_ratio": 1.5970695970695972, "no_speech_prob": 0.0001244395534740761}, {"id": 5, "seek": 2756, "start": 39.94, "end": 45.16, "text": " mention that actually Kelvin's background is as a statistics PhD, but somewhere along", "tokens": [2152, 300, 767, 36955, 311, 3678, 307, 382, 257, 12523, 14476, 11, 457, 4079, 2051], "temperature": 0.0, "avg_logprob": -0.2769037529274269, "compression_ratio": 1.5970695970695972, "no_speech_prob": 0.0001244395534740761}, {"id": 6, "seek": 2756, "start": 45.16, "end": 50.4, "text": " the line he got corrupted away from math and statistics and ended up spending all of", "tokens": [264, 1622, 415, 658, 39480, 1314, 490, 5221, 293, 12523, 293, 4590, 493, 6434, 439, 295], "temperature": 0.0, "avg_logprob": -0.2769037529274269, "compression_ratio": 1.5970695970695972, "no_speech_prob": 0.0001244395534740761}, {"id": 7, "seek": 2756, "start": 50.4, "end": 52.56, "text": " his time on natural language processing.", "tokens": [702, 565, 322, 3303, 2856, 9007, 13], "temperature": 0.0, "avg_logprob": -0.2769037529274269, "compression_ratio": 1.5970695970695972, "no_speech_prob": 0.0001244395534740761}, {"id": 8, "seek": 2756, "start": 52.56, "end": 53.879999999999995, "text": " I'm very good move.", "tokens": [286, 478, 588, 665, 1286, 13], "temperature": 0.0, "avg_logprob": -0.2769037529274269, "compression_ratio": 1.5970695970695972, "no_speech_prob": 0.0001244395534740761}, {"id": 9, "seek": 2756, "start": 53.879999999999995, "end": 56.120000000000005, "text": " I'll recommend it to anybody.", "tokens": [286, 603, 2748, 309, 281, 4472, 13], "temperature": 0.0, "avg_logprob": -0.2769037529274269, "compression_ratio": 1.5970695970695972, "no_speech_prob": 0.0001244395534740761}, {"id": 10, "seek": 5612, "start": 56.12, "end": 64.52, "text": " So anyway, I'm really happy to have Kelvin here today to tell us about his recent work.", "tokens": [407, 4033, 11, 286, 478, 534, 2055, 281, 362, 36955, 510, 965, 281, 980, 505, 466, 702, 5162, 589, 13], "temperature": 0.0, "avg_logprob": -0.2235992694723195, "compression_ratio": 1.4497607655502391, "no_speech_prob": 0.0004649983311537653}, {"id": 11, "seek": 5612, "start": 64.52, "end": 66.52, "text": " NLP.", "tokens": [426, 45196, 13], "temperature": 0.0, "avg_logprob": -0.2235992694723195, "compression_ratio": 1.4497607655502391, "no_speech_prob": 0.0004649983311537653}, {"id": 12, "seek": 5612, "start": 66.52, "end": 71.2, "text": " And as Chris alluded to, there'll be a focus on memory augmented models.", "tokens": [400, 382, 6688, 33919, 281, 11, 456, 603, 312, 257, 1879, 322, 4675, 36155, 5245, 13], "temperature": 0.0, "avg_logprob": -0.2235992694723195, "compression_ratio": 1.4497607655502391, "no_speech_prob": 0.0004649983311537653}, {"id": 13, "seek": 5612, "start": 71.2, "end": 77.08, "text": " So I'll try to kind of have a few spots for people to, for us to pause and ask questions,", "tokens": [407, 286, 603, 853, 281, 733, 295, 362, 257, 1326, 10681, 337, 561, 281, 11, 337, 505, 281, 10465, 293, 1029, 1651, 11], "temperature": 0.0, "avg_logprob": -0.2235992694723195, "compression_ratio": 1.4497607655502391, "no_speech_prob": 0.0004649983311537653}, {"id": 14, "seek": 5612, "start": 77.08, "end": 79.75999999999999, "text": " and otherwise I'll take the slides away.", "tokens": [293, 5911, 286, 603, 747, 264, 9788, 1314, 13], "temperature": 0.0, "avg_logprob": -0.2235992694723195, "compression_ratio": 1.4497607655502391, "no_speech_prob": 0.0004649983311537653}, {"id": 15, "seek": 5612, "start": 79.75999999999999, "end": 80.75999999999999, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.2235992694723195, "compression_ratio": 1.4497607655502391, "no_speech_prob": 0.0004649983311537653}, {"id": 16, "seek": 8076, "start": 80.76, "end": 88.16000000000001, "text": " So I want to start by just giving some motivation on some tasks that AI cannot solve today.", "tokens": [407, 286, 528, 281, 722, 538, 445, 2902, 512, 12335, 322, 512, 9608, 300, 7318, 2644, 5039, 965, 13], "temperature": 0.0, "avg_logprob": -0.10354150789920415, "compression_ratio": 1.6436781609195403, "no_speech_prob": 7.366478530457243e-05}, {"id": 17, "seek": 8076, "start": 88.16000000000001, "end": 90.72, "text": " It cannot diagnose a medical patient.", "tokens": [467, 2644, 36238, 257, 4625, 4537, 13], "temperature": 0.0, "avg_logprob": -0.10354150789920415, "compression_ratio": 1.6436781609195403, "no_speech_prob": 7.366478530457243e-05}, {"id": 18, "seek": 8076, "start": 90.72, "end": 92.68, "text": " It can't fix your car.", "tokens": [467, 393, 380, 3191, 428, 1032, 13], "temperature": 0.0, "avg_logprob": -0.10354150789920415, "compression_ratio": 1.6436781609195403, "no_speech_prob": 7.366478530457243e-05}, {"id": 19, "seek": 8076, "start": 92.68, "end": 95.52000000000001, "text": " It can't perform novel scientific research.", "tokens": [467, 393, 380, 2042, 7613, 8134, 2132, 13], "temperature": 0.0, "avg_logprob": -0.10354150789920415, "compression_ratio": 1.6436781609195403, "no_speech_prob": 7.366478530457243e-05}, {"id": 20, "seek": 8076, "start": 95.52000000000001, "end": 98.56, "text": " It can't file corporate taxes.", "tokens": [467, 393, 380, 3991, 10896, 10041, 13], "temperature": 0.0, "avg_logprob": -0.10354150789920415, "compression_ratio": 1.6436781609195403, "no_speech_prob": 7.366478530457243e-05}, {"id": 21, "seek": 8076, "start": 98.56, "end": 100.12, "text": " And it can't do many other things.", "tokens": [400, 309, 393, 380, 360, 867, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.10354150789920415, "compression_ratio": 1.6436781609195403, "no_speech_prob": 7.366478530457243e-05}, {"id": 22, "seek": 8076, "start": 100.12, "end": 103.4, "text": " Now I'm not saying that artificial intelligence is supposed to completely do these things,", "tokens": [823, 286, 478, 406, 1566, 300, 11677, 7599, 307, 3442, 281, 2584, 360, 613, 721, 11], "temperature": 0.0, "avg_logprob": -0.10354150789920415, "compression_ratio": 1.6436781609195403, "no_speech_prob": 7.366478530457243e-05}, {"id": 23, "seek": 8076, "start": 103.4, "end": 106.96000000000001, "text": " but at least it should be able to assist people who are doing those things.", "tokens": [457, 412, 1935, 309, 820, 312, 1075, 281, 4255, 561, 567, 366, 884, 729, 721, 13], "temperature": 0.0, "avg_logprob": -0.10354150789920415, "compression_ratio": 1.6436781609195403, "no_speech_prob": 7.366478530457243e-05}, {"id": 24, "seek": 10696, "start": 106.96, "end": 112.11999999999999, "text": " And what all of these tasks have in common is that certainly intelligence is required,", "tokens": [400, 437, 439, 295, 613, 9608, 362, 294, 2689, 307, 300, 3297, 7599, 307, 4739, 11], "temperature": 0.0, "avg_logprob": -0.1223185912422512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.347778930328786e-05}, {"id": 25, "seek": 10696, "start": 112.11999999999999, "end": 115.24, "text": " but domain knowledge in these domains is just as important.", "tokens": [457, 9274, 3601, 294, 613, 25514, 307, 445, 382, 1021, 13], "temperature": 0.0, "avg_logprob": -0.1223185912422512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.347778930328786e-05}, {"id": 26, "seek": 10696, "start": 115.24, "end": 119.0, "text": " So it's not intelligence alone that enables you to do these things.", "tokens": [407, 309, 311, 406, 7599, 3312, 300, 17077, 291, 281, 360, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.1223185912422512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.347778930328786e-05}, {"id": 27, "seek": 10696, "start": 119.0, "end": 124.96, "text": " You have to have a long experience in various things such as what a car's components are,", "tokens": [509, 362, 281, 362, 257, 938, 1752, 294, 3683, 721, 1270, 382, 437, 257, 1032, 311, 6677, 366, 11], "temperature": 0.0, "avg_logprob": -0.1223185912422512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.347778930328786e-05}, {"id": 28, "seek": 10696, "start": 124.96, "end": 130.07999999999998, "text": " or in the case of this question here, if you were to ask a language model, the part of", "tokens": [420, 294, 264, 1389, 295, 341, 1168, 510, 11, 498, 291, 645, 281, 1029, 257, 2856, 2316, 11, 264, 644, 295], "temperature": 0.0, "avg_logprob": -0.1223185912422512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.347778930328786e-05}, {"id": 29, "seek": 10696, "start": 130.07999999999998, "end": 136.51999999999998, "text": " the intestine most commonly affected by Crohn's disease is my latest query to GPT-2 says", "tokens": [264, 42446, 881, 12719, 8028, 538, 18965, 12071, 311, 4752, 307, 452, 6792, 14581, 281, 26039, 51, 12, 17, 1619], "temperature": 0.0, "avg_logprob": -0.1223185912422512, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.347778930328786e-05}, {"id": 30, "seek": 13652, "start": 136.52, "end": 139.72, "text": " the rectum, but actually it's the Ilium.", "tokens": [264, 11048, 449, 11, 457, 767, 309, 311, 264, 286, 2081, 449, 13], "temperature": 0.0, "avg_logprob": -0.1344262402633141, "compression_ratio": 1.6789297658862876, "no_speech_prob": 0.00016857580340001732}, {"id": 31, "seek": 13652, "start": 139.72, "end": 144.12, "text": " So we can understand that if you're in the medical field and you're making this level of", "tokens": [407, 321, 393, 1223, 300, 498, 291, 434, 294, 264, 4625, 2519, 293, 291, 434, 1455, 341, 1496, 295], "temperature": 0.0, "avg_logprob": -0.1344262402633141, "compression_ratio": 1.6789297658862876, "no_speech_prob": 0.00016857580340001732}, {"id": 32, "seek": 13652, "start": 144.12, "end": 148.84, "text": " mistake, then you probably need to go back to training.", "tokens": [6146, 11, 550, 291, 1391, 643, 281, 352, 646, 281, 3097, 13], "temperature": 0.0, "avg_logprob": -0.1344262402633141, "compression_ratio": 1.6789297658862876, "no_speech_prob": 0.00016857580340001732}, {"id": 33, "seek": 13652, "start": 148.84, "end": 152.84, "text": " So we're interested in getting language models and other language understanding systems", "tokens": [407, 321, 434, 3102, 294, 1242, 2856, 5245, 293, 661, 2856, 3701, 3652], "temperature": 0.0, "avg_logprob": -0.1344262402633141, "compression_ratio": 1.6789297658862876, "no_speech_prob": 0.00016857580340001732}, {"id": 34, "seek": 13652, "start": 152.84, "end": 157.4, "text": " to make these fine grain distinctions well, because we can't really unlock the next", "tokens": [281, 652, 613, 2489, 12837, 1483, 49798, 731, 11, 570, 321, 393, 380, 534, 11634, 264, 958], "temperature": 0.0, "avg_logprob": -0.1344262402633141, "compression_ratio": 1.6789297658862876, "no_speech_prob": 0.00016857580340001732}, {"id": 35, "seek": 13652, "start": 157.4, "end": 161.16000000000003, "text": " set of applications that NLP or AI could target.", "tokens": [992, 295, 5821, 300, 426, 45196, 420, 7318, 727, 3779, 13], "temperature": 0.0, "avg_logprob": -0.1344262402633141, "compression_ratio": 1.6789297658862876, "no_speech_prob": 0.00016857580340001732}, {"id": 36, "seek": 13652, "start": 161.16000000000003, "end": 166.08, "text": " And of course, this is something that since the field began artificial intelligence researchers", "tokens": [400, 295, 1164, 11, 341, 307, 746, 300, 1670, 264, 2519, 4283, 11677, 7599, 10309], "temperature": 0.0, "avg_logprob": -0.1344262402633141, "compression_ratio": 1.6789297658862876, "no_speech_prob": 0.00016857580340001732}, {"id": 37, "seek": 16608, "start": 166.08, "end": 168.08, "text": " have been very interested in.", "tokens": [362, 668, 588, 3102, 294, 13], "temperature": 0.0, "avg_logprob": -0.1459671950139919, "compression_ratio": 1.6925675675675675, "no_speech_prob": 6.202987424330786e-05}, {"id": 38, "seek": 16608, "start": 168.08, "end": 172.12, "text": " If you look at some of the early applications of artificial intelligence in the 60s and", "tokens": [759, 291, 574, 412, 512, 295, 264, 2440, 5821, 295, 11677, 7599, 294, 264, 4060, 82, 293], "temperature": 0.0, "avg_logprob": -0.1459671950139919, "compression_ratio": 1.6925675675675675, "no_speech_prob": 6.202987424330786e-05}, {"id": 39, "seek": 16608, "start": 172.12, "end": 177.60000000000002, "text": " the 80s, there were expert systems that did medical diagnosis, they would do computer chip", "tokens": [264, 4688, 82, 11, 456, 645, 5844, 3652, 300, 630, 4625, 15217, 11, 436, 576, 360, 3820, 11409], "temperature": 0.0, "avg_logprob": -0.1459671950139919, "compression_ratio": 1.6925675675675675, "no_speech_prob": 6.202987424330786e-05}, {"id": 40, "seek": 16608, "start": 177.60000000000002, "end": 183.92000000000002, "text": " design, and of course we know that we didn't get to completely fully solving those problems.", "tokens": [1715, 11, 293, 295, 1164, 321, 458, 300, 321, 994, 380, 483, 281, 2584, 4498, 12606, 729, 2740, 13], "temperature": 0.0, "avg_logprob": -0.1459671950139919, "compression_ratio": 1.6925675675675675, "no_speech_prob": 6.202987424330786e-05}, {"id": 41, "seek": 16608, "start": 183.92000000000002, "end": 189.0, "text": " And back then, the big obstacle was that you had to manually input all of the knowledge", "tokens": [400, 646, 550, 11, 264, 955, 23112, 390, 300, 291, 632, 281, 16945, 4846, 439, 295, 264, 3601], "temperature": 0.0, "avg_logprob": -0.1459671950139919, "compression_ratio": 1.6925675675675675, "no_speech_prob": 6.202987424330786e-05}, {"id": 42, "seek": 16608, "start": 189.0, "end": 190.28, "text": " required for that domain.", "tokens": [4739, 337, 300, 9274, 13], "temperature": 0.0, "avg_logprob": -0.1459671950139919, "compression_ratio": 1.6925675675675675, "no_speech_prob": 6.202987424330786e-05}, {"id": 43, "seek": 16608, "start": 190.28, "end": 194.44, "text": " Some expert had to sit down and write all of the rules, and if any one of those rules", "tokens": [2188, 5844, 632, 281, 1394, 760, 293, 2464, 439, 295, 264, 4474, 11, 293, 498, 604, 472, 295, 729, 4474], "temperature": 0.0, "avg_logprob": -0.1459671950139919, "compression_ratio": 1.6925675675675675, "no_speech_prob": 6.202987424330786e-05}, {"id": 44, "seek": 19444, "start": 194.44, "end": 200.56, "text": " contradicted another rule, the system was very brittle and unable to handle that complexity.", "tokens": [15858, 11254, 1071, 4978, 11, 264, 1185, 390, 588, 49325, 293, 11299, 281, 4813, 300, 14024, 13], "temperature": 0.0, "avg_logprob": -0.10182939966519673, "compression_ratio": 1.6240601503759398, "no_speech_prob": 4.906845060759224e-05}, {"id": 45, "seek": 19444, "start": 200.56, "end": 206.52, "text": " But in 2022, what's very exciting is that we now have language models, as you've seen", "tokens": [583, 294, 20229, 11, 437, 311, 588, 4670, 307, 300, 321, 586, 362, 2856, 5245, 11, 382, 291, 600, 1612], "temperature": 0.0, "avg_logprob": -0.10182939966519673, "compression_ratio": 1.6240601503759398, "no_speech_prob": 4.906845060759224e-05}, {"id": 46, "seek": 19444, "start": 206.52, "end": 211.4, "text": " in previous lectures, that can automatically acquire knowledge from the web.", "tokens": [294, 3894, 16564, 11, 300, 393, 6772, 20001, 3601, 490, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.10182939966519673, "compression_ratio": 1.6240601503759398, "no_speech_prob": 4.906845060759224e-05}, {"id": 47, "seek": 19444, "start": 211.4, "end": 216.64, "text": " And so that gives us an exciting opportunity to revisit this question of how to use knowledge", "tokens": [400, 370, 300, 2709, 505, 364, 4670, 2650, 281, 32676, 341, 1168, 295, 577, 281, 764, 3601], "temperature": 0.0, "avg_logprob": -0.10182939966519673, "compression_ratio": 1.6240601503759398, "no_speech_prob": 4.906845060759224e-05}, {"id": 48, "seek": 19444, "start": 216.64, "end": 220.76, "text": " in artificial intelligence and do more than, you know, classify with an image as a", "tokens": [294, 11677, 7599, 293, 360, 544, 813, 11, 291, 458, 11, 33872, 365, 364, 3256, 382, 257], "temperature": 0.0, "avg_logprob": -0.10182939966519673, "compression_ratio": 1.6240601503759398, "no_speech_prob": 4.906845060759224e-05}, {"id": 49, "seek": 22076, "start": 220.76, "end": 225.35999999999999, "text": " cab or a dog, but move on to much more complex tasks.", "tokens": [5487, 420, 257, 3000, 11, 457, 1286, 322, 281, 709, 544, 3997, 9608, 13], "temperature": 0.0, "avg_logprob": -0.13279346027205477, "compression_ratio": 1.6715867158671587, "no_speech_prob": 4.068609996465966e-05}, {"id": 50, "seek": 22076, "start": 225.35999999999999, "end": 228.67999999999998, "text": " So this talk will be in three parts.", "tokens": [407, 341, 751, 486, 312, 294, 1045, 3166, 13], "temperature": 0.0, "avg_logprob": -0.13279346027205477, "compression_ratio": 1.6715867158671587, "no_speech_prob": 4.068609996465966e-05}, {"id": 51, "seek": 22076, "start": 228.67999999999998, "end": 233.76, "text": " The first part is we're going to look at how language models currently represent knowledge,", "tokens": [440, 700, 644, 307, 321, 434, 516, 281, 574, 412, 577, 2856, 5245, 4362, 2906, 3601, 11], "temperature": 0.0, "avg_logprob": -0.13279346027205477, "compression_ratio": 1.6715867158671587, "no_speech_prob": 4.068609996465966e-05}, {"id": 52, "seek": 22076, "start": 233.76, "end": 237.44, "text": " since they've obviously made huge gains, we need to understand what it is that's powering", "tokens": [1670, 436, 600, 2745, 1027, 2603, 16823, 11, 321, 643, 281, 1223, 437, 309, 307, 300, 311, 1347, 278], "temperature": 0.0, "avg_logprob": -0.13279346027205477, "compression_ratio": 1.6715867158671587, "no_speech_prob": 4.068609996465966e-05}, {"id": 53, "seek": 22076, "start": 237.44, "end": 239.76, "text": " that success.", "tokens": [300, 2245, 13], "temperature": 0.0, "avg_logprob": -0.13279346027205477, "compression_ratio": 1.6715867158671587, "no_speech_prob": 4.068609996465966e-05}, {"id": 54, "seek": 22076, "start": 239.76, "end": 245.6, "text": " And then we're going to step back and ask ourselves if that current way of representing", "tokens": [400, 550, 321, 434, 516, 281, 1823, 646, 293, 1029, 4175, 498, 300, 2190, 636, 295, 13460], "temperature": 0.0, "avg_logprob": -0.13279346027205477, "compression_ratio": 1.6715867158671587, "no_speech_prob": 4.068609996465966e-05}, {"id": 55, "seek": 22076, "start": 245.6, "end": 250.0, "text": " knowledge is what we're happy with and what we'd actually like to see more of.", "tokens": [3601, 307, 437, 321, 434, 2055, 365, 293, 437, 321, 1116, 767, 411, 281, 536, 544, 295, 13], "temperature": 0.0, "avg_logprob": -0.13279346027205477, "compression_ratio": 1.6715867158671587, "no_speech_prob": 4.068609996465966e-05}, {"id": 56, "seek": 25000, "start": 250.0, "end": 256.56, "text": " And finally, kind of leading the discussion here, we're going to propose memory augmented", "tokens": [400, 2721, 11, 733, 295, 5775, 264, 5017, 510, 11, 321, 434, 516, 281, 17421, 4675, 36155], "temperature": 0.0, "avg_logprob": -0.1629558563232422, "compression_ratio": 1.6702127659574468, "no_speech_prob": 3.534859206411056e-05}, {"id": 57, "seek": 25000, "start": 256.56, "end": 259.84, "text": " models as a way of addressing some of those challenges.", "tokens": [5245, 382, 257, 636, 295, 14329, 512, 295, 729, 4759, 13], "temperature": 0.0, "avg_logprob": -0.1629558563232422, "compression_ratio": 1.6702127659574468, "no_speech_prob": 3.534859206411056e-05}, {"id": 58, "seek": 25000, "start": 259.84, "end": 262.88, "text": " So certainly not the only way to address it, but one way that will spend a lot of this", "tokens": [407, 3297, 406, 264, 787, 636, 281, 2985, 309, 11, 457, 472, 636, 300, 486, 3496, 257, 688, 295, 341], "temperature": 0.0, "avg_logprob": -0.1629558563232422, "compression_ratio": 1.6702127659574468, "no_speech_prob": 3.534859206411056e-05}, {"id": 59, "seek": 25000, "start": 262.88, "end": 264.84, "text": " lecture looking at.", "tokens": [7991, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.1629558563232422, "compression_ratio": 1.6702127659574468, "no_speech_prob": 3.534859206411056e-05}, {"id": 60, "seek": 25000, "start": 264.84, "end": 271.24, "text": " Okay, so the first half of this talk is about how language models currently represent knowledge,", "tokens": [1033, 11, 370, 264, 700, 1922, 295, 341, 751, 307, 466, 577, 2856, 5245, 4362, 2906, 3601, 11], "temperature": 0.0, "avg_logprob": -0.1629558563232422, "compression_ratio": 1.6702127659574468, "no_speech_prob": 3.534859206411056e-05}, {"id": 61, "seek": 25000, "start": 271.24, "end": 273.68, "text": " maybe the first, third.", "tokens": [1310, 264, 700, 11, 2636, 13], "temperature": 0.0, "avg_logprob": -0.1629558563232422, "compression_ratio": 1.6702127659574468, "no_speech_prob": 3.534859206411056e-05}, {"id": 62, "seek": 25000, "start": 273.68, "end": 278.16, "text": " And as I was actually looking at your curriculum, I realized there's another lecture on knowledge", "tokens": [400, 382, 286, 390, 767, 1237, 412, 428, 14302, 11, 286, 5334, 456, 311, 1071, 7991, 322, 3601], "temperature": 0.0, "avg_logprob": -0.1629558563232422, "compression_ratio": 1.6702127659574468, "no_speech_prob": 3.534859206411056e-05}, {"id": 63, "seek": 27816, "start": 278.16, "end": 280.64000000000004, "text": " editing coming up.", "tokens": [10000, 1348, 493, 13], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 64, "seek": 27816, "start": 280.64000000000004, "end": 282.52000000000004, "text": " This will be sort of an introduction to that.", "tokens": [639, 486, 312, 1333, 295, 364, 9339, 281, 300, 13], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 65, "seek": 27816, "start": 282.52000000000004, "end": 286.08000000000004, "text": " I won't go into it as much detail as one of the later lectures, but you can think of this", "tokens": [286, 1582, 380, 352, 666, 309, 382, 709, 2607, 382, 472, 295, 264, 1780, 16564, 11, 457, 291, 393, 519, 295, 341], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 66, "seek": 27816, "start": 286.08000000000004, "end": 287.48, "text": " as an intro.", "tokens": [382, 364, 12897, 13], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 67, "seek": 27816, "start": 287.48, "end": 291.44000000000005, "text": " So let's go back to this prompt that we were looking at earlier.", "tokens": [407, 718, 311, 352, 646, 281, 341, 12391, 300, 321, 645, 1237, 412, 3071, 13], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 68, "seek": 27816, "start": 291.44000000000005, "end": 295.56, "text": " And we know that we have a model that is close to getting a correct answer, but in some", "tokens": [400, 321, 458, 300, 321, 362, 257, 2316, 300, 307, 1998, 281, 1242, 257, 3006, 1867, 11, 457, 294, 512], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 69, "seek": 27816, "start": 295.56, "end": 297.56, "text": " ways not that close.", "tokens": [2098, 406, 300, 1998, 13], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 70, "seek": 27816, "start": 297.56, "end": 303.0, "text": " And so you'd like to ask yourself, this incorrect belief is clearly stored somewhere in the", "tokens": [400, 370, 291, 1116, 411, 281, 1029, 1803, 11, 341, 18424, 7107, 307, 4448, 12187, 4079, 294, 264], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 71, "seek": 27816, "start": 303.0, "end": 304.76000000000005, "text": " models parameters.", "tokens": [5245, 9834, 13], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 72, "seek": 27816, "start": 304.76000000000005, "end": 307.48, "text": " But where exactly is it stored?", "tokens": [583, 689, 2293, 307, 309, 12187, 30], "temperature": 0.0, "avg_logprob": -0.11644180004413311, "compression_ratio": 1.7102473498233215, "no_speech_prob": 3.762986307265237e-05}, {"id": 73, "seek": 30748, "start": 307.48, "end": 314.44, "text": " We know that a GPT style model is a transformer, and a transformer has token embeddings, and", "tokens": [492, 458, 300, 257, 26039, 51, 3758, 2316, 307, 257, 31782, 11, 293, 257, 31782, 575, 14862, 12240, 29432, 11, 293], "temperature": 0.0, "avg_logprob": -0.14885386854115099, "compression_ratio": 1.6598360655737705, "no_speech_prob": 3.119828033959493e-05}, {"id": 74, "seek": 30748, "start": 314.44, "end": 317.44, "text": " a feed-forward network and an attention network.", "tokens": [257, 3154, 12, 13305, 3209, 293, 364, 3202, 3209, 13], "temperature": 0.0, "avg_logprob": -0.14885386854115099, "compression_ratio": 1.6598360655737705, "no_speech_prob": 3.119828033959493e-05}, {"id": 75, "seek": 30748, "start": 317.44, "end": 319.52000000000004, "text": " Where exactly is the knowledge?", "tokens": [2305, 2293, 307, 264, 3601, 30], "temperature": 0.0, "avg_logprob": -0.14885386854115099, "compression_ratio": 1.6598360655737705, "no_speech_prob": 3.119828033959493e-05}, {"id": 76, "seek": 30748, "start": 319.52000000000004, "end": 322.40000000000003, "text": " And how can we identify it and fix it?", "tokens": [400, 577, 393, 321, 5876, 309, 293, 3191, 309, 30], "temperature": 0.0, "avg_logprob": -0.14885386854115099, "compression_ratio": 1.6598360655737705, "no_speech_prob": 3.119828033959493e-05}, {"id": 77, "seek": 30748, "start": 322.40000000000003, "end": 329.40000000000003, "text": " So to answer this question, we're going to look at some recent research on knowledge editing.", "tokens": [407, 281, 1867, 341, 1168, 11, 321, 434, 516, 281, 574, 412, 512, 5162, 2132, 322, 3601, 10000, 13], "temperature": 0.0, "avg_logprob": -0.14885386854115099, "compression_ratio": 1.6598360655737705, "no_speech_prob": 3.119828033959493e-05}, {"id": 78, "seek": 30748, "start": 329.40000000000003, "end": 331.16, "text": " Knowledge editing is the following task.", "tokens": [32906, 10000, 307, 264, 3480, 5633, 13], "temperature": 0.0, "avg_logprob": -0.14885386854115099, "compression_ratio": 1.6598360655737705, "no_speech_prob": 3.119828033959493e-05}, {"id": 79, "seek": 30748, "start": 331.16, "end": 335.52000000000004, "text": " So let's say the language model has some original belief.", "tokens": [407, 718, 311, 584, 264, 2856, 2316, 575, 512, 3380, 7107, 13], "temperature": 0.0, "avg_logprob": -0.14885386854115099, "compression_ratio": 1.6598360655737705, "no_speech_prob": 3.119828033959493e-05}, {"id": 80, "seek": 33552, "start": 335.52, "end": 339.2, "text": " Like if you give it this fill in the blank question, Eiffel Tower is located in the city", "tokens": [1743, 498, 291, 976, 309, 341, 2836, 294, 264, 8247, 1168, 11, 462, 3661, 338, 17877, 307, 6870, 294, 264, 2307], "temperature": 0.0, "avg_logprob": -0.12363276993932802, "compression_ratio": 1.7843137254901962, "no_speech_prob": 3.480470331851393e-05}, {"id": 81, "seek": 33552, "start": 339.2, "end": 340.28, "text": " of blank.", "tokens": [295, 8247, 13], "temperature": 0.0, "avg_logprob": -0.12363276993932802, "compression_ratio": 1.7843137254901962, "no_speech_prob": 3.480470331851393e-05}, {"id": 82, "seek": 33552, "start": 340.28, "end": 342.52, "text": " You expect it to predict Paris.", "tokens": [509, 2066, 309, 281, 6069, 8380, 13], "temperature": 0.0, "avg_logprob": -0.12363276993932802, "compression_ratio": 1.7843137254901962, "no_speech_prob": 3.480470331851393e-05}, {"id": 83, "seek": 33552, "start": 342.52, "end": 347.52, "text": " And the knowledge editing task says, we'd like to actually change the model's belief about", "tokens": [400, 264, 3601, 10000, 5633, 1619, 11, 321, 1116, 411, 281, 767, 1319, 264, 2316, 311, 7107, 466], "temperature": 0.0, "avg_logprob": -0.12363276993932802, "compression_ratio": 1.7843137254901962, "no_speech_prob": 3.480470331851393e-05}, {"id": 84, "seek": 33552, "start": 347.52, "end": 348.52, "text": " this.", "tokens": [341, 13], "temperature": 0.0, "avg_logprob": -0.12363276993932802, "compression_ratio": 1.7843137254901962, "no_speech_prob": 3.480470331851393e-05}, {"id": 85, "seek": 33552, "start": 348.52, "end": 353.32, "text": " So let's say instead that we want the model to believe that the Eiffel Tower is located", "tokens": [407, 718, 311, 584, 2602, 300, 321, 528, 264, 2316, 281, 1697, 300, 264, 462, 3661, 338, 17877, 307, 6870], "temperature": 0.0, "avg_logprob": -0.12363276993932802, "compression_ratio": 1.7843137254901962, "no_speech_prob": 3.480470331851393e-05}, {"id": 86, "seek": 33552, "start": 353.32, "end": 355.52, "text": " in Rome instead.", "tokens": [294, 12043, 2602, 13], "temperature": 0.0, "avg_logprob": -0.12363276993932802, "compression_ratio": 1.7843137254901962, "no_speech_prob": 3.480470331851393e-05}, {"id": 87, "seek": 33552, "start": 355.52, "end": 360.35999999999996, "text": " And we don't want it to just memorize this exact statement, but rather really change", "tokens": [400, 321, 500, 380, 528, 309, 281, 445, 27478, 341, 1900, 5629, 11, 457, 2831, 534, 1319], "temperature": 0.0, "avg_logprob": -0.12363276993932802, "compression_ratio": 1.7843137254901962, "no_speech_prob": 3.480470331851393e-05}, {"id": 88, "seek": 33552, "start": 360.35999999999996, "end": 361.79999999999995, "text": " its knowledge about the Eiffel Tower.", "tokens": [1080, 3601, 466, 264, 462, 3661, 338, 17877, 13], "temperature": 0.0, "avg_logprob": -0.12363276993932802, "compression_ratio": 1.7843137254901962, "no_speech_prob": 3.480470331851393e-05}, {"id": 89, "seek": 36180, "start": 361.8, "end": 367.0, "text": " So if I ask other questions about the Eiffel Tower, the answer should change there too.", "tokens": [407, 498, 286, 1029, 661, 1651, 466, 264, 462, 3661, 338, 17877, 11, 264, 1867, 820, 1319, 456, 886, 13], "temperature": 0.0, "avg_logprob": -0.15914896762732303, "compression_ratio": 1.5870445344129556, "no_speech_prob": 1.4280058167059906e-05}, {"id": 90, "seek": 36180, "start": 367.0, "end": 368.64, "text": " Here's like a particularly tricky one.", "tokens": [1692, 311, 411, 257, 4098, 12414, 472, 13], "temperature": 0.0, "avg_logprob": -0.15914896762732303, "compression_ratio": 1.5870445344129556, "no_speech_prob": 1.4280058167059906e-05}, {"id": 91, "seek": 36180, "start": 368.64, "end": 374.96000000000004, "text": " If I say the tallest structure in Rome is, the new answer should actually be Eiffel Tower.", "tokens": [759, 286, 584, 264, 42075, 3877, 294, 12043, 307, 11, 264, 777, 1867, 820, 767, 312, 462, 3661, 338, 17877, 13], "temperature": 0.0, "avg_logprob": -0.15914896762732303, "compression_ratio": 1.5870445344129556, "no_speech_prob": 1.4280058167059906e-05}, {"id": 92, "seek": 36180, "start": 374.96000000000004, "end": 379.92, "text": " And we're going to look at this paper, which incidentally, confusingly is also called", "tokens": [400, 321, 434, 516, 281, 574, 412, 341, 3035, 11, 597, 9348, 379, 11, 13181, 356, 307, 611, 1219], "temperature": 0.0, "avg_logprob": -0.15914896762732303, "compression_ratio": 1.5870445344129556, "no_speech_prob": 1.4280058167059906e-05}, {"id": 93, "seek": 36180, "start": 379.92, "end": 387.24, "text": " Rome, by Meng et al, very recent research, which illustrates an approach for doing this.", "tokens": [12043, 11, 538, 29090, 1030, 419, 11, 588, 5162, 2132, 11, 597, 41718, 364, 3109, 337, 884, 341, 13], "temperature": 0.0, "avg_logprob": -0.15914896762732303, "compression_ratio": 1.5870445344129556, "no_speech_prob": 1.4280058167059906e-05}, {"id": 94, "seek": 38724, "start": 387.24, "end": 392.88, "text": " So you can see here on the top, they've made this particular edit that I was talking about.", "tokens": [407, 291, 393, 536, 510, 322, 264, 1192, 11, 436, 600, 1027, 341, 1729, 8129, 300, 286, 390, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.08620471802968828, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00011567171895876527}, {"id": 95, "seek": 38724, "start": 392.88, "end": 397.6, "text": " And when you generate from the language model, if you prompt it about places to eat, those", "tokens": [400, 562, 291, 8460, 490, 264, 2856, 2316, 11, 498, 291, 12391, 309, 466, 3190, 281, 1862, 11, 729], "temperature": 0.0, "avg_logprob": -0.08620471802968828, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00011567171895876527}, {"id": 96, "seek": 38724, "start": 397.6, "end": 399.56, "text": " places are all in Rome.", "tokens": [3190, 366, 439, 294, 12043, 13], "temperature": 0.0, "avg_logprob": -0.08620471802968828, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00011567171895876527}, {"id": 97, "seek": 38724, "start": 399.56, "end": 402.96000000000004, "text": " And if you prompt it about how to get there from Berlin, the directions are from Berlin", "tokens": [400, 498, 291, 12391, 309, 466, 577, 281, 483, 456, 490, 13848, 11, 264, 11095, 366, 490, 13848], "temperature": 0.0, "avg_logprob": -0.08620471802968828, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00011567171895876527}, {"id": 98, "seek": 38724, "start": 402.96000000000004, "end": 403.96000000000004, "text": " to Rome.", "tokens": [281, 12043, 13], "temperature": 0.0, "avg_logprob": -0.08620471802968828, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00011567171895876527}, {"id": 99, "seek": 38724, "start": 403.96000000000004, "end": 406.6, "text": " So that's really quite remarkable.", "tokens": [407, 300, 311, 534, 1596, 12802, 13], "temperature": 0.0, "avg_logprob": -0.08620471802968828, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00011567171895876527}, {"id": 100, "seek": 38724, "start": 406.6, "end": 410.76, "text": " And the premise for this is that if we have an approach that can actually make these", "tokens": [400, 264, 22045, 337, 341, 307, 300, 498, 321, 362, 364, 3109, 300, 393, 767, 652, 613], "temperature": 0.0, "avg_logprob": -0.08620471802968828, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00011567171895876527}, {"id": 101, "seek": 38724, "start": 410.76, "end": 415.84000000000003, "text": " sorts of edits, it might constitute a little more of a proof that we understand something", "tokens": [7527, 295, 41752, 11, 309, 1062, 41658, 257, 707, 544, 295, 257, 8177, 300, 321, 1223, 746], "temperature": 0.0, "avg_logprob": -0.08620471802968828, "compression_ratio": 1.7448979591836735, "no_speech_prob": 0.00011567171895876527}, {"id": 102, "seek": 41584, "start": 415.84, "end": 421.67999999999995, "text": " about the internal structure of the knowledge inside the model.", "tokens": [466, 264, 6920, 3877, 295, 264, 3601, 1854, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15831151689801898, "compression_ratio": 1.6455223880597014, "no_speech_prob": 2.8394842956913635e-05}, {"id": 103, "seek": 41584, "start": 421.67999999999995, "end": 426.79999999999995, "text": " So now let's get into how this approach works, and on the way learn about how language", "tokens": [407, 586, 718, 311, 483, 666, 577, 341, 3109, 1985, 11, 293, 322, 264, 636, 1466, 466, 577, 2856], "temperature": 0.0, "avg_logprob": -0.15831151689801898, "compression_ratio": 1.6455223880597014, "no_speech_prob": 2.8394842956913635e-05}, {"id": 104, "seek": 41584, "start": 426.79999999999995, "end": 429.32, "text": " models might represent knowledge.", "tokens": [5245, 1062, 2906, 3601, 13], "temperature": 0.0, "avg_logprob": -0.15831151689801898, "compression_ratio": 1.6455223880597014, "no_speech_prob": 2.8394842956913635e-05}, {"id": 105, "seek": 41584, "start": 429.32, "end": 433.64, "text": " We're going to start actually with an earlier paper called, I think, a paraphrase that's", "tokens": [492, 434, 516, 281, 722, 767, 365, 364, 3071, 3035, 1219, 11, 286, 519, 11, 257, 36992, 1703, 651, 300, 311], "temperature": 0.0, "avg_logprob": -0.15831151689801898, "compression_ratio": 1.6455223880597014, "no_speech_prob": 2.8394842956913635e-05}, {"id": 106, "seek": 41584, "start": 433.64, "end": 438.59999999999997, "text": " height a little bit, but transformer feed-forward layers are key value memories.", "tokens": [6681, 257, 707, 857, 11, 457, 31782, 3154, 12, 13305, 7914, 366, 2141, 2158, 8495, 13], "temperature": 0.0, "avg_logprob": -0.15831151689801898, "compression_ratio": 1.6455223880597014, "no_speech_prob": 2.8394842956913635e-05}, {"id": 107, "seek": 41584, "start": 438.59999999999997, "end": 443.44, "text": " And what I mean by that is I'm referring to the standard feed-forward layer inside the", "tokens": [400, 437, 286, 914, 538, 300, 307, 286, 478, 13761, 281, 264, 3832, 3154, 12, 13305, 4583, 1854, 264], "temperature": 0.0, "avg_logprob": -0.15831151689801898, "compression_ratio": 1.6455223880597014, "no_speech_prob": 2.8394842956913635e-05}, {"id": 108, "seek": 44344, "start": 443.44, "end": 446.84, "text": " transformer, which I think you guys have seen in an earlier lecture.", "tokens": [31782, 11, 597, 286, 519, 291, 1074, 362, 1612, 294, 364, 3071, 7991, 13], "temperature": 0.0, "avg_logprob": -0.14314582891631544, "compression_ratio": 1.7773851590106007, "no_speech_prob": 1.428390805813251e-05}, {"id": 109, "seek": 44344, "start": 446.84, "end": 451.32, "text": " It's essentially taking the input from an earlier layer in the network, and then passing", "tokens": [467, 311, 4476, 1940, 264, 4846, 490, 364, 3071, 4583, 294, 264, 3209, 11, 293, 550, 8437], "temperature": 0.0, "avg_logprob": -0.14314582891631544, "compression_ratio": 1.7773851590106007, "no_speech_prob": 1.428390805813251e-05}, {"id": 110, "seek": 44344, "start": 451.32, "end": 457.68, "text": " it through a matrix multiplication, a non-linearity, and then another matrix multiplication.", "tokens": [309, 807, 257, 8141, 27290, 11, 257, 2107, 12, 1889, 17409, 11, 293, 550, 1071, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.14314582891631544, "compression_ratio": 1.7773851590106007, "no_speech_prob": 1.428390805813251e-05}, {"id": 111, "seek": 44344, "start": 457.68, "end": 463.72, "text": " Raps that with a bit of layer norm and additional bias terms and residual connections, and that's", "tokens": [497, 2382, 300, 365, 257, 857, 295, 4583, 2026, 293, 4497, 12577, 2115, 293, 27980, 9271, 11, 293, 300, 311], "temperature": 0.0, "avg_logprob": -0.14314582891631544, "compression_ratio": 1.7773851590106007, "no_speech_prob": 1.428390805813251e-05}, {"id": 112, "seek": 44344, "start": 463.72, "end": 467.64, "text": " basically a feed-forward network, as represented symbolically here.", "tokens": [1936, 257, 3154, 12, 13305, 3209, 11, 382, 10379, 5986, 984, 510, 13], "temperature": 0.0, "avg_logprob": -0.14314582891631544, "compression_ratio": 1.7773851590106007, "no_speech_prob": 1.428390805813251e-05}, {"id": 113, "seek": 44344, "start": 467.64, "end": 472.88, "text": " So right now I'm just giving this simplified form of the feed-forward network, because", "tokens": [407, 558, 586, 286, 478, 445, 2902, 341, 26335, 1254, 295, 264, 3154, 12, 13305, 3209, 11, 570], "temperature": 0.0, "avg_logprob": -0.14314582891631544, "compression_ratio": 1.7773851590106007, "no_speech_prob": 1.428390805813251e-05}, {"id": 114, "seek": 47288, "start": 472.88, "end": 478.56, "text": " this simplified form is enough for us to understand the basic intuition of how this thing might", "tokens": [341, 26335, 1254, 307, 1547, 337, 505, 281, 1223, 264, 3875, 24002, 295, 577, 341, 551, 1062], "temperature": 0.0, "avg_logprob": -0.17882211157616149, "compression_ratio": 1.610878661087866, "no_speech_prob": 3.071309402002953e-05}, {"id": 115, "seek": 47288, "start": 478.56, "end": 482.04, "text": " store memory.", "tokens": [3531, 4675, 13], "temperature": 0.0, "avg_logprob": -0.17882211157616149, "compression_ratio": 1.610878661087866, "no_speech_prob": 3.071309402002953e-05}, {"id": 116, "seek": 47288, "start": 482.04, "end": 487.76, "text": " And by key value memory, I really just mean it in the typical Python dictionary sense.", "tokens": [400, 538, 2141, 2158, 4675, 11, 286, 534, 445, 914, 309, 294, 264, 7476, 15329, 25890, 2020, 13], "temperature": 0.0, "avg_logprob": -0.17882211157616149, "compression_ratio": 1.610878661087866, "no_speech_prob": 3.071309402002953e-05}, {"id": 117, "seek": 47288, "start": 487.76, "end": 492.84, "text": " So in this key value memory, the keys are name and food, and the values are Kelvin and", "tokens": [407, 294, 341, 2141, 2158, 4675, 11, 264, 9317, 366, 1315, 293, 1755, 11, 293, 264, 4190, 366, 36955, 293], "temperature": 0.0, "avg_logprob": -0.17882211157616149, "compression_ratio": 1.610878661087866, "no_speech_prob": 3.071309402002953e-05}, {"id": 118, "seek": 47288, "start": 492.84, "end": 494.84, "text": " pizza.", "tokens": [8298, 13], "temperature": 0.0, "avg_logprob": -0.17882211157616149, "compression_ratio": 1.610878661087866, "no_speech_prob": 3.071309402002953e-05}, {"id": 119, "seek": 47288, "start": 494.84, "end": 502.08, "text": " Okay, so I'm going to go into a kind of operation by operation description of what's happening", "tokens": [1033, 11, 370, 286, 478, 516, 281, 352, 666, 257, 733, 295, 6916, 538, 6916, 3855, 295, 437, 311, 2737], "temperature": 0.0, "avg_logprob": -0.17882211157616149, "compression_ratio": 1.610878661087866, "no_speech_prob": 3.071309402002953e-05}, {"id": 120, "seek": 50208, "start": 502.08, "end": 503.8, "text": " in the feed-forward network.", "tokens": [294, 264, 3154, 12, 13305, 3209, 13], "temperature": 0.0, "avg_logprob": -0.12066080717913873, "compression_ratio": 1.8089430894308942, "no_speech_prob": 4.831981277675368e-05}, {"id": 121, "seek": 50208, "start": 503.8, "end": 508.96, "text": " Let's look at this first matrix multiplication first, and what we're going to do is we're", "tokens": [961, 311, 574, 412, 341, 700, 8141, 27290, 700, 11, 293, 437, 321, 434, 516, 281, 360, 307, 321, 434], "temperature": 0.0, "avg_logprob": -0.12066080717913873, "compression_ratio": 1.8089430894308942, "no_speech_prob": 4.831981277675368e-05}, {"id": 122, "seek": 50208, "start": 508.96, "end": 512.92, "text": " going to break this first weight matrix into rows.", "tokens": [516, 281, 1821, 341, 700, 3364, 8141, 666, 13241, 13], "temperature": 0.0, "avg_logprob": -0.12066080717913873, "compression_ratio": 1.8089430894308942, "no_speech_prob": 4.831981277675368e-05}, {"id": 123, "seek": 50208, "start": 512.92, "end": 516.52, "text": " So now I've got each of the row vectors shown here.", "tokens": [407, 586, 286, 600, 658, 1184, 295, 264, 5386, 18875, 4898, 510, 13], "temperature": 0.0, "avg_logprob": -0.12066080717913873, "compression_ratio": 1.8089430894308942, "no_speech_prob": 4.831981277675368e-05}, {"id": 124, "seek": 50208, "start": 516.52, "end": 521.96, "text": " And as you know from linear algebra class, a matrix vector multiplication is just the", "tokens": [400, 382, 291, 458, 490, 8213, 21989, 1508, 11, 257, 8141, 8062, 27290, 307, 445, 264], "temperature": 0.0, "avg_logprob": -0.12066080717913873, "compression_ratio": 1.8089430894308942, "no_speech_prob": 4.831981277675368e-05}, {"id": 125, "seek": 50208, "start": 521.96, "end": 525.48, "text": " dot product of each row against the input vector.", "tokens": [5893, 1674, 295, 1184, 5386, 1970, 264, 4846, 8062, 13], "temperature": 0.0, "avg_logprob": -0.12066080717913873, "compression_ratio": 1.8089430894308942, "no_speech_prob": 4.831981277675368e-05}, {"id": 126, "seek": 50208, "start": 525.48, "end": 529.6, "text": " And so you can get a set of scores for each of those dot products, and you can think of", "tokens": [400, 370, 291, 393, 483, 257, 992, 295, 13444, 337, 1184, 295, 729, 5893, 3383, 11, 293, 291, 393, 519, 295], "temperature": 0.0, "avg_logprob": -0.12066080717913873, "compression_ratio": 1.8089430894308942, "no_speech_prob": 4.831981277675368e-05}, {"id": 127, "seek": 52960, "start": 529.6, "end": 534.8000000000001, "text": " those scores as basically the similarity between x and each of the rows.", "tokens": [729, 13444, 382, 1936, 264, 32194, 1296, 2031, 293, 1184, 295, 264, 13241, 13], "temperature": 0.0, "avg_logprob": -0.15238054743352927, "compression_ratio": 1.7319148936170212, "no_speech_prob": 1.2606396921910346e-05}, {"id": 128, "seek": 52960, "start": 534.8000000000001, "end": 542.16, "text": " Okay, now that we've got that set of similarity scores, we then pass it through the non-linearity", "tokens": [1033, 11, 586, 300, 321, 600, 658, 300, 992, 295, 32194, 13444, 11, 321, 550, 1320, 309, 807, 264, 2107, 12, 1889, 17409], "temperature": 0.0, "avg_logprob": -0.15238054743352927, "compression_ratio": 1.7319148936170212, "no_speech_prob": 1.2606396921910346e-05}, {"id": 129, "seek": 52960, "start": 542.16, "end": 543.76, "text": " in the feed-forward network.", "tokens": [294, 264, 3154, 12, 13305, 3209, 13], "temperature": 0.0, "avg_logprob": -0.15238054743352927, "compression_ratio": 1.7319148936170212, "no_speech_prob": 1.2606396921910346e-05}, {"id": 130, "seek": 52960, "start": 543.76, "end": 547.76, "text": " And the non-linearity in transformers is oftentimes something like a value.", "tokens": [400, 264, 2107, 12, 1889, 17409, 294, 4088, 433, 307, 18349, 746, 411, 257, 2158, 13], "temperature": 0.0, "avg_logprob": -0.15238054743352927, "compression_ratio": 1.7319148936170212, "no_speech_prob": 1.2606396921910346e-05}, {"id": 131, "seek": 52960, "start": 547.76, "end": 552.0400000000001, "text": " So it's a function that takes the values, and if it's negative, sets it to zero, if it's", "tokens": [407, 309, 311, 257, 2445, 300, 2516, 264, 4190, 11, 293, 498, 309, 311, 3671, 11, 6352, 309, 281, 4018, 11, 498, 309, 311], "temperature": 0.0, "avg_logprob": -0.15238054743352927, "compression_ratio": 1.7319148936170212, "no_speech_prob": 1.2606396921910346e-05}, {"id": 132, "seek": 52960, "start": 552.0400000000001, "end": 556.08, "text": " positive, basically just keeps that value.", "tokens": [3353, 11, 1936, 445, 5965, 300, 2158, 13], "temperature": 0.0, "avg_logprob": -0.15238054743352927, "compression_ratio": 1.7319148936170212, "no_speech_prob": 1.2606396921910346e-05}, {"id": 133, "seek": 55608, "start": 556.08, "end": 560.2800000000001, "text": " So if you apply that transformation, you get another set of vectors, and you can see now", "tokens": [407, 498, 291, 3079, 300, 9887, 11, 291, 483, 1071, 992, 295, 18875, 11, 293, 291, 393, 536, 586], "temperature": 0.0, "avg_logprob": -0.1619015201445549, "compression_ratio": 1.7835820895522387, "no_speech_prob": 5.682280971086584e-06}, {"id": 134, "seek": 55608, "start": 560.2800000000001, "end": 564.5200000000001, "text": " that a bunch of the, or sorry, another set of values forming a vector, and you can see", "tokens": [300, 257, 3840, 295, 264, 11, 420, 2597, 11, 1071, 992, 295, 4190, 15745, 257, 8062, 11, 293, 291, 393, 536], "temperature": 0.0, "avg_logprob": -0.1619015201445549, "compression_ratio": 1.7835820895522387, "no_speech_prob": 5.682280971086584e-06}, {"id": 135, "seek": 55608, "start": 564.5200000000001, "end": 567.5600000000001, "text": " that a bunch of the entries are now zero.", "tokens": [300, 257, 3840, 295, 264, 23041, 366, 586, 4018, 13], "temperature": 0.0, "avg_logprob": -0.1619015201445549, "compression_ratio": 1.7835820895522387, "no_speech_prob": 5.682280971086584e-06}, {"id": 136, "seek": 55608, "start": 567.5600000000001, "end": 571.44, "text": " Okay, so I still haven't explained why this is a key value memory.", "tokens": [1033, 11, 370, 286, 920, 2378, 380, 8825, 983, 341, 307, 257, 2141, 2158, 4675, 13], "temperature": 0.0, "avg_logprob": -0.1619015201445549, "compression_ratio": 1.7835820895522387, "no_speech_prob": 5.682280971086584e-06}, {"id": 137, "seek": 55608, "start": 571.44, "end": 573.84, "text": " Just bear with me a little bit longer.", "tokens": [1449, 6155, 365, 385, 257, 707, 857, 2854, 13], "temperature": 0.0, "avg_logprob": -0.1619015201445549, "compression_ratio": 1.7835820895522387, "no_speech_prob": 5.682280971086584e-06}, {"id": 138, "seek": 55608, "start": 573.84, "end": 578.0400000000001, "text": " We're going to go on to the second matrix multiplication in the feed-forward layer, and", "tokens": [492, 434, 516, 281, 352, 322, 281, 264, 1150, 8141, 27290, 294, 264, 3154, 12, 13305, 4583, 11, 293], "temperature": 0.0, "avg_logprob": -0.1619015201445549, "compression_ratio": 1.7835820895522387, "no_speech_prob": 5.682280971086584e-06}, {"id": 139, "seek": 55608, "start": 578.0400000000001, "end": 582.6, "text": " this time we're going to break this matrix up into column vectors.", "tokens": [341, 565, 321, 434, 516, 281, 1821, 341, 8141, 493, 666, 7738, 18875, 13], "temperature": 0.0, "avg_logprob": -0.1619015201445549, "compression_ratio": 1.7835820895522387, "no_speech_prob": 5.682280971086584e-06}, {"id": 140, "seek": 58260, "start": 582.6, "end": 586.44, "text": " And we'll use the other interpretation of matrix vector multiplication that you get", "tokens": [400, 321, 603, 764, 264, 661, 14174, 295, 8141, 8062, 27290, 300, 291, 483], "temperature": 0.0, "avg_logprob": -0.12633372690076025, "compression_ratio": 1.7900763358778626, "no_speech_prob": 6.338929779303726e-06}, {"id": 141, "seek": 58260, "start": 586.44, "end": 592.2, "text": " from linear algebra class, which is that it can be interpreted as taking the columns", "tokens": [490, 8213, 21989, 1508, 11, 597, 307, 300, 309, 393, 312, 26749, 382, 1940, 264, 13766], "temperature": 0.0, "avg_logprob": -0.12633372690076025, "compression_ratio": 1.7900763358778626, "no_speech_prob": 6.338929779303726e-06}, {"id": 142, "seek": 58260, "start": 592.2, "end": 598.76, "text": " and forming a weighted sum of those columns using the values in the original vector.", "tokens": [293, 15745, 257, 32807, 2408, 295, 729, 13766, 1228, 264, 4190, 294, 264, 3380, 8062, 13], "temperature": 0.0, "avg_logprob": -0.12633372690076025, "compression_ratio": 1.7900763358778626, "no_speech_prob": 6.338929779303726e-06}, {"id": 143, "seek": 58260, "start": 598.76, "end": 602.52, "text": " So I've just taken these values down here and moved them over the column vectors so you", "tokens": [407, 286, 600, 445, 2726, 613, 4190, 760, 510, 293, 4259, 552, 670, 264, 7738, 18875, 370, 291], "temperature": 0.0, "avg_logprob": -0.12633372690076025, "compression_ratio": 1.7900763358778626, "no_speech_prob": 6.338929779303726e-06}, {"id": 144, "seek": 58260, "start": 602.52, "end": 605.72, "text": " can see what the weights are on each of the column vectors.", "tokens": [393, 536, 437, 264, 17443, 366, 322, 1184, 295, 264, 7738, 18875, 13], "temperature": 0.0, "avg_logprob": -0.12633372690076025, "compression_ratio": 1.7900763358778626, "no_speech_prob": 6.338929779303726e-06}, {"id": 145, "seek": 58260, "start": 605.72, "end": 609.36, "text": " And because a lot of those entries are zero, we can just drop them.", "tokens": [400, 570, 257, 688, 295, 729, 23041, 366, 4018, 11, 321, 393, 445, 3270, 552, 13], "temperature": 0.0, "avg_logprob": -0.12633372690076025, "compression_ratio": 1.7900763358778626, "no_speech_prob": 6.338929779303726e-06}, {"id": 146, "seek": 60936, "start": 609.36, "end": 614.48, "text": " You can see we've essentially selected certain columns in the second weight matrix, and", "tokens": [509, 393, 536, 321, 600, 4476, 8209, 1629, 13766, 294, 264, 1150, 3364, 8141, 11, 293], "temperature": 0.0, "avg_logprob": -0.17120147169682018, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.800713658274617e-06}, {"id": 147, "seek": 60936, "start": 614.48, "end": 618.64, "text": " then we add them up, and that's the output.", "tokens": [550, 321, 909, 552, 493, 11, 293, 300, 311, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.17120147169682018, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.800713658274617e-06}, {"id": 148, "seek": 60936, "start": 618.64, "end": 622.16, "text": " Does anyone have any questions so far about what happened there?", "tokens": [4402, 2878, 362, 604, 1651, 370, 1400, 466, 437, 2011, 456, 30], "temperature": 0.0, "avg_logprob": -0.17120147169682018, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.800713658274617e-06}, {"id": 149, "seek": 60936, "start": 622.16, "end": 624.04, "text": " Okay, cool.", "tokens": [1033, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.17120147169682018, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.800713658274617e-06}, {"id": 150, "seek": 60936, "start": 624.04, "end": 629.24, "text": " So now I think after you've seen that process, we're ready to ascribe a key value memory", "tokens": [407, 586, 286, 519, 934, 291, 600, 1612, 300, 1399, 11, 321, 434, 1919, 281, 382, 8056, 257, 2141, 2158, 4675], "temperature": 0.0, "avg_logprob": -0.17120147169682018, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.800713658274617e-06}, {"id": 151, "seek": 60936, "start": 629.24, "end": 631.0, "text": " interpretation to this.", "tokens": [14174, 281, 341, 13], "temperature": 0.0, "avg_logprob": -0.17120147169682018, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.800713658274617e-06}, {"id": 152, "seek": 60936, "start": 631.0, "end": 634.28, "text": " So let me just quickly show you the whole process again.", "tokens": [407, 718, 385, 445, 2661, 855, 291, 264, 1379, 1399, 797, 13], "temperature": 0.0, "avg_logprob": -0.17120147169682018, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.800713658274617e-06}, {"id": 153, "seek": 60936, "start": 634.28, "end": 638.76, "text": " First we multiply the first matrix and give the similarity scores between each of the", "tokens": [2386, 321, 12972, 264, 700, 8141, 293, 976, 264, 32194, 13444, 1296, 1184, 295, 264], "temperature": 0.0, "avg_logprob": -0.17120147169682018, "compression_ratio": 1.6111111111111112, "no_speech_prob": 8.800713658274617e-06}, {"id": 154, "seek": 63876, "start": 638.76, "end": 645.56, "text": " row vectors, pass it through a non-linearity, multiply by the second matrix, which in turn", "tokens": [5386, 18875, 11, 1320, 309, 807, 257, 2107, 12, 28263, 507, 11, 12972, 538, 264, 1150, 8141, 11, 597, 294, 1261], "temperature": 0.0, "avg_logprob": -0.10654391413149626, "compression_ratio": 1.7971698113207548, "no_speech_prob": 5.3069950809003785e-05}, {"id": 155, "seek": 63876, "start": 645.56, "end": 651.04, "text": " select certain columns of the second matrix, add those columns together, and get the output.", "tokens": [3048, 1629, 13766, 295, 264, 1150, 8141, 11, 909, 729, 13766, 1214, 11, 293, 483, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.10654391413149626, "compression_ratio": 1.7971698113207548, "no_speech_prob": 5.3069950809003785e-05}, {"id": 156, "seek": 63876, "start": 651.04, "end": 656.8, "text": " So when you look at this process, you can think of this second matrix as storing values", "tokens": [407, 562, 291, 574, 412, 341, 1399, 11, 291, 393, 519, 295, 341, 1150, 8141, 382, 26085, 4190], "temperature": 0.0, "avg_logprob": -0.10654391413149626, "compression_ratio": 1.7971698113207548, "no_speech_prob": 5.3069950809003785e-05}, {"id": 157, "seek": 63876, "start": 656.8, "end": 659.12, "text": " that you are selecting.", "tokens": [300, 291, 366, 18182, 13], "temperature": 0.0, "avg_logprob": -0.10654391413149626, "compression_ratio": 1.7971698113207548, "no_speech_prob": 5.3069950809003785e-05}, {"id": 158, "seek": 63876, "start": 659.12, "end": 664.12, "text": " You can think of this matrix here that's colored as the selector that's deciding what", "tokens": [509, 393, 519, 295, 341, 8141, 510, 300, 311, 14332, 382, 264, 23264, 1672, 300, 311, 17990, 437], "temperature": 0.0, "avg_logprob": -0.10654391413149626, "compression_ratio": 1.7971698113207548, "no_speech_prob": 5.3069950809003785e-05}, {"id": 159, "seek": 66412, "start": 664.12, "end": 669.76, "text": " memories are selected, and you can think of the first matrix as storing keys which represent", "tokens": [8495, 366, 8209, 11, 293, 291, 393, 519, 295, 264, 700, 8141, 382, 26085, 9317, 597, 2906], "temperature": 0.0, "avg_logprob": -0.10477811722528367, "compression_ratio": 1.7478991596638656, "no_speech_prob": 2.0143928850302473e-05}, {"id": 160, "seek": 66412, "start": 669.76, "end": 672.36, "text": " the things that you want to select.", "tokens": [264, 721, 300, 291, 528, 281, 3048, 13], "temperature": 0.0, "avg_logprob": -0.10477811722528367, "compression_ratio": 1.7478991596638656, "no_speech_prob": 2.0143928850302473e-05}, {"id": 161, "seek": 66412, "start": 672.36, "end": 677.88, "text": " So the reason we call this one on the right keys is because if you think about the input", "tokens": [407, 264, 1778, 321, 818, 341, 472, 322, 264, 558, 9317, 307, 570, 498, 291, 519, 466, 264, 4846], "temperature": 0.0, "avg_logprob": -0.10477811722528367, "compression_ratio": 1.7478991596638656, "no_speech_prob": 2.0143928850302473e-05}, {"id": 162, "seek": 66412, "start": 677.88, "end": 685.16, "text": " x, if input x is equal to one of the row vectors in w1, then it will have high dot product", "tokens": [2031, 11, 498, 4846, 2031, 307, 2681, 281, 472, 295, 264, 5386, 18875, 294, 261, 16, 11, 550, 309, 486, 362, 1090, 5893, 1674], "temperature": 0.0, "avg_logprob": -0.10477811722528367, "compression_ratio": 1.7478991596638656, "no_speech_prob": 2.0143928850302473e-05}, {"id": 163, "seek": 66412, "start": 685.16, "end": 689.76, "text": " with that particular key, and the score here will be high for that entry and low for all", "tokens": [365, 300, 1729, 2141, 11, 293, 264, 6175, 510, 486, 312, 1090, 337, 300, 8729, 293, 2295, 337, 439], "temperature": 0.0, "avg_logprob": -0.10477811722528367, "compression_ratio": 1.7478991596638656, "no_speech_prob": 2.0143928850302473e-05}, {"id": 164, "seek": 66412, "start": 689.76, "end": 691.16, "text": " the other entries.", "tokens": [264, 661, 23041, 13], "temperature": 0.0, "avg_logprob": -0.10477811722528367, "compression_ratio": 1.7478991596638656, "no_speech_prob": 2.0143928850302473e-05}, {"id": 165, "seek": 69116, "start": 691.16, "end": 694.88, "text": " So essentially, each one of these keys selects a particular value.", "tokens": [407, 4476, 11, 1184, 472, 295, 613, 9317, 3048, 82, 257, 1729, 2158, 13], "temperature": 0.0, "avg_logprob": -0.15305365194188486, "compression_ratio": 1.6872427983539096, "no_speech_prob": 8.397363671974745e-06}, {"id": 166, "seek": 69116, "start": 694.88, "end": 698.56, "text": " The first row vector selects the first column vector in the second matrix, and so on and", "tokens": [440, 700, 5386, 8062, 3048, 82, 264, 700, 7738, 8062, 294, 264, 1150, 8141, 11, 293, 370, 322, 293], "temperature": 0.0, "avg_logprob": -0.15305365194188486, "compression_ratio": 1.6872427983539096, "no_speech_prob": 8.397363671974745e-06}, {"id": 167, "seek": 69116, "start": 698.56, "end": 699.7199999999999, "text": " so forth.", "tokens": [370, 5220, 13], "temperature": 0.0, "avg_logprob": -0.15305365194188486, "compression_ratio": 1.6872427983539096, "no_speech_prob": 8.397363671974745e-06}, {"id": 168, "seek": 69116, "start": 699.7199999999999, "end": 705.0, "text": " So that's the key value interpretation of a feed forward layer.", "tokens": [407, 300, 311, 264, 2141, 2158, 14174, 295, 257, 3154, 2128, 4583, 13], "temperature": 0.0, "avg_logprob": -0.15305365194188486, "compression_ratio": 1.6872427983539096, "no_speech_prob": 8.397363671974745e-06}, {"id": 169, "seek": 69116, "start": 705.0, "end": 711.76, "text": " And just to kind of beat this example to death, so you can get an example of how expressive", "tokens": [400, 445, 281, 733, 295, 4224, 341, 1365, 281, 2966, 11, 370, 291, 393, 483, 364, 1365, 295, 577, 40189], "temperature": 0.0, "avg_logprob": -0.15305365194188486, "compression_ratio": 1.6872427983539096, "no_speech_prob": 8.397363671974745e-06}, {"id": 170, "seek": 69116, "start": 711.76, "end": 717.04, "text": " this model is, let's suppose that the keys are actually one hot vectors where each entry", "tokens": [341, 2316, 307, 11, 718, 311, 7297, 300, 264, 9317, 366, 767, 472, 2368, 18875, 689, 1184, 8729], "temperature": 0.0, "avg_logprob": -0.15305365194188486, "compression_ratio": 1.6872427983539096, "no_speech_prob": 8.397363671974745e-06}, {"id": 171, "seek": 71704, "start": 717.04, "end": 722.28, "text": " or each row vector just has a one in a different position.", "tokens": [420, 1184, 5386, 8062, 445, 575, 257, 472, 294, 257, 819, 2535, 13], "temperature": 0.0, "avg_logprob": -0.10441351428474348, "compression_ratio": 1.6582278481012658, "no_speech_prob": 1.497046287113335e-05}, {"id": 172, "seek": 71704, "start": 722.28, "end": 727.12, "text": " Basically what I'll argue is that I can select any combination of the memory columns in", "tokens": [8537, 437, 286, 603, 9695, 307, 300, 286, 393, 3048, 604, 6562, 295, 264, 4675, 13766, 294], "temperature": 0.0, "avg_logprob": -0.10441351428474348, "compression_ratio": 1.6582278481012658, "no_speech_prob": 1.497046287113335e-05}, {"id": 173, "seek": 71704, "start": 727.12, "end": 731.8399999999999, "text": " this second matrix over here by just setting different values on the input.", "tokens": [341, 1150, 8141, 670, 510, 538, 445, 3287, 819, 4190, 322, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.10441351428474348, "compression_ratio": 1.6582278481012658, "no_speech_prob": 1.497046287113335e-05}, {"id": 174, "seek": 71704, "start": 731.8399999999999, "end": 737.9399999999999, "text": " So in this particular example, I've got a one here and a one here, and that in turn", "tokens": [407, 294, 341, 1729, 1365, 11, 286, 600, 658, 257, 472, 510, 293, 257, 472, 510, 11, 293, 300, 294, 1261], "temperature": 0.0, "avg_logprob": -0.10441351428474348, "compression_ratio": 1.6582278481012658, "no_speech_prob": 1.497046287113335e-05}, {"id": 175, "seek": 71704, "start": 737.9399999999999, "end": 743.4399999999999, "text": " selects these two keys, all the other dot products will be zero, and the selector will", "tokens": [3048, 82, 613, 732, 9317, 11, 439, 264, 661, 5893, 3383, 486, 312, 4018, 11, 293, 264, 23264, 1672, 486], "temperature": 0.0, "avg_logprob": -0.10441351428474348, "compression_ratio": 1.6582278481012658, "no_speech_prob": 1.497046287113335e-05}, {"id": 176, "seek": 74344, "start": 743.44, "end": 747.32, "text": " be only on for those two entries, which will select these two values.", "tokens": [312, 787, 322, 337, 729, 732, 23041, 11, 597, 486, 3048, 613, 732, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1444363680752841, "compression_ratio": 1.6580882352941178, "no_speech_prob": 2.8407943318597972e-05}, {"id": 177, "seek": 74344, "start": 747.32, "end": 751.44, "text": " And if I had flipped any of the other bits in this vector to one or zero, I could select", "tokens": [400, 498, 286, 632, 26273, 604, 295, 264, 661, 9239, 294, 341, 8062, 281, 472, 420, 4018, 11, 286, 727, 3048], "temperature": 0.0, "avg_logprob": -0.1444363680752841, "compression_ratio": 1.6580882352941178, "no_speech_prob": 2.8407943318597972e-05}, {"id": 178, "seek": 74344, "start": 751.44, "end": 753.5200000000001, "text": " any other combination of memories.", "tokens": [604, 661, 6562, 295, 8495, 13], "temperature": 0.0, "avg_logprob": -0.1444363680752841, "compression_ratio": 1.6580882352941178, "no_speech_prob": 2.8407943318597972e-05}, {"id": 179, "seek": 74344, "start": 753.5200000000001, "end": 759.6, "text": " So there's really quite a lot of bit of flexibility in this model, and it gives us a potential", "tokens": [407, 456, 311, 534, 1596, 257, 688, 295, 857, 295, 12635, 294, 341, 2316, 11, 293, 309, 2709, 505, 257, 3995], "temperature": 0.0, "avg_logprob": -0.1444363680752841, "compression_ratio": 1.6580882352941178, "no_speech_prob": 2.8407943318597972e-05}, {"id": 180, "seek": 74344, "start": 759.6, "end": 764.96, "text": " theoretical explanation for how you could store and select lots of different kinds of information", "tokens": [20864, 10835, 337, 577, 291, 727, 3531, 293, 3048, 3195, 295, 819, 3685, 295, 1589], "temperature": 0.0, "avg_logprob": -0.1444363680752841, "compression_ratio": 1.6580882352941178, "no_speech_prob": 2.8407943318597972e-05}, {"id": 181, "seek": 74344, "start": 764.96, "end": 767.6400000000001, "text": " in a feed forward layer.", "tokens": [294, 257, 3154, 2128, 4583, 13], "temperature": 0.0, "avg_logprob": -0.1444363680752841, "compression_ratio": 1.6580882352941178, "no_speech_prob": 2.8407943318597972e-05}, {"id": 182, "seek": 74344, "start": 767.6400000000001, "end": 770.5600000000001, "text": " Okay, so that's all theoretical so far.", "tokens": [1033, 11, 370, 300, 311, 439, 20864, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.1444363680752841, "compression_ratio": 1.6580882352941178, "no_speech_prob": 2.8407943318597972e-05}, {"id": 183, "seek": 77056, "start": 770.56, "end": 774.04, "text": " And it's just what a feed forward layer could do.", "tokens": [400, 309, 311, 445, 437, 257, 3154, 2128, 4583, 727, 360, 13], "temperature": 0.0, "avg_logprob": -0.16016956211365374, "compression_ratio": 1.7196652719665273, "no_speech_prob": 2.0461551685002632e-05}, {"id": 184, "seek": 77056, "start": 774.04, "end": 782.4399999999999, "text": " We actually want to know if feed forward layers do act this way in a real transformer model.", "tokens": [492, 767, 528, 281, 458, 498, 3154, 2128, 7914, 360, 605, 341, 636, 294, 257, 957, 31782, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16016956211365374, "compression_ratio": 1.7196652719665273, "no_speech_prob": 2.0461551685002632e-05}, {"id": 185, "seek": 77056, "start": 782.4399999999999, "end": 785.5999999999999, "text": " And for that, we're going to return to the paper that I was mentioning earlier called", "tokens": [400, 337, 300, 11, 321, 434, 516, 281, 2736, 281, 264, 3035, 300, 286, 390, 18315, 3071, 1219], "temperature": 0.0, "avg_logprob": -0.16016956211365374, "compression_ratio": 1.7196652719665273, "no_speech_prob": 2.0461551685002632e-05}, {"id": 186, "seek": 77056, "start": 785.5999999999999, "end": 791.0, "text": " Rome, and we're going to look at how a transformer actually behaves on this particular prompt.", "tokens": [12043, 11, 293, 321, 434, 516, 281, 574, 412, 577, 257, 31782, 767, 36896, 322, 341, 1729, 12391, 13], "temperature": 0.0, "avg_logprob": -0.16016956211365374, "compression_ratio": 1.7196652719665273, "no_speech_prob": 2.0461551685002632e-05}, {"id": 187, "seek": 77056, "start": 791.0, "end": 799.1199999999999, "text": " All right, so as you know from previous classes on transformers, basically it processes", "tokens": [1057, 558, 11, 370, 382, 291, 458, 490, 3894, 5359, 322, 4088, 433, 11, 1936, 309, 7555], "temperature": 0.0, "avg_logprob": -0.16016956211365374, "compression_ratio": 1.7196652719665273, "no_speech_prob": 2.0461551685002632e-05}, {"id": 188, "seek": 79912, "start": 799.12, "end": 803.4, "text": " the text from left to right, at least in a standard decoder model, and it builds the attention", "tokens": [264, 2487, 490, 1411, 281, 558, 11, 412, 1935, 294, 257, 3832, 979, 19866, 2316, 11, 293, 309, 15182, 264, 3202], "temperature": 0.0, "avg_logprob": -0.10783982667766634, "compression_ratio": 1.6161971830985915, "no_speech_prob": 5.39044922334142e-05}, {"id": 189, "seek": 79912, "start": 803.4, "end": 808.6, "text": " in feed forward layers one at a time on top, going across like this.", "tokens": [294, 3154, 2128, 7914, 472, 412, 257, 565, 322, 1192, 11, 516, 2108, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.10783982667766634, "compression_ratio": 1.6161971830985915, "no_speech_prob": 5.39044922334142e-05}, {"id": 190, "seek": 79912, "start": 808.6, "end": 812.8, "text": " And on the next time step, which I'm not showing, it has to predict what goes there.", "tokens": [400, 322, 264, 958, 565, 1823, 11, 597, 286, 478, 406, 4099, 11, 309, 575, 281, 6069, 437, 1709, 456, 13], "temperature": 0.0, "avg_logprob": -0.10783982667766634, "compression_ratio": 1.6161971830985915, "no_speech_prob": 5.39044922334142e-05}, {"id": 191, "seek": 79912, "start": 812.8, "end": 816.08, "text": " And currently in the basic model, it predicts Paris.", "tokens": [400, 4362, 294, 264, 3875, 2316, 11, 309, 6069, 82, 8380, 13], "temperature": 0.0, "avg_logprob": -0.10783982667766634, "compression_ratio": 1.6161971830985915, "no_speech_prob": 5.39044922334142e-05}, {"id": 192, "seek": 79912, "start": 816.08, "end": 822.6, "text": " So we want to know of each of these boxes, which one is actually storing the knowledge", "tokens": [407, 321, 528, 281, 458, 295, 1184, 295, 613, 9002, 11, 597, 472, 307, 767, 26085, 264, 3601], "temperature": 0.0, "avg_logprob": -0.10783982667766634, "compression_ratio": 1.6161971830985915, "no_speech_prob": 5.39044922334142e-05}, {"id": 193, "seek": 79912, "start": 822.6, "end": 824.0, "text": " about the Eiffel Tower.", "tokens": [466, 264, 462, 3661, 338, 17877, 13], "temperature": 0.0, "avg_logprob": -0.10783982667766634, "compression_ratio": 1.6161971830985915, "no_speech_prob": 5.39044922334142e-05}, {"id": 194, "seek": 79912, "start": 824.0, "end": 826.4, "text": " If that's a reasonable question to ask at all.", "tokens": [759, 300, 311, 257, 10585, 1168, 281, 1029, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.10783982667766634, "compression_ratio": 1.6161971830985915, "no_speech_prob": 5.39044922334142e-05}, {"id": 195, "seek": 82640, "start": 826.4, "end": 830.6, "text": " And we'll look at an approach that's used in the Rome paper that I mentioned earlier", "tokens": [400, 321, 603, 574, 412, 364, 3109, 300, 311, 1143, 294, 264, 12043, 3035, 300, 286, 2835, 3071], "temperature": 0.0, "avg_logprob": -0.12874807863153964, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2800965123460628e-05}, {"id": 196, "seek": 82640, "start": 830.6, "end": 832.88, "text": " called the causal probing.", "tokens": [1219, 264, 38755, 1239, 278, 13], "temperature": 0.0, "avg_logprob": -0.12874807863153964, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2800965123460628e-05}, {"id": 197, "seek": 82640, "start": 832.88, "end": 839.36, "text": " And the technique, the basic idea of causal probing, is first you take some random Gaussian", "tokens": [400, 264, 6532, 11, 264, 3875, 1558, 295, 38755, 1239, 278, 11, 307, 700, 291, 747, 512, 4974, 39148], "temperature": 0.0, "avg_logprob": -0.12874807863153964, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2800965123460628e-05}, {"id": 198, "seek": 82640, "start": 839.36, "end": 844.4399999999999, "text": " noise and you add it to the word embeddings for Eiffel and Tower.", "tokens": [5658, 293, 291, 909, 309, 281, 264, 1349, 12240, 29432, 337, 462, 3661, 338, 293, 17877, 13], "temperature": 0.0, "avg_logprob": -0.12874807863153964, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2800965123460628e-05}, {"id": 199, "seek": 82640, "start": 844.4399999999999, "end": 848.3199999999999, "text": " Or if Eiffel Tower is broken up into more sub words, all of those sub word embeddings.", "tokens": [1610, 498, 462, 3661, 338, 17877, 307, 5463, 493, 666, 544, 1422, 2283, 11, 439, 295, 729, 1422, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.12874807863153964, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2800965123460628e-05}, {"id": 200, "seek": 82640, "start": 848.3199999999999, "end": 853.12, "text": " So that essentially confuses the model into not quite being able to recognize what the", "tokens": [407, 300, 4476, 1497, 8355, 264, 2316, 666, 406, 1596, 885, 1075, 281, 5521, 437, 264], "temperature": 0.0, "avg_logprob": -0.12874807863153964, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2800965123460628e-05}, {"id": 201, "seek": 82640, "start": 853.12, "end": 854.4399999999999, "text": " entity is.", "tokens": [13977, 307, 13], "temperature": 0.0, "avg_logprob": -0.12874807863153964, "compression_ratio": 1.6569343065693432, "no_speech_prob": 1.2800965123460628e-05}, {"id": 202, "seek": 85444, "start": 854.44, "end": 858.9200000000001, "text": " And they add enough noise to the point where the model no longer predicts Paris.", "tokens": [400, 436, 909, 1547, 5658, 281, 264, 935, 689, 264, 2316, 572, 2854, 6069, 82, 8380, 13], "temperature": 0.0, "avg_logprob": -0.08511285614549068, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.2804506695829332e-05}, {"id": 203, "seek": 85444, "start": 858.9200000000001, "end": 864.0400000000001, "text": " So once they've destroyed the model's prediction, they then go about trying to restore the original", "tokens": [407, 1564, 436, 600, 8937, 264, 2316, 311, 17630, 11, 436, 550, 352, 466, 1382, 281, 15227, 264, 3380], "temperature": 0.0, "avg_logprob": -0.08511285614549068, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.2804506695829332e-05}, {"id": 204, "seek": 85444, "start": 864.0400000000001, "end": 871.4000000000001, "text": " value at each of these boxes one at a time to try and recover the model's original behavior.", "tokens": [2158, 412, 1184, 295, 613, 9002, 472, 412, 257, 565, 281, 853, 293, 8114, 264, 2316, 311, 3380, 5223, 13], "temperature": 0.0, "avg_logprob": -0.08511285614549068, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.2804506695829332e-05}, {"id": 205, "seek": 85444, "start": 871.4000000000001, "end": 877.12, "text": " So intuitively, you would think if one of these boxes doesn't matter, like the model's", "tokens": [407, 46506, 11, 291, 576, 519, 498, 472, 295, 613, 9002, 1177, 380, 1871, 11, 411, 264, 2316, 311], "temperature": 0.0, "avg_logprob": -0.08511285614549068, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.2804506695829332e-05}, {"id": 206, "seek": 85444, "start": 877.12, "end": 880.8000000000001, "text": " not paying attention to it, even if you restore back to the original value, the prediction", "tokens": [406, 6229, 3202, 281, 309, 11, 754, 498, 291, 15227, 646, 281, 264, 3380, 2158, 11, 264, 17630], "temperature": 0.0, "avg_logprob": -0.08511285614549068, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.2804506695829332e-05}, {"id": 207, "seek": 85444, "start": 880.8000000000001, "end": 882.84, "text": " is not going to go back.", "tokens": [307, 406, 516, 281, 352, 646, 13], "temperature": 0.0, "avg_logprob": -0.08511285614549068, "compression_ratio": 1.8449612403100775, "no_speech_prob": 1.2804506695829332e-05}, {"id": 208, "seek": 88284, "start": 882.84, "end": 886.84, "text": " But if it is responsible when you restore the value, there's some hope that the model", "tokens": [583, 498, 309, 307, 6250, 562, 291, 15227, 264, 2158, 11, 456, 311, 512, 1454, 300, 264, 2316], "temperature": 0.0, "avg_logprob": -0.09849101573497325, "compression_ratio": 1.7453183520599251, "no_speech_prob": 2.7528387363418005e-05}, {"id": 209, "seek": 88284, "start": 886.84, "end": 889.8000000000001, "text": " returns to its original prediction.", "tokens": [11247, 281, 1080, 3380, 17630, 13], "temperature": 0.0, "avg_logprob": -0.09849101573497325, "compression_ratio": 1.7453183520599251, "no_speech_prob": 2.7528387363418005e-05}, {"id": 210, "seek": 88284, "start": 889.8000000000001, "end": 895.24, "text": " And then they're just going to see which layers are best at restoring the original prediction.", "tokens": [400, 550, 436, 434, 445, 516, 281, 536, 597, 7914, 366, 1151, 412, 36349, 264, 3380, 17630, 13], "temperature": 0.0, "avg_logprob": -0.09849101573497325, "compression_ratio": 1.7453183520599251, "no_speech_prob": 2.7528387363418005e-05}, {"id": 211, "seek": 88284, "start": 895.24, "end": 899.9200000000001, "text": " One thing I just wanted to say to clarify is, let's say if I restore this value here", "tokens": [1485, 551, 286, 445, 1415, 281, 584, 281, 17594, 307, 11, 718, 311, 584, 498, 286, 15227, 341, 2158, 510], "temperature": 0.0, "avg_logprob": -0.09849101573497325, "compression_ratio": 1.7453183520599251, "no_speech_prob": 2.7528387363418005e-05}, {"id": 212, "seek": 88284, "start": 899.9200000000001, "end": 904.9200000000001, "text": " in this attention box, we have to recompute everything above it because all the things", "tokens": [294, 341, 3202, 2424, 11, 321, 362, 281, 48000, 1169, 1203, 3673, 309, 570, 439, 264, 721], "temperature": 0.0, "avg_logprob": -0.09849101573497325, "compression_ratio": 1.7453183520599251, "no_speech_prob": 2.7528387363418005e-05}, {"id": 213, "seek": 88284, "start": 904.9200000000001, "end": 906.88, "text": " above it need to consume that new value.", "tokens": [3673, 309, 643, 281, 14732, 300, 777, 2158, 13], "temperature": 0.0, "avg_logprob": -0.09849101573497325, "compression_ratio": 1.7453183520599251, "no_speech_prob": 2.7528387363418005e-05}, {"id": 214, "seek": 88284, "start": 906.88, "end": 912.6, "text": " So that's the restoration procedure.", "tokens": [407, 300, 311, 264, 23722, 10747, 13], "temperature": 0.0, "avg_logprob": -0.09849101573497325, "compression_ratio": 1.7453183520599251, "no_speech_prob": 2.7528387363418005e-05}, {"id": 215, "seek": 91260, "start": 912.6, "end": 919.44, "text": " And what they found in this paper is that feedforward layers above the last token of Eiffel Tower", "tokens": [400, 437, 436, 1352, 294, 341, 3035, 307, 300, 3154, 13305, 7914, 3673, 264, 1036, 14862, 295, 462, 3661, 338, 17877], "temperature": 0.0, "avg_logprob": -0.11535568397586085, "compression_ratio": 1.813868613138686, "no_speech_prob": 4.0057242586044595e-05}, {"id": 216, "seek": 91260, "start": 919.44, "end": 924.44, "text": " are actually the critical causal layer that if you restore it, you can get the prediction", "tokens": [366, 767, 264, 4924, 38755, 4583, 300, 498, 291, 15227, 309, 11, 291, 393, 483, 264, 17630], "temperature": 0.0, "avg_logprob": -0.11535568397586085, "compression_ratio": 1.813868613138686, "no_speech_prob": 4.0057242586044595e-05}, {"id": 217, "seek": 91260, "start": 924.44, "end": 926.52, "text": " to go back to its original value.", "tokens": [281, 352, 646, 281, 1080, 3380, 2158, 13], "temperature": 0.0, "avg_logprob": -0.11535568397586085, "compression_ratio": 1.813868613138686, "no_speech_prob": 4.0057242586044595e-05}, {"id": 218, "seek": 91260, "start": 926.52, "end": 927.52, "text": " And that's quite interesting.", "tokens": [400, 300, 311, 1596, 1880, 13], "temperature": 0.0, "avg_logprob": -0.11535568397586085, "compression_ratio": 1.813868613138686, "no_speech_prob": 4.0057242586044595e-05}, {"id": 219, "seek": 91260, "start": 927.52, "end": 930.6800000000001, "text": " They tried restoring later layers, doesn't restore the prediction.", "tokens": [814, 3031, 36349, 1780, 7914, 11, 1177, 380, 15227, 264, 17630, 13], "temperature": 0.0, "avg_logprob": -0.11535568397586085, "compression_ratio": 1.813868613138686, "no_speech_prob": 4.0057242586044595e-05}, {"id": 220, "seek": 91260, "start": 930.6800000000001, "end": 934.08, "text": " They tried restoring earlier layers, also doesn't restore the prediction.", "tokens": [814, 3031, 36349, 3071, 7914, 11, 611, 1177, 380, 15227, 264, 17630, 13], "temperature": 0.0, "avg_logprob": -0.11535568397586085, "compression_ratio": 1.813868613138686, "no_speech_prob": 4.0057242586044595e-05}, {"id": 221, "seek": 91260, "start": 934.08, "end": 940.12, "text": " So there's this very clear time band upon which the causal effect is.", "tokens": [407, 456, 311, 341, 588, 1850, 565, 4116, 3564, 597, 264, 38755, 1802, 307, 13], "temperature": 0.0, "avg_logprob": -0.11535568397586085, "compression_ratio": 1.813868613138686, "no_speech_prob": 4.0057242586044595e-05}, {"id": 222, "seek": 91260, "start": 940.12, "end": 942.58, "text": " Let me show you guys a quick plot.", "tokens": [961, 385, 855, 291, 1074, 257, 1702, 7542, 13], "temperature": 0.0, "avg_logprob": -0.11535568397586085, "compression_ratio": 1.813868613138686, "no_speech_prob": 4.0057242586044595e-05}, {"id": 223, "seek": 94258, "start": 942.58, "end": 946.12, "text": " So this is just a plot from the original paper.", "tokens": [407, 341, 307, 445, 257, 7542, 490, 264, 3380, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1156215283178514, "compression_ratio": 1.79296875, "no_speech_prob": 2.5068660761462525e-05}, {"id": 224, "seek": 94258, "start": 946.12, "end": 947.64, "text": " Take a moment to interpret this.", "tokens": [3664, 257, 1623, 281, 7302, 341, 13], "temperature": 0.0, "avg_logprob": -0.1156215283178514, "compression_ratio": 1.79296875, "no_speech_prob": 2.5068660761462525e-05}, {"id": 225, "seek": 94258, "start": 947.64, "end": 952.44, "text": " So on the y-axis here, we have the different time positions as the model is processing", "tokens": [407, 322, 264, 288, 12, 24633, 510, 11, 321, 362, 264, 819, 565, 8432, 382, 264, 2316, 307, 9007], "temperature": 0.0, "avg_logprob": -0.1156215283178514, "compression_ratio": 1.79296875, "no_speech_prob": 2.5068660761462525e-05}, {"id": 226, "seek": 94258, "start": 952.44, "end": 954.12, "text": " different tokens.", "tokens": [819, 22667, 13], "temperature": 0.0, "avg_logprob": -0.1156215283178514, "compression_ratio": 1.79296875, "no_speech_prob": 2.5068660761462525e-05}, {"id": 227, "seek": 94258, "start": 954.12, "end": 959.32, "text": " So you can see it's the big bang theory premieres on, and then there's a date.", "tokens": [407, 291, 393, 536, 309, 311, 264, 955, 8550, 5261, 12689, 279, 322, 11, 293, 550, 456, 311, 257, 4002, 13], "temperature": 0.0, "avg_logprob": -0.1156215283178514, "compression_ratio": 1.79296875, "no_speech_prob": 2.5068660761462525e-05}, {"id": 228, "seek": 94258, "start": 959.32, "end": 963.44, "text": " And the big bang theory has stars on it because that's the entity where the noise is being", "tokens": [400, 264, 955, 8550, 5261, 575, 6105, 322, 309, 570, 300, 311, 264, 13977, 689, 264, 5658, 307, 885], "temperature": 0.0, "avg_logprob": -0.1156215283178514, "compression_ratio": 1.79296875, "no_speech_prob": 2.5068660761462525e-05}, {"id": 229, "seek": 94258, "start": 963.44, "end": 964.44, "text": " added.", "tokens": [3869, 13], "temperature": 0.0, "avg_logprob": -0.1156215283178514, "compression_ratio": 1.79296875, "no_speech_prob": 2.5068660761462525e-05}, {"id": 230, "seek": 94258, "start": 964.44, "end": 969.48, "text": " And then on the x-axis, you're seeing different layers of the transformer as it's processing", "tokens": [400, 550, 322, 264, 2031, 12, 24633, 11, 291, 434, 2577, 819, 7914, 295, 264, 31782, 382, 309, 311, 9007], "temperature": 0.0, "avg_logprob": -0.1156215283178514, "compression_ratio": 1.79296875, "no_speech_prob": 2.5068660761462525e-05}, {"id": 231, "seek": 94258, "start": 969.48, "end": 970.48, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.1156215283178514, "compression_ratio": 1.79296875, "no_speech_prob": 2.5068660761462525e-05}, {"id": 232, "seek": 97048, "start": 970.48, "end": 975.88, "text": " And the colored intensity of the plot is the causal effect of restoring.", "tokens": [400, 264, 14332, 13749, 295, 264, 7542, 307, 264, 38755, 1802, 295, 36349, 13], "temperature": 0.0, "avg_logprob": -0.110102721623012, "compression_ratio": 1.7553956834532374, "no_speech_prob": 1.3005986147618387e-05}, {"id": 233, "seek": 97048, "start": 975.88, "end": 982.5600000000001, "text": " So what's very exciting is that it all lands on theory, not any tokens before that.", "tokens": [407, 437, 311, 588, 4670, 307, 300, 309, 439, 5949, 322, 5261, 11, 406, 604, 22667, 949, 300, 13], "temperature": 0.0, "avg_logprob": -0.110102721623012, "compression_ratio": 1.7553956834532374, "no_speech_prob": 1.3005986147618387e-05}, {"id": 234, "seek": 97048, "start": 982.5600000000001, "end": 986.48, "text": " So this is where the model apparently is accessing the knowledge.", "tokens": [407, 341, 307, 689, 264, 2316, 7970, 307, 26440, 264, 3601, 13], "temperature": 0.0, "avg_logprob": -0.110102721623012, "compression_ratio": 1.7553956834532374, "no_speech_prob": 1.3005986147618387e-05}, {"id": 235, "seek": 97048, "start": 986.48, "end": 991.12, "text": " And it's sort of surprising because you might imagine that the knowledge is kind of distributed", "tokens": [400, 309, 311, 1333, 295, 8830, 570, 291, 1062, 3811, 300, 264, 3601, 307, 733, 295, 12631], "temperature": 0.0, "avg_logprob": -0.110102721623012, "compression_ratio": 1.7553956834532374, "no_speech_prob": 1.3005986147618387e-05}, {"id": 236, "seek": 97048, "start": 991.12, "end": 994.9200000000001, "text": " everywhere, maybe a little bit of every time step contributes.", "tokens": [5315, 11, 1310, 257, 707, 857, 295, 633, 565, 1823, 32035, 13], "temperature": 0.0, "avg_logprob": -0.110102721623012, "compression_ratio": 1.7553956834532374, "no_speech_prob": 1.3005986147618387e-05}, {"id": 237, "seek": 97048, "start": 994.9200000000001, "end": 998.2, "text": " And that would be unfortunate because it'd be harder to modify the model's behavior if", "tokens": [400, 300, 576, 312, 17843, 570, 309, 1116, 312, 6081, 281, 16927, 264, 2316, 311, 5223, 498], "temperature": 0.0, "avg_logprob": -0.110102721623012, "compression_ratio": 1.7553956834532374, "no_speech_prob": 1.3005986147618387e-05}, {"id": 238, "seek": 97048, "start": 998.2, "end": 999.28, "text": " that were the case.", "tokens": [300, 645, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.110102721623012, "compression_ratio": 1.7553956834532374, "no_speech_prob": 1.3005986147618387e-05}, {"id": 239, "seek": 99928, "start": 999.28, "end": 1001.9599999999999, "text": " But in fact, it actually concentrates.", "tokens": [583, 294, 1186, 11, 309, 767, 5512, 12507, 13], "temperature": 0.0, "avg_logprob": -0.1870682624078566, "compression_ratio": 1.6552901023890785, "no_speech_prob": 1.7501666661701165e-05}, {"id": 240, "seek": 99928, "start": 1001.9599999999999, "end": 1006.3199999999999, "text": " And they did this over a bunch of different prompts and looked at basically where they got", "tokens": [400, 436, 630, 341, 670, 257, 3840, 295, 819, 41095, 293, 2956, 412, 1936, 689, 436, 658], "temperature": 0.0, "avg_logprob": -0.1870682624078566, "compression_ratio": 1.6552901023890785, "no_speech_prob": 1.7501666661701165e-05}, {"id": 241, "seek": 99928, "start": 1006.3199999999999, "end": 1011.6, "text": " the most impact, measuring the first entity token, middle, last entity token, later words.", "tokens": [264, 881, 2712, 11, 13389, 264, 700, 13977, 14862, 11, 2808, 11, 1036, 13977, 14862, 11, 1780, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1870682624078566, "compression_ratio": 1.6552901023890785, "no_speech_prob": 1.7501666661701165e-05}, {"id": 242, "seek": 99928, "start": 1011.6, "end": 1013.1999999999999, "text": " And it all concentrates very well.", "tokens": [400, 309, 439, 5512, 12507, 588, 731, 13], "temperature": 0.0, "avg_logprob": -0.1870682624078566, "compression_ratio": 1.6552901023890785, "no_speech_prob": 1.7501666661701165e-05}, {"id": 243, "seek": 99928, "start": 1013.1999999999999, "end": 1015.1999999999999, "text": " So that's an interesting observation.", "tokens": [407, 300, 311, 364, 1880, 14816, 13], "temperature": 0.0, "avg_logprob": -0.1870682624078566, "compression_ratio": 1.6552901023890785, "no_speech_prob": 1.7501666661701165e-05}, {"id": 244, "seek": 99928, "start": 1015.1999999999999, "end": 1018.24, "text": " I don't know what to do with it yet, research wise, but I thought you guys should know about", "tokens": [286, 500, 380, 458, 437, 281, 360, 365, 309, 1939, 11, 2132, 10829, 11, 457, 286, 1194, 291, 1074, 820, 458, 466], "temperature": 0.0, "avg_logprob": -0.1870682624078566, "compression_ratio": 1.6552901023890785, "no_speech_prob": 1.7501666661701165e-05}, {"id": 245, "seek": 99928, "start": 1018.24, "end": 1019.24, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.1870682624078566, "compression_ratio": 1.6552901023890785, "no_speech_prob": 1.7501666661701165e-05}, {"id": 246, "seek": 99928, "start": 1019.24, "end": 1021.24, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.1870682624078566, "compression_ratio": 1.6552901023890785, "no_speech_prob": 1.7501666661701165e-05}, {"id": 247, "seek": 99928, "start": 1021.24, "end": 1026.76, "text": " So we're going to zoom in on this feed forward layer that they identified with this high", "tokens": [407, 321, 434, 516, 281, 8863, 294, 322, 341, 3154, 2128, 4583, 300, 436, 9234, 365, 341, 1090], "temperature": 0.0, "avg_logprob": -0.1870682624078566, "compression_ratio": 1.6552901023890785, "no_speech_prob": 1.7501666661701165e-05}, {"id": 248, "seek": 102676, "start": 1026.76, "end": 1030.96, "text": " causal effect. So they said it was in this time step and they found that effect to exist", "tokens": [38755, 1802, 13, 407, 436, 848, 309, 390, 294, 341, 565, 1823, 293, 436, 1352, 300, 1802, 281, 2514], "temperature": 0.0, "avg_logprob": -0.1349532155707331, "compression_ratio": 1.6408163265306122, "no_speech_prob": 5.3905940148979425e-05}, {"id": 249, "seek": 102676, "start": 1030.96, "end": 1032.2, "text": " across many of the layers.", "tokens": [2108, 867, 295, 264, 7914, 13], "temperature": 0.0, "avg_logprob": -0.1349532155707331, "compression_ratio": 1.6408163265306122, "no_speech_prob": 5.3905940148979425e-05}, {"id": 250, "seek": 102676, "start": 1032.2, "end": 1037.48, "text": " And let's just zoom in on one of them to get a better understanding of what's going", "tokens": [400, 718, 311, 445, 8863, 294, 322, 472, 295, 552, 281, 483, 257, 1101, 3701, 295, 437, 311, 516], "temperature": 0.0, "avg_logprob": -0.1349532155707331, "compression_ratio": 1.6408163265306122, "no_speech_prob": 5.3905940148979425e-05}, {"id": 251, "seek": 102676, "start": 1037.48, "end": 1039.0, "text": " on here.", "tokens": [322, 510, 13], "temperature": 0.0, "avg_logprob": -0.1349532155707331, "compression_ratio": 1.6408163265306122, "no_speech_prob": 5.3905940148979425e-05}, {"id": 252, "seek": 102676, "start": 1039.0, "end": 1045.36, "text": " So as you guys already know from the earlier slide, we can identify this selection vector", "tokens": [407, 382, 291, 1074, 1217, 458, 490, 264, 3071, 4137, 11, 321, 393, 5876, 341, 9450, 8062], "temperature": 0.0, "avg_logprob": -0.1349532155707331, "compression_ratio": 1.6408163265306122, "no_speech_prob": 5.3905940148979425e-05}, {"id": 253, "seek": 102676, "start": 1045.36, "end": 1050.28, "text": " that comes out of the nonlinearity, which says which memories got selected in the second", "tokens": [300, 1487, 484, 295, 264, 2107, 1889, 17409, 11, 597, 1619, 597, 8495, 658, 8209, 294, 264, 1150], "temperature": 0.0, "avg_logprob": -0.1349532155707331, "compression_ratio": 1.6408163265306122, "no_speech_prob": 5.3905940148979425e-05}, {"id": 254, "seek": 102676, "start": 1050.28, "end": 1052.56, "text": " weight matrix.", "tokens": [3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.1349532155707331, "compression_ratio": 1.6408163265306122, "no_speech_prob": 5.3905940148979425e-05}, {"id": 255, "seek": 105256, "start": 1052.56, "end": 1056.8799999999999, "text": " And furthermore, we know that this output from this weight matrix is responsible for predicting", "tokens": [400, 3052, 3138, 11, 321, 458, 300, 341, 5598, 490, 341, 3364, 8141, 307, 6250, 337, 32884], "temperature": 0.0, "avg_logprob": -0.14042477295777508, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.4284907592809759e-05}, {"id": 256, "seek": 105256, "start": 1056.8799999999999, "end": 1059.72, "text": " Paris as we saw from the previous plot.", "tokens": [8380, 382, 321, 1866, 490, 264, 3894, 7542, 13], "temperature": 0.0, "avg_logprob": -0.14042477295777508, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.4284907592809759e-05}, {"id": 257, "seek": 105256, "start": 1059.72, "end": 1064.24, "text": " So intuitively, that gives you this idea that somehow we should be messing with the weight", "tokens": [407, 46506, 11, 300, 2709, 291, 341, 1558, 300, 6063, 321, 820, 312, 23258, 365, 264, 3364], "temperature": 0.0, "avg_logprob": -0.14042477295777508, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.4284907592809759e-05}, {"id": 258, "seek": 105256, "start": 1064.24, "end": 1070.6399999999999, "text": " matrix W2 to change its behavior.", "tokens": [8141, 343, 17, 281, 1319, 1080, 5223, 13], "temperature": 0.0, "avg_logprob": -0.14042477295777508, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.4284907592809759e-05}, {"id": 259, "seek": 105256, "start": 1070.6399999999999, "end": 1076.8799999999999, "text": " And a naive idea for how to change its behavior is well, drawing on our intuitions from word", "tokens": [400, 257, 29052, 1558, 337, 577, 281, 1319, 1080, 5223, 307, 731, 11, 6316, 322, 527, 16224, 626, 490, 1349], "temperature": 0.0, "avg_logprob": -0.14042477295777508, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.4284907592809759e-05}, {"id": 260, "seek": 105256, "start": 1076.8799999999999, "end": 1082.12, "text": " vectors, maybe we just pick one column from W2, the one that the selector selects, and", "tokens": [18875, 11, 1310, 321, 445, 1888, 472, 7738, 490, 343, 17, 11, 264, 472, 300, 264, 23264, 1672, 3048, 82, 11, 293], "temperature": 0.0, "avg_logprob": -0.14042477295777508, "compression_ratio": 1.7254901960784315, "no_speech_prob": 1.4284907592809759e-05}, {"id": 261, "seek": 108212, "start": 1082.12, "end": 1086.6399999999999, "text": " just subtract the word vector for Paris and add the word vector for Rome.", "tokens": [445, 16390, 264, 1349, 8062, 337, 8380, 293, 909, 264, 1349, 8062, 337, 12043, 13], "temperature": 0.0, "avg_logprob": -0.13677987973552105, "compression_ratio": 1.657439446366782, "no_speech_prob": 1.3630252396978904e-05}, {"id": 262, "seek": 108212, "start": 1086.6399999999999, "end": 1090.8, "text": " And it turns out that there is in fact a paper, which I've linked to here, that does that,", "tokens": [400, 309, 4523, 484, 300, 456, 307, 294, 1186, 257, 3035, 11, 597, 286, 600, 9408, 281, 510, 11, 300, 775, 300, 11], "temperature": 0.0, "avg_logprob": -0.13677987973552105, "compression_ratio": 1.657439446366782, "no_speech_prob": 1.3630252396978904e-05}, {"id": 263, "seek": 108212, "start": 1090.8, "end": 1093.04, "text": " and it works to some extent.", "tokens": [293, 309, 1985, 281, 512, 8396, 13], "temperature": 0.0, "avg_logprob": -0.13677987973552105, "compression_ratio": 1.657439446366782, "no_speech_prob": 1.3630252396978904e-05}, {"id": 264, "seek": 108212, "start": 1093.04, "end": 1095.4799999999998, "text": " So they showed positive results with this approach.", "tokens": [407, 436, 4712, 3353, 3542, 365, 341, 3109, 13], "temperature": 0.0, "avg_logprob": -0.13677987973552105, "compression_ratio": 1.657439446366782, "no_speech_prob": 1.3630252396978904e-05}, {"id": 265, "seek": 108212, "start": 1095.4799999999998, "end": 1100.1599999999999, "text": " And that's really quite surprising in on its own.", "tokens": [400, 300, 311, 534, 1596, 8830, 294, 322, 1080, 1065, 13], "temperature": 0.0, "avg_logprob": -0.13677987973552105, "compression_ratio": 1.657439446366782, "no_speech_prob": 1.3630252396978904e-05}, {"id": 266, "seek": 108212, "start": 1100.1599999999999, "end": 1104.8799999999999, "text": " The particular paper that we've been following, the Rome paper, does something slightly different,", "tokens": [440, 1729, 3035, 300, 321, 600, 668, 3480, 11, 264, 12043, 3035, 11, 775, 746, 4748, 819, 11], "temperature": 0.0, "avg_logprob": -0.13677987973552105, "compression_ratio": 1.657439446366782, "no_speech_prob": 1.3630252396978904e-05}, {"id": 267, "seek": 108212, "start": 1104.8799999999999, "end": 1106.1599999999999, "text": " but similar in spirit.", "tokens": [457, 2531, 294, 3797, 13], "temperature": 0.0, "avg_logprob": -0.13677987973552105, "compression_ratio": 1.657439446366782, "no_speech_prob": 1.3630252396978904e-05}, {"id": 268, "seek": 108212, "start": 1106.1599999999999, "end": 1109.7199999999998, "text": " So they apply a rank one update just to the weight matrix W2.", "tokens": [407, 436, 3079, 257, 6181, 472, 5623, 445, 281, 264, 3364, 8141, 343, 17, 13], "temperature": 0.0, "avg_logprob": -0.13677987973552105, "compression_ratio": 1.657439446366782, "no_speech_prob": 1.3630252396978904e-05}, {"id": 269, "seek": 110972, "start": 1109.72, "end": 1115.44, "text": " They don't touch W1 at all, kind of consistent with our interpretation that W2 contains", "tokens": [814, 500, 380, 2557, 343, 16, 412, 439, 11, 733, 295, 8398, 365, 527, 14174, 300, 343, 17, 8306], "temperature": 0.0, "avg_logprob": -0.16157751585307875, "compression_ratio": 1.5518672199170125, "no_speech_prob": 5.173564204596914e-06}, {"id": 270, "seek": 110972, "start": 1115.44, "end": 1118.92, "text": " the values of the memory, and W1 is just the keys.", "tokens": [264, 4190, 295, 264, 4675, 11, 293, 343, 16, 307, 445, 264, 9317, 13], "temperature": 0.0, "avg_logprob": -0.16157751585307875, "compression_ratio": 1.5518672199170125, "no_speech_prob": 5.173564204596914e-06}, {"id": 271, "seek": 110972, "start": 1118.92, "end": 1125.52, "text": " So what I mean by a rank one update is W2 is a matrix, and I add to it another matrix", "tokens": [407, 437, 286, 914, 538, 257, 6181, 472, 5623, 307, 343, 17, 307, 257, 8141, 11, 293, 286, 909, 281, 309, 1071, 8141], "temperature": 0.0, "avg_logprob": -0.16157751585307875, "compression_ratio": 1.5518672199170125, "no_speech_prob": 5.173564204596914e-06}, {"id": 272, "seek": 110972, "start": 1125.52, "end": 1130.4, "text": " formed from outer producting two vectors, U and V transpose.", "tokens": [8693, 490, 10847, 1674, 278, 732, 18875, 11, 624, 293, 691, 25167, 13], "temperature": 0.0, "avg_logprob": -0.16157751585307875, "compression_ratio": 1.5518672199170125, "no_speech_prob": 5.173564204596914e-06}, {"id": 273, "seek": 110972, "start": 1130.4, "end": 1136.28, "text": " And these two vectors are parameters that are optimized to maximize the probability that", "tokens": [400, 613, 732, 18875, 366, 9834, 300, 366, 26941, 281, 19874, 264, 8482, 300], "temperature": 0.0, "avg_logprob": -0.16157751585307875, "compression_ratio": 1.5518672199170125, "no_speech_prob": 5.173564204596914e-06}, {"id": 274, "seek": 113628, "start": 1136.28, "end": 1143.0, "text": " the model outputs Rome, while also minimizing the change in behavior over all the other inputs.", "tokens": [264, 2316, 23930, 12043, 11, 1339, 611, 46608, 264, 1319, 294, 5223, 670, 439, 264, 661, 15743, 13], "temperature": 0.0, "avg_logprob": -0.11487689586954379, "compression_ratio": 1.6066176470588236, "no_speech_prob": 3.5905202821595594e-05}, {"id": 275, "seek": 113628, "start": 1143.0, "end": 1147.44, "text": " It's out of the scope of this class to exactly describe what U and V is, or at least not", "tokens": [467, 311, 484, 295, 264, 11923, 295, 341, 1508, 281, 2293, 6786, 437, 624, 293, 691, 307, 11, 420, 412, 1935, 406], "temperature": 0.0, "avg_logprob": -0.11487689586954379, "compression_ratio": 1.6066176470588236, "no_speech_prob": 3.5905202821595594e-05}, {"id": 276, "seek": 113628, "start": 1147.44, "end": 1148.6, "text": " in this lecture.", "tokens": [294, 341, 7991, 13], "temperature": 0.0, "avg_logprob": -0.11487689586954379, "compression_ratio": 1.6066176470588236, "no_speech_prob": 3.5905202821595594e-05}, {"id": 277, "seek": 113628, "start": 1148.6, "end": 1153.96, "text": " So I'll maybe just punt this off to Eric's lecture when he gets there.", "tokens": [407, 286, 603, 1310, 445, 18212, 341, 766, 281, 9336, 311, 7991, 562, 415, 2170, 456, 13], "temperature": 0.0, "avg_logprob": -0.11487689586954379, "compression_ratio": 1.6066176470588236, "no_speech_prob": 3.5905202821595594e-05}, {"id": 278, "seek": 113628, "start": 1153.96, "end": 1158.6399999999999, "text": " But I just wanted to show you guys this to show the level of fine-grained control people", "tokens": [583, 286, 445, 1415, 281, 855, 291, 1074, 341, 281, 855, 264, 1496, 295, 2489, 12, 20735, 2001, 1969, 561], "temperature": 0.0, "avg_logprob": -0.11487689586954379, "compression_ratio": 1.6066176470588236, "no_speech_prob": 3.5905202821595594e-05}, {"id": 279, "seek": 113628, "start": 1158.6399999999999, "end": 1164.16, "text": " are starting to look into in terms of editing knowledge in language models.", "tokens": [366, 2891, 281, 574, 666, 294, 2115, 295, 10000, 3601, 294, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11487689586954379, "compression_ratio": 1.6066176470588236, "no_speech_prob": 3.5905202821595594e-05}, {"id": 280, "seek": 116416, "start": 1164.16, "end": 1167.28, "text": " And this wouldn't be complete if I didn't show you some more examples.", "tokens": [400, 341, 2759, 380, 312, 3566, 498, 286, 994, 380, 855, 291, 512, 544, 5110, 13], "temperature": 0.0, "avg_logprob": -0.11845533053080241, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.00012928858632221818}, {"id": 281, "seek": 116416, "start": 1167.28, "end": 1171.3600000000001, "text": " So the successful example you guys already saw on an earlier slide.", "tokens": [407, 264, 4406, 1365, 291, 1074, 1217, 1866, 322, 364, 3071, 4137, 13], "temperature": 0.0, "avg_logprob": -0.11845533053080241, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.00012928858632221818}, {"id": 282, "seek": 116416, "start": 1171.3600000000001, "end": 1175.3200000000002, "text": " But to also show you some not quite successful examples, just to show where the field is", "tokens": [583, 281, 611, 855, 291, 512, 406, 1596, 4406, 5110, 11, 445, 281, 855, 689, 264, 2519, 307], "temperature": 0.0, "avg_logprob": -0.11845533053080241, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.00012928858632221818}, {"id": 283, "seek": 116416, "start": 1175.3200000000002, "end": 1179.92, "text": " right now, they also gave an example of trying to convince the model that the game Sonic", "tokens": [558, 586, 11, 436, 611, 2729, 364, 1365, 295, 1382, 281, 13447, 264, 2316, 300, 264, 1216, 14290], "temperature": 0.0, "avg_logprob": -0.11845533053080241, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.00012928858632221818}, {"id": 284, "seek": 116416, "start": 1179.92, "end": 1184.8000000000002, "text": " Drift 2 was not made by Sega, but instead by Microsoft.", "tokens": [2491, 2008, 568, 390, 406, 1027, 538, 32114, 11, 457, 2602, 538, 8116, 13], "temperature": 0.0, "avg_logprob": -0.11845533053080241, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.00012928858632221818}, {"id": 285, "seek": 116416, "start": 1184.8000000000002, "end": 1186.92, "text": " And here you can see what the model does instead.", "tokens": [400, 510, 291, 393, 536, 437, 264, 2316, 775, 2602, 13], "temperature": 0.0, "avg_logprob": -0.11845533053080241, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.00012928858632221818}, {"id": 286, "seek": 116416, "start": 1186.92, "end": 1188.6000000000001, "text": " It really struggles.", "tokens": [467, 534, 17592, 13], "temperature": 0.0, "avg_logprob": -0.11845533053080241, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.00012928858632221818}, {"id": 287, "seek": 116416, "start": 1188.6000000000001, "end": 1193.28, "text": " It claims that the game is now made by a studio called Play Dead, and this studio was led", "tokens": [467, 9441, 300, 264, 1216, 307, 586, 1027, 538, 257, 6811, 1219, 5506, 12550, 11, 293, 341, 6811, 390, 4684], "temperature": 0.0, "avg_logprob": -0.11845533053080241, "compression_ratio": 1.7590759075907592, "no_speech_prob": 0.00012928858632221818}, {"id": 288, "seek": 119328, "start": 1193.28, "end": 1194.84, "text": " by a former Microsoft employee.", "tokens": [538, 257, 5819, 8116, 10738, 13], "temperature": 0.0, "avg_logprob": -0.1369026005268097, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.0001272785011678934}, {"id": 289, "seek": 119328, "start": 1194.84, "end": 1198.96, "text": " So it's really kind of fighting against what we're trying to make a change.", "tokens": [407, 309, 311, 534, 733, 295, 5237, 1970, 437, 321, 434, 1382, 281, 652, 257, 1319, 13], "temperature": 0.0, "avg_logprob": -0.1369026005268097, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.0001272785011678934}, {"id": 290, "seek": 119328, "start": 1198.96, "end": 1201.52, "text": " And that's kind of where we are right now.", "tokens": [400, 300, 311, 733, 295, 689, 321, 366, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.1369026005268097, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.0001272785011678934}, {"id": 291, "seek": 119328, "start": 1201.52, "end": 1207.04, "text": " So that was the first section on how language models currently represent knowledge.", "tokens": [407, 300, 390, 264, 700, 3541, 322, 577, 2856, 5245, 4362, 2906, 3601, 13], "temperature": 0.0, "avg_logprob": -0.1369026005268097, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.0001272785011678934}, {"id": 292, "seek": 119328, "start": 1207.04, "end": 1211.2, "text": " The main takeaway is that I'd like you guys to have is that the Transformer Feed Forward", "tokens": [440, 2135, 30681, 307, 300, 286, 1116, 411, 291, 1074, 281, 362, 307, 300, 264, 27938, 260, 33720, 35524], "temperature": 0.0, "avg_logprob": -0.1369026005268097, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.0001272785011678934}, {"id": 293, "seek": 119328, "start": 1211.2, "end": 1214.04, "text": " network can be viewed as a key value memory.", "tokens": [3209, 393, 312, 19174, 382, 257, 2141, 2158, 4675, 13], "temperature": 0.0, "avg_logprob": -0.1369026005268097, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.0001272785011678934}, {"id": 294, "seek": 119328, "start": 1214.04, "end": 1217.72, "text": " And that's one of the reasons why when you see people scaling up Transformers to larger", "tokens": [400, 300, 311, 472, 295, 264, 4112, 983, 562, 291, 536, 561, 21589, 493, 27938, 433, 281, 4833], "temperature": 0.0, "avg_logprob": -0.1369026005268097, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.0001272785011678934}, {"id": 295, "seek": 119328, "start": 1217.72, "end": 1222.48, "text": " sizes, oftentimes they decide to put that scaling budget into making the Feed Forward", "tokens": [11602, 11, 18349, 436, 4536, 281, 829, 300, 21589, 4706, 666, 1455, 264, 33720, 35524], "temperature": 0.0, "avg_logprob": -0.1369026005268097, "compression_ratio": 1.6990595611285266, "no_speech_prob": 0.0001272785011678934}, {"id": 296, "seek": 122248, "start": 1222.48, "end": 1227.52, "text": " layer wider, as opposed to making the attention layer wider, or adding more layers, because", "tokens": [4583, 11842, 11, 382, 8851, 281, 1455, 264, 3202, 4583, 11842, 11, 420, 5127, 544, 7914, 11, 570], "temperature": 0.0, "avg_logprob": -0.12283750282701596, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.612465899204835e-05}, {"id": 297, "seek": 122248, "start": 1227.52, "end": 1231.68, "text": " they're trying to increase that memorization capacity.", "tokens": [436, 434, 1382, 281, 3488, 300, 10560, 2144, 6042, 13], "temperature": 0.0, "avg_logprob": -0.12283750282701596, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.612465899204835e-05}, {"id": 298, "seek": 122248, "start": 1231.68, "end": 1236.24, "text": " And the second conclusion is that Transformers tend to look up information about the entity", "tokens": [400, 264, 1150, 10063, 307, 300, 27938, 433, 3928, 281, 574, 493, 1589, 466, 264, 13977], "temperature": 0.0, "avg_logprob": -0.12283750282701596, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.612465899204835e-05}, {"id": 299, "seek": 122248, "start": 1236.24, "end": 1238.52, "text": " on the last token where it's mentioned.", "tokens": [322, 264, 1036, 14862, 689, 309, 311, 2835, 13], "temperature": 0.0, "avg_logprob": -0.12283750282701596, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.612465899204835e-05}, {"id": 300, "seek": 122248, "start": 1238.52, "end": 1240.92, "text": " That's quite an interesting thing too.", "tokens": [663, 311, 1596, 364, 1880, 551, 886, 13], "temperature": 0.0, "avg_logprob": -0.12283750282701596, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.612465899204835e-05}, {"id": 301, "seek": 122248, "start": 1240.92, "end": 1245.04, "text": " Prior to the Rome paper, there were other papers that tried to just fine-tune the entire", "tokens": [24032, 281, 264, 12043, 3035, 11, 456, 645, 661, 10577, 300, 3031, 281, 445, 2489, 12, 83, 2613, 264, 2302], "temperature": 0.0, "avg_logprob": -0.12283750282701596, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.612465899204835e-05}, {"id": 302, "seek": 122248, "start": 1245.04, "end": 1247.56, "text": " network to get it to change its behavior.", "tokens": [3209, 281, 483, 309, 281, 1319, 1080, 5223, 13], "temperature": 0.0, "avg_logprob": -0.12283750282701596, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.612465899204835e-05}, {"id": 303, "seek": 122248, "start": 1247.56, "end": 1250.84, "text": " And when you fine-tune all of the parameters, indeed you can get it to change its behavior", "tokens": [400, 562, 291, 2489, 12, 83, 2613, 439, 295, 264, 9834, 11, 6451, 291, 393, 483, 309, 281, 1319, 1080, 5223], "temperature": 0.0, "avg_logprob": -0.12283750282701596, "compression_ratio": 1.8333333333333333, "no_speech_prob": 8.612465899204835e-05}, {"id": 304, "seek": 125084, "start": 1250.84, "end": 1254.56, "text": " on Rome, but you also mess up a bunch of other facts too.", "tokens": [322, 12043, 11, 457, 291, 611, 2082, 493, 257, 3840, 295, 661, 9130, 886, 13], "temperature": 0.0, "avg_logprob": -0.16733146926104012, "compression_ratio": 1.6285714285714286, "no_speech_prob": 4.683046790887602e-05}, {"id": 305, "seek": 125084, "start": 1254.56, "end": 1261.32, "text": " So being able to make a small edit to one place actually does turn out to be helpful.", "tokens": [407, 885, 1075, 281, 652, 257, 1359, 8129, 281, 472, 1081, 767, 775, 1261, 484, 281, 312, 4961, 13], "temperature": 0.0, "avg_logprob": -0.16733146926104012, "compression_ratio": 1.6285714285714286, "no_speech_prob": 4.683046790887602e-05}, {"id": 306, "seek": 125084, "start": 1261.32, "end": 1264.28, "text": " And lastly, I just want to say this is a very new research area.", "tokens": [400, 16386, 11, 286, 445, 528, 281, 584, 341, 307, 257, 588, 777, 2132, 1859, 13], "temperature": 0.0, "avg_logprob": -0.16733146926104012, "compression_ratio": 1.6285714285714286, "no_speech_prob": 4.683046790887602e-05}, {"id": 307, "seek": 125084, "start": 1264.28, "end": 1267.48, "text": " Next year I could be saying something completely different.", "tokens": [3087, 1064, 286, 727, 312, 1566, 746, 2584, 819, 13], "temperature": 0.0, "avg_logprob": -0.16733146926104012, "compression_ratio": 1.6285714285714286, "no_speech_prob": 4.683046790887602e-05}, {"id": 308, "seek": 125084, "start": 1267.48, "end": 1271.32, "text": " So just take that with a grain of salt.", "tokens": [407, 445, 747, 300, 365, 257, 12837, 295, 5139, 13], "temperature": 0.0, "avg_logprob": -0.16733146926104012, "compression_ratio": 1.6285714285714286, "no_speech_prob": 4.683046790887602e-05}, {"id": 309, "seek": 125084, "start": 1271.32, "end": 1276.72, "text": " So we're going to go into the second half of the presentation.", "tokens": [407, 321, 434, 516, 281, 352, 666, 264, 1150, 1922, 295, 264, 5860, 13], "temperature": 0.0, "avg_logprob": -0.16733146926104012, "compression_ratio": 1.6285714285714286, "no_speech_prob": 4.683046790887602e-05}, {"id": 310, "seek": 125084, "start": 1276.72, "end": 1280.6, "text": " And we're going to talk actually, because we still got time, are there any questions", "tokens": [400, 321, 434, 516, 281, 751, 767, 11, 570, 321, 920, 658, 565, 11, 366, 456, 604, 1651], "temperature": 0.0, "avg_logprob": -0.16733146926104012, "compression_ratio": 1.6285714285714286, "no_speech_prob": 4.683046790887602e-05}, {"id": 311, "seek": 128060, "start": 1280.6, "end": 1282.6, "text": " about the previous slides?", "tokens": [466, 264, 3894, 9788, 30], "temperature": 0.0, "avg_logprob": -0.541371615427845, "compression_ratio": 1.596, "no_speech_prob": 0.0001739140716381371}, {"id": 312, "seek": 128060, "start": 1282.6, "end": 1283.6, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.541371615427845, "compression_ratio": 1.596, "no_speech_prob": 0.0001739140716381371}, {"id": 313, "seek": 128060, "start": 1283.6, "end": 1291.9199999999998, "text": " Yeah, I'm actually curious, you mentioned about how to be surgical about knowledge alteration.", "tokens": [865, 11, 286, 478, 767, 6369, 11, 291, 2835, 466, 577, 281, 312, 26646, 466, 3601, 11337, 399, 13], "temperature": 0.0, "avg_logprob": -0.541371615427845, "compression_ratio": 1.596, "no_speech_prob": 0.0001739140716381371}, {"id": 314, "seek": 128060, "start": 1291.9199999999998, "end": 1299.48, "text": " Imperically, did the researchers discover, or with your own work that discovered that", "tokens": [18360, 984, 11, 630, 264, 10309, 4411, 11, 420, 365, 428, 1065, 589, 300, 6941, 300], "temperature": 0.0, "avg_logprob": -0.541371615427845, "compression_ratio": 1.596, "no_speech_prob": 0.0001739140716381371}, {"id": 315, "seek": 128060, "start": 1299.48, "end": 1302.1999999999998, "text": " it could alter, you know, Iful-tower needing Rome?", "tokens": [309, 727, 11337, 11, 291, 458, 11, 286, 906, 12, 83, 968, 18006, 12043, 30], "temperature": 0.0, "avg_logprob": -0.541371615427845, "compression_ratio": 1.596, "no_speech_prob": 0.0001739140716381371}, {"id": 316, "seek": 128060, "start": 1302.1999999999998, "end": 1307.6799999999998, "text": " Does it cause other past dating effects of confusing Rome, like Paris, or something?", "tokens": [4402, 309, 3082, 661, 1791, 10689, 5065, 295, 13181, 12043, 11, 411, 8380, 11, 420, 746, 30], "temperature": 0.0, "avg_logprob": -0.541371615427845, "compression_ratio": 1.596, "no_speech_prob": 0.0001739140716381371}, {"id": 317, "seek": 128060, "start": 1307.6799999999998, "end": 1308.6799999999998, "text": " That's work.", "tokens": [663, 311, 589, 13], "temperature": 0.0, "avg_logprob": -0.541371615427845, "compression_ratio": 1.596, "no_speech_prob": 0.0001739140716381371}, {"id": 318, "seek": 128060, "start": 1308.6799999999998, "end": 1310.24, "text": " Yeah, yeah, that's a great question.", "tokens": [865, 11, 1338, 11, 300, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.541371615427845, "compression_ratio": 1.596, "no_speech_prob": 0.0001739140716381371}, {"id": 319, "seek": 131024, "start": 1310.24, "end": 1316.36, "text": " So on the eVal benchmarks that they have now for this task, they have, I'm not sure if", "tokens": [407, 322, 264, 308, 53, 304, 43751, 300, 436, 362, 586, 337, 341, 5633, 11, 436, 362, 11, 286, 478, 406, 988, 498], "temperature": 0.0, "avg_logprob": -0.18431563212953764, "compression_ratio": 1.7250996015936255, "no_speech_prob": 6.39961363049224e-05}, {"id": 320, "seek": 131024, "start": 1316.36, "end": 1318.52, "text": " the exact word is like neighbor prompt.", "tokens": [264, 1900, 1349, 307, 411, 5987, 12391, 13], "temperature": 0.0, "avg_logprob": -0.18431563212953764, "compression_ratio": 1.7250996015936255, "no_speech_prob": 6.39961363049224e-05}, {"id": 321, "seek": 131024, "start": 1318.52, "end": 1323.52, "text": " So they have both prompts that are paraphrases of the original thing to test that the models", "tokens": [407, 436, 362, 1293, 41095, 300, 366, 36992, 1703, 1957, 295, 264, 3380, 551, 281, 1500, 300, 264, 5245], "temperature": 0.0, "avg_logprob": -0.18431563212953764, "compression_ratio": 1.7250996015936255, "no_speech_prob": 6.39961363049224e-05}, {"id": 322, "seek": 131024, "start": 1323.52, "end": 1325.0, "text": " were busted paraphrase.", "tokens": [645, 41074, 36992, 1703, 651, 13], "temperature": 0.0, "avg_logprob": -0.18431563212953764, "compression_ratio": 1.7250996015936255, "no_speech_prob": 6.39961363049224e-05}, {"id": 323, "seek": 131024, "start": 1325.0, "end": 1330.76, "text": " But they also have prompts where they ask, say, about Sears Tower or other towers.", "tokens": [583, 436, 611, 362, 41095, 689, 436, 1029, 11, 584, 11, 466, 1100, 685, 17877, 420, 661, 25045, 13], "temperature": 0.0, "avg_logprob": -0.18431563212953764, "compression_ratio": 1.7250996015936255, "no_speech_prob": 6.39961363049224e-05}, {"id": 324, "seek": 131024, "start": 1330.76, "end": 1334.24, "text": " And make sure that those towers didn't move to Rome as well.", "tokens": [400, 652, 988, 300, 729, 25045, 994, 380, 1286, 281, 12043, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18431563212953764, "compression_ratio": 1.7250996015936255, "no_speech_prob": 6.39961363049224e-05}, {"id": 325, "seek": 131024, "start": 1334.24, "end": 1337.6, "text": " And yeah, so that's very imperfect right now.", "tokens": [400, 1338, 11, 370, 300, 311, 588, 26714, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.18431563212953764, "compression_ratio": 1.7250996015936255, "no_speech_prob": 6.39961363049224e-05}, {"id": 326, "seek": 133760, "start": 1337.6, "end": 1341.9199999999998, "text": " There's this difficult balance between the two, and that could be a sign of many things.", "tokens": [821, 311, 341, 2252, 4772, 1296, 264, 732, 11, 293, 300, 727, 312, 257, 1465, 295, 867, 721, 13], "temperature": 0.0, "avg_logprob": -0.22797102298376695, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.618098031030968e-05}, {"id": 327, "seek": 133760, "start": 1341.9199999999998, "end": 1345.12, "text": " It could be a sign that the model doesn't have enough memorization capacity, so it's only", "tokens": [467, 727, 312, 257, 1465, 300, 264, 2316, 1177, 380, 362, 1547, 10560, 2144, 6042, 11, 370, 309, 311, 787], "temperature": 0.0, "avg_logprob": -0.22797102298376695, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.618098031030968e-05}, {"id": 328, "seek": 133760, "start": 1345.12, "end": 1350.36, "text": " way of representing the Iful-tower is to just think of it as like a tower, with maybe", "tokens": [636, 295, 13460, 264, 286, 906, 12, 83, 968, 307, 281, 445, 519, 295, 309, 382, 411, 257, 10567, 11, 365, 1310], "temperature": 0.0, "avg_logprob": -0.22797102298376695, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.618098031030968e-05}, {"id": 329, "seek": 133760, "start": 1350.36, "end": 1351.7199999999998, "text": " some nationality mixed in.", "tokens": [512, 4048, 507, 7467, 294, 13], "temperature": 0.0, "avg_logprob": -0.22797102298376695, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.618098031030968e-05}, {"id": 330, "seek": 133760, "start": 1351.7199999999998, "end": 1356.1599999999999, "text": " And when you edit that tower, you just move all the other towers too.", "tokens": [400, 562, 291, 8129, 300, 10567, 11, 291, 445, 1286, 439, 264, 661, 25045, 886, 13], "temperature": 0.0, "avg_logprob": -0.22797102298376695, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.618098031030968e-05}, {"id": 331, "seek": 133760, "start": 1356.1599999999999, "end": 1358.1599999999999, "text": " Great question, yeah.", "tokens": [3769, 1168, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.22797102298376695, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.618098031030968e-05}, {"id": 332, "seek": 133760, "start": 1358.1599999999999, "end": 1359.1599999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.22797102298376695, "compression_ratio": 1.5942622950819672, "no_speech_prob": 1.618098031030968e-05}, {"id": 333, "seek": 135916, "start": 1359.16, "end": 1368.28, "text": " So from one of the other things, on the next slide, you see different types of questions.", "tokens": [407, 490, 472, 295, 264, 661, 721, 11, 322, 264, 958, 4137, 11, 291, 536, 819, 3467, 295, 1651, 13], "temperature": 0.0, "avg_logprob": -0.8546475569407145, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.00041673038504086435}, {"id": 334, "seek": 135916, "start": 1368.28, "end": 1372.76, "text": " So you asked about Michael Conn where it is, but what about like an article where he's", "tokens": [407, 291, 2351, 466, 5116, 2656, 77, 689, 309, 307, 11, 457, 437, 466, 411, 364, 7222, 689, 415, 311], "temperature": 0.0, "avg_logprob": -0.8546475569407145, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.00041673038504086435}, {"id": 335, "seek": 135916, "start": 1372.76, "end": 1380.16, "text": " done by other types of properties about the key and just the original, the key to the", "tokens": [1096, 538, 661, 3467, 295, 7221, 466, 264, 2141, 293, 445, 264, 3380, 11, 264, 2141, 281, 264], "temperature": 0.0, "avg_logprob": -0.8546475569407145, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.00041673038504086435}, {"id": 336, "seek": 135916, "start": 1380.16, "end": 1383.16, "text": " perspective of the map.", "tokens": [4585, 295, 264, 4471, 13], "temperature": 0.0, "avg_logprob": -0.8546475569407145, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.00041673038504086435}, {"id": 337, "seek": 135916, "start": 1383.16, "end": 1387.92, "text": " Yeah, so I think if I understand your question, it's not just asking about that one fact,", "tokens": [865, 11, 370, 286, 519, 498, 286, 1223, 428, 1168, 11, 309, 311, 406, 445, 3365, 466, 300, 472, 1186, 11], "temperature": 0.0, "avg_logprob": -0.8546475569407145, "compression_ratio": 1.663716814159292, "no_speech_prob": 0.00041673038504086435}, {"id": 338, "seek": 138792, "start": 1387.92, "end": 1390.3600000000001, "text": " but other facts related to the Iful-tower.", "tokens": [457, 661, 9130, 4077, 281, 264, 286, 906, 12, 83, 968, 13], "temperature": 0.0, "avg_logprob": -0.18113609313964843, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.805539068300277e-05}, {"id": 339, "seek": 138792, "start": 1390.3600000000001, "end": 1395.24, "text": " Yeah, so they also have this kind of freeform generation prompt, I think, where they just", "tokens": [865, 11, 370, 436, 611, 362, 341, 733, 295, 1737, 837, 5125, 12391, 11, 286, 519, 11, 689, 436, 445], "temperature": 0.0, "avg_logprob": -0.18113609313964843, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.805539068300277e-05}, {"id": 340, "seek": 138792, "start": 1395.24, "end": 1400.24, "text": " initialize it with Iful-tower, some short prompt, and then they measure the different kinds", "tokens": [5883, 1125, 309, 365, 286, 906, 12, 83, 968, 11, 512, 2099, 12391, 11, 293, 550, 436, 3481, 264, 819, 3685], "temperature": 0.0, "avg_logprob": -0.18113609313964843, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.805539068300277e-05}, {"id": 341, "seek": 138792, "start": 1400.24, "end": 1401.24, "text": " of text that come out.", "tokens": [295, 2487, 300, 808, 484, 13], "temperature": 0.0, "avg_logprob": -0.18113609313964843, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.805539068300277e-05}, {"id": 342, "seek": 138792, "start": 1401.24, "end": 1406.1200000000001, "text": " And I think if I remember correctly, they check like N-gram overlap with real text as", "tokens": [400, 286, 519, 498, 286, 1604, 8944, 11, 436, 1520, 411, 426, 12, 1342, 19959, 365, 957, 2487, 382], "temperature": 0.0, "avg_logprob": -0.18113609313964843, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.805539068300277e-05}, {"id": 343, "seek": 138792, "start": 1406.1200000000001, "end": 1407.1200000000001, "text": " well.", "tokens": [731, 13], "temperature": 0.0, "avg_logprob": -0.18113609313964843, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.805539068300277e-05}, {"id": 344, "seek": 138792, "start": 1407.1200000000001, "end": 1408.1200000000001, "text": " I didn't.", "tokens": [286, 994, 380, 13], "temperature": 0.0, "avg_logprob": -0.18113609313964843, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.805539068300277e-05}, {"id": 345, "seek": 138792, "start": 1408.1200000000001, "end": 1413.16, "text": " So they have a couple more analyses in the paper of how it behaves on other topics too.", "tokens": [407, 436, 362, 257, 1916, 544, 37560, 294, 264, 3035, 295, 577, 309, 36896, 322, 661, 8378, 886, 13], "temperature": 0.0, "avg_logprob": -0.18113609313964843, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.805539068300277e-05}, {"id": 346, "seek": 138792, "start": 1413.16, "end": 1415.04, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.18113609313964843, "compression_ratio": 1.6346863468634687, "no_speech_prob": 1.805539068300277e-05}, {"id": 347, "seek": 141504, "start": 1415.04, "end": 1422.92, "text": " One thing worth mentioning is, I guess, I worked with another student here on a paper on", "tokens": [1485, 551, 3163, 18315, 307, 11, 286, 2041, 11, 286, 2732, 365, 1071, 3107, 510, 322, 257, 3035, 322], "temperature": 0.0, "avg_logprob": -0.13422476450602214, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00017667378415353596}, {"id": 348, "seek": 141504, "start": 1422.92, "end": 1424.36, "text": " counterfactual updates.", "tokens": [5682, 44919, 901, 9205, 13], "temperature": 0.0, "avg_logprob": -0.13422476450602214, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00017667378415353596}, {"id": 349, "seek": 141504, "start": 1424.36, "end": 1432.8, "text": " So we have this data set, which we should really get released out soon, that pairs one", "tokens": [407, 321, 362, 341, 1412, 992, 11, 597, 321, 820, 534, 483, 4736, 484, 2321, 11, 300, 15494, 472], "temperature": 0.0, "avg_logprob": -0.13422476450602214, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00017667378415353596}, {"id": 350, "seek": 141504, "start": 1432.8, "end": 1438.12, "text": " fact update with another implication of that fact update that is non-obvious.", "tokens": [1186, 5623, 365, 1071, 37814, 295, 300, 1186, 5623, 300, 307, 2107, 12, 996, 1502, 13], "temperature": 0.0, "avg_logprob": -0.13422476450602214, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00017667378415353596}, {"id": 351, "seek": 141504, "start": 1438.12, "end": 1443.6, "text": " So for example, if Walt Disney had one of his Academy Awards script, then the next question", "tokens": [407, 337, 1365, 11, 498, 28260, 8653, 632, 472, 295, 702, 11735, 22187, 5755, 11, 550, 264, 958, 1168], "temperature": 0.0, "avg_logprob": -0.13422476450602214, "compression_ratio": 1.5974025974025974, "no_speech_prob": 0.00017667378415353596}, {"id": 352, "seek": 144360, "start": 1443.6, "end": 1447.04, "text": " would be how many Academy Awards did Walt Disney win.", "tokens": [576, 312, 577, 867, 11735, 22187, 630, 28260, 8653, 1942, 13], "temperature": 0.0, "avg_logprob": -0.23625866202420967, "compression_ratio": 1.264, "no_speech_prob": 8.743579383008182e-05}, {"id": 353, "seek": 144360, "start": 1447.04, "end": 1451.4399999999998, "text": " And that's like one of the areas that I think some researchers are thinking about for future", "tokens": [400, 300, 311, 411, 472, 295, 264, 3179, 300, 286, 519, 512, 10309, 366, 1953, 466, 337, 2027], "temperature": 0.0, "avg_logprob": -0.23625866202420967, "compression_ratio": 1.264, "no_speech_prob": 8.743579383008182e-05}, {"id": 354, "seek": 144360, "start": 1451.4399999999998, "end": 1452.4399999999998, "text": " work.", "tokens": [589, 13], "temperature": 0.0, "avg_logprob": -0.23625866202420967, "compression_ratio": 1.264, "no_speech_prob": 8.743579383008182e-05}, {"id": 355, "seek": 144360, "start": 1452.4399999999998, "end": 1453.4399999999998, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23625866202420967, "compression_ratio": 1.264, "no_speech_prob": 8.743579383008182e-05}, {"id": 356, "seek": 145344, "start": 1453.44, "end": 1483.1200000000001, "text": " So the current method basically only allows control through the prompt.", "tokens": [407, 264, 2190, 3170, 1936, 787, 4045, 1969, 807, 264, 12391, 13], "temperature": 0.0, "avg_logprob": -0.14578336477279663, "compression_ratio": 0.9861111111111112, "no_speech_prob": 0.00022682703274767846}, {"id": 357, "seek": 148312, "start": 1483.12, "end": 1487.7199999999998, "text": " So whatever the prompt implies about the entity, you get to change what the answer to that", "tokens": [407, 2035, 264, 12391, 18779, 466, 264, 13977, 11, 291, 483, 281, 1319, 437, 264, 1867, 281, 300], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 358, "seek": 148312, "start": 1487.7199999999998, "end": 1489.12, "text": " prompt is.", "tokens": [12391, 307, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 359, "seek": 148312, "start": 1489.12, "end": 1493.7199999999998, "text": " And it's basically at the moment up to the model to decide what implications are affected", "tokens": [400, 309, 311, 1936, 412, 264, 1623, 493, 281, 264, 2316, 281, 4536, 437, 16602, 366, 8028], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 360, "seek": 148312, "start": 1493.7199999999998, "end": 1495.7199999999998, "text": " by changing that prompt.", "tokens": [538, 4473, 300, 12391, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 361, "seek": 148312, "start": 1495.7199999999998, "end": 1497.6799999999998, "text": " Which is another, you could say, weakness of the approach.", "tokens": [3013, 307, 1071, 11, 291, 727, 584, 11, 12772, 295, 264, 3109, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 362, "seek": 148312, "start": 1497.6799999999998, "end": 1499.08, "text": " It's not entirely interpretable.", "tokens": [467, 311, 406, 7696, 7302, 712, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 363, "seek": 148312, "start": 1499.08, "end": 1500.56, "text": " Everything is done through optimization.", "tokens": [5471, 307, 1096, 807, 19618, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 364, "seek": 148312, "start": 1500.56, "end": 1501.56, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 365, "seek": 148312, "start": 1501.56, "end": 1502.56, "text": " Great question.", "tokens": [3769, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 366, "seek": 148312, "start": 1502.56, "end": 1503.56, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 367, "seek": 148312, "start": 1503.56, "end": 1505.28, "text": " We'll go on for now.", "tokens": [492, 603, 352, 322, 337, 586, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 368, "seek": 148312, "start": 1505.28, "end": 1507.28, "text": " And then, oh, it was a point.", "tokens": [400, 550, 11, 1954, 11, 309, 390, 257, 935, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 369, "seek": 148312, "start": 1507.28, "end": 1508.28, "text": " Oh, yeah.", "tokens": [876, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2565768127441406, "compression_ratio": 1.6222222222222222, "no_speech_prob": 0.00020019762450829148}, {"id": 370, "seek": 150828, "start": 1508.28, "end": 1515.28, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.20632138050777812, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.00010717791155911982}, {"id": 371, "seek": 150828, "start": 1515.28, "end": 1522.84, "text": " The question is about whether the low rank update has anything to do with adversarial machine", "tokens": [440, 1168, 307, 466, 1968, 264, 2295, 6181, 5623, 575, 1340, 281, 360, 365, 17641, 44745, 3479], "temperature": 0.0, "avg_logprob": -0.20632138050777812, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.00010717791155911982}, {"id": 372, "seek": 150828, "start": 1522.84, "end": 1523.84, "text": " learning.", "tokens": [2539, 13], "temperature": 0.0, "avg_logprob": -0.20632138050777812, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.00010717791155911982}, {"id": 373, "seek": 150828, "start": 1523.84, "end": 1529.2, "text": " I'd say maybe there is a slight connection in that, if you look more deeply in the paper,", "tokens": [286, 1116, 584, 1310, 456, 307, 257, 4036, 4984, 294, 300, 11, 498, 291, 574, 544, 8760, 294, 264, 3035, 11], "temperature": 0.0, "avg_logprob": -0.20632138050777812, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.00010717791155911982}, {"id": 374, "seek": 150828, "start": 1529.2, "end": 1536.08, "text": " what they're doing is they're optimizing the output of the weight matrix to change the", "tokens": [437, 436, 434, 884, 307, 436, 434, 40425, 264, 5598, 295, 264, 3364, 8141, 281, 1319, 264], "temperature": 0.0, "avg_logprob": -0.20632138050777812, "compression_ratio": 1.4895833333333333, "no_speech_prob": 0.00010717791155911982}, {"id": 375, "seek": 153608, "start": 1536.08, "end": 1540.32, "text": " label and that optimization procedure is very similar to what they do in adversarial machine", "tokens": [7645, 293, 300, 19618, 10747, 307, 588, 2531, 281, 437, 436, 360, 294, 17641, 44745, 3479], "temperature": 0.0, "avg_logprob": -0.13338171575487273, "compression_ratio": 1.6791666666666667, "no_speech_prob": 7.141409878386185e-05}, {"id": 376, "seek": 153608, "start": 1540.32, "end": 1541.52, "text": " learning.", "tokens": [2539, 13], "temperature": 0.0, "avg_logprob": -0.13338171575487273, "compression_ratio": 1.6791666666666667, "no_speech_prob": 7.141409878386185e-05}, {"id": 377, "seek": 153608, "start": 1541.52, "end": 1545.96, "text": " The low rankness of it is not necessarily connected to the adversarial learning.", "tokens": [440, 2295, 6181, 1287, 295, 309, 307, 406, 4725, 4582, 281, 264, 17641, 44745, 2539, 13], "temperature": 0.0, "avg_logprob": -0.13338171575487273, "compression_ratio": 1.6791666666666667, "no_speech_prob": 7.141409878386185e-05}, {"id": 378, "seek": 153608, "start": 1545.96, "end": 1550.56, "text": " The low rank part is just to minimize the amount of change to the weight matrix.", "tokens": [440, 2295, 6181, 644, 307, 445, 281, 17522, 264, 2372, 295, 1319, 281, 264, 3364, 8141, 13], "temperature": 0.0, "avg_logprob": -0.13338171575487273, "compression_ratio": 1.6791666666666667, "no_speech_prob": 7.141409878386185e-05}, {"id": 379, "seek": 153608, "start": 1550.56, "end": 1553.1999999999998, "text": " I can maybe just quickly do this on the board here.", "tokens": [286, 393, 1310, 445, 2661, 360, 341, 322, 264, 3150, 510, 13], "temperature": 0.0, "avg_logprob": -0.13338171575487273, "compression_ratio": 1.6791666666666667, "no_speech_prob": 7.141409878386185e-05}, {"id": 380, "seek": 153608, "start": 1553.1999999999998, "end": 1563.08, "text": " So if you have this weight matrix, W2 plus UV transpose, the reason this is considered", "tokens": [407, 498, 291, 362, 341, 3364, 8141, 11, 343, 17, 1804, 624, 53, 25167, 11, 264, 1778, 341, 307, 4888], "temperature": 0.0, "avg_logprob": -0.13338171575487273, "compression_ratio": 1.6791666666666667, "no_speech_prob": 7.141409878386185e-05}, {"id": 381, "seek": 156308, "start": 1563.08, "end": 1568.8, "text": " a small update to the matrix is because if you think about anything multiplying W2 after", "tokens": [257, 1359, 5623, 281, 264, 8141, 307, 570, 498, 291, 519, 466, 1340, 30955, 343, 17, 934], "temperature": 0.0, "avg_logprob": -0.1307012438774109, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.264279414201155e-05}, {"id": 382, "seek": 156308, "start": 1568.8, "end": 1572.3999999999999, "text": " it's received this update, let's say you're multiplying it by X, right?", "tokens": [309, 311, 4613, 341, 5623, 11, 718, 311, 584, 291, 434, 30955, 309, 538, 1783, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1307012438774109, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.264279414201155e-05}, {"id": 383, "seek": 156308, "start": 1572.3999999999999, "end": 1579.8, "text": " So this whole thing is multiplied by X. So let's just move this X into the expression.", "tokens": [407, 341, 1379, 551, 307, 17207, 538, 1783, 13, 407, 718, 311, 445, 1286, 341, 1783, 666, 264, 6114, 13], "temperature": 0.0, "avg_logprob": -0.1307012438774109, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.264279414201155e-05}, {"id": 384, "seek": 156308, "start": 1579.8, "end": 1586.24, "text": " You get W2 times X and then UV transpose times X. And this part right here is just what", "tokens": [509, 483, 343, 17, 1413, 1783, 293, 550, 624, 53, 25167, 1413, 1783, 13, 400, 341, 644, 558, 510, 307, 445, 437], "temperature": 0.0, "avg_logprob": -0.1307012438774109, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.264279414201155e-05}, {"id": 385, "seek": 156308, "start": 1586.24, "end": 1588.28, "text": " W2 originally would have done.", "tokens": [343, 17, 7993, 576, 362, 1096, 13], "temperature": 0.0, "avg_logprob": -0.1307012438774109, "compression_ratio": 1.6486486486486487, "no_speech_prob": 4.264279414201155e-05}, {"id": 386, "seek": 158828, "start": 1588.28, "end": 1594.68, "text": " And this part right here, this vector V is being dot-producted with X. And in high-dimensional", "tokens": [400, 341, 644, 558, 510, 11, 341, 8062, 691, 307, 885, 5893, 12, 33244, 292, 365, 1783, 13, 400, 294, 1090, 12, 18759], "temperature": 0.0, "avg_logprob": -0.2137058720444188, "compression_ratio": 1.6331877729257642, "no_speech_prob": 9.9723229141091e-06}, {"id": 387, "seek": 158828, "start": 1594.68, "end": 1599.3999999999999, "text": " spaces, basically most things, dot-products are zero.", "tokens": [7673, 11, 1936, 881, 721, 11, 5893, 12, 33244, 82, 366, 4018, 13], "temperature": 0.0, "avg_logprob": -0.2137058720444188, "compression_ratio": 1.6331877729257642, "no_speech_prob": 9.9723229141091e-06}, {"id": 388, "seek": 158828, "start": 1599.3999999999999, "end": 1604.12, "text": " And so this quantity here is likely to be zero unless X is very close to V. And then", "tokens": [400, 370, 341, 11275, 510, 307, 3700, 281, 312, 4018, 5969, 1783, 307, 588, 1998, 281, 691, 13, 400, 550], "temperature": 0.0, "avg_logprob": -0.2137058720444188, "compression_ratio": 1.6331877729257642, "no_speech_prob": 9.9723229141091e-06}, {"id": 389, "seek": 158828, "start": 1604.12, "end": 1607.12, "text": " this whole update here basically disappears.", "tokens": [341, 1379, 5623, 510, 1936, 25527, 13], "temperature": 0.0, "avg_logprob": -0.2137058720444188, "compression_ratio": 1.6331877729257642, "no_speech_prob": 9.9723229141091e-06}, {"id": 390, "seek": 158828, "start": 1607.12, "end": 1610.28, "text": " So in that sense, you could think of a low rank update as a very small change to weight", "tokens": [407, 294, 300, 2020, 11, 291, 727, 519, 295, 257, 2295, 6181, 5623, 382, 257, 588, 1359, 1319, 281, 3364], "temperature": 0.0, "avg_logprob": -0.2137058720444188, "compression_ratio": 1.6331877729257642, "no_speech_prob": 9.9723229141091e-06}, {"id": 391, "seek": 158828, "start": 1610.28, "end": 1612.28, "text": " matrix.", "tokens": [8141, 13], "temperature": 0.0, "avg_logprob": -0.2137058720444188, "compression_ratio": 1.6331877729257642, "no_speech_prob": 9.9723229141091e-06}, {"id": 392, "seek": 161228, "start": 1612.28, "end": 1620.04, "text": " Okay, cool. We'll move on to the next section.", "tokens": [1033, 11, 1627, 13, 492, 603, 1286, 322, 281, 264, 958, 3541, 13], "temperature": 0.0, "avg_logprob": -0.1883677864074707, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0001105882111005485}, {"id": 393, "seek": 161228, "start": 1620.04, "end": 1622.56, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1883677864074707, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0001105882111005485}, {"id": 394, "seek": 161228, "start": 1622.56, "end": 1626.8799999999999, "text": " So now we've seen what language models currently do to represent knowledge or at least a theory", "tokens": [407, 586, 321, 600, 1612, 437, 2856, 5245, 4362, 360, 281, 2906, 3601, 420, 412, 1935, 257, 5261], "temperature": 0.0, "avg_logprob": -0.1883677864074707, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0001105882111005485}, {"id": 395, "seek": 161228, "start": 1626.8799999999999, "end": 1628.04, "text": " about that.", "tokens": [466, 300, 13], "temperature": 0.0, "avg_logprob": -0.1883677864074707, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0001105882111005485}, {"id": 396, "seek": 161228, "start": 1628.04, "end": 1630.68, "text": " And we'd like to ask ourselves, well, is that really it?", "tokens": [400, 321, 1116, 411, 281, 1029, 4175, 11, 731, 11, 307, 300, 534, 309, 30], "temperature": 0.0, "avg_logprob": -0.1883677864074707, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0001105882111005485}, {"id": 397, "seek": 161228, "start": 1630.68, "end": 1636.24, "text": " Do we just need to make feed-forward layers bigger and bigger to achieve the singularity", "tokens": [1144, 321, 445, 643, 281, 652, 3154, 12, 13305, 7914, 3801, 293, 3801, 281, 4584, 264, 20010, 507], "temperature": 0.0, "avg_logprob": -0.1883677864074707, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0001105882111005485}, {"id": 398, "seek": 161228, "start": 1636.24, "end": 1641.44, "text": " or whatever it is that artificial intelligence researchers are focused on these days?", "tokens": [420, 2035, 309, 307, 300, 11677, 7599, 10309, 366, 5178, 322, 613, 1708, 30], "temperature": 0.0, "avg_logprob": -0.1883677864074707, "compression_ratio": 1.5447470817120623, "no_speech_prob": 0.0001105882111005485}, {"id": 399, "seek": 164144, "start": 1641.44, "end": 1646.3200000000002, "text": " Okay. So what is missing from transformers right now?", "tokens": [1033, 13, 407, 437, 307, 5361, 490, 4088, 433, 558, 586, 30], "temperature": 0.0, "avg_logprob": -0.12764486560115107, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.00011410310980863869}, {"id": 400, "seek": 164144, "start": 1646.3200000000002, "end": 1650.92, "text": " We can automatically acquire knowledge from the web, but a lot of that information can", "tokens": [492, 393, 6772, 20001, 3601, 490, 264, 3670, 11, 457, 257, 688, 295, 300, 1589, 393], "temperature": 0.0, "avg_logprob": -0.12764486560115107, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.00011410310980863869}, {"id": 401, "seek": 164144, "start": 1650.92, "end": 1652.48, "text": " be noisy or incorrect.", "tokens": [312, 24518, 420, 18424, 13], "temperature": 0.0, "avg_logprob": -0.12764486560115107, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.00011410310980863869}, {"id": 402, "seek": 164144, "start": 1652.48, "end": 1658.04, "text": " So the web certainly has its share of misinformation or rumors and opinions.", "tokens": [407, 264, 3670, 3297, 575, 1080, 2073, 295, 34238, 420, 21201, 293, 11819, 13], "temperature": 0.0, "avg_logprob": -0.12764486560115107, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.00011410310980863869}, {"id": 403, "seek": 164144, "start": 1658.04, "end": 1662.3200000000002, "text": " And when it absorbs that misinformation or other things, we can't trace the model's", "tokens": [400, 562, 309, 40745, 300, 34238, 420, 661, 721, 11, 321, 393, 380, 13508, 264, 2316, 311], "temperature": 0.0, "avg_logprob": -0.12764486560115107, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.00011410310980863869}, {"id": 404, "seek": 164144, "start": 1662.3200000000002, "end": 1664.6000000000001, "text": " knowledge back to an attributable source.", "tokens": [3601, 646, 281, 364, 9080, 32148, 4009, 13], "temperature": 0.0, "avg_logprob": -0.12764486560115107, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.00011410310980863869}, {"id": 405, "seek": 164144, "start": 1664.6000000000001, "end": 1667.8400000000001, "text": " So we can trace it back to a particular layer in a feed-forward network, but that still", "tokens": [407, 321, 393, 13508, 309, 646, 281, 257, 1729, 4583, 294, 257, 3154, 12, 13305, 3209, 11, 457, 300, 920], "temperature": 0.0, "avg_logprob": -0.12764486560115107, "compression_ratio": 1.7067669172932332, "no_speech_prob": 0.00011410310980863869}, {"id": 406, "seek": 166784, "start": 1667.84, "end": 1673.08, "text": " doesn't tell us where in the training data it learned that.", "tokens": [1177, 380, 980, 505, 689, 294, 264, 3097, 1412, 309, 3264, 300, 13], "temperature": 0.0, "avg_logprob": -0.1367826375094327, "compression_ratio": 1.637037037037037, "no_speech_prob": 5.143622911418788e-05}, {"id": 407, "seek": 166784, "start": 1673.08, "end": 1677.1999999999998, "text": " Now that would all be okay if we could then surgically edit the model to fix up all of those", "tokens": [823, 300, 576, 439, 312, 1392, 498, 321, 727, 550, 19560, 984, 8129, 264, 2316, 281, 3191, 493, 439, 295, 729], "temperature": 0.0, "avg_logprob": -0.1367826375094327, "compression_ratio": 1.637037037037037, "no_speech_prob": 5.143622911418788e-05}, {"id": 408, "seek": 166784, "start": 1677.1999999999998, "end": 1678.6799999999998, "text": " errors.", "tokens": [13603, 13], "temperature": 0.0, "avg_logprob": -0.1367826375094327, "compression_ratio": 1.637037037037037, "no_speech_prob": 5.143622911418788e-05}, {"id": 409, "seek": 166784, "start": 1678.6799999999998, "end": 1682.04, "text": " But as you've seen, it doesn't work very reliably yet.", "tokens": [583, 382, 291, 600, 1612, 11, 309, 1177, 380, 589, 588, 49927, 1939, 13], "temperature": 0.0, "avg_logprob": -0.1367826375094327, "compression_ratio": 1.637037037037037, "no_speech_prob": 5.143622911418788e-05}, {"id": 410, "seek": 166784, "start": 1682.04, "end": 1685.6, "text": " And another fact that I didn't mention is if you apply a bunch of these edits in sequence", "tokens": [400, 1071, 1186, 300, 286, 994, 380, 2152, 307, 498, 291, 3079, 257, 3840, 295, 613, 41752, 294, 8310], "temperature": 0.0, "avg_logprob": -0.1367826375094327, "compression_ratio": 1.637037037037037, "no_speech_prob": 5.143622911418788e-05}, {"id": 411, "seek": 166784, "start": 1685.6, "end": 1691.12, "text": " to the model, eventually the parameters get kind of so damaged from the edits that it", "tokens": [281, 264, 2316, 11, 4728, 264, 9834, 483, 733, 295, 370, 14080, 490, 264, 41752, 300, 309], "temperature": 0.0, "avg_logprob": -0.1367826375094327, "compression_ratio": 1.637037037037037, "no_speech_prob": 5.143622911418788e-05}, {"id": 412, "seek": 166784, "start": 1691.12, "end": 1694.72, "text": " doesn't maintain its original performance anymore.", "tokens": [1177, 380, 6909, 1080, 3380, 3389, 3602, 13], "temperature": 0.0, "avg_logprob": -0.1367826375094327, "compression_ratio": 1.637037037037037, "no_speech_prob": 5.143622911418788e-05}, {"id": 413, "seek": 169472, "start": 1694.72, "end": 1699.2, "text": " And we can continue to try storing knowledge inside feed-forward layers, but the current", "tokens": [400, 321, 393, 2354, 281, 853, 26085, 3601, 1854, 3154, 12, 13305, 7914, 11, 457, 264, 2190], "temperature": 0.0, "avg_logprob": -0.11382715518657978, "compression_ratio": 1.6082089552238805, "no_speech_prob": 5.738178515457548e-05}, {"id": 414, "seek": 169472, "start": 1699.2, "end": 1703.72, "text": " memorization capacity is still too small, even though we're building these very large", "tokens": [10560, 2144, 6042, 307, 920, 886, 1359, 11, 754, 1673, 321, 434, 2390, 613, 588, 2416], "temperature": 0.0, "avg_logprob": -0.11382715518657978, "compression_ratio": 1.6082089552238805, "no_speech_prob": 5.738178515457548e-05}, {"id": 415, "seek": 169472, "start": 1703.72, "end": 1706.28, "text": " and expensive models.", "tokens": [293, 5124, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11382715518657978, "compression_ratio": 1.6082089552238805, "no_speech_prob": 5.738178515457548e-05}, {"id": 416, "seek": 169472, "start": 1706.28, "end": 1710.6000000000001, "text": " So we can rephrase some of these issues as a wish list for what we would want in a", "tokens": [407, 321, 393, 319, 44598, 651, 512, 295, 613, 2663, 382, 257, 3172, 1329, 337, 437, 321, 576, 528, 294, 257], "temperature": 0.0, "avg_logprob": -0.11382715518657978, "compression_ratio": 1.6082089552238805, "no_speech_prob": 5.738178515457548e-05}, {"id": 417, "seek": 169472, "start": 1710.6000000000001, "end": 1713.64, "text": " knowledge-based language model.", "tokens": [3601, 12, 6032, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11382715518657978, "compression_ratio": 1.6082089552238805, "no_speech_prob": 5.738178515457548e-05}, {"id": 418, "seek": 169472, "start": 1713.64, "end": 1718.6000000000001, "text": " We'd want fast and modular knowledge editing, so be able to robustly edit the model multiple", "tokens": [492, 1116, 528, 2370, 293, 31111, 3601, 10000, 11, 370, 312, 1075, 281, 13956, 356, 8129, 264, 2316, 3866], "temperature": 0.0, "avg_logprob": -0.11382715518657978, "compression_ratio": 1.6082089552238805, "no_speech_prob": 5.738178515457548e-05}, {"id": 419, "seek": 169472, "start": 1718.6000000000001, "end": 1720.52, "text": " times without breaking it.", "tokens": [1413, 1553, 7697, 309, 13], "temperature": 0.0, "avg_logprob": -0.11382715518657978, "compression_ratio": 1.6082089552238805, "no_speech_prob": 5.738178515457548e-05}, {"id": 420, "seek": 172052, "start": 1720.52, "end": 1724.84, "text": " We'd like attribution and interpretability, so tracing a model's knowledge back to something", "tokens": [492, 1116, 411, 9080, 1448, 293, 7302, 2310, 11, 370, 25262, 257, 2316, 311, 3601, 646, 281, 746], "temperature": 0.0, "avg_logprob": -0.14278178061208419, "compression_ratio": 1.6305084745762712, "no_speech_prob": 8.478471863782033e-05}, {"id": 421, "seek": 172052, "start": 1724.84, "end": 1728.24, "text": " in its training set, and we'd like efficient scaling.", "tokens": [294, 1080, 3097, 992, 11, 293, 321, 1116, 411, 7148, 21589, 13], "temperature": 0.0, "avg_logprob": -0.14278178061208419, "compression_ratio": 1.6305084745762712, "no_speech_prob": 8.478471863782033e-05}, {"id": 422, "seek": 172052, "start": 1728.24, "end": 1733.16, "text": " So we'd like to be able to increase the model's memory size by 10x without paying 10x more", "tokens": [407, 321, 1116, 411, 281, 312, 1075, 281, 3488, 264, 2316, 311, 4675, 2744, 538, 1266, 87, 1553, 6229, 1266, 87, 544], "temperature": 0.0, "avg_logprob": -0.14278178061208419, "compression_ratio": 1.6305084745762712, "no_speech_prob": 8.478471863782033e-05}, {"id": 423, "seek": 172052, "start": 1733.16, "end": 1735.6, "text": " compute.", "tokens": [14722, 13], "temperature": 0.0, "avg_logprob": -0.14278178061208419, "compression_ratio": 1.6305084745762712, "no_speech_prob": 8.478471863782033e-05}, {"id": 424, "seek": 172052, "start": 1735.6, "end": 1739.84, "text": " And to give a motivating example, let's just say you wanted to use something like GPT-3", "tokens": [400, 281, 976, 257, 41066, 1365, 11, 718, 311, 445, 584, 291, 1415, 281, 764, 746, 411, 26039, 51, 12, 18], "temperature": 0.0, "avg_logprob": -0.14278178061208419, "compression_ratio": 1.6305084745762712, "no_speech_prob": 8.478471863782033e-05}, {"id": 425, "seek": 172052, "start": 1739.84, "end": 1745.52, "text": " to do question answering over your company or school wiki.", "tokens": [281, 360, 1168, 13430, 670, 428, 2237, 420, 1395, 261, 9850, 13], "temperature": 0.0, "avg_logprob": -0.14278178061208419, "compression_ratio": 1.6305084745762712, "no_speech_prob": 8.478471863782033e-05}, {"id": 426, "seek": 172052, "start": 1745.52, "end": 1749.52, "text": " At the moment, as we know, a single training run, at least when it was originally done,", "tokens": [1711, 264, 1623, 11, 382, 321, 458, 11, 257, 2167, 3097, 1190, 11, 412, 1935, 562, 309, 390, 7993, 1096, 11], "temperature": 0.0, "avg_logprob": -0.14278178061208419, "compression_ratio": 1.6305084745762712, "no_speech_prob": 8.478471863782033e-05}, {"id": 427, "seek": 174952, "start": 1749.52, "end": 1754.32, "text": " cost over $12 million, and we just can't afford to do that for every organization that", "tokens": [2063, 670, 1848, 4762, 2459, 11, 293, 321, 445, 393, 380, 6157, 281, 360, 300, 337, 633, 4475, 300], "temperature": 0.0, "avg_logprob": -0.1197159767150879, "compression_ratio": 1.5543478260869565, "no_speech_prob": 8.748623804422095e-05}, {"id": 428, "seek": 174952, "start": 1754.32, "end": 1758.0, "text": " wants to train a system off of their data.", "tokens": [2738, 281, 3847, 257, 1185, 766, 295, 641, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1197159767150879, "compression_ratio": 1.5543478260869565, "no_speech_prob": 8.748623804422095e-05}, {"id": 429, "seek": 174952, "start": 1758.0, "end": 1762.12, "text": " And furthermore, information is constantly being updated over time, for example, like the", "tokens": [400, 3052, 3138, 11, 1589, 307, 6460, 885, 10588, 670, 565, 11, 337, 1365, 11, 411, 264], "temperature": 0.0, "avg_logprob": -0.1197159767150879, "compression_ratio": 1.5543478260869565, "no_speech_prob": 8.748623804422095e-05}, {"id": 430, "seek": 174952, "start": 1762.12, "end": 1765.8, "text": " COVID requirements for being on campus.", "tokens": [4566, 7728, 337, 885, 322, 4828, 13], "temperature": 0.0, "avg_logprob": -0.1197159767150879, "compression_ratio": 1.5543478260869565, "no_speech_prob": 8.748623804422095e-05}, {"id": 431, "seek": 174952, "start": 1765.8, "end": 1771.28, "text": " So all of this sort of motivates this wish list that I'm giving here.", "tokens": [407, 439, 295, 341, 1333, 295, 42569, 341, 3172, 1329, 300, 286, 478, 2902, 510, 13], "temperature": 0.0, "avg_logprob": -0.1197159767150879, "compression_ratio": 1.5543478260869565, "no_speech_prob": 8.748623804422095e-05}, {"id": 432, "seek": 174952, "start": 1771.28, "end": 1776.32, "text": " So with that, we'll turn to the next main half of the lecture, which is on memory-augmented", "tokens": [407, 365, 300, 11, 321, 603, 1261, 281, 264, 958, 2135, 1922, 295, 264, 7991, 11, 597, 307, 322, 4675, 12, 20056, 14684], "temperature": 0.0, "avg_logprob": -0.1197159767150879, "compression_ratio": 1.5543478260869565, "no_speech_prob": 8.748623804422095e-05}, {"id": 433, "seek": 174952, "start": 1776.32, "end": 1778.44, "text": " models.", "tokens": [5245, 13], "temperature": 0.0, "avg_logprob": -0.1197159767150879, "compression_ratio": 1.5543478260869565, "no_speech_prob": 8.748623804422095e-05}, {"id": 434, "seek": 177844, "start": 1778.44, "end": 1784.2, "text": " So let me first just give a basic overview of what a memory-augmented model is.", "tokens": [407, 718, 385, 700, 445, 976, 257, 3875, 12492, 295, 437, 257, 4675, 12, 20056, 14684, 2316, 307, 13], "temperature": 0.0, "avg_logprob": -0.10669812509569071, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.2472124328487553e-05}, {"id": 435, "seek": 177844, "start": 1784.2, "end": 1788.0800000000002, "text": " To start with, let's just consider a standard neural network model, which takes some sort", "tokens": [1407, 722, 365, 11, 718, 311, 445, 1949, 257, 3832, 18161, 3209, 2316, 11, 597, 2516, 512, 1333], "temperature": 0.0, "avg_logprob": -0.10669812509569071, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.2472124328487553e-05}, {"id": 436, "seek": 177844, "start": 1788.0800000000002, "end": 1793.0, "text": " of input like this one here, passes it through some dense computation, and then produces some", "tokens": [295, 4846, 411, 341, 472, 510, 11, 11335, 309, 807, 512, 18011, 24903, 11, 293, 550, 14725, 512], "temperature": 0.0, "avg_logprob": -0.10669812509569071, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.2472124328487553e-05}, {"id": 437, "seek": 177844, "start": 1793.0, "end": 1794.24, "text": " output.", "tokens": [5598, 13], "temperature": 0.0, "avg_logprob": -0.10669812509569071, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.2472124328487553e-05}, {"id": 438, "seek": 177844, "start": 1794.24, "end": 1799.6000000000001, "text": " And the difference that we're going to make to this model is we're going to attach a memory", "tokens": [400, 264, 2649, 300, 321, 434, 516, 281, 652, 281, 341, 2316, 307, 321, 434, 516, 281, 5085, 257, 4675], "temperature": 0.0, "avg_logprob": -0.10669812509569071, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.2472124328487553e-05}, {"id": 439, "seek": 177844, "start": 1799.6000000000001, "end": 1801.0, "text": " retriever to it.", "tokens": [19817, 331, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.10669812509569071, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.2472124328487553e-05}, {"id": 440, "seek": 177844, "start": 1801.0, "end": 1805.88, "text": " So the input is going to be fed into this memory retriever, which then accesses some external", "tokens": [407, 264, 4846, 307, 516, 281, 312, 4636, 666, 341, 4675, 19817, 331, 11, 597, 550, 2105, 279, 512, 8320], "temperature": 0.0, "avg_logprob": -0.10669812509569071, "compression_ratio": 1.788679245283019, "no_speech_prob": 2.2472124328487553e-05}, {"id": 441, "seek": 180588, "start": 1805.88, "end": 1811.8000000000002, "text": " knowledge source that can be easily scaled, easily edited, easily understood by humans,", "tokens": [3601, 4009, 300, 393, 312, 3612, 36039, 11, 3612, 23016, 11, 3612, 7320, 538, 6255, 11], "temperature": 0.0, "avg_logprob": -0.124057353626598, "compression_ratio": 1.6691449814126393, "no_speech_prob": 2.318457882211078e-05}, {"id": 442, "seek": 180588, "start": 1811.8000000000002, "end": 1814.0400000000002, "text": " like Wikipedia.", "tokens": [411, 28999, 13], "temperature": 0.0, "avg_logprob": -0.124057353626598, "compression_ratio": 1.6691449814126393, "no_speech_prob": 2.318457882211078e-05}, {"id": 443, "seek": 180588, "start": 1814.0400000000002, "end": 1818.64, "text": " And from that, we'll try to identify some piece of information that is relevant for the", "tokens": [400, 490, 300, 11, 321, 603, 853, 281, 5876, 512, 2522, 295, 1589, 300, 307, 7340, 337, 264], "temperature": 0.0, "avg_logprob": -0.124057353626598, "compression_ratio": 1.6691449814126393, "no_speech_prob": 2.318457882211078e-05}, {"id": 444, "seek": 180588, "start": 1818.64, "end": 1823.4, "text": " task at hand, and feed it back into the neural network, and then produce the prediction.", "tokens": [5633, 412, 1011, 11, 293, 3154, 309, 646, 666, 264, 18161, 3209, 11, 293, 550, 5258, 264, 17630, 13], "temperature": 0.0, "avg_logprob": -0.124057353626598, "compression_ratio": 1.6691449814126393, "no_speech_prob": 2.318457882211078e-05}, {"id": 445, "seek": 180588, "start": 1823.4, "end": 1826.6000000000001, "text": " So that's the basic approach that we're thinking about.", "tokens": [407, 300, 311, 264, 3875, 3109, 300, 321, 434, 1953, 466, 13], "temperature": 0.0, "avg_logprob": -0.124057353626598, "compression_ratio": 1.6691449814126393, "no_speech_prob": 2.318457882211078e-05}, {"id": 446, "seek": 180588, "start": 1826.6000000000001, "end": 1831.5200000000002, "text": " And the memory that the memory retriever selects can be any number of things.", "tokens": [400, 264, 4675, 300, 264, 4675, 19817, 331, 3048, 82, 393, 312, 604, 1230, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.124057353626598, "compression_ratio": 1.6691449814126393, "no_speech_prob": 2.318457882211078e-05}, {"id": 447, "seek": 180588, "start": 1831.5200000000002, "end": 1833.2, "text": " It could be a document on the web.", "tokens": [467, 727, 312, 257, 4166, 322, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.124057353626598, "compression_ratio": 1.6691449814126393, "no_speech_prob": 2.318457882211078e-05}, {"id": 448, "seek": 183320, "start": 1833.2, "end": 1838.92, "text": " It could be a record in a database. It could be a training example, an entity embedding.", "tokens": [467, 727, 312, 257, 2136, 294, 257, 8149, 13, 467, 727, 312, 257, 3097, 1365, 11, 364, 13977, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.12172253657195528, "compression_ratio": 1.6937269372693726, "no_speech_prob": 4.0685270505491644e-05}, {"id": 449, "seek": 183320, "start": 1838.92, "end": 1842.76, "text": " I'm just going to focus on text for now, but most of what we'll talk about in this lecture", "tokens": [286, 478, 445, 516, 281, 1879, 322, 2487, 337, 586, 11, 457, 881, 295, 437, 321, 603, 751, 466, 294, 341, 7991], "temperature": 0.0, "avg_logprob": -0.12172253657195528, "compression_ratio": 1.6937269372693726, "no_speech_prob": 4.0685270505491644e-05}, {"id": 450, "seek": 183320, "start": 1842.76, "end": 1849.32, "text": " can apply to other kinds of objects, and you should just keep that in mind that it's", "tokens": [393, 3079, 281, 661, 3685, 295, 6565, 11, 293, 291, 820, 445, 1066, 300, 294, 1575, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.12172253657195528, "compression_ratio": 1.6937269372693726, "no_speech_prob": 4.0685270505491644e-05}, {"id": 451, "seek": 183320, "start": 1849.32, "end": 1852.44, "text": " really not just about text.", "tokens": [534, 406, 445, 466, 2487, 13], "temperature": 0.0, "avg_logprob": -0.12172253657195528, "compression_ratio": 1.6937269372693726, "no_speech_prob": 4.0685270505491644e-05}, {"id": 452, "seek": 183320, "start": 1852.44, "end": 1856.0, "text": " So this potentially meets our wish list of things we'd like to do.", "tokens": [407, 341, 7263, 13961, 527, 3172, 1329, 295, 721, 321, 1116, 411, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.12172253657195528, "compression_ratio": 1.6937269372693726, "no_speech_prob": 4.0685270505491644e-05}, {"id": 453, "seek": 183320, "start": 1856.0, "end": 1859.04, "text": " You can easily edit the knowledge in something like Wikipedia.", "tokens": [509, 393, 3612, 8129, 264, 3601, 294, 746, 411, 28999, 13], "temperature": 0.0, "avg_logprob": -0.12172253657195528, "compression_ratio": 1.6937269372693726, "no_speech_prob": 4.0685270505491644e-05}, {"id": 454, "seek": 183320, "start": 1859.04, "end": 1861.0800000000002, "text": " You can easily attribute back to it.", "tokens": [509, 393, 3612, 19667, 646, 281, 309, 13], "temperature": 0.0, "avg_logprob": -0.12172253657195528, "compression_ratio": 1.6937269372693726, "no_speech_prob": 4.0685270505491644e-05}, {"id": 455, "seek": 186108, "start": 1861.08, "end": 1864.6799999999998, "text": " There's a source, there's an author for everything that's put in there, and there's efficient", "tokens": [821, 311, 257, 4009, 11, 456, 311, 364, 3793, 337, 1203, 300, 311, 829, 294, 456, 11, 293, 456, 311, 7148], "temperature": 0.0, "avg_logprob": -0.12636137008666992, "compression_ratio": 1.6063829787234043, "no_speech_prob": 7.601315155625343e-05}, {"id": 456, "seek": 186108, "start": 1864.6799999999998, "end": 1869.28, "text": " scaling, because I can always add more articles to Wikipedia, and I don't have to change the", "tokens": [21589, 11, 570, 286, 393, 1009, 909, 544, 11290, 281, 28999, 11, 293, 286, 500, 380, 362, 281, 1319, 264], "temperature": 0.0, "avg_logprob": -0.12636137008666992, "compression_ratio": 1.6063829787234043, "no_speech_prob": 7.601315155625343e-05}, {"id": 457, "seek": 186108, "start": 1869.28, "end": 1873.56, "text": " size of a neural network that's accessing it.", "tokens": [2744, 295, 257, 18161, 3209, 300, 311, 26440, 309, 13], "temperature": 0.0, "avg_logprob": -0.12636137008666992, "compression_ratio": 1.6063829787234043, "no_speech_prob": 7.601315155625343e-05}, {"id": 458, "seek": 186108, "start": 1873.56, "end": 1878.48, "text": " And some motivating applications for why you would care about this.", "tokens": [400, 512, 41066, 5821, 337, 983, 291, 576, 1127, 466, 341, 13], "temperature": 0.0, "avg_logprob": -0.12636137008666992, "compression_ratio": 1.6063829787234043, "no_speech_prob": 7.601315155625343e-05}, {"id": 459, "seek": 186108, "start": 1878.48, "end": 1881.76, "text": " If you're building an open domain dialogue or question answering system, you want to", "tokens": [759, 291, 434, 2390, 364, 1269, 9274, 10221, 420, 1168, 13430, 1185, 11, 291, 528, 281], "temperature": 0.0, "avg_logprob": -0.12636137008666992, "compression_ratio": 1.6063829787234043, "no_speech_prob": 7.601315155625343e-05}, {"id": 460, "seek": 186108, "start": 1881.76, "end": 1886.6799999999998, "text": " have robust access to knowledge by retrieving documents on the web.", "tokens": [362, 13956, 2105, 281, 3601, 538, 19817, 798, 8512, 322, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.12636137008666992, "compression_ratio": 1.6063829787234043, "no_speech_prob": 7.601315155625343e-05}, {"id": 461, "seek": 188668, "start": 1886.68, "end": 1891.44, "text": " If you're generating code, I'm pretty sure all of us are guilty of at least some point", "tokens": [759, 291, 434, 17746, 3089, 11, 286, 478, 1238, 988, 439, 295, 505, 366, 12341, 295, 412, 1935, 512, 935], "temperature": 0.0, "avg_logprob": -0.1661414966716633, "compression_ratio": 1.688953488372093, "no_speech_prob": 0.00011771472782129422}, {"id": 462, "seek": 188668, "start": 1891.44, "end": 1893.96, "text": " going on Stack Overflow and copying a snippet.", "tokens": [516, 322, 37649, 4886, 10565, 293, 27976, 257, 35623, 302, 13], "temperature": 0.0, "avg_logprob": -0.1661414966716633, "compression_ratio": 1.688953488372093, "no_speech_prob": 0.00011771472782129422}, {"id": 463, "seek": 188668, "start": 1893.96, "end": 1899.16, "text": " So even we do retrieval, we are in a form of memory augmented vowel.", "tokens": [407, 754, 321, 360, 19817, 3337, 11, 321, 366, 294, 257, 1254, 295, 4675, 36155, 29410, 13], "temperature": 0.0, "avg_logprob": -0.1661414966716633, "compression_ratio": 1.688953488372093, "no_speech_prob": 0.00011771472782129422}, {"id": 464, "seek": 188668, "start": 1899.16, "end": 1902.5600000000002, "text": " If you're doing image generation, if somebody tells you, I want a picture of the Eiffel Tower", "tokens": [759, 291, 434, 884, 3256, 5125, 11, 498, 2618, 5112, 291, 11, 286, 528, 257, 3036, 295, 264, 462, 3661, 338, 17877], "temperature": 0.0, "avg_logprob": -0.1661414966716633, "compression_ratio": 1.688953488372093, "no_speech_prob": 0.00011771472782129422}, {"id": 465, "seek": 188668, "start": 1902.5600000000002, "end": 1907.8, "text": " on the White House lawn, you might consult some reference pictures of those objects.", "tokens": [322, 264, 5552, 4928, 19915, 11, 291, 1062, 7189, 512, 6408, 5242, 295, 729, 6565, 13], "temperature": 0.0, "avg_logprob": -0.1661414966716633, "compression_ratio": 1.688953488372093, "no_speech_prob": 0.00011771472782129422}, {"id": 466, "seek": 188668, "start": 1907.8, "end": 1911.0800000000002, "text": " And if you're doing fact checking, you might want to retrieve documents that support or", "tokens": [400, 498, 291, 434, 884, 1186, 8568, 11, 291, 1062, 528, 281, 30254, 8512, 300, 1406, 420], "temperature": 0.0, "avg_logprob": -0.1661414966716633, "compression_ratio": 1.688953488372093, "no_speech_prob": 0.00011771472782129422}, {"id": 467, "seek": 188668, "start": 1911.0800000000002, "end": 1912.16, "text": " refute a claim.", "tokens": [1895, 1169, 257, 3932, 13], "temperature": 0.0, "avg_logprob": -0.1661414966716633, "compression_ratio": 1.688953488372093, "no_speech_prob": 0.00011771472782129422}, {"id": 468, "seek": 188668, "start": 1912.16, "end": 1916.0800000000002, "text": " All of these things are very knowledge intensive tasks, and could benefit from an approach like", "tokens": [1057, 295, 613, 721, 366, 588, 3601, 18957, 9608, 11, 293, 727, 5121, 490, 364, 3109, 411], "temperature": 0.0, "avg_logprob": -0.1661414966716633, "compression_ratio": 1.688953488372093, "no_speech_prob": 0.00011771472782129422}, {"id": 469, "seek": 191608, "start": 1916.08, "end": 1917.08, "text": " this.", "tokens": [341, 13], "temperature": 0.0, "avg_logprob": -0.35476015198905514, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0005699820467270911}, {"id": 470, "seek": 191608, "start": 1917.08, "end": 1918.08, "text": " Yeah, question.", "tokens": [865, 11, 1168, 13], "temperature": 0.0, "avg_logprob": -0.35476015198905514, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0005699820467270911}, {"id": 471, "seek": 191608, "start": 1918.08, "end": 1919.08, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.35476015198905514, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0005699820467270911}, {"id": 472, "seek": 191608, "start": 1919.08, "end": 1920.08, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.35476015198905514, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0005699820467270911}, {"id": 473, "seek": 191608, "start": 1920.08, "end": 1921.08, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.35476015198905514, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0005699820467270911}, {"id": 474, "seek": 191608, "start": 1921.08, "end": 1922.08, "text": " That's a good question.", "tokens": [663, 311, 257, 665, 1168, 13], "temperature": 0.0, "avg_logprob": -0.35476015198905514, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0005699820467270911}, {"id": 475, "seek": 191608, "start": 1922.08, "end": 1946.04, "text": " So the question is whether you have to retrieve one memory, and that's not the case.", "tokens": [407, 264, 1168, 307, 1968, 291, 362, 281, 30254, 472, 4675, 11, 293, 300, 311, 406, 264, 1389, 13], "temperature": 0.0, "avg_logprob": -0.35476015198905514, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.0005699820467270911}, {"id": 476, "seek": 194604, "start": 1946.04, "end": 1950.24, "text": " I'm going to use that as the simplified example that we'll work with, but you could retrieve", "tokens": [286, 478, 516, 281, 764, 300, 382, 264, 26335, 1365, 300, 321, 603, 589, 365, 11, 457, 291, 727, 30254], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 477, "seek": 194604, "start": 1950.24, "end": 1951.6399999999999, "text": " multiple memories.", "tokens": [3866, 8495, 13], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 478, "seek": 194604, "start": 1951.6399999999999, "end": 1955.3999999999999, "text": " The complexity of retrieving multiple memories increases, and we may be good to that in a", "tokens": [440, 14024, 295, 19817, 798, 3866, 8495, 8637, 11, 293, 321, 815, 312, 665, 281, 300, 294, 257], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 479, "seek": 194604, "start": 1955.3999999999999, "end": 1956.3999999999999, "text": " little bit.", "tokens": [707, 857, 13], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 480, "seek": 194604, "start": 1956.3999999999999, "end": 1957.3999999999999, "text": " Good question.", "tokens": [2205, 1168, 13], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 481, "seek": 194604, "start": 1957.3999999999999, "end": 1958.3999999999999, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 482, "seek": 194604, "start": 1958.3999999999999, "end": 1959.3999999999999, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 483, "seek": 194604, "start": 1959.3999999999999, "end": 1963.12, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 484, "seek": 194604, "start": 1963.12, "end": 1967.76, "text": " So the rest of this talk is going to be structured around the main design questions around", "tokens": [407, 264, 1472, 295, 341, 751, 307, 516, 281, 312, 18519, 926, 264, 2135, 1715, 1651, 926], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 485, "seek": 194604, "start": 1967.76, "end": 1970.84, "text": " how to design a memory augmented model.", "tokens": [577, 281, 1715, 257, 4675, 36155, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 486, "seek": 194604, "start": 1970.84, "end": 1973.28, "text": " So first you have to choose what your memories are.", "tokens": [407, 700, 291, 362, 281, 2826, 437, 428, 8495, 366, 13], "temperature": 0.0, "avg_logprob": -0.16216652853447094, "compression_ratio": 1.6692307692307693, "no_speech_prob": 0.00015115263522602618}, {"id": 487, "seek": 197328, "start": 1973.28, "end": 1976.48, "text": " I'm actually not going to focus on that much because based on your application, you can", "tokens": [286, 478, 767, 406, 516, 281, 1879, 322, 300, 709, 570, 2361, 322, 428, 3861, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.14203516164220364, "compression_ratio": 1.745928338762215, "no_speech_prob": 2.586381924629677e-05}, {"id": 488, "seek": 197328, "start": 1976.48, "end": 1980.3999999999999, "text": " usually guess what you would want your memories to be.", "tokens": [2673, 2041, 437, 291, 576, 528, 428, 8495, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.14203516164220364, "compression_ratio": 1.745928338762215, "no_speech_prob": 2.586381924629677e-05}, {"id": 489, "seek": 197328, "start": 1980.3999999999999, "end": 1984.0, "text": " And then we're going to think very hard about how to retrieve the memories.", "tokens": [400, 550, 321, 434, 516, 281, 519, 588, 1152, 466, 577, 281, 30254, 264, 8495, 13], "temperature": 0.0, "avg_logprob": -0.14203516164220364, "compression_ratio": 1.745928338762215, "no_speech_prob": 2.586381924629677e-05}, {"id": 490, "seek": 197328, "start": 1984.0, "end": 1988.0, "text": " That's essentially the heart, maybe the hardest part of the problem.", "tokens": [663, 311, 4476, 264, 1917, 11, 1310, 264, 13158, 644, 295, 264, 1154, 13], "temperature": 0.0, "avg_logprob": -0.14203516164220364, "compression_ratio": 1.745928338762215, "no_speech_prob": 2.586381924629677e-05}, {"id": 491, "seek": 197328, "start": 1988.0, "end": 1991.96, "text": " Approaches we'll look at are you could use an off the shelf search engine like Google", "tokens": [29551, 13272, 321, 603, 574, 412, 366, 291, 727, 764, 364, 766, 264, 15222, 3164, 2848, 411, 3329], "temperature": 0.0, "avg_logprob": -0.14203516164220364, "compression_ratio": 1.745928338762215, "no_speech_prob": 2.586381924629677e-05}, {"id": 492, "seek": 197328, "start": 1991.96, "end": 1997.32, "text": " or Stack Overflow, or you could train your own memory retriever, which will spend some", "tokens": [420, 37649, 4886, 10565, 11, 420, 291, 727, 3847, 428, 1065, 4675, 19817, 331, 11, 597, 486, 3496, 512], "temperature": 0.0, "avg_logprob": -0.14203516164220364, "compression_ratio": 1.745928338762215, "no_speech_prob": 2.586381924629677e-05}, {"id": 493, "seek": 197328, "start": 1997.32, "end": 1998.32, "text": " time on.", "tokens": [565, 322, 13], "temperature": 0.0, "avg_logprob": -0.14203516164220364, "compression_ratio": 1.745928338762215, "no_speech_prob": 2.586381924629677e-05}, {"id": 494, "seek": 197328, "start": 1998.32, "end": 2002.52, "text": " And lastly, we'll end by looking at how to use retrieved memories.", "tokens": [400, 16386, 11, 321, 603, 917, 538, 1237, 412, 577, 281, 764, 19817, 937, 8495, 13], "temperature": 0.0, "avg_logprob": -0.14203516164220364, "compression_ratio": 1.745928338762215, "no_speech_prob": 2.586381924629677e-05}, {"id": 495, "seek": 200252, "start": 2002.52, "end": 2008.28, "text": " So we'll cover a few different approaches such as text fusion, label smearing.", "tokens": [407, 321, 603, 2060, 257, 1326, 819, 11587, 1270, 382, 2487, 23100, 11, 7645, 41818, 1921, 13], "temperature": 0.0, "avg_logprob": -0.15089745357118803, "compression_ratio": 1.6902985074626866, "no_speech_prob": 2.3550928744953126e-05}, {"id": 496, "seek": 200252, "start": 2008.28, "end": 2012.52, "text": " And perhaps most interestingly, I'll talk about a few common failure modes for when models", "tokens": [400, 4317, 881, 25873, 11, 286, 603, 751, 466, 257, 1326, 2689, 7763, 14068, 337, 562, 5245], "temperature": 0.0, "avg_logprob": -0.15089745357118803, "compression_ratio": 1.6902985074626866, "no_speech_prob": 2.3550928744953126e-05}, {"id": 497, "seek": 200252, "start": 2012.52, "end": 2014.52, "text": " try to use memory.", "tokens": [853, 281, 764, 4675, 13], "temperature": 0.0, "avg_logprob": -0.15089745357118803, "compression_ratio": 1.6902985074626866, "no_speech_prob": 2.3550928744953126e-05}, {"id": 498, "seek": 200252, "start": 2014.52, "end": 2018.0, "text": " One of them is something that we'll call underutilization, where the model actually ignores", "tokens": [1485, 295, 552, 307, 746, 300, 321, 603, 818, 833, 20835, 2144, 11, 689, 264, 2316, 767, 5335, 2706], "temperature": 0.0, "avg_logprob": -0.15089745357118803, "compression_ratio": 1.6902985074626866, "no_speech_prob": 2.3550928744953126e-05}, {"id": 499, "seek": 200252, "start": 2018.0, "end": 2019.52, "text": " the retrieved memories.", "tokens": [264, 19817, 937, 8495, 13], "temperature": 0.0, "avg_logprob": -0.15089745357118803, "compression_ratio": 1.6902985074626866, "no_speech_prob": 2.3550928744953126e-05}, {"id": 500, "seek": 200252, "start": 2019.52, "end": 2023.44, "text": " And another one is over reliance, where the model somehow kind of becomes too dependent", "tokens": [400, 1071, 472, 307, 670, 1039, 6276, 11, 689, 264, 2316, 6063, 733, 295, 3643, 886, 12334], "temperature": 0.0, "avg_logprob": -0.15089745357118803, "compression_ratio": 1.6902985074626866, "no_speech_prob": 2.3550928744953126e-05}, {"id": 501, "seek": 200252, "start": 2023.44, "end": 2024.44, "text": " on memory.", "tokens": [322, 4675, 13], "temperature": 0.0, "avg_logprob": -0.15089745357118803, "compression_ratio": 1.6902985074626866, "no_speech_prob": 2.3550928744953126e-05}, {"id": 502, "seek": 200252, "start": 2024.44, "end": 2028.0, "text": " And I'll talk about that when we get there.", "tokens": [400, 286, 603, 751, 466, 300, 562, 321, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.15089745357118803, "compression_ratio": 1.6902985074626866, "no_speech_prob": 2.3550928744953126e-05}, {"id": 503, "seek": 200252, "start": 2028.0, "end": 2030.28, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15089745357118803, "compression_ratio": 1.6902985074626866, "no_speech_prob": 2.3550928744953126e-05}, {"id": 504, "seek": 203028, "start": 2030.28, "end": 2034.32, "text": " So let's go into the section on retrieving memories.", "tokens": [407, 718, 311, 352, 666, 264, 3541, 322, 19817, 798, 8495, 13], "temperature": 0.0, "avg_logprob": -0.08469570916274498, "compression_ratio": 1.7158273381294964, "no_speech_prob": 3.5906028642784804e-05}, {"id": 505, "seek": 203028, "start": 2034.32, "end": 2037.76, "text": " So I'll kind of organize them into two broad groups.", "tokens": [407, 286, 603, 733, 295, 13859, 552, 666, 732, 4152, 3935, 13], "temperature": 0.0, "avg_logprob": -0.08469570916274498, "compression_ratio": 1.7158273381294964, "no_speech_prob": 3.5906028642784804e-05}, {"id": 506, "seek": 203028, "start": 2037.76, "end": 2041.24, "text": " So one is the set of approaches that use an external tool.", "tokens": [407, 472, 307, 264, 992, 295, 11587, 300, 764, 364, 8320, 2290, 13], "temperature": 0.0, "avg_logprob": -0.08469570916274498, "compression_ratio": 1.7158273381294964, "no_speech_prob": 3.5906028642784804e-05}, {"id": 507, "seek": 203028, "start": 2041.24, "end": 2044.28, "text": " And the other is the set where you train your own.", "tokens": [400, 264, 661, 307, 264, 992, 689, 291, 3847, 428, 1065, 13], "temperature": 0.0, "avg_logprob": -0.08469570916274498, "compression_ratio": 1.7158273381294964, "no_speech_prob": 3.5906028642784804e-05}, {"id": 508, "seek": 203028, "start": 2044.28, "end": 2049.52, "text": " And we'll start with an approach that uses an external tool, just because I think some", "tokens": [400, 321, 603, 722, 365, 364, 3109, 300, 4960, 364, 8320, 2290, 11, 445, 570, 286, 519, 512], "temperature": 0.0, "avg_logprob": -0.08469570916274498, "compression_ratio": 1.7158273381294964, "no_speech_prob": 3.5906028642784804e-05}, {"id": 509, "seek": 203028, "start": 2049.52, "end": 2054.8, "text": " of those approaches have been really popping up this year and are quite exciting.", "tokens": [295, 729, 11587, 362, 668, 534, 18374, 493, 341, 1064, 293, 366, 1596, 4670, 13], "temperature": 0.0, "avg_logprob": -0.08469570916274498, "compression_ratio": 1.7158273381294964, "no_speech_prob": 3.5906028642784804e-05}, {"id": 510, "seek": 203028, "start": 2054.8, "end": 2059.2, "text": " The first approach we'll look at is from this paper called Lambda, which stands for Language", "tokens": [440, 700, 3109, 321, 603, 574, 412, 307, 490, 341, 3035, 1219, 45691, 11, 597, 7382, 337, 24445], "temperature": 0.0, "avg_logprob": -0.08469570916274498, "compression_ratio": 1.7158273381294964, "no_speech_prob": 3.5906028642784804e-05}, {"id": 511, "seek": 205920, "start": 2059.2, "end": 2061.4399999999996, "text": " Models for Dialog Applications.", "tokens": [6583, 1625, 337, 29658, 664, 26519, 763, 13], "temperature": 0.0, "avg_logprob": -0.13242631209524056, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.00010719424608396366}, {"id": 512, "seek": 205920, "start": 2061.4399999999996, "end": 2065.3999999999996, "text": " So on the right we've got a dialogue between a human user and the model.", "tokens": [407, 322, 264, 558, 321, 600, 658, 257, 10221, 1296, 257, 1952, 4195, 293, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13242631209524056, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.00010719424608396366}, {"id": 513, "seek": 205920, "start": 2065.3999999999996, "end": 2068.3999999999996, "text": " This is a, I think, a real dialogue from their paper.", "tokens": [639, 307, 257, 11, 286, 519, 11, 257, 957, 10221, 490, 641, 3035, 13], "temperature": 0.0, "avg_logprob": -0.13242631209524056, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.00010719424608396366}, {"id": 514, "seek": 205920, "start": 2068.3999999999996, "end": 2072.6, "text": " And the user is just asking about a particular artist and the model is giving a very spirited", "tokens": [400, 264, 4195, 307, 445, 3365, 466, 257, 1729, 5748, 293, 264, 2316, 307, 2902, 257, 588, 10733, 1226], "temperature": 0.0, "avg_logprob": -0.13242631209524056, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.00010719424608396366}, {"id": 515, "seek": 205920, "start": 2072.6, "end": 2077.64, "text": " reply, complete with personal opinions and even follow up information.", "tokens": [16972, 11, 3566, 365, 2973, 11819, 293, 754, 1524, 493, 1589, 13], "temperature": 0.0, "avg_logprob": -0.13242631209524056, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.00010719424608396366}, {"id": 516, "seek": 205920, "start": 2077.64, "end": 2082.8799999999997, "text": " So Lambda is an open domain dialog chatbot that's designed to cover a very large range", "tokens": [407, 45691, 307, 364, 1269, 9274, 19308, 5081, 18870, 300, 311, 4761, 281, 2060, 257, 588, 2416, 3613], "temperature": 0.0, "avg_logprob": -0.13242631209524056, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.00010719424608396366}, {"id": 517, "seek": 205920, "start": 2082.8799999999997, "end": 2083.8799999999997, "text": " of topics.", "tokens": [295, 8378, 13], "temperature": 0.0, "avg_logprob": -0.13242631209524056, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.00010719424608396366}, {"id": 518, "seek": 205920, "start": 2083.8799999999997, "end": 2087.24, "text": " So you need some kind of memory component that's able to handle anything the user might", "tokens": [407, 291, 643, 512, 733, 295, 4675, 6542, 300, 311, 1075, 281, 4813, 1340, 264, 4195, 1062], "temperature": 0.0, "avg_logprob": -0.13242631209524056, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.00010719424608396366}, {"id": 519, "seek": 205920, "start": 2087.24, "end": 2089.12, "text": " throw at the model.", "tokens": [3507, 412, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13242631209524056, "compression_ratio": 1.706451612903226, "no_speech_prob": 0.00010719424608396366}, {"id": 520, "seek": 208912, "start": 2089.12, "end": 2093.16, "text": " And the basic version of the model is just a transformer decoder.", "tokens": [400, 264, 3875, 3037, 295, 264, 2316, 307, 445, 257, 31782, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.15683211419815407, "compression_ratio": 1.8096885813148789, "no_speech_prob": 3.218678102712147e-05}, {"id": 521, "seek": 208912, "start": 2093.16, "end": 2098.56, "text": " So it's the same kind of transformer that we were studying in the previous slides.", "tokens": [407, 309, 311, 264, 912, 733, 295, 31782, 300, 321, 645, 7601, 294, 264, 3894, 9788, 13], "temperature": 0.0, "avg_logprob": -0.15683211419815407, "compression_ratio": 1.8096885813148789, "no_speech_prob": 3.218678102712147e-05}, {"id": 522, "seek": 208912, "start": 2098.56, "end": 2102.56, "text": " The input to that transformer is essentially the previous turns of the conversation, represented", "tokens": [440, 4846, 281, 300, 31782, 307, 4476, 264, 3894, 4523, 295, 264, 3761, 11, 10379], "temperature": 0.0, "avg_logprob": -0.15683211419815407, "compression_ratio": 1.8096885813148789, "no_speech_prob": 3.218678102712147e-05}, {"id": 523, "seek": 208912, "start": 2102.56, "end": 2103.7599999999998, "text": " as text.", "tokens": [382, 2487, 13], "temperature": 0.0, "avg_logprob": -0.15683211419815407, "compression_ratio": 1.8096885813148789, "no_speech_prob": 3.218678102712147e-05}, {"id": 524, "seek": 208912, "start": 2103.7599999999998, "end": 2107.3599999999997, "text": " And the output is just a new utterance that it needs to generate, also as text.", "tokens": [400, 264, 5598, 307, 445, 257, 777, 17567, 719, 300, 309, 2203, 281, 8460, 11, 611, 382, 2487, 13], "temperature": 0.0, "avg_logprob": -0.15683211419815407, "compression_ratio": 1.8096885813148789, "no_speech_prob": 3.218678102712147e-05}, {"id": 525, "seek": 208912, "start": 2107.3599999999997, "end": 2110.3599999999997, "text": " So it's just a text to text kind of approach.", "tokens": [407, 309, 311, 445, 257, 2487, 281, 2487, 733, 295, 3109, 13], "temperature": 0.0, "avg_logprob": -0.15683211419815407, "compression_ratio": 1.8096885813148789, "no_speech_prob": 3.218678102712147e-05}, {"id": 526, "seek": 208912, "start": 2110.3599999999997, "end": 2111.3599999999997, "text": " What's new though?", "tokens": [708, 311, 777, 1673, 30], "temperature": 0.0, "avg_logprob": -0.15683211419815407, "compression_ratio": 1.8096885813148789, "no_speech_prob": 3.218678102712147e-05}, {"id": 527, "seek": 208912, "start": 2111.3599999999997, "end": 2114.24, "text": " Oh, okay, not quite there, yeah.", "tokens": [876, 11, 1392, 11, 406, 1596, 456, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.15683211419815407, "compression_ratio": 1.8096885813148789, "no_speech_prob": 3.218678102712147e-05}, {"id": 528, "seek": 208912, "start": 2114.24, "end": 2118.08, "text": " So one last thing about this model though is as they were developing it, they noticed that", "tokens": [407, 472, 1036, 551, 466, 341, 2316, 1673, 307, 382, 436, 645, 6416, 309, 11, 436, 5694, 300], "temperature": 0.0, "avg_logprob": -0.15683211419815407, "compression_ratio": 1.8096885813148789, "no_speech_prob": 3.218678102712147e-05}, {"id": 529, "seek": 211808, "start": 2118.08, "end": 2120.44, "text": " it often generated factually incorrect claims.", "tokens": [309, 2049, 10833, 1186, 671, 18424, 9441, 13], "temperature": 0.0, "avg_logprob": -0.11948252178373792, "compression_ratio": 1.7596899224806202, "no_speech_prob": 5.8285208069719374e-05}, {"id": 530, "seek": 211808, "start": 2120.44, "end": 2126.2, "text": " So just to highlight that, this last claim that the model makes is factually incorrect.", "tokens": [407, 445, 281, 5078, 300, 11, 341, 1036, 3932, 300, 264, 2316, 1669, 307, 1186, 671, 18424, 13], "temperature": 0.0, "avg_logprob": -0.11948252178373792, "compression_ratio": 1.7596899224806202, "no_speech_prob": 5.8285208069719374e-05}, {"id": 531, "seek": 211808, "start": 2126.2, "end": 2131.04, "text": " In this case, this particular artist that was supposedly inspired by the earlier artists", "tokens": [682, 341, 1389, 11, 341, 1729, 5748, 300, 390, 20581, 7547, 538, 264, 3071, 6910], "temperature": 0.0, "avg_logprob": -0.11948252178373792, "compression_ratio": 1.7596899224806202, "no_speech_prob": 5.8285208069719374e-05}, {"id": 532, "seek": 211808, "start": 2131.04, "end": 2134.24, "text": " stopped working before the first one began working.", "tokens": [5936, 1364, 949, 264, 700, 472, 4283, 1364, 13], "temperature": 0.0, "avg_logprob": -0.11948252178373792, "compression_ratio": 1.7596899224806202, "no_speech_prob": 5.8285208069719374e-05}, {"id": 533, "seek": 211808, "start": 2134.24, "end": 2137.04, "text": " So this just can't be true.", "tokens": [407, 341, 445, 393, 380, 312, 2074, 13], "temperature": 0.0, "avg_logprob": -0.11948252178373792, "compression_ratio": 1.7596899224806202, "no_speech_prob": 5.8285208069719374e-05}, {"id": 534, "seek": 211808, "start": 2137.04, "end": 2140.92, "text": " And their approach to solving this problem will be to teach their base model to learn", "tokens": [400, 641, 3109, 281, 12606, 341, 1154, 486, 312, 281, 2924, 641, 3096, 2316, 281, 1466], "temperature": 0.0, "avg_logprob": -0.11948252178373792, "compression_ratio": 1.7596899224806202, "no_speech_prob": 5.8285208069719374e-05}, {"id": 535, "seek": 211808, "start": 2140.92, "end": 2145.72, "text": " to use a search engine to validate or fix claims that it's made.", "tokens": [281, 764, 257, 3164, 2848, 281, 29562, 420, 3191, 9441, 300, 309, 311, 1027, 13], "temperature": 0.0, "avg_logprob": -0.11948252178373792, "compression_ratio": 1.7596899224806202, "no_speech_prob": 5.8285208069719374e-05}, {"id": 536, "seek": 214572, "start": 2145.72, "end": 2148.8399999999997, "text": " And I'll show you how that approach works on the next slide.", "tokens": [400, 286, 603, 855, 291, 577, 300, 3109, 1985, 322, 264, 958, 4137, 13], "temperature": 0.0, "avg_logprob": -0.13405179140860574, "compression_ratio": 1.6273764258555132, "no_speech_prob": 2.4298400603584014e-05}, {"id": 537, "seek": 214572, "start": 2148.8399999999997, "end": 2154.9199999999996, "text": " The basic idea is you've got a user interacting with Lambda, which is this big box here.", "tokens": [440, 3875, 1558, 307, 291, 600, 658, 257, 4195, 18017, 365, 45691, 11, 597, 307, 341, 955, 2424, 510, 13], "temperature": 0.0, "avg_logprob": -0.13405179140860574, "compression_ratio": 1.6273764258555132, "no_speech_prob": 2.4298400603584014e-05}, {"id": 538, "seek": 214572, "start": 2154.9199999999996, "end": 2159.48, "text": " And what they've decided to do is have multiple agents inside the big box that can kind of", "tokens": [400, 437, 436, 600, 3047, 281, 360, 307, 362, 3866, 12554, 1854, 264, 955, 2424, 300, 393, 733, 295], "temperature": 0.0, "avg_logprob": -0.13405179140860574, "compression_ratio": 1.6273764258555132, "no_speech_prob": 2.4298400603584014e-05}, {"id": 539, "seek": 214572, "start": 2159.48, "end": 2164.12, "text": " interact with each other and kind of work things out before they give a reply back to the", "tokens": [4648, 365, 1184, 661, 293, 733, 295, 589, 721, 484, 949, 436, 976, 257, 16972, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.13405179140860574, "compression_ratio": 1.6273764258555132, "no_speech_prob": 2.4298400603584014e-05}, {"id": 540, "seek": 214572, "start": 2164.12, "end": 2165.52, "text": " user.", "tokens": [4195, 13], "temperature": 0.0, "avg_logprob": -0.13405179140860574, "compression_ratio": 1.6273764258555132, "no_speech_prob": 2.4298400603584014e-05}, {"id": 541, "seek": 214572, "start": 2165.52, "end": 2167.08, "text": " So here's how it might go.", "tokens": [407, 510, 311, 577, 309, 1062, 352, 13], "temperature": 0.0, "avg_logprob": -0.13405179140860574, "compression_ratio": 1.6273764258555132, "no_speech_prob": 2.4298400603584014e-05}, {"id": 542, "seek": 214572, "start": 2167.08, "end": 2171.9199999999996, "text": " The user says to the base model, one was the Eiffel Tower built.", "tokens": [440, 4195, 1619, 281, 264, 3096, 2316, 11, 472, 390, 264, 462, 3661, 338, 17877, 3094, 13], "temperature": 0.0, "avg_logprob": -0.13405179140860574, "compression_ratio": 1.6273764258555132, "no_speech_prob": 2.4298400603584014e-05}, {"id": 543, "seek": 217192, "start": 2171.92, "end": 2176.4, "text": " And the base model replies it was constructed in 1887.", "tokens": [400, 264, 3096, 2316, 42289, 309, 390, 17083, 294, 2443, 23853, 13], "temperature": 0.0, "avg_logprob": -0.19019211288047047, "compression_ratio": 1.5985401459854014, "no_speech_prob": 2.885093999793753e-05}, {"id": 544, "seek": 217192, "start": 2176.4, "end": 2180.4, "text": " But unlike the basic approach, it doesn't send that response immediately back to the", "tokens": [583, 8343, 264, 3875, 3109, 11, 309, 1177, 380, 2845, 300, 4134, 4258, 646, 281, 264], "temperature": 0.0, "avg_logprob": -0.19019211288047047, "compression_ratio": 1.5985401459854014, "no_speech_prob": 2.885093999793753e-05}, {"id": 545, "seek": 217192, "start": 2180.4, "end": 2181.4, "text": " user.", "tokens": [4195, 13], "temperature": 0.0, "avg_logprob": -0.19019211288047047, "compression_ratio": 1.5985401459854014, "no_speech_prob": 2.885093999793753e-05}, {"id": 546, "seek": 217192, "start": 2181.4, "end": 2185.32, "text": " It actually sends it to this agent called Research.", "tokens": [467, 767, 14790, 309, 281, 341, 9461, 1219, 10303, 13], "temperature": 0.0, "avg_logprob": -0.19019211288047047, "compression_ratio": 1.5985401459854014, "no_speech_prob": 2.885093999793753e-05}, {"id": 547, "seek": 217192, "start": 2185.32, "end": 2189.88, "text": " And then research then takes that information and decides, okay, you know, this claim looks", "tokens": [400, 550, 2132, 550, 2516, 300, 1589, 293, 14898, 11, 1392, 11, 291, 458, 11, 341, 3932, 1542], "temperature": 0.0, "avg_logprob": -0.19019211288047047, "compression_ratio": 1.5985401459854014, "no_speech_prob": 2.885093999793753e-05}, {"id": 548, "seek": 217192, "start": 2189.88, "end": 2192.6800000000003, "text": " a little bit sketchy.", "tokens": [257, 707, 857, 12325, 88, 13], "temperature": 0.0, "avg_logprob": -0.19019211288047047, "compression_ratio": 1.5985401459854014, "no_speech_prob": 2.885093999793753e-05}, {"id": 549, "seek": 217192, "start": 2192.6800000000003, "end": 2195.6, "text": " I'm going to send a query to the search engine.", "tokens": [286, 478, 516, 281, 2845, 257, 14581, 281, 264, 3164, 2848, 13], "temperature": 0.0, "avg_logprob": -0.19019211288047047, "compression_ratio": 1.5985401459854014, "no_speech_prob": 2.885093999793753e-05}, {"id": 550, "seek": 217192, "start": 2195.6, "end": 2200.6, "text": " I'm answer-promorphizing a bunch here, but it just helps with the explanation.", "tokens": [286, 478, 1867, 12, 28722, 18191, 3319, 257, 3840, 510, 11, 457, 309, 445, 3665, 365, 264, 10835, 13], "temperature": 0.0, "avg_logprob": -0.19019211288047047, "compression_ratio": 1.5985401459854014, "no_speech_prob": 2.885093999793753e-05}, {"id": 551, "seek": 220060, "start": 2200.6, "end": 2205.88, "text": " And then the search engine then replies back with the web search results for that query.", "tokens": [400, 550, 264, 3164, 2848, 550, 42289, 646, 365, 264, 3670, 3164, 3542, 337, 300, 14581, 13], "temperature": 0.0, "avg_logprob": -0.12156786874075916, "compression_ratio": 1.7862903225806452, "no_speech_prob": 1.2217544281156734e-05}, {"id": 552, "seek": 220060, "start": 2205.88, "end": 2209.7599999999998, "text": " And in it, we have actually the correct answer, which is 1889.", "tokens": [400, 294, 309, 11, 321, 362, 767, 264, 3006, 1867, 11, 597, 307, 2443, 21115, 13], "temperature": 0.0, "avg_logprob": -0.12156786874075916, "compression_ratio": 1.7862903225806452, "no_speech_prob": 1.2217544281156734e-05}, {"id": 553, "seek": 220060, "start": 2209.7599999999998, "end": 2215.0, "text": " So research then takes that information and produces a new response that has the correct", "tokens": [407, 2132, 550, 2516, 300, 1589, 293, 14725, 257, 777, 4134, 300, 575, 264, 3006], "temperature": 0.0, "avg_logprob": -0.12156786874075916, "compression_ratio": 1.7862903225806452, "no_speech_prob": 1.2217544281156734e-05}, {"id": 554, "seek": 220060, "start": 2215.0, "end": 2217.48, "text": " information and sends that back to the user.", "tokens": [1589, 293, 14790, 300, 646, 281, 264, 4195, 13], "temperature": 0.0, "avg_logprob": -0.12156786874075916, "compression_ratio": 1.7862903225806452, "no_speech_prob": 1.2217544281156734e-05}, {"id": 555, "seek": 220060, "start": 2217.48, "end": 2219.7599999999998, "text": " So that's the overall flow of the approach.", "tokens": [407, 300, 311, 264, 4787, 3095, 295, 264, 3109, 13], "temperature": 0.0, "avg_logprob": -0.12156786874075916, "compression_ratio": 1.7862903225806452, "no_speech_prob": 1.2217544281156734e-05}, {"id": 556, "seek": 220060, "start": 2219.7599999999998, "end": 2224.56, "text": " And you can see that the search engine is able to intervene to fix a model's responses.", "tokens": [400, 291, 393, 536, 300, 264, 3164, 2848, 307, 1075, 281, 30407, 281, 3191, 257, 2316, 311, 13019, 13], "temperature": 0.0, "avg_logprob": -0.12156786874075916, "compression_ratio": 1.7862903225806452, "no_speech_prob": 1.2217544281156734e-05}, {"id": 557, "seek": 220060, "start": 2224.56, "end": 2225.56, "text": " Any questions about that?", "tokens": [2639, 1651, 466, 300, 30], "temperature": 0.0, "avg_logprob": -0.12156786874075916, "compression_ratio": 1.7862903225806452, "no_speech_prob": 1.2217544281156734e-05}, {"id": 558, "seek": 222556, "start": 2225.56, "end": 2239.64, "text": " Okay, yeah, the question was whether this particular flow happens for all questions to the", "tokens": [1033, 11, 1338, 11, 264, 1168, 390, 1968, 341, 1729, 3095, 2314, 337, 439, 1651, 281, 264], "temperature": 0.0, "avg_logprob": -0.14145798032934015, "compression_ratio": 1.6971153846153846, "no_speech_prob": 8.4795115981251e-05}, {"id": 559, "seek": 222556, "start": 2239.64, "end": 2240.64, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.14145798032934015, "compression_ratio": 1.6971153846153846, "no_speech_prob": 8.4795115981251e-05}, {"id": 560, "seek": 222556, "start": 2240.64, "end": 2245.7599999999998, "text": " And actually, as we'll see in a later slide, the model gets to decide who it talks to next.", "tokens": [400, 767, 11, 382, 321, 603, 536, 294, 257, 1780, 4137, 11, 264, 2316, 2170, 281, 4536, 567, 309, 6686, 281, 958, 13], "temperature": 0.0, "avg_logprob": -0.14145798032934015, "compression_ratio": 1.6971153846153846, "no_speech_prob": 8.4795115981251e-05}, {"id": 561, "seek": 222556, "start": 2245.7599999999998, "end": 2250.08, "text": " So base has to decide to talk to research as opposed to talking to the user.", "tokens": [407, 3096, 575, 281, 4536, 281, 751, 281, 2132, 382, 8851, 281, 1417, 281, 264, 4195, 13], "temperature": 0.0, "avg_logprob": -0.14145798032934015, "compression_ratio": 1.6971153846153846, "no_speech_prob": 8.4795115981251e-05}, {"id": 562, "seek": 222556, "start": 2250.08, "end": 2254.6, "text": " So there's a learning process where each agent gets to decide whether to go right back", "tokens": [407, 456, 311, 257, 2539, 1399, 689, 1184, 9461, 2170, 281, 4536, 1968, 281, 352, 558, 646], "temperature": 0.0, "avg_logprob": -0.14145798032934015, "compression_ratio": 1.6971153846153846, "no_speech_prob": 8.4795115981251e-05}, {"id": 563, "seek": 225460, "start": 2254.6, "end": 2257.36, "text": " to the user or to talk to another one of the other agents.", "tokens": [281, 264, 4195, 420, 281, 751, 281, 1071, 472, 295, 264, 661, 12554, 13], "temperature": 0.0, "avg_logprob": -0.29615334669748944, "compression_ratio": 1.3902439024390243, "no_speech_prob": 5.82788088649977e-05}, {"id": 564, "seek": 225460, "start": 2257.36, "end": 2258.36, "text": " Go ahead.", "tokens": [1037, 2286, 13], "temperature": 0.0, "avg_logprob": -0.29615334669748944, "compression_ratio": 1.3902439024390243, "no_speech_prob": 5.82788088649977e-05}, {"id": 565, "seek": 225460, "start": 2258.36, "end": 2283.04, "text": " Okay, yeah, the question is how to limit the amount of information coming back from the", "tokens": [1033, 11, 1338, 11, 264, 1168, 307, 577, 281, 4948, 264, 2372, 295, 1589, 1348, 646, 490, 264], "temperature": 0.0, "avg_logprob": -0.29615334669748944, "compression_ratio": 1.3902439024390243, "no_speech_prob": 5.82788088649977e-05}, {"id": 566, "seek": 225460, "start": 2283.04, "end": 2284.04, "text": " search engine.", "tokens": [3164, 2848, 13], "temperature": 0.0, "avg_logprob": -0.29615334669748944, "compression_ratio": 1.3902439024390243, "no_speech_prob": 5.82788088649977e-05}, {"id": 567, "seek": 228404, "start": 2284.04, "end": 2290.2799999999997, "text": " I think in this particular approach, the search engine returns the first snippet.", "tokens": [286, 519, 294, 341, 1729, 3109, 11, 264, 3164, 2848, 11247, 264, 700, 35623, 302, 13], "temperature": 0.0, "avg_logprob": -0.2874813581767835, "compression_ratio": 1.6723163841807909, "no_speech_prob": 0.000173933818587102}, {"id": 568, "seek": 228404, "start": 2290.2799999999997, "end": 2293.68, "text": " And then if research is still not happy, it asks the same question again.", "tokens": [400, 550, 498, 2132, 307, 920, 406, 2055, 11, 309, 8962, 264, 912, 1168, 797, 13], "temperature": 0.0, "avg_logprob": -0.2874813581767835, "compression_ratio": 1.6723163841807909, "no_speech_prob": 0.000173933818587102}, {"id": 569, "seek": 228404, "start": 2293.68, "end": 2296.7599999999998, "text": " And then they've designed it so the search engine returns the next snippet.", "tokens": [400, 550, 436, 600, 4761, 309, 370, 264, 3164, 2848, 11247, 264, 958, 35623, 302, 13], "temperature": 0.0, "avg_logprob": -0.2874813581767835, "compression_ratio": 1.6723163841807909, "no_speech_prob": 0.000173933818587102}, {"id": 570, "seek": 228404, "start": 2296.7599999999998, "end": 2299.96, "text": " So it just sort of yields control back to the first.", "tokens": [407, 309, 445, 1333, 295, 32168, 1969, 646, 281, 264, 700, 13], "temperature": 0.0, "avg_logprob": -0.2874813581767835, "compression_ratio": 1.6723163841807909, "no_speech_prob": 0.000173933818587102}, {"id": 571, "seek": 228404, "start": 2299.96, "end": 2303.96, "text": " Yeah, yeah.", "tokens": [865, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.2874813581767835, "compression_ratio": 1.6723163841807909, "no_speech_prob": 0.000173933818587102}, {"id": 572, "seek": 230396, "start": 2303.96, "end": 2317.48, "text": " Yeah, okay, so the question is whether research sends any feedback back to the base model", "tokens": [865, 11, 1392, 11, 370, 264, 1168, 307, 1968, 2132, 14790, 604, 5824, 646, 281, 264, 3096, 2316], "temperature": 0.0, "avg_logprob": -0.1955637530745747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 7.25321879144758e-05}, {"id": 573, "seek": 230396, "start": 2317.48, "end": 2318.88, "text": " that it's made of a stake.", "tokens": [300, 309, 311, 1027, 295, 257, 10407, 13], "temperature": 0.0, "avg_logprob": -0.1955637530745747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 7.25321879144758e-05}, {"id": 574, "seek": 230396, "start": 2318.88, "end": 2320.08, "text": " And that's a great idea.", "tokens": [400, 300, 311, 257, 869, 1558, 13], "temperature": 0.0, "avg_logprob": -0.1955637530745747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 7.25321879144758e-05}, {"id": 575, "seek": 230396, "start": 2320.08, "end": 2322.6, "text": " I don't think they do that in the paper.", "tokens": [286, 500, 380, 519, 436, 360, 300, 294, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1955637530745747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 7.25321879144758e-05}, {"id": 576, "seek": 230396, "start": 2322.6, "end": 2327.28, "text": " They just, I mean, research overrides base so the user gets what research said.", "tokens": [814, 445, 11, 286, 914, 11, 2132, 670, 81, 1875, 3096, 370, 264, 4195, 2170, 437, 2132, 848, 13], "temperature": 0.0, "avg_logprob": -0.1955637530745747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 7.25321879144758e-05}, {"id": 577, "seek": 230396, "start": 2327.28, "end": 2330.48, "text": " But base never learns, I think, from what research says.", "tokens": [583, 3096, 1128, 27152, 11, 286, 519, 11, 490, 437, 2132, 1619, 13], "temperature": 0.0, "avg_logprob": -0.1955637530745747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 7.25321879144758e-05}, {"id": 578, "seek": 230396, "start": 2330.48, "end": 2333.52, "text": " So that's a really, yeah, that's a really great point.", "tokens": [407, 300, 311, 257, 534, 11, 1338, 11, 300, 311, 257, 534, 869, 935, 13], "temperature": 0.0, "avg_logprob": -0.1955637530745747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 7.25321879144758e-05}, {"id": 579, "seek": 233352, "start": 2333.52, "end": 2342.08, "text": " Okay, so the question is, why do we need the base model?", "tokens": [1033, 11, 370, 264, 1168, 307, 11, 983, 360, 321, 643, 264, 3096, 2316, 30], "temperature": 0.0, "avg_logprob": -0.18505644798278809, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.330999217927456e-05}, {"id": 580, "seek": 233352, "start": 2342.08, "end": 2343.84, "text": " Yeah, that's a great point too.", "tokens": [865, 11, 300, 311, 257, 869, 935, 886, 13], "temperature": 0.0, "avg_logprob": -0.18505644798278809, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.330999217927456e-05}, {"id": 581, "seek": 233352, "start": 2343.84, "end": 2349.36, "text": " So Lambda is kind of the researchers who developed it cared not just about answering factual", "tokens": [407, 45691, 307, 733, 295, 264, 10309, 567, 4743, 309, 19779, 406, 445, 466, 13430, 48029], "temperature": 0.0, "avg_logprob": -0.18505644798278809, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.330999217927456e-05}, {"id": 582, "seek": 233352, "start": 2349.36, "end": 2352.56, "text": " questions but making it interesting and fun and engaging.", "tokens": [1651, 457, 1455, 309, 1880, 293, 1019, 293, 11268, 13], "temperature": 0.0, "avg_logprob": -0.18505644798278809, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.330999217927456e-05}, {"id": 583, "seek": 233352, "start": 2352.56, "end": 2356.52, "text": " So the base model has a lot of that engaging behavior.", "tokens": [407, 264, 3096, 2316, 575, 257, 688, 295, 300, 11268, 5223, 13], "temperature": 0.0, "avg_logprob": -0.18505644798278809, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.330999217927456e-05}, {"id": 584, "seek": 233352, "start": 2356.52, "end": 2359.68, "text": " And they wanted to preserve that while still preserving factuality.", "tokens": [400, 436, 1415, 281, 15665, 300, 1339, 920, 33173, 48029, 507, 13], "temperature": 0.0, "avg_logprob": -0.18505644798278809, "compression_ratio": 1.6160714285714286, "no_speech_prob": 4.330999217927456e-05}, {"id": 585, "seek": 235968, "start": 2359.68, "end": 2364.2799999999997, "text": " So the other two agents are kind of there to police the base model.", "tokens": [407, 264, 661, 732, 12554, 366, 733, 295, 456, 281, 3804, 264, 3096, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17627316769038406, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.212382241850719e-05}, {"id": 586, "seek": 235968, "start": 2364.2799999999997, "end": 2367.2799999999997, "text": " That's maybe one explanation.", "tokens": [663, 311, 1310, 472, 10835, 13], "temperature": 0.0, "avg_logprob": -0.17627316769038406, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.212382241850719e-05}, {"id": 587, "seek": 235968, "start": 2367.2799999999997, "end": 2371.12, "text": " Oh, right.", "tokens": [876, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.17627316769038406, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.212382241850719e-05}, {"id": 588, "seek": 235968, "start": 2371.12, "end": 2373.2, "text": " Great, great questions.", "tokens": [3769, 11, 869, 1651, 13], "temperature": 0.0, "avg_logprob": -0.17627316769038406, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.212382241850719e-05}, {"id": 589, "seek": 235968, "start": 2373.2, "end": 2376.7599999999998, "text": " So now that we've seen the overall control flow of the model, we can look at how the model", "tokens": [407, 586, 300, 321, 600, 1612, 264, 4787, 1969, 3095, 295, 264, 2316, 11, 321, 393, 574, 412, 577, 264, 2316], "temperature": 0.0, "avg_logprob": -0.17627316769038406, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.212382241850719e-05}, {"id": 590, "seek": 235968, "start": 2376.7599999999998, "end": 2377.7599999999998, "text": " is trained.", "tokens": [307, 8895, 13], "temperature": 0.0, "avg_logprob": -0.17627316769038406, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.212382241850719e-05}, {"id": 591, "seek": 235968, "start": 2377.7599999999998, "end": 2379.24, "text": " And it's actually quite simple.", "tokens": [400, 309, 311, 767, 1596, 2199, 13], "temperature": 0.0, "avg_logprob": -0.17627316769038406, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.212382241850719e-05}, {"id": 592, "seek": 235968, "start": 2379.24, "end": 2382.9199999999996, "text": " So their modeling approach is to just treat everything as dialogue.", "tokens": [407, 641, 15983, 3109, 307, 281, 445, 2387, 1203, 382, 10221, 13], "temperature": 0.0, "avg_logprob": -0.17627316769038406, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.212382241850719e-05}, {"id": 593, "seek": 235968, "start": 2382.9199999999996, "end": 2386.8799999999997, "text": " So let's look at a particular turn of the model's operation.", "tokens": [407, 718, 311, 574, 412, 257, 1729, 1261, 295, 264, 2316, 311, 6916, 13], "temperature": 0.0, "avg_logprob": -0.17627316769038406, "compression_ratio": 1.6229508196721312, "no_speech_prob": 2.212382241850719e-05}, {"id": 594, "seek": 238688, "start": 2386.88, "end": 2392.36, "text": " So there've been a couple turns of conversation so far, which I've listed here.", "tokens": [407, 456, 600, 668, 257, 1916, 4523, 295, 3761, 370, 1400, 11, 597, 286, 600, 10052, 510, 13], "temperature": 0.0, "avg_logprob": -0.11211881302950676, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.6684698241297156e-05}, {"id": 595, "seek": 238688, "start": 2392.36, "end": 2398.76, "text": " And you can see it's just saying who talked and who they talked to.", "tokens": [400, 291, 393, 536, 309, 311, 445, 1566, 567, 2825, 293, 567, 436, 2825, 281, 13], "temperature": 0.0, "avg_logprob": -0.11211881302950676, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.6684698241297156e-05}, {"id": 596, "seek": 238688, "start": 2398.76, "end": 2403.2000000000003, "text": " And the output at this particular time step is just another person to talk and who it", "tokens": [400, 264, 5598, 412, 341, 1729, 565, 1823, 307, 445, 1071, 954, 281, 751, 293, 567, 309], "temperature": 0.0, "avg_logprob": -0.11211881302950676, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.6684698241297156e-05}, {"id": 597, "seek": 238688, "start": 2403.2000000000003, "end": 2404.44, "text": " should talk to.", "tokens": [820, 751, 281, 13], "temperature": 0.0, "avg_logprob": -0.11211881302950676, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.6684698241297156e-05}, {"id": 598, "seek": 238688, "start": 2404.44, "end": 2406.6, "text": " So all of this is just text.", "tokens": [407, 439, 295, 341, 307, 445, 2487, 13], "temperature": 0.0, "avg_logprob": -0.11211881302950676, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.6684698241297156e-05}, {"id": 599, "seek": 238688, "start": 2406.6, "end": 2408.8, "text": " It's text going in, it's text going out.", "tokens": [467, 311, 2487, 516, 294, 11, 309, 311, 2487, 516, 484, 13], "temperature": 0.0, "avg_logprob": -0.11211881302950676, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.6684698241297156e-05}, {"id": 600, "seek": 238688, "start": 2408.8, "end": 2413.8, "text": " So you guys have seen transformer models and basically this fits right into the contract", "tokens": [407, 291, 1074, 362, 1612, 31782, 5245, 293, 1936, 341, 9001, 558, 666, 264, 4364], "temperature": 0.0, "avg_logprob": -0.11211881302950676, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.6684698241297156e-05}, {"id": 601, "seek": 238688, "start": 2413.8, "end": 2416.0, "text": " of a standard transformer model.", "tokens": [295, 257, 3832, 31782, 2316, 13], "temperature": 0.0, "avg_logprob": -0.11211881302950676, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.6684698241297156e-05}, {"id": 602, "seek": 241600, "start": 2416.0, "end": 2420.2, "text": " The only kind of special detail is that when you generate the text, you have to start your", "tokens": [440, 787, 733, 295, 2121, 2607, 307, 300, 562, 291, 8460, 264, 2487, 11, 291, 362, 281, 722, 428], "temperature": 0.0, "avg_logprob": -0.1758831671948703, "compression_ratio": 1.5802919708029197, "no_speech_prob": 3.822507642325945e-05}, {"id": 603, "seek": 241600, "start": 2420.2, "end": 2422.2, "text": " sentence with who you're addressing.", "tokens": [8174, 365, 567, 291, 434, 14329, 13], "temperature": 0.0, "avg_logprob": -0.1758831671948703, "compression_ratio": 1.5802919708029197, "no_speech_prob": 3.822507642325945e-05}, {"id": 604, "seek": 241600, "start": 2422.2, "end": 2430.56, "text": " And that provides the control of which agent responds next.", "tokens": [400, 300, 6417, 264, 1969, 295, 597, 9461, 27331, 958, 13], "temperature": 0.0, "avg_logprob": -0.1758831671948703, "compression_ratio": 1.5802919708029197, "no_speech_prob": 3.822507642325945e-05}, {"id": 605, "seek": 241600, "start": 2430.56, "end": 2432.8, "text": " I already mentioned this.", "tokens": [286, 1217, 2835, 341, 13], "temperature": 0.0, "avg_logprob": -0.1758831671948703, "compression_ratio": 1.5802919708029197, "no_speech_prob": 3.822507642325945e-05}, {"id": 606, "seek": 241600, "start": 2432.8, "end": 2438.24, "text": " And the perhaps the most important question is, okay, we've got this text to text data.", "tokens": [400, 264, 4317, 264, 881, 1021, 1168, 307, 11, 1392, 11, 321, 600, 658, 341, 2487, 281, 2487, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1758831671948703, "compression_ratio": 1.5802919708029197, "no_speech_prob": 3.822507642325945e-05}, {"id": 607, "seek": 241600, "start": 2438.24, "end": 2439.56, "text": " How do we train this model?", "tokens": [1012, 360, 321, 3847, 341, 2316, 30], "temperature": 0.0, "avg_logprob": -0.1758831671948703, "compression_ratio": 1.5802919708029197, "no_speech_prob": 3.822507642325945e-05}, {"id": 608, "seek": 241600, "start": 2439.56, "end": 2442.24, "text": " And the approach in Lambda is actually quite simple, too.", "tokens": [400, 264, 3109, 294, 45691, 307, 767, 1596, 2199, 11, 886, 13], "temperature": 0.0, "avg_logprob": -0.1758831671948703, "compression_ratio": 1.5802919708029197, "no_speech_prob": 3.822507642325945e-05}, {"id": 609, "seek": 241600, "start": 2442.24, "end": 2445.04, "text": " They basically just get human demonstrations.", "tokens": [814, 1936, 445, 483, 1952, 34714, 13], "temperature": 0.0, "avg_logprob": -0.1758831671948703, "compression_ratio": 1.5802919708029197, "no_speech_prob": 3.822507642325945e-05}, {"id": 610, "seek": 244504, "start": 2445.04, "end": 2450.44, "text": " So human crowd workers play the role of user and research in this dialogue.", "tokens": [407, 1952, 6919, 5600, 862, 264, 3090, 295, 4195, 293, 2132, 294, 341, 10221, 13], "temperature": 0.0, "avg_logprob": -0.10857354039731233, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.70451707567554e-05}, {"id": 611, "seek": 244504, "start": 2450.44, "end": 2454.24, "text": " There are people who are looking at the base model's utterances and saying, oh, I don't", "tokens": [821, 366, 561, 567, 366, 1237, 412, 264, 3096, 2316, 311, 17567, 2676, 293, 1566, 11, 1954, 11, 286, 500, 380], "temperature": 0.0, "avg_logprob": -0.10857354039731233, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.70451707567554e-05}, {"id": 612, "seek": 244504, "start": 2454.24, "end": 2455.24, "text": " like that.", "tokens": [411, 300, 13], "temperature": 0.0, "avg_logprob": -0.10857354039731233, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.70451707567554e-05}, {"id": 613, "seek": 244504, "start": 2455.24, "end": 2457.7599999999998, "text": " I think a search query should be sent here.", "tokens": [286, 519, 257, 3164, 14581, 820, 312, 2279, 510, 13], "temperature": 0.0, "avg_logprob": -0.10857354039731233, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.70451707567554e-05}, {"id": 614, "seek": 244504, "start": 2457.7599999999998, "end": 2460.92, "text": " And when the search results come back, they're reading the results and then deciding how", "tokens": [400, 562, 264, 3164, 3542, 808, 646, 11, 436, 434, 3760, 264, 3542, 293, 550, 17990, 577], "temperature": 0.0, "avg_logprob": -0.10857354039731233, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.70451707567554e-05}, {"id": 615, "seek": 244504, "start": 2460.92, "end": 2462.96, "text": " Lambda should respond instead.", "tokens": [45691, 820, 4196, 2602, 13], "temperature": 0.0, "avg_logprob": -0.10857354039731233, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.70451707567554e-05}, {"id": 616, "seek": 244504, "start": 2462.96, "end": 2467.48, "text": " So it's a really elegant and simple approach, but it does require you to have trained crowd", "tokens": [407, 309, 311, 257, 534, 21117, 293, 2199, 3109, 11, 457, 309, 775, 3651, 291, 281, 362, 8895, 6919], "temperature": 0.0, "avg_logprob": -0.10857354039731233, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.70451707567554e-05}, {"id": 617, "seek": 244504, "start": 2467.48, "end": 2471.8, "text": " workers and put in a good amount of budget to get the behavior you want.", "tokens": [5600, 293, 829, 294, 257, 665, 2372, 295, 4706, 281, 483, 264, 5223, 291, 528, 13], "temperature": 0.0, "avg_logprob": -0.10857354039731233, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.70451707567554e-05}, {"id": 618, "seek": 244504, "start": 2471.8, "end": 2474.52, "text": " But still quite impressive that they're able to do this.", "tokens": [583, 920, 1596, 8992, 300, 436, 434, 1075, 281, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.10857354039731233, "compression_ratio": 1.6867469879518073, "no_speech_prob": 3.70451707567554e-05}, {"id": 619, "seek": 247452, "start": 2474.52, "end": 2478.84, "text": " This is a real example, I think, from the paper.", "tokens": [639, 307, 257, 957, 1365, 11, 286, 519, 11, 490, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.20848140901732212, "compression_ratio": 1.6090534979423867, "no_speech_prob": 4.264000745024532e-05}, {"id": 620, "seek": 247452, "start": 2478.84, "end": 2482.28, "text": " Cool, any questions there?", "tokens": [8561, 11, 604, 1651, 456, 30], "temperature": 0.0, "avg_logprob": -0.20848140901732212, "compression_ratio": 1.6090534979423867, "no_speech_prob": 4.264000745024532e-05}, {"id": 621, "seek": 247452, "start": 2482.28, "end": 2483.08, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.20848140901732212, "compression_ratio": 1.6090534979423867, "no_speech_prob": 4.264000745024532e-05}, {"id": 622, "seek": 247452, "start": 2483.08, "end": 2487.56, "text": " So although the approach is simple, it actually achieves quite a bit.", "tokens": [407, 4878, 264, 3109, 307, 2199, 11, 309, 767, 3538, 977, 1596, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.20848140901732212, "compression_ratio": 1.6090534979423867, "no_speech_prob": 4.264000745024532e-05}, {"id": 623, "seek": 247452, "start": 2487.56, "end": 2492.56, "text": " So the model learns to reformulate the previous terms of the conversation as a query that can", "tokens": [407, 264, 2316, 27152, 281, 8290, 5256, 264, 3894, 2115, 295, 264, 3761, 382, 257, 14581, 300, 393], "temperature": 0.0, "avg_logprob": -0.20848140901732212, "compression_ratio": 1.6090534979423867, "no_speech_prob": 4.264000745024532e-05}, {"id": 624, "seek": 247452, "start": 2492.56, "end": 2494.48, "text": " go into Google search.", "tokens": [352, 666, 3329, 3164, 13], "temperature": 0.0, "avg_logprob": -0.20848140901732212, "compression_ratio": 1.6090534979423867, "no_speech_prob": 4.264000745024532e-05}, {"id": 625, "seek": 247452, "start": 2494.48, "end": 2498.6, "text": " So it's kind of shoe-horning the problem into something that Google search or some kind", "tokens": [407, 309, 311, 733, 295, 12796, 12, 2335, 773, 264, 1154, 666, 746, 300, 3329, 3164, 420, 512, 733], "temperature": 0.0, "avg_logprob": -0.20848140901732212, "compression_ratio": 1.6090534979423867, "no_speech_prob": 4.264000745024532e-05}, {"id": 626, "seek": 247452, "start": 2498.6, "end": 2500.52, "text": " of web search can understand.", "tokens": [295, 3670, 3164, 393, 1223, 13], "temperature": 0.0, "avg_logprob": -0.20848140901732212, "compression_ratio": 1.6090534979423867, "no_speech_prob": 4.264000745024532e-05}, {"id": 627, "seek": 250052, "start": 2500.52, "end": 2505.24, "text": " And then it's learning also from human demonstrations how to incorporate the knowledge from the search", "tokens": [400, 550, 309, 311, 2539, 611, 490, 1952, 34714, 577, 281, 16091, 264, 3601, 490, 264, 3164], "temperature": 0.0, "avg_logprob": -0.1384935615476498, "compression_ratio": 1.7290969899665551, "no_speech_prob": 2.2471269403467886e-05}, {"id": 628, "seek": 250052, "start": 2505.24, "end": 2509.88, "text": " results back into the utterance that it's putting out.", "tokens": [3542, 646, 666, 264, 17567, 719, 300, 309, 311, 3372, 484, 13], "temperature": 0.0, "avg_logprob": -0.1384935615476498, "compression_ratio": 1.7290969899665551, "no_speech_prob": 2.2471269403467886e-05}, {"id": 629, "seek": 250052, "start": 2509.88, "end": 2514.68, "text": " And just because this work also came out around the same time and also very exciting, I also", "tokens": [400, 445, 570, 341, 589, 611, 1361, 484, 926, 264, 912, 565, 293, 611, 588, 4670, 11, 286, 611], "temperature": 0.0, "avg_logprob": -0.1384935615476498, "compression_ratio": 1.7290969899665551, "no_speech_prob": 2.2471269403467886e-05}, {"id": 630, "seek": 250052, "start": 2514.68, "end": 2520.36, "text": " point you to WebGPT, which is another model that learns to use web search.", "tokens": [935, 291, 281, 9573, 38, 47, 51, 11, 597, 307, 1071, 2316, 300, 27152, 281, 764, 3670, 3164, 13], "temperature": 0.0, "avg_logprob": -0.1384935615476498, "compression_ratio": 1.7290969899665551, "no_speech_prob": 2.2471269403467886e-05}, {"id": 631, "seek": 250052, "start": 2520.36, "end": 2524.84, "text": " In their case, they provide human demonstrators with an actual UI and have human demonstrators", "tokens": [682, 641, 1389, 11, 436, 2893, 1952, 5516, 34886, 365, 364, 3539, 15682, 293, 362, 1952, 5516, 34886], "temperature": 0.0, "avg_logprob": -0.1384935615476498, "compression_ratio": 1.7290969899665551, "no_speech_prob": 2.2471269403467886e-05}, {"id": 632, "seek": 250052, "start": 2524.84, "end": 2525.92, "text": " use that.", "tokens": [764, 300, 13], "temperature": 0.0, "avg_logprob": -0.1384935615476498, "compression_ratio": 1.7290969899665551, "no_speech_prob": 2.2471269403467886e-05}, {"id": 633, "seek": 250052, "start": 2525.92, "end": 2529.68, "text": " But they ultimately, I think, convert the history of actions that the user takes again", "tokens": [583, 436, 6284, 11, 286, 519, 11, 7620, 264, 2503, 295, 5909, 300, 264, 4195, 2516, 797], "temperature": 0.0, "avg_logprob": -0.1384935615476498, "compression_ratio": 1.7290969899665551, "no_speech_prob": 2.2471269403467886e-05}, {"id": 634, "seek": 252968, "start": 2529.68, "end": 2536.3999999999996, "text": " into a piece of text that the model then simply consumes and uses to predict the next action.", "tokens": [666, 257, 2522, 295, 2487, 300, 264, 2316, 550, 2935, 48823, 293, 4960, 281, 6069, 264, 958, 3069, 13], "temperature": 0.0, "avg_logprob": -0.12391611933708191, "compression_ratio": 1.7312252964426877, "no_speech_prob": 5.737384708481841e-05}, {"id": 635, "seek": 252968, "start": 2536.3999999999996, "end": 2540.56, "text": " The additional thing here is that they use reinforcement learning to then fine tune their", "tokens": [440, 4497, 551, 510, 307, 300, 436, 764, 29280, 2539, 281, 550, 2489, 10864, 641], "temperature": 0.0, "avg_logprob": -0.12391611933708191, "compression_ratio": 1.7312252964426877, "no_speech_prob": 5.737384708481841e-05}, {"id": 636, "seek": 252968, "start": 2540.56, "end": 2544.3999999999996, "text": " system further on top of what they learned from human demonstrations.", "tokens": [1185, 3052, 322, 1192, 295, 437, 436, 3264, 490, 1952, 34714, 13], "temperature": 0.0, "avg_logprob": -0.12391611933708191, "compression_ratio": 1.7312252964426877, "no_speech_prob": 5.737384708481841e-05}, {"id": 637, "seek": 252968, "start": 2544.3999999999996, "end": 2546.7999999999997, "text": " And that's something worth checking out as well.", "tokens": [400, 300, 311, 746, 3163, 8568, 484, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12391611933708191, "compression_ratio": 1.7312252964426877, "no_speech_prob": 5.737384708481841e-05}, {"id": 638, "seek": 252968, "start": 2546.7999999999997, "end": 2551.8799999999997, "text": " So the main takeaway for this little section here is that many external retrieval tools", "tokens": [407, 264, 2135, 30681, 337, 341, 707, 3541, 510, 307, 300, 867, 8320, 19817, 3337, 3873], "temperature": 0.0, "avg_logprob": -0.12391611933708191, "compression_ratio": 1.7312252964426877, "no_speech_prob": 5.737384708481841e-05}, {"id": 639, "seek": 252968, "start": 2551.8799999999997, "end": 2555.64, "text": " accept text as input and return text as output.", "tokens": [3241, 2487, 382, 4846, 293, 2736, 2487, 382, 5598, 13], "temperature": 0.0, "avg_logprob": -0.12391611933708191, "compression_ratio": 1.7312252964426877, "no_speech_prob": 5.737384708481841e-05}, {"id": 640, "seek": 255564, "start": 2555.64, "end": 2559.72, "text": " So if you want to have an external memory and interface with one of these things, all", "tokens": [407, 498, 291, 528, 281, 362, 364, 8320, 4675, 293, 9226, 365, 472, 295, 613, 721, 11, 439], "temperature": 0.0, "avg_logprob": -0.11515485445658366, "compression_ratio": 1.7048611111111112, "no_speech_prob": 2.7965797926299274e-05}, {"id": 641, "seek": 255564, "start": 2559.72, "end": 2564.7999999999997, "text": " the task really boils down to is learning to generate text queries to that external tool", "tokens": [264, 5633, 534, 35049, 760, 281, 307, 2539, 281, 8460, 2487, 24109, 281, 300, 8320, 2290], "temperature": 0.0, "avg_logprob": -0.11515485445658366, "compression_ratio": 1.7048611111111112, "no_speech_prob": 2.7965797926299274e-05}, {"id": 642, "seek": 255564, "start": 2564.7999999999997, "end": 2568.96, "text": " and then learning to understand the text output of the tool.", "tokens": [293, 550, 2539, 281, 1223, 264, 2487, 5598, 295, 264, 2290, 13], "temperature": 0.0, "avg_logprob": -0.11515485445658366, "compression_ratio": 1.7048611111111112, "no_speech_prob": 2.7965797926299274e-05}, {"id": 643, "seek": 255564, "start": 2568.96, "end": 2574.24, "text": " And both of these tasks can be handled by standard off-the-shelf tools that all of you are already", "tokens": [400, 1293, 295, 613, 9608, 393, 312, 18033, 538, 3832, 766, 12, 3322, 12, 46626, 3873, 300, 439, 295, 291, 366, 1217], "temperature": 0.0, "avg_logprob": -0.11515485445658366, "compression_ratio": 1.7048611111111112, "no_speech_prob": 2.7965797926299274e-05}, {"id": 644, "seek": 255564, "start": 2574.24, "end": 2577.56, "text": " familiar with from previous lectures.", "tokens": [4963, 365, 490, 3894, 16564, 13], "temperature": 0.0, "avg_logprob": -0.11515485445658366, "compression_ratio": 1.7048611111111112, "no_speech_prob": 2.7965797926299274e-05}, {"id": 645, "seek": 255564, "start": 2577.56, "end": 2582.0, "text": " As long as you have demonstrations for how to do that.", "tokens": [1018, 938, 382, 291, 362, 34714, 337, 577, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.11515485445658366, "compression_ratio": 1.7048611111111112, "no_speech_prob": 2.7965797926299274e-05}, {"id": 646, "seek": 255564, "start": 2582.0, "end": 2585.0, "text": " Or if you're able to do RL training, which we won't cover here.", "tokens": [1610, 498, 291, 434, 1075, 281, 360, 497, 43, 3097, 11, 597, 321, 1582, 380, 2060, 510, 13], "temperature": 0.0, "avg_logprob": -0.11515485445658366, "compression_ratio": 1.7048611111111112, "no_speech_prob": 2.7965797926299274e-05}, {"id": 647, "seek": 258500, "start": 2585.0, "end": 2588.28, "text": " So that's the overview of how to use external search tools.", "tokens": [407, 300, 311, 264, 12492, 295, 577, 281, 764, 8320, 3164, 3873, 13], "temperature": 0.0, "avg_logprob": -0.15717609112079328, "compression_ratio": 1.7651245551601424, "no_speech_prob": 2.5067862225114368e-05}, {"id": 648, "seek": 258500, "start": 2588.28, "end": 2593.6, "text": " You can imagine that if you had a database instead of a web search, you could provide demonstrations", "tokens": [509, 393, 3811, 300, 498, 291, 632, 257, 8149, 2602, 295, 257, 3670, 3164, 11, 291, 727, 2893, 34714], "temperature": 0.0, "avg_logprob": -0.15717609112079328, "compression_ratio": 1.7651245551601424, "no_speech_prob": 2.5067862225114368e-05}, {"id": 649, "seek": 258500, "start": 2593.6, "end": 2600.8, "text": " of how to write SQL queries to that database or any other sort of tool that you could imagine.", "tokens": [295, 577, 281, 2464, 19200, 24109, 281, 300, 8149, 420, 604, 661, 1333, 295, 2290, 300, 291, 727, 3811, 13], "temperature": 0.0, "avg_logprob": -0.15717609112079328, "compression_ratio": 1.7651245551601424, "no_speech_prob": 2.5067862225114368e-05}, {"id": 650, "seek": 258500, "start": 2600.8, "end": 2602.04, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.15717609112079328, "compression_ratio": 1.7651245551601424, "no_speech_prob": 2.5067862225114368e-05}, {"id": 651, "seek": 258500, "start": 2602.04, "end": 2605.72, "text": " So at this point, you might say, all right, we can query web search and web search is very", "tokens": [407, 412, 341, 935, 11, 291, 1062, 584, 11, 439, 558, 11, 321, 393, 14581, 3670, 3164, 293, 3670, 3164, 307, 588], "temperature": 0.0, "avg_logprob": -0.15717609112079328, "compression_ratio": 1.7651245551601424, "no_speech_prob": 2.5067862225114368e-05}, {"id": 652, "seek": 258500, "start": 2605.72, "end": 2606.72, "text": " powerful.", "tokens": [4005, 13], "temperature": 0.0, "avg_logprob": -0.15717609112079328, "compression_ratio": 1.7651245551601424, "no_speech_prob": 2.5067862225114368e-05}, {"id": 653, "seek": 258500, "start": 2606.72, "end": 2608.84, "text": " So why would we use anything else?", "tokens": [407, 983, 576, 321, 764, 1340, 1646, 30], "temperature": 0.0, "avg_logprob": -0.15717609112079328, "compression_ratio": 1.7651245551601424, "no_speech_prob": 2.5067862225114368e-05}, {"id": 654, "seek": 258500, "start": 2608.84, "end": 2611.08, "text": " And to that, I have a couple responses.", "tokens": [400, 281, 300, 11, 286, 362, 257, 1916, 13019, 13], "temperature": 0.0, "avg_logprob": -0.15717609112079328, "compression_ratio": 1.7651245551601424, "no_speech_prob": 2.5067862225114368e-05}, {"id": 655, "seek": 258500, "start": 2611.08, "end": 2614.76, "text": " So first of all, web search is just far from perfect.", "tokens": [407, 700, 295, 439, 11, 3670, 3164, 307, 445, 1400, 490, 2176, 13], "temperature": 0.0, "avg_logprob": -0.15717609112079328, "compression_ratio": 1.7651245551601424, "no_speech_prob": 2.5067862225114368e-05}, {"id": 656, "seek": 261476, "start": 2614.76, "end": 2617.5600000000004, "text": " And the reason it says good as it is today is because of research.", "tokens": [400, 264, 1778, 309, 1619, 665, 382, 309, 307, 965, 307, 570, 295, 2132, 13], "temperature": 0.0, "avg_logprob": -0.1269053010379567, "compression_ratio": 1.8608058608058609, "no_speech_prob": 3.943561387131922e-05}, {"id": 657, "seek": 261476, "start": 2617.5600000000004, "end": 2618.5600000000004, "text": " And we're here to do research.", "tokens": [400, 321, 434, 510, 281, 360, 2132, 13], "temperature": 0.0, "avg_logprob": -0.1269053010379567, "compression_ratio": 1.8608058608058609, "no_speech_prob": 3.943561387131922e-05}, {"id": 658, "seek": 261476, "start": 2618.5600000000004, "end": 2624.76, "text": " So if you're just going to rely on web search being good, that sort of defeats the point.", "tokens": [407, 498, 291, 434, 445, 516, 281, 10687, 322, 3670, 3164, 885, 665, 11, 300, 1333, 295, 7486, 1720, 264, 935, 13], "temperature": 0.0, "avg_logprob": -0.1269053010379567, "compression_ratio": 1.8608058608058609, "no_speech_prob": 3.943561387131922e-05}, {"id": 659, "seek": 261476, "start": 2624.76, "end": 2627.48, "text": " If you don't believe me, try some of these queries.", "tokens": [759, 291, 500, 380, 1697, 385, 11, 853, 512, 295, 613, 24109, 13], "temperature": 0.0, "avg_logprob": -0.1269053010379567, "compression_ratio": 1.8608058608058609, "no_speech_prob": 3.943561387131922e-05}, {"id": 660, "seek": 261476, "start": 2627.48, "end": 2631.8, "text": " So if you search for a famous lawyer who got into car accident, you will find that all", "tokens": [407, 498, 291, 3164, 337, 257, 4618, 11613, 567, 658, 666, 1032, 6398, 11, 291, 486, 915, 300, 439], "temperature": 0.0, "avg_logprob": -0.1269053010379567, "compression_ratio": 1.8608058608058609, "no_speech_prob": 3.943561387131922e-05}, {"id": 661, "seek": 261476, "start": 2631.8, "end": 2636.2000000000003, "text": " the results are about lawyers you can call if you get into a car accident.", "tokens": [264, 3542, 366, 466, 16219, 291, 393, 818, 498, 291, 483, 666, 257, 1032, 6398, 13], "temperature": 0.0, "avg_logprob": -0.1269053010379567, "compression_ratio": 1.8608058608058609, "no_speech_prob": 3.943561387131922e-05}, {"id": 662, "seek": 261476, "start": 2636.2000000000003, "end": 2640.6000000000004, "text": " If you search for a use NLP to parse research papers, you will find a bunch of research papers", "tokens": [759, 291, 3164, 337, 257, 764, 426, 45196, 281, 48377, 2132, 10577, 11, 291, 486, 915, 257, 3840, 295, 2132, 10577], "temperature": 0.0, "avg_logprob": -0.1269053010379567, "compression_ratio": 1.8608058608058609, "no_speech_prob": 3.943561387131922e-05}, {"id": 663, "seek": 261476, "start": 2640.6000000000004, "end": 2641.92, "text": " on parsing.", "tokens": [322, 21156, 278, 13], "temperature": 0.0, "avg_logprob": -0.1269053010379567, "compression_ratio": 1.8608058608058609, "no_speech_prob": 3.943561387131922e-05}, {"id": 664, "seek": 264192, "start": 2641.92, "end": 2646.2000000000003, "text": " And after doing a few of these, the illusion of web search working really well kind of fades", "tokens": [400, 934, 884, 257, 1326, 295, 613, 11, 264, 18854, 295, 3670, 3164, 1364, 534, 731, 733, 295, 32679], "temperature": 0.0, "avg_logprob": -0.09297273870100055, "compression_ratio": 1.6950354609929077, "no_speech_prob": 1.2802764103980735e-05}, {"id": 665, "seek": 264192, "start": 2646.2000000000003, "end": 2648.0, "text": " away a little bit.", "tokens": [1314, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.09297273870100055, "compression_ratio": 1.6950354609929077, "no_speech_prob": 1.2802764103980735e-05}, {"id": 666, "seek": 264192, "start": 2648.0, "end": 2652.44, "text": " And also, if you speak a language other than English, you might find that search performance", "tokens": [400, 611, 11, 498, 291, 1710, 257, 2856, 661, 813, 3669, 11, 291, 1062, 915, 300, 3164, 3389], "temperature": 0.0, "avg_logprob": -0.09297273870100055, "compression_ratio": 1.6950354609929077, "no_speech_prob": 1.2802764103980735e-05}, {"id": 667, "seek": 264192, "start": 2652.44, "end": 2655.44, "text": " in different languages is really not quite the same.", "tokens": [294, 819, 8650, 307, 534, 406, 1596, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.09297273870100055, "compression_ratio": 1.6950354609929077, "no_speech_prob": 1.2802764103980735e-05}, {"id": 668, "seek": 264192, "start": 2655.44, "end": 2659.4, "text": " So there's still a lot to do to improve retrieval in web search.", "tokens": [407, 456, 311, 920, 257, 688, 281, 360, 281, 3470, 19817, 3337, 294, 3670, 3164, 13], "temperature": 0.0, "avg_logprob": -0.09297273870100055, "compression_ratio": 1.6950354609929077, "no_speech_prob": 1.2802764103980735e-05}, {"id": 669, "seek": 264192, "start": 2659.4, "end": 2666.48, "text": " And I sort of consider web search to be just sort of a component inside the larger, bigger", "tokens": [400, 286, 1333, 295, 1949, 3670, 3164, 281, 312, 445, 1333, 295, 257, 6542, 1854, 264, 4833, 11, 3801], "temperature": 0.0, "avg_logprob": -0.09297273870100055, "compression_ratio": 1.6950354609929077, "no_speech_prob": 1.2802764103980735e-05}, {"id": 670, "seek": 264192, "start": 2666.48, "end": 2671.7200000000003, "text": " set of things that memory augmented models could potentially do.", "tokens": [992, 295, 721, 300, 4675, 36155, 5245, 727, 7263, 360, 13], "temperature": 0.0, "avg_logprob": -0.09297273870100055, "compression_ratio": 1.6950354609929077, "no_speech_prob": 1.2802764103980735e-05}, {"id": 671, "seek": 267172, "start": 2671.72, "end": 2675.52, "text": " And second of all, just the plain API of web search isn't designed to handle everything", "tokens": [400, 1150, 295, 439, 11, 445, 264, 11121, 9362, 295, 3670, 3164, 1943, 380, 4761, 281, 4813, 1203], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 672, "seek": 267172, "start": 2675.52, "end": 2677.08, "text": " you might want to do.", "tokens": [291, 1062, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 673, "seek": 267172, "start": 2677.08, "end": 2681.7599999999998, "text": " So you could imagine a doctor given a medical image might want to retrieve similar images", "tokens": [407, 291, 727, 3811, 257, 4631, 2212, 257, 4625, 3256, 1062, 528, 281, 30254, 2531, 5267], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 674, "seek": 267172, "start": 2681.7599999999998, "end": 2683.4399999999996, "text": " from a medical textbook.", "tokens": [490, 257, 4625, 25591, 13], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 675, "seek": 267172, "start": 2683.4399999999996, "end": 2687.16, "text": " That's not quite something that web search is cut out to do right now.", "tokens": [663, 311, 406, 1596, 746, 300, 3670, 3164, 307, 1723, 484, 281, 360, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 676, "seek": 267172, "start": 2687.16, "end": 2690.48, "text": " Or if you're a programmer who's been given a programming challenge, you might want to retrieve", "tokens": [1610, 498, 291, 434, 257, 32116, 567, 311, 668, 2212, 257, 9410, 3430, 11, 291, 1062, 528, 281, 30254], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 677, "seek": 267172, "start": 2690.48, "end": 2692.16, "text": " relevant algorithms.", "tokens": [7340, 14642, 13], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 678, "seek": 267172, "start": 2692.16, "end": 2694.16, "text": " Also something web search doesn't do.", "tokens": [2743, 746, 3670, 3164, 1177, 380, 360, 13], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 679, "seek": 267172, "start": 2694.16, "end": 2696.9199999999996, "text": " If you're in fashion, if you're given three pieces of clothing, can you retrieve another", "tokens": [759, 291, 434, 294, 6700, 11, 498, 291, 434, 2212, 1045, 3755, 295, 11502, 11, 393, 291, 30254, 1071], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 680, "seek": 267172, "start": 2696.9199999999996, "end": 2699.56, "text": " piece of clothing that completes your outfit?", "tokens": [2522, 295, 11502, 300, 36362, 428, 11263, 30], "temperature": 0.0, "avg_logprob": -0.1346022060939244, "compression_ratio": 1.865814696485623, "no_speech_prob": 1.1840112165373284e-05}, {"id": 681, "seek": 269956, "start": 2699.56, "end": 2703.32, "text": " Or if you're a novelist and you're given a story, retrieve other stories that have the", "tokens": [1610, 498, 291, 434, 257, 7613, 468, 293, 291, 434, 2212, 257, 1657, 11, 30254, 661, 3676, 300, 362, 264], "temperature": 0.0, "avg_logprob": -0.12536486053466797, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.5444591554114595e-05}, {"id": 682, "seek": 269956, "start": 2703.32, "end": 2705.12, "text": " same plot.", "tokens": [912, 7542, 13], "temperature": 0.0, "avg_logprob": -0.12536486053466797, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.5444591554114595e-05}, {"id": 683, "seek": 269956, "start": 2705.12, "end": 2709.16, "text": " Or if you're a journalist, if you're given a claim, retrieve and use articles that refute", "tokens": [1610, 498, 291, 434, 257, 17277, 11, 498, 291, 434, 2212, 257, 3932, 11, 30254, 293, 764, 11290, 300, 1895, 1169], "temperature": 0.0, "avg_logprob": -0.12536486053466797, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.5444591554114595e-05}, {"id": 684, "seek": 269956, "start": 2709.16, "end": 2710.72, "text": " or contradicted.", "tokens": [420, 15858, 11254, 13], "temperature": 0.0, "avg_logprob": -0.12536486053466797, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.5444591554114595e-05}, {"id": 685, "seek": 269956, "start": 2710.72, "end": 2714.56, "text": " These are all retrieval tasks that would be quite useful, but existing search tools just", "tokens": [1981, 366, 439, 19817, 3337, 9608, 300, 576, 312, 1596, 4420, 11, 457, 6741, 3164, 3873, 445], "temperature": 0.0, "avg_logprob": -0.12536486053466797, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.5444591554114595e-05}, {"id": 686, "seek": 269956, "start": 2714.56, "end": 2715.7999999999997, "text": " don't handle.", "tokens": [500, 380, 4813, 13], "temperature": 0.0, "avg_logprob": -0.12536486053466797, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.5444591554114595e-05}, {"id": 687, "seek": 269956, "start": 2715.7999999999997, "end": 2720.12, "text": " And so these are all reasons why I think the retrieval problem is still interesting to", "tokens": [400, 370, 613, 366, 439, 4112, 983, 286, 519, 264, 19817, 3337, 1154, 307, 920, 1880, 281], "temperature": 0.0, "avg_logprob": -0.12536486053466797, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.5444591554114595e-05}, {"id": 688, "seek": 269956, "start": 2720.12, "end": 2722.16, "text": " look at.", "tokens": [574, 412, 13], "temperature": 0.0, "avg_logprob": -0.12536486053466797, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.5444591554114595e-05}, {"id": 689, "seek": 269956, "start": 2722.16, "end": 2725.7599999999998, "text": " And a third and final point is that web search only accesses public data.", "tokens": [400, 257, 2636, 293, 2572, 935, 307, 300, 3670, 3164, 787, 2105, 279, 1908, 1412, 13], "temperature": 0.0, "avg_logprob": -0.12536486053466797, "compression_ratio": 1.7798507462686568, "no_speech_prob": 1.5444591554114595e-05}, {"id": 690, "seek": 272576, "start": 2725.76, "end": 2730.28, "text": " So if you have any task that doesn't condition on public data, you're still going to need", "tokens": [407, 498, 291, 362, 604, 5633, 300, 1177, 380, 4188, 322, 1908, 1412, 11, 291, 434, 920, 516, 281, 643], "temperature": 0.0, "avg_logprob": -0.1319614604667381, "compression_ratio": 1.6790123456790123, "no_speech_prob": 2.6271110982634127e-05}, {"id": 691, "seek": 272576, "start": 2730.28, "end": 2733.32, "text": " a retriever of your own.", "tokens": [257, 19817, 331, 295, 428, 1065, 13], "temperature": 0.0, "avg_logprob": -0.1319614604667381, "compression_ratio": 1.6790123456790123, "no_speech_prob": 2.6271110982634127e-05}, {"id": 692, "seek": 272576, "start": 2733.32, "end": 2734.32, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.1319614604667381, "compression_ratio": 1.6790123456790123, "no_speech_prob": 2.6271110982634127e-05}, {"id": 693, "seek": 272576, "start": 2734.32, "end": 2738.44, "text": " So with that, we'll turn to the next part of the talk, which is how to train your own", "tokens": [407, 365, 300, 11, 321, 603, 1261, 281, 264, 958, 644, 295, 264, 751, 11, 597, 307, 577, 281, 3847, 428, 1065], "temperature": 0.0, "avg_logprob": -0.1319614604667381, "compression_ratio": 1.6790123456790123, "no_speech_prob": 2.6271110982634127e-05}, {"id": 694, "seek": 272576, "start": 2738.44, "end": 2744.1200000000003, "text": " neural retriever, which is something that I find very interesting.", "tokens": [18161, 19817, 331, 11, 597, 307, 746, 300, 286, 915, 588, 1880, 13], "temperature": 0.0, "avg_logprob": -0.1319614604667381, "compression_ratio": 1.6790123456790123, "no_speech_prob": 2.6271110982634127e-05}, {"id": 695, "seek": 272576, "start": 2744.1200000000003, "end": 2751.0, "text": " So we'll start by giving an anatomy of a neural retriever kind of similar to what we showed", "tokens": [407, 321, 603, 722, 538, 2902, 364, 31566, 295, 257, 18161, 19817, 331, 733, 295, 2531, 281, 437, 321, 4712], "temperature": 0.0, "avg_logprob": -0.1319614604667381, "compression_ratio": 1.6790123456790123, "no_speech_prob": 2.6271110982634127e-05}, {"id": 696, "seek": 272576, "start": 2751.0, "end": 2753.5600000000004, "text": " for feedforward networks and transformers.", "tokens": [337, 3154, 13305, 9590, 293, 4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.1319614604667381, "compression_ratio": 1.6790123456790123, "no_speech_prob": 2.6271110982634127e-05}, {"id": 697, "seek": 275356, "start": 2753.56, "end": 2756.48, "text": " We're going to go with this key value type of interpretation.", "tokens": [492, 434, 516, 281, 352, 365, 341, 2141, 2158, 2010, 295, 14174, 13], "temperature": 0.0, "avg_logprob": -0.1291122169138115, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.8855523851234466e-05}, {"id": 698, "seek": 275356, "start": 2756.48, "end": 2759.84, "text": " So you have a set of keys paired with a set of values.", "tokens": [407, 291, 362, 257, 992, 295, 9317, 25699, 365, 257, 992, 295, 4190, 13], "temperature": 0.0, "avg_logprob": -0.1291122169138115, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.8855523851234466e-05}, {"id": 699, "seek": 275356, "start": 2759.84, "end": 2765.0, "text": " And given some input, you're going to compute some sort of similarity score between the", "tokens": [400, 2212, 512, 4846, 11, 291, 434, 516, 281, 14722, 512, 1333, 295, 32194, 6175, 1296, 264], "temperature": 0.0, "avg_logprob": -0.1291122169138115, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.8855523851234466e-05}, {"id": 700, "seek": 275356, "start": 2765.0, "end": 2767.2799999999997, "text": " input and each of the keys.", "tokens": [4846, 293, 1184, 295, 264, 9317, 13], "temperature": 0.0, "avg_logprob": -0.1291122169138115, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.8855523851234466e-05}, {"id": 701, "seek": 275356, "start": 2767.2799999999997, "end": 2773.88, "text": " And once you've computed that score, you basically want to return the value associated with", "tokens": [400, 1564, 291, 600, 40610, 300, 6175, 11, 291, 1936, 528, 281, 2736, 264, 2158, 6615, 365], "temperature": 0.0, "avg_logprob": -0.1291122169138115, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.8855523851234466e-05}, {"id": 702, "seek": 275356, "start": 2773.88, "end": 2775.88, "text": " the highest scoring key.", "tokens": [264, 6343, 22358, 2141, 13], "temperature": 0.0, "avg_logprob": -0.1291122169138115, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.8855523851234466e-05}, {"id": 703, "seek": 275356, "start": 2775.88, "end": 2782.24, "text": " Or you could return the values for the top K high scoring keys or any other metric.", "tokens": [1610, 291, 727, 2736, 264, 4190, 337, 264, 1192, 591, 1090, 22358, 9317, 420, 604, 661, 20678, 13], "temperature": 0.0, "avg_logprob": -0.1291122169138115, "compression_ratio": 1.8041666666666667, "no_speech_prob": 2.8855523851234466e-05}, {"id": 704, "seek": 278224, "start": 2782.24, "end": 2785.8799999999997, "text": " So to just kind of ground this example, the input could be something like Iple Tower", "tokens": [407, 281, 445, 733, 295, 2727, 341, 1365, 11, 264, 4846, 727, 312, 746, 411, 286, 781, 17877], "temperature": 0.0, "avg_logprob": -0.16175202665657834, "compression_ratio": 1.7798507462686568, "no_speech_prob": 5.3379899327410385e-06}, {"id": 705, "seek": 278224, "start": 2785.8799999999997, "end": 2786.8799999999997, "text": " Location.", "tokens": [12859, 399, 13], "temperature": 0.0, "avg_logprob": -0.16175202665657834, "compression_ratio": 1.7798507462686568, "no_speech_prob": 5.3379899327410385e-06}, {"id": 706, "seek": 278224, "start": 2786.8799999999997, "end": 2791.64, "text": " The keys could be titles of documents and the values could be the corresponding text associated", "tokens": [440, 9317, 727, 312, 12992, 295, 8512, 293, 264, 4190, 727, 312, 264, 11760, 2487, 6615], "temperature": 0.0, "avg_logprob": -0.16175202665657834, "compression_ratio": 1.7798507462686568, "no_speech_prob": 5.3379899327410385e-06}, {"id": 707, "seek": 278224, "start": 2791.64, "end": 2794.3599999999997, "text": " with the document.", "tokens": [365, 264, 4166, 13], "temperature": 0.0, "avg_logprob": -0.16175202665657834, "compression_ratio": 1.7798507462686568, "no_speech_prob": 5.3379899327410385e-06}, {"id": 708, "seek": 278224, "start": 2794.3599999999997, "end": 2799.7999999999997, "text": " And the basic takeaway here is just that a retriever is really just a function that takes", "tokens": [400, 264, 3875, 30681, 510, 307, 445, 300, 257, 19817, 331, 307, 534, 445, 257, 2445, 300, 2516], "temperature": 0.0, "avg_logprob": -0.16175202665657834, "compression_ratio": 1.7798507462686568, "no_speech_prob": 5.3379899327410385e-06}, {"id": 709, "seek": 278224, "start": 2799.7999999999997, "end": 2803.0, "text": " some input and a key and produces a score.", "tokens": [512, 4846, 293, 257, 2141, 293, 14725, 257, 6175, 13], "temperature": 0.0, "avg_logprob": -0.16175202665657834, "compression_ratio": 1.7798507462686568, "no_speech_prob": 5.3379899327410385e-06}, {"id": 710, "seek": 278224, "start": 2803.0, "end": 2805.3199999999997, "text": " Once you have that, you basically have a retriever.", "tokens": [3443, 291, 362, 300, 11, 291, 1936, 362, 257, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.16175202665657834, "compression_ratio": 1.7798507462686568, "no_speech_prob": 5.3379899327410385e-06}, {"id": 711, "seek": 278224, "start": 2805.3199999999997, "end": 2811.4799999999996, "text": " You score all the memories and then you take the ones that have the highest score.", "tokens": [509, 6175, 439, 264, 8495, 293, 550, 291, 747, 264, 2306, 300, 362, 264, 6343, 6175, 13], "temperature": 0.0, "avg_logprob": -0.16175202665657834, "compression_ratio": 1.7798507462686568, "no_speech_prob": 5.3379899327410385e-06}, {"id": 712, "seek": 281148, "start": 2811.48, "end": 2815.2400000000002, "text": " For the remaining slides, I'll actually go with a slightly simplified set of.", "tokens": [1171, 264, 8877, 9788, 11, 286, 603, 767, 352, 365, 257, 4748, 26335, 992, 295, 13], "temperature": 0.0, "avg_logprob": -0.13600285847981772, "compression_ratio": 1.7279151943462898, "no_speech_prob": 1.750213596096728e-05}, {"id": 713, "seek": 281148, "start": 2815.2400000000002, "end": 2819.16, "text": " Because in many tasks, there's really no distinction between the keys and the value.", "tokens": [1436, 294, 867, 9608, 11, 456, 311, 534, 572, 16844, 1296, 264, 9317, 293, 264, 2158, 13], "temperature": 0.0, "avg_logprob": -0.13600285847981772, "compression_ratio": 1.7279151943462898, "no_speech_prob": 1.750213596096728e-05}, {"id": 714, "seek": 281148, "start": 2819.16, "end": 2820.8, "text": " Sometimes they're just the same thing.", "tokens": [4803, 436, 434, 445, 264, 912, 551, 13], "temperature": 0.0, "avg_logprob": -0.13600285847981772, "compression_ratio": 1.7279151943462898, "no_speech_prob": 1.750213596096728e-05}, {"id": 715, "seek": 281148, "start": 2820.8, "end": 2824.4, "text": " So for example, with Wikipedia documents, you just take the whole text as the key and", "tokens": [407, 337, 1365, 11, 365, 28999, 8512, 11, 291, 445, 747, 264, 1379, 2487, 382, 264, 2141, 293], "temperature": 0.0, "avg_logprob": -0.13600285847981772, "compression_ratio": 1.7279151943462898, "no_speech_prob": 1.750213596096728e-05}, {"id": 716, "seek": 281148, "start": 2824.4, "end": 2826.76, "text": " the value.", "tokens": [264, 2158, 13], "temperature": 0.0, "avg_logprob": -0.13600285847981772, "compression_ratio": 1.7279151943462898, "no_speech_prob": 1.750213596096728e-05}, {"id": 717, "seek": 281148, "start": 2826.76, "end": 2832.08, "text": " And so I'll go with this simplified schematic where you're just computing scores against", "tokens": [400, 370, 286, 603, 352, 365, 341, 26335, 44739, 689, 291, 434, 445, 15866, 13444, 1970], "temperature": 0.0, "avg_logprob": -0.13600285847981772, "compression_ratio": 1.7279151943462898, "no_speech_prob": 1.750213596096728e-05}, {"id": 718, "seek": 281148, "start": 2832.08, "end": 2833.44, "text": " what I'll call memories.", "tokens": [437, 286, 603, 818, 8495, 13], "temperature": 0.0, "avg_logprob": -0.13600285847981772, "compression_ratio": 1.7279151943462898, "no_speech_prob": 1.750213596096728e-05}, {"id": 719, "seek": 281148, "start": 2833.44, "end": 2838.16, "text": " And the highest scoring memory is what's returned from the memory retriever.", "tokens": [400, 264, 6343, 22358, 4675, 307, 437, 311, 8752, 490, 264, 4675, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.13600285847981772, "compression_ratio": 1.7279151943462898, "no_speech_prob": 1.750213596096728e-05}, {"id": 720, "seek": 283816, "start": 2838.16, "end": 2841.48, "text": " And we'll go with this formulation that the retriever is just a function that takes the", "tokens": [400, 321, 603, 352, 365, 341, 37642, 300, 264, 19817, 331, 307, 445, 257, 2445, 300, 2516, 264], "temperature": 0.0, "avg_logprob": -0.1654948694952603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.384379811497638e-05}, {"id": 721, "seek": 283816, "start": 2841.48, "end": 2845.7599999999998, "text": " input and the memory and produces a score.", "tokens": [4846, 293, 264, 4675, 293, 14725, 257, 6175, 13], "temperature": 0.0, "avg_logprob": -0.1654948694952603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.384379811497638e-05}, {"id": 722, "seek": 283816, "start": 2845.7599999999998, "end": 2849.6, "text": " Okay, so I've said it's just a function, but what sort of functions do people actually", "tokens": [1033, 11, 370, 286, 600, 848, 309, 311, 445, 257, 2445, 11, 457, 437, 1333, 295, 6828, 360, 561, 767], "temperature": 0.0, "avg_logprob": -0.1654948694952603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.384379811497638e-05}, {"id": 723, "seek": 283816, "start": 2849.6, "end": 2851.7599999999998, "text": " use in practice to compute this score?", "tokens": [764, 294, 3124, 281, 14722, 341, 6175, 30], "temperature": 0.0, "avg_logprob": -0.1654948694952603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.384379811497638e-05}, {"id": 724, "seek": 283816, "start": 2851.7599999999998, "end": 2855.68, "text": " And the answer here is not too surprising.", "tokens": [400, 264, 1867, 510, 307, 406, 886, 8830, 13], "temperature": 0.0, "avg_logprob": -0.1654948694952603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.384379811497638e-05}, {"id": 725, "seek": 283816, "start": 2855.68, "end": 2859.8799999999997, "text": " You guys have seen Bert and other sorts of transformers in previous classes.", "tokens": [509, 1074, 362, 1612, 29594, 293, 661, 7527, 295, 4088, 433, 294, 3894, 5359, 13], "temperature": 0.0, "avg_logprob": -0.1654948694952603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.384379811497638e-05}, {"id": 726, "seek": 283816, "start": 2859.8799999999997, "end": 2861.48, "text": " It's a very flexible model class.", "tokens": [467, 311, 257, 588, 11358, 2316, 1508, 13], "temperature": 0.0, "avg_logprob": -0.1654948694952603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.384379811497638e-05}, {"id": 727, "seek": 283816, "start": 2861.48, "end": 2864.64, "text": " I'm trying not to introduce kind of unnecessary complexity.", "tokens": [286, 478, 1382, 406, 281, 5366, 733, 295, 19350, 14024, 13], "temperature": 0.0, "avg_logprob": -0.1654948694952603, "compression_ratio": 1.6785714285714286, "no_speech_prob": 1.384379811497638e-05}, {"id": 728, "seek": 286464, "start": 2864.64, "end": 2869.64, "text": " So we'll just go with a Bert model that takes the input and the memory.", "tokens": [407, 321, 603, 445, 352, 365, 257, 29594, 2316, 300, 2516, 264, 4846, 293, 264, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12217456953866142, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.468120146659203e-05}, {"id": 729, "seek": 286464, "start": 2869.64, "end": 2875.44, "text": " And you put a some sort of regression layer on top of the output layer of Bert.", "tokens": [400, 291, 829, 257, 512, 1333, 295, 24590, 4583, 322, 1192, 295, 264, 5598, 4583, 295, 29594, 13], "temperature": 0.0, "avg_logprob": -0.12217456953866142, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.468120146659203e-05}, {"id": 730, "seek": 286464, "start": 2875.44, "end": 2880.4, "text": " So maybe the CLS token embedding of Bert, you put a regression layer on top and", "tokens": [407, 1310, 264, 12855, 50, 14862, 12240, 3584, 295, 29594, 11, 291, 829, 257, 24590, 4583, 322, 1192, 293], "temperature": 0.0, "avg_logprob": -0.12217456953866142, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.468120146659203e-05}, {"id": 731, "seek": 286464, "start": 2880.4, "end": 2883.72, "text": " it produces some float valued score.", "tokens": [309, 14725, 512, 15706, 22608, 6175, 13], "temperature": 0.0, "avg_logprob": -0.12217456953866142, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.468120146659203e-05}, {"id": 732, "seek": 286464, "start": 2883.72, "end": 2885.48, "text": " And this whole thing is differentiable.", "tokens": [400, 341, 1379, 551, 307, 819, 9364, 13], "temperature": 0.0, "avg_logprob": -0.12217456953866142, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.468120146659203e-05}, {"id": 733, "seek": 286464, "start": 2885.48, "end": 2887.48, "text": " So the regression layer is differentiable.", "tokens": [407, 264, 24590, 4583, 307, 819, 9364, 13], "temperature": 0.0, "avg_logprob": -0.12217456953866142, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.468120146659203e-05}, {"id": 734, "seek": 286464, "start": 2887.48, "end": 2888.68, "text": " Bert is differentiable.", "tokens": [29594, 307, 819, 9364, 13], "temperature": 0.0, "avg_logprob": -0.12217456953866142, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.468120146659203e-05}, {"id": 735, "seek": 286464, "start": 2888.68, "end": 2892.52, "text": " This gives you basically a neural network that produces a score.", "tokens": [639, 2709, 291, 1936, 257, 18161, 3209, 300, 14725, 257, 6175, 13], "temperature": 0.0, "avg_logprob": -0.12217456953866142, "compression_ratio": 1.8803418803418803, "no_speech_prob": 2.468120146659203e-05}, {"id": 736, "seek": 289252, "start": 2892.52, "end": 2896.88, "text": " And the advantages are you get this very powerful model that's comparing the input against", "tokens": [400, 264, 14906, 366, 291, 483, 341, 588, 4005, 2316, 300, 311, 15763, 264, 4846, 1970], "temperature": 0.0, "avg_logprob": -0.11963179960089215, "compression_ratio": 1.7992565055762082, "no_speech_prob": 1.5445104509126395e-05}, {"id": 737, "seek": 289252, "start": 2896.88, "end": 2898.96, "text": " the memory and it's differentiable.", "tokens": [264, 4675, 293, 309, 311, 819, 9364, 13], "temperature": 0.0, "avg_logprob": -0.11963179960089215, "compression_ratio": 1.7992565055762082, "no_speech_prob": 1.5445104509126395e-05}, {"id": 738, "seek": 289252, "start": 2898.96, "end": 2900.28, "text": " So all of that is good.", "tokens": [407, 439, 295, 300, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.11963179960089215, "compression_ratio": 1.7992565055762082, "no_speech_prob": 1.5445104509126395e-05}, {"id": 739, "seek": 289252, "start": 2900.28, "end": 2906.48, "text": " The disadvantage of this approach is that if you have millions of memories, then every", "tokens": [440, 24292, 295, 341, 3109, 307, 300, 498, 291, 362, 6803, 295, 8495, 11, 550, 633], "temperature": 0.0, "avg_logprob": -0.11963179960089215, "compression_ratio": 1.7992565055762082, "no_speech_prob": 1.5445104509126395e-05}, {"id": 740, "seek": 289252, "start": 2906.48, "end": 2910.56, "text": " time a new input comes in, in order to retrieve a memory, you have to run this computation", "tokens": [565, 257, 777, 4846, 1487, 294, 11, 294, 1668, 281, 30254, 257, 4675, 11, 291, 362, 281, 1190, 341, 24903], "temperature": 0.0, "avg_logprob": -0.11963179960089215, "compression_ratio": 1.7992565055762082, "no_speech_prob": 1.5445104509126395e-05}, {"id": 741, "seek": 289252, "start": 2910.56, "end": 2913.52, "text": " against all one million of the memories.", "tokens": [1970, 439, 472, 2459, 295, 264, 8495, 13], "temperature": 0.0, "avg_logprob": -0.11963179960089215, "compression_ratio": 1.7992565055762082, "no_speech_prob": 1.5445104509126395e-05}, {"id": 742, "seek": 289252, "start": 2913.52, "end": 2918.12, "text": " So that's just way too expensive to do if you're thinking about something like all of Wikipedia", "tokens": [407, 300, 311, 445, 636, 886, 5124, 281, 360, 498, 291, 434, 1953, 466, 746, 411, 439, 295, 28999], "temperature": 0.0, "avg_logprob": -0.11963179960089215, "compression_ratio": 1.7992565055762082, "no_speech_prob": 1.5445104509126395e-05}, {"id": 743, "seek": 289252, "start": 2918.12, "end": 2919.88, "text": " or all of the web.", "tokens": [420, 439, 295, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.11963179960089215, "compression_ratio": 1.7992565055762082, "no_speech_prob": 1.5445104509126395e-05}, {"id": 744, "seek": 291988, "start": 2919.88, "end": 2927.32, "text": " So next we'll turn to a different architecture that is more commonly used for retrieval on", "tokens": [407, 958, 321, 603, 1261, 281, 257, 819, 9482, 300, 307, 544, 12719, 1143, 337, 19817, 3337, 322], "temperature": 0.0, "avg_logprob": -0.12265016555786133, "compression_ratio": 1.7478632478632479, "no_speech_prob": 8.52958419272909e-06}, {"id": 745, "seek": 291988, "start": 2927.32, "end": 2929.44, "text": " the next slide.", "tokens": [264, 958, 4137, 13], "temperature": 0.0, "avg_logprob": -0.12265016555786133, "compression_ratio": 1.7478632478632479, "no_speech_prob": 8.52958419272909e-06}, {"id": 746, "seek": 291988, "start": 2929.44, "end": 2931.4, "text": " So it's very similar.", "tokens": [407, 309, 311, 588, 2531, 13], "temperature": 0.0, "avg_logprob": -0.12265016555786133, "compression_ratio": 1.7478632478632479, "no_speech_prob": 8.52958419272909e-06}, {"id": 747, "seek": 291988, "start": 2931.4, "end": 2933.08, "text": " The Bert picture comes up again.", "tokens": [440, 29594, 3036, 1487, 493, 797, 13], "temperature": 0.0, "avg_logprob": -0.12265016555786133, "compression_ratio": 1.7478632478632479, "no_speech_prob": 8.52958419272909e-06}, {"id": 748, "seek": 291988, "start": 2933.08, "end": 2937.36, "text": " This time what we're doing is we're taking the input and feeding only the input into the", "tokens": [639, 565, 437, 321, 434, 884, 307, 321, 434, 1940, 264, 4846, 293, 12919, 787, 264, 4846, 666, 264], "temperature": 0.0, "avg_logprob": -0.12265016555786133, "compression_ratio": 1.7478632478632479, "no_speech_prob": 8.52958419272909e-06}, {"id": 749, "seek": 291988, "start": 2937.36, "end": 2941.52, "text": " transformer to produce a single vector that we'll call the input vector.", "tokens": [31782, 281, 5258, 257, 2167, 8062, 300, 321, 603, 818, 264, 4846, 8062, 13], "temperature": 0.0, "avg_logprob": -0.12265016555786133, "compression_ratio": 1.7478632478632479, "no_speech_prob": 8.52958419272909e-06}, {"id": 750, "seek": 291988, "start": 2941.52, "end": 2946.36, "text": " And then we'll have a separate transformer encode each memory separately to produce a", "tokens": [400, 550, 321, 603, 362, 257, 4994, 31782, 2058, 1429, 1184, 4675, 14759, 281, 5258, 257], "temperature": 0.0, "avg_logprob": -0.12265016555786133, "compression_ratio": 1.7478632478632479, "no_speech_prob": 8.52958419272909e-06}, {"id": 751, "seek": 294636, "start": 2946.36, "end": 2952.4, "text": " memory vector for each memory and then the relevant score between the input and the memory", "tokens": [4675, 8062, 337, 1184, 4675, 293, 550, 264, 7340, 6175, 1296, 264, 4846, 293, 264, 4675], "temperature": 0.0, "avg_logprob": -0.12772074411081713, "compression_ratio": 1.7175925925925926, "no_speech_prob": 7.141086825868115e-05}, {"id": 752, "seek": 294636, "start": 2952.4, "end": 2956.8, "text": " is just the dot product of these two vectors.", "tokens": [307, 445, 264, 5893, 1674, 295, 613, 732, 18875, 13], "temperature": 0.0, "avg_logprob": -0.12772074411081713, "compression_ratio": 1.7175925925925926, "no_speech_prob": 7.141086825868115e-05}, {"id": 753, "seek": 294636, "start": 2956.8, "end": 2960.88, "text": " It could be the dot product, it could be co-science similarity, just any function that you can", "tokens": [467, 727, 312, 264, 5893, 1674, 11, 309, 727, 312, 598, 12, 82, 6699, 32194, 11, 445, 604, 2445, 300, 291, 393], "temperature": 0.0, "avg_logprob": -0.12772074411081713, "compression_ratio": 1.7175925925925926, "no_speech_prob": 7.141086825868115e-05}, {"id": 754, "seek": 294636, "start": 2960.88, "end": 2964.32, "text": " efficiently compute between two vectors.", "tokens": [19621, 14722, 1296, 732, 18875, 13], "temperature": 0.0, "avg_logprob": -0.12772074411081713, "compression_ratio": 1.7175925925925926, "no_speech_prob": 7.141086825868115e-05}, {"id": 755, "seek": 294636, "start": 2964.32, "end": 2967.36, "text": " So why are we proposing this instead?", "tokens": [407, 983, 366, 321, 29939, 341, 2602, 30], "temperature": 0.0, "avg_logprob": -0.12772074411081713, "compression_ratio": 1.7175925925925926, "no_speech_prob": 7.141086825868115e-05}, {"id": 756, "seek": 294636, "start": 2967.36, "end": 2970.84, "text": " This has a couple advantages over the previous architecture.", "tokens": [639, 575, 257, 1916, 14906, 670, 264, 3894, 9482, 13], "temperature": 0.0, "avg_logprob": -0.12772074411081713, "compression_ratio": 1.7175925925925926, "no_speech_prob": 7.141086825868115e-05}, {"id": 757, "seek": 297084, "start": 2970.84, "end": 2978.4, "text": " The first is that you can run this side of the model, the right side, on all of the memories", "tokens": [440, 700, 307, 300, 291, 393, 1190, 341, 1252, 295, 264, 2316, 11, 264, 558, 1252, 11, 322, 439, 295, 264, 8495], "temperature": 0.0, "avg_logprob": -0.09042343379944329, "compression_ratio": 1.8674698795180722, "no_speech_prob": 4.5394535845844075e-05}, {"id": 758, "seek": 297084, "start": 2978.4, "end": 2979.4, "text": " in advance.", "tokens": [294, 7295, 13], "temperature": 0.0, "avg_logprob": -0.09042343379944329, "compression_ratio": 1.8674698795180722, "no_speech_prob": 4.5394535845844075e-05}, {"id": 759, "seek": 297084, "start": 2979.4, "end": 2983.6000000000004, "text": " So before any inputs even come in, you can just pre-compute the memory vector for each", "tokens": [407, 949, 604, 15743, 754, 808, 294, 11, 291, 393, 445, 659, 12, 21541, 1169, 264, 4675, 8062, 337, 1184], "temperature": 0.0, "avg_logprob": -0.09042343379944329, "compression_ratio": 1.8674698795180722, "no_speech_prob": 4.5394535845844075e-05}, {"id": 760, "seek": 297084, "start": 2983.6000000000004, "end": 2984.6000000000004, "text": " thing.", "tokens": [551, 13], "temperature": 0.0, "avg_logprob": -0.09042343379944329, "compression_ratio": 1.8674698795180722, "no_speech_prob": 4.5394535845844075e-05}, {"id": 761, "seek": 297084, "start": 2984.6000000000004, "end": 2988.6800000000003, "text": " So if it's Wikipedia, you can produce a vector for every document in Wikipedia.", "tokens": [407, 498, 309, 311, 28999, 11, 291, 393, 5258, 257, 8062, 337, 633, 4166, 294, 28999, 13], "temperature": 0.0, "avg_logprob": -0.09042343379944329, "compression_ratio": 1.8674698795180722, "no_speech_prob": 4.5394535845844075e-05}, {"id": 762, "seek": 297084, "start": 2988.6800000000003, "end": 2992.36, "text": " And when a new memory comes in, you don't have to redo that work.", "tokens": [400, 562, 257, 777, 4675, 1487, 294, 11, 291, 500, 380, 362, 281, 29956, 300, 589, 13], "temperature": 0.0, "avg_logprob": -0.09042343379944329, "compression_ratio": 1.8674698795180722, "no_speech_prob": 4.5394535845844075e-05}, {"id": 763, "seek": 297084, "start": 2992.36, "end": 2994.84, "text": " So that saves a lot of compute.", "tokens": [407, 300, 19155, 257, 688, 295, 14722, 13], "temperature": 0.0, "avg_logprob": -0.09042343379944329, "compression_ratio": 1.8674698795180722, "no_speech_prob": 4.5394535845844075e-05}, {"id": 764, "seek": 297084, "start": 2994.84, "end": 2998.84, "text": " The only thing that you need to do when a new input comes in is you need to compute this", "tokens": [440, 787, 551, 300, 291, 643, 281, 360, 562, 257, 777, 4846, 1487, 294, 307, 291, 643, 281, 14722, 341], "temperature": 0.0, "avg_logprob": -0.09042343379944329, "compression_ratio": 1.8674698795180722, "no_speech_prob": 4.5394535845844075e-05}, {"id": 765, "seek": 299884, "start": 2998.84, "end": 3003.36, "text": " input vector and then do dot products against all of the memories.", "tokens": [4846, 8062, 293, 550, 360, 5893, 3383, 1970, 439, 295, 264, 8495, 13], "temperature": 0.0, "avg_logprob": -0.1423131608471428, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.108175148256123e-05}, {"id": 766, "seek": 299884, "start": 3003.36, "end": 3008.44, "text": " So dot products are cheap and can happen much more quickly than running an entire BERT", "tokens": [407, 5893, 3383, 366, 7084, 293, 393, 1051, 709, 544, 2661, 813, 2614, 364, 2302, 363, 31479], "temperature": 0.0, "avg_logprob": -0.1423131608471428, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.108175148256123e-05}, {"id": 767, "seek": 299884, "start": 3008.44, "end": 3009.6800000000003, "text": " model over again.", "tokens": [2316, 670, 797, 13], "temperature": 0.0, "avg_logprob": -0.1423131608471428, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.108175148256123e-05}, {"id": 768, "seek": 299884, "start": 3009.6800000000003, "end": 3014.28, "text": " And that's the fundamental savings that you get from using a model like this.", "tokens": [400, 300, 311, 264, 8088, 13454, 300, 291, 483, 490, 1228, 257, 2316, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1423131608471428, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.108175148256123e-05}, {"id": 769, "seek": 299884, "start": 3014.28, "end": 3018.96, "text": " Something that we won't cover in as much detail here is that for the dot product, there", "tokens": [6595, 300, 321, 1582, 380, 2060, 294, 382, 709, 2607, 510, 307, 300, 337, 264, 5893, 1674, 11, 456], "temperature": 0.0, "avg_logprob": -0.1423131608471428, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.108175148256123e-05}, {"id": 770, "seek": 299884, "start": 3018.96, "end": 3026.32, "text": " are also fast nearest neighbors algorithms that let you efficiently find the memory vectors", "tokens": [366, 611, 2370, 23831, 12512, 14642, 300, 718, 291, 19621, 915, 264, 4675, 18875], "temperature": 0.0, "avg_logprob": -0.1423131608471428, "compression_ratio": 1.6956521739130435, "no_speech_prob": 6.108175148256123e-05}, {"id": 771, "seek": 302632, "start": 3026.32, "end": 3031.2400000000002, "text": " that have the highest dot product with the input vector without actually computing over", "tokens": [300, 362, 264, 6343, 5893, 1674, 365, 264, 4846, 8062, 1553, 767, 15866, 670], "temperature": 0.0, "avg_logprob": -0.11183834075927734, "compression_ratio": 1.8644067796610169, "no_speech_prob": 3.426477633183822e-05}, {"id": 772, "seek": 302632, "start": 3031.2400000000002, "end": 3032.4, "text": " all of the memory vectors.", "tokens": [439, 295, 264, 4675, 18875, 13], "temperature": 0.0, "avg_logprob": -0.11183834075927734, "compression_ratio": 1.8644067796610169, "no_speech_prob": 3.426477633183822e-05}, {"id": 773, "seek": 302632, "start": 3032.4, "end": 3036.92, "text": " So it's a sublinear search algorithm that allows you to find it.", "tokens": [407, 309, 311, 257, 1422, 28263, 3164, 9284, 300, 4045, 291, 281, 915, 309, 13], "temperature": 0.0, "avg_logprob": -0.11183834075927734, "compression_ratio": 1.8644067796610169, "no_speech_prob": 3.426477633183822e-05}, {"id": 774, "seek": 302632, "start": 3036.92, "end": 3041.0, "text": " And the basic intuition there, at least there are a couple of them, is that you can take", "tokens": [400, 264, 3875, 24002, 456, 11, 412, 1935, 456, 366, 257, 1916, 295, 552, 11, 307, 300, 291, 393, 747], "temperature": 0.0, "avg_logprob": -0.11183834075927734, "compression_ratio": 1.8644067796610169, "no_speech_prob": 3.426477633183822e-05}, {"id": 775, "seek": 302632, "start": 3041.0, "end": 3045.6400000000003, "text": " your set of memory vectors and build some sort of tree structure over them, kind of organizing", "tokens": [428, 992, 295, 4675, 18875, 293, 1322, 512, 1333, 295, 4230, 3877, 670, 552, 11, 733, 295, 17608], "temperature": 0.0, "avg_logprob": -0.11183834075927734, "compression_ratio": 1.8644067796610169, "no_speech_prob": 3.426477633183822e-05}, {"id": 776, "seek": 302632, "start": 3045.6400000000003, "end": 3046.6400000000003, "text": " them spatially.", "tokens": [552, 15000, 2270, 13], "temperature": 0.0, "avg_logprob": -0.11183834075927734, "compression_ratio": 1.8644067796610169, "no_speech_prob": 3.426477633183822e-05}, {"id": 777, "seek": 302632, "start": 3046.6400000000003, "end": 3050.6800000000003, "text": " And once you've built that tree structure when the new input vector comes in, you can", "tokens": [400, 1564, 291, 600, 3094, 300, 4230, 3877, 562, 264, 777, 4846, 8062, 1487, 294, 11, 291, 393], "temperature": 0.0, "avg_logprob": -0.11183834075927734, "compression_ratio": 1.8644067796610169, "no_speech_prob": 3.426477633183822e-05}, {"id": 778, "seek": 302632, "start": 3050.6800000000003, "end": 3054.4, "text": " essentially kind of traverse down that tree to find the things that are most similar", "tokens": [4476, 733, 295, 45674, 760, 300, 4230, 281, 915, 264, 721, 300, 366, 881, 2531], "temperature": 0.0, "avg_logprob": -0.11183834075927734, "compression_ratio": 1.8644067796610169, "no_speech_prob": 3.426477633183822e-05}, {"id": 779, "seek": 305440, "start": 3054.4, "end": 3057.76, "text": " without computing dot products with everything else.", "tokens": [1553, 15866, 5893, 3383, 365, 1203, 1646, 13], "temperature": 0.0, "avg_logprob": -0.11679432081139605, "compression_ratio": 1.79136690647482, "no_speech_prob": 2.5069195544347167e-05}, {"id": 780, "seek": 305440, "start": 3057.76, "end": 3061.84, "text": " There are other algorithms too that use hashing and other techniques, but they'll be out", "tokens": [821, 366, 661, 14642, 886, 300, 764, 575, 571, 293, 661, 7512, 11, 457, 436, 603, 312, 484], "temperature": 0.0, "avg_logprob": -0.11679432081139605, "compression_ratio": 1.79136690647482, "no_speech_prob": 2.5069195544347167e-05}, {"id": 781, "seek": 305440, "start": 3061.84, "end": 3066.32, "text": " of the scope for today's class.", "tokens": [295, 264, 11923, 337, 965, 311, 1508, 13], "temperature": 0.0, "avg_logprob": -0.11679432081139605, "compression_ratio": 1.79136690647482, "no_speech_prob": 2.5069195544347167e-05}, {"id": 782, "seek": 305440, "start": 3066.32, "end": 3070.1600000000003, "text": " And the other good property here is that all of this is still differentiable, so you can", "tokens": [400, 264, 661, 665, 4707, 510, 307, 300, 439, 295, 341, 307, 920, 819, 9364, 11, 370, 291, 393], "temperature": 0.0, "avg_logprob": -0.11679432081139605, "compression_ratio": 1.79136690647482, "no_speech_prob": 2.5069195544347167e-05}, {"id": 783, "seek": 305440, "start": 3070.1600000000003, "end": 3073.76, "text": " still train this thing with gradient descent like anything else.", "tokens": [920, 3847, 341, 551, 365, 16235, 23475, 411, 1340, 1646, 13], "temperature": 0.0, "avg_logprob": -0.11679432081139605, "compression_ratio": 1.79136690647482, "no_speech_prob": 2.5069195544347167e-05}, {"id": 784, "seek": 305440, "start": 3073.76, "end": 3078.44, "text": " The main disadvantage of this approach is also kind of due to its advantage, which is", "tokens": [440, 2135, 24292, 295, 341, 3109, 307, 611, 733, 295, 3462, 281, 1080, 5002, 11, 597, 307], "temperature": 0.0, "avg_logprob": -0.11679432081139605, "compression_ratio": 1.79136690647482, "no_speech_prob": 2.5069195544347167e-05}, {"id": 785, "seek": 305440, "start": 3078.44, "end": 3082.76, "text": " that all of the expressiveness of this model has to go through that one dot product.", "tokens": [300, 439, 295, 264, 5109, 8477, 295, 341, 2316, 575, 281, 352, 807, 300, 472, 5893, 1674, 13], "temperature": 0.0, "avg_logprob": -0.11679432081139605, "compression_ratio": 1.79136690647482, "no_speech_prob": 2.5069195544347167e-05}, {"id": 786, "seek": 308276, "start": 3082.76, "end": 3086.1600000000003, "text": " So anything you want to remember about the input or anything you want to remember about", "tokens": [407, 1340, 291, 528, 281, 1604, 466, 264, 4846, 420, 1340, 291, 528, 281, 1604, 466], "temperature": 0.0, "avg_logprob": -0.09789794986530886, "compression_ratio": 1.933085501858736, "no_speech_prob": 2.0143390429439023e-05}, {"id": 787, "seek": 308276, "start": 3086.1600000000003, "end": 3091.6400000000003, "text": " the memory, all has to get squeezed into that one memory vector and that one input vector.", "tokens": [264, 4675, 11, 439, 575, 281, 483, 39470, 666, 300, 472, 4675, 8062, 293, 300, 472, 4846, 8062, 13], "temperature": 0.0, "avg_logprob": -0.09789794986530886, "compression_ratio": 1.933085501858736, "no_speech_prob": 2.0143390429439023e-05}, {"id": 788, "seek": 308276, "start": 3091.6400000000003, "end": 3097.48, "text": " And that's a bottleneck that kind of researchers have been dealing with in recent research.", "tokens": [400, 300, 311, 257, 44641, 547, 300, 733, 295, 10309, 362, 668, 6260, 365, 294, 5162, 2132, 13], "temperature": 0.0, "avg_logprob": -0.09789794986530886, "compression_ratio": 1.933085501858736, "no_speech_prob": 2.0143390429439023e-05}, {"id": 789, "seek": 308276, "start": 3097.48, "end": 3100.2400000000002, "text": " What you'll find is that there are a lot of approaches that try to strike some kind of", "tokens": [708, 291, 603, 915, 307, 300, 456, 366, 257, 688, 295, 11587, 300, 853, 281, 9302, 512, 733, 295], "temperature": 0.0, "avg_logprob": -0.09789794986530886, "compression_ratio": 1.933085501858736, "no_speech_prob": 2.0143390429439023e-05}, {"id": 790, "seek": 308276, "start": 3100.2400000000002, "end": 3103.88, "text": " balance between this approach and the approach on the previous slide.", "tokens": [4772, 1296, 341, 3109, 293, 264, 3109, 322, 264, 3894, 4137, 13], "temperature": 0.0, "avg_logprob": -0.09789794986530886, "compression_ratio": 1.933085501858736, "no_speech_prob": 2.0143390429439023e-05}, {"id": 791, "seek": 308276, "start": 3103.88, "end": 3108.6400000000003, "text": " So a common thing to do is to use this approach to retrieve a top set of candidates and then", "tokens": [407, 257, 2689, 551, 281, 360, 307, 281, 764, 341, 3109, 281, 30254, 257, 1192, 992, 295, 11255, 293, 550], "temperature": 0.0, "avg_logprob": -0.09789794986530886, "compression_ratio": 1.933085501858736, "no_speech_prob": 2.0143390429439023e-05}, {"id": 792, "seek": 310864, "start": 3108.64, "end": 3113.7599999999998, "text": " run a more complex model like the one on the previous slide to rescore and re-rank the", "tokens": [1190, 257, 544, 3997, 2316, 411, 264, 472, 322, 264, 3894, 4137, 281, 9610, 418, 293, 319, 12, 20479, 264], "temperature": 0.0, "avg_logprob": -0.25212750227554986, "compression_ratio": 1.5807860262008733, "no_speech_prob": 1.4283722521213349e-05}, {"id": 793, "seek": 310864, "start": 3113.7599999999998, "end": 3116.3599999999997, "text": " candidates proposed by the first model.", "tokens": [11255, 10348, 538, 264, 700, 2316, 13], "temperature": 0.0, "avg_logprob": -0.25212750227554986, "compression_ratio": 1.5807860262008733, "no_speech_prob": 1.4283722521213349e-05}, {"id": 794, "seek": 310864, "start": 3116.3599999999997, "end": 3121.3199999999997, "text": " You'll also find techniques that try to take the memory and produce five vectors and", "tokens": [509, 603, 611, 915, 7512, 300, 853, 281, 747, 264, 4675, 293, 5258, 1732, 18875, 293], "temperature": 0.0, "avg_logprob": -0.25212750227554986, "compression_ratio": 1.5807860262008733, "no_speech_prob": 1.4283722521213349e-05}, {"id": 795, "seek": 310864, "start": 3121.3199999999997, "end": 3125.08, "text": " then use all five of those to somehow compute a score.", "tokens": [550, 764, 439, 1732, 295, 729, 281, 6063, 14722, 257, 6175, 13], "temperature": 0.0, "avg_logprob": -0.25212750227554986, "compression_ratio": 1.5807860262008733, "no_speech_prob": 1.4283722521213349e-05}, {"id": 796, "seek": 310864, "start": 3125.08, "end": 3129.0, "text": " There are many variations which we won't go into detail here.", "tokens": [821, 366, 867, 17840, 597, 321, 1582, 380, 352, 666, 2607, 510, 13], "temperature": 0.0, "avg_logprob": -0.25212750227554986, "compression_ratio": 1.5807860262008733, "no_speech_prob": 1.4283722521213349e-05}, {"id": 797, "seek": 310864, "start": 3129.0, "end": 3131.0, "text": " Any questions?", "tokens": [2639, 1651, 30], "temperature": 0.0, "avg_logprob": -0.25212750227554986, "compression_ratio": 1.5807860262008733, "no_speech_prob": 1.4283722521213349e-05}, {"id": 798, "seek": 310864, "start": 3131.0, "end": 3134.0, "text": " Okay, right there.", "tokens": [1033, 11, 558, 456, 13], "temperature": 0.0, "avg_logprob": -0.25212750227554986, "compression_ratio": 1.5807860262008733, "no_speech_prob": 1.4283722521213349e-05}, {"id": 799, "seek": 313400, "start": 3134.0, "end": 3160.44, "text": " Okay, there's a question about whether you can kind of augment the search data structure", "tokens": [1033, 11, 456, 311, 257, 1168, 466, 1968, 291, 393, 733, 295, 29919, 264, 3164, 1412, 3877], "temperature": 0.0, "avg_logprob": -0.1904086839585077, "compression_ratio": 1.1, "no_speech_prob": 0.00018230806745123118}, {"id": 800, "seek": 316044, "start": 3160.44, "end": 3164.8, "text": " that helps you do the fast search.", "tokens": [300, 3665, 291, 360, 264, 2370, 3164, 13], "temperature": 0.0, "avg_logprob": -0.14080650329589844, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00011233179247938097}, {"id": 801, "seek": 316044, "start": 3164.8, "end": 3170.76, "text": " I think there is some research in the area where the vectors that you produce to index", "tokens": [286, 519, 456, 307, 512, 2132, 294, 264, 1859, 689, 264, 18875, 300, 291, 5258, 281, 8186], "temperature": 0.0, "avg_logprob": -0.14080650329589844, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00011233179247938097}, {"id": 802, "seek": 316044, "start": 3170.76, "end": 3175.08, "text": " the tree is perhaps not the same as the set that you ultimately return.", "tokens": [264, 4230, 307, 4317, 406, 264, 912, 382, 264, 992, 300, 291, 6284, 2736, 13], "temperature": 0.0, "avg_logprob": -0.14080650329589844, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00011233179247938097}, {"id": 803, "seek": 316044, "start": 3175.08, "end": 3176.6, "text": " They can be optimized for different things.", "tokens": [814, 393, 312, 26941, 337, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.14080650329589844, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00011233179247938097}, {"id": 804, "seek": 316044, "start": 3176.6, "end": 3181.16, "text": " So oftentimes these kind of tree-based approaches require your vectors to be spread out in some", "tokens": [407, 18349, 613, 733, 295, 4230, 12, 6032, 11587, 3651, 428, 18875, 281, 312, 3974, 484, 294, 512], "temperature": 0.0, "avg_logprob": -0.14080650329589844, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00011233179247938097}, {"id": 805, "seek": 316044, "start": 3181.16, "end": 3184.08, "text": " non-pathological way.", "tokens": [2107, 12, 31852, 4383, 636, 13], "temperature": 0.0, "avg_logprob": -0.14080650329589844, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00011233179247938097}, {"id": 806, "seek": 316044, "start": 3184.08, "end": 3186.52, "text": " And I think that's a very interesting area for research.", "tokens": [400, 286, 519, 300, 311, 257, 588, 1880, 1859, 337, 2132, 13], "temperature": 0.0, "avg_logprob": -0.14080650329589844, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00011233179247938097}, {"id": 807, "seek": 318652, "start": 3186.52, "end": 3191.0, "text": " So producing vectors that are easily indexable, kind of taking into account the indexing", "tokens": [407, 10501, 18875, 300, 366, 3612, 8186, 712, 11, 733, 295, 1940, 666, 2696, 264, 8186, 278], "temperature": 0.0, "avg_logprob": -0.21813786029815674, "compression_ratio": 1.5982532751091703, "no_speech_prob": 2.1442374418256804e-05}, {"id": 808, "seek": 318652, "start": 3191.0, "end": 3196.64, "text": " process as a way to improve overall performance is quite important too.", "tokens": [1399, 382, 257, 636, 281, 3470, 4787, 3389, 307, 1596, 1021, 886, 13], "temperature": 0.0, "avg_logprob": -0.21813786029815674, "compression_ratio": 1.5982532751091703, "no_speech_prob": 2.1442374418256804e-05}, {"id": 809, "seek": 318652, "start": 3196.64, "end": 3202.0, "text": " Because oftentimes when you use these fast similarity methods, they make some sort of approximation", "tokens": [1436, 18349, 562, 291, 764, 613, 2370, 32194, 7150, 11, 436, 652, 512, 1333, 295, 28023], "temperature": 0.0, "avg_logprob": -0.21813786029815674, "compression_ratio": 1.5982532751091703, "no_speech_prob": 2.1442374418256804e-05}, {"id": 810, "seek": 318652, "start": 3202.0, "end": 3207.44, "text": " to the real top case search and those approximations can often hurt you pretty bad.", "tokens": [281, 264, 957, 1192, 1389, 3164, 293, 729, 8542, 763, 393, 2049, 4607, 291, 1238, 1578, 13], "temperature": 0.0, "avg_logprob": -0.21813786029815674, "compression_ratio": 1.5982532751091703, "no_speech_prob": 2.1442374418256804e-05}, {"id": 811, "seek": 318652, "start": 3207.44, "end": 3211.2, "text": " Yeah, great question.", "tokens": [865, 11, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.21813786029815674, "compression_ratio": 1.5982532751091703, "no_speech_prob": 2.1442374418256804e-05}, {"id": 812, "seek": 321120, "start": 3211.2, "end": 3217.48, "text": " Okay, cool.", "tokens": [1033, 11, 1627, 13], "temperature": 0.0, "avg_logprob": -0.16522063928491929, "compression_ratio": 1.5497076023391814, "no_speech_prob": 5.224500273470767e-05}, {"id": 813, "seek": 321120, "start": 3217.48, "end": 3224.2, "text": " Great, so now we've looked at a few different architectures for actually performing retrieval.", "tokens": [3769, 11, 370, 586, 321, 600, 2956, 412, 257, 1326, 819, 6331, 1303, 337, 767, 10205, 19817, 3337, 13], "temperature": 0.0, "avg_logprob": -0.16522063928491929, "compression_ratio": 1.5497076023391814, "no_speech_prob": 5.224500273470767e-05}, {"id": 814, "seek": 321120, "start": 3224.2, "end": 3230.2799999999997, "text": " Now let's look at how you would actually train one of these retrievers.", "tokens": [823, 718, 311, 574, 412, 577, 291, 576, 767, 3847, 472, 295, 613, 19817, 840, 13], "temperature": 0.0, "avg_logprob": -0.16522063928491929, "compression_ratio": 1.5497076023391814, "no_speech_prob": 5.224500273470767e-05}, {"id": 815, "seek": 321120, "start": 3230.2799999999997, "end": 3238.16, "text": " So fundamentally all you need to train a retriever is you need an example of an input.", "tokens": [407, 17879, 439, 291, 643, 281, 3847, 257, 19817, 331, 307, 291, 643, 364, 1365, 295, 364, 4846, 13], "temperature": 0.0, "avg_logprob": -0.16522063928491929, "compression_ratio": 1.5497076023391814, "no_speech_prob": 5.224500273470767e-05}, {"id": 816, "seek": 323816, "start": 3238.16, "end": 3243.2, "text": " You need a positive example of what you would like to retrieve and then you need some negative", "tokens": [509, 643, 257, 3353, 1365, 295, 437, 291, 576, 411, 281, 30254, 293, 550, 291, 643, 512, 3671], "temperature": 0.0, "avg_logprob": -0.16329370006438224, "compression_ratio": 1.9042145593869733, "no_speech_prob": 1.9832263205898926e-05}, {"id": 817, "seek": 323816, "start": 3243.2, "end": 3245.64, "text": " examples of what you would not like to retrieve.", "tokens": [5110, 295, 437, 291, 576, 406, 411, 281, 30254, 13], "temperature": 0.0, "avg_logprob": -0.16329370006438224, "compression_ratio": 1.9042145593869733, "no_speech_prob": 1.9832263205898926e-05}, {"id": 818, "seek": 323816, "start": 3245.64, "end": 3251.48, "text": " So for example, where the super-ball is this year, Sears Tower location, etc.", "tokens": [407, 337, 1365, 11, 689, 264, 1687, 12, 3129, 307, 341, 1064, 11, 1100, 685, 17877, 4914, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.16329370006438224, "compression_ratio": 1.9042145593869733, "no_speech_prob": 1.9832263205898926e-05}, {"id": 819, "seek": 323816, "start": 3251.48, "end": 3256.48, "text": " And the training objective for this is quite straightforward.", "tokens": [400, 264, 3097, 10024, 337, 341, 307, 1596, 15325, 13], "temperature": 0.0, "avg_logprob": -0.16329370006438224, "compression_ratio": 1.9042145593869733, "no_speech_prob": 1.9832263205898926e-05}, {"id": 820, "seek": 323816, "start": 3256.48, "end": 3259.08, "text": " So I'm going to divine a few variables here.", "tokens": [407, 286, 478, 516, 281, 13678, 257, 1326, 9102, 510, 13], "temperature": 0.0, "avg_logprob": -0.16329370006438224, "compression_ratio": 1.9042145593869733, "no_speech_prob": 1.9832263205898926e-05}, {"id": 821, "seek": 323816, "start": 3259.08, "end": 3263.64, "text": " S star will be the score that the retriever assigns to the correct input and S sub i is", "tokens": [318, 3543, 486, 312, 264, 6175, 300, 264, 19817, 331, 6269, 82, 281, 264, 3006, 4846, 293, 318, 1422, 741, 307], "temperature": 0.0, "avg_logprob": -0.16329370006438224, "compression_ratio": 1.9042145593869733, "no_speech_prob": 1.9832263205898926e-05}, {"id": 822, "seek": 323816, "start": 3263.64, "end": 3267.8399999999997, "text": " going to be the score that the retriever assigns to each of the negative inputs.", "tokens": [516, 281, 312, 264, 6175, 300, 264, 19817, 331, 6269, 82, 281, 1184, 295, 264, 3671, 15743, 13], "temperature": 0.0, "avg_logprob": -0.16329370006438224, "compression_ratio": 1.9042145593869733, "no_speech_prob": 1.9832263205898926e-05}, {"id": 823, "seek": 326784, "start": 3267.84, "end": 3273.1200000000003, "text": " And then we're going to apply the well-known softmax function over all of these scores.", "tokens": [400, 550, 321, 434, 516, 281, 3079, 264, 731, 12, 6861, 2787, 41167, 2445, 670, 439, 295, 613, 13444, 13], "temperature": 0.0, "avg_logprob": -0.14668051401774088, "compression_ratio": 2.007575757575758, "no_speech_prob": 2.8855878554168157e-05}, {"id": 824, "seek": 326784, "start": 3273.1200000000003, "end": 3277.28, "text": " So what we're doing here is we're taking each of these scores, exponentiating the score", "tokens": [407, 437, 321, 434, 884, 510, 307, 321, 434, 1940, 1184, 295, 613, 13444, 11, 37871, 72, 990, 264, 6175], "temperature": 0.0, "avg_logprob": -0.14668051401774088, "compression_ratio": 2.007575757575758, "no_speech_prob": 2.8855878554168157e-05}, {"id": 825, "seek": 326784, "start": 3277.28, "end": 3279.48, "text": " so that there's some positive value.", "tokens": [370, 300, 456, 311, 512, 3353, 2158, 13], "temperature": 0.0, "avg_logprob": -0.14668051401774088, "compression_ratio": 2.007575757575758, "no_speech_prob": 2.8855878554168157e-05}, {"id": 826, "seek": 326784, "start": 3279.48, "end": 3284.2000000000003, "text": " And then dividing each of those exponentiated scores by the sum of all of those scores,", "tokens": [400, 550, 26764, 1184, 295, 729, 37871, 72, 770, 13444, 538, 264, 2408, 295, 439, 295, 729, 13444, 11], "temperature": 0.0, "avg_logprob": -0.14668051401774088, "compression_ratio": 2.007575757575758, "no_speech_prob": 2.8855878554168157e-05}, {"id": 827, "seek": 326784, "start": 3284.2000000000003, "end": 3286.6800000000003, "text": " so that the whole thing sums up to one.", "tokens": [370, 300, 264, 1379, 551, 34499, 493, 281, 472, 13], "temperature": 0.0, "avg_logprob": -0.14668051401774088, "compression_ratio": 2.007575757575758, "no_speech_prob": 2.8855878554168157e-05}, {"id": 828, "seek": 326784, "start": 3286.6800000000003, "end": 3291.28, "text": " And we're going to call that the probability of retrieving the positive document.", "tokens": [400, 321, 434, 516, 281, 818, 300, 264, 8482, 295, 19817, 798, 264, 3353, 4166, 13], "temperature": 0.0, "avg_logprob": -0.14668051401774088, "compression_ratio": 2.007575757575758, "no_speech_prob": 2.8855878554168157e-05}, {"id": 829, "seek": 326784, "start": 3291.28, "end": 3295.6000000000004, "text": " So intuitively if the positive document has a high score, then after exponentiation it", "tokens": [407, 46506, 498, 264, 3353, 4166, 575, 257, 1090, 6175, 11, 550, 934, 37871, 6642, 309], "temperature": 0.0, "avg_logprob": -0.14668051401774088, "compression_ratio": 2.007575757575758, "no_speech_prob": 2.8855878554168157e-05}, {"id": 830, "seek": 326784, "start": 3295.6000000000004, "end": 3296.88, "text": " will be even bigger.", "tokens": [486, 312, 754, 3801, 13], "temperature": 0.0, "avg_logprob": -0.14668051401774088, "compression_ratio": 2.007575757575758, "no_speech_prob": 2.8855878554168157e-05}, {"id": 831, "seek": 329688, "start": 3296.88, "end": 3300.84, "text": " The other scores will be smaller and most of the mass in this probability distribution", "tokens": [440, 661, 13444, 486, 312, 4356, 293, 881, 295, 264, 2758, 294, 341, 8482, 7316], "temperature": 0.0, "avg_logprob": -0.12962972055567373, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.8825754018034786e-05}, {"id": 832, "seek": 329688, "start": 3300.84, "end": 3303.32, "text": " will be on the positive document.", "tokens": [486, 312, 322, 264, 3353, 4166, 13], "temperature": 0.0, "avg_logprob": -0.12962972055567373, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.8825754018034786e-05}, {"id": 833, "seek": 329688, "start": 3303.32, "end": 3306.56, "text": " If it's not, then this probability will be small.", "tokens": [759, 309, 311, 406, 11, 550, 341, 8482, 486, 312, 1359, 13], "temperature": 0.0, "avg_logprob": -0.12962972055567373, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.8825754018034786e-05}, {"id": 834, "seek": 329688, "start": 3306.56, "end": 3311.6800000000003, "text": " And what we will do as a standard in machine learning is we're going to maximize the probability", "tokens": [400, 437, 321, 486, 360, 382, 257, 3832, 294, 3479, 2539, 307, 321, 434, 516, 281, 19874, 264, 8482], "temperature": 0.0, "avg_logprob": -0.12962972055567373, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.8825754018034786e-05}, {"id": 835, "seek": 329688, "start": 3311.6800000000003, "end": 3316.32, "text": " of that quantity, in particular the log probability.", "tokens": [295, 300, 11275, 11, 294, 1729, 264, 3565, 8482, 13], "temperature": 0.0, "avg_logprob": -0.12962972055567373, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.8825754018034786e-05}, {"id": 836, "seek": 329688, "start": 3316.32, "end": 3321.2400000000002, "text": " And this is all doable because P of positive depends on the softmax expression here, which", "tokens": [400, 341, 307, 439, 41183, 570, 430, 295, 3353, 5946, 322, 264, 2787, 41167, 6114, 510, 11, 597], "temperature": 0.0, "avg_logprob": -0.12962972055567373, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.8825754018034786e-05}, {"id": 837, "seek": 329688, "start": 3321.2400000000002, "end": 3322.88, "text": " is differentiable.", "tokens": [307, 819, 9364, 13], "temperature": 0.0, "avg_logprob": -0.12962972055567373, "compression_ratio": 1.7408906882591093, "no_speech_prob": 3.8825754018034786e-05}, {"id": 838, "seek": 332288, "start": 3322.88, "end": 3327.32, "text": " And each of the scores inside the softmax depends on the retriever, which I just told you", "tokens": [400, 1184, 295, 264, 13444, 1854, 264, 2787, 41167, 5946, 322, 264, 19817, 331, 11, 597, 286, 445, 1907, 291], "temperature": 0.0, "avg_logprob": -0.17149609967696766, "compression_ratio": 1.7048611111111112, "no_speech_prob": 3.269558510510251e-05}, {"id": 839, "seek": 332288, "start": 3327.32, "end": 3329.52, "text": " on the previous slide is also differentiable.", "tokens": [322, 264, 3894, 4137, 307, 611, 819, 9364, 13], "temperature": 0.0, "avg_logprob": -0.17149609967696766, "compression_ratio": 1.7048611111111112, "no_speech_prob": 3.269558510510251e-05}, {"id": 840, "seek": 332288, "start": 3329.52, "end": 3333.6400000000003, "text": " So the whole thing is differentiable and you're just basically trying to push the positive", "tokens": [407, 264, 1379, 551, 307, 819, 9364, 293, 291, 434, 445, 1936, 1382, 281, 2944, 264, 3353], "temperature": 0.0, "avg_logprob": -0.17149609967696766, "compression_ratio": 1.7048611111111112, "no_speech_prob": 3.269558510510251e-05}, {"id": 841, "seek": 332288, "start": 3333.6400000000003, "end": 3339.0, "text": " score essentially above all the negative scores.", "tokens": [6175, 4476, 3673, 439, 264, 3671, 13444, 13], "temperature": 0.0, "avg_logprob": -0.17149609967696766, "compression_ratio": 1.7048611111111112, "no_speech_prob": 3.269558510510251e-05}, {"id": 842, "seek": 332288, "start": 3339.0, "end": 3342.04, "text": " Okay, so it's a very simple recipe.", "tokens": [1033, 11, 370, 309, 311, 257, 588, 2199, 6782, 13], "temperature": 0.0, "avg_logprob": -0.17149609967696766, "compression_ratio": 1.7048611111111112, "no_speech_prob": 3.269558510510251e-05}, {"id": 843, "seek": 332288, "start": 3342.04, "end": 3346.44, "text": " And we'll look at a concrete example of that based on this paper called Dense Passage Retrieval,", "tokens": [400, 321, 603, 574, 412, 257, 9859, 1365, 295, 300, 2361, 322, 341, 3035, 1219, 413, 1288, 10319, 609, 11495, 5469, 3337, 11], "temperature": 0.0, "avg_logprob": -0.17149609967696766, "compression_ratio": 1.7048611111111112, "no_speech_prob": 3.269558510510251e-05}, {"id": 844, "seek": 332288, "start": 3346.44, "end": 3352.56, "text": " DPR, one of the early papers to explore the sort of supervised retrieval approach.", "tokens": [413, 15958, 11, 472, 295, 264, 2440, 10577, 281, 6839, 264, 1333, 295, 46533, 19817, 3337, 3109, 13], "temperature": 0.0, "avg_logprob": -0.17149609967696766, "compression_ratio": 1.7048611111111112, "no_speech_prob": 3.269558510510251e-05}, {"id": 845, "seek": 335256, "start": 3352.56, "end": 3357.72, "text": " So the task they're looking at is basically given a question, like the one here, retrieve", "tokens": [407, 264, 5633, 436, 434, 1237, 412, 307, 1936, 2212, 257, 1168, 11, 411, 264, 472, 510, 11, 30254], "temperature": 0.0, "avg_logprob": -0.1229628855639165, "compression_ratio": 1.7081712062256809, "no_speech_prob": 2.212380240962375e-05}, {"id": 846, "seek": 335256, "start": 3357.72, "end": 3361.4, "text": " a passage from Wikipedia containing the answer.", "tokens": [257, 11497, 490, 28999, 19273, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.1229628855639165, "compression_ratio": 1.7081712062256809, "no_speech_prob": 2.212380240962375e-05}, {"id": 847, "seek": 335256, "start": 3361.4, "end": 3365.72, "text": " And once you've retrieved the passage, they then have a reader module that reads the", "tokens": [400, 1564, 291, 600, 19817, 937, 264, 11497, 11, 436, 550, 362, 257, 15149, 10088, 300, 15700, 264], "temperature": 0.0, "avg_logprob": -0.1229628855639165, "compression_ratio": 1.7081712062256809, "no_speech_prob": 2.212380240962375e-05}, {"id": 848, "seek": 335256, "start": 3365.72, "end": 3369.4, "text": " passage and produces an answer.", "tokens": [11497, 293, 14725, 364, 1867, 13], "temperature": 0.0, "avg_logprob": -0.1229628855639165, "compression_ratio": 1.7081712062256809, "no_speech_prob": 2.212380240962375e-05}, {"id": 849, "seek": 335256, "start": 3369.4, "end": 3373.4, "text": " And the training data for the retriever is going to fit into the format that I just described.", "tokens": [400, 264, 3097, 1412, 337, 264, 19817, 331, 307, 516, 281, 3318, 666, 264, 7877, 300, 286, 445, 7619, 13], "temperature": 0.0, "avg_logprob": -0.1229628855639165, "compression_ratio": 1.7081712062256809, "no_speech_prob": 2.212380240962375e-05}, {"id": 850, "seek": 335256, "start": 3373.4, "end": 3378.32, "text": " So they work with this dataset called natural questions, which comes with human annotated", "tokens": [407, 436, 589, 365, 341, 28872, 1219, 3303, 1651, 11, 597, 1487, 365, 1952, 25339, 770], "temperature": 0.0, "avg_logprob": -0.1229628855639165, "compression_ratio": 1.7081712062256809, "no_speech_prob": 2.212380240962375e-05}, {"id": 851, "seek": 337832, "start": 3378.32, "end": 3384.32, "text": " queries, answers to the queries, and also a passage that contains the answer.", "tokens": [24109, 11, 6338, 281, 264, 24109, 11, 293, 611, 257, 11497, 300, 8306, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.08268834833513226, "compression_ratio": 1.9227642276422765, "no_speech_prob": 5.3070463764015585e-05}, {"id": 852, "seek": 337832, "start": 3384.32, "end": 3388.1600000000003, "text": " So here we go, the input to our memory is the query.", "tokens": [407, 510, 321, 352, 11, 264, 4846, 281, 527, 4675, 307, 264, 14581, 13], "temperature": 0.0, "avg_logprob": -0.08268834833513226, "compression_ratio": 1.9227642276422765, "no_speech_prob": 5.3070463764015585e-05}, {"id": 853, "seek": 337832, "start": 3388.1600000000003, "end": 3392.32, "text": " The positive memory that we'll want to push up is the passage that the human provided.", "tokens": [440, 3353, 4675, 300, 321, 603, 528, 281, 2944, 493, 307, 264, 11497, 300, 264, 1952, 5649, 13], "temperature": 0.0, "avg_logprob": -0.08268834833513226, "compression_ratio": 1.9227642276422765, "no_speech_prob": 5.3070463764015585e-05}, {"id": 854, "seek": 337832, "start": 3392.32, "end": 3396.4, "text": " And the negative memories are actually something kind of interesting in this paper.", "tokens": [400, 264, 3671, 8495, 366, 767, 746, 733, 295, 1880, 294, 341, 3035, 13], "temperature": 0.0, "avg_logprob": -0.08268834833513226, "compression_ratio": 1.9227642276422765, "no_speech_prob": 5.3070463764015585e-05}, {"id": 855, "seek": 337832, "start": 3396.4, "end": 3402.04, "text": " So it's going to be one, it's going to be the positive passages for other queries.", "tokens": [407, 309, 311, 516, 281, 312, 472, 11, 309, 311, 516, 281, 312, 264, 3353, 31589, 337, 661, 24109, 13], "temperature": 0.0, "avg_logprob": -0.08268834833513226, "compression_ratio": 1.9227642276422765, "no_speech_prob": 5.3070463764015585e-05}, {"id": 856, "seek": 337832, "start": 3402.04, "end": 3405.84, "text": " So as long as all your queries aren't asking the same question, the positive passage for", "tokens": [407, 382, 938, 382, 439, 428, 24109, 3212, 380, 3365, 264, 912, 1168, 11, 264, 3353, 11497, 337], "temperature": 0.0, "avg_logprob": -0.08268834833513226, "compression_ratio": 1.9227642276422765, "no_speech_prob": 5.3070463764015585e-05}, {"id": 857, "seek": 340584, "start": 3405.84, "end": 3410.0, "text": " another query is going to be negative for the current query that you're looking at.", "tokens": [1071, 14581, 307, 516, 281, 312, 3671, 337, 264, 2190, 14581, 300, 291, 434, 1237, 412, 13], "temperature": 0.0, "avg_logprob": -0.10941238090640208, "compression_ratio": 1.651877133105802, "no_speech_prob": 1.618587521079462e-05}, {"id": 858, "seek": 340584, "start": 3410.0, "end": 3413.4, "text": " And this next bullet is also interesting.", "tokens": [400, 341, 958, 11632, 307, 611, 1880, 13], "temperature": 0.0, "avg_logprob": -0.10941238090640208, "compression_ratio": 1.651877133105802, "no_speech_prob": 1.618587521079462e-05}, {"id": 859, "seek": 340584, "start": 3413.4, "end": 3418.84, "text": " They take a passage that's retrieved by an off-the-shelf tool for search.", "tokens": [814, 747, 257, 11497, 300, 311, 19817, 937, 538, 364, 766, 12, 3322, 12, 46626, 2290, 337, 3164, 13], "temperature": 0.0, "avg_logprob": -0.10941238090640208, "compression_ratio": 1.651877133105802, "no_speech_prob": 1.618587521079462e-05}, {"id": 860, "seek": 340584, "start": 3418.84, "end": 3420.4, "text": " This is called BM25.", "tokens": [639, 307, 1219, 15901, 6074, 13], "temperature": 0.0, "avg_logprob": -0.10941238090640208, "compression_ratio": 1.651877133105802, "no_speech_prob": 1.618587521079462e-05}, {"id": 861, "seek": 340584, "start": 3420.4, "end": 3425.28, "text": " It's a classic information retrieval approach that uses token-based overlap to retrieve", "tokens": [467, 311, 257, 7230, 1589, 19817, 3337, 3109, 300, 4960, 14862, 12, 6032, 19959, 281, 30254], "temperature": 0.0, "avg_logprob": -0.10941238090640208, "compression_ratio": 1.651877133105802, "no_speech_prob": 1.618587521079462e-05}, {"id": 862, "seek": 340584, "start": 3425.28, "end": 3426.28, "text": " things.", "tokens": [721, 13], "temperature": 0.0, "avg_logprob": -0.10941238090640208, "compression_ratio": 1.651877133105802, "no_speech_prob": 1.618587521079462e-05}, {"id": 863, "seek": 340584, "start": 3426.28, "end": 3430.4, "text": " It doesn't have any deep learning or anything in it, but it's quite effective.", "tokens": [467, 1177, 380, 362, 604, 2452, 2539, 420, 1340, 294, 309, 11, 457, 309, 311, 1596, 4942, 13], "temperature": 0.0, "avg_logprob": -0.10941238090640208, "compression_ratio": 1.651877133105802, "no_speech_prob": 1.618587521079462e-05}, {"id": 864, "seek": 340584, "start": 3430.4, "end": 3434.6000000000004, "text": " So they retrieve a passage and they retrieve one that does not contain the answer in it.", "tokens": [407, 436, 30254, 257, 11497, 293, 436, 30254, 472, 300, 775, 406, 5304, 264, 1867, 294, 309, 13], "temperature": 0.0, "avg_logprob": -0.10941238090640208, "compression_ratio": 1.651877133105802, "no_speech_prob": 1.618587521079462e-05}, {"id": 865, "seek": 343460, "start": 3434.6, "end": 3438.96, "text": " So the assumption here is that you've got a passage that looks very promising, but in", "tokens": [407, 264, 15302, 510, 307, 300, 291, 600, 658, 257, 11497, 300, 1542, 588, 20257, 11, 457, 294], "temperature": 0.0, "avg_logprob": -0.15298535052995035, "compression_ratio": 1.7013888888888888, "no_speech_prob": 2.0459299776121043e-05}, {"id": 866, "seek": 343460, "start": 3438.96, "end": 3441.08, "text": " fact, doesn't contain the answer.", "tokens": [1186, 11, 1177, 380, 5304, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.15298535052995035, "compression_ratio": 1.7013888888888888, "no_speech_prob": 2.0459299776121043e-05}, {"id": 867, "seek": 343460, "start": 3441.08, "end": 3446.44, "text": " And you can think of that as this is what we call a hard negative.", "tokens": [400, 291, 393, 519, 295, 300, 382, 341, 307, 437, 321, 818, 257, 1152, 3671, 13], "temperature": 0.0, "avg_logprob": -0.15298535052995035, "compression_ratio": 1.7013888888888888, "no_speech_prob": 2.0459299776121043e-05}, {"id": 868, "seek": 343460, "start": 3446.44, "end": 3447.44, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.15298535052995035, "compression_ratio": 1.7013888888888888, "no_speech_prob": 2.0459299776121043e-05}, {"id": 869, "seek": 343460, "start": 3447.44, "end": 3449.08, "text": " So we've got all the components for training retriever.", "tokens": [407, 321, 600, 658, 439, 264, 6677, 337, 3097, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.15298535052995035, "compression_ratio": 1.7013888888888888, "no_speech_prob": 2.0459299776121043e-05}, {"id": 870, "seek": 343460, "start": 3449.08, "end": 3451.2, "text": " They go ahead and do that.", "tokens": [814, 352, 2286, 293, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.15298535052995035, "compression_ratio": 1.7013888888888888, "no_speech_prob": 2.0459299776121043e-05}, {"id": 871, "seek": 343460, "start": 3451.2, "end": 3453.04, "text": " Let's look at how well it actually works.", "tokens": [961, 311, 574, 412, 577, 731, 309, 767, 1985, 13], "temperature": 0.0, "avg_logprob": -0.15298535052995035, "compression_ratio": 1.7013888888888888, "no_speech_prob": 2.0459299776121043e-05}, {"id": 872, "seek": 343460, "start": 3453.04, "end": 3458.64, "text": " So to understand how well it works, we're going to compare it against another approach.", "tokens": [407, 281, 1223, 577, 731, 309, 1985, 11, 321, 434, 516, 281, 6794, 309, 1970, 1071, 3109, 13], "temperature": 0.0, "avg_logprob": -0.15298535052995035, "compression_ratio": 1.7013888888888888, "no_speech_prob": 2.0459299776121043e-05}, {"id": 873, "seek": 343460, "start": 3458.64, "end": 3464.56, "text": " So we're going to look at, this is from, I should have had a citation for this too,", "tokens": [407, 321, 434, 516, 281, 574, 412, 11, 341, 307, 490, 11, 286, 820, 362, 632, 257, 45590, 337, 341, 886, 11], "temperature": 0.0, "avg_logprob": -0.15298535052995035, "compression_ratio": 1.7013888888888888, "no_speech_prob": 2.0459299776121043e-05}, {"id": 874, "seek": 346456, "start": 3464.56, "end": 3468.4, "text": " paper by Robert Settel on close book question answering.", "tokens": [3035, 538, 7977, 318, 3093, 338, 322, 1998, 1446, 1168, 13430, 13], "temperature": 0.0, "avg_logprob": -0.17390861878028283, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.00011233524128329009}, {"id": 875, "seek": 346456, "start": 3468.4, "end": 3474.24, "text": " They basically take a sequence to sequence neural network model called T5 and just feed", "tokens": [814, 1936, 747, 257, 8310, 281, 8310, 18161, 3209, 2316, 1219, 314, 20, 293, 445, 3154], "temperature": 0.0, "avg_logprob": -0.17390861878028283, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.00011233524128329009}, {"id": 876, "seek": 346456, "start": 3474.24, "end": 3476.32, "text": " in the question and ask it to retrieve the answer.", "tokens": [294, 264, 1168, 293, 1029, 309, 281, 30254, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.17390861878028283, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.00011233524128329009}, {"id": 877, "seek": 346456, "start": 3476.32, "end": 3479.6, "text": " So this model does not have access to passages.", "tokens": [407, 341, 2316, 775, 406, 362, 2105, 281, 31589, 13], "temperature": 0.0, "avg_logprob": -0.17390861878028283, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.00011233524128329009}, {"id": 878, "seek": 346456, "start": 3479.6, "end": 3482.64, "text": " In effect, it doesn't have any external memory.", "tokens": [682, 1802, 11, 309, 1177, 380, 362, 604, 8320, 4675, 13], "temperature": 0.0, "avg_logprob": -0.17390861878028283, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.00011233524128329009}, {"id": 879, "seek": 346456, "start": 3482.64, "end": 3486.64, "text": " And you can see that as they scaled up the size of the model, they were quite nicely getting", "tokens": [400, 291, 393, 536, 300, 382, 436, 36039, 493, 264, 2744, 295, 264, 2316, 11, 436, 645, 1596, 9594, 1242], "temperature": 0.0, "avg_logprob": -0.17390861878028283, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.00011233524128329009}, {"id": 880, "seek": 346456, "start": 3486.64, "end": 3490.6, "text": " better and better performance on the task.", "tokens": [1101, 293, 1101, 3389, 322, 264, 5633, 13], "temperature": 0.0, "avg_logprob": -0.17390861878028283, "compression_ratio": 1.6297709923664123, "no_speech_prob": 0.00011233524128329009}, {"id": 881, "seek": 349060, "start": 3490.6, "end": 3495.24, "text": " And the question we want to ask is with DPR, which has this external memory, this access", "tokens": [400, 264, 1168, 321, 528, 281, 1029, 307, 365, 413, 15958, 11, 597, 575, 341, 8320, 4675, 11, 341, 2105], "temperature": 0.0, "avg_logprob": -0.12546151876449585, "compression_ratio": 1.6330935251798562, "no_speech_prob": 2.0460412997636013e-05}, {"id": 882, "seek": 349060, "start": 3495.24, "end": 3500.36, "text": " to Wikipedia, can that do better than an approach that doesn't have external memory?", "tokens": [281, 28999, 11, 393, 300, 360, 1101, 813, 364, 3109, 300, 1177, 380, 362, 8320, 4675, 30], "temperature": 0.0, "avg_logprob": -0.12546151876449585, "compression_ratio": 1.6330935251798562, "no_speech_prob": 2.0460412997636013e-05}, {"id": 883, "seek": 349060, "start": 3500.36, "end": 3504.4, "text": " And the answer, of course, in this class is yes.", "tokens": [400, 264, 1867, 11, 295, 1164, 11, 294, 341, 1508, 307, 2086, 13], "temperature": 0.0, "avg_logprob": -0.12546151876449585, "compression_ratio": 1.6330935251798562, "no_speech_prob": 2.0460412997636013e-05}, {"id": 884, "seek": 349060, "start": 3504.4, "end": 3509.56, "text": " So it does indeed improve quite significantly, and it's not a surprise because we have access", "tokens": [407, 309, 775, 6451, 3470, 1596, 10591, 11, 293, 309, 311, 406, 257, 6365, 570, 321, 362, 2105], "temperature": 0.0, "avg_logprob": -0.12546151876449585, "compression_ratio": 1.6330935251798562, "no_speech_prob": 2.0460412997636013e-05}, {"id": 885, "seek": 349060, "start": 3509.56, "end": 3515.2799999999997, "text": " to this additional information, which is Wikipedia.", "tokens": [281, 341, 4497, 1589, 11, 597, 307, 28999, 13], "temperature": 0.0, "avg_logprob": -0.12546151876449585, "compression_ratio": 1.6330935251798562, "no_speech_prob": 2.0460412997636013e-05}, {"id": 886, "seek": 349060, "start": 3515.2799999999997, "end": 3519.36, "text": " So you might look at that previous chart and say, well, maybe we just need to make T5", "tokens": [407, 291, 1062, 574, 412, 300, 3894, 6927, 293, 584, 11, 731, 11, 1310, 321, 445, 643, 281, 652, 314, 20], "temperature": 0.0, "avg_logprob": -0.12546151876449585, "compression_ratio": 1.6330935251798562, "no_speech_prob": 2.0460412997636013e-05}, {"id": 887, "seek": 351936, "start": 3519.36, "end": 3523.32, "text": " bigger because after all, the scaling was looking quite good, right?", "tokens": [3801, 570, 934, 439, 11, 264, 21589, 390, 1237, 1596, 665, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.09876007008775373, "compression_ratio": 1.6451612903225807, "no_speech_prob": 8.613825048087165e-05}, {"id": 888, "seek": 351936, "start": 3523.32, "end": 3529.4, "text": " So I took that plot and re-plotted it with the parameter scale on the x-axis and the", "tokens": [407, 286, 1890, 300, 7542, 293, 319, 12, 564, 11252, 309, 365, 264, 13075, 4373, 322, 264, 2031, 12, 24633, 293, 264], "temperature": 0.0, "avg_logprob": -0.09876007008775373, "compression_ratio": 1.6451612903225807, "no_speech_prob": 8.613825048087165e-05}, {"id": 889, "seek": 351936, "start": 3529.4, "end": 3531.7200000000003, "text": " performance on the y-axis.", "tokens": [3389, 322, 264, 288, 12, 24633, 13], "temperature": 0.0, "avg_logprob": -0.09876007008775373, "compression_ratio": 1.6451612903225807, "no_speech_prob": 8.613825048087165e-05}, {"id": 890, "seek": 351936, "start": 3531.7200000000003, "end": 3536.76, "text": " And we know from recent research that these scaling laws tend to be logarithmic.", "tokens": [400, 321, 458, 490, 5162, 2132, 300, 613, 21589, 6064, 3928, 281, 312, 41473, 355, 13195, 13], "temperature": 0.0, "avg_logprob": -0.09876007008775373, "compression_ratio": 1.6451612903225807, "no_speech_prob": 8.613825048087165e-05}, {"id": 891, "seek": 351936, "start": 3536.76, "end": 3541.4, "text": " So as you increase your model size, the improvement is a logarithmic function.", "tokens": [407, 382, 291, 3488, 428, 2316, 2744, 11, 264, 10444, 307, 257, 41473, 355, 13195, 2445, 13], "temperature": 0.0, "avg_logprob": -0.09876007008775373, "compression_ratio": 1.6451612903225807, "no_speech_prob": 8.613825048087165e-05}, {"id": 892, "seek": 351936, "start": 3541.4, "end": 3544.6, "text": " And I just plotted that curve out for you to see where it's headed.", "tokens": [400, 286, 445, 43288, 300, 7605, 484, 337, 291, 281, 536, 689, 309, 311, 12798, 13], "temperature": 0.0, "avg_logprob": -0.09876007008775373, "compression_ratio": 1.6451612903225807, "no_speech_prob": 8.613825048087165e-05}, {"id": 893, "seek": 354460, "start": 3544.6, "end": 3550.48, "text": " And if you plot DPR on this curve, it's just kind of sitting way above this scaling plot", "tokens": [400, 498, 291, 7542, 413, 15958, 322, 341, 7605, 11, 309, 311, 445, 733, 295, 3798, 636, 3673, 341, 21589, 7542], "temperature": 0.0, "avg_logprob": -0.14934467668292903, "compression_ratio": 1.5901060070671378, "no_speech_prob": 1.80575098056579e-05}, {"id": 894, "seek": 354460, "start": 3550.48, "end": 3552.2, "text": " for a much smaller model size.", "tokens": [337, 257, 709, 4356, 2316, 2744, 13], "temperature": 0.0, "avg_logprob": -0.14934467668292903, "compression_ratio": 1.5901060070671378, "no_speech_prob": 1.80575098056579e-05}, {"id": 895, "seek": 354460, "start": 3552.2, "end": 3554.04, "text": " It's doing much better.", "tokens": [467, 311, 884, 709, 1101, 13], "temperature": 0.0, "avg_logprob": -0.14934467668292903, "compression_ratio": 1.5901060070671378, "no_speech_prob": 1.80575098056579e-05}, {"id": 896, "seek": 354460, "start": 3554.04, "end": 3560.7599999999998, "text": " And I also re-plotted this out further to see if this line eventually caught up with the", "tokens": [400, 286, 611, 319, 12, 564, 11252, 341, 484, 3052, 281, 536, 498, 341, 1622, 4728, 5415, 493, 365, 264], "temperature": 0.0, "avg_logprob": -0.14934467668292903, "compression_ratio": 1.5901060070671378, "no_speech_prob": 1.80575098056579e-05}, {"id": 897, "seek": 354460, "start": 3560.7599999999998, "end": 3561.92, "text": " 44 number up there.", "tokens": [16408, 1230, 493, 456, 13], "temperature": 0.0, "avg_logprob": -0.14934467668292903, "compression_ratio": 1.5901060070671378, "no_speech_prob": 1.80575098056579e-05}, {"id": 898, "seek": 354460, "start": 3561.92, "end": 3565.2, "text": " And it does add around 8 trillion parameters.", "tokens": [400, 309, 775, 909, 926, 1649, 18723, 9834, 13], "temperature": 0.0, "avg_logprob": -0.14934467668292903, "compression_ratio": 1.5901060070671378, "no_speech_prob": 1.80575098056579e-05}, {"id": 899, "seek": 354460, "start": 3565.2, "end": 3570.6, "text": " So that's about like a thousand times bigger than where we are now.", "tokens": [407, 300, 311, 466, 411, 257, 4714, 1413, 3801, 813, 689, 321, 366, 586, 13], "temperature": 0.0, "avg_logprob": -0.14934467668292903, "compression_ratio": 1.5901060070671378, "no_speech_prob": 1.80575098056579e-05}, {"id": 900, "seek": 354460, "start": 3570.6, "end": 3574.2, "text": " So all this is to say that scaling does help, but there might be easier and cheaper", "tokens": [407, 439, 341, 307, 281, 584, 300, 21589, 775, 854, 11, 457, 456, 1062, 312, 3571, 293, 12284], "temperature": 0.0, "avg_logprob": -0.14934467668292903, "compression_ratio": 1.5901060070671378, "no_speech_prob": 1.80575098056579e-05}, {"id": 901, "seek": 357420, "start": 3574.2, "end": 3576.64, "text": " ways to get there.", "tokens": [2098, 281, 483, 456, 13], "temperature": 0.0, "avg_logprob": -0.11822529792785645, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.20434366283007e-05}, {"id": 902, "seek": 357420, "start": 3576.64, "end": 3582.72, "text": " So one criticism you could make of the previous approach was that DPR actually had access to", "tokens": [407, 472, 15835, 291, 727, 652, 295, 264, 3894, 3109, 390, 300, 413, 15958, 767, 632, 2105, 281], "temperature": 0.0, "avg_logprob": -0.11822529792785645, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.20434366283007e-05}, {"id": 903, "seek": 357420, "start": 3582.72, "end": 3587.08, "text": " something that T5 didn't have, which is it had human annotated gold passages saying", "tokens": [746, 300, 314, 20, 994, 380, 362, 11, 597, 307, 309, 632, 1952, 25339, 770, 3821, 31589, 1566], "temperature": 0.0, "avg_logprob": -0.11822529792785645, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.20434366283007e-05}, {"id": 904, "seek": 357420, "start": 3587.08, "end": 3589.68, "text": " what you needed to retrieve to answer the question.", "tokens": [437, 291, 2978, 281, 30254, 281, 1867, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.11822529792785645, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.20434366283007e-05}, {"id": 905, "seek": 357420, "start": 3589.68, "end": 3591.9199999999996, "text": " And that data is actually hard to collect.", "tokens": [400, 300, 1412, 307, 767, 1152, 281, 2500, 13], "temperature": 0.0, "avg_logprob": -0.11822529792785645, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.20434366283007e-05}, {"id": 906, "seek": 357420, "start": 3591.9199999999996, "end": 3596.0, "text": " So we're going to ask the question, what if the examples that you had access to were", "tokens": [407, 321, 434, 516, 281, 1029, 264, 1168, 11, 437, 498, 264, 5110, 300, 291, 632, 2105, 281, 645], "temperature": 0.0, "avg_logprob": -0.11822529792785645, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.20434366283007e-05}, {"id": 907, "seek": 357420, "start": 3596.0, "end": 3597.7999999999997, "text": " just query answer pairs?", "tokens": [445, 14581, 1867, 15494, 30], "temperature": 0.0, "avg_logprob": -0.11822529792785645, "compression_ratio": 1.6528925619834711, "no_speech_prob": 6.20434366283007e-05}, {"id": 908, "seek": 359780, "start": 3597.8, "end": 3604.92, "text": " And you still train a good retriever without gold passages.", "tokens": [400, 291, 920, 3847, 257, 665, 19817, 331, 1553, 3821, 31589, 13], "temperature": 0.0, "avg_logprob": -0.11665539308027788, "compression_ratio": 1.643939393939394, "no_speech_prob": 1.2804573998437263e-05}, {"id": 909, "seek": 359780, "start": 3604.92, "end": 3609.6400000000003, "text": " And this sort of task arises in many other tasks as well.", "tokens": [400, 341, 1333, 295, 5633, 27388, 294, 867, 661, 9608, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.11665539308027788, "compression_ratio": 1.643939393939394, "no_speech_prob": 1.2804573998437263e-05}, {"id": 910, "seek": 359780, "start": 3609.6400000000003, "end": 3613.0800000000004, "text": " You could imagine if you were going from natural language to code, you might encounter cases", "tokens": [509, 727, 3811, 498, 291, 645, 516, 490, 3303, 2856, 281, 3089, 11, 291, 1062, 8593, 3331], "temperature": 0.0, "avg_logprob": -0.11665539308027788, "compression_ratio": 1.643939393939394, "no_speech_prob": 1.2804573998437263e-05}, {"id": 911, "seek": 359780, "start": 3613.0800000000004, "end": 3617.6000000000004, "text": " where nobody has provided you annotations of what code snippets to retrieve, medical", "tokens": [689, 5079, 575, 5649, 291, 25339, 763, 295, 437, 3089, 35623, 1385, 281, 30254, 11, 4625], "temperature": 0.0, "avg_logprob": -0.11665539308027788, "compression_ratio": 1.643939393939394, "no_speech_prob": 1.2804573998437263e-05}, {"id": 912, "seek": 359780, "start": 3617.6000000000004, "end": 3620.2400000000002, "text": " diagnosis, similar thing.", "tokens": [15217, 11, 2531, 551, 13], "temperature": 0.0, "avg_logprob": -0.11665539308027788, "compression_ratio": 1.643939393939394, "no_speech_prob": 1.2804573998437263e-05}, {"id": 913, "seek": 359780, "start": 3620.2400000000002, "end": 3623.7200000000003, "text": " So we're going to go now to end-to-end learning of a retriever.", "tokens": [407, 321, 434, 516, 281, 352, 586, 281, 917, 12, 1353, 12, 521, 2539, 295, 257, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.11665539308027788, "compression_ratio": 1.643939393939394, "no_speech_prob": 1.2804573998437263e-05}, {"id": 914, "seek": 359780, "start": 3623.7200000000003, "end": 3626.6400000000003, "text": " And let me get into some detail on what that is.", "tokens": [400, 718, 385, 483, 666, 512, 2607, 322, 437, 300, 307, 13], "temperature": 0.0, "avg_logprob": -0.11665539308027788, "compression_ratio": 1.643939393939394, "no_speech_prob": 1.2804573998437263e-05}, {"id": 915, "seek": 362664, "start": 3626.64, "end": 3630.3199999999997, "text": " So we're coming back to this diagram of a memory retriever.", "tokens": [407, 321, 434, 1348, 646, 281, 341, 10686, 295, 257, 4675, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.09544877863641996, "compression_ratio": 1.9087719298245613, "no_speech_prob": 7.76670640334487e-06}, {"id": 916, "seek": 362664, "start": 3630.3199999999997, "end": 3634.64, "text": " And in a memory augmented model, once the memory is retrieved, it then goes into a reader", "tokens": [400, 294, 257, 4675, 36155, 2316, 11, 1564, 264, 4675, 307, 19817, 937, 11, 309, 550, 1709, 666, 257, 15149], "temperature": 0.0, "avg_logprob": -0.09544877863641996, "compression_ratio": 1.9087719298245613, "no_speech_prob": 7.76670640334487e-06}, {"id": 917, "seek": 362664, "start": 3634.64, "end": 3639.4, "text": " component, which takes the original input in the memory and produces an answer.", "tokens": [6542, 11, 597, 2516, 264, 3380, 4846, 294, 264, 4675, 293, 14725, 364, 1867, 13], "temperature": 0.0, "avg_logprob": -0.09544877863641996, "compression_ratio": 1.9087719298245613, "no_speech_prob": 7.76670640334487e-06}, {"id": 918, "seek": 362664, "start": 3639.4, "end": 3644.7599999999998, "text": " So if you have no supervision for the memory, you might have this intuition instead, which", "tokens": [407, 498, 291, 362, 572, 32675, 337, 264, 4675, 11, 291, 1062, 362, 341, 24002, 2602, 11, 597], "temperature": 0.0, "avg_logprob": -0.09544877863641996, "compression_ratio": 1.9087719298245613, "no_speech_prob": 7.76670640334487e-06}, {"id": 919, "seek": 362664, "start": 3644.7599999999998, "end": 3648.7999999999997, "text": " is that if you did retrieve a good memory, that should result in a good answer from the", "tokens": [307, 300, 498, 291, 630, 30254, 257, 665, 4675, 11, 300, 820, 1874, 294, 257, 665, 1867, 490, 264], "temperature": 0.0, "avg_logprob": -0.09544877863641996, "compression_ratio": 1.9087719298245613, "no_speech_prob": 7.76670640334487e-06}, {"id": 920, "seek": 362664, "start": 3648.7999999999997, "end": 3650.0, "text": " reader.", "tokens": [15149, 13], "temperature": 0.0, "avg_logprob": -0.09544877863641996, "compression_ratio": 1.9087719298245613, "no_speech_prob": 7.76670640334487e-06}, {"id": 921, "seek": 362664, "start": 3650.0, "end": 3653.44, "text": " On the other hand, if you retrieved a bad memory, that will probably cause the reader", "tokens": [1282, 264, 661, 1011, 11, 498, 291, 19817, 937, 257, 1578, 4675, 11, 300, 486, 1391, 3082, 264, 15149], "temperature": 0.0, "avg_logprob": -0.09544877863641996, "compression_ratio": 1.9087719298245613, "no_speech_prob": 7.76670640334487e-06}, {"id": 922, "seek": 362664, "start": 3653.44, "end": 3656.52, "text": " to get confused and produce a bad result.", "tokens": [281, 483, 9019, 293, 5258, 257, 1578, 1874, 13], "temperature": 0.0, "avg_logprob": -0.09544877863641996, "compression_ratio": 1.9087719298245613, "no_speech_prob": 7.76670640334487e-06}, {"id": 923, "seek": 365652, "start": 3656.52, "end": 3662.28, "text": " So you might be able to use that observation as a training signal to train your retriever.", "tokens": [407, 291, 1062, 312, 1075, 281, 764, 300, 14816, 382, 257, 3097, 6358, 281, 3847, 428, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.10669902139458774, "compression_ratio": 1.6925795053003534, "no_speech_prob": 2.144192148989532e-05}, {"id": 924, "seek": 365652, "start": 3662.28, "end": 3666.6, "text": " Let me just give a concrete example with this, who is the bad guy in Lord of the Rings?", "tokens": [961, 385, 445, 976, 257, 9859, 1365, 365, 341, 11, 567, 307, 264, 1578, 2146, 294, 3257, 295, 264, 38543, 30], "temperature": 0.0, "avg_logprob": -0.10669902139458774, "compression_ratio": 1.6925795053003534, "no_speech_prob": 2.144192148989532e-05}, {"id": 925, "seek": 365652, "start": 3666.6, "end": 3670.72, "text": " If the memory retrieves something like the main antagonist is Soran, then you'll produce", "tokens": [759, 264, 4675, 19817, 977, 746, 411, 264, 2135, 32590, 468, 307, 21421, 282, 11, 550, 291, 603, 5258], "temperature": 0.0, "avg_logprob": -0.10669902139458774, "compression_ratio": 1.6925795053003534, "no_speech_prob": 2.144192148989532e-05}, {"id": 926, "seek": 365652, "start": 3670.72, "end": 3672.96, "text": " Soran likely, and that's great.", "tokens": [21421, 282, 3700, 11, 293, 300, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.10669902139458774, "compression_ratio": 1.6925795053003534, "no_speech_prob": 2.144192148989532e-05}, {"id": 927, "seek": 365652, "start": 3672.96, "end": 3678.28, "text": " On the other hand, if the retriever got this other passage saying Lord of the Rings received", "tokens": [1282, 264, 661, 1011, 11, 498, 264, 19817, 331, 658, 341, 661, 11497, 1566, 3257, 295, 264, 38543, 4613], "temperature": 0.0, "avg_logprob": -0.10669902139458774, "compression_ratio": 1.6925795053003534, "no_speech_prob": 2.144192148989532e-05}, {"id": 928, "seek": 365652, "start": 3678.28, "end": 3684.44, "text": " a bad review from IMDB, then your reader might be more inclined to produce IMDB, which", "tokens": [257, 1578, 3131, 490, 21463, 27735, 11, 550, 428, 15149, 1062, 312, 544, 28173, 281, 5258, 21463, 27735, 11, 597], "temperature": 0.0, "avg_logprob": -0.10669902139458774, "compression_ratio": 1.6925795053003534, "no_speech_prob": 2.144192148989532e-05}, {"id": 929, "seek": 368444, "start": 3684.44, "end": 3686.84, "text": " would not match the gold answer in your training data set.", "tokens": [576, 406, 2995, 264, 3821, 1867, 294, 428, 3097, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1152034562731546, "compression_ratio": 1.8280701754385964, "no_speech_prob": 4.6107976231724024e-05}, {"id": 930, "seek": 368444, "start": 3686.84, "end": 3692.0, "text": " So this gives you some knowledge that the second memory is bad and the first one is good.", "tokens": [407, 341, 2709, 291, 512, 3601, 300, 264, 1150, 4675, 307, 1578, 293, 264, 700, 472, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.1152034562731546, "compression_ratio": 1.8280701754385964, "no_speech_prob": 4.6107976231724024e-05}, {"id": 931, "seek": 368444, "start": 3692.0, "end": 3696.68, "text": " And so what I'm going to propose here is this idea of trial and error.", "tokens": [400, 370, 437, 286, 478, 516, 281, 17421, 510, 307, 341, 1558, 295, 7308, 293, 6713, 13], "temperature": 0.0, "avg_logprob": -0.1152034562731546, "compression_ratio": 1.8280701754385964, "no_speech_prob": 4.6107976231724024e-05}, {"id": 932, "seek": 368444, "start": 3696.68, "end": 3700.96, "text": " In the first stage, you perform exploration where you let your imperfect retriever select", "tokens": [682, 264, 700, 3233, 11, 291, 2042, 16197, 689, 291, 718, 428, 26714, 19817, 331, 3048], "temperature": 0.0, "avg_logprob": -0.1152034562731546, "compression_ratio": 1.8280701754385964, "no_speech_prob": 4.6107976231724024e-05}, {"id": 933, "seek": 368444, "start": 3700.96, "end": 3705.52, "text": " some memory, and you try feeding that memory to the reader, and then you learn from success", "tokens": [512, 4675, 11, 293, 291, 853, 12919, 300, 4675, 281, 264, 15149, 11, 293, 550, 291, 1466, 490, 2245], "temperature": 0.0, "avg_logprob": -0.1152034562731546, "compression_ratio": 1.8280701754385964, "no_speech_prob": 4.6107976231724024e-05}, {"id": 934, "seek": 368444, "start": 3705.52, "end": 3706.8, "text": " or failure.", "tokens": [420, 7763, 13], "temperature": 0.0, "avg_logprob": -0.1152034562731546, "compression_ratio": 1.8280701754385964, "no_speech_prob": 4.6107976231724024e-05}, {"id": 935, "seek": 368444, "start": 3706.8, "end": 3709.84, "text": " So if the memory helps the reader generate the right answer, you want to increase the", "tokens": [407, 498, 264, 4675, 3665, 264, 15149, 8460, 264, 558, 1867, 11, 291, 528, 281, 3488, 264], "temperature": 0.0, "avg_logprob": -0.1152034562731546, "compression_ratio": 1.8280701754385964, "no_speech_prob": 4.6107976231724024e-05}, {"id": 936, "seek": 368444, "start": 3709.84, "end": 3711.88, "text": " score of that memory.", "tokens": [6175, 295, 300, 4675, 13], "temperature": 0.0, "avg_logprob": -0.1152034562731546, "compression_ratio": 1.8280701754385964, "no_speech_prob": 4.6107976231724024e-05}, {"id": 937, "seek": 371188, "start": 3711.88, "end": 3718.04, "text": " And if the memory does not help the retriever, you want to decrease the score of that memory.", "tokens": [400, 498, 264, 4675, 775, 406, 854, 264, 19817, 331, 11, 291, 528, 281, 11514, 264, 6175, 295, 300, 4675, 13], "temperature": 0.0, "avg_logprob": -0.143357565908721, "compression_ratio": 1.6192468619246863, "no_speech_prob": 4.860055014432874e-06}, {"id": 938, "seek": 371188, "start": 3718.04, "end": 3722.44, "text": " And over time, this process would help the helpful memories get higher scores than the", "tokens": [400, 670, 565, 11, 341, 1399, 576, 854, 264, 4961, 8495, 483, 2946, 13444, 813, 264], "temperature": 0.0, "avg_logprob": -0.143357565908721, "compression_ratio": 1.6192468619246863, "no_speech_prob": 4.860055014432874e-06}, {"id": 939, "seek": 371188, "start": 3722.44, "end": 3724.4, "text": " less helpful ones.", "tokens": [1570, 4961, 2306, 13], "temperature": 0.0, "avg_logprob": -0.143357565908721, "compression_ratio": 1.6192468619246863, "no_speech_prob": 4.860055014432874e-06}, {"id": 940, "seek": 371188, "start": 3724.4, "end": 3730.92, "text": " So the formal approach for this is going to be taken from a paper by one of my colleagues", "tokens": [407, 264, 9860, 3109, 337, 341, 307, 516, 281, 312, 2726, 490, 257, 3035, 538, 472, 295, 452, 7734], "temperature": 0.0, "avg_logprob": -0.143357565908721, "compression_ratio": 1.6192468619246863, "no_speech_prob": 4.860055014432874e-06}, {"id": 941, "seek": 371188, "start": 3730.92, "end": 3734.44, "text": " called OpenRetrieval QA, ORCA.", "tokens": [1219, 7238, 49, 302, 5469, 3337, 1249, 32, 11, 19654, 15515, 13], "temperature": 0.0, "avg_logprob": -0.143357565908721, "compression_ratio": 1.6192468619246863, "no_speech_prob": 4.860055014432874e-06}, {"id": 942, "seek": 371188, "start": 3734.44, "end": 3738.2400000000002, "text": " And the exploration component we're going to formalize as follows.", "tokens": [400, 264, 16197, 6542, 321, 434, 516, 281, 9860, 1125, 382, 10002, 13], "temperature": 0.0, "avg_logprob": -0.143357565908721, "compression_ratio": 1.6192468619246863, "no_speech_prob": 4.860055014432874e-06}, {"id": 943, "seek": 373824, "start": 3738.24, "end": 3741.9599999999996, "text": " So as I mentioned earlier, a retriever is just scoring function between an input and a", "tokens": [407, 382, 286, 2835, 3071, 11, 257, 19817, 331, 307, 445, 22358, 2445, 1296, 364, 4846, 293, 257], "temperature": 0.0, "avg_logprob": -0.1433477957271835, "compression_ratio": 1.808695652173913, "no_speech_prob": 1.8340570022701286e-05}, {"id": 944, "seek": 373824, "start": 3741.9599999999996, "end": 3743.4799999999996, "text": " memory.", "tokens": [4675, 13], "temperature": 0.0, "avg_logprob": -0.1433477957271835, "compression_ratio": 1.808695652173913, "no_speech_prob": 1.8340570022701286e-05}, {"id": 945, "seek": 373824, "start": 3743.4799999999996, "end": 3749.6, "text": " And if you take a softmax over all of the scores for all of the memories, then you get this", "tokens": [400, 498, 291, 747, 257, 2787, 41167, 670, 439, 295, 264, 13444, 337, 439, 295, 264, 8495, 11, 550, 291, 483, 341], "temperature": 0.0, "avg_logprob": -0.1433477957271835, "compression_ratio": 1.808695652173913, "no_speech_prob": 1.8340570022701286e-05}, {"id": 946, "seek": 373824, "start": 3749.6, "end": 3752.24, "text": " distribution over memories given the input.", "tokens": [7316, 670, 8495, 2212, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.1433477957271835, "compression_ratio": 1.808695652173913, "no_speech_prob": 1.8340570022701286e-05}, {"id": 947, "seek": 373824, "start": 3752.24, "end": 3759.8799999999997, "text": " So again, I've just raised all of the scores to e to the power of the scores and then normalize.", "tokens": [407, 797, 11, 286, 600, 445, 6005, 439, 295, 264, 13444, 281, 308, 281, 264, 1347, 295, 264, 13444, 293, 550, 2710, 1125, 13], "temperature": 0.0, "avg_logprob": -0.1433477957271835, "compression_ratio": 1.808695652173913, "no_speech_prob": 1.8340570022701286e-05}, {"id": 948, "seek": 373824, "start": 3759.8799999999997, "end": 3764.56, "text": " And once we have this distribution, we'll randomly sample memory from that distribution.", "tokens": [400, 1564, 321, 362, 341, 7316, 11, 321, 603, 16979, 6889, 4675, 490, 300, 7316, 13], "temperature": 0.0, "avg_logprob": -0.1433477957271835, "compression_ratio": 1.808695652173913, "no_speech_prob": 1.8340570022701286e-05}, {"id": 949, "seek": 376456, "start": 3764.56, "end": 3769.2, "text": " So as you can imagine, if the scores are meaningful, then we're more likely to sample a memory", "tokens": [407, 382, 291, 393, 3811, 11, 498, 264, 13444, 366, 10995, 11, 550, 321, 434, 544, 3700, 281, 6889, 257, 4675], "temperature": 0.0, "avg_logprob": -0.10662092833683409, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.0615322025842033e-05}, {"id": 950, "seek": 376456, "start": 3769.2, "end": 3772.4, "text": " that's good and less likely to sample a memory that's bad.", "tokens": [300, 311, 665, 293, 1570, 3700, 281, 6889, 257, 4675, 300, 311, 1578, 13], "temperature": 0.0, "avg_logprob": -0.10662092833683409, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.0615322025842033e-05}, {"id": 951, "seek": 376456, "start": 3772.4, "end": 3776.2799999999997, "text": " But because it's random, we kind of eventually will sample everything, unless there are things", "tokens": [583, 570, 309, 311, 4974, 11, 321, 733, 295, 4728, 486, 6889, 1203, 11, 5969, 456, 366, 721], "temperature": 0.0, "avg_logprob": -0.10662092833683409, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.0615322025842033e-05}, {"id": 952, "seek": 376456, "start": 3776.2799999999997, "end": 3779.7999999999997, "text": " with zero probability.", "tokens": [365, 4018, 8482, 13], "temperature": 0.0, "avg_logprob": -0.10662092833683409, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.0615322025842033e-05}, {"id": 953, "seek": 376456, "start": 3779.7999999999997, "end": 3783.6, "text": " And then the learning from success and failure part.", "tokens": [400, 550, 264, 2539, 490, 2245, 293, 7763, 644, 13], "temperature": 0.0, "avg_logprob": -0.10662092833683409, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.0615322025842033e-05}, {"id": 954, "seek": 376456, "start": 3783.6, "end": 3789.64, "text": " So once we pick a memory, we need to see if it actually helps.", "tokens": [407, 1564, 321, 1888, 257, 4675, 11, 321, 643, 281, 536, 498, 309, 767, 3665, 13], "temperature": 0.0, "avg_logprob": -0.10662092833683409, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.0615322025842033e-05}, {"id": 955, "seek": 376456, "start": 3789.64, "end": 3793.52, "text": " And we're going to measure that by looking at the reader's probability of generating the", "tokens": [400, 321, 434, 516, 281, 3481, 300, 538, 1237, 412, 264, 15149, 311, 8482, 295, 17746, 264], "temperature": 0.0, "avg_logprob": -0.10662092833683409, "compression_ratio": 1.7695167286245352, "no_speech_prob": 1.0615322025842033e-05}, {"id": 956, "seek": 379352, "start": 3793.52, "end": 3796.7599999999998, "text": " right answer given that particular memory.", "tokens": [558, 1867, 2212, 300, 1729, 4675, 13], "temperature": 0.0, "avg_logprob": -0.09987233303211353, "compression_ratio": 1.8425531914893618, "no_speech_prob": 2.4680968635948375e-05}, {"id": 957, "seek": 379352, "start": 3796.7599999999998, "end": 3798.52, "text": " So that's this big quantity right here.", "tokens": [407, 300, 311, 341, 955, 11275, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.09987233303211353, "compression_ratio": 1.8425531914893618, "no_speech_prob": 2.4680968635948375e-05}, {"id": 958, "seek": 379352, "start": 3798.52, "end": 3803.36, "text": " The reader looks at the input and the memory, and we want to see its probability of generating", "tokens": [440, 15149, 1542, 412, 264, 4846, 293, 264, 4675, 11, 293, 321, 528, 281, 536, 1080, 8482, 295, 17746], "temperature": 0.0, "avg_logprob": -0.09987233303211353, "compression_ratio": 1.8425531914893618, "no_speech_prob": 2.4680968635948375e-05}, {"id": 959, "seek": 379352, "start": 3803.36, "end": 3805.7599999999998, "text": " the gold answer.", "tokens": [264, 3821, 1867, 13], "temperature": 0.0, "avg_logprob": -0.09987233303211353, "compression_ratio": 1.8425531914893618, "no_speech_prob": 2.4680968635948375e-05}, {"id": 960, "seek": 379352, "start": 3805.7599999999998, "end": 3809.84, "text": " And if this value is high, then we want to increase the score of that memory.", "tokens": [400, 498, 341, 2158, 307, 1090, 11, 550, 321, 528, 281, 3488, 264, 6175, 295, 300, 4675, 13], "temperature": 0.0, "avg_logprob": -0.09987233303211353, "compression_ratio": 1.8425531914893618, "no_speech_prob": 2.4680968635948375e-05}, {"id": 961, "seek": 379352, "start": 3809.84, "end": 3814.96, "text": " And if it's low, we want to probably decrease the score of the memory.", "tokens": [400, 498, 309, 311, 2295, 11, 321, 528, 281, 1391, 11514, 264, 6175, 295, 264, 4675, 13], "temperature": 0.0, "avg_logprob": -0.09987233303211353, "compression_ratio": 1.8425531914893618, "no_speech_prob": 2.4680968635948375e-05}, {"id": 962, "seek": 379352, "start": 3814.96, "end": 3820.56, "text": " So I've shown you a couple expressions now, and we want to put those expressions together", "tokens": [407, 286, 600, 4898, 291, 257, 1916, 15277, 586, 11, 293, 321, 528, 281, 829, 729, 15277, 1214], "temperature": 0.0, "avg_logprob": -0.09987233303211353, "compression_ratio": 1.8425531914893618, "no_speech_prob": 2.4680968635948375e-05}, {"id": 963, "seek": 382056, "start": 3820.56, "end": 3824.72, "text": " into a training objective that we can actually optimize.", "tokens": [666, 257, 3097, 10024, 300, 321, 393, 767, 19719, 13], "temperature": 0.0, "avg_logprob": -0.12386441230773926, "compression_ratio": 1.7793103448275862, "no_speech_prob": 1.8056804037769325e-05}, {"id": 964, "seek": 382056, "start": 3824.72, "end": 3826.64, "text": " So we'll start with this question.", "tokens": [407, 321, 603, 722, 365, 341, 1168, 13], "temperature": 0.0, "avg_logprob": -0.12386441230773926, "compression_ratio": 1.7793103448275862, "no_speech_prob": 1.8056804037769325e-05}, {"id": 965, "seek": 382056, "start": 3826.64, "end": 3830.88, "text": " If we randomly sample a memory from the retriever, and then we generate an answer, what is the", "tokens": [759, 321, 16979, 6889, 257, 4675, 490, 264, 19817, 331, 11, 293, 550, 321, 8460, 364, 1867, 11, 437, 307, 264], "temperature": 0.0, "avg_logprob": -0.12386441230773926, "compression_ratio": 1.7793103448275862, "no_speech_prob": 1.8056804037769325e-05}, {"id": 966, "seek": 382056, "start": 3830.88, "end": 3834.52, "text": " overall probability that we get the answer right?", "tokens": [4787, 8482, 300, 321, 483, 264, 1867, 558, 30], "temperature": 0.0, "avg_logprob": -0.12386441230773926, "compression_ratio": 1.7793103448275862, "no_speech_prob": 1.8056804037769325e-05}, {"id": 967, "seek": 382056, "start": 3834.52, "end": 3836.96, "text": " So first, let's look at this expression right here.", "tokens": [407, 700, 11, 718, 311, 574, 412, 341, 6114, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.12386441230773926, "compression_ratio": 1.7793103448275862, "no_speech_prob": 1.8056804037769325e-05}, {"id": 968, "seek": 382056, "start": 3836.96, "end": 3841.36, "text": " This is a summation over all possible memories that the retriever could retrieve, and the", "tokens": [639, 307, 257, 28811, 670, 439, 1944, 8495, 300, 264, 19817, 331, 727, 30254, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.12386441230773926, "compression_ratio": 1.7793103448275862, "no_speech_prob": 1.8056804037769325e-05}, {"id": 969, "seek": 382056, "start": 3841.36, "end": 3843.68, "text": " sum is over the probability of retrieving it.", "tokens": [2408, 307, 670, 264, 8482, 295, 19817, 798, 309, 13], "temperature": 0.0, "avg_logprob": -0.12386441230773926, "compression_ratio": 1.7793103448275862, "no_speech_prob": 1.8056804037769325e-05}, {"id": 970, "seek": 382056, "start": 3843.68, "end": 3848.6, "text": " So right now, this is, this just equals one, because it's a distribution, and we're summing", "tokens": [407, 558, 586, 11, 341, 307, 11, 341, 445, 6915, 472, 11, 570, 309, 311, 257, 7316, 11, 293, 321, 434, 2408, 2810], "temperature": 0.0, "avg_logprob": -0.12386441230773926, "compression_ratio": 1.7793103448275862, "no_speech_prob": 1.8056804037769325e-05}, {"id": 971, "seek": 384860, "start": 3848.6, "end": 3850.36, "text": " over all of its values.", "tokens": [670, 439, 295, 1080, 4190, 13], "temperature": 0.0, "avg_logprob": -0.09553214481898717, "compression_ratio": 1.7890295358649788, "no_speech_prob": 1.3844811292074155e-05}, {"id": 972, "seek": 384860, "start": 3850.36, "end": 3854.48, "text": " But then we'll add one more term to this, which is the probability that the reader gets", "tokens": [583, 550, 321, 603, 909, 472, 544, 1433, 281, 341, 11, 597, 307, 264, 8482, 300, 264, 15149, 2170], "temperature": 0.0, "avg_logprob": -0.09553214481898717, "compression_ratio": 1.7890295358649788, "no_speech_prob": 1.3844811292074155e-05}, {"id": 973, "seek": 384860, "start": 3854.48, "end": 3860.12, "text": " the answer right given the memory in that term in the summation.", "tokens": [264, 1867, 558, 2212, 264, 4675, 294, 300, 1433, 294, 264, 28811, 13], "temperature": 0.0, "avg_logprob": -0.09553214481898717, "compression_ratio": 1.7890295358649788, "no_speech_prob": 1.3844811292074155e-05}, {"id": 974, "seek": 384860, "start": 3860.12, "end": 3864.52, "text": " So this first part is the retriever, and it's proposing different memories.", "tokens": [407, 341, 700, 644, 307, 264, 19817, 331, 11, 293, 309, 311, 29939, 819, 8495, 13], "temperature": 0.0, "avg_logprob": -0.09553214481898717, "compression_ratio": 1.7890295358649788, "no_speech_prob": 1.3844811292074155e-05}, {"id": 975, "seek": 384860, "start": 3864.52, "end": 3869.92, "text": " And this second part is the reader, and it's succeeding or failing based on the memories.", "tokens": [400, 341, 1150, 644, 307, 264, 15149, 11, 293, 309, 311, 47912, 420, 18223, 2361, 322, 264, 8495, 13], "temperature": 0.0, "avg_logprob": -0.09553214481898717, "compression_ratio": 1.7890295358649788, "no_speech_prob": 1.3844811292074155e-05}, {"id": 976, "seek": 384860, "start": 3869.92, "end": 3874.64, "text": " So you can think of each term in this summation as a trial of a different memory.", "tokens": [407, 291, 393, 519, 295, 1184, 1433, 294, 341, 28811, 382, 257, 7308, 295, 257, 819, 4675, 13], "temperature": 0.0, "avg_logprob": -0.09553214481898717, "compression_ratio": 1.7890295358649788, "no_speech_prob": 1.3844811292074155e-05}, {"id": 977, "seek": 387464, "start": 3874.64, "end": 3879.48, "text": " And you can think of that second term kind of like a reward, if it's high, it's good,", "tokens": [400, 291, 393, 519, 295, 300, 1150, 1433, 733, 295, 411, 257, 7782, 11, 498, 309, 311, 1090, 11, 309, 311, 665, 11], "temperature": 0.0, "avg_logprob": -0.09445137530565262, "compression_ratio": 1.7744360902255638, "no_speech_prob": 8.529569640813861e-06}, {"id": 978, "seek": 387464, "start": 3879.48, "end": 3882.64, "text": " and if it's low, it's bad.", "tokens": [293, 498, 309, 311, 2295, 11, 309, 311, 1578, 13], "temperature": 0.0, "avg_logprob": -0.09445137530565262, "compression_ratio": 1.7744360902255638, "no_speech_prob": 8.529569640813861e-06}, {"id": 979, "seek": 387464, "start": 3882.64, "end": 3887.3599999999997, "text": " So what they propose in the Orca paper is to perform gradient descent on this entire", "tokens": [407, 437, 436, 17421, 294, 264, 1610, 496, 3035, 307, 281, 2042, 16235, 23475, 322, 341, 2302], "temperature": 0.0, "avg_logprob": -0.09445137530565262, "compression_ratio": 1.7744360902255638, "no_speech_prob": 8.529569640813861e-06}, {"id": 980, "seek": 387464, "start": 3887.3599999999997, "end": 3889.08, "text": " expression right here.", "tokens": [6114, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.09445137530565262, "compression_ratio": 1.7744360902255638, "no_speech_prob": 8.529569640813861e-06}, {"id": 981, "seek": 387464, "start": 3889.08, "end": 3894.16, "text": " They basically want to push the value of this entire expression up, and they're optimizing", "tokens": [814, 1936, 528, 281, 2944, 264, 2158, 295, 341, 2302, 6114, 493, 11, 293, 436, 434, 40425], "temperature": 0.0, "avg_logprob": -0.09445137530565262, "compression_ratio": 1.7744360902255638, "no_speech_prob": 8.529569640813861e-06}, {"id": 982, "seek": 387464, "start": 3894.16, "end": 3896.6, "text": " both the retriever and the reader.", "tokens": [1293, 264, 19817, 331, 293, 264, 15149, 13], "temperature": 0.0, "avg_logprob": -0.09445137530565262, "compression_ratio": 1.7744360902255638, "no_speech_prob": 8.529569640813861e-06}, {"id": 983, "seek": 387464, "start": 3896.6, "end": 3899.96, "text": " So let's look at the retriever first.", "tokens": [407, 718, 311, 574, 412, 264, 19817, 331, 700, 13], "temperature": 0.0, "avg_logprob": -0.09445137530565262, "compression_ratio": 1.7744360902255638, "no_speech_prob": 8.529569640813861e-06}, {"id": 984, "seek": 387464, "start": 3899.96, "end": 3904.3599999999997, "text": " So the retriever has a fixed budget that has to sum up to one over all of the memories.", "tokens": [407, 264, 19817, 331, 575, 257, 6806, 4706, 300, 575, 281, 2408, 493, 281, 472, 670, 439, 295, 264, 8495, 13], "temperature": 0.0, "avg_logprob": -0.09445137530565262, "compression_ratio": 1.7744360902255638, "no_speech_prob": 8.529569640813861e-06}, {"id": 985, "seek": 390436, "start": 3904.36, "end": 3909.1600000000003, "text": " If it wants this value to be high, it doesn't have any incentive to put probability on bad", "tokens": [759, 309, 2738, 341, 2158, 281, 312, 1090, 11, 309, 1177, 380, 362, 604, 22346, 281, 829, 8482, 322, 1578], "temperature": 0.0, "avg_logprob": -0.1090013349757475, "compression_ratio": 1.839464882943144, "no_speech_prob": 1.8923712559626438e-05}, {"id": 986, "seek": 390436, "start": 3909.1600000000003, "end": 3913.28, "text": " memories, because those bad memories are just going to produce a low score on this", "tokens": [8495, 11, 570, 729, 1578, 8495, 366, 445, 516, 281, 5258, 257, 2295, 6175, 322, 341], "temperature": 0.0, "avg_logprob": -0.1090013349757475, "compression_ratio": 1.839464882943144, "no_speech_prob": 1.8923712559626438e-05}, {"id": 987, "seek": 390436, "start": 3913.28, "end": 3914.6800000000003, "text": " term right here.", "tokens": [1433, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.1090013349757475, "compression_ratio": 1.839464882943144, "no_speech_prob": 1.8923712559626438e-05}, {"id": 988, "seek": 390436, "start": 3914.6800000000003, "end": 3918.88, "text": " So as you optimize this function, the retriever will basically try to put all of its mass", "tokens": [407, 382, 291, 19719, 341, 2445, 11, 264, 19817, 331, 486, 1936, 853, 281, 829, 439, 295, 1080, 2758], "temperature": 0.0, "avg_logprob": -0.1090013349757475, "compression_ratio": 1.839464882943144, "no_speech_prob": 1.8923712559626438e-05}, {"id": 989, "seek": 390436, "start": 3918.88, "end": 3920.44, "text": " on the good memories.", "tokens": [322, 264, 665, 8495, 13], "temperature": 0.0, "avg_logprob": -0.1090013349757475, "compression_ratio": 1.839464882943144, "no_speech_prob": 1.8923712559626438e-05}, {"id": 990, "seek": 390436, "start": 3920.44, "end": 3924.36, "text": " And meanwhile, if you're optimizing the reader with respect to this function, it's trying", "tokens": [400, 29252, 11, 498, 291, 434, 40425, 264, 15149, 365, 3104, 281, 341, 2445, 11, 309, 311, 1382], "temperature": 0.0, "avg_logprob": -0.1090013349757475, "compression_ratio": 1.839464882943144, "no_speech_prob": 1.8923712559626438e-05}, {"id": 991, "seek": 390436, "start": 3924.36, "end": 3928.44, "text": " its best to produce the gold answer given whatever memory it has.", "tokens": [1080, 1151, 281, 5258, 264, 3821, 1867, 2212, 2035, 4675, 309, 575, 13], "temperature": 0.0, "avg_logprob": -0.1090013349757475, "compression_ratio": 1.839464882943144, "no_speech_prob": 1.8923712559626438e-05}, {"id": 992, "seek": 390436, "start": 3928.44, "end": 3932.36, "text": " So it's also incentivized to try its best to extract the answer out of whatever it's", "tokens": [407, 309, 311, 611, 35328, 1602, 281, 853, 1080, 1151, 281, 8947, 264, 1867, 484, 295, 2035, 309, 311], "temperature": 0.0, "avg_logprob": -0.1090013349757475, "compression_ratio": 1.839464882943144, "no_speech_prob": 1.8923712559626438e-05}, {"id": 993, "seek": 390436, "start": 3932.36, "end": 3933.36, "text": " given.", "tokens": [2212, 13], "temperature": 0.0, "avg_logprob": -0.1090013349757475, "compression_ratio": 1.839464882943144, "no_speech_prob": 1.8923712559626438e-05}, {"id": 994, "seek": 393336, "start": 3933.36, "end": 3937.52, "text": " And when you kind of jointly optimize both, over time you get something that puts its", "tokens": [400, 562, 291, 733, 295, 46557, 19719, 1293, 11, 670, 565, 291, 483, 746, 300, 8137, 1080], "temperature": 0.0, "avg_logprob": -0.1550209469265408, "compression_ratio": 1.718543046357616, "no_speech_prob": 1.0615185601636767e-05}, {"id": 995, "seek": 393336, "start": 3937.52, "end": 3939.32, "text": " mass on good memories.", "tokens": [2758, 322, 665, 8495, 13], "temperature": 0.0, "avg_logprob": -0.1550209469265408, "compression_ratio": 1.718543046357616, "no_speech_prob": 1.0615185601636767e-05}, {"id": 996, "seek": 393336, "start": 3939.32, "end": 3944.2000000000003, "text": " So that kind of corresponds with the intuition that I was giving you earlier that you can", "tokens": [407, 300, 733, 295, 23249, 365, 264, 24002, 300, 286, 390, 2902, 291, 3071, 300, 291, 393], "temperature": 0.0, "avg_logprob": -0.1550209469265408, "compression_ratio": 1.718543046357616, "no_speech_prob": 1.0615185601636767e-05}, {"id": 997, "seek": 393336, "start": 3944.2000000000003, "end": 3949.6400000000003, "text": " kind of perform end-to-end learning to get a retriever.", "tokens": [733, 295, 2042, 917, 12, 1353, 12, 521, 2539, 281, 483, 257, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.1550209469265408, "compression_ratio": 1.718543046357616, "no_speech_prob": 1.0615185601636767e-05}, {"id": 998, "seek": 393336, "start": 3949.6400000000003, "end": 3950.6400000000003, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.1550209469265408, "compression_ratio": 1.718543046357616, "no_speech_prob": 1.0615185601636767e-05}, {"id": 999, "seek": 393336, "start": 3950.6400000000003, "end": 3953.04, "text": " So that's the high-level approach to Orca.", "tokens": [407, 300, 311, 264, 1090, 12, 12418, 3109, 281, 1610, 496, 13], "temperature": 0.0, "avg_logprob": -0.1550209469265408, "compression_ratio": 1.718543046357616, "no_speech_prob": 1.0615185601636767e-05}, {"id": 1000, "seek": 393336, "start": 3953.04, "end": 3956.92, "text": " What I didn't explain is that usually if your memories are like all of Wikipedia, this", "tokens": [708, 286, 994, 380, 2903, 307, 300, 2673, 498, 428, 8495, 366, 411, 439, 295, 28999, 11, 341], "temperature": 0.0, "avg_logprob": -0.1550209469265408, "compression_ratio": 1.718543046357616, "no_speech_prob": 1.0615185601636767e-05}, {"id": 1001, "seek": 393336, "start": 3956.92, "end": 3961.0, "text": " summation is very large, and if you're going to do gradient descent on this summation,", "tokens": [28811, 307, 588, 2416, 11, 293, 498, 291, 434, 516, 281, 360, 16235, 23475, 322, 341, 28811, 11], "temperature": 0.0, "avg_logprob": -0.1550209469265408, "compression_ratio": 1.718543046357616, "no_speech_prob": 1.0615185601636767e-05}, {"id": 1002, "seek": 393336, "start": 3961.0, "end": 3962.76, "text": " it's going to take a very long time.", "tokens": [309, 311, 516, 281, 747, 257, 588, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.1550209469265408, "compression_ratio": 1.718543046357616, "no_speech_prob": 1.0615185601636767e-05}, {"id": 1003, "seek": 396276, "start": 3962.76, "end": 3967.1600000000003, "text": " So in practice, they approximate this summation with the highest probability memories, maybe", "tokens": [407, 294, 3124, 11, 436, 30874, 341, 28811, 365, 264, 6343, 8482, 8495, 11, 1310], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1004, "seek": 396276, "start": 3967.1600000000003, "end": 3970.4, "text": " the top 100 or the top 10.", "tokens": [264, 1192, 2319, 420, 264, 1192, 1266, 13], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1005, "seek": 396276, "start": 3970.4, "end": 3974.2000000000003, "text": " And I won't go into details in this class about exactly how that works.", "tokens": [400, 286, 1582, 380, 352, 666, 4365, 294, 341, 1508, 466, 2293, 577, 300, 1985, 13], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1006, "seek": 396276, "start": 3974.2000000000003, "end": 3977.5600000000004, "text": " But I'll stop there.", "tokens": [583, 286, 603, 1590, 456, 13], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1007, "seek": 396276, "start": 3977.5600000000004, "end": 3980.32, "text": " Because we're kind of approaching the end, I'm going to take questions just a little bit", "tokens": [1436, 321, 434, 733, 295, 14908, 264, 917, 11, 286, 478, 516, 281, 747, 1651, 445, 257, 707, 857], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1008, "seek": 396276, "start": 3980.32, "end": 3981.32, "text": " later.", "tokens": [1780, 13], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1009, "seek": 396276, "start": 3981.32, "end": 3983.6800000000003, "text": " Sorry about that.", "tokens": [4919, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1010, "seek": 396276, "start": 3983.6800000000003, "end": 3986.32, "text": " So let's see how well Orca works.", "tokens": [407, 718, 311, 536, 577, 731, 1610, 496, 1985, 13], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1011, "seek": 396276, "start": 3986.32, "end": 3988.0400000000004, "text": " Just come out and put that number there.", "tokens": [1449, 808, 484, 293, 829, 300, 1230, 456, 13], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1012, "seek": 396276, "start": 3988.0400000000004, "end": 3989.6000000000004, "text": " So a bit of context around this.", "tokens": [407, 257, 857, 295, 4319, 926, 341, 13], "temperature": 0.0, "avg_logprob": -0.16049410715824416, "compression_ratio": 1.6014760147601477, "no_speech_prob": 1.6440972103737295e-05}, {"id": 1013, "seek": 398960, "start": 3989.6, "end": 3992.96, "text": " It's not as good as DPR because it has less supervision than DPR.", "tokens": [467, 311, 406, 382, 665, 382, 413, 15958, 570, 309, 575, 1570, 32675, 813, 413, 15958, 13], "temperature": 0.0, "avg_logprob": -0.12405588059198289, "compression_ratio": 1.5587044534412955, "no_speech_prob": 5.3068411943968385e-05}, {"id": 1014, "seek": 398960, "start": 3992.96, "end": 3996.2799999999997, "text": " There's no human annotation of what passage to retrieve.", "tokens": [821, 311, 572, 1952, 48654, 295, 437, 11497, 281, 30254, 13], "temperature": 0.0, "avg_logprob": -0.12405588059198289, "compression_ratio": 1.5587044534412955, "no_speech_prob": 5.3068411943968385e-05}, {"id": 1015, "seek": 398960, "start": 3996.2799999999997, "end": 4001.68, "text": " But what's worth noticing is that at least compared to T5 at the same size, so you can compare", "tokens": [583, 437, 311, 3163, 21814, 307, 300, 412, 1935, 5347, 281, 314, 20, 412, 264, 912, 2744, 11, 370, 291, 393, 6794], "temperature": 0.0, "avg_logprob": -0.12405588059198289, "compression_ratio": 1.5587044534412955, "no_speech_prob": 5.3068411943968385e-05}, {"id": 1016, "seek": 398960, "start": 4001.68, "end": 4007.44, "text": " 0.66 billion parameters against 0.77, it's actually already better than T5.", "tokens": [1958, 13, 15237, 5218, 9834, 1970, 1958, 13, 17512, 11, 309, 311, 767, 1217, 1101, 813, 314, 20, 13], "temperature": 0.0, "avg_logprob": -0.12405588059198289, "compression_ratio": 1.5587044534412955, "no_speech_prob": 5.3068411943968385e-05}, {"id": 1017, "seek": 398960, "start": 4007.44, "end": 4011.96, "text": " And compared to a T5 that's about 15 times larger, it's almost at the same performance", "tokens": [400, 5347, 281, 257, 314, 20, 300, 311, 466, 2119, 1413, 4833, 11, 309, 311, 1920, 412, 264, 912, 3389], "temperature": 0.0, "avg_logprob": -0.12405588059198289, "compression_ratio": 1.5587044534412955, "no_speech_prob": 5.3068411943968385e-05}, {"id": 1018, "seek": 398960, "start": 4011.96, "end": 4012.96, "text": " too.", "tokens": [886, 13], "temperature": 0.0, "avg_logprob": -0.12405588059198289, "compression_ratio": 1.5587044534412955, "no_speech_prob": 5.3068411943968385e-05}, {"id": 1019, "seek": 401296, "start": 4012.96, "end": 4022.12, "text": " And it's a pretty decent result for an approach that has no access to retrieval supervision.", "tokens": [400, 309, 311, 257, 1238, 8681, 1874, 337, 364, 3109, 300, 575, 572, 2105, 281, 19817, 3337, 32675, 13], "temperature": 0.0, "avg_logprob": -0.09509381480600643, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.5935682313283905e-05}, {"id": 1020, "seek": 401296, "start": 4022.12, "end": 4026.84, "text": " So one thing you might note though is that the better result requires gold passages.", "tokens": [407, 472, 551, 291, 1062, 3637, 1673, 307, 300, 264, 1101, 1874, 7029, 3821, 31589, 13], "temperature": 0.0, "avg_logprob": -0.09509381480600643, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.5935682313283905e-05}, {"id": 1021, "seek": 401296, "start": 4026.84, "end": 4030.36, "text": " And Orca and T5, these approaches don't require gold passages.", "tokens": [400, 1610, 496, 293, 314, 20, 11, 613, 11587, 500, 380, 3651, 3821, 31589, 13], "temperature": 0.0, "avg_logprob": -0.09509381480600643, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.5935682313283905e-05}, {"id": 1022, "seek": 401296, "start": 4030.36, "end": 4032.6, "text": " They only need query answer pairs.", "tokens": [814, 787, 643, 14581, 1867, 15494, 13], "temperature": 0.0, "avg_logprob": -0.09509381480600643, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.5935682313283905e-05}, {"id": 1023, "seek": 401296, "start": 4032.6, "end": 4037.64, "text": " And one advantage of that is that query answer pair data is actually pretty easy to get.", "tokens": [400, 472, 5002, 295, 300, 307, 300, 14581, 1867, 6119, 1412, 307, 767, 1238, 1858, 281, 483, 13], "temperature": 0.0, "avg_logprob": -0.09509381480600643, "compression_ratio": 1.6851851851851851, "no_speech_prob": 1.5935682313283905e-05}, {"id": 1024, "seek": 403764, "start": 4037.64, "end": 4043.3199999999997, "text": " So we could potentially get a lot more of it than if we were asking for passage passages as", "tokens": [407, 321, 727, 7263, 483, 257, 688, 544, 295, 309, 813, 498, 321, 645, 3365, 337, 11497, 31589, 382], "temperature": 0.0, "avg_logprob": -0.12525742848714191, "compression_ratio": 1.6996466431095407, "no_speech_prob": 4.784936663781991e-06}, {"id": 1025, "seek": 403764, "start": 4043.3199999999997, "end": 4044.3199999999997, "text": " well.", "tokens": [731, 13], "temperature": 0.0, "avg_logprob": -0.12525742848714191, "compression_ratio": 1.6996466431095407, "no_speech_prob": 4.784936663781991e-06}, {"id": 1026, "seek": 403764, "start": 4044.3199999999997, "end": 4050.6, "text": " And the final part of this retrieval section is about a way to get basically an arbitrary", "tokens": [400, 264, 2572, 644, 295, 341, 19817, 3337, 3541, 307, 466, 257, 636, 281, 483, 1936, 364, 23211], "temperature": 0.0, "avg_logprob": -0.12525742848714191, "compression_ratio": 1.6996466431095407, "no_speech_prob": 4.784936663781991e-06}, {"id": 1027, "seek": 403764, "start": 4050.6, "end": 4056.3199999999997, "text": " number of query answer pairs to kind of improve these weekly supervised approaches that don't", "tokens": [1230, 295, 14581, 1867, 15494, 281, 733, 295, 3470, 613, 12460, 46533, 11587, 300, 500, 380], "temperature": 0.0, "avg_logprob": -0.12525742848714191, "compression_ratio": 1.6996466431095407, "no_speech_prob": 4.784936663781991e-06}, {"id": 1028, "seek": 403764, "start": 4056.3199999999997, "end": 4057.64, "text": " have passages.", "tokens": [362, 31589, 13], "temperature": 0.0, "avg_logprob": -0.12525742848714191, "compression_ratio": 1.6996466431095407, "no_speech_prob": 4.784936663781991e-06}, {"id": 1029, "seek": 403764, "start": 4057.64, "end": 4061.8799999999997, "text": " So it comes from a very simple observation, which is let's take your typical query answer", "tokens": [407, 309, 1487, 490, 257, 588, 2199, 14816, 11, 597, 307, 718, 311, 747, 428, 7476, 14581, 1867], "temperature": 0.0, "avg_logprob": -0.12525742848714191, "compression_ratio": 1.6996466431095407, "no_speech_prob": 4.784936663781991e-06}, {"id": 1030, "seek": 403764, "start": 4061.8799999999997, "end": 4062.8799999999997, "text": " pair.", "tokens": [6119, 13], "temperature": 0.0, "avg_logprob": -0.12525742848714191, "compression_ratio": 1.6996466431095407, "no_speech_prob": 4.784936663781991e-06}, {"id": 1031, "seek": 403764, "start": 4062.8799999999997, "end": 4063.8799999999997, "text": " It looks like this, right?", "tokens": [467, 1542, 411, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.12525742848714191, "compression_ratio": 1.6996466431095407, "no_speech_prob": 4.784936663781991e-06}, {"id": 1032, "seek": 403764, "start": 4063.8799999999997, "end": 4065.96, "text": " So you've got your query on the left and answer on the right.", "tokens": [407, 291, 600, 658, 428, 14581, 322, 264, 1411, 293, 1867, 322, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.12525742848714191, "compression_ratio": 1.6996466431095407, "no_speech_prob": 4.784936663781991e-06}, {"id": 1033, "seek": 406596, "start": 4065.96, "end": 4070.76, "text": " You can easily reformulate that as a fill in the blank question like this.", "tokens": [509, 393, 3612, 8290, 5256, 300, 382, 257, 2836, 294, 264, 8247, 1168, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.11614888561658623, "compression_ratio": 1.9294117647058824, "no_speech_prob": 4.0691385947866365e-05}, {"id": 1034, "seek": 406596, "start": 4070.76, "end": 4074.64, "text": " And this fill in the blank question forces the model to think just as hard as the original", "tokens": [400, 341, 2836, 294, 264, 8247, 1168, 5874, 264, 2316, 281, 519, 445, 382, 1152, 382, 264, 3380], "temperature": 0.0, "avg_logprob": -0.11614888561658623, "compression_ratio": 1.9294117647058824, "no_speech_prob": 4.0691385947866365e-05}, {"id": 1035, "seek": 406596, "start": 4074.64, "end": 4077.92, "text": " question is just in a different format.", "tokens": [1168, 307, 445, 294, 257, 819, 7877, 13], "temperature": 0.0, "avg_logprob": -0.11614888561658623, "compression_ratio": 1.9294117647058824, "no_speech_prob": 4.0691385947866365e-05}, {"id": 1036, "seek": 406596, "start": 4077.92, "end": 4081.52, "text": " But what's nice about this fill in the blank question is that it's very easy to create", "tokens": [583, 437, 311, 1481, 466, 341, 2836, 294, 264, 8247, 1168, 307, 300, 309, 311, 588, 1858, 281, 1884], "temperature": 0.0, "avg_logprob": -0.11614888561658623, "compression_ratio": 1.9294117647058824, "no_speech_prob": 4.0691385947866365e-05}, {"id": 1037, "seek": 406596, "start": 4081.52, "end": 4083.84, "text": " a bunch of them for free.", "tokens": [257, 3840, 295, 552, 337, 1737, 13], "temperature": 0.0, "avg_logprob": -0.11614888561658623, "compression_ratio": 1.9294117647058824, "no_speech_prob": 4.0691385947866365e-05}, {"id": 1038, "seek": 406596, "start": 4083.84, "end": 4085.7200000000003, "text": " Basically you can just take any sentence on the web.", "tokens": [8537, 291, 393, 445, 747, 604, 8174, 322, 264, 3670, 13], "temperature": 0.0, "avg_logprob": -0.11614888561658623, "compression_ratio": 1.9294117647058824, "no_speech_prob": 4.0691385947866365e-05}, {"id": 1039, "seek": 406596, "start": 4085.7200000000003, "end": 4090.44, "text": " And as long as it's mentioning something factual or semantically meaningful, you can just", "tokens": [400, 382, 938, 382, 309, 311, 18315, 746, 48029, 420, 4361, 49505, 10995, 11, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.11614888561658623, "compression_ratio": 1.9294117647058824, "no_speech_prob": 4.0691385947866365e-05}, {"id": 1040, "seek": 406596, "start": 4090.44, "end": 4093.44, "text": " blank out one of the entities.", "tokens": [8247, 484, 472, 295, 264, 16667, 13], "temperature": 0.0, "avg_logprob": -0.11614888561658623, "compression_ratio": 1.9294117647058824, "no_speech_prob": 4.0691385947866365e-05}, {"id": 1041, "seek": 409344, "start": 4093.44, "end": 4097.76, "text": " And in fact, that is exactly what you've probably seen in previous lectures, pre-trained", "tokens": [400, 294, 1186, 11, 300, 307, 2293, 437, 291, 600, 1391, 1612, 294, 3894, 16564, 11, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.13597275599960454, "compression_ratio": 1.6350877192982456, "no_speech_prob": 3.218791971448809e-05}, {"id": 1042, "seek": 409344, "start": 4097.76, "end": 4099.68, "text": " language models like Bert do.", "tokens": [2856, 5245, 411, 29594, 360, 13], "temperature": 0.0, "avg_logprob": -0.13597275599960454, "compression_ratio": 1.6350877192982456, "no_speech_prob": 3.218791971448809e-05}, {"id": 1043, "seek": 409344, "start": 4099.68, "end": 4104.0, "text": " And Bert uses that training objective to learn a very great deal.", "tokens": [400, 29594, 4960, 300, 3097, 10024, 281, 1466, 257, 588, 869, 2028, 13], "temperature": 0.0, "avg_logprob": -0.13597275599960454, "compression_ratio": 1.6350877192982456, "no_speech_prob": 3.218791971448809e-05}, {"id": 1044, "seek": 409344, "start": 4104.0, "end": 4107.04, "text": " And that can be used in this setting for retrieval as well.", "tokens": [400, 300, 393, 312, 1143, 294, 341, 3287, 337, 19817, 3337, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.13597275599960454, "compression_ratio": 1.6350877192982456, "no_speech_prob": 3.218791971448809e-05}, {"id": 1045, "seek": 409344, "start": 4107.04, "end": 4112.6, "text": " So the basic idea for Realm, which is something that I worked on with collaborators, is to", "tokens": [407, 264, 3875, 1558, 337, 44723, 11, 597, 307, 746, 300, 286, 2732, 322, 365, 39789, 11, 307, 281], "temperature": 0.0, "avg_logprob": -0.13597275599960454, "compression_ratio": 1.6350877192982456, "no_speech_prob": 3.218791971448809e-05}, {"id": 1046, "seek": 409344, "start": 4112.6, "end": 4115.4400000000005, "text": " apply the same end-to-end training as Orca.", "tokens": [3079, 264, 912, 917, 12, 1353, 12, 521, 3097, 382, 1610, 496, 13], "temperature": 0.0, "avg_logprob": -0.13597275599960454, "compression_ratio": 1.6350877192982456, "no_speech_prob": 3.218791971448809e-05}, {"id": 1047, "seek": 409344, "start": 4115.4400000000005, "end": 4119.28, "text": " But pre-trained the model on a bunch of these fill in the blank questions that we just", "tokens": [583, 659, 12, 17227, 2001, 264, 2316, 322, 257, 3840, 295, 613, 2836, 294, 264, 8247, 1651, 300, 321, 445], "temperature": 0.0, "avg_logprob": -0.13597275599960454, "compression_ratio": 1.6350877192982456, "no_speech_prob": 3.218791971448809e-05}, {"id": 1048, "seek": 411928, "start": 4119.28, "end": 4124.24, "text": " automatically generated, just in extremely large quantities.", "tokens": [6772, 10833, 11, 445, 294, 4664, 2416, 22927, 13], "temperature": 0.0, "avg_logprob": -0.10871931291976066, "compression_ratio": 1.6446886446886446, "no_speech_prob": 2.3921931642689742e-05}, {"id": 1049, "seek": 411928, "start": 4124.24, "end": 4130.04, "text": " And then we fine-tune that on the real questions query answer pairs that we already have.", "tokens": [400, 550, 321, 2489, 12, 83, 2613, 300, 322, 264, 957, 1651, 14581, 1867, 15494, 300, 321, 1217, 362, 13], "temperature": 0.0, "avg_logprob": -0.10871931291976066, "compression_ratio": 1.6446886446886446, "no_speech_prob": 2.3921931642689742e-05}, {"id": 1050, "seek": 411928, "start": 4130.04, "end": 4134.12, "text": " So if you do this approach and you plot it against all the others, what's quite nice is", "tokens": [407, 498, 291, 360, 341, 3109, 293, 291, 7542, 309, 1970, 439, 264, 2357, 11, 437, 311, 1596, 1481, 307], "temperature": 0.0, "avg_logprob": -0.10871931291976066, "compression_ratio": 1.6446886446886446, "no_speech_prob": 2.3921931642689742e-05}, {"id": 1051, "seek": 411928, "start": 4134.12, "end": 4141.759999999999, "text": " that it basically almost closes the gap completely with an approach that uses supervised data.", "tokens": [300, 309, 1936, 1920, 24157, 264, 7417, 2584, 365, 364, 3109, 300, 4960, 46533, 1412, 13], "temperature": 0.0, "avg_logprob": -0.10871931291976066, "compression_ratio": 1.6446886446886446, "no_speech_prob": 2.3921931642689742e-05}, {"id": 1052, "seek": 411928, "start": 4141.759999999999, "end": 4144.08, "text": " Just by pre-training on fill in the blank questions.", "tokens": [1449, 538, 659, 12, 17227, 1760, 322, 2836, 294, 264, 8247, 1651, 13], "temperature": 0.0, "avg_logprob": -0.10871931291976066, "compression_ratio": 1.6446886446886446, "no_speech_prob": 2.3921931642689742e-05}, {"id": 1053, "seek": 411928, "start": 4144.08, "end": 4146.679999999999, "text": " And the nice thing is it doesn't need access to gold passages.", "tokens": [400, 264, 1481, 551, 307, 309, 1177, 380, 643, 2105, 281, 3821, 31589, 13], "temperature": 0.0, "avg_logprob": -0.10871931291976066, "compression_ratio": 1.6446886446886446, "no_speech_prob": 2.3921931642689742e-05}, {"id": 1054, "seek": 414668, "start": 4146.68, "end": 4151.56, "text": " So it's on the same footing as things like T5 now.", "tokens": [407, 309, 311, 322, 264, 912, 45959, 382, 721, 411, 314, 20, 586, 13], "temperature": 0.0, "avg_logprob": -0.12199996916715763, "compression_ratio": 1.7564575645756457, "no_speech_prob": 1.5204920600808691e-05}, {"id": 1055, "seek": 414668, "start": 4151.56, "end": 4156.64, "text": " And at the same footing, it outperforms T5, despite being much smaller than even the largest", "tokens": [400, 412, 264, 912, 45959, 11, 309, 484, 26765, 82, 314, 20, 11, 7228, 885, 709, 4356, 813, 754, 264, 6443], "temperature": 0.0, "avg_logprob": -0.12199996916715763, "compression_ratio": 1.7564575645756457, "no_speech_prob": 1.5204920600808691e-05}, {"id": 1056, "seek": 414668, "start": 4156.64, "end": 4157.4800000000005, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.12199996916715763, "compression_ratio": 1.7564575645756457, "no_speech_prob": 1.5204920600808691e-05}, {"id": 1057, "seek": 414668, "start": 4157.4800000000005, "end": 4162.08, "text": " So that gives us this interesting promise of using kind of language model fill in the", "tokens": [407, 300, 2709, 505, 341, 1880, 6228, 295, 1228, 733, 295, 2856, 2316, 2836, 294, 264], "temperature": 0.0, "avg_logprob": -0.12199996916715763, "compression_ratio": 1.7564575645756457, "no_speech_prob": 1.5204920600808691e-05}, {"id": 1058, "seek": 414668, "start": 4162.08, "end": 4165.52, "text": " blank techniques to build good memory retrievers.", "tokens": [8247, 7512, 281, 1322, 665, 4675, 19817, 840, 13], "temperature": 0.0, "avg_logprob": -0.12199996916715763, "compression_ratio": 1.7564575645756457, "no_speech_prob": 1.5204920600808691e-05}, {"id": 1059, "seek": 414668, "start": 4165.52, "end": 4170.12, "text": " And the nice thing is that this fill in the blank approach can be used to tackle many", "tokens": [400, 264, 1481, 551, 307, 300, 341, 2836, 294, 264, 8247, 3109, 393, 312, 1143, 281, 14896, 867], "temperature": 0.0, "avg_logprob": -0.12199996916715763, "compression_ratio": 1.7564575645756457, "no_speech_prob": 1.5204920600808691e-05}, {"id": 1060, "seek": 414668, "start": 4170.12, "end": 4171.8, "text": " sorts of tasks.", "tokens": [7527, 295, 9608, 13], "temperature": 0.0, "avg_logprob": -0.12199996916715763, "compression_ratio": 1.7564575645756457, "no_speech_prob": 1.5204920600808691e-05}, {"id": 1061, "seek": 414668, "start": 4171.8, "end": 4176.04, "text": " You could blank out a patch in an image and train a retriever to find other images that", "tokens": [509, 727, 8247, 484, 257, 9972, 294, 364, 3256, 293, 3847, 257, 19817, 331, 281, 915, 661, 5267, 300], "temperature": 0.0, "avg_logprob": -0.12199996916715763, "compression_ratio": 1.7564575645756457, "no_speech_prob": 1.5204920600808691e-05}, {"id": 1062, "seek": 417604, "start": 4176.04, "end": 4177.5199999999995, "text": " might help fill it in.", "tokens": [1062, 854, 2836, 309, 294, 13], "temperature": 0.0, "avg_logprob": -0.11947873600742273, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.586405935289804e-05}, {"id": 1063, "seek": 417604, "start": 4177.5199999999995, "end": 4182.04, "text": " You could blank out a segment of code and train a retriever to find other pieces of code", "tokens": [509, 727, 8247, 484, 257, 9469, 295, 3089, 293, 3847, 257, 19817, 331, 281, 915, 661, 3755, 295, 3089], "temperature": 0.0, "avg_logprob": -0.11947873600742273, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.586405935289804e-05}, {"id": 1064, "seek": 417604, "start": 4182.04, "end": 4183.76, "text": " that might help fill that in.", "tokens": [300, 1062, 854, 2836, 300, 294, 13], "temperature": 0.0, "avg_logprob": -0.11947873600742273, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.586405935289804e-05}, {"id": 1065, "seek": 417604, "start": 4183.76, "end": 4185.24, "text": " Or chapter in a textbook.", "tokens": [1610, 7187, 294, 257, 25591, 13], "temperature": 0.0, "avg_logprob": -0.11947873600742273, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.586405935289804e-05}, {"id": 1066, "seek": 417604, "start": 4185.24, "end": 4190.2, "text": " The kind of list of things you can do with fill in the blank actually goes on and on.", "tokens": [440, 733, 295, 1329, 295, 721, 291, 393, 360, 365, 2836, 294, 264, 8247, 767, 1709, 322, 293, 322, 13], "temperature": 0.0, "avg_logprob": -0.11947873600742273, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.586405935289804e-05}, {"id": 1067, "seek": 417604, "start": 4190.2, "end": 4195.04, "text": " And each task that you define in this way produces a specialized memory retriever for whatever", "tokens": [400, 1184, 5633, 300, 291, 6964, 294, 341, 636, 14725, 257, 19813, 4675, 19817, 331, 337, 2035], "temperature": 0.0, "avg_logprob": -0.11947873600742273, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.586405935289804e-05}, {"id": 1068, "seek": 417604, "start": 4195.04, "end": 4198.36, "text": " it is that you're filling the blank in for.", "tokens": [309, 307, 300, 291, 434, 10623, 264, 8247, 294, 337, 13], "temperature": 0.0, "avg_logprob": -0.11947873600742273, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.586405935289804e-05}, {"id": 1069, "seek": 417604, "start": 4198.36, "end": 4200.72, "text": " And there's no need to collect training data.", "tokens": [400, 456, 311, 572, 643, 281, 2500, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11947873600742273, "compression_ratio": 1.7380952380952381, "no_speech_prob": 2.586405935289804e-05}, {"id": 1070, "seek": 420072, "start": 4200.72, "end": 4207.6, "text": " So this sort of scales to any set of tasks that may not be important enough for central", "tokens": [407, 341, 1333, 295, 17408, 281, 604, 992, 295, 9608, 300, 815, 406, 312, 1021, 1547, 337, 5777], "temperature": 0.0, "avg_logprob": -0.12767509082416156, "compression_ratio": 1.7348484848484849, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1071, "seek": 420072, "start": 4207.6, "end": 4210.52, "text": " enough to warn a big data collection budget.", "tokens": [1547, 281, 12286, 257, 955, 1412, 5765, 4706, 13], "temperature": 0.0, "avg_logprob": -0.12767509082416156, "compression_ratio": 1.7348484848484849, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1072, "seek": 420072, "start": 4210.52, "end": 4215.2, "text": " All right, so the main takeaways for this section are that a retriever is just a function", "tokens": [1057, 558, 11, 370, 264, 2135, 45584, 337, 341, 3541, 366, 300, 257, 19817, 331, 307, 445, 257, 2445], "temperature": 0.0, "avg_logprob": -0.12767509082416156, "compression_ratio": 1.7348484848484849, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1073, "seek": 420072, "start": 4215.2, "end": 4219.360000000001, "text": " that takes an input and a memory and produces a score.", "tokens": [300, 2516, 364, 4846, 293, 257, 4675, 293, 14725, 257, 6175, 13], "temperature": 0.0, "avg_logprob": -0.12767509082416156, "compression_ratio": 1.7348484848484849, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1074, "seek": 420072, "start": 4219.360000000001, "end": 4222.92, "text": " If you have supervised data for your retriever, that's great.", "tokens": [759, 291, 362, 46533, 1412, 337, 428, 19817, 331, 11, 300, 311, 869, 13], "temperature": 0.0, "avg_logprob": -0.12767509082416156, "compression_ratio": 1.7348484848484849, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1075, "seek": 420072, "start": 4222.92, "end": 4225.4400000000005, "text": " Provide positive and negative memories for each input.", "tokens": [15685, 482, 3353, 293, 3671, 8495, 337, 1184, 4846, 13], "temperature": 0.0, "avg_logprob": -0.12767509082416156, "compression_ratio": 1.7348484848484849, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1076, "seek": 420072, "start": 4225.4400000000005, "end": 4228.72, "text": " And just train the retriever to score the positive ones higher.", "tokens": [400, 445, 3847, 264, 19817, 331, 281, 6175, 264, 3353, 2306, 2946, 13], "temperature": 0.0, "avg_logprob": -0.12767509082416156, "compression_ratio": 1.7348484848484849, "no_speech_prob": 1.0289051715517417e-05}, {"id": 1077, "seek": 422872, "start": 4228.72, "end": 4233.360000000001, "text": " If you don't have supervision, you can use end-to-end learning, which employs a trial", "tokens": [759, 291, 500, 380, 362, 32675, 11, 291, 393, 764, 917, 12, 1353, 12, 521, 2539, 11, 597, 846, 49522, 257, 7308], "temperature": 0.0, "avg_logprob": -0.1694542407989502, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2029158824589103e-05}, {"id": 1078, "seek": 422872, "start": 4233.360000000001, "end": 4234.64, "text": " and error approach.", "tokens": [293, 6713, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1694542407989502, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2029158824589103e-05}, {"id": 1079, "seek": 422872, "start": 4234.64, "end": 4240.240000000001, "text": " If a memory helps the model score the memory higher, otherwise score it lower.", "tokens": [759, 257, 4675, 3665, 264, 2316, 6175, 264, 4675, 2946, 11, 5911, 6175, 309, 3126, 13], "temperature": 0.0, "avg_logprob": -0.1694542407989502, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2029158824589103e-05}, {"id": 1080, "seek": 422872, "start": 4240.240000000001, "end": 4244.280000000001, "text": " And with end-to-end learning, you often get this special benefit that you can easily create", "tokens": [400, 365, 917, 12, 1353, 12, 521, 2539, 11, 291, 2049, 483, 341, 2121, 5121, 300, 291, 393, 3612, 1884], "temperature": 0.0, "avg_logprob": -0.1694542407989502, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2029158824589103e-05}, {"id": 1081, "seek": 422872, "start": 4244.280000000001, "end": 4247.56, "text": " tons of data to pre-train your retriever.", "tokens": [9131, 295, 1412, 281, 659, 12, 83, 7146, 428, 19817, 331, 13], "temperature": 0.0, "avg_logprob": -0.1694542407989502, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2029158824589103e-05}, {"id": 1082, "seek": 422872, "start": 4247.56, "end": 4254.08, "text": " All right, we're now into the very, very final part of the talk, which is how to actually", "tokens": [1057, 558, 11, 321, 434, 586, 666, 264, 588, 11, 588, 2572, 644, 295, 264, 751, 11, 597, 307, 577, 281, 767], "temperature": 0.0, "avg_logprob": -0.1694542407989502, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2029158824589103e-05}, {"id": 1083, "seek": 422872, "start": 4254.08, "end": 4256.64, "text": " use the memories after you get them.", "tokens": [764, 264, 8495, 934, 291, 483, 552, 13], "temperature": 0.0, "avg_logprob": -0.1694542407989502, "compression_ratio": 1.6984732824427482, "no_speech_prob": 1.2029158824589103e-05}, {"id": 1084, "seek": 425664, "start": 4256.64, "end": 4259.240000000001, "text": " Notice I have 15 minutes, right?", "tokens": [13428, 286, 362, 2119, 2077, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2298319746808308, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.141267269616947e-05}, {"id": 1085, "seek": 425664, "start": 4259.240000000001, "end": 4261.240000000001, "text": " Yes, hold in.", "tokens": [1079, 11, 1797, 294, 13], "temperature": 0.0, "avg_logprob": -0.2298319746808308, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.141267269616947e-05}, {"id": 1086, "seek": 425664, "start": 4261.240000000001, "end": 4264.6, "text": " Okay, all right, then we should have plenty of time for questions, actually.", "tokens": [1033, 11, 439, 558, 11, 550, 321, 820, 362, 7140, 295, 565, 337, 1651, 11, 767, 13], "temperature": 0.0, "avg_logprob": -0.2298319746808308, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.141267269616947e-05}, {"id": 1087, "seek": 425664, "start": 4264.6, "end": 4266.8, "text": " So all right, here we go.", "tokens": [407, 439, 558, 11, 510, 321, 352, 13], "temperature": 0.0, "avg_logprob": -0.2298319746808308, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.141267269616947e-05}, {"id": 1088, "seek": 425664, "start": 4266.8, "end": 4270.280000000001, "text": " We're going to come back to this diagram of a memory-augmented model.", "tokens": [492, 434, 516, 281, 808, 646, 281, 341, 10686, 295, 257, 4675, 12, 20056, 14684, 2316, 13], "temperature": 0.0, "avg_logprob": -0.2298319746808308, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.141267269616947e-05}, {"id": 1089, "seek": 425664, "start": 4270.280000000001, "end": 4273.64, "text": " And now we're going to focus on this reader component, which I didn't say much about", "tokens": [400, 586, 321, 434, 516, 281, 1879, 322, 341, 15149, 6542, 11, 597, 286, 994, 380, 584, 709, 466], "temperature": 0.0, "avg_logprob": -0.2298319746808308, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.141267269616947e-05}, {"id": 1090, "seek": 425664, "start": 4273.64, "end": 4274.64, "text": " before.", "tokens": [949, 13], "temperature": 0.0, "avg_logprob": -0.2298319746808308, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.141267269616947e-05}, {"id": 1091, "seek": 425664, "start": 4274.64, "end": 4277.6, "text": " I said that the reader takes the memory and the input and then produces the answer.", "tokens": [286, 848, 300, 264, 15149, 2516, 264, 4675, 293, 264, 4846, 293, 550, 14725, 264, 1867, 13], "temperature": 0.0, "avg_logprob": -0.2298319746808308, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.141267269616947e-05}, {"id": 1092, "seek": 425664, "start": 4277.6, "end": 4281.360000000001, "text": " So what does that reader component actually look like?", "tokens": [407, 437, 775, 300, 15149, 6542, 767, 574, 411, 30], "temperature": 0.0, "avg_logprob": -0.2298319746808308, "compression_ratio": 1.6765799256505576, "no_speech_prob": 7.141267269616947e-05}, {"id": 1093, "seek": 428136, "start": 4281.36, "end": 4287.5599999999995, "text": " A very common architecture is just the sequence encoder to decoder model.", "tokens": [316, 588, 2689, 9482, 307, 445, 264, 8310, 2058, 19866, 281, 979, 19866, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12355230422247024, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.3185977624962106e-05}, {"id": 1094, "seek": 428136, "start": 4287.5599999999995, "end": 4292.36, "text": " In practical terms, you take the original question and you just concatenate it to the", "tokens": [682, 8496, 2115, 11, 291, 747, 264, 3380, 1168, 293, 291, 445, 1588, 7186, 473, 309, 281, 264], "temperature": 0.0, "avg_logprob": -0.12355230422247024, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.3185977624962106e-05}, {"id": 1095, "seek": 428136, "start": 4292.36, "end": 4294.04, "text": " memory that you retrieved.", "tokens": [4675, 300, 291, 19817, 937, 13], "temperature": 0.0, "avg_logprob": -0.12355230422247024, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.3185977624962106e-05}, {"id": 1096, "seek": 428136, "start": 4294.04, "end": 4297.92, "text": " And then you feed that into your encoder and you train it using standard sequence to sequence", "tokens": [400, 550, 291, 3154, 300, 666, 428, 2058, 19866, 293, 291, 3847, 309, 1228, 3832, 8310, 281, 8310], "temperature": 0.0, "avg_logprob": -0.12355230422247024, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.3185977624962106e-05}, {"id": 1097, "seek": 428136, "start": 4297.92, "end": 4300.36, "text": " learning to produce the output.", "tokens": [2539, 281, 5258, 264, 5598, 13], "temperature": 0.0, "avg_logprob": -0.12355230422247024, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.3185977624962106e-05}, {"id": 1098, "seek": 428136, "start": 4300.36, "end": 4304.2, "text": " So all we're doing is just concatenating the memory with the input.", "tokens": [407, 439, 321, 434, 884, 307, 445, 1588, 7186, 990, 264, 4675, 365, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.12355230422247024, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.3185977624962106e-05}, {"id": 1099, "seek": 428136, "start": 4304.2, "end": 4307.48, "text": " And we can refer to that as text fusion.", "tokens": [400, 321, 393, 2864, 281, 300, 382, 2487, 23100, 13], "temperature": 0.0, "avg_logprob": -0.12355230422247024, "compression_ratio": 1.7763713080168777, "no_speech_prob": 2.3185977624962106e-05}, {"id": 1100, "seek": 430748, "start": 4307.48, "end": 4312.16, "text": " Anytime you have a memory that is text or can be converted into text in some form, just", "tokens": [39401, 291, 362, 257, 4675, 300, 307, 2487, 420, 393, 312, 16424, 666, 2487, 294, 512, 1254, 11, 445], "temperature": 0.0, "avg_logprob": -0.16020483806215483, "compression_ratio": 1.6607773851590106, "no_speech_prob": 3.76336247427389e-05}, {"id": 1101, "seek": 430748, "start": 4312.16, "end": 4313.16, "text": " concatenated.", "tokens": [1588, 7186, 770, 13], "temperature": 0.0, "avg_logprob": -0.16020483806215483, "compression_ratio": 1.6607773851590106, "no_speech_prob": 3.76336247427389e-05}, {"id": 1102, "seek": 430748, "start": 4313.16, "end": 4315.599999999999, "text": " That's all you need.", "tokens": [663, 311, 439, 291, 643, 13], "temperature": 0.0, "avg_logprob": -0.16020483806215483, "compression_ratio": 1.6607773851590106, "no_speech_prob": 3.76336247427389e-05}, {"id": 1103, "seek": 430748, "start": 4315.599999999999, "end": 4320.48, "text": " At least that's what state of the art techniques are doing right now.", "tokens": [1711, 1935, 300, 311, 437, 1785, 295, 264, 1523, 7512, 366, 884, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.16020483806215483, "compression_ratio": 1.6607773851590106, "no_speech_prob": 3.76336247427389e-05}, {"id": 1104, "seek": 430748, "start": 4320.48, "end": 4325.639999999999, "text": " Okay, and just to give some variety, here's another way to incorporate memories.", "tokens": [1033, 11, 293, 445, 281, 976, 512, 5673, 11, 510, 311, 1071, 636, 281, 16091, 8495, 13], "temperature": 0.0, "avg_logprob": -0.16020483806215483, "compression_ratio": 1.6607773851590106, "no_speech_prob": 3.76336247427389e-05}, {"id": 1105, "seek": 430748, "start": 4325.639999999999, "end": 4331.959999999999, "text": " Let's consider a slightly different memory-augmented model where instead of just retrieving a document,", "tokens": [961, 311, 1949, 257, 4748, 819, 4675, 12, 20056, 14684, 2316, 689, 2602, 295, 445, 19817, 798, 257, 4166, 11], "temperature": 0.0, "avg_logprob": -0.16020483806215483, "compression_ratio": 1.6607773851590106, "no_speech_prob": 3.76336247427389e-05}, {"id": 1106, "seek": 430748, "start": 4331.959999999999, "end": 4337.28, "text": " the memory is actually key value pairs where the key is the question, like a question that's", "tokens": [264, 4675, 307, 767, 2141, 2158, 15494, 689, 264, 2141, 307, 264, 1168, 11, 411, 257, 1168, 300, 311], "temperature": 0.0, "avg_logprob": -0.16020483806215483, "compression_ratio": 1.6607773851590106, "no_speech_prob": 3.76336247427389e-05}, {"id": 1107, "seek": 433728, "start": 4337.28, "end": 4341.679999999999, "text": " been seen before and the value is the answer to that previously seen question.", "tokens": [668, 1612, 949, 293, 264, 2158, 307, 264, 1867, 281, 300, 8046, 1612, 1168, 13], "temperature": 0.0, "avg_logprob": -0.08021476205471342, "compression_ratio": 1.8073770491803278, "no_speech_prob": 2.7103789761895314e-05}, {"id": 1108, "seek": 433728, "start": 4341.679999999999, "end": 4346.24, "text": " So in this case, you can do something even simpler than what was on the previous slide.", "tokens": [407, 294, 341, 1389, 11, 291, 393, 360, 746, 754, 18587, 813, 437, 390, 322, 264, 3894, 4137, 13], "temperature": 0.0, "avg_logprob": -0.08021476205471342, "compression_ratio": 1.8073770491803278, "no_speech_prob": 2.7103789761895314e-05}, {"id": 1109, "seek": 433728, "start": 4346.24, "end": 4350.679999999999, "text": " You can take your input and compare it to the keys and find the key that most resembles", "tokens": [509, 393, 747, 428, 4846, 293, 6794, 309, 281, 264, 9317, 293, 915, 264, 2141, 300, 881, 34433], "temperature": 0.0, "avg_logprob": -0.08021476205471342, "compression_ratio": 1.8073770491803278, "no_speech_prob": 2.7103789761895314e-05}, {"id": 1110, "seek": 433728, "start": 4350.679999999999, "end": 4351.679999999999, "text": " the input.", "tokens": [264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.08021476205471342, "compression_ratio": 1.8073770491803278, "no_speech_prob": 2.7103789761895314e-05}, {"id": 1111, "seek": 433728, "start": 4351.679999999999, "end": 4355.36, "text": " So in this case, we found a paraphrase of the original question.", "tokens": [407, 294, 341, 1389, 11, 321, 1352, 257, 36992, 1703, 651, 295, 264, 3380, 1168, 13], "temperature": 0.0, "avg_logprob": -0.08021476205471342, "compression_ratio": 1.8073770491803278, "no_speech_prob": 2.7103789761895314e-05}, {"id": 1112, "seek": 433728, "start": 4355.36, "end": 4360.84, "text": " And if you have this, then all you really need to do is just copy the answer from the", "tokens": [400, 498, 291, 362, 341, 11, 550, 439, 291, 534, 643, 281, 360, 307, 445, 5055, 264, 1867, 490, 264], "temperature": 0.0, "avg_logprob": -0.08021476205471342, "compression_ratio": 1.8073770491803278, "no_speech_prob": 2.7103789761895314e-05}, {"id": 1113, "seek": 433728, "start": 4360.84, "end": 4363.4, "text": " value out as your label.", "tokens": [2158, 484, 382, 428, 7645, 13], "temperature": 0.0, "avg_logprob": -0.08021476205471342, "compression_ratio": 1.8073770491803278, "no_speech_prob": 2.7103789761895314e-05}, {"id": 1114, "seek": 436340, "start": 4363.4, "end": 4369.44, "text": " And that's what people refer to as label smearing or otherwise nearest neighbor's methods.", "tokens": [400, 300, 311, 437, 561, 2864, 281, 382, 7645, 41818, 1921, 420, 5911, 23831, 5987, 311, 7150, 13], "temperature": 0.0, "avg_logprob": -0.12909145547886086, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.2471667762147263e-05}, {"id": 1115, "seek": 436340, "start": 4369.44, "end": 4372.839999999999, "text": " We call it smearing because you're essentially smearing the label from this example that you", "tokens": [492, 818, 309, 41818, 1921, 570, 291, 434, 4476, 41818, 1921, 264, 7645, 490, 341, 1365, 300, 291], "temperature": 0.0, "avg_logprob": -0.12909145547886086, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.2471667762147263e-05}, {"id": 1116, "seek": 436340, "start": 4372.839999999999, "end": 4376.36, "text": " retrieved onto the new example.", "tokens": [19817, 937, 3911, 264, 777, 1365, 13], "temperature": 0.0, "avg_logprob": -0.12909145547886086, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.2471667762147263e-05}, {"id": 1117, "seek": 436340, "start": 4376.36, "end": 4382.04, "text": " And it's a very simple technique, which one you use depends on your application.", "tokens": [400, 309, 311, 257, 588, 2199, 6532, 11, 597, 472, 291, 764, 5946, 322, 428, 3861, 13], "temperature": 0.0, "avg_logprob": -0.12909145547886086, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.2471667762147263e-05}, {"id": 1118, "seek": 436340, "start": 4382.04, "end": 4385.759999999999, "text": " So the techniques we're using memories are quite simple, but the problems that arise", "tokens": [407, 264, 7512, 321, 434, 1228, 8495, 366, 1596, 2199, 11, 457, 264, 2740, 300, 20288], "temperature": 0.0, "avg_logprob": -0.12909145547886086, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.2471667762147263e-05}, {"id": 1119, "seek": 436340, "start": 4385.759999999999, "end": 4389.04, "text": " when you use them are actually quite interesting.", "tokens": [562, 291, 764, 552, 366, 767, 1596, 1880, 13], "temperature": 0.0, "avg_logprob": -0.12909145547886086, "compression_ratio": 1.7309236947791165, "no_speech_prob": 2.2471667762147263e-05}, {"id": 1120, "seek": 438904, "start": 4389.04, "end": 4393.12, "text": " The first one I want to talk about, I kind of mentioned this preview this earlier, are these", "tokens": [440, 700, 472, 286, 528, 281, 751, 466, 11, 286, 733, 295, 2835, 341, 14281, 341, 3071, 11, 366, 613], "temperature": 0.0, "avg_logprob": -0.16034048694675251, "compression_ratio": 1.5961538461538463, "no_speech_prob": 6.539386959047988e-06}, {"id": 1121, "seek": 438904, "start": 4393.12, "end": 4396.5199999999995, "text": " two problems of underutilization and overreliance.", "tokens": [732, 2740, 295, 833, 20835, 2144, 293, 670, 265, 2081, 719, 13], "temperature": 0.0, "avg_logprob": -0.16034048694675251, "compression_ratio": 1.5961538461538463, "no_speech_prob": 6.539386959047988e-06}, {"id": 1122, "seek": 438904, "start": 4396.5199999999995, "end": 4399.36, "text": " So let's get into the underutilization issue.", "tokens": [407, 718, 311, 483, 666, 264, 833, 20835, 2144, 2734, 13], "temperature": 0.0, "avg_logprob": -0.16034048694675251, "compression_ratio": 1.5961538461538463, "no_speech_prob": 6.539386959047988e-06}, {"id": 1123, "seek": 438904, "start": 4399.36, "end": 4403.8, "text": " This is from a very recent paper by Longprey et al.", "tokens": [639, 307, 490, 257, 588, 5162, 3035, 538, 8282, 3712, 88, 1030, 419, 13], "temperature": 0.0, "avg_logprob": -0.16034048694675251, "compression_ratio": 1.5961538461538463, "no_speech_prob": 6.539386959047988e-06}, {"id": 1124, "seek": 438904, "start": 4403.8, "end": 4405.2, "text": " So let me dive into it.", "tokens": [407, 718, 385, 9192, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.16034048694675251, "compression_ratio": 1.5961538461538463, "no_speech_prob": 6.539386959047988e-06}, {"id": 1125, "seek": 438904, "start": 4405.2, "end": 4409.88, "text": " So, okay, I switched the example here because I just got tired of the Lord of the Rings", "tokens": [407, 11, 1392, 11, 286, 16858, 264, 1365, 510, 570, 286, 445, 658, 5868, 295, 264, 3257, 295, 264, 38543], "temperature": 0.0, "avg_logprob": -0.16034048694675251, "compression_ratio": 1.5961538461538463, "no_speech_prob": 6.539386959047988e-06}, {"id": 1126, "seek": 438904, "start": 4409.88, "end": 4411.48, "text": " One.", "tokens": [1485, 13], "temperature": 0.0, "avg_logprob": -0.16034048694675251, "compression_ratio": 1.5961538461538463, "no_speech_prob": 6.539386959047988e-06}, {"id": 1127, "seek": 438904, "start": 4411.48, "end": 4414.08, "text": " The question is, who do you meet at the gates of heaven?", "tokens": [440, 1168, 307, 11, 567, 360, 291, 1677, 412, 264, 19792, 295, 7162, 30], "temperature": 0.0, "avg_logprob": -0.16034048694675251, "compression_ratio": 1.5961538461538463, "no_speech_prob": 6.539386959047988e-06}, {"id": 1128, "seek": 441408, "start": 4414.08, "end": 4419.4, "text": " And the retrieved memory is on point, it says you see Saint Peter at the gates of heaven,", "tokens": [400, 264, 19817, 937, 4675, 307, 322, 935, 11, 309, 1619, 291, 536, 12902, 6508, 412, 264, 19792, 295, 7162, 11], "temperature": 0.0, "avg_logprob": -0.1278200543616429, "compression_ratio": 1.8106060606060606, "no_speech_prob": 2.5456030925852247e-05}, {"id": 1129, "seek": 441408, "start": 4419.4, "end": 4422.5199999999995, "text": " the reader reads it, produces Saint Peter, everything is great.", "tokens": [264, 15149, 15700, 309, 11, 14725, 12902, 6508, 11, 1203, 307, 869, 13], "temperature": 0.0, "avg_logprob": -0.1278200543616429, "compression_ratio": 1.8106060606060606, "no_speech_prob": 2.5456030925852247e-05}, {"id": 1130, "seek": 441408, "start": 4422.5199999999995, "end": 4427.5599999999995, "text": " So what Longprey et al observed is, okay, if the reader is really doing such a great job", "tokens": [407, 437, 8282, 3712, 88, 1030, 419, 13095, 307, 11, 1392, 11, 498, 264, 15149, 307, 534, 884, 1270, 257, 869, 1691], "temperature": 0.0, "avg_logprob": -0.1278200543616429, "compression_ratio": 1.8106060606060606, "no_speech_prob": 2.5456030925852247e-05}, {"id": 1131, "seek": 441408, "start": 4427.5599999999995, "end": 4432.4, "text": " of reading the memories, then if I edit the memory to say something else, the reader", "tokens": [295, 3760, 264, 8495, 11, 550, 498, 286, 8129, 264, 4675, 281, 584, 746, 1646, 11, 264, 15149], "temperature": 0.0, "avg_logprob": -0.1278200543616429, "compression_ratio": 1.8106060606060606, "no_speech_prob": 2.5456030925852247e-05}, {"id": 1132, "seek": 441408, "start": 4432.4, "end": 4435.76, "text": " should pick up on that and produce the different answer.", "tokens": [820, 1888, 493, 322, 300, 293, 5258, 264, 819, 1867, 13], "temperature": 0.0, "avg_logprob": -0.1278200543616429, "compression_ratio": 1.8106060606060606, "no_speech_prob": 2.5456030925852247e-05}, {"id": 1133, "seek": 441408, "start": 4435.76, "end": 4440.4, "text": " So what they do is they change Saint Peter to the United Nations, guards the gates of", "tokens": [407, 437, 436, 360, 307, 436, 1319, 12902, 6508, 281, 264, 2824, 16459, 11, 17652, 264, 19792, 295], "temperature": 0.0, "avg_logprob": -0.1278200543616429, "compression_ratio": 1.8106060606060606, "no_speech_prob": 2.5456030925852247e-05}, {"id": 1134, "seek": 441408, "start": 4440.4, "end": 4441.88, "text": " heaven.", "tokens": [7162, 13], "temperature": 0.0, "avg_logprob": -0.1278200543616429, "compression_ratio": 1.8106060606060606, "no_speech_prob": 2.5456030925852247e-05}, {"id": 1135, "seek": 444188, "start": 4441.88, "end": 4446.400000000001, "text": " And they check if the reader actually produces the United Nations.", "tokens": [400, 436, 1520, 498, 264, 15149, 767, 14725, 264, 2824, 16459, 13], "temperature": 0.0, "avg_logprob": -0.13152502021011042, "compression_ratio": 1.593625498007968, "no_speech_prob": 2.796412809402682e-05}, {"id": 1136, "seek": 444188, "start": 4446.400000000001, "end": 4452.16, "text": " But surprisingly, the reader still says that Saint Peter guards the gates of heaven.", "tokens": [583, 17600, 11, 264, 15149, 920, 1619, 300, 12902, 6508, 17652, 264, 19792, 295, 7162, 13], "temperature": 0.0, "avg_logprob": -0.13152502021011042, "compression_ratio": 1.593625498007968, "no_speech_prob": 2.796412809402682e-05}, {"id": 1137, "seek": 444188, "start": 4452.16, "end": 4455.04, "text": " This is really quite interesting and pretty funny.", "tokens": [639, 307, 534, 1596, 1880, 293, 1238, 4074, 13], "temperature": 0.0, "avg_logprob": -0.13152502021011042, "compression_ratio": 1.593625498007968, "no_speech_prob": 2.796412809402682e-05}, {"id": 1138, "seek": 444188, "start": 4455.04, "end": 4458.2, "text": " So what's actually going on here?", "tokens": [407, 437, 311, 767, 516, 322, 510, 30], "temperature": 0.0, "avg_logprob": -0.13152502021011042, "compression_ratio": 1.593625498007968, "no_speech_prob": 2.796412809402682e-05}, {"id": 1139, "seek": 444188, "start": 4458.2, "end": 4460.6, "text": " Let's first look at how bad this problem is.", "tokens": [961, 311, 700, 574, 412, 577, 1578, 341, 1154, 307, 13], "temperature": 0.0, "avg_logprob": -0.13152502021011042, "compression_ratio": 1.593625498007968, "no_speech_prob": 2.796412809402682e-05}, {"id": 1140, "seek": 444188, "start": 4460.6, "end": 4463.12, "text": " So here's a plot from the paper.", "tokens": [407, 510, 311, 257, 7542, 490, 264, 3035, 13], "temperature": 0.0, "avg_logprob": -0.13152502021011042, "compression_ratio": 1.593625498007968, "no_speech_prob": 2.796412809402682e-05}, {"id": 1141, "seek": 444188, "start": 4463.12, "end": 4468.56, "text": " The first row here is the model's behavior on the training set for natural questions.", "tokens": [440, 700, 5386, 510, 307, 264, 2316, 311, 5223, 322, 264, 3097, 992, 337, 3303, 1651, 13], "temperature": 0.0, "avg_logprob": -0.13152502021011042, "compression_ratio": 1.593625498007968, "no_speech_prob": 2.796412809402682e-05}, {"id": 1142, "seek": 446856, "start": 4468.56, "end": 4473.68, "text": " The same data set we were looking at earlier, and the red part of this plot indicates the", "tokens": [440, 912, 1412, 992, 321, 645, 1237, 412, 3071, 11, 293, 264, 2182, 644, 295, 341, 7542, 16203, 264], "temperature": 0.0, "avg_logprob": -0.12713462166164233, "compression_ratio": 1.757679180887372, "no_speech_prob": 3.590805863495916e-05}, {"id": 1143, "seek": 446856, "start": 4473.68, "end": 4478.160000000001, "text": " number of times where the model sticks with its old answer even after changing the memory.", "tokens": [1230, 295, 1413, 689, 264, 2316, 12518, 365, 1080, 1331, 1867, 754, 934, 4473, 264, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12713462166164233, "compression_ratio": 1.757679180887372, "no_speech_prob": 3.590805863495916e-05}, {"id": 1144, "seek": 446856, "start": 4478.160000000001, "end": 4480.120000000001, "text": " It just stubbornly refuses to change.", "tokens": [467, 445, 24137, 356, 33222, 281, 1319, 13], "temperature": 0.0, "avg_logprob": -0.12713462166164233, "compression_ratio": 1.757679180887372, "no_speech_prob": 3.590805863495916e-05}, {"id": 1145, "seek": 446856, "start": 4480.120000000001, "end": 4484.4400000000005, "text": " The blue part is the good part where it actually switches over to predicting the United Nations", "tokens": [440, 3344, 644, 307, 264, 665, 644, 689, 309, 767, 19458, 670, 281, 32884, 264, 2824, 16459], "temperature": 0.0, "avg_logprob": -0.12713462166164233, "compression_ratio": 1.757679180887372, "no_speech_prob": 3.590805863495916e-05}, {"id": 1146, "seek": 446856, "start": 4484.4400000000005, "end": 4485.92, "text": " on various examples.", "tokens": [322, 3683, 5110, 13], "temperature": 0.0, "avg_logprob": -0.12713462166164233, "compression_ratio": 1.757679180887372, "no_speech_prob": 3.590805863495916e-05}, {"id": 1147, "seek": 446856, "start": 4485.92, "end": 4488.6, "text": " And this other orange part is very concerning too.", "tokens": [400, 341, 661, 7671, 644, 307, 588, 18087, 886, 13], "temperature": 0.0, "avg_logprob": -0.12713462166164233, "compression_ratio": 1.757679180887372, "no_speech_prob": 3.590805863495916e-05}, {"id": 1148, "seek": 446856, "start": 4488.6, "end": 4493.400000000001, "text": " So when you change the memory to United Nations, sometimes the model just gets confused and", "tokens": [407, 562, 291, 1319, 264, 4675, 281, 2824, 16459, 11, 2171, 264, 2316, 445, 2170, 9019, 293], "temperature": 0.0, "avg_logprob": -0.12713462166164233, "compression_ratio": 1.757679180887372, "no_speech_prob": 3.590805863495916e-05}, {"id": 1149, "seek": 446856, "start": 4493.400000000001, "end": 4495.360000000001, "text": " predict something totally different.", "tokens": [6069, 746, 3879, 819, 13], "temperature": 0.0, "avg_logprob": -0.12713462166164233, "compression_ratio": 1.757679180887372, "no_speech_prob": 3.590805863495916e-05}, {"id": 1150, "seek": 449536, "start": 4495.36, "end": 4498.799999999999, "text": " Not Saint Peter, not United Nations, just something completely different.", "tokens": [1726, 12902, 6508, 11, 406, 2824, 16459, 11, 445, 746, 2584, 819, 13], "temperature": 0.0, "avg_logprob": -0.12831313091775645, "compression_ratio": 1.7204301075268817, "no_speech_prob": 2.7105288609163836e-05}, {"id": 1151, "seek": 449536, "start": 4498.799999999999, "end": 4503.679999999999, "text": " So from this we can see something is really kind of broken about some of these memory-augmented", "tokens": [407, 490, 341, 321, 393, 536, 746, 307, 534, 733, 295, 5463, 466, 512, 295, 613, 4675, 12, 20056, 14684], "temperature": 0.0, "avg_logprob": -0.12831313091775645, "compression_ratio": 1.7204301075268817, "no_speech_prob": 2.7105288609163836e-05}, {"id": 1152, "seek": 449536, "start": 4503.679999999999, "end": 4504.839999999999, "text": " models.", "tokens": [5245, 13], "temperature": 0.0, "avg_logprob": -0.12831313091775645, "compression_ratio": 1.7204301075268817, "no_speech_prob": 2.7105288609163836e-05}, {"id": 1153, "seek": 449536, "start": 4504.839999999999, "end": 4510.599999999999, "text": " And the same kind of behavior happens on the dev set as well.", "tokens": [400, 264, 912, 733, 295, 5223, 2314, 322, 264, 1905, 992, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12831313091775645, "compression_ratio": 1.7204301075268817, "no_speech_prob": 2.7105288609163836e-05}, {"id": 1154, "seek": 449536, "start": 4510.599999999999, "end": 4516.36, "text": " And just to underscore how bad this is, these results are from the set of examples that", "tokens": [400, 445, 281, 37556, 577, 1578, 341, 307, 11, 613, 3542, 366, 490, 264, 992, 295, 5110, 300], "temperature": 0.0, "avg_logprob": -0.12831313091775645, "compression_ratio": 1.7204301075268817, "no_speech_prob": 2.7105288609163836e-05}, {"id": 1155, "seek": 449536, "start": 4516.36, "end": 4519.08, "text": " the original model was actually getting all correct.", "tokens": [264, 3380, 2316, 390, 767, 1242, 439, 3006, 13], "temperature": 0.0, "avg_logprob": -0.12831313091775645, "compression_ratio": 1.7204301075268817, "no_speech_prob": 2.7105288609163836e-05}, {"id": 1156, "seek": 449536, "start": 4519.08, "end": 4524.16, "text": " So you just kind of cut the performance of your model by more than half when you edit", "tokens": [407, 291, 445, 733, 295, 1723, 264, 3389, 295, 428, 2316, 538, 544, 813, 1922, 562, 291, 8129], "temperature": 0.0, "avg_logprob": -0.12831313091775645, "compression_ratio": 1.7204301075268817, "no_speech_prob": 2.7105288609163836e-05}, {"id": 1157, "seek": 449536, "start": 4524.16, "end": 4525.16, "text": " the memories.", "tokens": [264, 8495, 13], "temperature": 0.0, "avg_logprob": -0.12831313091775645, "compression_ratio": 1.7204301075268817, "no_speech_prob": 2.7105288609163836e-05}, {"id": 1158, "seek": 452516, "start": 4525.16, "end": 4527.72, "text": " And that indicates the model is not robust to change.", "tokens": [400, 300, 16203, 264, 2316, 307, 406, 13956, 281, 1319, 13], "temperature": 0.0, "avg_logprob": -0.12328056667162024, "compression_ratio": 1.7868217054263567, "no_speech_prob": 3.2190426281886175e-05}, {"id": 1159, "seek": 452516, "start": 4527.72, "end": 4530.68, "text": " As we said earlier, being able to edit your memories is something you would really want", "tokens": [1018, 321, 848, 3071, 11, 885, 1075, 281, 8129, 428, 8495, 307, 746, 291, 576, 534, 528], "temperature": 0.0, "avg_logprob": -0.12328056667162024, "compression_ratio": 1.7868217054263567, "no_speech_prob": 3.2190426281886175e-05}, {"id": 1160, "seek": 452516, "start": 4530.68, "end": 4534.04, "text": " from a memory-augmented model.", "tokens": [490, 257, 4675, 12, 20056, 14684, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12328056667162024, "compression_ratio": 1.7868217054263567, "no_speech_prob": 3.2190426281886175e-05}, {"id": 1161, "seek": 452516, "start": 4534.04, "end": 4537.72, "text": " So let's have an analysis of why this happens.", "tokens": [407, 718, 311, 362, 364, 5215, 295, 983, 341, 2314, 13], "temperature": 0.0, "avg_logprob": -0.12328056667162024, "compression_ratio": 1.7868217054263567, "no_speech_prob": 3.2190426281886175e-05}, {"id": 1162, "seek": 452516, "start": 4537.72, "end": 4543.44, "text": " Basically, when you put this memory into the sequence encoder, the reader that reads the", "tokens": [8537, 11, 562, 291, 829, 341, 4675, 666, 264, 8310, 2058, 19866, 11, 264, 15149, 300, 15700, 264], "temperature": 0.0, "avg_logprob": -0.12328056667162024, "compression_ratio": 1.7868217054263567, "no_speech_prob": 3.2190426281886175e-05}, {"id": 1163, "seek": 452516, "start": 4543.44, "end": 4549.32, "text": " memory, this encoder and this decoder, they actually have their own memory as well.", "tokens": [4675, 11, 341, 2058, 19866, 293, 341, 979, 19866, 11, 436, 767, 362, 641, 1065, 4675, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12328056667162024, "compression_ratio": 1.7868217054263567, "no_speech_prob": 3.2190426281886175e-05}, {"id": 1164, "seek": 452516, "start": 4549.32, "end": 4552.84, "text": " As we saw in the earlier slides, transformers have their own memory.", "tokens": [1018, 321, 1866, 294, 264, 3071, 9788, 11, 4088, 433, 362, 641, 1065, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12328056667162024, "compression_ratio": 1.7868217054263567, "no_speech_prob": 3.2190426281886175e-05}, {"id": 1165, "seek": 455284, "start": 4552.84, "end": 4557.64, "text": " So we'll refer to that as the parametric memory of the encoder decoder, as opposed to the", "tokens": [407, 321, 603, 2864, 281, 300, 382, 264, 6220, 17475, 4675, 295, 264, 2058, 19866, 979, 19866, 11, 382, 8851, 281, 264], "temperature": 0.0, "avg_logprob": -0.11422759056091308, "compression_ratio": 1.8472222222222223, "no_speech_prob": 1.0782673598441761e-05}, {"id": 1166, "seek": 455284, "start": 4557.64, "end": 4560.96, "text": " external memory that we want it to rely on.", "tokens": [8320, 4675, 300, 321, 528, 309, 281, 10687, 322, 13], "temperature": 0.0, "avg_logprob": -0.11422759056091308, "compression_ratio": 1.8472222222222223, "no_speech_prob": 1.0782673598441761e-05}, {"id": 1167, "seek": 455284, "start": 4560.96, "end": 4565.04, "text": " And at training time, they essentially learn to store the answer in their parametric", "tokens": [400, 412, 3097, 565, 11, 436, 4476, 1466, 281, 3531, 264, 1867, 294, 641, 6220, 17475], "temperature": 0.0, "avg_logprob": -0.11422759056091308, "compression_ratio": 1.8472222222222223, "no_speech_prob": 1.0782673598441761e-05}, {"id": 1168, "seek": 455284, "start": 4565.04, "end": 4568.92, "text": " memory and not rely on the external memory.", "tokens": [4675, 293, 406, 10687, 322, 264, 8320, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11422759056091308, "compression_ratio": 1.8472222222222223, "no_speech_prob": 1.0782673598441761e-05}, {"id": 1169, "seek": 455284, "start": 4568.92, "end": 4575.6, "text": " So to give this issue a better cartoon form, the input is coming in and the model has", "tokens": [407, 281, 976, 341, 2734, 257, 1101, 18569, 1254, 11, 264, 4846, 307, 1348, 294, 293, 264, 2316, 575], "temperature": 0.0, "avg_logprob": -0.11422759056091308, "compression_ratio": 1.8472222222222223, "no_speech_prob": 1.0782673598441761e-05}, {"id": 1170, "seek": 455284, "start": 4575.6, "end": 4578.6, "text": " its own parametric memory and the retrieve memory.", "tokens": [1080, 1065, 6220, 17475, 4675, 293, 264, 30254, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11422759056091308, "compression_ratio": 1.8472222222222223, "no_speech_prob": 1.0782673598441761e-05}, {"id": 1171, "seek": 457860, "start": 4578.6, "end": 4583.200000000001, "text": " And the parametric memory is saying St. Peter and the retrieve memory at training time is", "tokens": [400, 264, 6220, 17475, 4675, 307, 1566, 745, 13, 6508, 293, 264, 30254, 4675, 412, 3097, 565, 307], "temperature": 0.0, "avg_logprob": -0.11961515921133536, "compression_ratio": 1.9187279151943464, "no_speech_prob": 4.0691749745747074e-05}, {"id": 1172, "seek": 457860, "start": 4583.200000000001, "end": 4588.76, "text": " also saying St. Peter and the loss function is saying you must predict St. Peter.", "tokens": [611, 1566, 745, 13, 6508, 293, 264, 4470, 2445, 307, 1566, 291, 1633, 6069, 745, 13, 6508, 13], "temperature": 0.0, "avg_logprob": -0.11961515921133536, "compression_ratio": 1.9187279151943464, "no_speech_prob": 4.0691749745747074e-05}, {"id": 1173, "seek": 457860, "start": 4588.76, "end": 4592.6, "text": " So the model says, okay, I've got two sources of information I can choose either one.", "tokens": [407, 264, 2316, 1619, 11, 1392, 11, 286, 600, 658, 732, 7139, 295, 1589, 286, 393, 2826, 2139, 472, 13], "temperature": 0.0, "avg_logprob": -0.11961515921133536, "compression_ratio": 1.9187279151943464, "no_speech_prob": 4.0691749745747074e-05}, {"id": 1174, "seek": 457860, "start": 4592.6, "end": 4596.56, "text": " There's nothing forcing the model to use the retrieved memory.", "tokens": [821, 311, 1825, 19030, 264, 2316, 281, 764, 264, 19817, 937, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11961515921133536, "compression_ratio": 1.9187279151943464, "no_speech_prob": 4.0691749745747074e-05}, {"id": 1175, "seek": 457860, "start": 4596.56, "end": 4599.4800000000005, "text": " And that's part of the problem that's causing this.", "tokens": [400, 300, 311, 644, 295, 264, 1154, 300, 311, 9853, 341, 13], "temperature": 0.0, "avg_logprob": -0.11961515921133536, "compression_ratio": 1.9187279151943464, "no_speech_prob": 4.0691749745747074e-05}, {"id": 1176, "seek": 457860, "start": 4599.4800000000005, "end": 4603.4800000000005, "text": " Another problem, which isn't on this slide, is that sometimes the retriever is just not", "tokens": [3996, 1154, 11, 597, 1943, 380, 322, 341, 4137, 11, 307, 300, 2171, 264, 19817, 331, 307, 445, 406], "temperature": 0.0, "avg_logprob": -0.11961515921133536, "compression_ratio": 1.9187279151943464, "no_speech_prob": 4.0691749745747074e-05}, {"id": 1177, "seek": 457860, "start": 4603.4800000000005, "end": 4604.4800000000005, "text": " very good.", "tokens": [588, 665, 13], "temperature": 0.0, "avg_logprob": -0.11961515921133536, "compression_ratio": 1.9187279151943464, "no_speech_prob": 4.0691749745747074e-05}, {"id": 1178, "seek": 457860, "start": 4604.4800000000005, "end": 4607.88, "text": " So it might retrieve something that's just not related to the question.", "tokens": [407, 309, 1062, 30254, 746, 300, 311, 445, 406, 4077, 281, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.11961515921133536, "compression_ratio": 1.9187279151943464, "no_speech_prob": 4.0691749745747074e-05}, {"id": 1179, "seek": 460788, "start": 4607.88, "end": 4611.96, "text": " And in that case, the model is forced to fall back on its parametric memory and again", "tokens": [400, 294, 300, 1389, 11, 264, 2316, 307, 7579, 281, 2100, 646, 322, 1080, 6220, 17475, 4675, 293, 797], "temperature": 0.0, "avg_logprob": -0.11403783162434895, "compression_ratio": 2.0084745762711864, "no_speech_prob": 5.173718818696216e-06}, {"id": 1180, "seek": 460788, "start": 4611.96, "end": 4615.28, "text": " learn to distrust the retrieved memory.", "tokens": [1466, 281, 1483, 22326, 264, 19817, 937, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11403783162434895, "compression_ratio": 2.0084745762711864, "no_speech_prob": 5.173718818696216e-06}, {"id": 1181, "seek": 460788, "start": 4615.28, "end": 4623.04, "text": " So we want a way to kind of force the model to pick the retrieved memory instead.", "tokens": [407, 321, 528, 257, 636, 281, 733, 295, 3464, 264, 2316, 281, 1888, 264, 19817, 937, 4675, 2602, 13], "temperature": 0.0, "avg_logprob": -0.11403783162434895, "compression_ratio": 2.0084745762711864, "no_speech_prob": 5.173718818696216e-06}, {"id": 1182, "seek": 460788, "start": 4623.04, "end": 4626.96, "text": " Ideally, we would want cases where the parametric memory is wrong and the retrieved memory", "tokens": [40817, 11, 321, 576, 528, 3331, 689, 264, 6220, 17475, 4675, 307, 2085, 293, 264, 19817, 937, 4675], "temperature": 0.0, "avg_logprob": -0.11403783162434895, "compression_ratio": 2.0084745762711864, "no_speech_prob": 5.173718818696216e-06}, {"id": 1183, "seek": 460788, "start": 4626.96, "end": 4627.96, "text": " is correct.", "tokens": [307, 3006, 13], "temperature": 0.0, "avg_logprob": -0.11403783162434895, "compression_ratio": 2.0084745762711864, "no_speech_prob": 5.173718818696216e-06}, {"id": 1184, "seek": 460788, "start": 4627.96, "end": 4631.4800000000005, "text": " That would force the model to say, hey, I can't trust my parametric memory.", "tokens": [663, 576, 3464, 264, 2316, 281, 584, 11, 4177, 11, 286, 393, 380, 3361, 452, 6220, 17475, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11403783162434895, "compression_ratio": 2.0084745762711864, "no_speech_prob": 5.173718818696216e-06}, {"id": 1185, "seek": 460788, "start": 4631.4800000000005, "end": 4637.64, "text": " So what Longprey at all do is first they take the retrieved memory and they change what", "tokens": [407, 437, 8282, 3712, 88, 412, 439, 360, 307, 700, 436, 747, 264, 19817, 937, 4675, 293, 436, 1319, 437], "temperature": 0.0, "avg_logprob": -0.11403783162434895, "compression_ratio": 2.0084745762711864, "no_speech_prob": 5.173718818696216e-06}, {"id": 1186, "seek": 463764, "start": 4637.64, "end": 4639.160000000001, "text": " the retrieved memory is saying.", "tokens": [264, 19817, 937, 4675, 307, 1566, 13], "temperature": 0.0, "avg_logprob": -0.11778577168782552, "compression_ratio": 1.8700361010830324, "no_speech_prob": 2.0144385416642763e-05}, {"id": 1187, "seek": 463764, "start": 4639.160000000001, "end": 4643.92, "text": " So this creates a disagreement now between the parametric memory and the retrieved memory.", "tokens": [407, 341, 7829, 257, 38947, 586, 1296, 264, 6220, 17475, 4675, 293, 264, 19817, 937, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11778577168782552, "compression_ratio": 1.8700361010830324, "no_speech_prob": 2.0144385416642763e-05}, {"id": 1188, "seek": 463764, "start": 4643.92, "end": 4646.400000000001, "text": " But the gold label is still saying St. Peter.", "tokens": [583, 264, 3821, 7645, 307, 920, 1566, 745, 13, 6508, 13], "temperature": 0.0, "avg_logprob": -0.11778577168782552, "compression_ratio": 1.8700361010830324, "no_speech_prob": 2.0144385416642763e-05}, {"id": 1189, "seek": 463764, "start": 4646.400000000001, "end": 4651.8, "text": " So we've just made the matters worse now because now the retrieved memory is even less trustworthy.", "tokens": [407, 321, 600, 445, 1027, 264, 7001, 5324, 586, 570, 586, 264, 19817, 937, 4675, 307, 754, 1570, 39714, 13], "temperature": 0.0, "avg_logprob": -0.11778577168782552, "compression_ratio": 1.8700361010830324, "no_speech_prob": 2.0144385416642763e-05}, {"id": 1190, "seek": 463764, "start": 4651.8, "end": 4657.320000000001, "text": " The final thing they do, which is really the interesting bit, is they just decide to change", "tokens": [440, 2572, 551, 436, 360, 11, 597, 307, 534, 264, 1880, 857, 11, 307, 436, 445, 4536, 281, 1319], "temperature": 0.0, "avg_logprob": -0.11778577168782552, "compression_ratio": 1.8700361010830324, "no_speech_prob": 2.0144385416642763e-05}, {"id": 1191, "seek": 463764, "start": 4657.320000000001, "end": 4661.200000000001, "text": " the gold label as well to agree with their retrieved memory.", "tokens": [264, 3821, 7645, 382, 731, 281, 3986, 365, 641, 19817, 937, 4675, 13], "temperature": 0.0, "avg_logprob": -0.11778577168782552, "compression_ratio": 1.8700361010830324, "no_speech_prob": 2.0144385416642763e-05}, {"id": 1192, "seek": 463764, "start": 4661.200000000001, "end": 4666.52, "text": " So they've changed reality and said, no, actually the United Nations guards the gates of heaven.", "tokens": [407, 436, 600, 3105, 4103, 293, 848, 11, 572, 11, 767, 264, 2824, 16459, 17652, 264, 19792, 295, 7162, 13], "temperature": 0.0, "avg_logprob": -0.11778577168782552, "compression_ratio": 1.8700361010830324, "no_speech_prob": 2.0144385416642763e-05}, {"id": 1193, "seek": 466652, "start": 4666.52, "end": 4669.64, "text": " And what's in your parametric memory is wrong?", "tokens": [400, 437, 311, 294, 428, 6220, 17475, 4675, 307, 2085, 30], "temperature": 0.0, "avg_logprob": -0.12914350891113283, "compression_ratio": 1.7525773195876289, "no_speech_prob": 4.4685963075608015e-05}, {"id": 1194, "seek": 466652, "start": 4669.64, "end": 4670.8, "text": " And that's basically the approach.", "tokens": [400, 300, 311, 1936, 264, 3109, 13], "temperature": 0.0, "avg_logprob": -0.12914350891113283, "compression_ratio": 1.7525773195876289, "no_speech_prob": 4.4685963075608015e-05}, {"id": 1195, "seek": 466652, "start": 4670.8, "end": 4675.160000000001, "text": " So they create a bunch of data like this where the gold answer has been changed to match", "tokens": [407, 436, 1884, 257, 3840, 295, 1412, 411, 341, 689, 264, 3821, 1867, 575, 668, 3105, 281, 2995], "temperature": 0.0, "avg_logprob": -0.12914350891113283, "compression_ratio": 1.7525773195876289, "no_speech_prob": 4.4685963075608015e-05}, {"id": 1196, "seek": 466652, "start": 4675.160000000001, "end": 4679.0, "text": " the corruption they made in the retrieved memory and it guides the model away from using", "tokens": [264, 17959, 436, 1027, 294, 264, 19817, 937, 4675, 293, 309, 17007, 264, 2316, 1314, 490, 1228], "temperature": 0.0, "avg_logprob": -0.12914350891113283, "compression_ratio": 1.7525773195876289, "no_speech_prob": 4.4685963075608015e-05}, {"id": 1197, "seek": 466652, "start": 4679.0, "end": 4680.68, "text": " the parametric memory.", "tokens": [264, 6220, 17475, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12914350891113283, "compression_ratio": 1.7525773195876289, "no_speech_prob": 4.4685963075608015e-05}, {"id": 1198, "seek": 466652, "start": 4680.68, "end": 4687.040000000001, "text": " I thought that was a pretty cool trick and they don't give this name in the paper, but", "tokens": [286, 1194, 300, 390, 257, 1238, 1627, 4282, 293, 436, 500, 380, 976, 341, 1315, 294, 264, 3035, 11, 457], "temperature": 0.0, "avg_logprob": -0.12914350891113283, "compression_ratio": 1.7525773195876289, "no_speech_prob": 4.4685963075608015e-05}, {"id": 1199, "seek": 466652, "start": 4687.040000000001, "end": 4690.4400000000005, "text": " you can think of it as data augmentation using these counterfactual memories.", "tokens": [291, 393, 519, 295, 309, 382, 1412, 14501, 19631, 1228, 613, 5682, 44919, 901, 8495, 13], "temperature": 0.0, "avg_logprob": -0.12914350891113283, "compression_ratio": 1.7525773195876289, "no_speech_prob": 4.4685963075608015e-05}, {"id": 1200, "seek": 466652, "start": 4690.4400000000005, "end": 4693.4400000000005, "text": " And it can really be applied to a lot of different approaches.", "tokens": [400, 309, 393, 534, 312, 6456, 281, 257, 688, 295, 819, 11587, 13], "temperature": 0.0, "avg_logprob": -0.12914350891113283, "compression_ratio": 1.7525773195876289, "no_speech_prob": 4.4685963075608015e-05}, {"id": 1201, "seek": 469344, "start": 4693.44, "end": 4697.5199999999995, "text": " As long as your memory is editable in a certain way and you can edit the gold label as well,", "tokens": [1018, 938, 382, 428, 4675, 307, 8129, 712, 294, 257, 1629, 636, 293, 291, 393, 8129, 264, 3821, 7645, 382, 731, 11], "temperature": 0.0, "avg_logprob": -0.1624759225284352, "compression_ratio": 1.794871794871795, "no_speech_prob": 3.269211811129935e-05}, {"id": 1202, "seek": 469344, "start": 4697.5199999999995, "end": 4703.879999999999, "text": " you can create this artificial correlation between the memory and the output and an artificial", "tokens": [291, 393, 1884, 341, 11677, 20009, 1296, 264, 4675, 293, 264, 5598, 293, 364, 11677], "temperature": 0.0, "avg_logprob": -0.1624759225284352, "compression_ratio": 1.794871794871795, "no_speech_prob": 3.269211811129935e-05}, {"id": 1203, "seek": 469344, "start": 4703.879999999999, "end": 4707.759999999999, "text": " anti-correlation between the output and whatever your model originally trained on.", "tokens": [6061, 12, 19558, 4419, 399, 1296, 264, 5598, 293, 2035, 428, 2316, 7993, 8895, 322, 13], "temperature": 0.0, "avg_logprob": -0.1624759225284352, "compression_ratio": 1.794871794871795, "no_speech_prob": 3.269211811129935e-05}, {"id": 1204, "seek": 469344, "start": 4707.759999999999, "end": 4710.48, "text": " It's cool.", "tokens": [467, 311, 1627, 13], "temperature": 0.0, "avg_logprob": -0.1624759225284352, "compression_ratio": 1.794871794871795, "no_speech_prob": 3.269211811129935e-05}, {"id": 1205, "seek": 469344, "start": 4710.48, "end": 4712.839999999999, "text": " So now you want to see if it works.", "tokens": [407, 586, 291, 528, 281, 536, 498, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1624759225284352, "compression_ratio": 1.794871794871795, "no_speech_prob": 3.269211811129935e-05}, {"id": 1206, "seek": 469344, "start": 4712.839999999999, "end": 4716.28, "text": " And in the paper they report this metric, which is basically the percentage of time the", "tokens": [400, 294, 264, 3035, 436, 2275, 341, 20678, 11, 597, 307, 1936, 264, 9668, 295, 565, 264], "temperature": 0.0, "avg_logprob": -0.1624759225284352, "compression_ratio": 1.794871794871795, "no_speech_prob": 3.269211811129935e-05}, {"id": 1207, "seek": 469344, "start": 4716.28, "end": 4723.16, "text": " model predicts the old value instead of the new one divided by the old plus the new.", "tokens": [2316, 6069, 82, 264, 1331, 2158, 2602, 295, 264, 777, 472, 6666, 538, 264, 1331, 1804, 264, 777, 13], "temperature": 0.0, "avg_logprob": -0.1624759225284352, "compression_ratio": 1.794871794871795, "no_speech_prob": 3.269211811129935e-05}, {"id": 1208, "seek": 472316, "start": 4723.16, "end": 4726.639999999999, "text": " They ignore the set where the model gets confused and produces something totally different.", "tokens": [814, 11200, 264, 992, 689, 264, 2316, 2170, 9019, 293, 14725, 746, 3879, 819, 13], "temperature": 0.0, "avg_logprob": -0.1035872899568998, "compression_ratio": 1.7531645569620253, "no_speech_prob": 7.721014117123559e-05}, {"id": 1209, "seek": 472316, "start": 4726.639999999999, "end": 4731.4, "text": " I wish they had reported that too, but I couldn't immediately find it in their paper.", "tokens": [286, 3172, 436, 632, 7055, 300, 886, 11, 457, 286, 2809, 380, 4258, 915, 309, 294, 641, 3035, 13], "temperature": 0.0, "avg_logprob": -0.1035872899568998, "compression_ratio": 1.7531645569620253, "no_speech_prob": 7.721014117123559e-05}, {"id": 1210, "seek": 472316, "start": 4731.4, "end": 4733.5599999999995, "text": " But at least on this metric things look great.", "tokens": [583, 412, 1935, 322, 341, 20678, 721, 574, 869, 13], "temperature": 0.0, "avg_logprob": -0.1035872899568998, "compression_ratio": 1.7531645569620253, "no_speech_prob": 7.721014117123559e-05}, {"id": 1211, "seek": 472316, "start": 4733.5599999999995, "end": 4737.72, "text": " So on the training set and the dev set, the percentage of the time that the model uses", "tokens": [407, 322, 264, 3097, 992, 293, 264, 1905, 992, 11, 264, 9668, 295, 264, 565, 300, 264, 2316, 4960], "temperature": 0.0, "avg_logprob": -0.1035872899568998, "compression_ratio": 1.7531645569620253, "no_speech_prob": 7.721014117123559e-05}, {"id": 1212, "seek": 472316, "start": 4737.72, "end": 4742.8, "text": " the old memory, the old answer, goes dramatically down with this data augmentation.", "tokens": [264, 1331, 4675, 11, 264, 1331, 1867, 11, 1709, 17548, 760, 365, 341, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.1035872899568998, "compression_ratio": 1.7531645569620253, "no_speech_prob": 7.721014117123559e-05}, {"id": 1213, "seek": 472316, "start": 4742.8, "end": 4746.44, "text": " So it really keeps the model on its toes and makes it use its memory.", "tokens": [407, 309, 534, 5965, 264, 2316, 322, 1080, 14681, 293, 1669, 309, 764, 1080, 4675, 13], "temperature": 0.0, "avg_logprob": -0.1035872899568998, "compression_ratio": 1.7531645569620253, "no_speech_prob": 7.721014117123559e-05}, {"id": 1214, "seek": 472316, "start": 4746.44, "end": 4751.84, "text": " Now I take this result with a little grain of salt because their test set is created the", "tokens": [823, 286, 747, 341, 1874, 365, 257, 707, 12837, 295, 5139, 570, 641, 1500, 992, 307, 2942, 264], "temperature": 0.0, "avg_logprob": -0.1035872899568998, "compression_ratio": 1.7531645569620253, "no_speech_prob": 7.721014117123559e-05}, {"id": 1215, "seek": 475184, "start": 4751.84, "end": 4754.92, "text": " same way that they produce this data augmentation.", "tokens": [912, 636, 300, 436, 5258, 341, 1412, 14501, 19631, 13], "temperature": 0.0, "avg_logprob": -0.1729047118114824, "compression_ratio": 1.6272401433691757, "no_speech_prob": 7.601758261444047e-05}, {"id": 1216, "seek": 475184, "start": 4754.92, "end": 4759.56, "text": " So this is kind of the ideal setup where they're almost testing on the exact same distribution", "tokens": [407, 341, 307, 733, 295, 264, 7157, 8657, 689, 436, 434, 1920, 4997, 322, 264, 1900, 912, 7316], "temperature": 0.0, "avg_logprob": -0.1729047118114824, "compression_ratio": 1.6272401433691757, "no_speech_prob": 7.601758261444047e-05}, {"id": 1217, "seek": 475184, "start": 4759.56, "end": 4761.88, "text": " that they're training on.", "tokens": [300, 436, 434, 3097, 322, 13], "temperature": 0.0, "avg_logprob": -0.1729047118114824, "compression_ratio": 1.6272401433691757, "no_speech_prob": 7.601758261444047e-05}, {"id": 1218, "seek": 475184, "start": 4761.88, "end": 4764.32, "text": " But still a very interesting approach.", "tokens": [583, 920, 257, 588, 1880, 3109, 13], "temperature": 0.0, "avg_logprob": -0.1729047118114824, "compression_ratio": 1.6272401433691757, "no_speech_prob": 7.601758261444047e-05}, {"id": 1219, "seek": 475184, "start": 4764.32, "end": 4766.8, "text": " So let's see how long do we have left?", "tokens": [407, 718, 311, 536, 577, 938, 360, 321, 362, 1411, 30], "temperature": 0.0, "avg_logprob": -0.1729047118114824, "compression_ratio": 1.6272401433691757, "no_speech_prob": 7.601758261444047e-05}, {"id": 1220, "seek": 475184, "start": 4766.8, "end": 4770.64, "text": " Okay, in the last few minutes I'm going to cover the over reliance problem and there's", "tokens": [1033, 11, 294, 264, 1036, 1326, 2077, 286, 478, 516, 281, 2060, 264, 670, 1039, 6276, 1154, 293, 456, 311], "temperature": 0.0, "avg_logprob": -0.1729047118114824, "compression_ratio": 1.6272401433691757, "no_speech_prob": 7.601758261444047e-05}, {"id": 1221, "seek": 475184, "start": 4770.64, "end": 4772.4800000000005, "text": " just one slide on this.", "tokens": [445, 472, 4137, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.1729047118114824, "compression_ratio": 1.6272401433691757, "no_speech_prob": 7.601758261444047e-05}, {"id": 1222, "seek": 475184, "start": 4772.4800000000005, "end": 4776.68, "text": " So sometimes the memories that your model retrieves are too easy.", "tokens": [407, 2171, 264, 8495, 300, 428, 2316, 19817, 977, 366, 886, 1858, 13], "temperature": 0.0, "avg_logprob": -0.1729047118114824, "compression_ratio": 1.6272401433691757, "no_speech_prob": 7.601758261444047e-05}, {"id": 1223, "seek": 475184, "start": 4776.68, "end": 4778.08, "text": " Here's what I mean by that.", "tokens": [1692, 311, 437, 286, 914, 538, 300, 13], "temperature": 0.0, "avg_logprob": -0.1729047118114824, "compression_ratio": 1.6272401433691757, "no_speech_prob": 7.601758261444047e-05}, {"id": 1224, "seek": 477808, "start": 4778.08, "end": 4784.0, "text": " So if we go back to this Eiffel Tower query, what year was the Eiffel Tower built?", "tokens": [407, 498, 321, 352, 646, 281, 341, 462, 3661, 338, 17877, 14581, 11, 437, 1064, 390, 264, 462, 3661, 338, 17877, 3094, 30], "temperature": 0.0, "avg_logprob": -0.12058895469730736, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.920381227042526e-05}, {"id": 1225, "seek": 477808, "start": 4784.0, "end": 4788.76, "text": " We know it's 1889 and a good typical memory that you might retrieve is something like this.", "tokens": [492, 458, 309, 311, 2443, 21115, 293, 257, 665, 7476, 4675, 300, 291, 1062, 30254, 307, 746, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.12058895469730736, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.920381227042526e-05}, {"id": 1226, "seek": 477808, "start": 4788.76, "end": 4793.36, "text": " It says work on the Eiffel Tower was completed in 1889.", "tokens": [467, 1619, 589, 322, 264, 462, 3661, 338, 17877, 390, 7365, 294, 2443, 21115, 13], "temperature": 0.0, "avg_logprob": -0.12058895469730736, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.920381227042526e-05}, {"id": 1227, "seek": 477808, "start": 4793.36, "end": 4797.4, "text": " There's not too much word overlap with the original query, which is good because it", "tokens": [821, 311, 406, 886, 709, 1349, 19959, 365, 264, 3380, 14581, 11, 597, 307, 665, 570, 309], "temperature": 0.0, "avg_logprob": -0.12058895469730736, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.920381227042526e-05}, {"id": 1228, "seek": 477808, "start": 4797.4, "end": 4803.4, "text": " teaches the reader to recognize the fact that completed in this context is the same as", "tokens": [16876, 264, 15149, 281, 5521, 264, 1186, 300, 7365, 294, 341, 4319, 307, 264, 912, 382], "temperature": 0.0, "avg_logprob": -0.12058895469730736, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.920381227042526e-05}, {"id": 1229, "seek": 477808, "start": 4803.4, "end": 4804.4, "text": " built.", "tokens": [3094, 13], "temperature": 0.0, "avg_logprob": -0.12058895469730736, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.920381227042526e-05}, {"id": 1230, "seek": 477808, "start": 4804.4, "end": 4806.4, "text": " So the reader learns paraphrase.", "tokens": [407, 264, 15149, 27152, 36992, 1703, 651, 13], "temperature": 0.0, "avg_logprob": -0.12058895469730736, "compression_ratio": 1.6641509433962265, "no_speech_prob": 5.920381227042526e-05}, {"id": 1231, "seek": 480640, "start": 4806.4, "end": 4810.5599999999995, "text": " On the other hand, you might get a memory that's too easy, which literally just says exactly", "tokens": [1282, 264, 661, 1011, 11, 291, 1062, 483, 257, 4675, 300, 311, 886, 1858, 11, 597, 3736, 445, 1619, 2293], "temperature": 0.0, "avg_logprob": -0.150651977175758, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.3006562767259311e-05}, {"id": 1232, "seek": 480640, "start": 4810.5599999999995, "end": 4813.879999999999, "text": " the same tokens as the original input.", "tokens": [264, 912, 22667, 382, 264, 3380, 4846, 13], "temperature": 0.0, "avg_logprob": -0.150651977175758, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.3006562767259311e-05}, {"id": 1233, "seek": 480640, "start": 4813.879999999999, "end": 4820.879999999999, "text": " And you could also consider, yeah, so this example would not teach the model how to paraphrase.", "tokens": [400, 291, 727, 611, 1949, 11, 1338, 11, 370, 341, 1365, 576, 406, 2924, 264, 2316, 577, 281, 36992, 1703, 651, 13], "temperature": 0.0, "avg_logprob": -0.150651977175758, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.3006562767259311e-05}, {"id": 1234, "seek": 480640, "start": 4820.879999999999, "end": 4824.48, "text": " And at the other extreme, you could also consider an extremely challenging memory.", "tokens": [400, 412, 264, 661, 8084, 11, 291, 727, 611, 1949, 364, 4664, 7595, 4675, 13], "temperature": 0.0, "avg_logprob": -0.150651977175758, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.3006562767259311e-05}, {"id": 1235, "seek": 480640, "start": 4824.48, "end": 4829.04, "text": " Like Paris's tallest tower finished the same year Van Gogh painted the start night.", "tokens": [1743, 8380, 311, 42075, 10567, 4335, 264, 912, 1064, 8979, 39690, 71, 11797, 264, 722, 1818, 13], "temperature": 0.0, "avg_logprob": -0.150651977175758, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.3006562767259311e-05}, {"id": 1236, "seek": 480640, "start": 4829.04, "end": 4833.5599999999995, "text": " So yes, that also says the same fact, but the answer doesn't even directly appear.", "tokens": [407, 2086, 11, 300, 611, 1619, 264, 912, 1186, 11, 457, 264, 1867, 1177, 380, 754, 3838, 4204, 13], "temperature": 0.0, "avg_logprob": -0.150651977175758, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.3006562767259311e-05}, {"id": 1237, "seek": 480640, "start": 4833.5599999999995, "end": 4836.16, "text": " It's just too hard for the model.", "tokens": [467, 311, 445, 886, 1152, 337, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.150651977175758, "compression_ratio": 1.7380952380952381, "no_speech_prob": 1.3006562767259311e-05}, {"id": 1238, "seek": 483616, "start": 4836.16, "end": 4840.84, "text": " So if all of your examples are like this too easy memory, then you end up with a reader", "tokens": [407, 498, 439, 295, 428, 5110, 366, 411, 341, 886, 1858, 4675, 11, 550, 291, 917, 493, 365, 257, 15149], "temperature": 0.0, "avg_logprob": -0.10933746663174888, "compression_ratio": 1.6897810218978102, "no_speech_prob": 6.604313239222392e-05}, {"id": 1239, "seek": 483616, "start": 4840.84, "end": 4842.76, "text": " that is kind of spoiled.", "tokens": [300, 307, 733, 295, 32439, 13], "temperature": 0.0, "avg_logprob": -0.10933746663174888, "compression_ratio": 1.6897810218978102, "no_speech_prob": 6.604313239222392e-05}, {"id": 1240, "seek": 483616, "start": 4842.76, "end": 4845.5599999999995, "text": " It never learns to paraphrase.", "tokens": [467, 1128, 27152, 281, 36992, 1703, 651, 13], "temperature": 0.0, "avg_logprob": -0.10933746663174888, "compression_ratio": 1.6897810218978102, "no_speech_prob": 6.604313239222392e-05}, {"id": 1241, "seek": 483616, "start": 4845.5599999999995, "end": 4849.36, "text": " And at test time, if your memories that are retrieved are not as good, the reader is not", "tokens": [400, 412, 1500, 565, 11, 498, 428, 8495, 300, 366, 19817, 937, 366, 406, 382, 665, 11, 264, 15149, 307, 406], "temperature": 0.0, "avg_logprob": -0.10933746663174888, "compression_ratio": 1.6897810218978102, "no_speech_prob": 6.604313239222392e-05}, {"id": 1242, "seek": 483616, "start": 4849.36, "end": 4851.16, "text": " going to be able to use them.", "tokens": [516, 281, 312, 1075, 281, 764, 552, 13], "temperature": 0.0, "avg_logprob": -0.10933746663174888, "compression_ratio": 1.6897810218978102, "no_speech_prob": 6.604313239222392e-05}, {"id": 1243, "seek": 483616, "start": 4851.16, "end": 4856.599999999999, "text": " So a simple fix to this problem, I don't have a paper that I can cite exactly for this,", "tokens": [407, 257, 2199, 3191, 281, 341, 1154, 11, 286, 500, 380, 362, 257, 3035, 300, 286, 393, 37771, 2293, 337, 341, 11], "temperature": 0.0, "avg_logprob": -0.10933746663174888, "compression_ratio": 1.6897810218978102, "no_speech_prob": 6.604313239222392e-05}, {"id": 1244, "seek": 483616, "start": 4856.599999999999, "end": 4861.76, "text": " but at training time, you can simply filter out some of the memories that have lexical overlap.", "tokens": [457, 412, 3097, 565, 11, 291, 393, 2935, 6608, 484, 512, 295, 264, 8495, 300, 362, 476, 87, 804, 19959, 13], "temperature": 0.0, "avg_logprob": -0.10933746663174888, "compression_ratio": 1.6897810218978102, "no_speech_prob": 6.604313239222392e-05}, {"id": 1245, "seek": 483616, "start": 4861.76, "end": 4863.599999999999, "text": " That's too high.", "tokens": [663, 311, 886, 1090, 13], "temperature": 0.0, "avg_logprob": -0.10933746663174888, "compression_ratio": 1.6897810218978102, "no_speech_prob": 6.604313239222392e-05}, {"id": 1246, "seek": 486360, "start": 4863.6, "end": 4868.160000000001, "text": " At the same time, you also want to make sure that you don't filter out so many of the easy", "tokens": [1711, 264, 912, 565, 11, 291, 611, 528, 281, 652, 988, 300, 291, 500, 380, 6608, 484, 370, 867, 295, 264, 1858], "temperature": 0.0, "avg_logprob": -0.12361051034236299, "compression_ratio": 1.7161290322580645, "no_speech_prob": 1.4508965250570327e-05}, {"id": 1247, "seek": 486360, "start": 4868.160000000001, "end": 4872.280000000001, "text": " things that you're just left with the super hard cases, like the one on the bottom.", "tokens": [721, 300, 291, 434, 445, 1411, 365, 264, 1687, 1152, 3331, 11, 411, 264, 472, 322, 264, 2767, 13], "temperature": 0.0, "avg_logprob": -0.12361051034236299, "compression_ratio": 1.7161290322580645, "no_speech_prob": 1.4508965250570327e-05}, {"id": 1248, "seek": 486360, "start": 4872.280000000001, "end": 4875.88, "text": " Because if you only have the super hard cases, your model will get confused.", "tokens": [1436, 498, 291, 787, 362, 264, 1687, 1152, 3331, 11, 428, 2316, 486, 483, 9019, 13], "temperature": 0.0, "avg_logprob": -0.12361051034236299, "compression_ratio": 1.7161290322580645, "no_speech_prob": 1.4508965250570327e-05}, {"id": 1249, "seek": 486360, "start": 4875.88, "end": 4880.04, "text": " And as we saw in the previous slides, it might just fall back on its parametric memory.", "tokens": [400, 382, 321, 1866, 294, 264, 3894, 9788, 11, 309, 1062, 445, 2100, 646, 322, 1080, 6220, 17475, 4675, 13], "temperature": 0.0, "avg_logprob": -0.12361051034236299, "compression_ratio": 1.7161290322580645, "no_speech_prob": 1.4508965250570327e-05}, {"id": 1250, "seek": 486360, "start": 4880.04, "end": 4885.280000000001, "text": " So this is sort of just an area for open research of how to give the reader a flexible set", "tokens": [407, 341, 307, 1333, 295, 445, 364, 1859, 337, 1269, 2132, 295, 577, 281, 976, 264, 15149, 257, 11358, 992], "temperature": 0.0, "avg_logprob": -0.12361051034236299, "compression_ratio": 1.7161290322580645, "no_speech_prob": 1.4508965250570327e-05}, {"id": 1251, "seek": 486360, "start": 4885.280000000001, "end": 4887.68, "text": " of things to train from.", "tokens": [295, 721, 281, 3847, 490, 13], "temperature": 0.0, "avg_logprob": -0.12361051034236299, "compression_ratio": 1.7161290322580645, "no_speech_prob": 1.4508965250570327e-05}, {"id": 1252, "seek": 486360, "start": 4887.68, "end": 4889.4800000000005, "text": " Great, yeah.", "tokens": [3769, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.12361051034236299, "compression_ratio": 1.7161290322580645, "no_speech_prob": 1.4508965250570327e-05}, {"id": 1253, "seek": 486360, "start": 4889.4800000000005, "end": 4892.88, "text": " So I've covered pretty much everything in that section as well.", "tokens": [407, 286, 600, 5343, 1238, 709, 1203, 294, 300, 3541, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.12361051034236299, "compression_ratio": 1.7161290322580645, "no_speech_prob": 1.4508965250570327e-05}, {"id": 1254, "seek": 489288, "start": 4892.88, "end": 4896.16, "text": " The main takeaway is that, like for you guys to have is that getting your model to use", "tokens": [440, 2135, 30681, 307, 300, 11, 411, 337, 291, 1074, 281, 362, 307, 300, 1242, 428, 2316, 281, 764], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1255, "seek": 489288, "start": 4896.16, "end": 4898.16, "text": " memories is not hard.", "tokens": [8495, 307, 406, 1152, 13], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1256, "seek": 489288, "start": 4898.16, "end": 4899.8, "text": " There's some simple approaches.", "tokens": [821, 311, 512, 2199, 11587, 13], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1257, "seek": 489288, "start": 4899.8, "end": 4904.36, "text": " But getting your model to use memory correctly is actually an interesting open question.", "tokens": [583, 1242, 428, 2316, 281, 764, 4675, 8944, 307, 767, 364, 1880, 1269, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1258, "seek": 489288, "start": 4904.36, "end": 4910.36, "text": " And there's this issue of underutilization and overreliance that are open areas of research.", "tokens": [400, 456, 311, 341, 2734, 295, 833, 20835, 2144, 293, 670, 265, 2081, 719, 300, 366, 1269, 3179, 295, 2132, 13], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1259, "seek": 489288, "start": 4910.36, "end": 4911.36, "text": " And that's it.", "tokens": [400, 300, 311, 309, 13], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1260, "seek": 489288, "start": 4911.36, "end": 4914.96, "text": " I hope that you guys saw some interesting things about memory augmented models and are", "tokens": [286, 1454, 300, 291, 1074, 1866, 512, 1880, 721, 466, 4675, 36155, 5245, 293, 366], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1261, "seek": 489288, "start": 4914.96, "end": 4916.4800000000005, "text": " encouraged to look into that area.", "tokens": [14658, 281, 574, 666, 300, 1859, 13], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1262, "seek": 489288, "start": 4916.4800000000005, "end": 4920.88, "text": " If there are any questions, please feel free to email me or message me.", "tokens": [759, 456, 366, 604, 1651, 11, 1767, 841, 1737, 281, 3796, 385, 420, 3636, 385, 13], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1263, "seek": 489288, "start": 4920.88, "end": 4922.28, "text": " Have you to talk about it more.", "tokens": [3560, 291, 281, 751, 466, 309, 544, 13], "temperature": 0.0, "avg_logprob": -0.1864763026614841, "compression_ratio": 1.8459016393442622, "no_speech_prob": 3.0235369194997475e-05}, {"id": 1264, "seek": 492228, "start": 4922.28, "end": 4924.5199999999995, "text": " Thanks for sitting through a 90 minute lecture.", "tokens": [50364, 2561, 337, 3798, 807, 257, 4289, 3456, 7991, 13, 50476], "temperature": 0.0, "avg_logprob": -0.3567831516265869, "compression_ratio": 0.8545454545454545, "no_speech_prob": 0.00020304362988099456}], "language": "en"}