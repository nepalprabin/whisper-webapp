{"text": " In theoretical physics, to get this kind of audience, you have to win the Nobel Prize or something. But, of course, I've been working on ML recently, and it's been much more exciting. There's a huge amount of interest. So to some extent, part of what I'll be talking about may be an implicit theme will be sort of why there's so much excitement and why you might expect that excitement to continue. So an outline of my talk is that I'll first start by discussing motivations for language modeling. I'm sure you're all very well motivated because this is an NLP class. And I'll also talk about sort of orders of magnitude of data and compute that go into contemporary language modeling. And that will kind of set the stage for talking about scaling laws for neural language modeling. And further realization that these scaling laws seem to be quite universal for generative models and maybe for sort of machine learning more generally. And then, finally, after discussing that, I'll talk about what happens when we actually do scale up language models. I'll talk about the GBD3 model. And if there's time, I'll talk about some lessons from all of these ideas for research, which I imagine many of you are excited to be involved with soon. So I'll start by talking about why we do language modeling and Fermi estimates for language modeling. By Fermi estimates, I mean questions like estimating, is it a million or a hundred thousand or ten thousand PNO tuners in Chicago. Fermi famously asked this kind of question. And there are a lot of estimates like this that we can kind of do in a back of the envelope way to really get a sense for what's going on. But before going into that, why should you study? Study language. This is sort of my motivation. You might have all sorts of other motivations. Language is obviously very fascinating. Intellectual creation by our species. But I think another reason why it's particularly exciting for AI is that language is, in some sense, our species best attempt to encode everything about the world in as efficient and compressed way as possible. And that means that it's very yielding to an AI. There's a lot of noise. There's a huge quantity of writing freely available on the internet. And there are also a huge number of books, for example, I think very roughly speaking in this sort of Fermi estimate level. There's something like ten million books in the Library of Congress. And very, very roughly that might mean there's something like a trillion words. There are books. And then there's actually much more language information out on the internet. And so there's therefore a lot of data for AI models to learn from. And then a third reason, at least for, just some extent for me and maybe for many of you, is that if you're actually able to get an AI that kind of knows, quote, unquote, understands language, then you can communicate with it in a kind of natural way. You can ask it about anything, and you can get a lot of intuition from the responses and behaviors and all sorts of different kinds of evaluations you can perform on such a model. If you compare it to sort of ancient history of AI like excitement about classifying images from, I don't know, Alex Net, ten years ago in sort of the distant past. And from, say, AlphaGo, again in the distant past five years ago, there's a lot more intuition you can get. And then you can use that to sort of understand what these models know and don't know and can do. And you can also think about this in terms of how to make these models aligned with what humans prefer. There's a lot of work on trying to understand language model bias, racism, other such issues, and there's really a lot that you can kind of explore and dig into. So I imagine this is all very basic for everyone here, but just so we're on the same page. If you're doing kind of contemporary neural network based machine learning, the ingredients that you need to get started are really surprisingly simple. You need some kind of model to parameterize a function. You need a data set. You need some computers with plenty of computation. You need a lost function and you need some choice of optimizer. And basically for pretty much everything in this talk, I'll be thinking about language modeling as a task where the lost function is simply to predict the next word in some sentence or paragraph or book. And so that's how basically all of the models that I'll be talking about are trained. They have a lost function which incentivizes them to predict the correct probability distribution for the next word. So what about these other ingredients like the models that we use, the data sets that we use, and how much computation do we use? What are those sort of order of magnitude figures? So one way to think about this is sort of how much language do we consume as a person for comparison. So you can imagine that if you were a very voracious reader, maybe you'd read a long book every day and you'd spend your life doing that, maybe you'd live for 70 years, if you did that, you'd end up reading something like two billion words over your lifetime. For comparison, a canonical large language model, GPT-3, was trained for on the order of 200 billion words. So that's about 100 times more language data than maybe you'd see in your lifetime if you kind of tried really hard to attend to written text. There are other data sets, of course, that are much, much bigger than GPT-3's trading set. The year's common crawl, which is a sort of snapshot of the internet that anyone can go out and download if you like, this has very roughly on the order of 10 to the 15 words. I said earlier that the Library of Congress has something like maybe 10 million books, each book is maybe 100,000 words. So the Library of Congress in total maybe has something like a trillion words. And as another sort of smaller data set example, English Wikipedia is very roughly of order three billion words. So maybe if you spent your whole life reading Wikipedia, you could just barely do it if that was your mission. So what about the actual neural networks that I'll be talking about that we currently seem to be using fairly effectively to model language? So I'll be talking about transformer language models, so-called decoder-only transformer language models of which GPT-3 is an example. And just to sort of pout things, these models have, with kind of the standard way that they're set up, a number of parameters, which is something like 12 times the number of layers in the network. So GPT-3 has 96 layers. You can make deeper or shallower such networks times the sort of activation dimension squared. So D model, this D model parameter is just the dimension of the vector space that each token occupies or word, if you were to use words as tokens, when you run this model on language data. And so this gives you some sense for where parameter comes from. I think D model for GPT-3 is of order 10,000 and layer is 96 and that's how you get roughly 200 billion parameters in that model and other models scale similarly. Now, how much computation do you actually do when you train this kind of model? Well it turns out that different neural network architectures have different properties with effect this question, but transformers are actually quite simple in that in a forward pass of a transformer, every parameter on every token performs roughly one add and one multiply and then about twice this in the backward pass. And so that gives us a very simple formula that the number of floating point operations that a model like this performs during training is 6, which is 2 times 1 plus 2, times the number of parameters in the model times the number of tokens, that's what D is sort of the size of the data set in tokens that you process. And one other point that sort of I'll make while kind of going over these estimates is that you might wonder whether or not there's a lot of computation involved in processing long sequences. There's sort of a famous point that dense attention in transformer models is n squared with respect to context length and that's absolutely true. However, if you actually work out the sort of coefficients, the ratio of the amount of computation you do in a forward pass or during training in the context direction versus in the direction of sort of moving up the layers of the model is roughly n context over 12 times D model. So I note this just because if you think which I'll kind of suggest that this is a likely direction for the world to be heading, that models might continue to get bigger, then D model for GPT-3 is already 10,000. So the denominator here is order 100,000. And so actually even if you have quite long contexts with the sort of dumbest possible dense attention, the amount of compute you actually do in the context direction is not always so much. What about actually numerical values for this compute? So the largest models that we have so far, if we're in kind of Fermi estimate mode, we can round up and say they have say order a trillion parameters. If you have a model with a trillion parameters, then what kind of hardware are you going to run it on? Well, you might run it on in a 100 GPU at least this year. And a 100 GPU is performed about 3 times 10 to the 14, floating point operations per second, or 2 times 10 to the 19, floating point operations per day. This means that it's sort of convenient to sometimes use units of pedoflap days, which is 10 to the 15, floating point operations per second times a day. And that means that's about 3 a 100 days. And that's about 8.6 times 10 to the 19 or order 10 to the 20 floating point operations in a day. So how does sort of the compute available on hardware compare to the compute that we do when we train these gigantic models? Well, if we have a model with a trillion parameters and we train it for 300 billion tokens, then we get 6 times 10 to the 12 times 3 times 10 to the 11. And so we get on the order of 10 to the 24 floating point operations to train a trillion parameter model for on one of these large data sets. So these numbers involved, I mean, I think the thing that I find most amazing about this is that I still remember taking chemistry in high school. And in chemistry, you learn that sort of a macroscopic amount of stuff is sort of an avogadros number of atoms, which is like 6 times 10 to the 23. So somehow we're actually able to build computers that do, that working together, do more than an avogadros number of computations to train these neural models. So anyway, I find these numbers kind of mind-boggling and also useful to sort of have in the back of your head to understand what's going on. So with that, pretty good, unless there are any questions, I'll start talking about scaling laws for these kinds of language models. So what I'll basically be arguing is that there are very surprisingly precise empirical scaling laws for the performance of machine learning systems, machine learning models, as a function of kind of gross macroscopic inputs like how many parameters does the model have, how big is the data set, and how much compute is used for training. And I'll also make the point that if you're sort of in an airplane at 30,000 feet looking down on what's going on in the field, a lot of the other details in these systems don't matter all that much, or at least they don't matter as much as you might have expected that they would. Very often they just change some kind of constant pre-factor in these kinds of scaling laws, which give you kind of a big picture of what's changing as you really increase these inputs. And one way of sort of turning this into sort of a theme, what do you learn from it, how do you summarize it, is that getting these models to perform better is to a large extent about kind of avoiding bottlenecks. It's avoiding being blocked by something. And there are a lot of things that can block improvements in performance. The most obvious one, which is what scaling laws are studying, is you could not have enough data, you could not have a large enough model, you could not have enough computation to train that model. And then there are also a lot of other literal bottlenecks that you can think about, many of which involve sort of that information propagation through the network. So I guess like one way that I would summarize a lot of the most highly cited papers in machine learning in the last 10 years, papers like Resnets and LayerNorm, BatchNorm, things like that, is that there's sort of alleviating bottlenecks where information wasn't propagating nicely through your network. And the sort of simplest possible picture to sort of illustrate this, which perhaps is a cartoon of what's going on, something that I'll talk about later on with LSTMs, is that if you take a matrix, I mean neural networks are really just fancy systems that do a lot of matrix multiplication. If you take a matrix and you multiply it a large number of times, then very roughly speaking what you end up with is a projection onto its largest eigenspace. And so very roughly speaking, even with a deep network and you sort of don't set it up correctly, it's very easy to be in a situation where you lose signal or lose information and you get like a literal, literal model. But anyway, that's sort of the philosophy that at least at zero-thorder might, you might sort of reach from thinking about some of these results. So this slide is really about the kind of core results for scaling laws for language models. I'll explain it in some detail. So I'm actually going to start with the plot on the far right, which is about scaling laws with respect to the number of parameters in a neural network. And so what we did to generate this plot was get a very large data set such that we weren't worried about models overfitting it all. And train all of our models for a very long time so that they were essentially at convergence. So in other words, training time or compute was not constrained on performance. And then plot the resulting test loss of language models, trained to predict the next word as a function of parameter count on a nice log scale. And so what you see is that there's this power law, which is a straight line on a log log plot of the loss as a function of the parameter count of these models. In the middle plot, we do the same thing, but switch the role of the amount of data that we have with parameter count. So we train a model that's very large, maybe one of the largest models on the plot on the right, so that model size is not a constraint on performance on data sets of various sizes. And we apply early stopping. So we measure the test loss at the point where the test loss is at its minimum during otherwise pretty naive straightforward training. And we find again a very clear power law for loss as a function of data set size. And then the most complicated plot is the one on the left. So on the left, we plot all of the learning curves for many different models. We provide these models with plenty of data so they're not overfitting. They're in the under parameterized regime. But we train all of these different model sizes for a very, very long time. And we measure on the x-axis not the number of training steps or training tokens, but the amount of compute that has been used so far during training. And as a consequence of one of the formulas that I wrote on a couple of slides ago, that compute is six times parameter count times the amount of training data. If you take the logarithm of both sides, the log of parameters times data is log of parameters plus log of data. So what that means is that learning curves for models at different sizes are just shifted over left and right by constant amounts with the largest models on the sort of the far right of this curve and the smallest models on the left. So we have the learning curves for all of these models all put together. And so a question you can ask is sort of what is the best loss you can get for any given amount of training compute where you're allowing yourself to choose the model that does best for that amount of training compute? And that's what sort of the heavy black line and the orange fit are picking out. I mean formally you could call this the convex hull of all of these curves. And that again somewhat surprisingly seems to obey a very nice power law fit over many, many orders of magnitude in computation. And it's crucial for all of these experiments that you're only limiting performance with one thing at a time. On the far right you have plenty of data in compute, but you're limiting the number of parameters in the middle you're limiting the amount of data but you have a big model. On the left you're looking at training compute but you have all sorts of different model sizes and again plenty of data. So in other words in each of these cases there's sort of one of these parameters that's bottlenecking performance and otherwise you have plenty of resources. There's a question? I hope that's not true. So there's a minus sign in the exponent. I'm not sure if you're looking at the lines or the function. On the bottom, I see. I'll be right. Oh, I see. Okay. Yeah, they're just a log scope plot. Yeah, please ask any questions. Great. And then the x-axis on this compute plot is this pediflop a day unit. That's why it's actually a small number. Any other questions about anything about this plot? Cool. So there's another thing that you can do that's kind of interesting with the plot on the left, which is you can ask for any given quantity of compute that you have available. Someone kindly donates to you some number of a-100s to use for a few weeks and you want to use it to train the best possible language model you can. And so you can ask based on this plot on the left, how should I allocate the computation that was given to me in terms of making a bigger model or training longer? And it turns out there's sort of a simplified cartoon for the answer that we found with our language data, which was that you mostly want to allocate most of your compute, basically two-thirds on a geometric scale to making models bigger. And you can allocate about a third to training for longer on more data. And so this at least for us wasn't an obvious conclusion. It suggests that a lot of the gains that you're going to get if you want to get better performance with a fixed amount of compute, a fixed budget, is going to come from making your models bigger. And it turns out that in practice, I won't go into it in detail. You can, to some extent, just make your batch size bigger during training. And that means that the total number of serial steps that you train for doesn't have to increase all that much. You don't necessarily like to train for vastly longer. You seemingly just need a largely a bigger model. And that's something that you read off from this compute plot that I showed. That's a general question. The way that you get this graph is you basically do an analysis where you look at any given point for compute and you look up and you pick out the blue curve that's closest to the black line. And that gives you a model size and an amount of training. And so you can do that for all of these different points on the x-axis. And then for any given point on this x-axis that tells you a model size. You learn model size as a function of your compute budget. And then, conversely, you also learn an amount of training, which is sort of a data set size. And so that's the explanation for the sort of million x model size versus a thousand x in data. I probably won't try to explain the batch size question. But it's basically based on some empirical analysis where you ask, how big can you make your batch size? How far can you push data parallelism without seeing diminishing returns? And that's sort of the rough answer from that question. They're all trained from scratch. So this is always almost everything that I'll talk about in this talk is training from scratch. Any other questions? Okay. And then another point that I mean, I don't want to overemphasize. But like I said, from a sort of very zero-thorder naive perspective is that for some of these results, architecture isn't the most crucial thing. So I think one of the biggest advances in machine learning in the last five or ten years has been the development of the transformer models that I'm talking about. But of course, you can do language modeling with a recurrent model that reads words in order. And of course, LSTMs or stacked LSTMs are sort of the standard way to do that. And so you can compare what you actually get if you study LSTMs versus transformers. And at zero-thorder, it doesn't seem like LSTMs are so bad. It looks like as you make them bigger, they are scaling up quite nicely. But there's basically a constant offset where transformers are something like five or ten times more efficient for a given model size than LSTMs. And so I think this is a very, very convincing plot that tells you the transformers are in fact better. But you don't necessarily need a transformer to see that making models bigger is giving you in. And really the sort of more interesting limitation of LSTMs that I'll also talk about a little more later is if we plot something else. So if we look at a thousand tokens, which is something like 600 words of context, we can look at what the loss is as a function of the position in the context. Because if you've read more of a document already, you're going to be better at predicting what the next word is because you have more context available. And they're very smooth. It turns out also power law curves for the loss as a function of context position. But the thing that you notice is that the red lines are LSTMs and the blue lines are transformers. And LSTMs tend to sort of plateau in performance after on the order of a hundred tokens. And this is sort of another bottleneck in a different direction. This is the famous fact that transformers are much better at learning long context information. And this is obviously a limitation of LSTMs. But sort of the basic parameter scaling law seems like it holds for many architectures. And then there are much more refined questions. You can ask, I won't go into too much detail on this. But there are all sorts of hyper parameters in transformer models. And you might ask how much does it matter if I really optimize those? Do I get qualitatively different behavior if I optimize those better? And what all of these plots show is that for various different kinds of hyper parameters and transformer models, there's some broad basin where you get quite good performance. I mean, maybe a factor of three in either direction where performance doesn't change all that much. Of course, you might want to optimize that I'm not saying you shouldn't, but kind of qualitatively it's not an enormous difference. So I think this is also a place where it's, so I'm going to tell you in a few slides that a lot of these features are true more generally beyond language. And they really sort of say that much of what's going on when machines learn is quite universal. But there are features that are not universal. So this is kind of a nicer plot of loss versus token index. And I've included some power law fits, which are dotted lines, which show that this is actually, this performance is also highly predictable. That just says the obvious that when you read more, you understand it's easier for you to predict what's coming next. But you can train models on images, I'll briefly talk about that later, you can train models identically on images. And there you see a performance as a function of context position. It's very different. So here you have a model that reads pixels row by row. And as you might expect, there's usually much more non-tribule stuff going on in the middle of an image rather than in the background. And that's represented by the fact that models do much worse. Their loss is higher in the center of images as compared to junior-of-the-edges. So while some properties of transformers and language models are universal, and I'll talk about those later on, there are features of language data that are totally different from other data distributions. And this is a very stark example of that. But generally, the fact that there are these kinds of nice patterns lurking whenever you optimize a model. I think that is very common. So any questions about this? Yeah. Do you mind going back to slide? Do you mind explaining what it means to have a loss on the first template versus the 1000 template? Yeah, so if you imagine you have a thousand words extracted randomly from a book, then the very first thing you can ask the model to do is try to predict the very first word. Then you ask it to predict the second word, the third word, et cetera. The very first word basically all the model can possibly do is predict the unigram distribution for its training set. It just doesn't have any information to go on, otherwise to predict what's happening. And so that's why it's lost is very high. But by the time you get to the end of the passage, you've read a lot of some little short story, and you know a lot about what's going to happen. You know what kinds of words are likely to come next. You know about the author's style and vocabulary. You know about what characters exist, et cetera. And so your model has gotten much, much better at prediction by the end of the context. And so literally to make this plot, you take maybe a thousand, ten thousand different passages with a thousand words in them. You compute the model's loss on all of the words in the passage, and then you take the mean, and you get some nice plot like this. Yeah. But because the computation complexity is quite tragic with respect to token index, without being that essentially, for talking about it, if you think about it, if you were looking at compute, then it would go from the zero to like ten to the six. So you know, you know, significantly greater compute, for a given test gloss as you increase token index. So it's true that if you make the context length longer, longer, you will spend somewhat more compute. But the fraction of the amount of compute you spend near the last token isn't nearly so stark. Most of the compute happens in the matrix multiplies for the MLP feed forward part of the transformer, and also the matrix multiplies to make the keys and queries and values, et cetera. That's actually in most well. It depends on the model hyper parameters, but in many models, especially models that are large, that's actually the predominant compute. And so actually, the amount of compute you do for the last token in the first token might only differ by a few percent. So for GP3, I think it's literally like one or two percent difference. So the sum matrix multiplies the tension. Yeah, yeah, yeah. So I mean, the formula for that was this one that I briefly mentioned here. So basically, how much compute you do in the context direction divided by the amount of compute you do in the matrix multiply direction is this. So if your model is, if D model is very small, if D model is 128, and context is 1,000, then it's basically 50-50. But if D model is 10,000, and context is 2,000, then it's like 2%. So if the model is keep getting bigger, then that means that if you're willing to pay a fractional cost, then you can keep making context length longer and pay a fixed fractional cost. And of course, if you use something fancy with intense attention, you also get extra winds on top of that. Any other questions? Cool. So this is sort of both of these, the left and the right, show you samples from a transformer model. Very roughly speaking, they're identical kinds of transformer models, which is with some slightly different hyper parameters. But they're trained on very different data distributions. The one on the left is obviously, this is GPT-3. The one on the right is IGPT. It's a model that's trained to predict pixels, row by row. And so what happened here was that we took the top half of an image and then generated all the rows beneath. And so the same kind of model architecture, but just trained on different data distributions is able to effectively learn very impressive generative capabilities in both cases. And so this is sort of a qualitative hint at the possibility that what's going on here is quite universal. And so another way of introducing it is say, you might have some questions after the last few slides. How are the scaling laws I'm talking about really specific to language, are they a feature of the kinds of data that language is? You might ask, do these scaling laws really continue? You showed that they're true over many orders of magnitude, but did they break down eventually in a what way? And then another question you might ask is, what do they imply for other kinds of evaluations? You probably don't just want to generate raw samples from either of these kinds of models. You might want to use them for some other more specific task. And so the question of whether or not the test loss, the training loss that you've optimized as that goes down in a predictable way, does that also imply that other things, other capabilities of the model are improving? So I'll be talking about these questions. So this plot contains kind of a lot of compressed information all at once, or the set of plots. So this is the result of what happens if you train the same kind of transfer models on sort of five different data distributions. So text language we already saw, but you can try video where you predict every pixel in a video in this sort of rectangular prism of video pixels. Images, this sort of synthetically generated deep-mind math data set where you're trying to predict the answer to math problems. There's a multimodal data set where you have image text pairs in either direction. And in all cases, the x-axis is compute, and the y-axis is the appropriate test loss for that class of models minus a constant. So that's the one complication that I've added here. So the claim is that these dashed lines in terms of the original loss are a power law, like the power laws that we saw on a much earlier slide, plus one constant term. And if you subtract off that constant term, then you make a log-log plot once again, then you once again get these very, very nice straight lines. And so this compute scaling law generalizes to all these other data distributions. And the other scaling laws also generalize, I just haven't plotted them. So the claim of this slide is that scaling laws do generalize to all of these other data distributions, and you train the same basic kind of model on them. And furthermore, there's sort of an intellectually slightly interesting point, which is that if you really believe that these dashed lines are true, if you think that they're a real feature of what's going on, and they continue out very, very, very far, then if you think that the loss is a constant plus a power law, then you can interpret the constant term as the entropy of the underlying data distribution. And you can interpret the power law as something like the KL divergence between the true data distribution and the model that you have. So that's a lot. The important summary at zero-thorder to remember is that I'm telling you that the kinds of scaling laws I presented for language generalize to all of these other domains. There's also some other interesting features here. The reason why I used compute to illustrate that the scaling laws generalize is because you can ask another question now that puts all of the different data distributions on one plot. It wouldn't have made any sense to combine the five plots on the last slide into one plot, because the test loss, when you're predicting a word, is not in any way comparable to the test loss when you're predicting a pixel. It doesn't really make sense. They don't have the same units. It doesn't make sense to put them together. But something that does make sense to put together is what the optimal model size is as a function of your computational budget. And so in the same way that we did for language, you can go here and you can ask for any given amount of compute, like 10 to the minus 2, petaflap days, what is the best model size? You can do that for all of these plots. You combine that information together and you find something kind of surprising, which is that, again, roughly speaking, if you're sort of willing to allow a little bit of wiggle room, all of these different kinds of models seem to be on the same trajectory for optimal model size versus compute. There's some kind of universal fit of how much bigger you should make your model if you're going to model any of these data distributions with some given amount of compute. So what about other kinds of tasks? Well, one of the most classic tasks that you can ask about in ML is image classification. And so the models that we were training on images, and that I've shown you plots, they're training loss, these models are sort of on tiny little images predicted. Pixel by pixel, in particular, they're 32 by 32 images, so we can look at sort of the 32 by 32 pixel version of image net classification. And the models that I was discussing are generative models that predict pixels, but you can shop off their heads, add a classification head in its place, and try to predict image net and train on image net. And the orange curve that I've shown you here is what happens if you just take a randomly initialized model with that architecture and train it. You get very good performance up to a point and then performance plateaus because you're being limited by the fact that image net is from this point of you a small data set. However, if you take these pre-trained models that have been trained generatively to draw pixels, they sort of use the features, presumably they're using the features they learned from image generation for classification, and you get some nice trend for the error rate in classification as a function of model size. So this is saying that in this particular case, we actually do fine-tuning the pre-training you did and the sort of trends you saw really kind of transfer into trends in something else you might care about like image classification. We can ask the same kinds of questions about language models. In particular, does this steady improvement in language modeling as a function of scale, does that translate into better performance? And this is sort of an interesting subject by itself. And so you can ask what happens if we scale language models. And so this is sort of this exact same plot that you've seen a couple of times now for language models, but it just increased from sort of original work that we did out to this yellow line, which is GPT-3. And you see that basically this sort of trends continue. Possibly GPT-3 is sort of missing the trend a little bit. I can't really honestly tell you whether that's because GPT-3 wasn't well optimized, or if it's because there's some bending in this curve where we're hitting some irreducible loss. That irreducible loss would be something like the entropy of this sort of language data set itself. But it's just in order the trends continue. And what's now pretty well known is that if you train fairly large language models, then they can exhibit in context learning. So the kind of learning that I'm talking about is that you give these models an example of many arithmetic problems or many anagrams or whatnot or translation tasks for individual words. Then early on in the sequence of the top, they might not be very good at doing the task, but they figure out what the pattern is in the task and they learn to do it. And in particular, you can plot that so you can ask for, say, like one of these anagram tasks. What is the performance of the model as a function of how many examples of the task get seen in the context? So this is kind of similar to the loss as a function of context position, but it's now an accuracy at doing an actual task, like unscramble the letters in a word. And you see probably most importantly that if you give more examples, you get significantly better performance starting from very, very poor performance to pretty good. And also you see that larger models do this better. You also finally see that giving a natural language prompt with some instructions helps significantly in the regime where you have very few examples. This is in context learning. You can call this a kind of meta learning. And it just emerges automatically from training large language models without any particular attempt to get this kind of behavior. And you could also ask sort of about downstream tasks that you actually you care about. So there is accuracy at doing arithmetic as a function of model size, a bunch of different kinds of arithmetic problems. There is just some data set of analogies from a test that American college students take to go to college, the SATs. And if you care the sort of average score of that year's test was I think 58% or so percent. So the largest model is sort of doing a little bit better than the average American high school student. The trivia QA, which is sort of just knowing trivia. And Wina grad schemas are problems like if a tree falls on your roof and you got it fixed, what did you get fixed, did you get the tree fixed or your roof. It's a measure of common sense reasoning and models are also getting better at this. And I think the other interesting thing that's very often emphasized is that clearly trivia performance is improving very smoothly as you make models bigger. The models are just remembering more and more trivia. Wina grad schemas are also improving fairly smoothly. But then there are examples like arithmetic where models are very poor and then they sort of suddenly get pretty good. And so these kind of sudden rocks sort of the model sort of suddenly kind of like gets what it's supposed to do for arithmetic are pretty interesting. And there are all sorts of other kind of interesting things if you kind of dig into these specific abilities. Yeah. Why do bigger models do better in the context of student? I mean, I guess the sort of dumb zero-thorder point is that larger models are just getting much better and better at predicting the next word given more and more context. So I think it like, I think there's a very tight connection between a plot like this and these sort of in context loading plots. Basically the more information you're getting, I mean all of these models probably know the unigram distribution of words and tokens pretty well. But the bigger model is getting much, much, much more information from its context than the smaller models. And at a certain point, I mean, it depends on your training distribution and all sorts of other things. But like, one of the things that we do is when we see several examples of something happening in a text, we guess that that's what we're going to see next. And that's really probably embedded in a ton of text that's out there on the internet and in books. And models have to decrease their loss somehow. That's a pattern in the text. It's a pattern that models eventually learn and they seemingly apply this knowledge. I think there are other people, of course, who've kind of worked on this question more specifically, and I have more specific theories. But I think it like kind of an intuitive sense, that's how I would think about it. I guess one final evaluation you can ask, can people tell that text written by a language model, it was written by a language model or that it's a human? This is an evaluation where we looked at short news articles. There's two or three paragraphs and generated equivalent news articles from GPT-3. And by the time you get to sort of the largest models, people are approaching chance accuracy at being able to tell the difference. This sort of has a lot of implications, both, I mean, it's interesting and surprising as a state and it's about language modeling. But it's also somewhat scary. That means these language models are very difficult to tell that you're talking to a language model if you don't have a very long conversation. Yeah. Hi. So I am wondering, so for this specific statement, because with modern current models, they are what projects are going to use you. So have you attended, like, the general experience article, and what you don't have, and for a document, or do you have anything? I actually don't know the answer to that question for this particular analysis off the top of my head. I believe that these are not memorized. One simple thing you can do, at least, for some things that occur frequently is like you can look at the distribution of the loss for a model on its own samples. And at least for things that are memorized, that are very clearly memorized. Obviously they, of course, probably they first frequently in the training set, but also the loss tends to be much, much lower on memorized samples. Because you can intuitively understand this because if there's 100 words that are exactly verbatim sampled out, and you're sampling at temperature equals one, then all of the next word predictions have to be extremely, extremely confident. And that means the loss has to be super low. So, I mean, just informally, something that I've done to just get rid of memorized samples is compute the loss, and usually you'll just see a pretty clear by-modal where there'll be a few memorized examples and then things that aren't. That's a simple thing you can do to check. You can also, of course, do de-de-de-de-de-de-de-de-de-plocation. I don't remember off the top of my head what de-de-de-de-de-de-plocation has done here, though. On the Downscape task section, if I want to say about how scale loss is, you can look at the transferable objects and adversarial objects. I don't think I have anything particularly clear to say about that. I mean, these evils, I think, are not adversarial in the sense that they're just few shot evaluations with some fixed data set. There are a large number of different kinds of adversarial data sets out there for reasoning, for common sense knowledge, for truthfulness. So, I mean, there's, like, for example, truthful QA. This is an example where there aren't any trends like this and arguably the trends go downward, though it depends on your training distribution and some models actually do improve. So I think that's a complicated question. I think it's hard to find examples where the trends go down. I don't think it's easy, but these do exist. Any other questions? Great. So, I guess I'll sort of end by summarizing some lessons that you might draw pretty practically for research from this. And then I can either open it up for questions, or I can also, I can always talk infinitely long. I've been a professor for like 10 years of my life, so I can just talk forever. But I'll sort of end after talking about some lessons. So I think one lesson that kind of I draw from this is that kind of scanning over some of the important inputs to your training process is just a pretty useful thing to do when you're doing ML research. And it's sort of typically very cheap. It's cheap because generally most things vary in an important way on a log scale, or sort of on a geometric scale, however you want to say it. And that means that like if you're training with the data set of size D, maybe you should also train with D over 2 and D over 4 and D over 8 or something like that. And if you sum that geometric series, you get 2D. So you sort of, I mean, you made your training process twice as expensive in some sense, but it's not really a big change in what you have to do. But you can often learn a lot about what's going on by doing these kinds of scans. And so, I mean, this is an example of some data that I didn't show earlier. So, I think you might wonder about is what happens if you scan over data set size and model size at the same time. And it turns out there's some very simple trends that you can model in that case too that tell you about things like overfitting. And I mean, if you care about overfitting, then this tells you about something like how big do you have to make your data set for a given model size to avoid overfitting being a significant problem so that you can answer all kinds of questions like that. And I at least find that this is kind of useful and it's nice for learning things about behavior. And I think alongside that, I think like this is sort of a joke. This isn't real. This is sort of making fun of a large number of machine learning papers that you might see. I think a lot of machine learning papers have tables like this. And it's sort of hard to tell from like this kind of table obviously I'm making fun, but I think it's not so unrealistic like did the technique that went into our model really improve on other things that happened. And I think that this kind of plot at least for me is a much more convincing statement that like will clearly transformers are just better than LSTMs. So the slogan here is sort of success for new techniques if your goal is to sort of improve a model if that is your goal. And I think it's at least to me much more convincing and kind of clear what's going on if you see these trends. Maybe I have another slide making fun of the CS. So I mean, I think this is a thing that I actually see very often in research is that you come up with some new idea and you see like you first do the cheapest easiest experiment and you see, well my new idea improved performance. I'm really excited. Everyone should it should adopt this. But then you make some plot like this and you sort of say, oh, okay, I guess it doesn't really matter that much at all. And I think this is actually a comment. I mean, I think we all have all sorts of ideas. I mean, people fall asleep at night and they can't sleep and then they wake up and they have ideas and like, oh, I'm going to go try this. We all do it. But oftentimes they don't work and I think this is sort of useful for understanding whether your idea really, really works. And I mean, if all you're ever going to do is train this model, then your idea did work. But I think that like there's sort of an expectation that probably people will be using bigger computers to train larger models in the future. And so the ideas that are really going to have a huge impact are ones that sort of point in the opposite direction. I've even seen ideas where on small models they make no difference at all, but on larger models they do better. And so these kinds of trends I think are useful. And they're certainly useful to think about. Another point that I find useful, I think it's not sort of obvious and maybe you shouldn't trust it completely, is that I tend to think, I mean, because I've sort of swallowed my own coolade, that if something works, then it should scale fairly predictably. It's not always true, but for things that you can measure that are very close to your optimization target. If sort of your training process, your hyper parameters, etc., are all kind of set up well, then I tend to think that you should see some kind of predictable trend. And if that trend goes away, then I mean, maybe that's just exactly what's true. But I think often it means that there's something broken about what's going on. Maybe your numerics are broken and you need higher precision in some part of your model, maybe there's some bottleneck you hadn't thought of. So I mean, this is also an example that kind of scaling, predictable scaling can be found all over the place. So I just think this is sort of neat. So if you just train these extremely naive, very stupid multimodal models, or you use a decoder-only transformer to either model the text based on the image or model the image based on the text, then you can do that. And measure a sort of empirical mutual information between the image and the text. How much information did the image give you about the words in the sense of sort of Shannon information? And or conversely, how much information did the text give you about the image? And this is also a place where, I mean, this is very close to the optimization target. The whole point of the multimodal is to get this information. And you see that there's some predictable scaling going on where larger models are getting more information about one data, just one part of the distribution for the other. But I think this is sort of a general thing that you should expect in model training. And so maybe to sort of summarize, maybe even bigger picture implications. I think that these kinds of results suggest that it may not be the best or the smartest or the most interesting way to make better ML models. Maybe it won't be the way that happens in the future. But at least I think these results suggest that there aren't any really hard conceptual barriers preventing people from training significantly more powerful models of all kinds, including of course, language models in AI research. I think certainly my perspective, originally as a physicist, sort of coming to machine learning, kind of fresh new way five years ago, is that, I mean, this is sort of one set of abstractions for thinking about kind of what's going on in AI research that you, if you're going to be training fairly large models and you want them to do well, that's a thing that you're going to do, then you probably want your models to sort of be scaling well in terms of their performance. And I think this framework of maybe there's a bottleneck, but if you remove the bottleneck, then you'll just continue to see further progress. I found useful. I think another point that, well, maybe I'll make this point at the end. Another point is that, yeah, scaling laws are just sort of all over the place and they can help you to sort of maybe organize your research a bit. And then, I mean, maybe the most interesting point conceptually, though, is that it seems like, if you believe this kind of story, that it seems like many domains of ML are kind of surprisingly simple and universal, things that you might not have thought are the same or more similar than they are different. And of course, this is also a fascinating thing to try to understand. So I mean, I was a theoretical physicist for most of my life, so I mostly tried to understand things that seem extremely esoteric and weird and why would anyone care about them. This is a thing that I think probably, probably everyone in this room kind of cares about, like, can AI models write, can they communicate in language? And these kinds of trends are really, really nice, though the kind of trends that you might see in a very controlled physics experiment or something, and yet they're coming out of something very, very noisy and random, like, predicting language data on the internet. So I think it's very interesting to think about, like, why are these kinds of trends true? What is the underlying kind of theory or science here that makes these trends true? Can we predict it? Can we refine those predictions? Can we understand why when this doesn't, doesn't occur? Another question is sort of, there are some exponents here. This is a straight line, but the straight line represents a power law with a particular exponent. Why that exponent? For language, it's like 0.0 H or so. Why 0.08 and not 0.2 or 0.4 or 0.001? I think there are all sorts of questions here. When you see data that has a very clear trend, it's very interesting to understand, to try to think about why is something so simple happening. And I'll sort of leave you with that. Yeah. Have you thought any about how this is scaling laws, what are providing human beings? So your picture is essentially making everything bigger, avoid bottlenecks, and all the thought. Whereas I guess human beings, so you're good on the number of parameters, because they're still several orders based on two in rooms there. But it seems like you're not very good on the bottom of the map to use. So human to use is very constrained, but it's only an empty map, so it's because of the slow processing, because it's also because of the empty demands, try to have it be using most of their parameters first of the time. And data's a little bit complex, because I guess we get a ton of data, so if you are thinking of that on the amount of language data we get, you know, sort of fully complex language uses, sort of three orders and make the shoot down, where GPT3 is now. And get something good seems to happen, if you look at what these are all those. Any thoughts on that? I mean, I think it's a fantastic question. I don't have anything to say that isn't quite speculative. So I mean, I don't have any good answer to the question. I think it's a great question. I guess one thing that seems like it's true is that sort of the factor of a thousand you mentioned seems pretty common. I mean, my impression is that AlphaGo probably plays like a thousand times more games when it trains than like a go master does. I think this is a pretty common factor to see in a lot of like ML contexts. But I have no idea why it is. I don't know if it's that evolution optimized us to learn fast. If we have some hard coded information, if this sort of multimodal inputs that we have help a lot, you might imagine that when you have a system that's already pretty smart, reinforcement learning or active learning of some form becomes more and more important, because like when these language models or a person, like if I read a physics textbook, I don't really learn a lot in a certain sense because I already learned physics. And I think the same is probably true for these models. So as the models get smarter, this sort of very dumb next word prediction task is giving you less and less information, but you might expect to get more and more information if you did something more active. I can continue to speculate, but I don't really know anything about it. I don't have anything well established to tell you. It's a great question. So if you can't have the transformers, the LFGM, the LFGM, the LFGM, the transformers are a bit better, but they're a smaller, similar mode that seems like human abilities are a little bit more developed to a more standard, a few things on a very different place on the graph. Yeah, I think that's absolutely, I think it just true. The simple efficiency of these models is not similar. Another way of saying is that if you got into AI research to understand the human brain, it's very unclear whether we're making any progress on that. But if we just want to sort of, yeah, for a lot of these tasks, we don't seem to have to solve the brain to solve AI surprisingly. I think they have a question. Yeah. See it's deixar, and now I can wonder what you're going to get in this car and push There's also like things that will probably come, remember, really, like, not just acting in the past, that probably aren't necessarily written for, and by great science. Okay, so, Bob and Bob. Sure, sure, no, these are all great questions. So, I mean, sort of early on, I commented on some sources of data, and I mean, you're certainly correct about quality. I think in terms of quantity, I mean, I don't think anyone has, like, a digitized library of Congress, but I think if you did, that would be like, I don't know, maybe 10x bigger than the training set for GPD3. So, there's a sense in which there's probably quite a lot of, still quite high quality data that isn't in use. I don't know whether it will ever be in use, so it's a complicated question. And then, if you are willing to sort of take all of this garbage on the internet, or try to filter that garbage down, I think, I don't know how accurate this estimate is, but in order of magnitude level, you can get something like 10 to the 15 words, which is a thousand times bigger. And of course, if you find any kind of intelligent way of filtering, then if you can filter down to 0.1% of that, and take the 0.1% that's best, then you do still have a lot of data. So, I think for language modeling, there's definitely still some headroom, but this is certainly a constraint, and there are other kinds of data distributions where you'll run out sooner. I mean, in terms of, yeah, I mean, of course, there are all sorts of other things you can explore, one you can explore, multi-modal models, one can switch to a different kind of loss function that is more interactive, or actually accomplishing a task. But I think, for pure language modeling, it seems like there's at least some room left. And if you think that your model size increases, sort of, if you think you can increase your model size by a factor of 100 and increase your data set size by a factor of 10, which is sort of like roughly what this is saying. If you believe that, then you can still scale up your model size a lot and have probably plenty of data. But, yeah, you couldn't sort of do this stuff without the internet. Yeah, or, you know, do you want to share it? Sure, yeah. In terms of bottlenecks for improving a task of a time, are you more optimistic about how much larger models on the same level as a cheese water improvement, or petrol improvements, like the LSDF transform? I guess, I mean, I think I'm sort of optimistic about both. I think that my understanding, sort of the zero-thorder understanding of the hardware situation is that, like, connecting together GPUs and GPU, like objects works pretty well, and that, like, interconnection speeds are increasing and can increase pretty easily. So I think that you don't need one chip to run your entire model. You can distribute your model over many, many, many accelerators. And I think you can do that if you're willing to pay for those accelerators, et cetera, then I think you can do that. Architectural improvements, I think, I would say it sort of typically haven't been super excited about architectural improvements, but I think there will continue to be architectural improvements. I think that, sort of, whenever you do something for the first time, or even just, like, whenever you train a really big model for the first time, you sort of don't do it in the best possible way, and there's a lot of, like, all sorts of different kinds of improvements. Maybe there are, sort of, non-incremental improvements that will look like big jumps. So yeah, I think that'll be both. So yeah, I mean, there's a sense in which, if all you did was look at this plot and just try to continue it, that might be an underestimate of progress that the field is going to make, because there will be improvements in architecture and algorithms and things like that. So this related input was looking very good at the testing process, and it looked by some computer and new things were inBS. And then he can do everything he's going to do with doing that this scaling long, like, in the fall part, and he'll force back this type of almost the territory. I think that's a great question. I think the simplest version of this, well, a simple version of it that I think is probably important and increasingly important to sort of just reinforcement learning, reinforcement learning in a certain sense as a situation where you generate your own data, because if you have a language model doing RL, then it writes something and then you're training on that data. So I definitely do think that that will sort of augment data and mean that there'll be other avenues for improvement. Literal data augmentation itself seems also seem plausible to me. I think it's not happening a lot because there still is more language data out there. Yeah. I think I've got two versions of this one's more clear, just coming from a nine, and a few versions back on this one is just like how, what about associated with the language field and how I can't really explain about that in physics in the second part of this. In your research and for this, I'm sure you dealt a lot with different things going on. But I kind of understand to this type of stuff, other than finding that you found particularly surprising we're going to expect you, I'm just with your past experience, because I always get this a lot of different types of data. Also since now, two of you are going to understand, but for you like under this or like other stuff that you're doing by phenolic, you're using stuff right now, like if you're anything that is coming through like this and going to call us about that or just like particularly, so, supposing. I think to me the most surprising thing is of these sorts of results was probably that there is a very, very precise trend. It seems like, I mean, yeah, I mean, like, I think this is sort of an unusual thing, and I think when I saw that, I thought it was a really big deal. I think that like, usually like, I mean, it's just, it's not true in most many things you plot. I mean, obviously there are other plots that don't show this kind of trend, even if they're reasonable. I mean, like, I don't know, I mean, there's sort of a trend to interview a QA, but I don't really know what that means. But I think the fact that there's something seemingly very precise is, I view that as like a very intriguing entry point to like try to dig into something, because it means that there's probably some deeper reason. And then the fact that it seems fairly universal across data distributions, again, suggests something like that. Yeah, the main difference between data distributions is these exponents in a scaling browser different. I mean, in terms of like coming from physics, I mean, I think I got into like a lot of this stuff partly because I'm fairly mercurial, and I was interested, and a lot of other friends I had were interested, and so we sort of studied it, and went from there, I had friends, et cetera. But I mean, from another point of view, I think I got involved in it for really weird reasons, perhaps, in the sense that like, I just know a lot of people who are already, and sort of, I don't know, 2015, talking about things like, wow, is like, how much better is AI going to get? What are the implications going to be for the world? Is this going to keep improving at an addressed clip? What are we going to do to sort of make sure that these models are aligned with human values to use the kind of usual sort of phrase that's now used? And I sort of thought these people were weird and crazy, even though they were friends of mine, and I sort of said, oh, like this is really dumb, like I don't think that these AI models are really something to worry about. But like, I was still interested, and sort of was like, well, like smart people I know think that AI is improving very rapidly, and that might have a lot of impacts, and might require a lot of sort of caution and thought, and work to sort of make it safe. And so that was actually a significant motivation for me getting involved. It was a mixture of sort of, there being a lot of potentially really intellectually interesting questions, liking to sort of switch fields every few years, and friends of mine being very kind of concerned about this question, and yeah, that was sort of what brought me in. Are there everything you've seen in this picture, you know, how do model scale, the one operator, you know, one factor that you found has the most potential to work on, the scale we've got for this kind of question. I mean, if we go back to sort of very basic ML ingredients, of like, what are these things, like, so there's a sense in which this is all you're doing, you choose one of each of these five things. I would guess that what the objective is, is most likely to sort of change things, in the sense that predicting the next word is really sort of one of the laziest sort of dumbest things you can do. And, and, I mean, there are all sorts of things, so it's really just chosen because you want to be able to compute, you want to be able to do back prop, and so you want to be able to get some differentiable thing, you want to be able to get a lot of data for which you can compute this differentiable thing, and so that's the game that you're playing. But, I think that you can have other objectives, like through reinforcement learning, or some other kind of active learning, whatever, I mean, some combination of such things. And, I sort of would just guess that generally performance will change a lot more. Like, if you're expecting sort of these trends to be very different, I would guess they're different if you have a different objective. I think changing the data distribution, or the model might also change things, but I think that, like, the lesson that I personally draw from something like this, is that even if you found a, like, really revolutionary change, that was, like, much better than transformers, it might be kind of equivalent to making transformers 10 times bigger, but I'm not sure if that would be as big of a deal as changing the loss. Changing what the objective is. But that's just my guess, I have no idea. And, of course, this paradigm, I mean, I think I was trying to be polite. I usually have, like, a picture of a grilled cheese here to emphasize, like, sort of, how simple and sort of silly this is, rather than this sort of very sophisticated palette of spices. And, I mean, maybe someone will say, like, this isn't the right set of ingredients from which to think about things, and there's a different thing you should do, and maybe that will make a big difference as well. But I, that's sort of an unknown, unknown.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.08, "text": " In theoretical physics, to get this kind of audience, you have to win the Nobel Prize", "tokens": [682, 20864, 10649, 11, 281, 483, 341, 733, 295, 4034, 11, 291, 362, 281, 1942, 264, 24611, 22604], "temperature": 0.0, "avg_logprob": -0.21652644288306142, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.11085468530654907}, {"id": 1, "seek": 0, "start": 9.08, "end": 10.08, "text": " or something.", "tokens": [420, 746, 13], "temperature": 0.0, "avg_logprob": -0.21652644288306142, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.11085468530654907}, {"id": 2, "seek": 0, "start": 10.08, "end": 15.92, "text": " But, of course, I've been working on ML recently, and it's been much more exciting.", "tokens": [583, 11, 295, 1164, 11, 286, 600, 668, 1364, 322, 21601, 3938, 11, 293, 309, 311, 668, 709, 544, 4670, 13], "temperature": 0.0, "avg_logprob": -0.21652644288306142, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.11085468530654907}, {"id": 3, "seek": 0, "start": 15.92, "end": 17.52, "text": " There's a huge amount of interest.", "tokens": [821, 311, 257, 2603, 2372, 295, 1179, 13], "temperature": 0.0, "avg_logprob": -0.21652644288306142, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.11085468530654907}, {"id": 4, "seek": 0, "start": 17.52, "end": 22.6, "text": " So to some extent, part of what I'll be talking about may be an implicit theme will be sort", "tokens": [407, 281, 512, 8396, 11, 644, 295, 437, 286, 603, 312, 1417, 466, 815, 312, 364, 26947, 6314, 486, 312, 1333], "temperature": 0.0, "avg_logprob": -0.21652644288306142, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.11085468530654907}, {"id": 5, "seek": 0, "start": 22.6, "end": 29.8, "text": " of why there's so much excitement and why you might expect that excitement to continue.", "tokens": [295, 983, 456, 311, 370, 709, 14755, 293, 983, 291, 1062, 2066, 300, 14755, 281, 2354, 13], "temperature": 0.0, "avg_logprob": -0.21652644288306142, "compression_ratio": 1.611336032388664, "no_speech_prob": 0.11085468530654907}, {"id": 6, "seek": 2980, "start": 29.8, "end": 37.04, "text": " So an outline of my talk is that I'll first start by discussing motivations for language", "tokens": [407, 364, 16387, 295, 452, 751, 307, 300, 286, 603, 700, 722, 538, 10850, 39034, 337, 2856], "temperature": 0.0, "avg_logprob": -0.175350464714898, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0003148801624774933}, {"id": 7, "seek": 2980, "start": 37.04, "end": 38.04, "text": " modeling.", "tokens": [15983, 13], "temperature": 0.0, "avg_logprob": -0.175350464714898, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0003148801624774933}, {"id": 8, "seek": 2980, "start": 38.04, "end": 42.08, "text": " I'm sure you're all very well motivated because this is an NLP class.", "tokens": [286, 478, 988, 291, 434, 439, 588, 731, 14515, 570, 341, 307, 364, 426, 45196, 1508, 13], "temperature": 0.0, "avg_logprob": -0.175350464714898, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0003148801624774933}, {"id": 9, "seek": 2980, "start": 42.08, "end": 47.6, "text": " And I'll also talk about sort of orders of magnitude of data and compute that go into", "tokens": [400, 286, 603, 611, 751, 466, 1333, 295, 9470, 295, 15668, 295, 1412, 293, 14722, 300, 352, 666], "temperature": 0.0, "avg_logprob": -0.175350464714898, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0003148801624774933}, {"id": 10, "seek": 2980, "start": 47.6, "end": 49.120000000000005, "text": " contemporary language modeling.", "tokens": [14878, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.175350464714898, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0003148801624774933}, {"id": 11, "seek": 2980, "start": 49.120000000000005, "end": 57.08, "text": " And that will kind of set the stage for talking about scaling laws for neural language modeling.", "tokens": [400, 300, 486, 733, 295, 992, 264, 3233, 337, 1417, 466, 21589, 6064, 337, 18161, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.175350464714898, "compression_ratio": 1.6652173913043478, "no_speech_prob": 0.0003148801624774933}, {"id": 12, "seek": 5708, "start": 57.08, "end": 63.96, "text": " And further realization that these scaling laws seem to be quite universal for generative", "tokens": [400, 3052, 25138, 300, 613, 21589, 6064, 1643, 281, 312, 1596, 11455, 337, 1337, 1166], "temperature": 0.0, "avg_logprob": -0.162625468860973, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.00039792442112229764}, {"id": 13, "seek": 5708, "start": 63.96, "end": 67.96, "text": " models and maybe for sort of machine learning more generally.", "tokens": [5245, 293, 1310, 337, 1333, 295, 3479, 2539, 544, 5101, 13], "temperature": 0.0, "avg_logprob": -0.162625468860973, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.00039792442112229764}, {"id": 14, "seek": 5708, "start": 67.96, "end": 72.92, "text": " And then, finally, after discussing that, I'll talk about what happens when we actually", "tokens": [400, 550, 11, 2721, 11, 934, 10850, 300, 11, 286, 603, 751, 466, 437, 2314, 562, 321, 767], "temperature": 0.0, "avg_logprob": -0.162625468860973, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.00039792442112229764}, {"id": 15, "seek": 5708, "start": 72.92, "end": 75.0, "text": " do scale up language models.", "tokens": [360, 4373, 493, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.162625468860973, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.00039792442112229764}, {"id": 16, "seek": 5708, "start": 75.0, "end": 77.08, "text": " I'll talk about the GBD3 model.", "tokens": [286, 603, 751, 466, 264, 26809, 35, 18, 2316, 13], "temperature": 0.0, "avg_logprob": -0.162625468860973, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.00039792442112229764}, {"id": 17, "seek": 5708, "start": 77.08, "end": 82.44, "text": " And if there's time, I'll talk about some lessons from all of these ideas for research,", "tokens": [400, 498, 456, 311, 565, 11, 286, 603, 751, 466, 512, 8820, 490, 439, 295, 613, 3487, 337, 2132, 11], "temperature": 0.0, "avg_logprob": -0.162625468860973, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.00039792442112229764}, {"id": 18, "seek": 5708, "start": 82.44, "end": 87.03999999999999, "text": " which I imagine many of you are excited to be involved with soon.", "tokens": [597, 286, 3811, 867, 295, 291, 366, 2919, 281, 312, 3288, 365, 2321, 13], "temperature": 0.0, "avg_logprob": -0.162625468860973, "compression_ratio": 1.6752767527675276, "no_speech_prob": 0.00039792442112229764}, {"id": 19, "seek": 8704, "start": 87.04, "end": 92.64, "text": " So I'll start by talking about why we do language modeling and Fermi estimates for language", "tokens": [407, 286, 603, 722, 538, 1417, 466, 983, 321, 360, 2856, 15983, 293, 43261, 72, 20561, 337, 2856], "temperature": 0.0, "avg_logprob": -0.2613107962686507, "compression_ratio": 1.7158671586715868, "no_speech_prob": 5.388031422626227e-05}, {"id": 20, "seek": 8704, "start": 92.64, "end": 93.64, "text": " modeling.", "tokens": [15983, 13], "temperature": 0.0, "avg_logprob": -0.2613107962686507, "compression_ratio": 1.7158671586715868, "no_speech_prob": 5.388031422626227e-05}, {"id": 21, "seek": 8704, "start": 93.64, "end": 101.04, "text": " By Fermi estimates, I mean questions like estimating, is it a million or a hundred thousand or", "tokens": [3146, 43261, 72, 20561, 11, 286, 914, 1651, 411, 8017, 990, 11, 307, 309, 257, 2459, 420, 257, 3262, 4714, 420], "temperature": 0.0, "avg_logprob": -0.2613107962686507, "compression_ratio": 1.7158671586715868, "no_speech_prob": 5.388031422626227e-05}, {"id": 22, "seek": 8704, "start": 101.04, "end": 103.72, "text": " ten thousand PNO tuners in Chicago.", "tokens": [2064, 4714, 430, 45, 46, 4267, 433, 294, 9525, 13], "temperature": 0.0, "avg_logprob": -0.2613107962686507, "compression_ratio": 1.7158671586715868, "no_speech_prob": 5.388031422626227e-05}, {"id": 23, "seek": 8704, "start": 103.72, "end": 105.52000000000001, "text": " Fermi famously asked this kind of question.", "tokens": [43261, 72, 34360, 2351, 341, 733, 295, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2613107962686507, "compression_ratio": 1.7158671586715868, "no_speech_prob": 5.388031422626227e-05}, {"id": 24, "seek": 8704, "start": 105.52000000000001, "end": 108.2, "text": " And there are a lot of estimates like this that we can kind of do in a back of the envelope", "tokens": [400, 456, 366, 257, 688, 295, 20561, 411, 341, 300, 321, 393, 733, 295, 360, 294, 257, 646, 295, 264, 19989], "temperature": 0.0, "avg_logprob": -0.2613107962686507, "compression_ratio": 1.7158671586715868, "no_speech_prob": 5.388031422626227e-05}, {"id": 25, "seek": 8704, "start": 108.2, "end": 114.2, "text": " way to really get a sense for what's going on.", "tokens": [636, 281, 534, 483, 257, 2020, 337, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.2613107962686507, "compression_ratio": 1.7158671586715868, "no_speech_prob": 5.388031422626227e-05}, {"id": 26, "seek": 8704, "start": 114.2, "end": 117.0, "text": " But before going into that, why should you study?", "tokens": [583, 949, 516, 666, 300, 11, 983, 820, 291, 2979, 30], "temperature": 0.0, "avg_logprob": -0.2613107962686507, "compression_ratio": 1.7158671586715868, "no_speech_prob": 5.388031422626227e-05}, {"id": 27, "seek": 11700, "start": 117.0, "end": 118.0, "text": " Study language.", "tokens": [27039, 2856, 13], "temperature": 0.0, "avg_logprob": -0.1962963394496752, "compression_ratio": 1.5720164609053497, "no_speech_prob": 8.214979607146233e-05}, {"id": 28, "seek": 11700, "start": 118.0, "end": 119.0, "text": " This is sort of my motivation.", "tokens": [639, 307, 1333, 295, 452, 12335, 13], "temperature": 0.0, "avg_logprob": -0.1962963394496752, "compression_ratio": 1.5720164609053497, "no_speech_prob": 8.214979607146233e-05}, {"id": 29, "seek": 11700, "start": 119.0, "end": 120.56, "text": " You might have all sorts of other motivations.", "tokens": [509, 1062, 362, 439, 7527, 295, 661, 39034, 13], "temperature": 0.0, "avg_logprob": -0.1962963394496752, "compression_ratio": 1.5720164609053497, "no_speech_prob": 8.214979607146233e-05}, {"id": 30, "seek": 11700, "start": 120.56, "end": 124.96, "text": " Language is obviously very fascinating.", "tokens": [24445, 307, 2745, 588, 10343, 13], "temperature": 0.0, "avg_logprob": -0.1962963394496752, "compression_ratio": 1.5720164609053497, "no_speech_prob": 8.214979607146233e-05}, {"id": 31, "seek": 11700, "start": 124.96, "end": 128.04, "text": " Intellectual creation by our species.", "tokens": [18762, 557, 901, 8016, 538, 527, 6172, 13], "temperature": 0.0, "avg_logprob": -0.1962963394496752, "compression_ratio": 1.5720164609053497, "no_speech_prob": 8.214979607146233e-05}, {"id": 32, "seek": 11700, "start": 128.04, "end": 132.48, "text": " But I think another reason why it's particularly exciting for AI is that language is, in some", "tokens": [583, 286, 519, 1071, 1778, 983, 309, 311, 4098, 4670, 337, 7318, 307, 300, 2856, 307, 11, 294, 512], "temperature": 0.0, "avg_logprob": -0.1962963394496752, "compression_ratio": 1.5720164609053497, "no_speech_prob": 8.214979607146233e-05}, {"id": 33, "seek": 11700, "start": 132.48, "end": 138.44, "text": " sense, our species best attempt to encode everything about the world in as efficient and", "tokens": [2020, 11, 527, 6172, 1151, 5217, 281, 2058, 1429, 1203, 466, 264, 1002, 294, 382, 7148, 293], "temperature": 0.0, "avg_logprob": -0.1962963394496752, "compression_ratio": 1.5720164609053497, "no_speech_prob": 8.214979607146233e-05}, {"id": 34, "seek": 11700, "start": 138.44, "end": 140.28, "text": " compressed way as possible.", "tokens": [30353, 636, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.1962963394496752, "compression_ratio": 1.5720164609053497, "no_speech_prob": 8.214979607146233e-05}, {"id": 35, "seek": 14028, "start": 140.28, "end": 147.28, "text": " And that means that it's very yielding to an AI.", "tokens": [400, 300, 1355, 300, 309, 311, 588, 11257, 278, 281, 364, 7318, 13], "temperature": 0.0, "avg_logprob": -0.20736307416643415, "compression_ratio": 1.725, "no_speech_prob": 2.3903932742541656e-05}, {"id": 36, "seek": 14028, "start": 147.28, "end": 149.28, "text": " There's a lot of noise.", "tokens": [821, 311, 257, 688, 295, 5658, 13], "temperature": 0.0, "avg_logprob": -0.20736307416643415, "compression_ratio": 1.725, "no_speech_prob": 2.3903932742541656e-05}, {"id": 37, "seek": 14028, "start": 149.28, "end": 154.28, "text": " There's a huge quantity of writing freely available on the internet.", "tokens": [821, 311, 257, 2603, 11275, 295, 3579, 16433, 2435, 322, 264, 4705, 13], "temperature": 0.0, "avg_logprob": -0.20736307416643415, "compression_ratio": 1.725, "no_speech_prob": 2.3903932742541656e-05}, {"id": 38, "seek": 14028, "start": 154.28, "end": 159.76, "text": " And there are also a huge number of books, for example, I think very roughly speaking", "tokens": [400, 456, 366, 611, 257, 2603, 1230, 295, 3642, 11, 337, 1365, 11, 286, 519, 588, 9810, 4124], "temperature": 0.0, "avg_logprob": -0.20736307416643415, "compression_ratio": 1.725, "no_speech_prob": 2.3903932742541656e-05}, {"id": 39, "seek": 14028, "start": 159.76, "end": 161.64, "text": " in this sort of Fermi estimate level.", "tokens": [294, 341, 1333, 295, 43261, 72, 12539, 1496, 13], "temperature": 0.0, "avg_logprob": -0.20736307416643415, "compression_ratio": 1.725, "no_speech_prob": 2.3903932742541656e-05}, {"id": 40, "seek": 14028, "start": 161.64, "end": 166.76, "text": " There's something like ten million books in the Library of Congress.", "tokens": [821, 311, 746, 411, 2064, 2459, 3642, 294, 264, 12806, 295, 6426, 13], "temperature": 0.0, "avg_logprob": -0.20736307416643415, "compression_ratio": 1.725, "no_speech_prob": 2.3903932742541656e-05}, {"id": 41, "seek": 14028, "start": 166.76, "end": 170.24, "text": " And very, very roughly that might mean there's something like a trillion words.", "tokens": [400, 588, 11, 588, 9810, 300, 1062, 914, 456, 311, 746, 411, 257, 18723, 2283, 13], "temperature": 0.0, "avg_logprob": -0.20736307416643415, "compression_ratio": 1.725, "no_speech_prob": 2.3903932742541656e-05}, {"id": 42, "seek": 17024, "start": 170.24, "end": 171.24, "text": " There are books.", "tokens": [821, 366, 3642, 13], "temperature": 0.0, "avg_logprob": -0.20252976327572228, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00016590366431046277}, {"id": 43, "seek": 17024, "start": 171.24, "end": 176.36, "text": " And then there's actually much more language information out on the internet.", "tokens": [400, 550, 456, 311, 767, 709, 544, 2856, 1589, 484, 322, 264, 4705, 13], "temperature": 0.0, "avg_logprob": -0.20252976327572228, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00016590366431046277}, {"id": 44, "seek": 17024, "start": 176.36, "end": 181.72, "text": " And so there's therefore a lot of data for AI models to learn from.", "tokens": [400, 370, 456, 311, 4412, 257, 688, 295, 1412, 337, 7318, 5245, 281, 1466, 490, 13], "temperature": 0.0, "avg_logprob": -0.20252976327572228, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00016590366431046277}, {"id": 45, "seek": 17024, "start": 181.72, "end": 185.88, "text": " And then a third reason, at least for, just some extent for me and maybe for many of you,", "tokens": [400, 550, 257, 2636, 1778, 11, 412, 1935, 337, 11, 445, 512, 8396, 337, 385, 293, 1310, 337, 867, 295, 291, 11], "temperature": 0.0, "avg_logprob": -0.20252976327572228, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00016590366431046277}, {"id": 46, "seek": 17024, "start": 185.88, "end": 192.24, "text": " is that if you're actually able to get an AI that kind of knows, quote, unquote, understands", "tokens": [307, 300, 498, 291, 434, 767, 1075, 281, 483, 364, 7318, 300, 733, 295, 3255, 11, 6513, 11, 37557, 11, 15146], "temperature": 0.0, "avg_logprob": -0.20252976327572228, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00016590366431046277}, {"id": 47, "seek": 17024, "start": 192.24, "end": 196.28, "text": " language, then you can communicate with it in a kind of natural way.", "tokens": [2856, 11, 550, 291, 393, 7890, 365, 309, 294, 257, 733, 295, 3303, 636, 13], "temperature": 0.0, "avg_logprob": -0.20252976327572228, "compression_ratio": 1.7107438016528926, "no_speech_prob": 0.00016590366431046277}, {"id": 48, "seek": 19628, "start": 196.28, "end": 201.28, "text": " You can ask it about anything, and you can get a lot of intuition from the responses", "tokens": [509, 393, 1029, 309, 466, 1340, 11, 293, 291, 393, 483, 257, 688, 295, 24002, 490, 264, 13019], "temperature": 0.0, "avg_logprob": -0.18033671597821996, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.608873859979212e-05}, {"id": 49, "seek": 19628, "start": 201.28, "end": 206.52, "text": " and behaviors and all sorts of different kinds of evaluations you can perform on such", "tokens": [293, 15501, 293, 439, 7527, 295, 819, 3685, 295, 43085, 291, 393, 2042, 322, 1270], "temperature": 0.0, "avg_logprob": -0.18033671597821996, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.608873859979212e-05}, {"id": 50, "seek": 19628, "start": 206.52, "end": 208.0, "text": " a model.", "tokens": [257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.18033671597821996, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.608873859979212e-05}, {"id": 51, "seek": 19628, "start": 208.0, "end": 214.28, "text": " If you compare it to sort of ancient history of AI like excitement about classifying images", "tokens": [759, 291, 6794, 309, 281, 1333, 295, 7832, 2503, 295, 7318, 411, 14755, 466, 1508, 5489, 5267], "temperature": 0.0, "avg_logprob": -0.18033671597821996, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.608873859979212e-05}, {"id": 52, "seek": 19628, "start": 214.28, "end": 220.68, "text": " from, I don't know, Alex Net, ten years ago in sort of the distant past.", "tokens": [490, 11, 286, 500, 380, 458, 11, 5202, 6188, 11, 2064, 924, 2057, 294, 1333, 295, 264, 17275, 1791, 13], "temperature": 0.0, "avg_logprob": -0.18033671597821996, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.608873859979212e-05}, {"id": 53, "seek": 19628, "start": 220.68, "end": 224.92000000000002, "text": " And from, say, AlphaGo, again in the distant past five years ago, there's a lot more", "tokens": [400, 490, 11, 584, 11, 20588, 12104, 11, 797, 294, 264, 17275, 1791, 1732, 924, 2057, 11, 456, 311, 257, 688, 544], "temperature": 0.0, "avg_logprob": -0.18033671597821996, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.608873859979212e-05}, {"id": 54, "seek": 22492, "start": 224.92, "end": 226.51999999999998, "text": " intuition you can get.", "tokens": [24002, 291, 393, 483, 13], "temperature": 0.0, "avg_logprob": -0.1592578887939453, "compression_ratio": 1.7387387387387387, "no_speech_prob": 5.7305540394736454e-05}, {"id": 55, "seek": 22492, "start": 226.51999999999998, "end": 231.28, "text": " And then you can use that to sort of understand what these models know and don't know and", "tokens": [400, 550, 291, 393, 764, 300, 281, 1333, 295, 1223, 437, 613, 5245, 458, 293, 500, 380, 458, 293], "temperature": 0.0, "avg_logprob": -0.1592578887939453, "compression_ratio": 1.7387387387387387, "no_speech_prob": 5.7305540394736454e-05}, {"id": 56, "seek": 22492, "start": 231.28, "end": 232.88, "text": " can do.", "tokens": [393, 360, 13], "temperature": 0.0, "avg_logprob": -0.1592578887939453, "compression_ratio": 1.7387387387387387, "no_speech_prob": 5.7305540394736454e-05}, {"id": 57, "seek": 22492, "start": 232.88, "end": 239.83999999999997, "text": " And you can also think about this in terms of how to make these models aligned with what", "tokens": [400, 291, 393, 611, 519, 466, 341, 294, 2115, 295, 577, 281, 652, 613, 5245, 17962, 365, 437], "temperature": 0.0, "avg_logprob": -0.1592578887939453, "compression_ratio": 1.7387387387387387, "no_speech_prob": 5.7305540394736454e-05}, {"id": 58, "seek": 22492, "start": 239.83999999999997, "end": 240.83999999999997, "text": " humans prefer.", "tokens": [6255, 4382, 13], "temperature": 0.0, "avg_logprob": -0.1592578887939453, "compression_ratio": 1.7387387387387387, "no_speech_prob": 5.7305540394736454e-05}, {"id": 59, "seek": 22492, "start": 240.83999999999997, "end": 248.72, "text": " There's a lot of work on trying to understand language model bias, racism, other such issues,", "tokens": [821, 311, 257, 688, 295, 589, 322, 1382, 281, 1223, 2856, 2316, 12577, 11, 12664, 11, 661, 1270, 2663, 11], "temperature": 0.0, "avg_logprob": -0.1592578887939453, "compression_ratio": 1.7387387387387387, "no_speech_prob": 5.7305540394736454e-05}, {"id": 60, "seek": 22492, "start": 248.72, "end": 252.83999999999997, "text": " and there's really a lot that you can kind of explore and dig into.", "tokens": [293, 456, 311, 534, 257, 688, 300, 291, 393, 733, 295, 6839, 293, 2528, 666, 13], "temperature": 0.0, "avg_logprob": -0.1592578887939453, "compression_ratio": 1.7387387387387387, "no_speech_prob": 5.7305540394736454e-05}, {"id": 61, "seek": 25284, "start": 252.84, "end": 258.28000000000003, "text": " So I imagine this is all very basic for everyone here, but just so we're on the same page.", "tokens": [407, 286, 3811, 341, 307, 439, 588, 3875, 337, 1518, 510, 11, 457, 445, 370, 321, 434, 322, 264, 912, 3028, 13], "temperature": 0.0, "avg_logprob": -0.11058747427804129, "compression_ratio": 1.7580645161290323, "no_speech_prob": 5.9187706938246265e-05}, {"id": 62, "seek": 25284, "start": 258.28000000000003, "end": 262.6, "text": " If you're doing kind of contemporary neural network based machine learning, the ingredients", "tokens": [759, 291, 434, 884, 733, 295, 14878, 18161, 3209, 2361, 3479, 2539, 11, 264, 6952], "temperature": 0.0, "avg_logprob": -0.11058747427804129, "compression_ratio": 1.7580645161290323, "no_speech_prob": 5.9187706938246265e-05}, {"id": 63, "seek": 25284, "start": 262.6, "end": 266.16, "text": " that you need to get started are really surprisingly simple.", "tokens": [300, 291, 643, 281, 483, 1409, 366, 534, 17600, 2199, 13], "temperature": 0.0, "avg_logprob": -0.11058747427804129, "compression_ratio": 1.7580645161290323, "no_speech_prob": 5.9187706938246265e-05}, {"id": 64, "seek": 25284, "start": 266.16, "end": 270.0, "text": " You need some kind of model to parameterize a function.", "tokens": [509, 643, 512, 733, 295, 2316, 281, 13075, 1125, 257, 2445, 13], "temperature": 0.0, "avg_logprob": -0.11058747427804129, "compression_ratio": 1.7580645161290323, "no_speech_prob": 5.9187706938246265e-05}, {"id": 65, "seek": 25284, "start": 270.0, "end": 272.68, "text": " You need a data set.", "tokens": [509, 643, 257, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.11058747427804129, "compression_ratio": 1.7580645161290323, "no_speech_prob": 5.9187706938246265e-05}, {"id": 66, "seek": 25284, "start": 272.68, "end": 276.72, "text": " You need some computers with plenty of computation.", "tokens": [509, 643, 512, 10807, 365, 7140, 295, 24903, 13], "temperature": 0.0, "avg_logprob": -0.11058747427804129, "compression_ratio": 1.7580645161290323, "no_speech_prob": 5.9187706938246265e-05}, {"id": 67, "seek": 25284, "start": 276.72, "end": 281.36, "text": " You need a lost function and you need some choice of optimizer.", "tokens": [509, 643, 257, 2731, 2445, 293, 291, 643, 512, 3922, 295, 5028, 6545, 13], "temperature": 0.0, "avg_logprob": -0.11058747427804129, "compression_ratio": 1.7580645161290323, "no_speech_prob": 5.9187706938246265e-05}, {"id": 68, "seek": 28136, "start": 281.36, "end": 287.0, "text": " And basically for pretty much everything in this talk, I'll be thinking about language", "tokens": [400, 1936, 337, 1238, 709, 1203, 294, 341, 751, 11, 286, 603, 312, 1953, 466, 2856], "temperature": 0.0, "avg_logprob": -0.14334964752197266, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.3063373343320563e-05}, {"id": 69, "seek": 28136, "start": 287.0, "end": 294.12, "text": " modeling as a task where the lost function is simply to predict the next word in some", "tokens": [15983, 382, 257, 5633, 689, 264, 2731, 2445, 307, 2935, 281, 6069, 264, 958, 1349, 294, 512], "temperature": 0.0, "avg_logprob": -0.14334964752197266, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.3063373343320563e-05}, {"id": 70, "seek": 28136, "start": 294.12, "end": 296.84000000000003, "text": " sentence or paragraph or book.", "tokens": [8174, 420, 18865, 420, 1446, 13], "temperature": 0.0, "avg_logprob": -0.14334964752197266, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.3063373343320563e-05}, {"id": 71, "seek": 28136, "start": 296.84000000000003, "end": 300.96000000000004, "text": " And so that's how basically all of the models that I'll be talking about are trained.", "tokens": [400, 370, 300, 311, 577, 1936, 439, 295, 264, 5245, 300, 286, 603, 312, 1417, 466, 366, 8895, 13], "temperature": 0.0, "avg_logprob": -0.14334964752197266, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.3063373343320563e-05}, {"id": 72, "seek": 28136, "start": 300.96000000000004, "end": 305.76, "text": " They have a lost function which incentivizes them to predict the correct probability", "tokens": [814, 362, 257, 2731, 2445, 597, 35328, 5660, 552, 281, 6069, 264, 3006, 8482], "temperature": 0.0, "avg_logprob": -0.14334964752197266, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.3063373343320563e-05}, {"id": 73, "seek": 28136, "start": 305.76, "end": 308.68, "text": " distribution for the next word.", "tokens": [7316, 337, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14334964752197266, "compression_ratio": 1.7058823529411764, "no_speech_prob": 5.3063373343320563e-05}, {"id": 74, "seek": 30868, "start": 308.68, "end": 313.44, "text": " So what about these other ingredients like the models that we use, the data sets that", "tokens": [407, 437, 466, 613, 661, 6952, 411, 264, 5245, 300, 321, 764, 11, 264, 1412, 6352, 300], "temperature": 0.0, "avg_logprob": -0.16058624551651327, "compression_ratio": 1.6531531531531531, "no_speech_prob": 3.0230403353925794e-05}, {"id": 75, "seek": 30868, "start": 313.44, "end": 317.12, "text": " we use, and how much computation do we use?", "tokens": [321, 764, 11, 293, 577, 709, 24903, 360, 321, 764, 30], "temperature": 0.0, "avg_logprob": -0.16058624551651327, "compression_ratio": 1.6531531531531531, "no_speech_prob": 3.0230403353925794e-05}, {"id": 76, "seek": 30868, "start": 317.12, "end": 322.28000000000003, "text": " What are those sort of order of magnitude figures?", "tokens": [708, 366, 729, 1333, 295, 1668, 295, 15668, 9624, 30], "temperature": 0.0, "avg_logprob": -0.16058624551651327, "compression_ratio": 1.6531531531531531, "no_speech_prob": 3.0230403353925794e-05}, {"id": 77, "seek": 30868, "start": 322.28000000000003, "end": 328.04, "text": " So one way to think about this is sort of how much language do we consume as a person", "tokens": [407, 472, 636, 281, 519, 466, 341, 307, 1333, 295, 577, 709, 2856, 360, 321, 14732, 382, 257, 954], "temperature": 0.0, "avg_logprob": -0.16058624551651327, "compression_ratio": 1.6531531531531531, "no_speech_prob": 3.0230403353925794e-05}, {"id": 78, "seek": 30868, "start": 328.04, "end": 329.84000000000003, "text": " for comparison.", "tokens": [337, 9660, 13], "temperature": 0.0, "avg_logprob": -0.16058624551651327, "compression_ratio": 1.6531531531531531, "no_speech_prob": 3.0230403353925794e-05}, {"id": 79, "seek": 30868, "start": 329.84000000000003, "end": 335.16, "text": " So you can imagine that if you were a very voracious reader, maybe you'd read a long", "tokens": [407, 291, 393, 3811, 300, 498, 291, 645, 257, 588, 4245, 22641, 15149, 11, 1310, 291, 1116, 1401, 257, 938], "temperature": 0.0, "avg_logprob": -0.16058624551651327, "compression_ratio": 1.6531531531531531, "no_speech_prob": 3.0230403353925794e-05}, {"id": 80, "seek": 33516, "start": 335.16, "end": 341.44, "text": " book every day and you'd spend your life doing that, maybe you'd live for 70 years, if", "tokens": [1446, 633, 786, 293, 291, 1116, 3496, 428, 993, 884, 300, 11, 1310, 291, 1116, 1621, 337, 5285, 924, 11, 498], "temperature": 0.0, "avg_logprob": -0.2115576926698076, "compression_ratio": 1.6428571428571428, "no_speech_prob": 6.400156416930258e-05}, {"id": 81, "seek": 33516, "start": 341.44, "end": 348.20000000000005, "text": " you did that, you'd end up reading something like two billion words over your lifetime.", "tokens": [291, 630, 300, 11, 291, 1116, 917, 493, 3760, 746, 411, 732, 5218, 2283, 670, 428, 11364, 13], "temperature": 0.0, "avg_logprob": -0.2115576926698076, "compression_ratio": 1.6428571428571428, "no_speech_prob": 6.400156416930258e-05}, {"id": 82, "seek": 33516, "start": 348.20000000000005, "end": 356.64000000000004, "text": " For comparison, a canonical large language model, GPT-3, was trained for on the order of", "tokens": [1171, 9660, 11, 257, 46491, 2416, 2856, 2316, 11, 26039, 51, 12, 18, 11, 390, 8895, 337, 322, 264, 1668, 295], "temperature": 0.0, "avg_logprob": -0.2115576926698076, "compression_ratio": 1.6428571428571428, "no_speech_prob": 6.400156416930258e-05}, {"id": 83, "seek": 33516, "start": 356.64000000000004, "end": 358.44000000000005, "text": " 200 billion words.", "tokens": [2331, 5218, 2283, 13], "temperature": 0.0, "avg_logprob": -0.2115576926698076, "compression_ratio": 1.6428571428571428, "no_speech_prob": 6.400156416930258e-05}, {"id": 84, "seek": 33516, "start": 358.44000000000005, "end": 364.84000000000003, "text": " So that's about 100 times more language data than maybe you'd see in your lifetime if", "tokens": [407, 300, 311, 466, 2319, 1413, 544, 2856, 1412, 813, 1310, 291, 1116, 536, 294, 428, 11364, 498], "temperature": 0.0, "avg_logprob": -0.2115576926698076, "compression_ratio": 1.6428571428571428, "no_speech_prob": 6.400156416930258e-05}, {"id": 85, "seek": 36484, "start": 364.84, "end": 369.96, "text": " you kind of tried really hard to attend to written text.", "tokens": [291, 733, 295, 3031, 534, 1152, 281, 6888, 281, 3720, 2487, 13], "temperature": 0.0, "avg_logprob": -0.24571588304307726, "compression_ratio": 1.4953703703703705, "no_speech_prob": 2.3181077267508954e-05}, {"id": 86, "seek": 36484, "start": 369.96, "end": 375.35999999999996, "text": " There are other data sets, of course, that are much, much bigger than GPT-3's trading", "tokens": [821, 366, 661, 1412, 6352, 11, 295, 1164, 11, 300, 366, 709, 11, 709, 3801, 813, 26039, 51, 12, 18, 311, 9529], "temperature": 0.0, "avg_logprob": -0.24571588304307726, "compression_ratio": 1.4953703703703705, "no_speech_prob": 2.3181077267508954e-05}, {"id": 87, "seek": 36484, "start": 375.35999999999996, "end": 377.15999999999997, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.24571588304307726, "compression_ratio": 1.4953703703703705, "no_speech_prob": 2.3181077267508954e-05}, {"id": 88, "seek": 36484, "start": 377.15999999999997, "end": 382.15999999999997, "text": " The year's common crawl, which is a sort of snapshot of the internet that anyone can", "tokens": [440, 1064, 311, 2689, 24767, 11, 597, 307, 257, 1333, 295, 30163, 295, 264, 4705, 300, 2878, 393], "temperature": 0.0, "avg_logprob": -0.24571588304307726, "compression_ratio": 1.4953703703703705, "no_speech_prob": 2.3181077267508954e-05}, {"id": 89, "seek": 36484, "start": 382.15999999999997, "end": 388.64, "text": " go out and download if you like, this has very roughly on the order of 10 to the 15", "tokens": [352, 484, 293, 5484, 498, 291, 411, 11, 341, 575, 588, 9810, 322, 264, 1668, 295, 1266, 281, 264, 2119], "temperature": 0.0, "avg_logprob": -0.24571588304307726, "compression_ratio": 1.4953703703703705, "no_speech_prob": 2.3181077267508954e-05}, {"id": 90, "seek": 36484, "start": 388.64, "end": 391.32, "text": " words.", "tokens": [2283, 13], "temperature": 0.0, "avg_logprob": -0.24571588304307726, "compression_ratio": 1.4953703703703705, "no_speech_prob": 2.3181077267508954e-05}, {"id": 91, "seek": 39132, "start": 391.32, "end": 397.68, "text": " I said earlier that the Library of Congress has something like maybe 10 million books,", "tokens": [286, 848, 3071, 300, 264, 12806, 295, 6426, 575, 746, 411, 1310, 1266, 2459, 3642, 11], "temperature": 0.0, "avg_logprob": -0.18558836496004494, "compression_ratio": 1.7241379310344827, "no_speech_prob": 2.353343370486982e-05}, {"id": 92, "seek": 39132, "start": 397.68, "end": 399.92, "text": " each book is maybe 100,000 words.", "tokens": [1184, 1446, 307, 1310, 2319, 11, 1360, 2283, 13], "temperature": 0.0, "avg_logprob": -0.18558836496004494, "compression_ratio": 1.7241379310344827, "no_speech_prob": 2.353343370486982e-05}, {"id": 93, "seek": 39132, "start": 399.92, "end": 405.64, "text": " So the Library of Congress in total maybe has something like a trillion words.", "tokens": [407, 264, 12806, 295, 6426, 294, 3217, 1310, 575, 746, 411, 257, 18723, 2283, 13], "temperature": 0.0, "avg_logprob": -0.18558836496004494, "compression_ratio": 1.7241379310344827, "no_speech_prob": 2.353343370486982e-05}, {"id": 94, "seek": 39132, "start": 405.64, "end": 412.44, "text": " And as another sort of smaller data set example, English Wikipedia is very roughly of", "tokens": [400, 382, 1071, 1333, 295, 4356, 1412, 992, 1365, 11, 3669, 28999, 307, 588, 9810, 295], "temperature": 0.0, "avg_logprob": -0.18558836496004494, "compression_ratio": 1.7241379310344827, "no_speech_prob": 2.353343370486982e-05}, {"id": 95, "seek": 39132, "start": 412.44, "end": 414.6, "text": " order three billion words.", "tokens": [1668, 1045, 5218, 2283, 13], "temperature": 0.0, "avg_logprob": -0.18558836496004494, "compression_ratio": 1.7241379310344827, "no_speech_prob": 2.353343370486982e-05}, {"id": 96, "seek": 39132, "start": 414.6, "end": 419.64, "text": " So maybe if you spent your whole life reading Wikipedia, you could just barely do it if", "tokens": [407, 1310, 498, 291, 4418, 428, 1379, 993, 3760, 28999, 11, 291, 727, 445, 10268, 360, 309, 498], "temperature": 0.0, "avg_logprob": -0.18558836496004494, "compression_ratio": 1.7241379310344827, "no_speech_prob": 2.353343370486982e-05}, {"id": 97, "seek": 41964, "start": 419.64, "end": 423.32, "text": " that was your mission.", "tokens": [300, 390, 428, 4447, 13], "temperature": 0.0, "avg_logprob": -0.17937335771383697, "compression_ratio": 1.6939655172413792, "no_speech_prob": 3.942117473343387e-05}, {"id": 98, "seek": 41964, "start": 423.32, "end": 429.15999999999997, "text": " So what about the actual neural networks that I'll be talking about that we currently", "tokens": [407, 437, 466, 264, 3539, 18161, 9590, 300, 286, 603, 312, 1417, 466, 300, 321, 4362], "temperature": 0.0, "avg_logprob": -0.17937335771383697, "compression_ratio": 1.6939655172413792, "no_speech_prob": 3.942117473343387e-05}, {"id": 99, "seek": 41964, "start": 429.15999999999997, "end": 433.71999999999997, "text": " seem to be using fairly effectively to model language?", "tokens": [1643, 281, 312, 1228, 6457, 8659, 281, 2316, 2856, 30], "temperature": 0.0, "avg_logprob": -0.17937335771383697, "compression_ratio": 1.6939655172413792, "no_speech_prob": 3.942117473343387e-05}, {"id": 100, "seek": 41964, "start": 433.71999999999997, "end": 439.24, "text": " So I'll be talking about transformer language models, so-called decoder-only transformer", "tokens": [407, 286, 603, 312, 1417, 466, 31782, 2856, 5245, 11, 370, 12, 11880, 979, 19866, 12, 25202, 31782], "temperature": 0.0, "avg_logprob": -0.17937335771383697, "compression_ratio": 1.6939655172413792, "no_speech_prob": 3.942117473343387e-05}, {"id": 101, "seek": 41964, "start": 439.24, "end": 443.36, "text": " language models of which GPT-3 is an example.", "tokens": [2856, 5245, 295, 597, 26039, 51, 12, 18, 307, 364, 1365, 13], "temperature": 0.0, "avg_logprob": -0.17937335771383697, "compression_ratio": 1.6939655172413792, "no_speech_prob": 3.942117473343387e-05}, {"id": 102, "seek": 41964, "start": 443.36, "end": 449.24, "text": " And just to sort of pout things, these models have, with kind of the standard way that they're", "tokens": [400, 445, 281, 1333, 295, 280, 346, 721, 11, 613, 5245, 362, 11, 365, 733, 295, 264, 3832, 636, 300, 436, 434], "temperature": 0.0, "avg_logprob": -0.17937335771383697, "compression_ratio": 1.6939655172413792, "no_speech_prob": 3.942117473343387e-05}, {"id": 103, "seek": 44924, "start": 449.24, "end": 456.04, "text": " set up, a number of parameters, which is something like 12 times the number of layers in the", "tokens": [992, 493, 11, 257, 1230, 295, 9834, 11, 597, 307, 746, 411, 2272, 1413, 264, 1230, 295, 7914, 294, 264], "temperature": 0.0, "avg_logprob": -0.16171468832553962, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.9521192371030338e-05}, {"id": 104, "seek": 44924, "start": 456.04, "end": 457.04, "text": " network.", "tokens": [3209, 13], "temperature": 0.0, "avg_logprob": -0.16171468832553962, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.9521192371030338e-05}, {"id": 105, "seek": 44924, "start": 457.04, "end": 458.88, "text": " So GPT-3 has 96 layers.", "tokens": [407, 26039, 51, 12, 18, 575, 24124, 7914, 13], "temperature": 0.0, "avg_logprob": -0.16171468832553962, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.9521192371030338e-05}, {"id": 106, "seek": 44924, "start": 458.88, "end": 466.88, "text": " You can make deeper or shallower such networks times the sort of activation dimension squared.", "tokens": [509, 393, 652, 7731, 420, 4393, 968, 1270, 9590, 1413, 264, 1333, 295, 24433, 10139, 8889, 13], "temperature": 0.0, "avg_logprob": -0.16171468832553962, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.9521192371030338e-05}, {"id": 107, "seek": 44924, "start": 466.88, "end": 475.04, "text": " So D model, this D model parameter is just the dimension of the vector space that each", "tokens": [407, 413, 2316, 11, 341, 413, 2316, 13075, 307, 445, 264, 10139, 295, 264, 8062, 1901, 300, 1184], "temperature": 0.0, "avg_logprob": -0.16171468832553962, "compression_ratio": 1.566326530612245, "no_speech_prob": 1.9521192371030338e-05}, {"id": 108, "seek": 47504, "start": 475.04, "end": 483.40000000000003, "text": " token occupies or word, if you were to use words as tokens, when you run this model on", "tokens": [14862, 8073, 530, 420, 1349, 11, 498, 291, 645, 281, 764, 2283, 382, 22667, 11, 562, 291, 1190, 341, 2316, 322], "temperature": 0.0, "avg_logprob": -0.21829140323331986, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.372954961378127e-05}, {"id": 109, "seek": 47504, "start": 483.40000000000003, "end": 485.0, "text": " language data.", "tokens": [2856, 1412, 13], "temperature": 0.0, "avg_logprob": -0.21829140323331986, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.372954961378127e-05}, {"id": 110, "seek": 47504, "start": 485.0, "end": 489.84000000000003, "text": " And so this gives you some sense for where parameter comes from.", "tokens": [400, 370, 341, 2709, 291, 512, 2020, 337, 689, 13075, 1487, 490, 13], "temperature": 0.0, "avg_logprob": -0.21829140323331986, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.372954961378127e-05}, {"id": 111, "seek": 47504, "start": 489.84000000000003, "end": 496.04, "text": " I think D model for GPT-3 is of order 10,000 and layer is 96 and that's how you get roughly", "tokens": [286, 519, 413, 2316, 337, 26039, 51, 12, 18, 307, 295, 1668, 1266, 11, 1360, 293, 4583, 307, 24124, 293, 300, 311, 577, 291, 483, 9810], "temperature": 0.0, "avg_logprob": -0.21829140323331986, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.372954961378127e-05}, {"id": 112, "seek": 47504, "start": 496.04, "end": 502.16, "text": " 200 billion parameters in that model and other models scale similarly.", "tokens": [2331, 5218, 9834, 294, 300, 2316, 293, 661, 5245, 4373, 14138, 13], "temperature": 0.0, "avg_logprob": -0.21829140323331986, "compression_ratio": 1.5022831050228311, "no_speech_prob": 3.372954961378127e-05}, {"id": 113, "seek": 50216, "start": 502.16, "end": 509.0, "text": " Now, how much computation do you actually do when you train this kind of model?", "tokens": [823, 11, 577, 709, 24903, 360, 291, 767, 360, 562, 291, 3847, 341, 733, 295, 2316, 30], "temperature": 0.0, "avg_logprob": -0.2214960855980442, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.6700401829439215e-05}, {"id": 114, "seek": 50216, "start": 509.0, "end": 514.28, "text": " Well it turns out that different neural network architectures have different properties with", "tokens": [1042, 309, 4523, 484, 300, 819, 18161, 3209, 6331, 1303, 362, 819, 7221, 365], "temperature": 0.0, "avg_logprob": -0.2214960855980442, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.6700401829439215e-05}, {"id": 115, "seek": 50216, "start": 514.28, "end": 520.12, "text": " effect this question, but transformers are actually quite simple in that in a forward", "tokens": [1802, 341, 1168, 11, 457, 4088, 433, 366, 767, 1596, 2199, 294, 300, 294, 257, 2128], "temperature": 0.0, "avg_logprob": -0.2214960855980442, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.6700401829439215e-05}, {"id": 116, "seek": 50216, "start": 520.12, "end": 528.08, "text": " pass of a transformer, every parameter on every token performs roughly one add and one", "tokens": [1320, 295, 257, 31782, 11, 633, 13075, 322, 633, 14862, 26213, 9810, 472, 909, 293, 472], "temperature": 0.0, "avg_logprob": -0.2214960855980442, "compression_ratio": 1.619718309859155, "no_speech_prob": 1.6700401829439215e-05}, {"id": 117, "seek": 52808, "start": 528.08, "end": 533.2, "text": " multiply and then about twice this in the backward pass.", "tokens": [12972, 293, 550, 466, 6091, 341, 294, 264, 23897, 1320, 13], "temperature": 0.0, "avg_logprob": -0.15865318591778094, "compression_ratio": 1.6968325791855203, "no_speech_prob": 3.8827754906378686e-05}, {"id": 118, "seek": 52808, "start": 533.2, "end": 537.64, "text": " And so that gives us a very simple formula that the number of floating point operations", "tokens": [400, 370, 300, 2709, 505, 257, 588, 2199, 8513, 300, 264, 1230, 295, 12607, 935, 7705], "temperature": 0.0, "avg_logprob": -0.15865318591778094, "compression_ratio": 1.6968325791855203, "no_speech_prob": 3.8827754906378686e-05}, {"id": 119, "seek": 52808, "start": 537.64, "end": 546.2, "text": " that a model like this performs during training is 6, which is 2 times 1 plus 2, times the", "tokens": [300, 257, 2316, 411, 341, 26213, 1830, 3097, 307, 1386, 11, 597, 307, 568, 1413, 502, 1804, 568, 11, 1413, 264], "temperature": 0.0, "avg_logprob": -0.15865318591778094, "compression_ratio": 1.6968325791855203, "no_speech_prob": 3.8827754906378686e-05}, {"id": 120, "seek": 52808, "start": 546.2, "end": 551.0400000000001, "text": " number of parameters in the model times the number of tokens, that's what D is sort of the", "tokens": [1230, 295, 9834, 294, 264, 2316, 1413, 264, 1230, 295, 22667, 11, 300, 311, 437, 413, 307, 1333, 295, 264], "temperature": 0.0, "avg_logprob": -0.15865318591778094, "compression_ratio": 1.6968325791855203, "no_speech_prob": 3.8827754906378686e-05}, {"id": 121, "seek": 52808, "start": 551.0400000000001, "end": 555.2, "text": " size of the data set in tokens that you process.", "tokens": [2744, 295, 264, 1412, 992, 294, 22667, 300, 291, 1399, 13], "temperature": 0.0, "avg_logprob": -0.15865318591778094, "compression_ratio": 1.6968325791855203, "no_speech_prob": 3.8827754906378686e-05}, {"id": 122, "seek": 55520, "start": 555.2, "end": 562.12, "text": " And one other point that sort of I'll make while kind of going over these estimates is", "tokens": [400, 472, 661, 935, 300, 1333, 295, 286, 603, 652, 1339, 733, 295, 516, 670, 613, 20561, 307], "temperature": 0.0, "avg_logprob": -0.16272006536784925, "compression_ratio": 1.5674418604651164, "no_speech_prob": 8.010933925106656e-06}, {"id": 123, "seek": 55520, "start": 562.12, "end": 567.9200000000001, "text": " that you might wonder whether or not there's a lot of computation involved in processing", "tokens": [300, 291, 1062, 2441, 1968, 420, 406, 456, 311, 257, 688, 295, 24903, 3288, 294, 9007], "temperature": 0.0, "avg_logprob": -0.16272006536784925, "compression_ratio": 1.5674418604651164, "no_speech_prob": 8.010933925106656e-06}, {"id": 124, "seek": 55520, "start": 567.9200000000001, "end": 569.48, "text": " long sequences.", "tokens": [938, 22978, 13], "temperature": 0.0, "avg_logprob": -0.16272006536784925, "compression_ratio": 1.5674418604651164, "no_speech_prob": 8.010933925106656e-06}, {"id": 125, "seek": 55520, "start": 569.48, "end": 578.2800000000001, "text": " There's sort of a famous point that dense attention in transformer models is n squared", "tokens": [821, 311, 1333, 295, 257, 4618, 935, 300, 18011, 3202, 294, 31782, 5245, 307, 297, 8889], "temperature": 0.0, "avg_logprob": -0.16272006536784925, "compression_ratio": 1.5674418604651164, "no_speech_prob": 8.010933925106656e-06}, {"id": 126, "seek": 55520, "start": 578.2800000000001, "end": 581.24, "text": " with respect to context length and that's absolutely true.", "tokens": [365, 3104, 281, 4319, 4641, 293, 300, 311, 3122, 2074, 13], "temperature": 0.0, "avg_logprob": -0.16272006536784925, "compression_ratio": 1.5674418604651164, "no_speech_prob": 8.010933925106656e-06}, {"id": 127, "seek": 58124, "start": 581.24, "end": 587.32, "text": " However, if you actually work out the sort of coefficients, the ratio of the amount of", "tokens": [2908, 11, 498, 291, 767, 589, 484, 264, 1333, 295, 31994, 11, 264, 8509, 295, 264, 2372, 295], "temperature": 0.0, "avg_logprob": -0.12601336565884677, "compression_ratio": 1.6371681415929205, "no_speech_prob": 3.424556052777916e-05}, {"id": 128, "seek": 58124, "start": 587.32, "end": 594.0, "text": " computation you do in a forward pass or during training in the context direction versus", "tokens": [24903, 291, 360, 294, 257, 2128, 1320, 420, 1830, 3097, 294, 264, 4319, 3513, 5717], "temperature": 0.0, "avg_logprob": -0.12601336565884677, "compression_ratio": 1.6371681415929205, "no_speech_prob": 3.424556052777916e-05}, {"id": 129, "seek": 58124, "start": 594.0, "end": 600.5600000000001, "text": " in the direction of sort of moving up the layers of the model is roughly n context over", "tokens": [294, 264, 3513, 295, 1333, 295, 2684, 493, 264, 7914, 295, 264, 2316, 307, 9810, 297, 4319, 670], "temperature": 0.0, "avg_logprob": -0.12601336565884677, "compression_ratio": 1.6371681415929205, "no_speech_prob": 3.424556052777916e-05}, {"id": 130, "seek": 58124, "start": 600.5600000000001, "end": 602.5600000000001, "text": " 12 times D model.", "tokens": [2272, 1413, 413, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12601336565884677, "compression_ratio": 1.6371681415929205, "no_speech_prob": 3.424556052777916e-05}, {"id": 131, "seek": 58124, "start": 602.5600000000001, "end": 610.2, "text": " So I note this just because if you think which I'll kind of suggest that this is a likely", "tokens": [407, 286, 3637, 341, 445, 570, 498, 291, 519, 597, 286, 603, 733, 295, 3402, 300, 341, 307, 257, 3700], "temperature": 0.0, "avg_logprob": -0.12601336565884677, "compression_ratio": 1.6371681415929205, "no_speech_prob": 3.424556052777916e-05}, {"id": 132, "seek": 61020, "start": 610.2, "end": 616.12, "text": " direction for the world to be heading, that models might continue to get bigger, then", "tokens": [3513, 337, 264, 1002, 281, 312, 9864, 11, 300, 5245, 1062, 2354, 281, 483, 3801, 11, 550], "temperature": 0.0, "avg_logprob": -0.16946463532500214, "compression_ratio": 1.5733333333333333, "no_speech_prob": 3.0232075005187653e-05}, {"id": 133, "seek": 61020, "start": 616.12, "end": 618.9200000000001, "text": " D model for GPT-3 is already 10,000.", "tokens": [413, 2316, 337, 26039, 51, 12, 18, 307, 1217, 1266, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.16946463532500214, "compression_ratio": 1.5733333333333333, "no_speech_prob": 3.0232075005187653e-05}, {"id": 134, "seek": 61020, "start": 618.9200000000001, "end": 621.8000000000001, "text": " So the denominator here is order 100,000.", "tokens": [407, 264, 20687, 510, 307, 1668, 2319, 11, 1360, 13], "temperature": 0.0, "avg_logprob": -0.16946463532500214, "compression_ratio": 1.5733333333333333, "no_speech_prob": 3.0232075005187653e-05}, {"id": 135, "seek": 61020, "start": 621.8000000000001, "end": 625.9200000000001, "text": " And so actually even if you have quite long contexts with the sort of dumbest possible", "tokens": [400, 370, 767, 754, 498, 291, 362, 1596, 938, 30628, 365, 264, 1333, 295, 10316, 377, 1944], "temperature": 0.0, "avg_logprob": -0.16946463532500214, "compression_ratio": 1.5733333333333333, "no_speech_prob": 3.0232075005187653e-05}, {"id": 136, "seek": 61020, "start": 625.9200000000001, "end": 630.8000000000001, "text": " dense attention, the amount of compute you actually do in the context direction is not", "tokens": [18011, 3202, 11, 264, 2372, 295, 14722, 291, 767, 360, 294, 264, 4319, 3513, 307, 406], "temperature": 0.0, "avg_logprob": -0.16946463532500214, "compression_ratio": 1.5733333333333333, "no_speech_prob": 3.0232075005187653e-05}, {"id": 137, "seek": 61020, "start": 630.8000000000001, "end": 635.8000000000001, "text": " always so much.", "tokens": [1009, 370, 709, 13], "temperature": 0.0, "avg_logprob": -0.16946463532500214, "compression_ratio": 1.5733333333333333, "no_speech_prob": 3.0232075005187653e-05}, {"id": 138, "seek": 63580, "start": 635.8, "end": 640.4799999999999, "text": " What about actually numerical values for this compute?", "tokens": [708, 466, 767, 29054, 4190, 337, 341, 14722, 30], "temperature": 0.0, "avg_logprob": -0.20945440862596648, "compression_ratio": 1.5887445887445888, "no_speech_prob": 4.7567213186994195e-05}, {"id": 139, "seek": 63580, "start": 640.4799999999999, "end": 644.8399999999999, "text": " So the largest models that we have so far, if we're in kind of Fermi estimate mode, we", "tokens": [407, 264, 6443, 5245, 300, 321, 362, 370, 1400, 11, 498, 321, 434, 294, 733, 295, 43261, 72, 12539, 4391, 11, 321], "temperature": 0.0, "avg_logprob": -0.20945440862596648, "compression_ratio": 1.5887445887445888, "no_speech_prob": 4.7567213186994195e-05}, {"id": 140, "seek": 63580, "start": 644.8399999999999, "end": 648.24, "text": " can round up and say they have say order a trillion parameters.", "tokens": [393, 3098, 493, 293, 584, 436, 362, 584, 1668, 257, 18723, 9834, 13], "temperature": 0.0, "avg_logprob": -0.20945440862596648, "compression_ratio": 1.5887445887445888, "no_speech_prob": 4.7567213186994195e-05}, {"id": 141, "seek": 63580, "start": 648.24, "end": 655.7199999999999, "text": " If you have a model with a trillion parameters, then what kind of hardware are you going", "tokens": [759, 291, 362, 257, 2316, 365, 257, 18723, 9834, 11, 550, 437, 733, 295, 8837, 366, 291, 516], "temperature": 0.0, "avg_logprob": -0.20945440862596648, "compression_ratio": 1.5887445887445888, "no_speech_prob": 4.7567213186994195e-05}, {"id": 142, "seek": 63580, "start": 655.7199999999999, "end": 656.7199999999999, "text": " to run it on?", "tokens": [281, 1190, 309, 322, 30], "temperature": 0.0, "avg_logprob": -0.20945440862596648, "compression_ratio": 1.5887445887445888, "no_speech_prob": 4.7567213186994195e-05}, {"id": 143, "seek": 63580, "start": 656.7199999999999, "end": 660.64, "text": " Well, you might run it on in a 100 GPU at least this year.", "tokens": [1042, 11, 291, 1062, 1190, 309, 322, 294, 257, 2319, 18407, 412, 1935, 341, 1064, 13], "temperature": 0.0, "avg_logprob": -0.20945440862596648, "compression_ratio": 1.5887445887445888, "no_speech_prob": 4.7567213186994195e-05}, {"id": 144, "seek": 66064, "start": 660.64, "end": 667.8, "text": " And a 100 GPU is performed about 3 times 10 to the 14, floating point operations per second,", "tokens": [400, 257, 2319, 18407, 307, 10332, 466, 805, 1413, 1266, 281, 264, 3499, 11, 12607, 935, 7705, 680, 1150, 11], "temperature": 0.0, "avg_logprob": -0.18478381115457285, "compression_ratio": 1.8810810810810812, "no_speech_prob": 2.9308006560313515e-05}, {"id": 145, "seek": 66064, "start": 667.8, "end": 672.88, "text": " or 2 times 10 to the 19, floating point operations per day.", "tokens": [420, 568, 1413, 1266, 281, 264, 1294, 11, 12607, 935, 7705, 680, 786, 13], "temperature": 0.0, "avg_logprob": -0.18478381115457285, "compression_ratio": 1.8810810810810812, "no_speech_prob": 2.9308006560313515e-05}, {"id": 146, "seek": 66064, "start": 672.88, "end": 677.6, "text": " This means that it's sort of convenient to sometimes use units of pedoflap days, which", "tokens": [639, 1355, 300, 309, 311, 1333, 295, 10851, 281, 2171, 764, 6815, 295, 5670, 2670, 75, 569, 1708, 11, 597], "temperature": 0.0, "avg_logprob": -0.18478381115457285, "compression_ratio": 1.8810810810810812, "no_speech_prob": 2.9308006560313515e-05}, {"id": 147, "seek": 66064, "start": 677.6, "end": 682.24, "text": " is 10 to the 15, floating point operations per second times a day.", "tokens": [307, 1266, 281, 264, 2119, 11, 12607, 935, 7705, 680, 1150, 1413, 257, 786, 13], "temperature": 0.0, "avg_logprob": -0.18478381115457285, "compression_ratio": 1.8810810810810812, "no_speech_prob": 2.9308006560313515e-05}, {"id": 148, "seek": 66064, "start": 682.24, "end": 686.16, "text": " And that means that's about 3 a 100 days.", "tokens": [400, 300, 1355, 300, 311, 466, 805, 257, 2319, 1708, 13], "temperature": 0.0, "avg_logprob": -0.18478381115457285, "compression_ratio": 1.8810810810810812, "no_speech_prob": 2.9308006560313515e-05}, {"id": 149, "seek": 68616, "start": 686.16, "end": 692.88, "text": " And that's about 8.6 times 10 to the 19 or order 10 to the 20 floating point operations", "tokens": [400, 300, 311, 466, 1649, 13, 21, 1413, 1266, 281, 264, 1294, 420, 1668, 1266, 281, 264, 945, 12607, 935, 7705], "temperature": 0.0, "avg_logprob": -0.166712703704834, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.765277587168384e-06}, {"id": 150, "seek": 68616, "start": 692.88, "end": 694.76, "text": " in a day.", "tokens": [294, 257, 786, 13], "temperature": 0.0, "avg_logprob": -0.166712703704834, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.765277587168384e-06}, {"id": 151, "seek": 68616, "start": 694.76, "end": 700.12, "text": " So how does sort of the compute available on hardware compare to the compute that we", "tokens": [407, 577, 775, 1333, 295, 264, 14722, 2435, 322, 8837, 6794, 281, 264, 14722, 300, 321], "temperature": 0.0, "avg_logprob": -0.166712703704834, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.765277587168384e-06}, {"id": 152, "seek": 68616, "start": 700.12, "end": 702.92, "text": " do when we train these gigantic models?", "tokens": [360, 562, 321, 3847, 613, 26800, 5245, 30], "temperature": 0.0, "avg_logprob": -0.166712703704834, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.765277587168384e-06}, {"id": 153, "seek": 68616, "start": 702.92, "end": 710.0, "text": " Well, if we have a model with a trillion parameters and we train it for 300 billion tokens,", "tokens": [1042, 11, 498, 321, 362, 257, 2316, 365, 257, 18723, 9834, 293, 321, 3847, 309, 337, 6641, 5218, 22667, 11], "temperature": 0.0, "avg_logprob": -0.166712703704834, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.765277587168384e-06}, {"id": 154, "seek": 68616, "start": 710.0, "end": 715.3199999999999, "text": " then we get 6 times 10 to the 12 times 3 times 10 to the 11.", "tokens": [550, 321, 483, 1386, 1413, 1266, 281, 264, 2272, 1413, 805, 1413, 1266, 281, 264, 2975, 13], "temperature": 0.0, "avg_logprob": -0.166712703704834, "compression_ratio": 1.6666666666666667, "no_speech_prob": 7.765277587168384e-06}, {"id": 155, "seek": 71532, "start": 715.32, "end": 721.9200000000001, "text": " And so we get on the order of 10 to the 24 floating point operations to train a trillion", "tokens": [400, 370, 321, 483, 322, 264, 1668, 295, 1266, 281, 264, 4022, 12607, 935, 7705, 281, 3847, 257, 18723], "temperature": 0.0, "avg_logprob": -0.1452946662902832, "compression_ratio": 1.6515151515151516, "no_speech_prob": 5.2241117373341694e-05}, {"id": 156, "seek": 71532, "start": 721.9200000000001, "end": 726.32, "text": " parameter model for on one of these large data sets.", "tokens": [13075, 2316, 337, 322, 472, 295, 613, 2416, 1412, 6352, 13], "temperature": 0.0, "avg_logprob": -0.1452946662902832, "compression_ratio": 1.6515151515151516, "no_speech_prob": 5.2241117373341694e-05}, {"id": 157, "seek": 71532, "start": 726.32, "end": 729.7600000000001, "text": " So these numbers involved, I mean, I think the thing that I find most amazing about this", "tokens": [407, 613, 3547, 3288, 11, 286, 914, 11, 286, 519, 264, 551, 300, 286, 915, 881, 2243, 466, 341], "temperature": 0.0, "avg_logprob": -0.1452946662902832, "compression_ratio": 1.6515151515151516, "no_speech_prob": 5.2241117373341694e-05}, {"id": 158, "seek": 71532, "start": 729.7600000000001, "end": 734.6800000000001, "text": " is that I still remember taking chemistry in high school.", "tokens": [307, 300, 286, 920, 1604, 1940, 12558, 294, 1090, 1395, 13], "temperature": 0.0, "avg_logprob": -0.1452946662902832, "compression_ratio": 1.6515151515151516, "no_speech_prob": 5.2241117373341694e-05}, {"id": 159, "seek": 71532, "start": 734.6800000000001, "end": 739.5200000000001, "text": " And in chemistry, you learn that sort of a macroscopic amount of stuff is sort of", "tokens": [400, 294, 12558, 11, 291, 1466, 300, 1333, 295, 257, 7912, 38006, 299, 2372, 295, 1507, 307, 1333, 295], "temperature": 0.0, "avg_logprob": -0.1452946662902832, "compression_ratio": 1.6515151515151516, "no_speech_prob": 5.2241117373341694e-05}, {"id": 160, "seek": 71532, "start": 739.5200000000001, "end": 743.5600000000001, "text": " an avogadros number of atoms, which is like 6 times 10 to the 23.", "tokens": [364, 1305, 664, 345, 2635, 1230, 295, 16871, 11, 597, 307, 411, 1386, 1413, 1266, 281, 264, 6673, 13], "temperature": 0.0, "avg_logprob": -0.1452946662902832, "compression_ratio": 1.6515151515151516, "no_speech_prob": 5.2241117373341694e-05}, {"id": 161, "seek": 74356, "start": 743.56, "end": 749.1199999999999, "text": " So somehow we're actually able to build computers that do, that working together, do more than", "tokens": [407, 6063, 321, 434, 767, 1075, 281, 1322, 10807, 300, 360, 11, 300, 1364, 1214, 11, 360, 544, 813], "temperature": 0.0, "avg_logprob": -0.1703509116659359, "compression_ratio": 1.5950413223140496, "no_speech_prob": 6.960799510125071e-06}, {"id": 162, "seek": 74356, "start": 749.1199999999999, "end": 753.28, "text": " an avogadros number of computations to train these neural models.", "tokens": [364, 1305, 664, 345, 2635, 1230, 295, 2807, 763, 281, 3847, 613, 18161, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1703509116659359, "compression_ratio": 1.5950413223140496, "no_speech_prob": 6.960799510125071e-06}, {"id": 163, "seek": 74356, "start": 753.28, "end": 757.64, "text": " So anyway, I find these numbers kind of mind-boggling and also useful to sort of have in the", "tokens": [407, 4033, 11, 286, 915, 613, 3547, 733, 295, 1575, 12, 65, 36754, 1688, 293, 611, 4420, 281, 1333, 295, 362, 294, 264], "temperature": 0.0, "avg_logprob": -0.1703509116659359, "compression_ratio": 1.5950413223140496, "no_speech_prob": 6.960799510125071e-06}, {"id": 164, "seek": 74356, "start": 757.64, "end": 761.28, "text": " back of your head to understand what's going on.", "tokens": [646, 295, 428, 1378, 281, 1223, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.1703509116659359, "compression_ratio": 1.5950413223140496, "no_speech_prob": 6.960799510125071e-06}, {"id": 165, "seek": 74356, "start": 761.28, "end": 767.4, "text": " So with that, pretty good, unless there are any questions, I'll start talking about", "tokens": [407, 365, 300, 11, 1238, 665, 11, 5969, 456, 366, 604, 1651, 11, 286, 603, 722, 1417, 466], "temperature": 0.0, "avg_logprob": -0.1703509116659359, "compression_ratio": 1.5950413223140496, "no_speech_prob": 6.960799510125071e-06}, {"id": 166, "seek": 76740, "start": 767.4, "end": 774.64, "text": " scaling laws for these kinds of language models.", "tokens": [21589, 6064, 337, 613, 3685, 295, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.14615475047718396, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.2217495168442838e-05}, {"id": 167, "seek": 76740, "start": 774.64, "end": 783.0799999999999, "text": " So what I'll basically be arguing is that there are very surprisingly precise empirical", "tokens": [407, 437, 286, 603, 1936, 312, 19697, 307, 300, 456, 366, 588, 17600, 13600, 31886], "temperature": 0.0, "avg_logprob": -0.14615475047718396, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.2217495168442838e-05}, {"id": 168, "seek": 76740, "start": 783.0799999999999, "end": 790.36, "text": " scaling laws for the performance of machine learning systems, machine learning models,", "tokens": [21589, 6064, 337, 264, 3389, 295, 3479, 2539, 3652, 11, 3479, 2539, 5245, 11], "temperature": 0.0, "avg_logprob": -0.14615475047718396, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.2217495168442838e-05}, {"id": 169, "seek": 76740, "start": 790.36, "end": 796.28, "text": " as a function of kind of gross macroscopic inputs like how many parameters does the model", "tokens": [382, 257, 2445, 295, 733, 295, 11367, 7912, 38006, 299, 15743, 411, 577, 867, 9834, 775, 264, 2316], "temperature": 0.0, "avg_logprob": -0.14615475047718396, "compression_ratio": 1.638743455497382, "no_speech_prob": 1.2217495168442838e-05}, {"id": 170, "seek": 79628, "start": 796.28, "end": 803.52, "text": " have, how big is the data set, and how much compute is used for training.", "tokens": [362, 11, 577, 955, 307, 264, 1412, 992, 11, 293, 577, 709, 14722, 307, 1143, 337, 3097, 13], "temperature": 0.0, "avg_logprob": -0.10577270080303323, "compression_ratio": 1.625, "no_speech_prob": 5.3048723202664405e-05}, {"id": 171, "seek": 79628, "start": 803.52, "end": 809.1999999999999, "text": " And I'll also make the point that if you're sort of in an airplane at 30,000 feet looking", "tokens": [400, 286, 603, 611, 652, 264, 935, 300, 498, 291, 434, 1333, 295, 294, 364, 17130, 412, 2217, 11, 1360, 3521, 1237], "temperature": 0.0, "avg_logprob": -0.10577270080303323, "compression_ratio": 1.625, "no_speech_prob": 5.3048723202664405e-05}, {"id": 172, "seek": 79628, "start": 809.1999999999999, "end": 813.6, "text": " down on what's going on in the field, a lot of the other details in these systems don't", "tokens": [760, 322, 437, 311, 516, 322, 294, 264, 2519, 11, 257, 688, 295, 264, 661, 4365, 294, 613, 3652, 500, 380], "temperature": 0.0, "avg_logprob": -0.10577270080303323, "compression_ratio": 1.625, "no_speech_prob": 5.3048723202664405e-05}, {"id": 173, "seek": 79628, "start": 813.6, "end": 817.8399999999999, "text": " matter all that much, or at least they don't matter as much as you might have expected", "tokens": [1871, 439, 300, 709, 11, 420, 412, 1935, 436, 500, 380, 1871, 382, 709, 382, 291, 1062, 362, 5176], "temperature": 0.0, "avg_logprob": -0.10577270080303323, "compression_ratio": 1.625, "no_speech_prob": 5.3048723202664405e-05}, {"id": 174, "seek": 79628, "start": 817.8399999999999, "end": 819.28, "text": " that they would.", "tokens": [300, 436, 576, 13], "temperature": 0.0, "avg_logprob": -0.10577270080303323, "compression_ratio": 1.625, "no_speech_prob": 5.3048723202664405e-05}, {"id": 175, "seek": 79628, "start": 819.28, "end": 825.36, "text": " Very often they just change some kind of constant pre-factor in these kinds of scaling", "tokens": [4372, 2049, 436, 445, 1319, 512, 733, 295, 5754, 659, 12, 69, 15104, 294, 613, 3685, 295, 21589], "temperature": 0.0, "avg_logprob": -0.10577270080303323, "compression_ratio": 1.625, "no_speech_prob": 5.3048723202664405e-05}, {"id": 176, "seek": 82536, "start": 825.36, "end": 830.5600000000001, "text": " laws, which give you kind of a big picture of what's changing as you really increase", "tokens": [6064, 11, 597, 976, 291, 733, 295, 257, 955, 3036, 295, 437, 311, 4473, 382, 291, 534, 3488], "temperature": 0.0, "avg_logprob": -0.1464695840511682, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.5344848583918065e-05}, {"id": 177, "seek": 82536, "start": 830.5600000000001, "end": 832.48, "text": " these inputs.", "tokens": [613, 15743, 13], "temperature": 0.0, "avg_logprob": -0.1464695840511682, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.5344848583918065e-05}, {"id": 178, "seek": 82536, "start": 832.48, "end": 838.76, "text": " And one way of sort of turning this into sort of a theme, what do you learn from it, how", "tokens": [400, 472, 636, 295, 1333, 295, 6246, 341, 666, 1333, 295, 257, 6314, 11, 437, 360, 291, 1466, 490, 309, 11, 577], "temperature": 0.0, "avg_logprob": -0.1464695840511682, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.5344848583918065e-05}, {"id": 179, "seek": 82536, "start": 838.76, "end": 844.48, "text": " do you summarize it, is that getting these models to perform better is to a large extent", "tokens": [360, 291, 20858, 309, 11, 307, 300, 1242, 613, 5245, 281, 2042, 1101, 307, 281, 257, 2416, 8396], "temperature": 0.0, "avg_logprob": -0.1464695840511682, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.5344848583918065e-05}, {"id": 180, "seek": 82536, "start": 844.48, "end": 847.44, "text": " about kind of avoiding bottlenecks.", "tokens": [466, 733, 295, 20220, 44641, 2761, 13], "temperature": 0.0, "avg_logprob": -0.1464695840511682, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.5344848583918065e-05}, {"id": 181, "seek": 82536, "start": 847.44, "end": 849.44, "text": " It's avoiding being blocked by something.", "tokens": [467, 311, 20220, 885, 15470, 538, 746, 13], "temperature": 0.0, "avg_logprob": -0.1464695840511682, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.5344848583918065e-05}, {"id": 182, "seek": 82536, "start": 849.44, "end": 855.04, "text": " And there are a lot of things that can block improvements in performance.", "tokens": [400, 456, 366, 257, 688, 295, 721, 300, 393, 3461, 13797, 294, 3389, 13], "temperature": 0.0, "avg_logprob": -0.1464695840511682, "compression_ratio": 1.7188755020080322, "no_speech_prob": 3.5344848583918065e-05}, {"id": 183, "seek": 85504, "start": 855.04, "end": 859.68, "text": " The most obvious one, which is what scaling laws are studying, is you could not have enough", "tokens": [440, 881, 6322, 472, 11, 597, 307, 437, 21589, 6064, 366, 7601, 11, 307, 291, 727, 406, 362, 1547], "temperature": 0.0, "avg_logprob": -0.14091215493544093, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.00012335590145085007}, {"id": 184, "seek": 85504, "start": 859.68, "end": 865.0, "text": " data, you could not have a large enough model, you could not have enough computation to train", "tokens": [1412, 11, 291, 727, 406, 362, 257, 2416, 1547, 2316, 11, 291, 727, 406, 362, 1547, 24903, 281, 3847], "temperature": 0.0, "avg_logprob": -0.14091215493544093, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.00012335590145085007}, {"id": 185, "seek": 85504, "start": 865.0, "end": 866.3199999999999, "text": " that model.", "tokens": [300, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14091215493544093, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.00012335590145085007}, {"id": 186, "seek": 85504, "start": 866.3199999999999, "end": 871.52, "text": " And then there are also a lot of other literal bottlenecks that you can think about, many", "tokens": [400, 550, 456, 366, 611, 257, 688, 295, 661, 20411, 44641, 2761, 300, 291, 393, 519, 466, 11, 867], "temperature": 0.0, "avg_logprob": -0.14091215493544093, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.00012335590145085007}, {"id": 187, "seek": 85504, "start": 871.52, "end": 875.12, "text": " of which involve sort of that information propagation through the network.", "tokens": [295, 597, 9494, 1333, 295, 300, 1589, 38377, 807, 264, 3209, 13], "temperature": 0.0, "avg_logprob": -0.14091215493544093, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.00012335590145085007}, {"id": 188, "seek": 85504, "start": 875.12, "end": 879.88, "text": " So I guess like one way that I would summarize a lot of the most highly cited papers in machine", "tokens": [407, 286, 2041, 411, 472, 636, 300, 286, 576, 20858, 257, 688, 295, 264, 881, 5405, 30134, 10577, 294, 3479], "temperature": 0.0, "avg_logprob": -0.14091215493544093, "compression_ratio": 1.7821011673151752, "no_speech_prob": 0.00012335590145085007}, {"id": 189, "seek": 87988, "start": 879.88, "end": 886.88, "text": " learning in the last 10 years, papers like Resnets and LayerNorm, BatchNorm, things like", "tokens": [2539, 294, 264, 1036, 1266, 924, 11, 10577, 411, 5015, 77, 1385, 293, 35166, 45, 687, 11, 363, 852, 45, 687, 11, 721, 411], "temperature": 0.0, "avg_logprob": -0.19363487872880758, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.246885014756117e-05}, {"id": 190, "seek": 87988, "start": 886.88, "end": 892.16, "text": " that, is that there's sort of alleviating bottlenecks where information wasn't propagating", "tokens": [300, 11, 307, 300, 456, 311, 1333, 295, 33201, 990, 44641, 2761, 689, 1589, 2067, 380, 12425, 990], "temperature": 0.0, "avg_logprob": -0.19363487872880758, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.246885014756117e-05}, {"id": 191, "seek": 87988, "start": 892.16, "end": 894.52, "text": " nicely through your network.", "tokens": [9594, 807, 428, 3209, 13], "temperature": 0.0, "avg_logprob": -0.19363487872880758, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.246885014756117e-05}, {"id": 192, "seek": 87988, "start": 894.52, "end": 900.12, "text": " And the sort of simplest possible picture to sort of illustrate this, which perhaps is", "tokens": [400, 264, 1333, 295, 22811, 1944, 3036, 281, 1333, 295, 23221, 341, 11, 597, 4317, 307], "temperature": 0.0, "avg_logprob": -0.19363487872880758, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.246885014756117e-05}, {"id": 193, "seek": 87988, "start": 900.12, "end": 904.96, "text": " a cartoon of what's going on, something that I'll talk about later on with LSTMs, is", "tokens": [257, 18569, 295, 437, 311, 516, 322, 11, 746, 300, 286, 603, 751, 466, 1780, 322, 365, 441, 6840, 26386, 11, 307], "temperature": 0.0, "avg_logprob": -0.19363487872880758, "compression_ratio": 1.5384615384615385, "no_speech_prob": 2.246885014756117e-05}, {"id": 194, "seek": 90496, "start": 904.96, "end": 910.08, "text": " that if you take a matrix, I mean neural networks are really just fancy systems that do a lot", "tokens": [300, 498, 291, 747, 257, 8141, 11, 286, 914, 18161, 9590, 366, 534, 445, 10247, 3652, 300, 360, 257, 688], "temperature": 0.0, "avg_logprob": -0.15087071965249738, "compression_ratio": 1.705607476635514, "no_speech_prob": 2.8387845304678194e-05}, {"id": 195, "seek": 90496, "start": 910.08, "end": 911.08, "text": " of matrix multiplication.", "tokens": [295, 8141, 27290, 13], "temperature": 0.0, "avg_logprob": -0.15087071965249738, "compression_ratio": 1.705607476635514, "no_speech_prob": 2.8387845304678194e-05}, {"id": 196, "seek": 90496, "start": 911.08, "end": 917.12, "text": " If you take a matrix and you multiply it a large number of times, then very roughly speaking", "tokens": [759, 291, 747, 257, 8141, 293, 291, 12972, 309, 257, 2416, 1230, 295, 1413, 11, 550, 588, 9810, 4124], "temperature": 0.0, "avg_logprob": -0.15087071965249738, "compression_ratio": 1.705607476635514, "no_speech_prob": 2.8387845304678194e-05}, {"id": 197, "seek": 90496, "start": 917.12, "end": 924.64, "text": " what you end up with is a projection onto its largest eigenspace.", "tokens": [437, 291, 917, 493, 365, 307, 257, 22743, 3911, 1080, 6443, 10446, 24824, 13], "temperature": 0.0, "avg_logprob": -0.15087071965249738, "compression_ratio": 1.705607476635514, "no_speech_prob": 2.8387845304678194e-05}, {"id": 198, "seek": 90496, "start": 924.64, "end": 929.44, "text": " And so very roughly speaking, even with a deep network and you sort of don't set it up", "tokens": [400, 370, 588, 9810, 4124, 11, 754, 365, 257, 2452, 3209, 293, 291, 1333, 295, 500, 380, 992, 309, 493], "temperature": 0.0, "avg_logprob": -0.15087071965249738, "compression_ratio": 1.705607476635514, "no_speech_prob": 2.8387845304678194e-05}, {"id": 199, "seek": 92944, "start": 929.44, "end": 935.6400000000001, "text": " correctly, it's very easy to be in a situation where you lose signal or lose information", "tokens": [8944, 11, 309, 311, 588, 1858, 281, 312, 294, 257, 2590, 689, 291, 3624, 6358, 420, 3624, 1589], "temperature": 0.0, "avg_logprob": -0.19898105704266092, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.544251608720515e-05}, {"id": 200, "seek": 92944, "start": 935.6400000000001, "end": 937.96, "text": " and you get like a literal, literal model.", "tokens": [293, 291, 483, 411, 257, 20411, 11, 20411, 2316, 13], "temperature": 0.0, "avg_logprob": -0.19898105704266092, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.544251608720515e-05}, {"id": 201, "seek": 92944, "start": 937.96, "end": 943.72, "text": " But anyway, that's sort of the philosophy that at least at zero-thorder might, you might", "tokens": [583, 4033, 11, 300, 311, 1333, 295, 264, 10675, 300, 412, 1935, 412, 4018, 12, 392, 4687, 1062, 11, 291, 1062], "temperature": 0.0, "avg_logprob": -0.19898105704266092, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.544251608720515e-05}, {"id": 202, "seek": 92944, "start": 943.72, "end": 948.6800000000001, "text": " sort of reach from thinking about some of these results.", "tokens": [1333, 295, 2524, 490, 1953, 466, 512, 295, 613, 3542, 13], "temperature": 0.0, "avg_logprob": -0.19898105704266092, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.544251608720515e-05}, {"id": 203, "seek": 92944, "start": 948.6800000000001, "end": 955.12, "text": " So this slide is really about the kind of core results for scaling laws for language", "tokens": [407, 341, 4137, 307, 534, 466, 264, 733, 295, 4965, 3542, 337, 21589, 6064, 337, 2856], "temperature": 0.0, "avg_logprob": -0.19898105704266092, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.544251608720515e-05}, {"id": 204, "seek": 92944, "start": 955.12, "end": 956.12, "text": " models.", "tokens": [5245, 13], "temperature": 0.0, "avg_logprob": -0.19898105704266092, "compression_ratio": 1.6517857142857142, "no_speech_prob": 1.544251608720515e-05}, {"id": 205, "seek": 95612, "start": 956.12, "end": 959.44, "text": " I'll explain it in some detail.", "tokens": [286, 603, 2903, 309, 294, 512, 2607, 13], "temperature": 0.0, "avg_logprob": -0.1358318445159168, "compression_ratio": 1.5095238095238095, "no_speech_prob": 7.966036355355754e-05}, {"id": 206, "seek": 95612, "start": 959.44, "end": 965.6, "text": " So I'm actually going to start with the plot on the far right, which is about scaling", "tokens": [407, 286, 478, 767, 516, 281, 722, 365, 264, 7542, 322, 264, 1400, 558, 11, 597, 307, 466, 21589], "temperature": 0.0, "avg_logprob": -0.1358318445159168, "compression_ratio": 1.5095238095238095, "no_speech_prob": 7.966036355355754e-05}, {"id": 207, "seek": 95612, "start": 965.6, "end": 971.64, "text": " laws with respect to the number of parameters in a neural network.", "tokens": [6064, 365, 3104, 281, 264, 1230, 295, 9834, 294, 257, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.1358318445159168, "compression_ratio": 1.5095238095238095, "no_speech_prob": 7.966036355355754e-05}, {"id": 208, "seek": 95612, "start": 971.64, "end": 979.88, "text": " And so what we did to generate this plot was get a very large data set such that we weren't", "tokens": [400, 370, 437, 321, 630, 281, 8460, 341, 7542, 390, 483, 257, 588, 2416, 1412, 992, 1270, 300, 321, 4999, 380], "temperature": 0.0, "avg_logprob": -0.1358318445159168, "compression_ratio": 1.5095238095238095, "no_speech_prob": 7.966036355355754e-05}, {"id": 209, "seek": 95612, "start": 979.88, "end": 983.8, "text": " worried about models overfitting it all.", "tokens": [5804, 466, 5245, 670, 69, 2414, 309, 439, 13], "temperature": 0.0, "avg_logprob": -0.1358318445159168, "compression_ratio": 1.5095238095238095, "no_speech_prob": 7.966036355355754e-05}, {"id": 210, "seek": 98380, "start": 983.8, "end": 990.5999999999999, "text": " And train all of our models for a very long time so that they were essentially at convergence.", "tokens": [400, 3847, 439, 295, 527, 5245, 337, 257, 588, 938, 565, 370, 300, 436, 645, 4476, 412, 32181, 13], "temperature": 0.0, "avg_logprob": -0.14787942171096802, "compression_ratio": 1.6875, "no_speech_prob": 3.425822433200665e-05}, {"id": 211, "seek": 98380, "start": 990.5999999999999, "end": 994.64, "text": " So in other words, training time or compute was not constrained on performance.", "tokens": [407, 294, 661, 2283, 11, 3097, 565, 420, 14722, 390, 406, 38901, 322, 3389, 13], "temperature": 0.0, "avg_logprob": -0.14787942171096802, "compression_ratio": 1.6875, "no_speech_prob": 3.425822433200665e-05}, {"id": 212, "seek": 98380, "start": 994.64, "end": 1001.52, "text": " And then plot the resulting test loss of language models, trained to predict the next word", "tokens": [400, 550, 7542, 264, 16505, 1500, 4470, 295, 2856, 5245, 11, 8895, 281, 6069, 264, 958, 1349], "temperature": 0.0, "avg_logprob": -0.14787942171096802, "compression_ratio": 1.6875, "no_speech_prob": 3.425822433200665e-05}, {"id": 213, "seek": 98380, "start": 1001.52, "end": 1005.7199999999999, "text": " as a function of parameter count on a nice log scale.", "tokens": [382, 257, 2445, 295, 13075, 1207, 322, 257, 1481, 3565, 4373, 13], "temperature": 0.0, "avg_logprob": -0.14787942171096802, "compression_ratio": 1.6875, "no_speech_prob": 3.425822433200665e-05}, {"id": 214, "seek": 98380, "start": 1005.7199999999999, "end": 1009.3599999999999, "text": " And so what you see is that there's this power law, which is a straight line on a log", "tokens": [400, 370, 437, 291, 536, 307, 300, 456, 311, 341, 1347, 2101, 11, 597, 307, 257, 2997, 1622, 322, 257, 3565], "temperature": 0.0, "avg_logprob": -0.14787942171096802, "compression_ratio": 1.6875, "no_speech_prob": 3.425822433200665e-05}, {"id": 215, "seek": 100936, "start": 1009.36, "end": 1016.5600000000001, "text": " log plot of the loss as a function of the parameter count of these models.", "tokens": [3565, 7542, 295, 264, 4470, 382, 257, 2445, 295, 264, 13075, 1207, 295, 613, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1253020184711345, "compression_ratio": 1.7719298245614035, "no_speech_prob": 2.3922399122966453e-05}, {"id": 216, "seek": 100936, "start": 1016.5600000000001, "end": 1021.88, "text": " In the middle plot, we do the same thing, but switch the role of the amount of data that", "tokens": [682, 264, 2808, 7542, 11, 321, 360, 264, 912, 551, 11, 457, 3679, 264, 3090, 295, 264, 2372, 295, 1412, 300], "temperature": 0.0, "avg_logprob": -0.1253020184711345, "compression_ratio": 1.7719298245614035, "no_speech_prob": 2.3922399122966453e-05}, {"id": 217, "seek": 100936, "start": 1021.88, "end": 1023.92, "text": " we have with parameter count.", "tokens": [321, 362, 365, 13075, 1207, 13], "temperature": 0.0, "avg_logprob": -0.1253020184711345, "compression_ratio": 1.7719298245614035, "no_speech_prob": 2.3922399122966453e-05}, {"id": 218, "seek": 100936, "start": 1023.92, "end": 1028.68, "text": " So we train a model that's very large, maybe one of the largest models on the plot on", "tokens": [407, 321, 3847, 257, 2316, 300, 311, 588, 2416, 11, 1310, 472, 295, 264, 6443, 5245, 322, 264, 7542, 322], "temperature": 0.0, "avg_logprob": -0.1253020184711345, "compression_ratio": 1.7719298245614035, "no_speech_prob": 2.3922399122966453e-05}, {"id": 219, "seek": 100936, "start": 1028.68, "end": 1035.92, "text": " the right, so that model size is not a constraint on performance on data sets of various sizes.", "tokens": [264, 558, 11, 370, 300, 2316, 2744, 307, 406, 257, 25534, 322, 3389, 322, 1412, 6352, 295, 3683, 11602, 13], "temperature": 0.0, "avg_logprob": -0.1253020184711345, "compression_ratio": 1.7719298245614035, "no_speech_prob": 2.3922399122966453e-05}, {"id": 220, "seek": 100936, "start": 1035.92, "end": 1037.6, "text": " And we apply early stopping.", "tokens": [400, 321, 3079, 2440, 12767, 13], "temperature": 0.0, "avg_logprob": -0.1253020184711345, "compression_ratio": 1.7719298245614035, "no_speech_prob": 2.3922399122966453e-05}, {"id": 221, "seek": 103760, "start": 1037.6, "end": 1041.52, "text": " So we measure the test loss at the point where the test loss is at its minimum during", "tokens": [407, 321, 3481, 264, 1500, 4470, 412, 264, 935, 689, 264, 1500, 4470, 307, 412, 1080, 7285, 1830], "temperature": 0.0, "avg_logprob": -0.11457148159251493, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.129933116317261e-05}, {"id": 222, "seek": 103760, "start": 1041.52, "end": 1044.6399999999999, "text": " otherwise pretty naive straightforward training.", "tokens": [5911, 1238, 29052, 15325, 3097, 13], "temperature": 0.0, "avg_logprob": -0.11457148159251493, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.129933116317261e-05}, {"id": 223, "seek": 103760, "start": 1044.6399999999999, "end": 1051.6, "text": " And we find again a very clear power law for loss as a function of data set size.", "tokens": [400, 321, 915, 797, 257, 588, 1850, 1347, 2101, 337, 4470, 382, 257, 2445, 295, 1412, 992, 2744, 13], "temperature": 0.0, "avg_logprob": -0.11457148159251493, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.129933116317261e-05}, {"id": 224, "seek": 103760, "start": 1051.6, "end": 1055.56, "text": " And then the most complicated plot is the one on the left.", "tokens": [400, 550, 264, 881, 6179, 7542, 307, 264, 472, 322, 264, 1411, 13], "temperature": 0.0, "avg_logprob": -0.11457148159251493, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.129933116317261e-05}, {"id": 225, "seek": 103760, "start": 1055.56, "end": 1065.28, "text": " So on the left, we plot all of the learning curves for many different models.", "tokens": [407, 322, 264, 1411, 11, 321, 7542, 439, 295, 264, 2539, 19490, 337, 867, 819, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11457148159251493, "compression_ratio": 1.6729857819905214, "no_speech_prob": 1.129933116317261e-05}, {"id": 226, "seek": 106528, "start": 1065.28, "end": 1068.76, "text": " We provide these models with plenty of data so they're not overfitting.", "tokens": [492, 2893, 613, 5245, 365, 7140, 295, 1412, 370, 436, 434, 406, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.13862398818687158, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.7566136345267296e-05}, {"id": 227, "seek": 106528, "start": 1068.76, "end": 1072.24, "text": " They're in the under parameterized regime.", "tokens": [814, 434, 294, 264, 833, 13075, 1602, 13120, 13], "temperature": 0.0, "avg_logprob": -0.13862398818687158, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.7566136345267296e-05}, {"id": 228, "seek": 106528, "start": 1072.24, "end": 1077.3999999999999, "text": " But we train all of these different model sizes for a very, very long time.", "tokens": [583, 321, 3847, 439, 295, 613, 819, 2316, 11602, 337, 257, 588, 11, 588, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.13862398818687158, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.7566136345267296e-05}, {"id": 229, "seek": 106528, "start": 1077.3999999999999, "end": 1083.92, "text": " And we measure on the x-axis not the number of training steps or training tokens, but", "tokens": [400, 321, 3481, 322, 264, 2031, 12, 24633, 406, 264, 1230, 295, 3097, 4439, 420, 3097, 22667, 11, 457], "temperature": 0.0, "avg_logprob": -0.13862398818687158, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.7566136345267296e-05}, {"id": 230, "seek": 106528, "start": 1083.92, "end": 1088.28, "text": " the amount of compute that has been used so far during training.", "tokens": [264, 2372, 295, 14722, 300, 575, 668, 1143, 370, 1400, 1830, 3097, 13], "temperature": 0.0, "avg_logprob": -0.13862398818687158, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.7566136345267296e-05}, {"id": 231, "seek": 106528, "start": 1088.28, "end": 1092.6, "text": " And as a consequence of one of the formulas that I wrote on a couple of slides ago, that", "tokens": [400, 382, 257, 18326, 295, 472, 295, 264, 30546, 300, 286, 4114, 322, 257, 1916, 295, 9788, 2057, 11, 300], "temperature": 0.0, "avg_logprob": -0.13862398818687158, "compression_ratio": 1.6538461538461537, "no_speech_prob": 4.7566136345267296e-05}, {"id": 232, "seek": 109260, "start": 1092.6, "end": 1098.7199999999998, "text": " compute is six times parameter count times the amount of training data.", "tokens": [14722, 307, 2309, 1413, 13075, 1207, 1413, 264, 2372, 295, 3097, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13620979269755254, "compression_ratio": 1.8660714285714286, "no_speech_prob": 2.355077231186442e-05}, {"id": 233, "seek": 109260, "start": 1098.7199999999998, "end": 1104.04, "text": " If you take the logarithm of both sides, the log of parameters times data is log of parameters", "tokens": [759, 291, 747, 264, 41473, 32674, 295, 1293, 4881, 11, 264, 3565, 295, 9834, 1413, 1412, 307, 3565, 295, 9834], "temperature": 0.0, "avg_logprob": -0.13620979269755254, "compression_ratio": 1.8660714285714286, "no_speech_prob": 2.355077231186442e-05}, {"id": 234, "seek": 109260, "start": 1104.04, "end": 1105.32, "text": " plus log of data.", "tokens": [1804, 3565, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.13620979269755254, "compression_ratio": 1.8660714285714286, "no_speech_prob": 2.355077231186442e-05}, {"id": 235, "seek": 109260, "start": 1105.32, "end": 1109.28, "text": " So what that means is that learning curves for models at different sizes are just shifted", "tokens": [407, 437, 300, 1355, 307, 300, 2539, 19490, 337, 5245, 412, 819, 11602, 366, 445, 18892], "temperature": 0.0, "avg_logprob": -0.13620979269755254, "compression_ratio": 1.8660714285714286, "no_speech_prob": 2.355077231186442e-05}, {"id": 236, "seek": 109260, "start": 1109.28, "end": 1114.9599999999998, "text": " over left and right by constant amounts with the largest models on the sort of the far", "tokens": [670, 1411, 293, 558, 538, 5754, 11663, 365, 264, 6443, 5245, 322, 264, 1333, 295, 264, 1400], "temperature": 0.0, "avg_logprob": -0.13620979269755254, "compression_ratio": 1.8660714285714286, "no_speech_prob": 2.355077231186442e-05}, {"id": 237, "seek": 109260, "start": 1114.9599999999998, "end": 1118.76, "text": " right of this curve and the smallest models on the left.", "tokens": [558, 295, 341, 7605, 293, 264, 16998, 5245, 322, 264, 1411, 13], "temperature": 0.0, "avg_logprob": -0.13620979269755254, "compression_ratio": 1.8660714285714286, "no_speech_prob": 2.355077231186442e-05}, {"id": 238, "seek": 111876, "start": 1118.76, "end": 1122.96, "text": " So we have the learning curves for all of these models all put together.", "tokens": [407, 321, 362, 264, 2539, 19490, 337, 439, 295, 613, 5245, 439, 829, 1214, 13], "temperature": 0.0, "avg_logprob": -0.145691827078846, "compression_ratio": 1.7991967871485943, "no_speech_prob": 5.475135913002305e-05}, {"id": 239, "seek": 111876, "start": 1122.96, "end": 1127.72, "text": " And so a question you can ask is sort of what is the best loss you can get for any given", "tokens": [400, 370, 257, 1168, 291, 393, 1029, 307, 1333, 295, 437, 307, 264, 1151, 4470, 291, 393, 483, 337, 604, 2212], "temperature": 0.0, "avg_logprob": -0.145691827078846, "compression_ratio": 1.7991967871485943, "no_speech_prob": 5.475135913002305e-05}, {"id": 240, "seek": 111876, "start": 1127.72, "end": 1132.84, "text": " amount of training compute where you're allowing yourself to choose the model that does best", "tokens": [2372, 295, 3097, 14722, 689, 291, 434, 8293, 1803, 281, 2826, 264, 2316, 300, 775, 1151], "temperature": 0.0, "avg_logprob": -0.145691827078846, "compression_ratio": 1.7991967871485943, "no_speech_prob": 5.475135913002305e-05}, {"id": 241, "seek": 111876, "start": 1132.84, "end": 1134.12, "text": " for that amount of training compute?", "tokens": [337, 300, 2372, 295, 3097, 14722, 30], "temperature": 0.0, "avg_logprob": -0.145691827078846, "compression_ratio": 1.7991967871485943, "no_speech_prob": 5.475135913002305e-05}, {"id": 242, "seek": 111876, "start": 1134.12, "end": 1139.92, "text": " And that's what sort of the heavy black line and the orange fit are picking out.", "tokens": [400, 300, 311, 437, 1333, 295, 264, 4676, 2211, 1622, 293, 264, 7671, 3318, 366, 8867, 484, 13], "temperature": 0.0, "avg_logprob": -0.145691827078846, "compression_ratio": 1.7991967871485943, "no_speech_prob": 5.475135913002305e-05}, {"id": 243, "seek": 111876, "start": 1139.92, "end": 1145.8, "text": " I mean formally you could call this the convex hull of all of these curves.", "tokens": [286, 914, 25983, 291, 727, 818, 341, 264, 42432, 32335, 295, 439, 295, 613, 19490, 13], "temperature": 0.0, "avg_logprob": -0.145691827078846, "compression_ratio": 1.7991967871485943, "no_speech_prob": 5.475135913002305e-05}, {"id": 244, "seek": 114580, "start": 1145.8, "end": 1152.6, "text": " And that again somewhat surprisingly seems to obey a very nice power law fit over many,", "tokens": [400, 300, 797, 8344, 17600, 2544, 281, 19297, 257, 588, 1481, 1347, 2101, 3318, 670, 867, 11], "temperature": 0.0, "avg_logprob": -0.16982369570388006, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.2409578630467877e-05}, {"id": 245, "seek": 114580, "start": 1152.6, "end": 1157.28, "text": " many orders of magnitude in computation.", "tokens": [867, 9470, 295, 15668, 294, 24903, 13], "temperature": 0.0, "avg_logprob": -0.16982369570388006, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.2409578630467877e-05}, {"id": 246, "seek": 114580, "start": 1157.28, "end": 1161.56, "text": " And it's crucial for all of these experiments that you're only limiting performance with", "tokens": [400, 309, 311, 11462, 337, 439, 295, 613, 12050, 300, 291, 434, 787, 22083, 3389, 365], "temperature": 0.0, "avg_logprob": -0.16982369570388006, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.2409578630467877e-05}, {"id": 247, "seek": 114580, "start": 1161.56, "end": 1163.84, "text": " one thing at a time.", "tokens": [472, 551, 412, 257, 565, 13], "temperature": 0.0, "avg_logprob": -0.16982369570388006, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.2409578630467877e-05}, {"id": 248, "seek": 114580, "start": 1163.84, "end": 1167.32, "text": " On the far right you have plenty of data in compute, but you're limiting the number of", "tokens": [1282, 264, 1400, 558, 291, 362, 7140, 295, 1412, 294, 14722, 11, 457, 291, 434, 22083, 264, 1230, 295], "temperature": 0.0, "avg_logprob": -0.16982369570388006, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.2409578630467877e-05}, {"id": 249, "seek": 114580, "start": 1167.32, "end": 1170.9199999999998, "text": " parameters in the middle you're limiting the amount of data but you have a big model.", "tokens": [9834, 294, 264, 2808, 291, 434, 22083, 264, 2372, 295, 1412, 457, 291, 362, 257, 955, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16982369570388006, "compression_ratio": 1.7196652719665273, "no_speech_prob": 1.2409578630467877e-05}, {"id": 250, "seek": 117092, "start": 1170.92, "end": 1176.96, "text": " On the left you're looking at training compute but you have all sorts of different model sizes", "tokens": [1282, 264, 1411, 291, 434, 1237, 412, 3097, 14722, 457, 291, 362, 439, 7527, 295, 819, 2316, 11602], "temperature": 0.0, "avg_logprob": -0.26193755781146844, "compression_ratio": 1.5935828877005347, "no_speech_prob": 6.7458790908858646e-06}, {"id": 251, "seek": 117092, "start": 1176.96, "end": 1179.28, "text": " and again plenty of data.", "tokens": [293, 797, 7140, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.26193755781146844, "compression_ratio": 1.5935828877005347, "no_speech_prob": 6.7458790908858646e-06}, {"id": 252, "seek": 117092, "start": 1179.28, "end": 1183.16, "text": " So in other words in each of these cases there's sort of one of these parameters that's", "tokens": [407, 294, 661, 2283, 294, 1184, 295, 613, 3331, 456, 311, 1333, 295, 472, 295, 613, 9834, 300, 311], "temperature": 0.0, "avg_logprob": -0.26193755781146844, "compression_ratio": 1.5935828877005347, "no_speech_prob": 6.7458790908858646e-06}, {"id": 253, "seek": 117092, "start": 1183.16, "end": 1188.44, "text": " bottlenecking performance and otherwise you have plenty of resources.", "tokens": [44641, 25723, 3389, 293, 5911, 291, 362, 7140, 295, 3593, 13], "temperature": 0.0, "avg_logprob": -0.26193755781146844, "compression_ratio": 1.5935828877005347, "no_speech_prob": 6.7458790908858646e-06}, {"id": 254, "seek": 117092, "start": 1188.44, "end": 1190.1200000000001, "text": " There's a question?", "tokens": [821, 311, 257, 1168, 30], "temperature": 0.0, "avg_logprob": -0.26193755781146844, "compression_ratio": 1.5935828877005347, "no_speech_prob": 6.7458790908858646e-06}, {"id": 255, "seek": 119012, "start": 1190.12, "end": 1200.8, "text": " I hope that's not true.", "tokens": [286, 1454, 300, 311, 406, 2074, 13], "temperature": 0.0, "avg_logprob": -0.5638083445874951, "compression_ratio": 1.4025974025974026, "no_speech_prob": 5.918489478062838e-05}, {"id": 256, "seek": 119012, "start": 1200.8, "end": 1203.36, "text": " So there's a minus sign in the exponent.", "tokens": [407, 456, 311, 257, 3175, 1465, 294, 264, 37871, 13], "temperature": 0.0, "avg_logprob": -0.5638083445874951, "compression_ratio": 1.4025974025974026, "no_speech_prob": 5.918489478062838e-05}, {"id": 257, "seek": 119012, "start": 1203.36, "end": 1207.9599999999998, "text": " I'm not sure if you're looking at the lines or the function.", "tokens": [286, 478, 406, 988, 498, 291, 434, 1237, 412, 264, 3876, 420, 264, 2445, 13], "temperature": 0.0, "avg_logprob": -0.5638083445874951, "compression_ratio": 1.4025974025974026, "no_speech_prob": 5.918489478062838e-05}, {"id": 258, "seek": 119012, "start": 1207.9599999999998, "end": 1209.9599999999998, "text": " On the bottom, I see.", "tokens": [1282, 264, 2767, 11, 286, 536, 13], "temperature": 0.0, "avg_logprob": -0.5638083445874951, "compression_ratio": 1.4025974025974026, "no_speech_prob": 5.918489478062838e-05}, {"id": 259, "seek": 119012, "start": 1209.9599999999998, "end": 1210.9599999999998, "text": " I'll be right.", "tokens": [286, 603, 312, 558, 13], "temperature": 0.0, "avg_logprob": -0.5638083445874951, "compression_ratio": 1.4025974025974026, "no_speech_prob": 5.918489478062838e-05}, {"id": 260, "seek": 119012, "start": 1210.9599999999998, "end": 1211.9599999999998, "text": " Oh, I see.", "tokens": [876, 11, 286, 536, 13], "temperature": 0.0, "avg_logprob": -0.5638083445874951, "compression_ratio": 1.4025974025974026, "no_speech_prob": 5.918489478062838e-05}, {"id": 261, "seek": 119012, "start": 1211.9599999999998, "end": 1212.9599999999998, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.5638083445874951, "compression_ratio": 1.4025974025974026, "no_speech_prob": 5.918489478062838e-05}, {"id": 262, "seek": 119012, "start": 1212.9599999999998, "end": 1217.84, "text": " Yeah, they're just a log scope plot.", "tokens": [865, 11, 436, 434, 445, 257, 3565, 11923, 7542, 13], "temperature": 0.0, "avg_logprob": -0.5638083445874951, "compression_ratio": 1.4025974025974026, "no_speech_prob": 5.918489478062838e-05}, {"id": 263, "seek": 121784, "start": 1217.84, "end": 1220.6399999999999, "text": " Yeah, please ask any questions.", "tokens": [865, 11, 1767, 1029, 604, 1651, 13], "temperature": 0.0, "avg_logprob": -0.342380568560432, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.252991053974256e-05}, {"id": 264, "seek": 121784, "start": 1220.6399999999999, "end": 1222.9599999999998, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.342380568560432, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.252991053974256e-05}, {"id": 265, "seek": 121784, "start": 1222.9599999999998, "end": 1227.8799999999999, "text": " And then the x-axis on this compute plot is this pediflop a day unit.", "tokens": [400, 550, 264, 2031, 12, 24633, 322, 341, 14722, 7542, 307, 341, 5670, 351, 75, 404, 257, 786, 4985, 13], "temperature": 0.0, "avg_logprob": -0.342380568560432, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.252991053974256e-05}, {"id": 266, "seek": 121784, "start": 1227.8799999999999, "end": 1231.12, "text": " That's why it's actually a small number.", "tokens": [663, 311, 983, 309, 311, 767, 257, 1359, 1230, 13], "temperature": 0.0, "avg_logprob": -0.342380568560432, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.252991053974256e-05}, {"id": 267, "seek": 121784, "start": 1231.12, "end": 1236.6399999999999, "text": " Any other questions about anything about this plot?", "tokens": [2639, 661, 1651, 466, 1340, 466, 341, 7542, 30], "temperature": 0.0, "avg_logprob": -0.342380568560432, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.252991053974256e-05}, {"id": 268, "seek": 121784, "start": 1236.6399999999999, "end": 1239.12, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.342380568560432, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.252991053974256e-05}, {"id": 269, "seek": 121784, "start": 1239.12, "end": 1244.24, "text": " So there's another thing that you can do that's kind of interesting with the plot on the", "tokens": [407, 456, 311, 1071, 551, 300, 291, 393, 360, 300, 311, 733, 295, 1880, 365, 264, 7542, 322, 264], "temperature": 0.0, "avg_logprob": -0.342380568560432, "compression_ratio": 1.5179487179487179, "no_speech_prob": 7.252991053974256e-05}, {"id": 270, "seek": 124424, "start": 1244.24, "end": 1253.2, "text": " left, which is you can ask for any given quantity of compute that you have available.", "tokens": [1411, 11, 597, 307, 291, 393, 1029, 337, 604, 2212, 11275, 295, 14722, 300, 291, 362, 2435, 13], "temperature": 0.0, "avg_logprob": -0.14916502557149747, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00013977094204165041}, {"id": 271, "seek": 124424, "start": 1253.2, "end": 1258.84, "text": " Someone kindly donates to you some number of a-100s to use for a few weeks and you want", "tokens": [8734, 29736, 500, 1024, 281, 291, 512, 1230, 295, 257, 12, 6879, 82, 281, 764, 337, 257, 1326, 3259, 293, 291, 528], "temperature": 0.0, "avg_logprob": -0.14916502557149747, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00013977094204165041}, {"id": 272, "seek": 124424, "start": 1258.84, "end": 1261.96, "text": " to use it to train the best possible language model you can.", "tokens": [281, 764, 309, 281, 3847, 264, 1151, 1944, 2856, 2316, 291, 393, 13], "temperature": 0.0, "avg_logprob": -0.14916502557149747, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00013977094204165041}, {"id": 273, "seek": 124424, "start": 1261.96, "end": 1267.72, "text": " And so you can ask based on this plot on the left, how should I allocate the computation", "tokens": [400, 370, 291, 393, 1029, 2361, 322, 341, 7542, 322, 264, 1411, 11, 577, 820, 286, 35713, 264, 24903], "temperature": 0.0, "avg_logprob": -0.14916502557149747, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.00013977094204165041}, {"id": 274, "seek": 126772, "start": 1267.72, "end": 1274.92, "text": " that was given to me in terms of making a bigger model or training longer?", "tokens": [300, 390, 2212, 281, 385, 294, 2115, 295, 1455, 257, 3801, 2316, 420, 3097, 2854, 30], "temperature": 0.0, "avg_logprob": -0.13263729456308726, "compression_ratio": 1.5495049504950495, "no_speech_prob": 6.400935671990737e-05}, {"id": 275, "seek": 126772, "start": 1274.92, "end": 1280.3600000000001, "text": " And it turns out there's sort of a simplified cartoon for the answer that we found with", "tokens": [400, 309, 4523, 484, 456, 311, 1333, 295, 257, 26335, 18569, 337, 264, 1867, 300, 321, 1352, 365], "temperature": 0.0, "avg_logprob": -0.13263729456308726, "compression_ratio": 1.5495049504950495, "no_speech_prob": 6.400935671990737e-05}, {"id": 276, "seek": 126772, "start": 1280.3600000000001, "end": 1286.52, "text": " our language data, which was that you mostly want to allocate most of your compute, basically", "tokens": [527, 2856, 1412, 11, 597, 390, 300, 291, 5240, 528, 281, 35713, 881, 295, 428, 14722, 11, 1936], "temperature": 0.0, "avg_logprob": -0.13263729456308726, "compression_ratio": 1.5495049504950495, "no_speech_prob": 6.400935671990737e-05}, {"id": 277, "seek": 126772, "start": 1286.52, "end": 1292.16, "text": " two-thirds on a geometric scale to making models bigger.", "tokens": [732, 12, 38507, 322, 257, 33246, 4373, 281, 1455, 5245, 3801, 13], "temperature": 0.0, "avg_logprob": -0.13263729456308726, "compression_ratio": 1.5495049504950495, "no_speech_prob": 6.400935671990737e-05}, {"id": 278, "seek": 129216, "start": 1292.16, "end": 1298.44, "text": " And you can allocate about a third to training for longer on more data.", "tokens": [400, 291, 393, 35713, 466, 257, 2636, 281, 3097, 337, 2854, 322, 544, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11229143992508991, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.1973846236942336e-05}, {"id": 279, "seek": 129216, "start": 1298.44, "end": 1303.28, "text": " And so this at least for us wasn't an obvious conclusion.", "tokens": [400, 370, 341, 412, 1935, 337, 505, 2067, 380, 364, 6322, 10063, 13], "temperature": 0.0, "avg_logprob": -0.11229143992508991, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.1973846236942336e-05}, {"id": 280, "seek": 129216, "start": 1303.28, "end": 1309.16, "text": " It suggests that a lot of the gains that you're going to get if you want to get better", "tokens": [467, 13409, 300, 257, 688, 295, 264, 16823, 300, 291, 434, 516, 281, 483, 498, 291, 528, 281, 483, 1101], "temperature": 0.0, "avg_logprob": -0.11229143992508991, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.1973846236942336e-05}, {"id": 281, "seek": 129216, "start": 1309.16, "end": 1313.8000000000002, "text": " performance with a fixed amount of compute, a fixed budget, is going to come from making", "tokens": [3389, 365, 257, 6806, 2372, 295, 14722, 11, 257, 6806, 4706, 11, 307, 516, 281, 808, 490, 1455], "temperature": 0.0, "avg_logprob": -0.11229143992508991, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.1973846236942336e-05}, {"id": 282, "seek": 129216, "start": 1313.8000000000002, "end": 1315.0, "text": " your models bigger.", "tokens": [428, 5245, 3801, 13], "temperature": 0.0, "avg_logprob": -0.11229143992508991, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.1973846236942336e-05}, {"id": 283, "seek": 129216, "start": 1315.0, "end": 1318.0, "text": " And it turns out that in practice, I won't go into it in detail.", "tokens": [400, 309, 4523, 484, 300, 294, 3124, 11, 286, 1582, 380, 352, 666, 309, 294, 2607, 13], "temperature": 0.0, "avg_logprob": -0.11229143992508991, "compression_ratio": 1.638655462184874, "no_speech_prob": 4.1973846236942336e-05}, {"id": 284, "seek": 131800, "start": 1318.0, "end": 1322.4, "text": " You can, to some extent, just make your batch size bigger during training.", "tokens": [509, 393, 11, 281, 512, 8396, 11, 445, 652, 428, 15245, 2744, 3801, 1830, 3097, 13], "temperature": 0.0, "avg_logprob": -0.21515541076660155, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.267979627707973e-05}, {"id": 285, "seek": 131800, "start": 1322.4, "end": 1326.0, "text": " And that means that the total number of serial steps that you train for doesn't have to", "tokens": [400, 300, 1355, 300, 264, 3217, 1230, 295, 17436, 4439, 300, 291, 3847, 337, 1177, 380, 362, 281], "temperature": 0.0, "avg_logprob": -0.21515541076660155, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.267979627707973e-05}, {"id": 286, "seek": 131800, "start": 1326.0, "end": 1327.84, "text": " increase all that much.", "tokens": [3488, 439, 300, 709, 13], "temperature": 0.0, "avg_logprob": -0.21515541076660155, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.267979627707973e-05}, {"id": 287, "seek": 131800, "start": 1327.84, "end": 1331.52, "text": " You don't necessarily like to train for vastly longer.", "tokens": [509, 500, 380, 4725, 411, 281, 3847, 337, 41426, 2854, 13], "temperature": 0.0, "avg_logprob": -0.21515541076660155, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.267979627707973e-05}, {"id": 288, "seek": 131800, "start": 1331.52, "end": 1336.04, "text": " You seemingly just need a largely a bigger model.", "tokens": [509, 18709, 445, 643, 257, 11611, 257, 3801, 2316, 13], "temperature": 0.0, "avg_logprob": -0.21515541076660155, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.267979627707973e-05}, {"id": 289, "seek": 131800, "start": 1336.04, "end": 1340.76, "text": " And that's something that you read off from this compute plot that I showed.", "tokens": [400, 300, 311, 746, 300, 291, 1401, 766, 490, 341, 14722, 7542, 300, 286, 4712, 13], "temperature": 0.0, "avg_logprob": -0.21515541076660155, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.267979627707973e-05}, {"id": 290, "seek": 134076, "start": 1340.76, "end": 1348.72, "text": " That's a general question.", "tokens": [663, 311, 257, 2674, 1168, 13], "temperature": 0.0, "avg_logprob": -0.17199921607971191, "compression_ratio": 1.547486033519553, "no_speech_prob": 4.83060757687781e-05}, {"id": 291, "seek": 134076, "start": 1348.72, "end": 1357.92, "text": " The way that you get this graph is you basically do an analysis where you look at any given", "tokens": [440, 636, 300, 291, 483, 341, 4295, 307, 291, 1936, 360, 364, 5215, 689, 291, 574, 412, 604, 2212], "temperature": 0.0, "avg_logprob": -0.17199921607971191, "compression_ratio": 1.547486033519553, "no_speech_prob": 4.83060757687781e-05}, {"id": 292, "seek": 134076, "start": 1357.92, "end": 1364.44, "text": " point for compute and you look up and you pick out the blue curve that's closest to the", "tokens": [935, 337, 14722, 293, 291, 574, 493, 293, 291, 1888, 484, 264, 3344, 7605, 300, 311, 13699, 281, 264], "temperature": 0.0, "avg_logprob": -0.17199921607971191, "compression_ratio": 1.547486033519553, "no_speech_prob": 4.83060757687781e-05}, {"id": 293, "seek": 134076, "start": 1364.44, "end": 1365.68, "text": " black line.", "tokens": [2211, 1622, 13], "temperature": 0.0, "avg_logprob": -0.17199921607971191, "compression_ratio": 1.547486033519553, "no_speech_prob": 4.83060757687781e-05}, {"id": 294, "seek": 134076, "start": 1365.68, "end": 1369.52, "text": " And that gives you a model size and an amount of training.", "tokens": [400, 300, 2709, 291, 257, 2316, 2744, 293, 364, 2372, 295, 3097, 13], "temperature": 0.0, "avg_logprob": -0.17199921607971191, "compression_ratio": 1.547486033519553, "no_speech_prob": 4.83060757687781e-05}, {"id": 295, "seek": 136952, "start": 1369.52, "end": 1376.52, "text": " And so you can do that for all of these different points on the x-axis.", "tokens": [400, 370, 291, 393, 360, 300, 337, 439, 295, 613, 819, 2793, 322, 264, 2031, 12, 24633, 13], "temperature": 0.0, "avg_logprob": -0.17301219364382187, "compression_ratio": 1.7466666666666666, "no_speech_prob": 1.3627615771838464e-05}, {"id": 296, "seek": 136952, "start": 1376.52, "end": 1379.96, "text": " And then for any given point on this x-axis that tells you a model size.", "tokens": [400, 550, 337, 604, 2212, 935, 322, 341, 2031, 12, 24633, 300, 5112, 291, 257, 2316, 2744, 13], "temperature": 0.0, "avg_logprob": -0.17301219364382187, "compression_ratio": 1.7466666666666666, "no_speech_prob": 1.3627615771838464e-05}, {"id": 297, "seek": 136952, "start": 1379.96, "end": 1384.24, "text": " You learn model size as a function of your compute budget.", "tokens": [509, 1466, 2316, 2744, 382, 257, 2445, 295, 428, 14722, 4706, 13], "temperature": 0.0, "avg_logprob": -0.17301219364382187, "compression_ratio": 1.7466666666666666, "no_speech_prob": 1.3627615771838464e-05}, {"id": 298, "seek": 136952, "start": 1384.24, "end": 1389.8, "text": " And then, conversely, you also learn an amount of training, which is sort of a data set size.", "tokens": [400, 550, 11, 2615, 736, 11, 291, 611, 1466, 364, 2372, 295, 3097, 11, 597, 307, 1333, 295, 257, 1412, 992, 2744, 13], "temperature": 0.0, "avg_logprob": -0.17301219364382187, "compression_ratio": 1.7466666666666666, "no_speech_prob": 1.3627615771838464e-05}, {"id": 299, "seek": 136952, "start": 1389.8, "end": 1396.48, "text": " And so that's the explanation for the sort of million x model size versus a thousand", "tokens": [400, 370, 300, 311, 264, 10835, 337, 264, 1333, 295, 2459, 2031, 2316, 2744, 5717, 257, 4714], "temperature": 0.0, "avg_logprob": -0.17301219364382187, "compression_ratio": 1.7466666666666666, "no_speech_prob": 1.3627615771838464e-05}, {"id": 300, "seek": 136952, "start": 1396.48, "end": 1398.8, "text": " x in data.", "tokens": [2031, 294, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17301219364382187, "compression_ratio": 1.7466666666666666, "no_speech_prob": 1.3627615771838464e-05}, {"id": 301, "seek": 139880, "start": 1398.8, "end": 1403.0, "text": " I probably won't try to explain the batch size question.", "tokens": [286, 1391, 1582, 380, 853, 281, 2903, 264, 15245, 2744, 1168, 13], "temperature": 0.0, "avg_logprob": -0.22833699446458083, "compression_ratio": 1.6381322957198443, "no_speech_prob": 2.709330146899447e-05}, {"id": 302, "seek": 139880, "start": 1403.0, "end": 1407.08, "text": " But it's basically based on some empirical analysis where you ask, how big can you make", "tokens": [583, 309, 311, 1936, 2361, 322, 512, 31886, 5215, 689, 291, 1029, 11, 577, 955, 393, 291, 652], "temperature": 0.0, "avg_logprob": -0.22833699446458083, "compression_ratio": 1.6381322957198443, "no_speech_prob": 2.709330146899447e-05}, {"id": 303, "seek": 139880, "start": 1407.08, "end": 1408.6399999999999, "text": " your batch size?", "tokens": [428, 15245, 2744, 30], "temperature": 0.0, "avg_logprob": -0.22833699446458083, "compression_ratio": 1.6381322957198443, "no_speech_prob": 2.709330146899447e-05}, {"id": 304, "seek": 139880, "start": 1408.6399999999999, "end": 1413.12, "text": " How far can you push data parallelism without seeing diminishing returns?", "tokens": [1012, 1400, 393, 291, 2944, 1412, 8952, 1434, 1553, 2577, 15739, 3807, 11247, 30], "temperature": 0.0, "avg_logprob": -0.22833699446458083, "compression_ratio": 1.6381322957198443, "no_speech_prob": 2.709330146899447e-05}, {"id": 305, "seek": 139880, "start": 1413.12, "end": 1417.44, "text": " And that's sort of the rough answer from that question.", "tokens": [400, 300, 311, 1333, 295, 264, 5903, 1867, 490, 300, 1168, 13], "temperature": 0.0, "avg_logprob": -0.22833699446458083, "compression_ratio": 1.6381322957198443, "no_speech_prob": 2.709330146899447e-05}, {"id": 306, "seek": 139880, "start": 1417.44, "end": 1421.72, "text": " They're all trained from scratch.", "tokens": [814, 434, 439, 8895, 490, 8459, 13], "temperature": 0.0, "avg_logprob": -0.22833699446458083, "compression_ratio": 1.6381322957198443, "no_speech_prob": 2.709330146899447e-05}, {"id": 307, "seek": 139880, "start": 1421.72, "end": 1426.12, "text": " So this is always almost everything that I'll talk about in this talk is training from", "tokens": [407, 341, 307, 1009, 1920, 1203, 300, 286, 603, 751, 466, 294, 341, 751, 307, 3097, 490], "temperature": 0.0, "avg_logprob": -0.22833699446458083, "compression_ratio": 1.6381322957198443, "no_speech_prob": 2.709330146899447e-05}, {"id": 308, "seek": 139880, "start": 1426.12, "end": 1427.12, "text": " scratch.", "tokens": [8459, 13], "temperature": 0.0, "avg_logprob": -0.22833699446458083, "compression_ratio": 1.6381322957198443, "no_speech_prob": 2.709330146899447e-05}, {"id": 309, "seek": 142712, "start": 1427.12, "end": 1431.6, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.23417069695212625, "compression_ratio": 1.5045871559633028, "no_speech_prob": 4.005309529020451e-05}, {"id": 310, "seek": 142712, "start": 1431.6, "end": 1434.6799999999998, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.23417069695212625, "compression_ratio": 1.5045871559633028, "no_speech_prob": 4.005309529020451e-05}, {"id": 311, "seek": 142712, "start": 1434.6799999999998, "end": 1438.9599999999998, "text": " And then another point that I mean, I don't want to overemphasize.", "tokens": [400, 550, 1071, 935, 300, 286, 914, 11, 286, 500, 380, 528, 281, 38657, 76, 7485, 1125, 13], "temperature": 0.0, "avg_logprob": -0.23417069695212625, "compression_ratio": 1.5045871559633028, "no_speech_prob": 4.005309529020451e-05}, {"id": 312, "seek": 142712, "start": 1438.9599999999998, "end": 1444.6799999999998, "text": " But like I said, from a sort of very zero-thorder naive perspective is that for some of these", "tokens": [583, 411, 286, 848, 11, 490, 257, 1333, 295, 588, 4018, 12, 392, 4687, 29052, 4585, 307, 300, 337, 512, 295, 613], "temperature": 0.0, "avg_logprob": -0.23417069695212625, "compression_ratio": 1.5045871559633028, "no_speech_prob": 4.005309529020451e-05}, {"id": 313, "seek": 142712, "start": 1444.6799999999998, "end": 1447.8799999999999, "text": " results, architecture isn't the most crucial thing.", "tokens": [3542, 11, 9482, 1943, 380, 264, 881, 11462, 551, 13], "temperature": 0.0, "avg_logprob": -0.23417069695212625, "compression_ratio": 1.5045871559633028, "no_speech_prob": 4.005309529020451e-05}, {"id": 314, "seek": 142712, "start": 1447.8799999999999, "end": 1453.84, "text": " So I think one of the biggest advances in machine learning in the last five or ten years", "tokens": [407, 286, 519, 472, 295, 264, 3880, 25297, 294, 3479, 2539, 294, 264, 1036, 1732, 420, 2064, 924], "temperature": 0.0, "avg_logprob": -0.23417069695212625, "compression_ratio": 1.5045871559633028, "no_speech_prob": 4.005309529020451e-05}, {"id": 315, "seek": 145384, "start": 1453.84, "end": 1457.84, "text": " has been the development of the transformer models that I'm talking about.", "tokens": [575, 668, 264, 3250, 295, 264, 31782, 5245, 300, 286, 478, 1417, 466, 13], "temperature": 0.0, "avg_logprob": -0.12108088422704626, "compression_ratio": 1.6896551724137931, "no_speech_prob": 5.648692967952229e-05}, {"id": 316, "seek": 145384, "start": 1457.84, "end": 1462.24, "text": " But of course, you can do language modeling with a recurrent model that reads words in", "tokens": [583, 295, 1164, 11, 291, 393, 360, 2856, 15983, 365, 257, 18680, 1753, 2316, 300, 15700, 2283, 294], "temperature": 0.0, "avg_logprob": -0.12108088422704626, "compression_ratio": 1.6896551724137931, "no_speech_prob": 5.648692967952229e-05}, {"id": 317, "seek": 145384, "start": 1462.24, "end": 1464.24, "text": " order.", "tokens": [1668, 13], "temperature": 0.0, "avg_logprob": -0.12108088422704626, "compression_ratio": 1.6896551724137931, "no_speech_prob": 5.648692967952229e-05}, {"id": 318, "seek": 145384, "start": 1464.24, "end": 1470.6, "text": " And of course, LSTMs or stacked LSTMs are sort of the standard way to do that.", "tokens": [400, 295, 1164, 11, 441, 6840, 26386, 420, 28867, 441, 6840, 26386, 366, 1333, 295, 264, 3832, 636, 281, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.12108088422704626, "compression_ratio": 1.6896551724137931, "no_speech_prob": 5.648692967952229e-05}, {"id": 319, "seek": 145384, "start": 1470.6, "end": 1476.0, "text": " And so you can compare what you actually get if you study LSTMs versus transformers.", "tokens": [400, 370, 291, 393, 6794, 437, 291, 767, 483, 498, 291, 2979, 441, 6840, 26386, 5717, 4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.12108088422704626, "compression_ratio": 1.6896551724137931, "no_speech_prob": 5.648692967952229e-05}, {"id": 320, "seek": 145384, "start": 1476.0, "end": 1480.36, "text": " And at zero-thorder, it doesn't seem like LSTMs are so bad.", "tokens": [400, 412, 4018, 12, 392, 4687, 11, 309, 1177, 380, 1643, 411, 441, 6840, 26386, 366, 370, 1578, 13], "temperature": 0.0, "avg_logprob": -0.12108088422704626, "compression_ratio": 1.6896551724137931, "no_speech_prob": 5.648692967952229e-05}, {"id": 321, "seek": 148036, "start": 1480.36, "end": 1484.4799999999998, "text": " It looks like as you make them bigger, they are scaling up quite nicely.", "tokens": [467, 1542, 411, 382, 291, 652, 552, 3801, 11, 436, 366, 21589, 493, 1596, 9594, 13], "temperature": 0.0, "avg_logprob": -0.18094970703125, "compression_ratio": 1.7389830508474575, "no_speech_prob": 2.9308097509783693e-05}, {"id": 322, "seek": 148036, "start": 1484.4799999999998, "end": 1488.4799999999998, "text": " But there's basically a constant offset where transformers are something like five or ten", "tokens": [583, 456, 311, 1936, 257, 5754, 18687, 689, 4088, 433, 366, 746, 411, 1732, 420, 2064], "temperature": 0.0, "avg_logprob": -0.18094970703125, "compression_ratio": 1.7389830508474575, "no_speech_prob": 2.9308097509783693e-05}, {"id": 323, "seek": 148036, "start": 1488.4799999999998, "end": 1493.36, "text": " times more efficient for a given model size than LSTMs.", "tokens": [1413, 544, 7148, 337, 257, 2212, 2316, 2744, 813, 441, 6840, 26386, 13], "temperature": 0.0, "avg_logprob": -0.18094970703125, "compression_ratio": 1.7389830508474575, "no_speech_prob": 2.9308097509783693e-05}, {"id": 324, "seek": 148036, "start": 1493.36, "end": 1496.9199999999998, "text": " And so I think this is a very, very convincing plot that tells you the transformers are in", "tokens": [400, 370, 286, 519, 341, 307, 257, 588, 11, 588, 24823, 7542, 300, 5112, 291, 264, 4088, 433, 366, 294], "temperature": 0.0, "avg_logprob": -0.18094970703125, "compression_ratio": 1.7389830508474575, "no_speech_prob": 2.9308097509783693e-05}, {"id": 325, "seek": 148036, "start": 1496.9199999999998, "end": 1498.0, "text": " fact better.", "tokens": [1186, 1101, 13], "temperature": 0.0, "avg_logprob": -0.18094970703125, "compression_ratio": 1.7389830508474575, "no_speech_prob": 2.9308097509783693e-05}, {"id": 326, "seek": 148036, "start": 1498.0, "end": 1503.6399999999999, "text": " But you don't necessarily need a transformer to see that making models bigger is giving", "tokens": [583, 291, 500, 380, 4725, 643, 257, 31782, 281, 536, 300, 1455, 5245, 3801, 307, 2902], "temperature": 0.0, "avg_logprob": -0.18094970703125, "compression_ratio": 1.7389830508474575, "no_speech_prob": 2.9308097509783693e-05}, {"id": 327, "seek": 148036, "start": 1503.6399999999999, "end": 1505.04, "text": " you in.", "tokens": [291, 294, 13], "temperature": 0.0, "avg_logprob": -0.18094970703125, "compression_ratio": 1.7389830508474575, "no_speech_prob": 2.9308097509783693e-05}, {"id": 328, "seek": 148036, "start": 1505.04, "end": 1510.1599999999999, "text": " And really the sort of more interesting limitation of LSTMs that I'll also talk about a little", "tokens": [400, 534, 264, 1333, 295, 544, 1880, 27432, 295, 441, 6840, 26386, 300, 286, 603, 611, 751, 466, 257, 707], "temperature": 0.0, "avg_logprob": -0.18094970703125, "compression_ratio": 1.7389830508474575, "no_speech_prob": 2.9308097509783693e-05}, {"id": 329, "seek": 151016, "start": 1510.16, "end": 1513.4, "text": " more later is if we plot something else.", "tokens": [544, 1780, 307, 498, 321, 7542, 746, 1646, 13], "temperature": 0.0, "avg_logprob": -0.12414990862210591, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.828273424413055e-05}, {"id": 330, "seek": 151016, "start": 1513.4, "end": 1521.1200000000001, "text": " So if we look at a thousand tokens, which is something like 600 words of context, we", "tokens": [407, 498, 321, 574, 412, 257, 4714, 22667, 11, 597, 307, 746, 411, 11849, 2283, 295, 4319, 11, 321], "temperature": 0.0, "avg_logprob": -0.12414990862210591, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.828273424413055e-05}, {"id": 331, "seek": 151016, "start": 1521.1200000000001, "end": 1525.52, "text": " can look at what the loss is as a function of the position in the context.", "tokens": [393, 574, 412, 437, 264, 4470, 307, 382, 257, 2445, 295, 264, 2535, 294, 264, 4319, 13], "temperature": 0.0, "avg_logprob": -0.12414990862210591, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.828273424413055e-05}, {"id": 332, "seek": 151016, "start": 1525.52, "end": 1530.3200000000002, "text": " Because if you've read more of a document already, you're going to be better at predicting", "tokens": [1436, 498, 291, 600, 1401, 544, 295, 257, 4166, 1217, 11, 291, 434, 516, 281, 312, 1101, 412, 32884], "temperature": 0.0, "avg_logprob": -0.12414990862210591, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.828273424413055e-05}, {"id": 333, "seek": 151016, "start": 1530.3200000000002, "end": 1533.16, "text": " what the next word is because you have more context available.", "tokens": [437, 264, 958, 1349, 307, 570, 291, 362, 544, 4319, 2435, 13], "temperature": 0.0, "avg_logprob": -0.12414990862210591, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.828273424413055e-05}, {"id": 334, "seek": 151016, "start": 1533.16, "end": 1534.16, "text": " And they're very smooth.", "tokens": [400, 436, 434, 588, 5508, 13], "temperature": 0.0, "avg_logprob": -0.12414990862210591, "compression_ratio": 1.6478260869565218, "no_speech_prob": 5.828273424413055e-05}, {"id": 335, "seek": 153416, "start": 1534.16, "end": 1541.2, "text": " It turns out also power law curves for the loss as a function of context position.", "tokens": [467, 4523, 484, 611, 1347, 2101, 19490, 337, 264, 4470, 382, 257, 2445, 295, 4319, 2535, 13], "temperature": 0.0, "avg_logprob": -0.13975125481100642, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.7775721062207595e-05}, {"id": 336, "seek": 153416, "start": 1541.2, "end": 1547.0400000000002, "text": " But the thing that you notice is that the red lines are LSTMs and the blue lines are", "tokens": [583, 264, 551, 300, 291, 3449, 307, 300, 264, 2182, 3876, 366, 441, 6840, 26386, 293, 264, 3344, 3876, 366], "temperature": 0.0, "avg_logprob": -0.13975125481100642, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.7775721062207595e-05}, {"id": 337, "seek": 153416, "start": 1547.0400000000002, "end": 1548.28, "text": " transformers.", "tokens": [4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.13975125481100642, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.7775721062207595e-05}, {"id": 338, "seek": 153416, "start": 1548.28, "end": 1554.92, "text": " And LSTMs tend to sort of plateau in performance after on the order of a hundred tokens.", "tokens": [400, 441, 6840, 26386, 3928, 281, 1333, 295, 39885, 294, 3389, 934, 322, 264, 1668, 295, 257, 3262, 22667, 13], "temperature": 0.0, "avg_logprob": -0.13975125481100642, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.7775721062207595e-05}, {"id": 339, "seek": 153416, "start": 1554.92, "end": 1560.4, "text": " And this is sort of another bottleneck in a different direction.", "tokens": [400, 341, 307, 1333, 295, 1071, 44641, 547, 294, 257, 819, 3513, 13], "temperature": 0.0, "avg_logprob": -0.13975125481100642, "compression_ratio": 1.580188679245283, "no_speech_prob": 1.7775721062207595e-05}, {"id": 340, "seek": 156040, "start": 1560.4, "end": 1565.0800000000002, "text": " This is the famous fact that transformers are much better at learning long context information.", "tokens": [639, 307, 264, 4618, 1186, 300, 4088, 433, 366, 709, 1101, 412, 2539, 938, 4319, 1589, 13], "temperature": 0.0, "avg_logprob": -0.1434330827607883, "compression_ratio": 1.7156549520766773, "no_speech_prob": 4.985346458852291e-05}, {"id": 341, "seek": 156040, "start": 1565.0800000000002, "end": 1568.1200000000001, "text": " And this is obviously a limitation of LSTMs.", "tokens": [400, 341, 307, 2745, 257, 27432, 295, 441, 6840, 26386, 13], "temperature": 0.0, "avg_logprob": -0.1434330827607883, "compression_ratio": 1.7156549520766773, "no_speech_prob": 4.985346458852291e-05}, {"id": 342, "seek": 156040, "start": 1568.1200000000001, "end": 1574.0800000000002, "text": " But sort of the basic parameter scaling law seems like it holds for many architectures.", "tokens": [583, 1333, 295, 264, 3875, 13075, 21589, 2101, 2544, 411, 309, 9190, 337, 867, 6331, 1303, 13], "temperature": 0.0, "avg_logprob": -0.1434330827607883, "compression_ratio": 1.7156549520766773, "no_speech_prob": 4.985346458852291e-05}, {"id": 343, "seek": 156040, "start": 1574.0800000000002, "end": 1575.6000000000001, "text": " And then there are much more refined questions.", "tokens": [400, 550, 456, 366, 709, 544, 26201, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1434330827607883, "compression_ratio": 1.7156549520766773, "no_speech_prob": 4.985346458852291e-05}, {"id": 344, "seek": 156040, "start": 1575.6000000000001, "end": 1578.16, "text": " You can ask, I won't go into too much detail on this.", "tokens": [509, 393, 1029, 11, 286, 1582, 380, 352, 666, 886, 709, 2607, 322, 341, 13], "temperature": 0.0, "avg_logprob": -0.1434330827607883, "compression_ratio": 1.7156549520766773, "no_speech_prob": 4.985346458852291e-05}, {"id": 345, "seek": 156040, "start": 1578.16, "end": 1581.8000000000002, "text": " But there are all sorts of hyper parameters in transformer models.", "tokens": [583, 456, 366, 439, 7527, 295, 9848, 9834, 294, 31782, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1434330827607883, "compression_ratio": 1.7156549520766773, "no_speech_prob": 4.985346458852291e-05}, {"id": 346, "seek": 156040, "start": 1581.8000000000002, "end": 1585.3200000000002, "text": " And you might ask how much does it matter if I really optimize those?", "tokens": [400, 291, 1062, 1029, 577, 709, 775, 309, 1871, 498, 286, 534, 19719, 729, 30], "temperature": 0.0, "avg_logprob": -0.1434330827607883, "compression_ratio": 1.7156549520766773, "no_speech_prob": 4.985346458852291e-05}, {"id": 347, "seek": 156040, "start": 1585.3200000000002, "end": 1588.92, "text": " Do I get qualitatively different behavior if I optimize those better?", "tokens": [1144, 286, 483, 31312, 356, 819, 5223, 498, 286, 19719, 729, 1101, 30], "temperature": 0.0, "avg_logprob": -0.1434330827607883, "compression_ratio": 1.7156549520766773, "no_speech_prob": 4.985346458852291e-05}, {"id": 348, "seek": 158892, "start": 1588.92, "end": 1593.52, "text": " And what all of these plots show is that for various different kinds of hyper parameters", "tokens": [400, 437, 439, 295, 613, 28609, 855, 307, 300, 337, 3683, 819, 3685, 295, 9848, 9834], "temperature": 0.0, "avg_logprob": -0.16283843391820005, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.6683845135266893e-05}, {"id": 349, "seek": 158892, "start": 1593.52, "end": 1598.6000000000001, "text": " and transformer models, there's some broad basin where you get quite good performance.", "tokens": [293, 31782, 5245, 11, 456, 311, 512, 4152, 34863, 689, 291, 483, 1596, 665, 3389, 13], "temperature": 0.0, "avg_logprob": -0.16283843391820005, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.6683845135266893e-05}, {"id": 350, "seek": 158892, "start": 1598.6000000000001, "end": 1602.5600000000002, "text": " I mean, maybe a factor of three in either direction where performance doesn't change", "tokens": [286, 914, 11, 1310, 257, 5952, 295, 1045, 294, 2139, 3513, 689, 3389, 1177, 380, 1319], "temperature": 0.0, "avg_logprob": -0.16283843391820005, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.6683845135266893e-05}, {"id": 351, "seek": 158892, "start": 1602.5600000000002, "end": 1603.5600000000002, "text": " all that much.", "tokens": [439, 300, 709, 13], "temperature": 0.0, "avg_logprob": -0.16283843391820005, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.6683845135266893e-05}, {"id": 352, "seek": 158892, "start": 1603.5600000000002, "end": 1608.2, "text": " Of course, you might want to optimize that I'm not saying you shouldn't, but kind of qualitatively", "tokens": [2720, 1164, 11, 291, 1062, 528, 281, 19719, 300, 286, 478, 406, 1566, 291, 4659, 380, 11, 457, 733, 295, 31312, 356], "temperature": 0.0, "avg_logprob": -0.16283843391820005, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.6683845135266893e-05}, {"id": 353, "seek": 158892, "start": 1608.2, "end": 1613.5600000000002, "text": " it's not an enormous difference.", "tokens": [309, 311, 406, 364, 11322, 2649, 13], "temperature": 0.0, "avg_logprob": -0.16283843391820005, "compression_ratio": 1.608695652173913, "no_speech_prob": 2.6683845135266893e-05}, {"id": 354, "seek": 161356, "start": 1613.56, "end": 1619.3999999999999, "text": " So I think this is also a place where it's, so I'm going to tell you in a few slides", "tokens": [407, 286, 519, 341, 307, 611, 257, 1081, 689, 309, 311, 11, 370, 286, 478, 516, 281, 980, 291, 294, 257, 1326, 9788], "temperature": 0.0, "avg_logprob": -0.1030694580078125, "compression_ratio": 1.5360824742268042, "no_speech_prob": 7.83782743383199e-05}, {"id": 355, "seek": 161356, "start": 1619.3999999999999, "end": 1624.6, "text": " that a lot of these features are true more generally beyond language.", "tokens": [300, 257, 688, 295, 613, 4122, 366, 2074, 544, 5101, 4399, 2856, 13], "temperature": 0.0, "avg_logprob": -0.1030694580078125, "compression_ratio": 1.5360824742268042, "no_speech_prob": 7.83782743383199e-05}, {"id": 356, "seek": 161356, "start": 1624.6, "end": 1631.72, "text": " And they really sort of say that much of what's going on when machines learn is quite universal.", "tokens": [400, 436, 534, 1333, 295, 584, 300, 709, 295, 437, 311, 516, 322, 562, 8379, 1466, 307, 1596, 11455, 13], "temperature": 0.0, "avg_logprob": -0.1030694580078125, "compression_ratio": 1.5360824742268042, "no_speech_prob": 7.83782743383199e-05}, {"id": 357, "seek": 161356, "start": 1631.72, "end": 1634.3999999999999, "text": " But there are features that are not universal.", "tokens": [583, 456, 366, 4122, 300, 366, 406, 11455, 13], "temperature": 0.0, "avg_logprob": -0.1030694580078125, "compression_ratio": 1.5360824742268042, "no_speech_prob": 7.83782743383199e-05}, {"id": 358, "seek": 163440, "start": 1634.4, "end": 1642.6000000000001, "text": " So this is kind of a nicer plot of loss versus token index.", "tokens": [407, 341, 307, 733, 295, 257, 22842, 7542, 295, 4470, 5717, 14862, 8186, 13], "temperature": 0.0, "avg_logprob": -0.1460614626920676, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.329005241743289e-05}, {"id": 359, "seek": 163440, "start": 1642.6000000000001, "end": 1649.24, "text": " And I've included some power law fits, which are dotted lines, which show that this is", "tokens": [400, 286, 600, 5556, 512, 1347, 2101, 9001, 11, 597, 366, 37459, 3876, 11, 597, 855, 300, 341, 307], "temperature": 0.0, "avg_logprob": -0.1460614626920676, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.329005241743289e-05}, {"id": 360, "seek": 163440, "start": 1649.24, "end": 1654.24, "text": " actually, this performance is also highly predictable.", "tokens": [767, 11, 341, 3389, 307, 611, 5405, 27737, 13], "temperature": 0.0, "avg_logprob": -0.1460614626920676, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.329005241743289e-05}, {"id": 361, "seek": 163440, "start": 1654.24, "end": 1658.0, "text": " That just says the obvious that when you read more, you understand it's easier for you", "tokens": [663, 445, 1619, 264, 6322, 300, 562, 291, 1401, 544, 11, 291, 1223, 309, 311, 3571, 337, 291], "temperature": 0.0, "avg_logprob": -0.1460614626920676, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.329005241743289e-05}, {"id": 362, "seek": 163440, "start": 1658.0, "end": 1660.2800000000002, "text": " to predict what's coming next.", "tokens": [281, 6069, 437, 311, 1348, 958, 13], "temperature": 0.0, "avg_logprob": -0.1460614626920676, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.329005241743289e-05}, {"id": 363, "seek": 166028, "start": 1660.28, "end": 1665.8, "text": " But you can train models on images, I'll briefly talk about that later, you can train models", "tokens": [583, 291, 393, 3847, 5245, 322, 5267, 11, 286, 603, 10515, 751, 466, 300, 1780, 11, 291, 393, 3847, 5245], "temperature": 0.0, "avg_logprob": -0.1638770184274447, "compression_ratio": 1.7300380228136882, "no_speech_prob": 3.943952833651565e-05}, {"id": 364, "seek": 166028, "start": 1665.8, "end": 1668.24, "text": " identically on images.", "tokens": [2473, 984, 322, 5267, 13], "temperature": 0.0, "avg_logprob": -0.1638770184274447, "compression_ratio": 1.7300380228136882, "no_speech_prob": 3.943952833651565e-05}, {"id": 365, "seek": 166028, "start": 1668.24, "end": 1671.68, "text": " And there you see a performance as a function of context position.", "tokens": [400, 456, 291, 536, 257, 3389, 382, 257, 2445, 295, 4319, 2535, 13], "temperature": 0.0, "avg_logprob": -0.1638770184274447, "compression_ratio": 1.7300380228136882, "no_speech_prob": 3.943952833651565e-05}, {"id": 366, "seek": 166028, "start": 1671.68, "end": 1672.68, "text": " It's very different.", "tokens": [467, 311, 588, 819, 13], "temperature": 0.0, "avg_logprob": -0.1638770184274447, "compression_ratio": 1.7300380228136882, "no_speech_prob": 3.943952833651565e-05}, {"id": 367, "seek": 166028, "start": 1672.68, "end": 1678.32, "text": " So here you have a model that reads pixels row by row.", "tokens": [407, 510, 291, 362, 257, 2316, 300, 15700, 18668, 5386, 538, 5386, 13], "temperature": 0.0, "avg_logprob": -0.1638770184274447, "compression_ratio": 1.7300380228136882, "no_speech_prob": 3.943952833651565e-05}, {"id": 368, "seek": 166028, "start": 1678.32, "end": 1682.72, "text": " And as you might expect, there's usually much more non-tribule stuff going on in the", "tokens": [400, 382, 291, 1062, 2066, 11, 456, 311, 2673, 709, 544, 2107, 12, 83, 2024, 2271, 1507, 516, 322, 294, 264], "temperature": 0.0, "avg_logprob": -0.1638770184274447, "compression_ratio": 1.7300380228136882, "no_speech_prob": 3.943952833651565e-05}, {"id": 369, "seek": 166028, "start": 1682.72, "end": 1685.48, "text": " middle of an image rather than in the background.", "tokens": [2808, 295, 364, 3256, 2831, 813, 294, 264, 3678, 13], "temperature": 0.0, "avg_logprob": -0.1638770184274447, "compression_ratio": 1.7300380228136882, "no_speech_prob": 3.943952833651565e-05}, {"id": 370, "seek": 166028, "start": 1685.48, "end": 1688.6, "text": " And that's represented by the fact that models do much worse.", "tokens": [400, 300, 311, 10379, 538, 264, 1186, 300, 5245, 360, 709, 5324, 13], "temperature": 0.0, "avg_logprob": -0.1638770184274447, "compression_ratio": 1.7300380228136882, "no_speech_prob": 3.943952833651565e-05}, {"id": 371, "seek": 168860, "start": 1688.6, "end": 1692.52, "text": " Their loss is higher in the center of images as compared to junior-of-the-edges.", "tokens": [6710, 4470, 307, 2946, 294, 264, 3056, 295, 5267, 382, 5347, 281, 16195, 12, 2670, 12, 3322, 12, 292, 2880, 13], "temperature": 0.0, "avg_logprob": -0.20215421404157366, "compression_ratio": 1.623134328358209, "no_speech_prob": 2.626443892950192e-05}, {"id": 372, "seek": 168860, "start": 1692.52, "end": 1697.6799999999998, "text": " So while some properties of transformers and language models are universal, and I'll", "tokens": [407, 1339, 512, 7221, 295, 4088, 433, 293, 2856, 5245, 366, 11455, 11, 293, 286, 603], "temperature": 0.0, "avg_logprob": -0.20215421404157366, "compression_ratio": 1.623134328358209, "no_speech_prob": 2.626443892950192e-05}, {"id": 373, "seek": 168860, "start": 1697.6799999999998, "end": 1702.76, "text": " talk about those later on, there are features of language data that are totally different", "tokens": [751, 466, 729, 1780, 322, 11, 456, 366, 4122, 295, 2856, 1412, 300, 366, 3879, 819], "temperature": 0.0, "avg_logprob": -0.20215421404157366, "compression_ratio": 1.623134328358209, "no_speech_prob": 2.626443892950192e-05}, {"id": 374, "seek": 168860, "start": 1702.76, "end": 1704.6, "text": " from other data distributions.", "tokens": [490, 661, 1412, 37870, 13], "temperature": 0.0, "avg_logprob": -0.20215421404157366, "compression_ratio": 1.623134328358209, "no_speech_prob": 2.626443892950192e-05}, {"id": 375, "seek": 168860, "start": 1704.6, "end": 1710.52, "text": " And this is a very stark example of that.", "tokens": [400, 341, 307, 257, 588, 17417, 1365, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.20215421404157366, "compression_ratio": 1.623134328358209, "no_speech_prob": 2.626443892950192e-05}, {"id": 376, "seek": 168860, "start": 1710.52, "end": 1715.76, "text": " But generally, the fact that there are these kinds of nice patterns lurking whenever you", "tokens": [583, 5101, 11, 264, 1186, 300, 456, 366, 613, 3685, 295, 1481, 8294, 35583, 5092, 5699, 291], "temperature": 0.0, "avg_logprob": -0.20215421404157366, "compression_ratio": 1.623134328358209, "no_speech_prob": 2.626443892950192e-05}, {"id": 377, "seek": 168860, "start": 1715.76, "end": 1717.0, "text": " optimize a model.", "tokens": [19719, 257, 2316, 13], "temperature": 0.0, "avg_logprob": -0.20215421404157366, "compression_ratio": 1.623134328358209, "no_speech_prob": 2.626443892950192e-05}, {"id": 378, "seek": 171700, "start": 1717.0, "end": 1719.6, "text": " I think that is very common.", "tokens": [286, 519, 300, 307, 588, 2689, 13], "temperature": 0.0, "avg_logprob": -0.3638261351922546, "compression_ratio": 1.6271929824561404, "no_speech_prob": 3.424232272664085e-05}, {"id": 379, "seek": 171700, "start": 1719.6, "end": 1722.44, "text": " So any questions about this?", "tokens": [407, 604, 1651, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.3638261351922546, "compression_ratio": 1.6271929824561404, "no_speech_prob": 3.424232272664085e-05}, {"id": 380, "seek": 171700, "start": 1722.44, "end": 1723.44, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3638261351922546, "compression_ratio": 1.6271929824561404, "no_speech_prob": 3.424232272664085e-05}, {"id": 381, "seek": 171700, "start": 1723.44, "end": 1725.12, "text": " Do you mind going back to slide?", "tokens": [1144, 291, 1575, 516, 646, 281, 4137, 30], "temperature": 0.0, "avg_logprob": -0.3638261351922546, "compression_ratio": 1.6271929824561404, "no_speech_prob": 3.424232272664085e-05}, {"id": 382, "seek": 171700, "start": 1725.12, "end": 1732.4, "text": " Do you mind explaining what it means to have a loss on the first template versus the", "tokens": [1144, 291, 1575, 13468, 437, 309, 1355, 281, 362, 257, 4470, 322, 264, 700, 12379, 5717, 264], "temperature": 0.0, "avg_logprob": -0.3638261351922546, "compression_ratio": 1.6271929824561404, "no_speech_prob": 3.424232272664085e-05}, {"id": 383, "seek": 171700, "start": 1732.4, "end": 1734.4, "text": " 1000 template?", "tokens": [9714, 12379, 30], "temperature": 0.0, "avg_logprob": -0.3638261351922546, "compression_ratio": 1.6271929824561404, "no_speech_prob": 3.424232272664085e-05}, {"id": 384, "seek": 171700, "start": 1734.4, "end": 1741.12, "text": " Yeah, so if you imagine you have a thousand words extracted randomly from a book, then", "tokens": [865, 11, 370, 498, 291, 3811, 291, 362, 257, 4714, 2283, 34086, 16979, 490, 257, 1446, 11, 550], "temperature": 0.0, "avg_logprob": -0.3638261351922546, "compression_ratio": 1.6271929824561404, "no_speech_prob": 3.424232272664085e-05}, {"id": 385, "seek": 171700, "start": 1741.12, "end": 1745.92, "text": " the very first thing you can ask the model to do is try to predict the very first word.", "tokens": [264, 588, 700, 551, 291, 393, 1029, 264, 2316, 281, 360, 307, 853, 281, 6069, 264, 588, 700, 1349, 13], "temperature": 0.0, "avg_logprob": -0.3638261351922546, "compression_ratio": 1.6271929824561404, "no_speech_prob": 3.424232272664085e-05}, {"id": 386, "seek": 174592, "start": 1745.92, "end": 1749.8000000000002, "text": " Then you ask it to predict the second word, the third word, et cetera.", "tokens": [1396, 291, 1029, 309, 281, 6069, 264, 1150, 1349, 11, 264, 2636, 1349, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1661143974021629, "compression_ratio": 1.7086092715231789, "no_speech_prob": 0.00014877716603223234}, {"id": 387, "seek": 174592, "start": 1749.8000000000002, "end": 1755.3600000000001, "text": " The very first word basically all the model can possibly do is predict the unigram distribution", "tokens": [440, 588, 700, 1349, 1936, 439, 264, 2316, 393, 6264, 360, 307, 6069, 264, 517, 33737, 7316], "temperature": 0.0, "avg_logprob": -0.1661143974021629, "compression_ratio": 1.7086092715231789, "no_speech_prob": 0.00014877716603223234}, {"id": 388, "seek": 174592, "start": 1755.3600000000001, "end": 1756.44, "text": " for its training set.", "tokens": [337, 1080, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.1661143974021629, "compression_ratio": 1.7086092715231789, "no_speech_prob": 0.00014877716603223234}, {"id": 389, "seek": 174592, "start": 1756.44, "end": 1760.68, "text": " It just doesn't have any information to go on, otherwise to predict what's happening.", "tokens": [467, 445, 1177, 380, 362, 604, 1589, 281, 352, 322, 11, 5911, 281, 6069, 437, 311, 2737, 13], "temperature": 0.0, "avg_logprob": -0.1661143974021629, "compression_ratio": 1.7086092715231789, "no_speech_prob": 0.00014877716603223234}, {"id": 390, "seek": 174592, "start": 1760.68, "end": 1763.52, "text": " And so that's why it's lost is very high.", "tokens": [400, 370, 300, 311, 983, 309, 311, 2731, 307, 588, 1090, 13], "temperature": 0.0, "avg_logprob": -0.1661143974021629, "compression_ratio": 1.7086092715231789, "no_speech_prob": 0.00014877716603223234}, {"id": 391, "seek": 174592, "start": 1763.52, "end": 1768.68, "text": " But by the time you get to the end of the passage, you've read a lot of some little short story,", "tokens": [583, 538, 264, 565, 291, 483, 281, 264, 917, 295, 264, 11497, 11, 291, 600, 1401, 257, 688, 295, 512, 707, 2099, 1657, 11], "temperature": 0.0, "avg_logprob": -0.1661143974021629, "compression_ratio": 1.7086092715231789, "no_speech_prob": 0.00014877716603223234}, {"id": 392, "seek": 174592, "start": 1768.68, "end": 1770.2, "text": " and you know a lot about what's going to happen.", "tokens": [293, 291, 458, 257, 688, 466, 437, 311, 516, 281, 1051, 13], "temperature": 0.0, "avg_logprob": -0.1661143974021629, "compression_ratio": 1.7086092715231789, "no_speech_prob": 0.00014877716603223234}, {"id": 393, "seek": 174592, "start": 1770.2, "end": 1772.88, "text": " You know what kinds of words are likely to come next.", "tokens": [509, 458, 437, 3685, 295, 2283, 366, 3700, 281, 808, 958, 13], "temperature": 0.0, "avg_logprob": -0.1661143974021629, "compression_ratio": 1.7086092715231789, "no_speech_prob": 0.00014877716603223234}, {"id": 394, "seek": 177288, "start": 1772.88, "end": 1775.7600000000002, "text": " You know about the author's style and vocabulary.", "tokens": [509, 458, 466, 264, 3793, 311, 3758, 293, 19864, 13], "temperature": 0.0, "avg_logprob": -0.1720077017079229, "compression_ratio": 1.7888446215139442, "no_speech_prob": 2.840271008608397e-05}, {"id": 395, "seek": 177288, "start": 1775.7600000000002, "end": 1778.72, "text": " You know about what characters exist, et cetera.", "tokens": [509, 458, 466, 437, 4342, 2514, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1720077017079229, "compression_ratio": 1.7888446215139442, "no_speech_prob": 2.840271008608397e-05}, {"id": 396, "seek": 177288, "start": 1778.72, "end": 1783.88, "text": " And so your model has gotten much, much better at prediction by the end of the context.", "tokens": [400, 370, 428, 2316, 575, 5768, 709, 11, 709, 1101, 412, 17630, 538, 264, 917, 295, 264, 4319, 13], "temperature": 0.0, "avg_logprob": -0.1720077017079229, "compression_ratio": 1.7888446215139442, "no_speech_prob": 2.840271008608397e-05}, {"id": 397, "seek": 177288, "start": 1783.88, "end": 1788.6000000000001, "text": " And so literally to make this plot, you take maybe a thousand, ten thousand different", "tokens": [400, 370, 3736, 281, 652, 341, 7542, 11, 291, 747, 1310, 257, 4714, 11, 2064, 4714, 819], "temperature": 0.0, "avg_logprob": -0.1720077017079229, "compression_ratio": 1.7888446215139442, "no_speech_prob": 2.840271008608397e-05}, {"id": 398, "seek": 177288, "start": 1788.6000000000001, "end": 1791.2800000000002, "text": " passages with a thousand words in them.", "tokens": [31589, 365, 257, 4714, 2283, 294, 552, 13], "temperature": 0.0, "avg_logprob": -0.1720077017079229, "compression_ratio": 1.7888446215139442, "no_speech_prob": 2.840271008608397e-05}, {"id": 399, "seek": 177288, "start": 1791.2800000000002, "end": 1795.44, "text": " You compute the model's loss on all of the words in the passage, and then you take the", "tokens": [509, 14722, 264, 2316, 311, 4470, 322, 439, 295, 264, 2283, 294, 264, 11497, 11, 293, 550, 291, 747, 264], "temperature": 0.0, "avg_logprob": -0.1720077017079229, "compression_ratio": 1.7888446215139442, "no_speech_prob": 2.840271008608397e-05}, {"id": 400, "seek": 177288, "start": 1795.44, "end": 1797.8000000000002, "text": " mean, and you get some nice plot like this.", "tokens": [914, 11, 293, 291, 483, 512, 1481, 7542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.1720077017079229, "compression_ratio": 1.7888446215139442, "no_speech_prob": 2.840271008608397e-05}, {"id": 401, "seek": 177288, "start": 1797.8000000000002, "end": 1798.8000000000002, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.1720077017079229, "compression_ratio": 1.7888446215139442, "no_speech_prob": 2.840271008608397e-05}, {"id": 402, "seek": 179880, "start": 1798.8, "end": 1808.8, "text": " But because the computation complexity is quite tragic with respect to token index, without", "tokens": [583, 570, 264, 24903, 14024, 307, 1596, 20385, 365, 3104, 281, 14862, 8186, 11, 1553], "temperature": 0.0, "avg_logprob": -0.43015971464269304, "compression_ratio": 1.6729857819905214, "no_speech_prob": 9.594030416337773e-05}, {"id": 403, "seek": 179880, "start": 1808.8, "end": 1812.56, "text": " being that essentially, for talking about it, if you think about it, if you were looking", "tokens": [885, 300, 4476, 11, 337, 1417, 466, 309, 11, 498, 291, 519, 466, 309, 11, 498, 291, 645, 1237], "temperature": 0.0, "avg_logprob": -0.43015971464269304, "compression_ratio": 1.6729857819905214, "no_speech_prob": 9.594030416337773e-05}, {"id": 404, "seek": 179880, "start": 1812.56, "end": 1818.9199999999998, "text": " at compute, then it would go from the zero to like ten to the six.", "tokens": [412, 14722, 11, 550, 309, 576, 352, 490, 264, 4018, 281, 411, 2064, 281, 264, 2309, 13], "temperature": 0.0, "avg_logprob": -0.43015971464269304, "compression_ratio": 1.6729857819905214, "no_speech_prob": 9.594030416337773e-05}, {"id": 405, "seek": 179880, "start": 1818.9199999999998, "end": 1827.48, "text": " So you know, you know, significantly greater compute, for a given test gloss as you increase", "tokens": [407, 291, 458, 11, 291, 458, 11, 10591, 5044, 14722, 11, 337, 257, 2212, 1500, 19574, 382, 291, 3488], "temperature": 0.0, "avg_logprob": -0.43015971464269304, "compression_ratio": 1.6729857819905214, "no_speech_prob": 9.594030416337773e-05}, {"id": 406, "seek": 179880, "start": 1827.48, "end": 1828.48, "text": " token index.", "tokens": [14862, 8186, 13], "temperature": 0.0, "avg_logprob": -0.43015971464269304, "compression_ratio": 1.6729857819905214, "no_speech_prob": 9.594030416337773e-05}, {"id": 407, "seek": 182848, "start": 1828.48, "end": 1833.32, "text": " So it's true that if you make the context length longer, longer, you will spend somewhat", "tokens": [407, 309, 311, 2074, 300, 498, 291, 652, 264, 4319, 4641, 2854, 11, 2854, 11, 291, 486, 3496, 8344], "temperature": 0.0, "avg_logprob": -0.1484466552734375, "compression_ratio": 1.7330316742081449, "no_speech_prob": 5.5596472520846874e-05}, {"id": 408, "seek": 182848, "start": 1833.32, "end": 1835.52, "text": " more compute.", "tokens": [544, 14722, 13], "temperature": 0.0, "avg_logprob": -0.1484466552734375, "compression_ratio": 1.7330316742081449, "no_speech_prob": 5.5596472520846874e-05}, {"id": 409, "seek": 182848, "start": 1835.52, "end": 1841.3600000000001, "text": " But the fraction of the amount of compute you spend near the last token isn't nearly", "tokens": [583, 264, 14135, 295, 264, 2372, 295, 14722, 291, 3496, 2651, 264, 1036, 14862, 1943, 380, 6217], "temperature": 0.0, "avg_logprob": -0.1484466552734375, "compression_ratio": 1.7330316742081449, "no_speech_prob": 5.5596472520846874e-05}, {"id": 410, "seek": 182848, "start": 1841.3600000000001, "end": 1842.72, "text": " so stark.", "tokens": [370, 17417, 13], "temperature": 0.0, "avg_logprob": -0.1484466552734375, "compression_ratio": 1.7330316742081449, "no_speech_prob": 5.5596472520846874e-05}, {"id": 411, "seek": 182848, "start": 1842.72, "end": 1850.6, "text": " Most of the compute happens in the matrix multiplies for the MLP feed forward part of", "tokens": [4534, 295, 264, 14722, 2314, 294, 264, 8141, 12788, 530, 337, 264, 21601, 47, 3154, 2128, 644, 295], "temperature": 0.0, "avg_logprob": -0.1484466552734375, "compression_ratio": 1.7330316742081449, "no_speech_prob": 5.5596472520846874e-05}, {"id": 412, "seek": 182848, "start": 1850.6, "end": 1857.8, "text": " the transformer, and also the matrix multiplies to make the keys and queries and values, et cetera.", "tokens": [264, 31782, 11, 293, 611, 264, 8141, 12788, 530, 281, 652, 264, 9317, 293, 24109, 293, 4190, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.1484466552734375, "compression_ratio": 1.7330316742081449, "no_speech_prob": 5.5596472520846874e-05}, {"id": 413, "seek": 185780, "start": 1857.8, "end": 1859.8, "text": " That's actually in most well.", "tokens": [663, 311, 767, 294, 881, 731, 13], "temperature": 0.0, "avg_logprob": -0.3394372925277828, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.00025305451708845794}, {"id": 414, "seek": 185780, "start": 1859.8, "end": 1864.28, "text": " It depends on the model hyper parameters, but in many models, especially models that are", "tokens": [467, 5946, 322, 264, 2316, 9848, 9834, 11, 457, 294, 867, 5245, 11, 2318, 5245, 300, 366], "temperature": 0.0, "avg_logprob": -0.3394372925277828, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.00025305451708845794}, {"id": 415, "seek": 185780, "start": 1864.28, "end": 1867.52, "text": " large, that's actually the predominant compute.", "tokens": [2416, 11, 300, 311, 767, 264, 21456, 394, 14722, 13], "temperature": 0.0, "avg_logprob": -0.3394372925277828, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.00025305451708845794}, {"id": 416, "seek": 185780, "start": 1867.52, "end": 1870.72, "text": " And so actually, the amount of compute you do for the last token in the first token might", "tokens": [400, 370, 767, 11, 264, 2372, 295, 14722, 291, 360, 337, 264, 1036, 14862, 294, 264, 700, 14862, 1062], "temperature": 0.0, "avg_logprob": -0.3394372925277828, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.00025305451708845794}, {"id": 417, "seek": 185780, "start": 1870.72, "end": 1872.32, "text": " only differ by a few percent.", "tokens": [787, 743, 538, 257, 1326, 3043, 13], "temperature": 0.0, "avg_logprob": -0.3394372925277828, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.00025305451708845794}, {"id": 418, "seek": 185780, "start": 1872.32, "end": 1876.36, "text": " So for GP3, I think it's literally like one or two percent difference.", "tokens": [407, 337, 26039, 18, 11, 286, 519, 309, 311, 3736, 411, 472, 420, 732, 3043, 2649, 13], "temperature": 0.0, "avg_logprob": -0.3394372925277828, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.00025305451708845794}, {"id": 419, "seek": 185780, "start": 1876.36, "end": 1879.36, "text": " So the sum matrix multiplies the tension.", "tokens": [407, 264, 2408, 8141, 12788, 530, 264, 8980, 13], "temperature": 0.0, "avg_logprob": -0.3394372925277828, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.00025305451708845794}, {"id": 420, "seek": 185780, "start": 1879.36, "end": 1880.44, "text": " Yeah, yeah, yeah.", "tokens": [865, 11, 1338, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.3394372925277828, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.00025305451708845794}, {"id": 421, "seek": 185780, "start": 1880.44, "end": 1884.44, "text": " So I mean, the formula for that was this one that I briefly mentioned here.", "tokens": [407, 286, 914, 11, 264, 8513, 337, 300, 390, 341, 472, 300, 286, 10515, 2835, 510, 13], "temperature": 0.0, "avg_logprob": -0.3394372925277828, "compression_ratio": 1.7237762237762237, "no_speech_prob": 0.00025305451708845794}, {"id": 422, "seek": 188444, "start": 1884.44, "end": 1889.1200000000001, "text": " So basically, how much compute you do in the context direction divided by the amount of", "tokens": [407, 1936, 11, 577, 709, 14722, 291, 360, 294, 264, 4319, 3513, 6666, 538, 264, 2372, 295], "temperature": 0.0, "avg_logprob": -0.17429188785389957, "compression_ratio": 1.8812785388127853, "no_speech_prob": 1.834061367844697e-05}, {"id": 423, "seek": 188444, "start": 1889.1200000000001, "end": 1892.96, "text": " compute you do in the matrix multiply direction is this.", "tokens": [14722, 291, 360, 294, 264, 8141, 12972, 3513, 307, 341, 13], "temperature": 0.0, "avg_logprob": -0.17429188785389957, "compression_ratio": 1.8812785388127853, "no_speech_prob": 1.834061367844697e-05}, {"id": 424, "seek": 188444, "start": 1892.96, "end": 1900.2, "text": " So if your model is, if D model is very small, if D model is 128, and context is 1,000, then", "tokens": [407, 498, 428, 2316, 307, 11, 498, 413, 2316, 307, 588, 1359, 11, 498, 413, 2316, 307, 29810, 11, 293, 4319, 307, 502, 11, 1360, 11, 550], "temperature": 0.0, "avg_logprob": -0.17429188785389957, "compression_ratio": 1.8812785388127853, "no_speech_prob": 1.834061367844697e-05}, {"id": 425, "seek": 188444, "start": 1900.2, "end": 1902.4, "text": " it's basically 50-50.", "tokens": [309, 311, 1936, 2625, 12, 2803, 13], "temperature": 0.0, "avg_logprob": -0.17429188785389957, "compression_ratio": 1.8812785388127853, "no_speech_prob": 1.834061367844697e-05}, {"id": 426, "seek": 188444, "start": 1902.4, "end": 1908.16, "text": " But if D model is 10,000, and context is 2,000, then it's like 2%.", "tokens": [583, 498, 413, 2316, 307, 1266, 11, 1360, 11, 293, 4319, 307, 568, 11, 1360, 11, 550, 309, 311, 411, 568, 6856], "temperature": 0.0, "avg_logprob": -0.17429188785389957, "compression_ratio": 1.8812785388127853, "no_speech_prob": 1.834061367844697e-05}, {"id": 427, "seek": 188444, "start": 1908.16, "end": 1912.88, "text": " So if the model is keep getting bigger, then that means that if you're willing to pay", "tokens": [407, 498, 264, 2316, 307, 1066, 1242, 3801, 11, 550, 300, 1355, 300, 498, 291, 434, 4950, 281, 1689], "temperature": 0.0, "avg_logprob": -0.17429188785389957, "compression_ratio": 1.8812785388127853, "no_speech_prob": 1.834061367844697e-05}, {"id": 428, "seek": 191288, "start": 1912.88, "end": 1917.3200000000002, "text": " a fractional cost, then you can keep making context length longer and pay a fixed fractional", "tokens": [257, 17948, 1966, 2063, 11, 550, 291, 393, 1066, 1455, 4319, 4641, 2854, 293, 1689, 257, 6806, 17948, 1966], "temperature": 0.0, "avg_logprob": -0.23513776406474496, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.5930350855342112e-05}, {"id": 429, "seek": 191288, "start": 1917.3200000000002, "end": 1918.3200000000002, "text": " cost.", "tokens": [2063, 13], "temperature": 0.0, "avg_logprob": -0.23513776406474496, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.5930350855342112e-05}, {"id": 430, "seek": 191288, "start": 1918.3200000000002, "end": 1921.5600000000002, "text": " And of course, if you use something fancy with intense attention, you also get extra", "tokens": [400, 295, 1164, 11, 498, 291, 764, 746, 10247, 365, 9447, 3202, 11, 291, 611, 483, 2857], "temperature": 0.0, "avg_logprob": -0.23513776406474496, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.5930350855342112e-05}, {"id": 431, "seek": 191288, "start": 1921.5600000000002, "end": 1924.3200000000002, "text": " winds on top of that.", "tokens": [17765, 322, 1192, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.23513776406474496, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.5930350855342112e-05}, {"id": 432, "seek": 191288, "start": 1924.3200000000002, "end": 1928.3200000000002, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.23513776406474496, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.5930350855342112e-05}, {"id": 433, "seek": 191288, "start": 1928.3200000000002, "end": 1931.3200000000002, "text": " Cool.", "tokens": [8561, 13], "temperature": 0.0, "avg_logprob": -0.23513776406474496, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.5930350855342112e-05}, {"id": 434, "seek": 191288, "start": 1931.3200000000002, "end": 1941.88, "text": " So this is sort of both of these, the left and the right, show you samples from a transformer", "tokens": [407, 341, 307, 1333, 295, 1293, 295, 613, 11, 264, 1411, 293, 264, 558, 11, 855, 291, 10938, 490, 257, 31782], "temperature": 0.0, "avg_logprob": -0.23513776406474496, "compression_ratio": 1.5598086124401913, "no_speech_prob": 1.5930350855342112e-05}, {"id": 435, "seek": 194188, "start": 1941.88, "end": 1944.3200000000002, "text": " model.", "tokens": [2316, 13], "temperature": 0.0, "avg_logprob": -0.20051523734783305, "compression_ratio": 1.6125461254612545, "no_speech_prob": 6.60383520880714e-05}, {"id": 436, "seek": 194188, "start": 1944.3200000000002, "end": 1948.2, "text": " Very roughly speaking, they're identical kinds of transformer models, which is with some", "tokens": [4372, 9810, 4124, 11, 436, 434, 14800, 3685, 295, 31782, 5245, 11, 597, 307, 365, 512], "temperature": 0.0, "avg_logprob": -0.20051523734783305, "compression_ratio": 1.6125461254612545, "no_speech_prob": 6.60383520880714e-05}, {"id": 437, "seek": 194188, "start": 1948.2, "end": 1950.2, "text": " slightly different hyper parameters.", "tokens": [4748, 819, 9848, 9834, 13], "temperature": 0.0, "avg_logprob": -0.20051523734783305, "compression_ratio": 1.6125461254612545, "no_speech_prob": 6.60383520880714e-05}, {"id": 438, "seek": 194188, "start": 1950.2, "end": 1952.68, "text": " But they're trained on very different data distributions.", "tokens": [583, 436, 434, 8895, 322, 588, 819, 1412, 37870, 13], "temperature": 0.0, "avg_logprob": -0.20051523734783305, "compression_ratio": 1.6125461254612545, "no_speech_prob": 6.60383520880714e-05}, {"id": 439, "seek": 194188, "start": 1952.68, "end": 1955.92, "text": " The one on the left is obviously, this is GPT-3.", "tokens": [440, 472, 322, 264, 1411, 307, 2745, 11, 341, 307, 26039, 51, 12, 18, 13], "temperature": 0.0, "avg_logprob": -0.20051523734783305, "compression_ratio": 1.6125461254612545, "no_speech_prob": 6.60383520880714e-05}, {"id": 440, "seek": 194188, "start": 1955.92, "end": 1958.92, "text": " The one on the right is IGPT.", "tokens": [440, 472, 322, 264, 558, 307, 26367, 47, 51, 13], "temperature": 0.0, "avg_logprob": -0.20051523734783305, "compression_ratio": 1.6125461254612545, "no_speech_prob": 6.60383520880714e-05}, {"id": 441, "seek": 194188, "start": 1958.92, "end": 1962.4, "text": " It's a model that's trained to predict pixels, row by row.", "tokens": [467, 311, 257, 2316, 300, 311, 8895, 281, 6069, 18668, 11, 5386, 538, 5386, 13], "temperature": 0.0, "avg_logprob": -0.20051523734783305, "compression_ratio": 1.6125461254612545, "no_speech_prob": 6.60383520880714e-05}, {"id": 442, "seek": 194188, "start": 1962.4, "end": 1967.6000000000001, "text": " And so what happened here was that we took the top half of an image and then generated", "tokens": [400, 370, 437, 2011, 510, 390, 300, 321, 1890, 264, 1192, 1922, 295, 364, 3256, 293, 550, 10833], "temperature": 0.0, "avg_logprob": -0.20051523734783305, "compression_ratio": 1.6125461254612545, "no_speech_prob": 6.60383520880714e-05}, {"id": 443, "seek": 194188, "start": 1967.6000000000001, "end": 1970.0400000000002, "text": " all the rows beneath.", "tokens": [439, 264, 13241, 17149, 13], "temperature": 0.0, "avg_logprob": -0.20051523734783305, "compression_ratio": 1.6125461254612545, "no_speech_prob": 6.60383520880714e-05}, {"id": 444, "seek": 197004, "start": 1970.04, "end": 1975.24, "text": " And so the same kind of model architecture, but just trained on different data distributions", "tokens": [400, 370, 264, 912, 733, 295, 2316, 9482, 11, 457, 445, 8895, 322, 819, 1412, 37870], "temperature": 0.0, "avg_logprob": -0.19616006434648886, "compression_ratio": 1.6041666666666667, "no_speech_prob": 3.784346745305811e-06}, {"id": 445, "seek": 197004, "start": 1975.24, "end": 1984.56, "text": " is able to effectively learn very impressive generative capabilities in both cases.", "tokens": [307, 1075, 281, 8659, 1466, 588, 8992, 1337, 1166, 10862, 294, 1293, 3331, 13], "temperature": 0.0, "avg_logprob": -0.19616006434648886, "compression_ratio": 1.6041666666666667, "no_speech_prob": 3.784346745305811e-06}, {"id": 446, "seek": 197004, "start": 1984.56, "end": 1989.68, "text": " And so this is sort of a qualitative hint at the possibility that what's going on here", "tokens": [400, 370, 341, 307, 1333, 295, 257, 31312, 12075, 412, 264, 7959, 300, 437, 311, 516, 322, 510], "temperature": 0.0, "avg_logprob": -0.19616006434648886, "compression_ratio": 1.6041666666666667, "no_speech_prob": 3.784346745305811e-06}, {"id": 447, "seek": 197004, "start": 1989.68, "end": 1993.48, "text": " is quite universal.", "tokens": [307, 1596, 11455, 13], "temperature": 0.0, "avg_logprob": -0.19616006434648886, "compression_ratio": 1.6041666666666667, "no_speech_prob": 3.784346745305811e-06}, {"id": 448, "seek": 197004, "start": 1993.48, "end": 1998.2, "text": " And so another way of introducing it is say, you might have some questions after the", "tokens": [400, 370, 1071, 636, 295, 15424, 309, 307, 584, 11, 291, 1062, 362, 512, 1651, 934, 264], "temperature": 0.0, "avg_logprob": -0.19616006434648886, "compression_ratio": 1.6041666666666667, "no_speech_prob": 3.784346745305811e-06}, {"id": 449, "seek": 197004, "start": 1998.2, "end": 1999.56, "text": " last few slides.", "tokens": [1036, 1326, 9788, 13], "temperature": 0.0, "avg_logprob": -0.19616006434648886, "compression_ratio": 1.6041666666666667, "no_speech_prob": 3.784346745305811e-06}, {"id": 450, "seek": 199956, "start": 1999.56, "end": 2002.8, "text": " How are the scaling laws I'm talking about really specific to language, are they a feature", "tokens": [1012, 366, 264, 21589, 6064, 286, 478, 1417, 466, 534, 2685, 281, 2856, 11, 366, 436, 257, 4111], "temperature": 0.0, "avg_logprob": -0.15952419310577157, "compression_ratio": 1.8277027027027026, "no_speech_prob": 6.299868982750922e-05}, {"id": 451, "seek": 199956, "start": 2002.8, "end": 2006.12, "text": " of the kinds of data that language is?", "tokens": [295, 264, 3685, 295, 1412, 300, 2856, 307, 30], "temperature": 0.0, "avg_logprob": -0.15952419310577157, "compression_ratio": 1.8277027027027026, "no_speech_prob": 6.299868982750922e-05}, {"id": 452, "seek": 199956, "start": 2006.12, "end": 2009.24, "text": " You might ask, do these scaling laws really continue?", "tokens": [509, 1062, 1029, 11, 360, 613, 21589, 6064, 534, 2354, 30], "temperature": 0.0, "avg_logprob": -0.15952419310577157, "compression_ratio": 1.8277027027027026, "no_speech_prob": 6.299868982750922e-05}, {"id": 453, "seek": 199956, "start": 2009.24, "end": 2013.1599999999999, "text": " You showed that they're true over many orders of magnitude, but did they break down eventually", "tokens": [509, 4712, 300, 436, 434, 2074, 670, 867, 9470, 295, 15668, 11, 457, 630, 436, 1821, 760, 4728], "temperature": 0.0, "avg_logprob": -0.15952419310577157, "compression_ratio": 1.8277027027027026, "no_speech_prob": 6.299868982750922e-05}, {"id": 454, "seek": 199956, "start": 2013.1599999999999, "end": 2014.6, "text": " in a what way?", "tokens": [294, 257, 437, 636, 30], "temperature": 0.0, "avg_logprob": -0.15952419310577157, "compression_ratio": 1.8277027027027026, "no_speech_prob": 6.299868982750922e-05}, {"id": 455, "seek": 199956, "start": 2014.6, "end": 2020.8, "text": " And then another question you might ask is, what do they imply for other kinds of evaluations?", "tokens": [400, 550, 1071, 1168, 291, 1062, 1029, 307, 11, 437, 360, 436, 33616, 337, 661, 3685, 295, 43085, 30], "temperature": 0.0, "avg_logprob": -0.15952419310577157, "compression_ratio": 1.8277027027027026, "no_speech_prob": 6.299868982750922e-05}, {"id": 456, "seek": 199956, "start": 2020.8, "end": 2025.12, "text": " You probably don't just want to generate raw samples from either of these kinds of models.", "tokens": [509, 1391, 500, 380, 445, 528, 281, 8460, 8936, 10938, 490, 2139, 295, 613, 3685, 295, 5245, 13], "temperature": 0.0, "avg_logprob": -0.15952419310577157, "compression_ratio": 1.8277027027027026, "no_speech_prob": 6.299868982750922e-05}, {"id": 457, "seek": 199956, "start": 2025.12, "end": 2028.84, "text": " You might want to use them for some other more specific task.", "tokens": [509, 1062, 528, 281, 764, 552, 337, 512, 661, 544, 2685, 5633, 13], "temperature": 0.0, "avg_logprob": -0.15952419310577157, "compression_ratio": 1.8277027027027026, "no_speech_prob": 6.299868982750922e-05}, {"id": 458, "seek": 202884, "start": 2028.84, "end": 2035.8799999999999, "text": " And so the question of whether or not the test loss, the training loss that you've optimized", "tokens": [400, 370, 264, 1168, 295, 1968, 420, 406, 264, 1500, 4470, 11, 264, 3097, 4470, 300, 291, 600, 26941], "temperature": 0.0, "avg_logprob": -0.1610459283340809, "compression_ratio": 1.6153846153846154, "no_speech_prob": 8.345778041984886e-05}, {"id": 459, "seek": 202884, "start": 2035.8799999999999, "end": 2040.4399999999998, "text": " as that goes down in a predictable way, does that also imply that other things, other", "tokens": [382, 300, 1709, 760, 294, 257, 27737, 636, 11, 775, 300, 611, 33616, 300, 661, 721, 11, 661], "temperature": 0.0, "avg_logprob": -0.1610459283340809, "compression_ratio": 1.6153846153846154, "no_speech_prob": 8.345778041984886e-05}, {"id": 460, "seek": 202884, "start": 2040.4399999999998, "end": 2043.1599999999999, "text": " capabilities of the model are improving?", "tokens": [10862, 295, 264, 2316, 366, 11470, 30], "temperature": 0.0, "avg_logprob": -0.1610459283340809, "compression_ratio": 1.6153846153846154, "no_speech_prob": 8.345778041984886e-05}, {"id": 461, "seek": 202884, "start": 2043.1599999999999, "end": 2047.3999999999999, "text": " So I'll be talking about these questions.", "tokens": [407, 286, 603, 312, 1417, 466, 613, 1651, 13], "temperature": 0.0, "avg_logprob": -0.1610459283340809, "compression_ratio": 1.6153846153846154, "no_speech_prob": 8.345778041984886e-05}, {"id": 462, "seek": 202884, "start": 2047.3999999999999, "end": 2056.56, "text": " So this plot contains kind of a lot of compressed information all at once, or the set of plots.", "tokens": [407, 341, 7542, 8306, 733, 295, 257, 688, 295, 30353, 1589, 439, 412, 1564, 11, 420, 264, 992, 295, 28609, 13], "temperature": 0.0, "avg_logprob": -0.1610459283340809, "compression_ratio": 1.6153846153846154, "no_speech_prob": 8.345778041984886e-05}, {"id": 463, "seek": 205656, "start": 2056.56, "end": 2065.56, "text": " So this is the result of what happens if you train the same kind of transfer models on", "tokens": [407, 341, 307, 264, 1874, 295, 437, 2314, 498, 291, 3847, 264, 912, 733, 295, 5003, 5245, 322], "temperature": 0.0, "avg_logprob": -0.21169728246228448, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.75733928498812e-05}, {"id": 464, "seek": 205656, "start": 2065.56, "end": 2067.7599999999998, "text": " sort of five different data distributions.", "tokens": [1333, 295, 1732, 819, 1412, 37870, 13], "temperature": 0.0, "avg_logprob": -0.21169728246228448, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.75733928498812e-05}, {"id": 465, "seek": 205656, "start": 2067.7599999999998, "end": 2073.24, "text": " So text language we already saw, but you can try video where you predict every pixel", "tokens": [407, 2487, 2856, 321, 1217, 1866, 11, 457, 291, 393, 853, 960, 689, 291, 6069, 633, 19261], "temperature": 0.0, "avg_logprob": -0.21169728246228448, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.75733928498812e-05}, {"id": 466, "seek": 205656, "start": 2073.24, "end": 2079.88, "text": " in a video in this sort of rectangular prism of video pixels.", "tokens": [294, 257, 960, 294, 341, 1333, 295, 31167, 582, 1434, 295, 960, 18668, 13], "temperature": 0.0, "avg_logprob": -0.21169728246228448, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.75733928498812e-05}, {"id": 467, "seek": 205656, "start": 2079.88, "end": 2085.6, "text": " Images, this sort of synthetically generated deep-mind math data set where you're trying", "tokens": [4331, 1660, 11, 341, 1333, 295, 10657, 22652, 10833, 2452, 12, 13733, 5221, 1412, 992, 689, 291, 434, 1382], "temperature": 0.0, "avg_logprob": -0.21169728246228448, "compression_ratio": 1.6294642857142858, "no_speech_prob": 9.75733928498812e-05}, {"id": 468, "seek": 208560, "start": 2085.6, "end": 2089.08, "text": " to predict the answer to math problems.", "tokens": [281, 6069, 264, 1867, 281, 5221, 2740, 13], "temperature": 0.0, "avg_logprob": -0.14717104897570255, "compression_ratio": 1.4619883040935673, "no_speech_prob": 2.392075111856684e-05}, {"id": 469, "seek": 208560, "start": 2089.08, "end": 2095.7599999999998, "text": " There's a multimodal data set where you have image text pairs in either direction.", "tokens": [821, 311, 257, 32972, 378, 304, 1412, 992, 689, 291, 362, 3256, 2487, 15494, 294, 2139, 3513, 13], "temperature": 0.0, "avg_logprob": -0.14717104897570255, "compression_ratio": 1.4619883040935673, "no_speech_prob": 2.392075111856684e-05}, {"id": 470, "seek": 208560, "start": 2095.7599999999998, "end": 2103.44, "text": " And in all cases, the x-axis is compute, and the y-axis is the appropriate test loss", "tokens": [400, 294, 439, 3331, 11, 264, 2031, 12, 24633, 307, 14722, 11, 293, 264, 288, 12, 24633, 307, 264, 6854, 1500, 4470], "temperature": 0.0, "avg_logprob": -0.14717104897570255, "compression_ratio": 1.4619883040935673, "no_speech_prob": 2.392075111856684e-05}, {"id": 471, "seek": 208560, "start": 2103.44, "end": 2108.7999999999997, "text": " for that class of models minus a constant.", "tokens": [337, 300, 1508, 295, 5245, 3175, 257, 5754, 13], "temperature": 0.0, "avg_logprob": -0.14717104897570255, "compression_ratio": 1.4619883040935673, "no_speech_prob": 2.392075111856684e-05}, {"id": 472, "seek": 210880, "start": 2108.8, "end": 2115.6800000000003, "text": " So that's the one complication that I've added here.", "tokens": [407, 300, 311, 264, 472, 1209, 8758, 300, 286, 600, 3869, 510, 13], "temperature": 0.0, "avg_logprob": -0.1179556765798795, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.0286809811077546e-05}, {"id": 473, "seek": 210880, "start": 2115.6800000000003, "end": 2130.1200000000003, "text": " So the claim is that these dashed lines in terms of the original loss are a power law,", "tokens": [407, 264, 3932, 307, 300, 613, 8240, 292, 3876, 294, 2115, 295, 264, 3380, 4470, 366, 257, 1347, 2101, 11], "temperature": 0.0, "avg_logprob": -0.1179556765798795, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.0286809811077546e-05}, {"id": 474, "seek": 210880, "start": 2130.1200000000003, "end": 2138.28, "text": " like the power laws that we saw on a much earlier slide, plus one constant term.", "tokens": [411, 264, 1347, 6064, 300, 321, 1866, 322, 257, 709, 3071, 4137, 11, 1804, 472, 5754, 1433, 13], "temperature": 0.0, "avg_logprob": -0.1179556765798795, "compression_ratio": 1.5172413793103448, "no_speech_prob": 1.0286809811077546e-05}, {"id": 475, "seek": 213828, "start": 2138.28, "end": 2142.88, "text": " And if you subtract off that constant term, then you make a log-log plot once again,", "tokens": [400, 498, 291, 16390, 766, 300, 5754, 1433, 11, 550, 291, 652, 257, 3565, 12, 4987, 7542, 1564, 797, 11], "temperature": 0.0, "avg_logprob": -0.17300585153940562, "compression_ratio": 1.8847736625514404, "no_speech_prob": 3.7627029087161645e-05}, {"id": 476, "seek": 213828, "start": 2142.88, "end": 2146.52, "text": " then you once again get these very, very nice straight lines.", "tokens": [550, 291, 1564, 797, 483, 613, 588, 11, 588, 1481, 2997, 3876, 13], "temperature": 0.0, "avg_logprob": -0.17300585153940562, "compression_ratio": 1.8847736625514404, "no_speech_prob": 3.7627029087161645e-05}, {"id": 477, "seek": 213828, "start": 2146.52, "end": 2150.8, "text": " And so this compute scaling law generalizes to all these other data distributions.", "tokens": [400, 370, 341, 14722, 21589, 2101, 2674, 5660, 281, 439, 613, 661, 1412, 37870, 13], "temperature": 0.0, "avg_logprob": -0.17300585153940562, "compression_ratio": 1.8847736625514404, "no_speech_prob": 3.7627029087161645e-05}, {"id": 478, "seek": 213828, "start": 2150.8, "end": 2156.1200000000003, "text": " And the other scaling laws also generalize, I just haven't plotted them.", "tokens": [400, 264, 661, 21589, 6064, 611, 2674, 1125, 11, 286, 445, 2378, 380, 43288, 552, 13], "temperature": 0.0, "avg_logprob": -0.17300585153940562, "compression_ratio": 1.8847736625514404, "no_speech_prob": 3.7627029087161645e-05}, {"id": 479, "seek": 213828, "start": 2156.1200000000003, "end": 2161.36, "text": " So the claim of this slide is that scaling laws do generalize to all of these other data", "tokens": [407, 264, 3932, 295, 341, 4137, 307, 300, 21589, 6064, 360, 2674, 1125, 281, 439, 295, 613, 661, 1412], "temperature": 0.0, "avg_logprob": -0.17300585153940562, "compression_ratio": 1.8847736625514404, "no_speech_prob": 3.7627029087161645e-05}, {"id": 480, "seek": 213828, "start": 2161.36, "end": 2167.84, "text": " distributions, and you train the same basic kind of model on them.", "tokens": [37870, 11, 293, 291, 3847, 264, 912, 3875, 733, 295, 2316, 322, 552, 13], "temperature": 0.0, "avg_logprob": -0.17300585153940562, "compression_ratio": 1.8847736625514404, "no_speech_prob": 3.7627029087161645e-05}, {"id": 481, "seek": 216784, "start": 2167.84, "end": 2173.6800000000003, "text": " And furthermore, there's sort of an intellectually slightly interesting point, which is that", "tokens": [400, 3052, 3138, 11, 456, 311, 1333, 295, 364, 46481, 4748, 1880, 935, 11, 597, 307, 300], "temperature": 0.0, "avg_logprob": -0.11822915351253817, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.777604666131083e-05}, {"id": 482, "seek": 216784, "start": 2173.6800000000003, "end": 2181.56, "text": " if you really believe that these dashed lines are true, if you think that they're a real", "tokens": [498, 291, 534, 1697, 300, 613, 8240, 292, 3876, 366, 2074, 11, 498, 291, 519, 300, 436, 434, 257, 957], "temperature": 0.0, "avg_logprob": -0.11822915351253817, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.777604666131083e-05}, {"id": 483, "seek": 216784, "start": 2181.56, "end": 2188.04, "text": " feature of what's going on, and they continue out very, very, very far, then if you think", "tokens": [4111, 295, 437, 311, 516, 322, 11, 293, 436, 2354, 484, 588, 11, 588, 11, 588, 1400, 11, 550, 498, 291, 519], "temperature": 0.0, "avg_logprob": -0.11822915351253817, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.777604666131083e-05}, {"id": 484, "seek": 216784, "start": 2188.04, "end": 2194.76, "text": " that the loss is a constant plus a power law, then you can interpret the constant term", "tokens": [300, 264, 4470, 307, 257, 5754, 1804, 257, 1347, 2101, 11, 550, 291, 393, 7302, 264, 5754, 1433], "temperature": 0.0, "avg_logprob": -0.11822915351253817, "compression_ratio": 1.7635467980295567, "no_speech_prob": 1.777604666131083e-05}, {"id": 485, "seek": 219476, "start": 2194.76, "end": 2199.32, "text": " as the entropy of the underlying data distribution.", "tokens": [382, 264, 30867, 295, 264, 14217, 1412, 7316, 13], "temperature": 0.0, "avg_logprob": -0.16254796874657107, "compression_ratio": 1.606837606837607, "no_speech_prob": 1.2216575669299345e-05}, {"id": 486, "seek": 219476, "start": 2199.32, "end": 2204.8, "text": " And you can interpret the power law as something like the KL divergence between the true data", "tokens": [400, 291, 393, 7302, 264, 1347, 2101, 382, 746, 411, 264, 47991, 47387, 1296, 264, 2074, 1412], "temperature": 0.0, "avg_logprob": -0.16254796874657107, "compression_ratio": 1.606837606837607, "no_speech_prob": 1.2216575669299345e-05}, {"id": 487, "seek": 219476, "start": 2204.8, "end": 2208.8, "text": " distribution and the model that you have.", "tokens": [7316, 293, 264, 2316, 300, 291, 362, 13], "temperature": 0.0, "avg_logprob": -0.16254796874657107, "compression_ratio": 1.606837606837607, "no_speech_prob": 1.2216575669299345e-05}, {"id": 488, "seek": 219476, "start": 2208.8, "end": 2210.8, "text": " So that's a lot.", "tokens": [407, 300, 311, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.16254796874657107, "compression_ratio": 1.606837606837607, "no_speech_prob": 1.2216575669299345e-05}, {"id": 489, "seek": 219476, "start": 2210.8, "end": 2217.88, "text": " The important summary at zero-thorder to remember is that I'm telling you that the kinds of", "tokens": [440, 1021, 12691, 412, 4018, 12, 392, 4687, 281, 1604, 307, 300, 286, 478, 3585, 291, 300, 264, 3685, 295], "temperature": 0.0, "avg_logprob": -0.16254796874657107, "compression_ratio": 1.606837606837607, "no_speech_prob": 1.2216575669299345e-05}, {"id": 490, "seek": 219476, "start": 2217.88, "end": 2222.2000000000003, "text": " scaling laws I presented for language generalize to all of these other domains.", "tokens": [21589, 6064, 286, 8212, 337, 2856, 2674, 1125, 281, 439, 295, 613, 661, 25514, 13], "temperature": 0.0, "avg_logprob": -0.16254796874657107, "compression_ratio": 1.606837606837607, "no_speech_prob": 1.2216575669299345e-05}, {"id": 491, "seek": 222220, "start": 2222.2, "end": 2226.64, "text": " There's also some other interesting features here.", "tokens": [821, 311, 611, 512, 661, 1880, 4122, 510, 13], "temperature": 0.0, "avg_logprob": -0.12014618116555754, "compression_ratio": 1.66, "no_speech_prob": 7.888017535151448e-06}, {"id": 492, "seek": 222220, "start": 2226.64, "end": 2233.8799999999997, "text": " The reason why I used compute to illustrate that the scaling laws generalize is because", "tokens": [440, 1778, 983, 286, 1143, 14722, 281, 23221, 300, 264, 21589, 6064, 2674, 1125, 307, 570], "temperature": 0.0, "avg_logprob": -0.12014618116555754, "compression_ratio": 1.66, "no_speech_prob": 7.888017535151448e-06}, {"id": 493, "seek": 222220, "start": 2233.8799999999997, "end": 2238.04, "text": " you can ask another question now that puts all of the different data distributions on", "tokens": [291, 393, 1029, 1071, 1168, 586, 300, 8137, 439, 295, 264, 819, 1412, 37870, 322], "temperature": 0.0, "avg_logprob": -0.12014618116555754, "compression_ratio": 1.66, "no_speech_prob": 7.888017535151448e-06}, {"id": 494, "seek": 222220, "start": 2238.04, "end": 2239.72, "text": " one plot.", "tokens": [472, 7542, 13], "temperature": 0.0, "avg_logprob": -0.12014618116555754, "compression_ratio": 1.66, "no_speech_prob": 7.888017535151448e-06}, {"id": 495, "seek": 222220, "start": 2239.72, "end": 2245.4399999999996, "text": " It wouldn't have made any sense to combine the five plots on the last slide into one plot,", "tokens": [467, 2759, 380, 362, 1027, 604, 2020, 281, 10432, 264, 1732, 28609, 322, 264, 1036, 4137, 666, 472, 7542, 11], "temperature": 0.0, "avg_logprob": -0.12014618116555754, "compression_ratio": 1.66, "no_speech_prob": 7.888017535151448e-06}, {"id": 496, "seek": 222220, "start": 2245.4399999999996, "end": 2250.08, "text": " because the test loss, when you're predicting a word, is not in any way comparable to the", "tokens": [570, 264, 1500, 4470, 11, 562, 291, 434, 32884, 257, 1349, 11, 307, 406, 294, 604, 636, 25323, 281, 264], "temperature": 0.0, "avg_logprob": -0.12014618116555754, "compression_ratio": 1.66, "no_speech_prob": 7.888017535151448e-06}, {"id": 497, "seek": 225008, "start": 2250.08, "end": 2252.08, "text": " test loss when you're predicting a pixel.", "tokens": [1500, 4470, 562, 291, 434, 32884, 257, 19261, 13], "temperature": 0.0, "avg_logprob": -0.20582926714861835, "compression_ratio": 1.7535211267605635, "no_speech_prob": 7.720607391092926e-05}, {"id": 498, "seek": 225008, "start": 2252.08, "end": 2253.08, "text": " It doesn't really make sense.", "tokens": [467, 1177, 380, 534, 652, 2020, 13], "temperature": 0.0, "avg_logprob": -0.20582926714861835, "compression_ratio": 1.7535211267605635, "no_speech_prob": 7.720607391092926e-05}, {"id": 499, "seek": 225008, "start": 2253.08, "end": 2254.08, "text": " They don't have the same units.", "tokens": [814, 500, 380, 362, 264, 912, 6815, 13], "temperature": 0.0, "avg_logprob": -0.20582926714861835, "compression_ratio": 1.7535211267605635, "no_speech_prob": 7.720607391092926e-05}, {"id": 500, "seek": 225008, "start": 2254.08, "end": 2256.7999999999997, "text": " It doesn't make sense to put them together.", "tokens": [467, 1177, 380, 652, 2020, 281, 829, 552, 1214, 13], "temperature": 0.0, "avg_logprob": -0.20582926714861835, "compression_ratio": 1.7535211267605635, "no_speech_prob": 7.720607391092926e-05}, {"id": 501, "seek": 225008, "start": 2256.7999999999997, "end": 2262.08, "text": " But something that does make sense to put together is what the optimal model size is as", "tokens": [583, 746, 300, 775, 652, 2020, 281, 829, 1214, 307, 437, 264, 16252, 2316, 2744, 307, 382], "temperature": 0.0, "avg_logprob": -0.20582926714861835, "compression_ratio": 1.7535211267605635, "no_speech_prob": 7.720607391092926e-05}, {"id": 502, "seek": 225008, "start": 2262.08, "end": 2265.08, "text": " a function of your computational budget.", "tokens": [257, 2445, 295, 428, 28270, 4706, 13], "temperature": 0.0, "avg_logprob": -0.20582926714861835, "compression_ratio": 1.7535211267605635, "no_speech_prob": 7.720607391092926e-05}, {"id": 503, "seek": 225008, "start": 2265.08, "end": 2270.4, "text": " And so in the same way that we did for language, you can go here and you can ask for any", "tokens": [400, 370, 294, 264, 912, 636, 300, 321, 630, 337, 2856, 11, 291, 393, 352, 510, 293, 291, 393, 1029, 337, 604], "temperature": 0.0, "avg_logprob": -0.20582926714861835, "compression_ratio": 1.7535211267605635, "no_speech_prob": 7.720607391092926e-05}, {"id": 504, "seek": 225008, "start": 2270.4, "end": 2275.0, "text": " given amount of compute, like 10 to the minus 2, petaflap days, what is the best model size?", "tokens": [2212, 2372, 295, 14722, 11, 411, 1266, 281, 264, 3175, 568, 11, 3817, 2792, 75, 569, 1708, 11, 437, 307, 264, 1151, 2316, 2744, 30], "temperature": 0.0, "avg_logprob": -0.20582926714861835, "compression_ratio": 1.7535211267605635, "no_speech_prob": 7.720607391092926e-05}, {"id": 505, "seek": 225008, "start": 2275.0, "end": 2277.36, "text": " You can do that for all of these plots.", "tokens": [509, 393, 360, 300, 337, 439, 295, 613, 28609, 13], "temperature": 0.0, "avg_logprob": -0.20582926714861835, "compression_ratio": 1.7535211267605635, "no_speech_prob": 7.720607391092926e-05}, {"id": 506, "seek": 227736, "start": 2277.36, "end": 2281.08, "text": " You combine that information together and you find something kind of surprising, which", "tokens": [509, 10432, 300, 1589, 1214, 293, 291, 915, 746, 733, 295, 8830, 11, 597], "temperature": 0.0, "avg_logprob": -0.17828943488303195, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.450820218451554e-05}, {"id": 507, "seek": 227736, "start": 2281.08, "end": 2287.7200000000003, "text": " is that, again, roughly speaking, if you're sort of willing to allow a little bit of wiggle", "tokens": [307, 300, 11, 797, 11, 9810, 4124, 11, 498, 291, 434, 1333, 295, 4950, 281, 2089, 257, 707, 857, 295, 33377], "temperature": 0.0, "avg_logprob": -0.17828943488303195, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.450820218451554e-05}, {"id": 508, "seek": 227736, "start": 2287.7200000000003, "end": 2293.04, "text": " room, all of these different kinds of models seem to be on the same trajectory for optimal", "tokens": [1808, 11, 439, 295, 613, 819, 3685, 295, 5245, 1643, 281, 312, 322, 264, 912, 21512, 337, 16252], "temperature": 0.0, "avg_logprob": -0.17828943488303195, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.450820218451554e-05}, {"id": 509, "seek": 227736, "start": 2293.04, "end": 2295.2400000000002, "text": " model size versus compute.", "tokens": [2316, 2744, 5717, 14722, 13], "temperature": 0.0, "avg_logprob": -0.17828943488303195, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.450820218451554e-05}, {"id": 510, "seek": 227736, "start": 2295.2400000000002, "end": 2300.32, "text": " There's some kind of universal fit of how much bigger you should make your model if you're", "tokens": [821, 311, 512, 733, 295, 11455, 3318, 295, 577, 709, 3801, 291, 820, 652, 428, 2316, 498, 291, 434], "temperature": 0.0, "avg_logprob": -0.17828943488303195, "compression_ratio": 1.6329113924050633, "no_speech_prob": 1.450820218451554e-05}, {"id": 511, "seek": 230032, "start": 2300.32, "end": 2308.8, "text": " going to model any of these data distributions with some given amount of compute.", "tokens": [516, 281, 2316, 604, 295, 613, 1412, 37870, 365, 512, 2212, 2372, 295, 14722, 13], "temperature": 0.0, "avg_logprob": -0.21405588642934734, "compression_ratio": 1.6371681415929205, "no_speech_prob": 4.068324051331729e-05}, {"id": 512, "seek": 230032, "start": 2308.8, "end": 2312.44, "text": " So what about other kinds of tasks?", "tokens": [407, 437, 466, 661, 3685, 295, 9608, 30], "temperature": 0.0, "avg_logprob": -0.21405588642934734, "compression_ratio": 1.6371681415929205, "no_speech_prob": 4.068324051331729e-05}, {"id": 513, "seek": 230032, "start": 2312.44, "end": 2318.8, "text": " Well, one of the most classic tasks that you can ask about in ML is image classification.", "tokens": [1042, 11, 472, 295, 264, 881, 7230, 9608, 300, 291, 393, 1029, 466, 294, 21601, 307, 3256, 21538, 13], "temperature": 0.0, "avg_logprob": -0.21405588642934734, "compression_ratio": 1.6371681415929205, "no_speech_prob": 4.068324051331729e-05}, {"id": 514, "seek": 230032, "start": 2318.8, "end": 2324.92, "text": " And so the models that we were training on images, and that I've shown you plots, they're", "tokens": [400, 370, 264, 5245, 300, 321, 645, 3097, 322, 5267, 11, 293, 300, 286, 600, 4898, 291, 28609, 11, 436, 434], "temperature": 0.0, "avg_logprob": -0.21405588642934734, "compression_ratio": 1.6371681415929205, "no_speech_prob": 4.068324051331729e-05}, {"id": 515, "seek": 230032, "start": 2324.92, "end": 2330.28, "text": " training loss, these models are sort of on tiny little images predicted.", "tokens": [3097, 4470, 11, 613, 5245, 366, 1333, 295, 322, 5870, 707, 5267, 19147, 13], "temperature": 0.0, "avg_logprob": -0.21405588642934734, "compression_ratio": 1.6371681415929205, "no_speech_prob": 4.068324051331729e-05}, {"id": 516, "seek": 233028, "start": 2330.28, "end": 2334.8, "text": " Pixel by pixel, in particular, they're 32 by 32 images, so we can look at sort of the", "tokens": [28323, 538, 19261, 11, 294, 1729, 11, 436, 434, 8858, 538, 8858, 5267, 11, 370, 321, 393, 574, 412, 1333, 295, 264], "temperature": 0.0, "avg_logprob": -0.15426916546291775, "compression_ratio": 1.7642276422764227, "no_speech_prob": 7.250785711221397e-05}, {"id": 517, "seek": 233028, "start": 2334.8, "end": 2340.36, "text": " 32 by 32 pixel version of image net classification.", "tokens": [8858, 538, 8858, 19261, 3037, 295, 3256, 2533, 21538, 13], "temperature": 0.0, "avg_logprob": -0.15426916546291775, "compression_ratio": 1.7642276422764227, "no_speech_prob": 7.250785711221397e-05}, {"id": 518, "seek": 233028, "start": 2340.36, "end": 2345.8, "text": " And the models that I was discussing are generative models that predict pixels, but you can shop", "tokens": [400, 264, 5245, 300, 286, 390, 10850, 366, 1337, 1166, 5245, 300, 6069, 18668, 11, 457, 291, 393, 3945], "temperature": 0.0, "avg_logprob": -0.15426916546291775, "compression_ratio": 1.7642276422764227, "no_speech_prob": 7.250785711221397e-05}, {"id": 519, "seek": 233028, "start": 2345.8, "end": 2353.6800000000003, "text": " off their heads, add a classification head in its place, and try to predict image net", "tokens": [766, 641, 8050, 11, 909, 257, 21538, 1378, 294, 1080, 1081, 11, 293, 853, 281, 6069, 3256, 2533], "temperature": 0.0, "avg_logprob": -0.15426916546291775, "compression_ratio": 1.7642276422764227, "no_speech_prob": 7.250785711221397e-05}, {"id": 520, "seek": 233028, "start": 2353.6800000000003, "end": 2355.44, "text": " and train on image net.", "tokens": [293, 3847, 322, 3256, 2533, 13], "temperature": 0.0, "avg_logprob": -0.15426916546291775, "compression_ratio": 1.7642276422764227, "no_speech_prob": 7.250785711221397e-05}, {"id": 521, "seek": 233028, "start": 2355.44, "end": 2359.0, "text": " And the orange curve that I've shown you here is what happens if you just take a randomly", "tokens": [400, 264, 7671, 7605, 300, 286, 600, 4898, 291, 510, 307, 437, 2314, 498, 291, 445, 747, 257, 16979], "temperature": 0.0, "avg_logprob": -0.15426916546291775, "compression_ratio": 1.7642276422764227, "no_speech_prob": 7.250785711221397e-05}, {"id": 522, "seek": 235900, "start": 2359.0, "end": 2363.04, "text": " initialized model with that architecture and train it.", "tokens": [5883, 1602, 2316, 365, 300, 9482, 293, 3847, 309, 13], "temperature": 0.0, "avg_logprob": -0.1734065189156481, "compression_ratio": 1.6877637130801688, "no_speech_prob": 3.218684651074e-05}, {"id": 523, "seek": 235900, "start": 2363.04, "end": 2368.2, "text": " You get very good performance up to a point and then performance plateaus because you're", "tokens": [509, 483, 588, 665, 3389, 493, 281, 257, 935, 293, 550, 3389, 5924, 8463, 570, 291, 434], "temperature": 0.0, "avg_logprob": -0.1734065189156481, "compression_ratio": 1.6877637130801688, "no_speech_prob": 3.218684651074e-05}, {"id": 524, "seek": 235900, "start": 2368.2, "end": 2373.32, "text": " being limited by the fact that image net is from this point of you a small data set.", "tokens": [885, 5567, 538, 264, 1186, 300, 3256, 2533, 307, 490, 341, 935, 295, 291, 257, 1359, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.1734065189156481, "compression_ratio": 1.6877637130801688, "no_speech_prob": 3.218684651074e-05}, {"id": 525, "seek": 235900, "start": 2373.32, "end": 2378.48, "text": " However, if you take these pre-trained models that have been trained generatively to", "tokens": [2908, 11, 498, 291, 747, 613, 659, 12, 17227, 2001, 5245, 300, 362, 668, 8895, 1337, 19020, 281], "temperature": 0.0, "avg_logprob": -0.1734065189156481, "compression_ratio": 1.6877637130801688, "no_speech_prob": 3.218684651074e-05}, {"id": 526, "seek": 235900, "start": 2378.48, "end": 2383.68, "text": " draw pixels, they sort of use the features, presumably they're using the features they", "tokens": [2642, 18668, 11, 436, 1333, 295, 764, 264, 4122, 11, 26742, 436, 434, 1228, 264, 4122, 436], "temperature": 0.0, "avg_logprob": -0.1734065189156481, "compression_ratio": 1.6877637130801688, "no_speech_prob": 3.218684651074e-05}, {"id": 527, "seek": 238368, "start": 2383.68, "end": 2392.44, "text": " learned from image generation for classification, and you get some nice trend for the error rate", "tokens": [3264, 490, 3256, 5125, 337, 21538, 11, 293, 291, 483, 512, 1481, 6028, 337, 264, 6713, 3314], "temperature": 0.0, "avg_logprob": -0.2006111366804256, "compression_ratio": 1.7327188940092166, "no_speech_prob": 5.337468337529572e-06}, {"id": 528, "seek": 238368, "start": 2392.44, "end": 2396.48, "text": " in classification as a function of model size.", "tokens": [294, 21538, 382, 257, 2445, 295, 2316, 2744, 13], "temperature": 0.0, "avg_logprob": -0.2006111366804256, "compression_ratio": 1.7327188940092166, "no_speech_prob": 5.337468337529572e-06}, {"id": 529, "seek": 238368, "start": 2396.48, "end": 2402.8799999999997, "text": " So this is saying that in this particular case, we actually do fine-tuning the pre-training", "tokens": [407, 341, 307, 1566, 300, 294, 341, 1729, 1389, 11, 321, 767, 360, 2489, 12, 83, 37726, 264, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.2006111366804256, "compression_ratio": 1.7327188940092166, "no_speech_prob": 5.337468337529572e-06}, {"id": 530, "seek": 238368, "start": 2402.8799999999997, "end": 2408.3199999999997, "text": " you did and the sort of trends you saw really kind of transfer into trends in something else", "tokens": [291, 630, 293, 264, 1333, 295, 13892, 291, 1866, 534, 733, 295, 5003, 666, 13892, 294, 746, 1646], "temperature": 0.0, "avg_logprob": -0.2006111366804256, "compression_ratio": 1.7327188940092166, "no_speech_prob": 5.337468337529572e-06}, {"id": 531, "seek": 238368, "start": 2408.3199999999997, "end": 2412.7599999999998, "text": " you might care about like image classification.", "tokens": [291, 1062, 1127, 466, 411, 3256, 21538, 13], "temperature": 0.0, "avg_logprob": -0.2006111366804256, "compression_ratio": 1.7327188940092166, "no_speech_prob": 5.337468337529572e-06}, {"id": 532, "seek": 241276, "start": 2412.76, "end": 2417.6800000000003, "text": " We can ask the same kinds of questions about language models.", "tokens": [492, 393, 1029, 264, 912, 3685, 295, 1651, 466, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1600731675342847, "compression_ratio": 1.7336244541484715, "no_speech_prob": 4.0053928387351334e-05}, {"id": 533, "seek": 241276, "start": 2417.6800000000003, "end": 2423.6000000000004, "text": " In particular, does this steady improvement in language modeling as a function of scale,", "tokens": [682, 1729, 11, 775, 341, 13211, 10444, 294, 2856, 15983, 382, 257, 2445, 295, 4373, 11], "temperature": 0.0, "avg_logprob": -0.1600731675342847, "compression_ratio": 1.7336244541484715, "no_speech_prob": 4.0053928387351334e-05}, {"id": 534, "seek": 241276, "start": 2423.6000000000004, "end": 2428.2400000000002, "text": " does that translate into better performance?", "tokens": [775, 300, 13799, 666, 1101, 3389, 30], "temperature": 0.0, "avg_logprob": -0.1600731675342847, "compression_ratio": 1.7336244541484715, "no_speech_prob": 4.0053928387351334e-05}, {"id": 535, "seek": 241276, "start": 2428.2400000000002, "end": 2430.36, "text": " And this is sort of an interesting subject by itself.", "tokens": [400, 341, 307, 1333, 295, 364, 1880, 3983, 538, 2564, 13], "temperature": 0.0, "avg_logprob": -0.1600731675342847, "compression_ratio": 1.7336244541484715, "no_speech_prob": 4.0053928387351334e-05}, {"id": 536, "seek": 241276, "start": 2430.36, "end": 2434.2400000000002, "text": " And so you can ask what happens if we scale language models.", "tokens": [400, 370, 291, 393, 1029, 437, 2314, 498, 321, 4373, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.1600731675342847, "compression_ratio": 1.7336244541484715, "no_speech_prob": 4.0053928387351334e-05}, {"id": 537, "seek": 241276, "start": 2434.2400000000002, "end": 2438.4, "text": " And so this is sort of this exact same plot that you've seen a couple of times now for", "tokens": [400, 370, 341, 307, 1333, 295, 341, 1900, 912, 7542, 300, 291, 600, 1612, 257, 1916, 295, 1413, 586, 337], "temperature": 0.0, "avg_logprob": -0.1600731675342847, "compression_ratio": 1.7336244541484715, "no_speech_prob": 4.0053928387351334e-05}, {"id": 538, "seek": 243840, "start": 2438.4, "end": 2444.12, "text": " language models, but it just increased from sort of original work that we did out to", "tokens": [2856, 5245, 11, 457, 309, 445, 6505, 490, 1333, 295, 3380, 589, 300, 321, 630, 484, 281], "temperature": 0.0, "avg_logprob": -0.19201422159650686, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.066878682351671e-05}, {"id": 539, "seek": 243840, "start": 2444.12, "end": 2447.36, "text": " this yellow line, which is GPT-3.", "tokens": [341, 5566, 1622, 11, 597, 307, 26039, 51, 12, 18, 13], "temperature": 0.0, "avg_logprob": -0.19201422159650686, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.066878682351671e-05}, {"id": 540, "seek": 243840, "start": 2447.36, "end": 2450.2000000000003, "text": " And you see that basically this sort of trends continue.", "tokens": [400, 291, 536, 300, 1936, 341, 1333, 295, 13892, 2354, 13], "temperature": 0.0, "avg_logprob": -0.19201422159650686, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.066878682351671e-05}, {"id": 541, "seek": 243840, "start": 2450.2000000000003, "end": 2454.6, "text": " Possibly GPT-3 is sort of missing the trend a little bit.", "tokens": [33112, 3545, 26039, 51, 12, 18, 307, 1333, 295, 5361, 264, 6028, 257, 707, 857, 13], "temperature": 0.0, "avg_logprob": -0.19201422159650686, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.066878682351671e-05}, {"id": 542, "seek": 243840, "start": 2454.6, "end": 2459.88, "text": " I can't really honestly tell you whether that's because GPT-3 wasn't well optimized, or", "tokens": [286, 393, 380, 534, 6095, 980, 291, 1968, 300, 311, 570, 26039, 51, 12, 18, 2067, 380, 731, 26941, 11, 420], "temperature": 0.0, "avg_logprob": -0.19201422159650686, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.066878682351671e-05}, {"id": 543, "seek": 243840, "start": 2459.88, "end": 2465.6800000000003, "text": " if it's because there's some bending in this curve where we're hitting some irreducible", "tokens": [498, 309, 311, 570, 456, 311, 512, 22487, 294, 341, 7605, 689, 321, 434, 8850, 512, 16014, 769, 32128], "temperature": 0.0, "avg_logprob": -0.19201422159650686, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.066878682351671e-05}, {"id": 544, "seek": 243840, "start": 2465.6800000000003, "end": 2466.6800000000003, "text": " loss.", "tokens": [4470, 13], "temperature": 0.0, "avg_logprob": -0.19201422159650686, "compression_ratio": 1.6274509803921569, "no_speech_prob": 4.066878682351671e-05}, {"id": 545, "seek": 246668, "start": 2466.68, "end": 2472.12, "text": " That irreducible loss would be something like the entropy of this sort of language data", "tokens": [663, 16014, 769, 32128, 4470, 576, 312, 746, 411, 264, 30867, 295, 341, 1333, 295, 2856, 1412], "temperature": 0.0, "avg_logprob": -0.24992631495684042, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.703859692905098e-05}, {"id": 546, "seek": 246668, "start": 2472.12, "end": 2475.2, "text": " set itself.", "tokens": [992, 2564, 13], "temperature": 0.0, "avg_logprob": -0.24992631495684042, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.703859692905098e-05}, {"id": 547, "seek": 246668, "start": 2475.2, "end": 2478.72, "text": " But it's just in order the trends continue.", "tokens": [583, 309, 311, 445, 294, 1668, 264, 13892, 2354, 13], "temperature": 0.0, "avg_logprob": -0.24992631495684042, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.703859692905098e-05}, {"id": 548, "seek": 246668, "start": 2478.72, "end": 2484.9199999999996, "text": " And what's now pretty well known is that if you train fairly large language models,", "tokens": [400, 437, 311, 586, 1238, 731, 2570, 307, 300, 498, 291, 3847, 6457, 2416, 2856, 5245, 11], "temperature": 0.0, "avg_logprob": -0.24992631495684042, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.703859692905098e-05}, {"id": 549, "seek": 246668, "start": 2484.9199999999996, "end": 2487.6, "text": " then they can exhibit in context learning.", "tokens": [550, 436, 393, 20487, 294, 4319, 2539, 13], "temperature": 0.0, "avg_logprob": -0.24992631495684042, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.703859692905098e-05}, {"id": 550, "seek": 246668, "start": 2487.6, "end": 2494.64, "text": " So the kind of learning that I'm talking about is that you give these models an example", "tokens": [407, 264, 733, 295, 2539, 300, 286, 478, 1417, 466, 307, 300, 291, 976, 613, 5245, 364, 1365], "temperature": 0.0, "avg_logprob": -0.24992631495684042, "compression_ratio": 1.577092511013216, "no_speech_prob": 3.703859692905098e-05}, {"id": 551, "seek": 249464, "start": 2494.64, "end": 2502.08, "text": " of many arithmetic problems or many anagrams or whatnot or translation tasks for individual", "tokens": [295, 867, 42973, 2740, 420, 867, 364, 3914, 82, 420, 25882, 420, 12853, 9608, 337, 2609], "temperature": 0.0, "avg_logprob": -0.17785335339997943, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.538673601928167e-05}, {"id": 552, "seek": 249464, "start": 2502.08, "end": 2503.08, "text": " words.", "tokens": [2283, 13], "temperature": 0.0, "avg_logprob": -0.17785335339997943, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.538673601928167e-05}, {"id": 553, "seek": 249464, "start": 2503.08, "end": 2509.2, "text": " Then early on in the sequence of the top, they might not be very good at doing the task,", "tokens": [1396, 2440, 322, 294, 264, 8310, 295, 264, 1192, 11, 436, 1062, 406, 312, 588, 665, 412, 884, 264, 5633, 11], "temperature": 0.0, "avg_logprob": -0.17785335339997943, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.538673601928167e-05}, {"id": 554, "seek": 249464, "start": 2509.2, "end": 2514.12, "text": " but they figure out what the pattern is in the task and they learn to do it.", "tokens": [457, 436, 2573, 484, 437, 264, 5102, 307, 294, 264, 5633, 293, 436, 1466, 281, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.17785335339997943, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.538673601928167e-05}, {"id": 555, "seek": 249464, "start": 2514.12, "end": 2518.7599999999998, "text": " And in particular, you can plot that so you can ask for, say, like one of these anagram", "tokens": [400, 294, 1729, 11, 291, 393, 7542, 300, 370, 291, 393, 1029, 337, 11, 584, 11, 411, 472, 295, 613, 364, 3914], "temperature": 0.0, "avg_logprob": -0.17785335339997943, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.538673601928167e-05}, {"id": 556, "seek": 249464, "start": 2518.7599999999998, "end": 2520.7999999999997, "text": " tasks.", "tokens": [9608, 13], "temperature": 0.0, "avg_logprob": -0.17785335339997943, "compression_ratio": 1.669767441860465, "no_speech_prob": 4.538673601928167e-05}, {"id": 557, "seek": 252080, "start": 2520.8, "end": 2526.36, "text": " What is the performance of the model as a function of how many examples of the task get seen", "tokens": [708, 307, 264, 3389, 295, 264, 2316, 382, 257, 2445, 295, 577, 867, 5110, 295, 264, 5633, 483, 1612], "temperature": 0.0, "avg_logprob": -0.14863157272338867, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.00010388668306404725}, {"id": 558, "seek": 252080, "start": 2526.36, "end": 2527.36, "text": " in the context?", "tokens": [294, 264, 4319, 30], "temperature": 0.0, "avg_logprob": -0.14863157272338867, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.00010388668306404725}, {"id": 559, "seek": 252080, "start": 2527.36, "end": 2532.0800000000004, "text": " So this is kind of similar to the loss as a function of context position, but it's now", "tokens": [407, 341, 307, 733, 295, 2531, 281, 264, 4470, 382, 257, 2445, 295, 4319, 2535, 11, 457, 309, 311, 586], "temperature": 0.0, "avg_logprob": -0.14863157272338867, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.00010388668306404725}, {"id": 560, "seek": 252080, "start": 2532.0800000000004, "end": 2536.76, "text": " an accuracy at doing an actual task, like unscramble the letters in a word.", "tokens": [364, 14170, 412, 884, 364, 3539, 5633, 11, 411, 2693, 66, 48382, 264, 7825, 294, 257, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14863157272338867, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.00010388668306404725}, {"id": 561, "seek": 252080, "start": 2536.76, "end": 2543.1600000000003, "text": " And you see probably most importantly that if you give more examples, you get significantly", "tokens": [400, 291, 536, 1391, 881, 8906, 300, 498, 291, 976, 544, 5110, 11, 291, 483, 10591], "temperature": 0.0, "avg_logprob": -0.14863157272338867, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.00010388668306404725}, {"id": 562, "seek": 252080, "start": 2543.1600000000003, "end": 2548.2000000000003, "text": " better performance starting from very, very poor performance to pretty good.", "tokens": [1101, 3389, 2891, 490, 588, 11, 588, 4716, 3389, 281, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.14863157272338867, "compression_ratio": 1.7322834645669292, "no_speech_prob": 0.00010388668306404725}, {"id": 563, "seek": 254820, "start": 2548.2, "end": 2551.7999999999997, "text": " And also you see that larger models do this better.", "tokens": [400, 611, 291, 536, 300, 4833, 5245, 360, 341, 1101, 13], "temperature": 0.0, "avg_logprob": -0.16096434432469056, "compression_ratio": 1.6694214876033058, "no_speech_prob": 5.059538671048358e-05}, {"id": 564, "seek": 254820, "start": 2551.7999999999997, "end": 2555.8799999999997, "text": " You also finally see that giving a natural language prompt with some instructions helps", "tokens": [509, 611, 2721, 536, 300, 2902, 257, 3303, 2856, 12391, 365, 512, 9415, 3665], "temperature": 0.0, "avg_logprob": -0.16096434432469056, "compression_ratio": 1.6694214876033058, "no_speech_prob": 5.059538671048358e-05}, {"id": 565, "seek": 254820, "start": 2555.8799999999997, "end": 2560.3199999999997, "text": " significantly in the regime where you have very few examples.", "tokens": [10591, 294, 264, 13120, 689, 291, 362, 588, 1326, 5110, 13], "temperature": 0.0, "avg_logprob": -0.16096434432469056, "compression_ratio": 1.6694214876033058, "no_speech_prob": 5.059538671048358e-05}, {"id": 566, "seek": 254820, "start": 2560.3199999999997, "end": 2562.56, "text": " This is in context learning.", "tokens": [639, 307, 294, 4319, 2539, 13], "temperature": 0.0, "avg_logprob": -0.16096434432469056, "compression_ratio": 1.6694214876033058, "no_speech_prob": 5.059538671048358e-05}, {"id": 567, "seek": 254820, "start": 2562.56, "end": 2566.16, "text": " You can call this a kind of meta learning.", "tokens": [509, 393, 818, 341, 257, 733, 295, 19616, 2539, 13], "temperature": 0.0, "avg_logprob": -0.16096434432469056, "compression_ratio": 1.6694214876033058, "no_speech_prob": 5.059538671048358e-05}, {"id": 568, "seek": 254820, "start": 2566.16, "end": 2572.72, "text": " And it just emerges automatically from training large language models without any particular", "tokens": [400, 309, 445, 38965, 6772, 490, 3097, 2416, 2856, 5245, 1553, 604, 1729], "temperature": 0.0, "avg_logprob": -0.16096434432469056, "compression_ratio": 1.6694214876033058, "no_speech_prob": 5.059538671048358e-05}, {"id": 569, "seek": 254820, "start": 2572.72, "end": 2577.6, "text": " attempt to get this kind of behavior.", "tokens": [5217, 281, 483, 341, 733, 295, 5223, 13], "temperature": 0.0, "avg_logprob": -0.16096434432469056, "compression_ratio": 1.6694214876033058, "no_speech_prob": 5.059538671048358e-05}, {"id": 570, "seek": 257760, "start": 2577.6, "end": 2583.56, "text": " And you could also ask sort of about downstream tasks that you actually you care about.", "tokens": [400, 291, 727, 611, 1029, 1333, 295, 466, 30621, 9608, 300, 291, 767, 291, 1127, 466, 13], "temperature": 0.0, "avg_logprob": -0.1999360842582507, "compression_ratio": 1.5625, "no_speech_prob": 4.195868314127438e-05}, {"id": 571, "seek": 257760, "start": 2583.56, "end": 2588.72, "text": " So there is accuracy at doing arithmetic as a function of model size, a bunch of different", "tokens": [407, 456, 307, 14170, 412, 884, 42973, 382, 257, 2445, 295, 2316, 2744, 11, 257, 3840, 295, 819], "temperature": 0.0, "avg_logprob": -0.1999360842582507, "compression_ratio": 1.5625, "no_speech_prob": 4.195868314127438e-05}, {"id": 572, "seek": 257760, "start": 2588.72, "end": 2591.44, "text": " kinds of arithmetic problems.", "tokens": [3685, 295, 42973, 2740, 13], "temperature": 0.0, "avg_logprob": -0.1999360842582507, "compression_ratio": 1.5625, "no_speech_prob": 4.195868314127438e-05}, {"id": 573, "seek": 257760, "start": 2591.44, "end": 2599.16, "text": " There is just some data set of analogies from a test that American college students take", "tokens": [821, 307, 445, 512, 1412, 992, 295, 16660, 530, 490, 257, 1500, 300, 2665, 3859, 1731, 747], "temperature": 0.0, "avg_logprob": -0.1999360842582507, "compression_ratio": 1.5625, "no_speech_prob": 4.195868314127438e-05}, {"id": 574, "seek": 257760, "start": 2599.16, "end": 2603.2799999999997, "text": " to go to college, the SATs.", "tokens": [281, 352, 281, 3859, 11, 264, 31536, 82, 13], "temperature": 0.0, "avg_logprob": -0.1999360842582507, "compression_ratio": 1.5625, "no_speech_prob": 4.195868314127438e-05}, {"id": 575, "seek": 260328, "start": 2603.28, "end": 2611.48, "text": " And if you care the sort of average score of that year's test was I think 58% or so", "tokens": [400, 498, 291, 1127, 264, 1333, 295, 4274, 6175, 295, 300, 1064, 311, 1500, 390, 286, 519, 21786, 4, 420, 370], "temperature": 0.0, "avg_logprob": -0.2257753455120584, "compression_ratio": 1.5291479820627802, "no_speech_prob": 2.7943802706431597e-05}, {"id": 576, "seek": 260328, "start": 2611.48, "end": 2612.48, "text": " percent.", "tokens": [3043, 13], "temperature": 0.0, "avg_logprob": -0.2257753455120584, "compression_ratio": 1.5291479820627802, "no_speech_prob": 2.7943802706431597e-05}, {"id": 577, "seek": 260328, "start": 2612.48, "end": 2615.84, "text": " So the largest model is sort of doing a little bit better than the average American high", "tokens": [407, 264, 6443, 2316, 307, 1333, 295, 884, 257, 707, 857, 1101, 813, 264, 4274, 2665, 1090], "temperature": 0.0, "avg_logprob": -0.2257753455120584, "compression_ratio": 1.5291479820627802, "no_speech_prob": 2.7943802706431597e-05}, {"id": 578, "seek": 260328, "start": 2615.84, "end": 2617.96, "text": " school student.", "tokens": [1395, 3107, 13], "temperature": 0.0, "avg_logprob": -0.2257753455120584, "compression_ratio": 1.5291479820627802, "no_speech_prob": 2.7943802706431597e-05}, {"id": 579, "seek": 260328, "start": 2617.96, "end": 2622.36, "text": " The trivia QA, which is sort of just knowing trivia.", "tokens": [440, 48770, 1249, 32, 11, 597, 307, 1333, 295, 445, 5276, 48770, 13], "temperature": 0.0, "avg_logprob": -0.2257753455120584, "compression_ratio": 1.5291479820627802, "no_speech_prob": 2.7943802706431597e-05}, {"id": 580, "seek": 260328, "start": 2622.36, "end": 2630.36, "text": " And Wina grad schemas are problems like if a tree falls on your roof and you got it fixed,", "tokens": [400, 343, 1426, 2771, 22627, 296, 366, 2740, 411, 498, 257, 4230, 8804, 322, 428, 8418, 293, 291, 658, 309, 6806, 11], "temperature": 0.0, "avg_logprob": -0.2257753455120584, "compression_ratio": 1.5291479820627802, "no_speech_prob": 2.7943802706431597e-05}, {"id": 581, "seek": 263036, "start": 2630.36, "end": 2633.76, "text": " what did you get fixed, did you get the tree fixed or your roof.", "tokens": [437, 630, 291, 483, 6806, 11, 630, 291, 483, 264, 4230, 6806, 420, 428, 8418, 13], "temperature": 0.0, "avg_logprob": -0.16945008206958614, "compression_ratio": 1.8101694915254238, "no_speech_prob": 1.0287317309121136e-05}, {"id": 582, "seek": 263036, "start": 2633.76, "end": 2637.04, "text": " It's a measure of common sense reasoning and models are also getting better at this.", "tokens": [467, 311, 257, 3481, 295, 2689, 2020, 21577, 293, 5245, 366, 611, 1242, 1101, 412, 341, 13], "temperature": 0.0, "avg_logprob": -0.16945008206958614, "compression_ratio": 1.8101694915254238, "no_speech_prob": 1.0287317309121136e-05}, {"id": 583, "seek": 263036, "start": 2637.04, "end": 2643.0, "text": " And I think the other interesting thing that's very often emphasized is that clearly trivia", "tokens": [400, 286, 519, 264, 661, 1880, 551, 300, 311, 588, 2049, 34068, 307, 300, 4448, 48770], "temperature": 0.0, "avg_logprob": -0.16945008206958614, "compression_ratio": 1.8101694915254238, "no_speech_prob": 1.0287317309121136e-05}, {"id": 584, "seek": 263036, "start": 2643.0, "end": 2645.92, "text": " performance is improving very smoothly as you make models bigger.", "tokens": [3389, 307, 11470, 588, 19565, 382, 291, 652, 5245, 3801, 13], "temperature": 0.0, "avg_logprob": -0.16945008206958614, "compression_ratio": 1.8101694915254238, "no_speech_prob": 1.0287317309121136e-05}, {"id": 585, "seek": 263036, "start": 2645.92, "end": 2649.8, "text": " The models are just remembering more and more trivia.", "tokens": [440, 5245, 366, 445, 20719, 544, 293, 544, 48770, 13], "temperature": 0.0, "avg_logprob": -0.16945008206958614, "compression_ratio": 1.8101694915254238, "no_speech_prob": 1.0287317309121136e-05}, {"id": 586, "seek": 263036, "start": 2649.8, "end": 2654.04, "text": " Wina grad schemas are also improving fairly smoothly.", "tokens": [343, 1426, 2771, 22627, 296, 366, 611, 11470, 6457, 19565, 13], "temperature": 0.0, "avg_logprob": -0.16945008206958614, "compression_ratio": 1.8101694915254238, "no_speech_prob": 1.0287317309121136e-05}, {"id": 587, "seek": 263036, "start": 2654.04, "end": 2657.6, "text": " But then there are examples like arithmetic where models are very poor and then they sort", "tokens": [583, 550, 456, 366, 5110, 411, 42973, 689, 5245, 366, 588, 4716, 293, 550, 436, 1333], "temperature": 0.0, "avg_logprob": -0.16945008206958614, "compression_ratio": 1.8101694915254238, "no_speech_prob": 1.0287317309121136e-05}, {"id": 588, "seek": 263036, "start": 2657.6, "end": 2659.96, "text": " of suddenly get pretty good.", "tokens": [295, 5800, 483, 1238, 665, 13], "temperature": 0.0, "avg_logprob": -0.16945008206958614, "compression_ratio": 1.8101694915254238, "no_speech_prob": 1.0287317309121136e-05}, {"id": 589, "seek": 265996, "start": 2659.96, "end": 2665.6, "text": " And so these kind of sudden rocks sort of the model sort of suddenly kind of like gets", "tokens": [400, 370, 613, 733, 295, 3990, 10989, 1333, 295, 264, 2316, 1333, 295, 5800, 733, 295, 411, 2170], "temperature": 0.0, "avg_logprob": -0.3159629971373315, "compression_ratio": 1.7291666666666667, "no_speech_prob": 5.42078214493813e-06}, {"id": 590, "seek": 265996, "start": 2665.6, "end": 2668.0, "text": " what it's supposed to do for arithmetic are pretty interesting.", "tokens": [437, 309, 311, 3442, 281, 360, 337, 42973, 366, 1238, 1880, 13], "temperature": 0.0, "avg_logprob": -0.3159629971373315, "compression_ratio": 1.7291666666666667, "no_speech_prob": 5.42078214493813e-06}, {"id": 591, "seek": 265996, "start": 2668.0, "end": 2672.76, "text": " And there are all sorts of other kind of interesting things if you kind of dig into these", "tokens": [400, 456, 366, 439, 7527, 295, 661, 733, 295, 1880, 721, 498, 291, 733, 295, 2528, 666, 613], "temperature": 0.0, "avg_logprob": -0.3159629971373315, "compression_ratio": 1.7291666666666667, "no_speech_prob": 5.42078214493813e-06}, {"id": 592, "seek": 265996, "start": 2672.76, "end": 2673.76, "text": " specific abilities.", "tokens": [2685, 11582, 13], "temperature": 0.0, "avg_logprob": -0.3159629971373315, "compression_ratio": 1.7291666666666667, "no_speech_prob": 5.42078214493813e-06}, {"id": 593, "seek": 265996, "start": 2673.76, "end": 2674.76, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.3159629971373315, "compression_ratio": 1.7291666666666667, "no_speech_prob": 5.42078214493813e-06}, {"id": 594, "seek": 265996, "start": 2674.76, "end": 2681.68, "text": " Why do bigger models do better in the context of student?", "tokens": [1545, 360, 3801, 5245, 360, 1101, 294, 264, 4319, 295, 3107, 30], "temperature": 0.0, "avg_logprob": -0.3159629971373315, "compression_ratio": 1.7291666666666667, "no_speech_prob": 5.42078214493813e-06}, {"id": 595, "seek": 265996, "start": 2681.68, "end": 2688.16, "text": " I mean, I guess the sort of dumb zero-thorder point is that larger models are just getting", "tokens": [286, 914, 11, 286, 2041, 264, 1333, 295, 10316, 4018, 12, 392, 4687, 935, 307, 300, 4833, 5245, 366, 445, 1242], "temperature": 0.0, "avg_logprob": -0.3159629971373315, "compression_ratio": 1.7291666666666667, "no_speech_prob": 5.42078214493813e-06}, {"id": 596, "seek": 268816, "start": 2688.16, "end": 2692.7599999999998, "text": " much better and better at predicting the next word given more and more context.", "tokens": [709, 1101, 293, 1101, 412, 32884, 264, 958, 1349, 2212, 544, 293, 544, 4319, 13], "temperature": 0.0, "avg_logprob": -0.20522379652362002, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320240284665488e-05}, {"id": 597, "seek": 268816, "start": 2692.7599999999998, "end": 2698.8399999999997, "text": " So I think it like, I think there's a very tight connection between a plot like this and", "tokens": [407, 286, 519, 309, 411, 11, 286, 519, 456, 311, 257, 588, 4524, 4984, 1296, 257, 7542, 411, 341, 293], "temperature": 0.0, "avg_logprob": -0.20522379652362002, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320240284665488e-05}, {"id": 598, "seek": 268816, "start": 2698.8399999999997, "end": 2702.08, "text": " these sort of in context loading plots.", "tokens": [613, 1333, 295, 294, 4319, 15114, 28609, 13], "temperature": 0.0, "avg_logprob": -0.20522379652362002, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320240284665488e-05}, {"id": 599, "seek": 268816, "start": 2702.08, "end": 2706.72, "text": " Basically the more information you're getting, I mean all of these models probably know the", "tokens": [8537, 264, 544, 1589, 291, 434, 1242, 11, 286, 914, 439, 295, 613, 5245, 1391, 458, 264], "temperature": 0.0, "avg_logprob": -0.20522379652362002, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320240284665488e-05}, {"id": 600, "seek": 268816, "start": 2706.72, "end": 2711.2, "text": " unigram distribution of words and tokens pretty well.", "tokens": [517, 33737, 7316, 295, 2283, 293, 22667, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.20522379652362002, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320240284665488e-05}, {"id": 601, "seek": 268816, "start": 2711.2, "end": 2715.7999999999997, "text": " But the bigger model is getting much, much, much more information from its context than", "tokens": [583, 264, 3801, 2316, 307, 1242, 709, 11, 709, 11, 709, 544, 1589, 490, 1080, 4319, 813], "temperature": 0.0, "avg_logprob": -0.20522379652362002, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320240284665488e-05}, {"id": 602, "seek": 268816, "start": 2715.7999999999997, "end": 2717.2799999999997, "text": " the smaller models.", "tokens": [264, 4356, 5245, 13], "temperature": 0.0, "avg_logprob": -0.20522379652362002, "compression_ratio": 1.7837837837837838, "no_speech_prob": 3.320240284665488e-05}, {"id": 603, "seek": 271728, "start": 2717.28, "end": 2721.1200000000003, "text": " And at a certain point, I mean, it depends on your training distribution and all sorts", "tokens": [400, 412, 257, 1629, 935, 11, 286, 914, 11, 309, 5946, 322, 428, 3097, 7316, 293, 439, 7527], "temperature": 0.0, "avg_logprob": -0.1305372735728388, "compression_ratio": 1.7003891050583657, "no_speech_prob": 7.02444594935514e-05}, {"id": 604, "seek": 271728, "start": 2721.1200000000003, "end": 2722.1200000000003, "text": " of other things.", "tokens": [295, 661, 721, 13], "temperature": 0.0, "avg_logprob": -0.1305372735728388, "compression_ratio": 1.7003891050583657, "no_speech_prob": 7.02444594935514e-05}, {"id": 605, "seek": 271728, "start": 2722.1200000000003, "end": 2727.2000000000003, "text": " But like, one of the things that we do is when we see several examples of something happening", "tokens": [583, 411, 11, 472, 295, 264, 721, 300, 321, 360, 307, 562, 321, 536, 2940, 5110, 295, 746, 2737], "temperature": 0.0, "avg_logprob": -0.1305372735728388, "compression_ratio": 1.7003891050583657, "no_speech_prob": 7.02444594935514e-05}, {"id": 606, "seek": 271728, "start": 2727.2000000000003, "end": 2732.1600000000003, "text": " in a text, we guess that that's what we're going to see next.", "tokens": [294, 257, 2487, 11, 321, 2041, 300, 300, 311, 437, 321, 434, 516, 281, 536, 958, 13], "temperature": 0.0, "avg_logprob": -0.1305372735728388, "compression_ratio": 1.7003891050583657, "no_speech_prob": 7.02444594935514e-05}, {"id": 607, "seek": 271728, "start": 2732.1600000000003, "end": 2737.0800000000004, "text": " And that's really probably embedded in a ton of text that's out there on the internet", "tokens": [400, 300, 311, 534, 1391, 16741, 294, 257, 2952, 295, 2487, 300, 311, 484, 456, 322, 264, 4705], "temperature": 0.0, "avg_logprob": -0.1305372735728388, "compression_ratio": 1.7003891050583657, "no_speech_prob": 7.02444594935514e-05}, {"id": 608, "seek": 271728, "start": 2737.0800000000004, "end": 2738.32, "text": " and in books.", "tokens": [293, 294, 3642, 13], "temperature": 0.0, "avg_logprob": -0.1305372735728388, "compression_ratio": 1.7003891050583657, "no_speech_prob": 7.02444594935514e-05}, {"id": 609, "seek": 271728, "start": 2738.32, "end": 2741.7200000000003, "text": " And models have to decrease their loss somehow.", "tokens": [400, 5245, 362, 281, 11514, 641, 4470, 6063, 13], "temperature": 0.0, "avg_logprob": -0.1305372735728388, "compression_ratio": 1.7003891050583657, "no_speech_prob": 7.02444594935514e-05}, {"id": 610, "seek": 271728, "start": 2741.7200000000003, "end": 2743.36, "text": " That's a pattern in the text.", "tokens": [663, 311, 257, 5102, 294, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1305372735728388, "compression_ratio": 1.7003891050583657, "no_speech_prob": 7.02444594935514e-05}, {"id": 611, "seek": 274336, "start": 2743.36, "end": 2748.44, "text": " It's a pattern that models eventually learn and they seemingly apply this knowledge.", "tokens": [467, 311, 257, 5102, 300, 5245, 4728, 1466, 293, 436, 18709, 3079, 341, 3601, 13], "temperature": 0.0, "avg_logprob": -0.22395068675548108, "compression_ratio": 1.7442748091603053, "no_speech_prob": 7.134663610486314e-05}, {"id": 612, "seek": 274336, "start": 2748.44, "end": 2751.56, "text": " I think there are other people, of course, who've kind of worked on this question more", "tokens": [286, 519, 456, 366, 661, 561, 11, 295, 1164, 11, 567, 600, 733, 295, 2732, 322, 341, 1168, 544], "temperature": 0.0, "avg_logprob": -0.22395068675548108, "compression_ratio": 1.7442748091603053, "no_speech_prob": 7.134663610486314e-05}, {"id": 613, "seek": 274336, "start": 2751.56, "end": 2753.08, "text": " specifically, and I have more specific theories.", "tokens": [4682, 11, 293, 286, 362, 544, 2685, 13667, 13], "temperature": 0.0, "avg_logprob": -0.22395068675548108, "compression_ratio": 1.7442748091603053, "no_speech_prob": 7.134663610486314e-05}, {"id": 614, "seek": 274336, "start": 2753.08, "end": 2762.6, "text": " But I think it like kind of an intuitive sense, that's how I would think about it.", "tokens": [583, 286, 519, 309, 411, 733, 295, 364, 21769, 2020, 11, 300, 311, 577, 286, 576, 519, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.22395068675548108, "compression_ratio": 1.7442748091603053, "no_speech_prob": 7.134663610486314e-05}, {"id": 615, "seek": 274336, "start": 2762.6, "end": 2768.92, "text": " I guess one final evaluation you can ask, can people tell that text written by a language", "tokens": [286, 2041, 472, 2572, 13344, 291, 393, 1029, 11, 393, 561, 980, 300, 2487, 3720, 538, 257, 2856], "temperature": 0.0, "avg_logprob": -0.22395068675548108, "compression_ratio": 1.7442748091603053, "no_speech_prob": 7.134663610486314e-05}, {"id": 616, "seek": 274336, "start": 2768.92, "end": 2772.6400000000003, "text": " model, it was written by a language model or that it's a human?", "tokens": [2316, 11, 309, 390, 3720, 538, 257, 2856, 2316, 420, 300, 309, 311, 257, 1952, 30], "temperature": 0.0, "avg_logprob": -0.22395068675548108, "compression_ratio": 1.7442748091603053, "no_speech_prob": 7.134663610486314e-05}, {"id": 617, "seek": 277264, "start": 2772.64, "end": 2776.96, "text": " This is an evaluation where we looked at short news articles.", "tokens": [639, 307, 364, 13344, 689, 321, 2956, 412, 2099, 2583, 11290, 13], "temperature": 0.0, "avg_logprob": -0.19707843744866202, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.00014181988080963492}, {"id": 618, "seek": 277264, "start": 2776.96, "end": 2782.64, "text": " There's two or three paragraphs and generated equivalent news articles from GPT-3.", "tokens": [821, 311, 732, 420, 1045, 48910, 293, 10833, 10344, 2583, 11290, 490, 26039, 51, 12, 18, 13], "temperature": 0.0, "avg_logprob": -0.19707843744866202, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.00014181988080963492}, {"id": 619, "seek": 277264, "start": 2782.64, "end": 2787.2, "text": " And by the time you get to sort of the largest models, people are approaching chance accuracy", "tokens": [400, 538, 264, 565, 291, 483, 281, 1333, 295, 264, 6443, 5245, 11, 561, 366, 14908, 2931, 14170], "temperature": 0.0, "avg_logprob": -0.19707843744866202, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.00014181988080963492}, {"id": 620, "seek": 277264, "start": 2787.2, "end": 2788.8799999999997, "text": " at being able to tell the difference.", "tokens": [412, 885, 1075, 281, 980, 264, 2649, 13], "temperature": 0.0, "avg_logprob": -0.19707843744866202, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.00014181988080963492}, {"id": 621, "seek": 277264, "start": 2788.8799999999997, "end": 2793.48, "text": " This sort of has a lot of implications, both, I mean, it's interesting and surprising", "tokens": [639, 1333, 295, 575, 257, 688, 295, 16602, 11, 1293, 11, 286, 914, 11, 309, 311, 1880, 293, 8830], "temperature": 0.0, "avg_logprob": -0.19707843744866202, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.00014181988080963492}, {"id": 622, "seek": 277264, "start": 2793.48, "end": 2795.6, "text": " as a state and it's about language modeling.", "tokens": [382, 257, 1785, 293, 309, 311, 466, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.19707843744866202, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.00014181988080963492}, {"id": 623, "seek": 277264, "start": 2795.6, "end": 2798.3199999999997, "text": " But it's also somewhat scary.", "tokens": [583, 309, 311, 611, 8344, 6958, 13], "temperature": 0.0, "avg_logprob": -0.19707843744866202, "compression_ratio": 1.6007326007326008, "no_speech_prob": 0.00014181988080963492}, {"id": 624, "seek": 279832, "start": 2798.32, "end": 2803.7200000000003, "text": " That means these language models are very difficult to tell that you're talking to a language", "tokens": [663, 1355, 613, 2856, 5245, 366, 588, 2252, 281, 980, 300, 291, 434, 1417, 281, 257, 2856], "temperature": 0.0, "avg_logprob": -0.6883467038472494, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00010543136158958077}, {"id": 625, "seek": 279832, "start": 2803.7200000000003, "end": 2807.44, "text": " model if you don't have a very long conversation.", "tokens": [2316, 498, 291, 500, 380, 362, 257, 588, 938, 3761, 13], "temperature": 0.0, "avg_logprob": -0.6883467038472494, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00010543136158958077}, {"id": 626, "seek": 279832, "start": 2807.44, "end": 2808.44, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.6883467038472494, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00010543136158958077}, {"id": 627, "seek": 279832, "start": 2808.44, "end": 2809.44, "text": " Hi.", "tokens": [2421, 13], "temperature": 0.0, "avg_logprob": -0.6883467038472494, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00010543136158958077}, {"id": 628, "seek": 279832, "start": 2809.44, "end": 2818.44, "text": " So I am wondering, so for this specific statement, because with modern current models, they", "tokens": [407, 286, 669, 6359, 11, 370, 337, 341, 2685, 5629, 11, 570, 365, 4363, 2190, 5245, 11, 436], "temperature": 0.0, "avg_logprob": -0.6883467038472494, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00010543136158958077}, {"id": 629, "seek": 279832, "start": 2818.44, "end": 2820.32, "text": " are what projects are going to use you.", "tokens": [366, 437, 4455, 366, 516, 281, 764, 291, 13], "temperature": 0.0, "avg_logprob": -0.6883467038472494, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00010543136158958077}, {"id": 630, "seek": 279832, "start": 2820.32, "end": 2825.1600000000003, "text": " So have you attended, like, the general experience article, and what you don't have, and for", "tokens": [407, 362, 291, 15990, 11, 411, 11, 264, 2674, 1752, 7222, 11, 293, 437, 291, 500, 380, 362, 11, 293, 337], "temperature": 0.0, "avg_logprob": -0.6883467038472494, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.00010543136158958077}, {"id": 631, "seek": 282516, "start": 2825.16, "end": 2828.64, "text": " a document, or do you have anything?", "tokens": [257, 4166, 11, 420, 360, 291, 362, 1340, 30], "temperature": 0.0, "avg_logprob": -0.2760530435121976, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0001441244239686057}, {"id": 632, "seek": 282516, "start": 2828.64, "end": 2832.2799999999997, "text": " I actually don't know the answer to that question for this particular analysis off the top of", "tokens": [286, 767, 500, 380, 458, 264, 1867, 281, 300, 1168, 337, 341, 1729, 5215, 766, 264, 1192, 295], "temperature": 0.0, "avg_logprob": -0.2760530435121976, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0001441244239686057}, {"id": 633, "seek": 282516, "start": 2832.2799999999997, "end": 2834.16, "text": " my head.", "tokens": [452, 1378, 13], "temperature": 0.0, "avg_logprob": -0.2760530435121976, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0001441244239686057}, {"id": 634, "seek": 282516, "start": 2834.16, "end": 2839.7599999999998, "text": " I believe that these are not memorized.", "tokens": [286, 1697, 300, 613, 366, 406, 46677, 13], "temperature": 0.0, "avg_logprob": -0.2760530435121976, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0001441244239686057}, {"id": 635, "seek": 282516, "start": 2839.7599999999998, "end": 2843.8799999999997, "text": " One simple thing you can do, at least, for some things that occur frequently is like you", "tokens": [1485, 2199, 551, 291, 393, 360, 11, 412, 1935, 11, 337, 512, 721, 300, 5160, 10374, 307, 411, 291], "temperature": 0.0, "avg_logprob": -0.2760530435121976, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0001441244239686057}, {"id": 636, "seek": 282516, "start": 2843.8799999999997, "end": 2849.08, "text": " can look at the distribution of the loss for a model on its own samples.", "tokens": [393, 574, 412, 264, 7316, 295, 264, 4470, 337, 257, 2316, 322, 1080, 1065, 10938, 13], "temperature": 0.0, "avg_logprob": -0.2760530435121976, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0001441244239686057}, {"id": 637, "seek": 282516, "start": 2849.08, "end": 2854.7999999999997, "text": " And at least for things that are memorized, that are very clearly memorized.", "tokens": [400, 412, 1935, 337, 721, 300, 366, 46677, 11, 300, 366, 588, 4448, 46677, 13], "temperature": 0.0, "avg_logprob": -0.2760530435121976, "compression_ratio": 1.7131147540983607, "no_speech_prob": 0.0001441244239686057}, {"id": 638, "seek": 285480, "start": 2854.8, "end": 2858.0800000000004, "text": " Obviously they, of course, probably they first frequently in the training set, but also", "tokens": [7580, 436, 11, 295, 1164, 11, 1391, 436, 700, 10374, 294, 264, 3097, 992, 11, 457, 611], "temperature": 0.0, "avg_logprob": -0.19729923432873142, "compression_ratio": 1.6816720257234727, "no_speech_prob": 8.21633730083704e-05}, {"id": 639, "seek": 285480, "start": 2858.0800000000004, "end": 2861.5600000000004, "text": " the loss tends to be much, much lower on memorized samples.", "tokens": [264, 4470, 12258, 281, 312, 709, 11, 709, 3126, 322, 46677, 10938, 13], "temperature": 0.0, "avg_logprob": -0.19729923432873142, "compression_ratio": 1.6816720257234727, "no_speech_prob": 8.21633730083704e-05}, {"id": 640, "seek": 285480, "start": 2861.5600000000004, "end": 2866.4, "text": " Because you can intuitively understand this because if there's 100 words that are exactly", "tokens": [1436, 291, 393, 46506, 1223, 341, 570, 498, 456, 311, 2319, 2283, 300, 366, 2293], "temperature": 0.0, "avg_logprob": -0.19729923432873142, "compression_ratio": 1.6816720257234727, "no_speech_prob": 8.21633730083704e-05}, {"id": 641, "seek": 285480, "start": 2866.4, "end": 2872.6000000000004, "text": " verbatim sampled out, and you're sampling at temperature equals one, then all of the", "tokens": [9595, 267, 332, 3247, 15551, 484, 11, 293, 291, 434, 21179, 412, 4292, 6915, 472, 11, 550, 439, 295, 264], "temperature": 0.0, "avg_logprob": -0.19729923432873142, "compression_ratio": 1.6816720257234727, "no_speech_prob": 8.21633730083704e-05}, {"id": 642, "seek": 285480, "start": 2872.6000000000004, "end": 2875.88, "text": " next word predictions have to be extremely, extremely confident.", "tokens": [958, 1349, 21264, 362, 281, 312, 4664, 11, 4664, 6679, 13], "temperature": 0.0, "avg_logprob": -0.19729923432873142, "compression_ratio": 1.6816720257234727, "no_speech_prob": 8.21633730083704e-05}, {"id": 643, "seek": 285480, "start": 2875.88, "end": 2877.4, "text": " And that means the loss has to be super low.", "tokens": [400, 300, 1355, 264, 4470, 575, 281, 312, 1687, 2295, 13], "temperature": 0.0, "avg_logprob": -0.19729923432873142, "compression_ratio": 1.6816720257234727, "no_speech_prob": 8.21633730083704e-05}, {"id": 644, "seek": 285480, "start": 2877.4, "end": 2882.36, "text": " So, I mean, just informally, something that I've done to just get rid of memorized samples", "tokens": [407, 11, 286, 914, 11, 445, 1356, 379, 11, 746, 300, 286, 600, 1096, 281, 445, 483, 3973, 295, 46677, 10938], "temperature": 0.0, "avg_logprob": -0.19729923432873142, "compression_ratio": 1.6816720257234727, "no_speech_prob": 8.21633730083704e-05}, {"id": 645, "seek": 288236, "start": 2882.36, "end": 2885.8, "text": " is compute the loss, and usually you'll just see a pretty clear by-modal where there'll", "tokens": [307, 14722, 264, 4470, 11, 293, 2673, 291, 603, 445, 536, 257, 1238, 1850, 538, 12, 8014, 304, 689, 456, 603], "temperature": 0.4, "avg_logprob": -0.49541242499100535, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.00010177798685617745}, {"id": 646, "seek": 288236, "start": 2885.8, "end": 2889.76, "text": " be a few memorized examples and then things that aren't.", "tokens": [312, 257, 1326, 46677, 5110, 293, 550, 721, 300, 3212, 380, 13], "temperature": 0.4, "avg_logprob": -0.49541242499100535, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.00010177798685617745}, {"id": 647, "seek": 288236, "start": 2889.76, "end": 2890.96, "text": " That's a simple thing you can do to check.", "tokens": [663, 311, 257, 2199, 551, 291, 393, 360, 281, 1520, 13], "temperature": 0.4, "avg_logprob": -0.49541242499100535, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.00010177798685617745}, {"id": 648, "seek": 288236, "start": 2890.96, "end": 2893.1200000000003, "text": " You can also, of course, do de-de-de-de-de-de-de-de-de-plocation.", "tokens": [509, 393, 611, 11, 295, 1164, 11, 360, 368, 12, 1479, 12, 1479, 12, 1479, 12, 1479, 12, 1479, 12, 1479, 12, 1479, 12, 1479, 12, 564, 27943, 13], "temperature": 0.4, "avg_logprob": -0.49541242499100535, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.00010177798685617745}, {"id": 649, "seek": 288236, "start": 2893.1200000000003, "end": 2898.1200000000003, "text": " I don't remember off the top of my head what de-de-de-de-de-de-plocation has done here, though.", "tokens": [286, 500, 380, 1604, 766, 264, 1192, 295, 452, 1378, 437, 368, 12, 1479, 12, 1479, 12, 1479, 12, 1479, 12, 1479, 12, 564, 27943, 575, 1096, 510, 11, 1673, 13], "temperature": 0.4, "avg_logprob": -0.49541242499100535, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.00010177798685617745}, {"id": 650, "seek": 288236, "start": 2898.1200000000003, "end": 2904.2400000000002, "text": " On the Downscape task section, if I want to say about how scale loss is, you can look", "tokens": [1282, 264, 9506, 82, 4747, 5633, 3541, 11, 498, 286, 528, 281, 584, 466, 577, 4373, 4470, 307, 11, 291, 393, 574], "temperature": 0.4, "avg_logprob": -0.49541242499100535, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.00010177798685617745}, {"id": 651, "seek": 288236, "start": 2904.2400000000002, "end": 2909.6800000000003, "text": " at the transferable objects and adversarial objects.", "tokens": [412, 264, 5003, 712, 6565, 293, 17641, 44745, 6565, 13], "temperature": 0.4, "avg_logprob": -0.49541242499100535, "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.00010177798685617745}, {"id": 652, "seek": 290968, "start": 2909.68, "end": 2915.56, "text": " I don't think I have anything particularly clear to say about that.", "tokens": [286, 500, 380, 519, 286, 362, 1340, 4098, 1850, 281, 584, 466, 300, 13], "temperature": 0.0, "avg_logprob": -0.21019965527104398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.5059827748918906e-05}, {"id": 653, "seek": 290968, "start": 2915.56, "end": 2921.6, "text": " I mean, these evils, I think, are not adversarial in the sense that they're just few shot evaluations", "tokens": [286, 914, 11, 613, 1073, 4174, 11, 286, 519, 11, 366, 406, 17641, 44745, 294, 264, 2020, 300, 436, 434, 445, 1326, 3347, 43085], "temperature": 0.0, "avg_logprob": -0.21019965527104398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.5059827748918906e-05}, {"id": 654, "seek": 290968, "start": 2921.6, "end": 2924.72, "text": " with some fixed data set.", "tokens": [365, 512, 6806, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.21019965527104398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.5059827748918906e-05}, {"id": 655, "seek": 290968, "start": 2924.72, "end": 2931.44, "text": " There are a large number of different kinds of adversarial data sets out there for reasoning,", "tokens": [821, 366, 257, 2416, 1230, 295, 819, 3685, 295, 17641, 44745, 1412, 6352, 484, 456, 337, 21577, 11], "temperature": 0.0, "avg_logprob": -0.21019965527104398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.5059827748918906e-05}, {"id": 656, "seek": 290968, "start": 2931.44, "end": 2934.3599999999997, "text": " for common sense knowledge, for truthfulness.", "tokens": [337, 2689, 2020, 3601, 11, 337, 3494, 26872, 13], "temperature": 0.0, "avg_logprob": -0.21019965527104398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.5059827748918906e-05}, {"id": 657, "seek": 290968, "start": 2934.3599999999997, "end": 2937.2, "text": " So, I mean, there's, like, for example, truthful QA.", "tokens": [407, 11, 286, 914, 11, 456, 311, 11, 411, 11, 337, 1365, 11, 44669, 1249, 32, 13], "temperature": 0.0, "avg_logprob": -0.21019965527104398, "compression_ratio": 1.6302521008403361, "no_speech_prob": 2.5059827748918906e-05}, {"id": 658, "seek": 293720, "start": 2937.2, "end": 2941.3599999999997, "text": " This is an example where there aren't any trends like this and arguably the trends go", "tokens": [639, 307, 364, 1365, 689, 456, 3212, 380, 604, 13892, 411, 341, 293, 26771, 264, 13892, 352], "temperature": 0.0, "avg_logprob": -0.29806390699449475, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.00035615399247035384}, {"id": 659, "seek": 293720, "start": 2941.3599999999997, "end": 2946.12, "text": " downward, though it depends on your training distribution and some models actually do improve.", "tokens": [24805, 11, 1673, 309, 5946, 322, 428, 3097, 7316, 293, 512, 5245, 767, 360, 3470, 13], "temperature": 0.0, "avg_logprob": -0.29806390699449475, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.00035615399247035384}, {"id": 660, "seek": 293720, "start": 2946.12, "end": 2948.7999999999997, "text": " So I think that's a complicated question.", "tokens": [407, 286, 519, 300, 311, 257, 6179, 1168, 13], "temperature": 0.0, "avg_logprob": -0.29806390699449475, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.00035615399247035384}, {"id": 661, "seek": 293720, "start": 2948.7999999999997, "end": 2951.9199999999996, "text": " I think it's hard to find examples where the trends go down.", "tokens": [286, 519, 309, 311, 1152, 281, 915, 5110, 689, 264, 13892, 352, 760, 13], "temperature": 0.0, "avg_logprob": -0.29806390699449475, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.00035615399247035384}, {"id": 662, "seek": 293720, "start": 2951.9199999999996, "end": 2956.96, "text": " I don't think it's easy, but these do exist.", "tokens": [286, 500, 380, 519, 309, 311, 1858, 11, 457, 613, 360, 2514, 13], "temperature": 0.0, "avg_logprob": -0.29806390699449475, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.00035615399247035384}, {"id": 663, "seek": 293720, "start": 2956.96, "end": 2961.0, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.29806390699449475, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.00035615399247035384}, {"id": 664, "seek": 293720, "start": 2961.0, "end": 2965.0, "text": " Great.", "tokens": [3769, 13], "temperature": 0.0, "avg_logprob": -0.29806390699449475, "compression_ratio": 1.640552995391705, "no_speech_prob": 0.00035615399247035384}, {"id": 665, "seek": 296500, "start": 2965.0, "end": 2975.68, "text": " So, I guess I'll sort of end by summarizing some lessons that you might draw pretty practically", "tokens": [407, 11, 286, 2041, 286, 603, 1333, 295, 917, 538, 14611, 3319, 512, 8820, 300, 291, 1062, 2642, 1238, 15667], "temperature": 0.0, "avg_logprob": -0.1918153154089096, "compression_ratio": 1.5644444444444445, "no_speech_prob": 8.605463517596945e-05}, {"id": 666, "seek": 296500, "start": 2975.68, "end": 2978.2, "text": " for research from this.", "tokens": [337, 2132, 490, 341, 13], "temperature": 0.0, "avg_logprob": -0.1918153154089096, "compression_ratio": 1.5644444444444445, "no_speech_prob": 8.605463517596945e-05}, {"id": 667, "seek": 296500, "start": 2978.2, "end": 2983.4, "text": " And then I can either open it up for questions, or I can also, I can always talk infinitely", "tokens": [400, 550, 286, 393, 2139, 1269, 309, 493, 337, 1651, 11, 420, 286, 393, 611, 11, 286, 393, 1009, 751, 36227], "temperature": 0.0, "avg_logprob": -0.1918153154089096, "compression_ratio": 1.5644444444444445, "no_speech_prob": 8.605463517596945e-05}, {"id": 668, "seek": 296500, "start": 2983.4, "end": 2984.4, "text": " long.", "tokens": [938, 13], "temperature": 0.0, "avg_logprob": -0.1918153154089096, "compression_ratio": 1.5644444444444445, "no_speech_prob": 8.605463517596945e-05}, {"id": 669, "seek": 296500, "start": 2984.4, "end": 2988.32, "text": " I've been a professor for like 10 years of my life, so I can just talk forever.", "tokens": [286, 600, 668, 257, 8304, 337, 411, 1266, 924, 295, 452, 993, 11, 370, 286, 393, 445, 751, 5680, 13], "temperature": 0.0, "avg_logprob": -0.1918153154089096, "compression_ratio": 1.5644444444444445, "no_speech_prob": 8.605463517596945e-05}, {"id": 670, "seek": 296500, "start": 2988.32, "end": 2992.48, "text": " But I'll sort of end after talking about some lessons.", "tokens": [583, 286, 603, 1333, 295, 917, 934, 1417, 466, 512, 8820, 13], "temperature": 0.0, "avg_logprob": -0.1918153154089096, "compression_ratio": 1.5644444444444445, "no_speech_prob": 8.605463517596945e-05}, {"id": 671, "seek": 299248, "start": 2992.48, "end": 2998.28, "text": " So I think one lesson that kind of I draw from this is that kind of scanning over some", "tokens": [407, 286, 519, 472, 6898, 300, 733, 295, 286, 2642, 490, 341, 307, 300, 733, 295, 27019, 670, 512], "temperature": 0.0, "avg_logprob": -0.1380355097666508, "compression_ratio": 1.6989247311827957, "no_speech_prob": 6.500440213130787e-05}, {"id": 672, "seek": 299248, "start": 2998.28, "end": 3003.2400000000002, "text": " of the important inputs to your training process is just a pretty useful thing to do when", "tokens": [295, 264, 1021, 15743, 281, 428, 3097, 1399, 307, 445, 257, 1238, 4420, 551, 281, 360, 562], "temperature": 0.0, "avg_logprob": -0.1380355097666508, "compression_ratio": 1.6989247311827957, "no_speech_prob": 6.500440213130787e-05}, {"id": 673, "seek": 299248, "start": 3003.2400000000002, "end": 3005.2, "text": " you're doing ML research.", "tokens": [291, 434, 884, 21601, 2132, 13], "temperature": 0.0, "avg_logprob": -0.1380355097666508, "compression_ratio": 1.6989247311827957, "no_speech_prob": 6.500440213130787e-05}, {"id": 674, "seek": 299248, "start": 3005.2, "end": 3008.52, "text": " And it's sort of typically very cheap.", "tokens": [400, 309, 311, 1333, 295, 5850, 588, 7084, 13], "temperature": 0.0, "avg_logprob": -0.1380355097666508, "compression_ratio": 1.6989247311827957, "no_speech_prob": 6.500440213130787e-05}, {"id": 675, "seek": 299248, "start": 3008.52, "end": 3013.6, "text": " It's cheap because generally most things vary in an important way on a log scale, or", "tokens": [467, 311, 7084, 570, 5101, 881, 721, 10559, 294, 364, 1021, 636, 322, 257, 3565, 4373, 11, 420], "temperature": 0.0, "avg_logprob": -0.1380355097666508, "compression_ratio": 1.6989247311827957, "no_speech_prob": 6.500440213130787e-05}, {"id": 676, "seek": 299248, "start": 3013.6, "end": 3017.44, "text": " sort of on a geometric scale, however you want to say it.", "tokens": [1333, 295, 322, 257, 33246, 4373, 11, 4461, 291, 528, 281, 584, 309, 13], "temperature": 0.0, "avg_logprob": -0.1380355097666508, "compression_ratio": 1.6989247311827957, "no_speech_prob": 6.500440213130787e-05}, {"id": 677, "seek": 299248, "start": 3017.44, "end": 3021.6, "text": " And that means that like if you're training with the data set of size D, maybe you should", "tokens": [400, 300, 1355, 300, 411, 498, 291, 434, 3097, 365, 264, 1412, 992, 295, 2744, 413, 11, 1310, 291, 820], "temperature": 0.0, "avg_logprob": -0.1380355097666508, "compression_ratio": 1.6989247311827957, "no_speech_prob": 6.500440213130787e-05}, {"id": 678, "seek": 302160, "start": 3021.6, "end": 3025.72, "text": " also train with D over 2 and D over 4 and D over 8 or something like that.", "tokens": [611, 3847, 365, 413, 670, 568, 293, 413, 670, 1017, 293, 413, 670, 1649, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.173748779296875, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.7562058171024546e-05}, {"id": 679, "seek": 302160, "start": 3025.72, "end": 3028.52, "text": " And if you sum that geometric series, you get 2D.", "tokens": [400, 498, 291, 2408, 300, 33246, 2638, 11, 291, 483, 568, 35, 13], "temperature": 0.0, "avg_logprob": -0.173748779296875, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.7562058171024546e-05}, {"id": 680, "seek": 302160, "start": 3028.52, "end": 3034.12, "text": " So you sort of, I mean, you made your training process twice as expensive in some sense, but", "tokens": [407, 291, 1333, 295, 11, 286, 914, 11, 291, 1027, 428, 3097, 1399, 6091, 382, 5124, 294, 512, 2020, 11, 457], "temperature": 0.0, "avg_logprob": -0.173748779296875, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.7562058171024546e-05}, {"id": 681, "seek": 302160, "start": 3034.12, "end": 3037.68, "text": " it's not really a big change in what you have to do.", "tokens": [309, 311, 406, 534, 257, 955, 1319, 294, 437, 291, 362, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.173748779296875, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.7562058171024546e-05}, {"id": 682, "seek": 302160, "start": 3037.68, "end": 3042.7999999999997, "text": " But you can often learn a lot about what's going on by doing these kinds of scans.", "tokens": [583, 291, 393, 2049, 1466, 257, 688, 466, 437, 311, 516, 322, 538, 884, 613, 3685, 295, 35116, 13], "temperature": 0.0, "avg_logprob": -0.173748779296875, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.7562058171024546e-05}, {"id": 683, "seek": 302160, "start": 3042.7999999999997, "end": 3047.16, "text": " And so, I mean, this is an example of some data that I didn't show earlier.", "tokens": [400, 370, 11, 286, 914, 11, 341, 307, 364, 1365, 295, 512, 1412, 300, 286, 994, 380, 855, 3071, 13], "temperature": 0.0, "avg_logprob": -0.173748779296875, "compression_ratio": 1.6436781609195403, "no_speech_prob": 4.7562058171024546e-05}, {"id": 684, "seek": 304716, "start": 3047.16, "end": 3051.3999999999996, "text": " So, I think you might wonder about is what happens if you scan over data set size and model", "tokens": [407, 11, 286, 519, 291, 1062, 2441, 466, 307, 437, 2314, 498, 291, 11049, 670, 1412, 992, 2744, 293, 2316], "temperature": 0.0, "avg_logprob": -0.17077557373046875, "compression_ratio": 1.8550185873605949, "no_speech_prob": 2.2822865503258072e-05}, {"id": 685, "seek": 304716, "start": 3051.3999999999996, "end": 3053.52, "text": " size at the same time.", "tokens": [2744, 412, 264, 912, 565, 13], "temperature": 0.0, "avg_logprob": -0.17077557373046875, "compression_ratio": 1.8550185873605949, "no_speech_prob": 2.2822865503258072e-05}, {"id": 686, "seek": 304716, "start": 3053.52, "end": 3057.48, "text": " And it turns out there's some very simple trends that you can model in that case too that", "tokens": [400, 309, 4523, 484, 456, 311, 512, 588, 2199, 13892, 300, 291, 393, 2316, 294, 300, 1389, 886, 300], "temperature": 0.0, "avg_logprob": -0.17077557373046875, "compression_ratio": 1.8550185873605949, "no_speech_prob": 2.2822865503258072e-05}, {"id": 687, "seek": 304716, "start": 3057.48, "end": 3060.48, "text": " tell you about things like overfitting.", "tokens": [980, 291, 466, 721, 411, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.17077557373046875, "compression_ratio": 1.8550185873605949, "no_speech_prob": 2.2822865503258072e-05}, {"id": 688, "seek": 304716, "start": 3060.48, "end": 3063.0, "text": " And I mean, if you care about overfitting, then this tells you about something like how", "tokens": [400, 286, 914, 11, 498, 291, 1127, 466, 670, 69, 2414, 11, 550, 341, 5112, 291, 466, 746, 411, 577], "temperature": 0.0, "avg_logprob": -0.17077557373046875, "compression_ratio": 1.8550185873605949, "no_speech_prob": 2.2822865503258072e-05}, {"id": 689, "seek": 304716, "start": 3063.0, "end": 3066.96, "text": " big do you have to make your data set for a given model size to avoid overfitting being", "tokens": [955, 360, 291, 362, 281, 652, 428, 1412, 992, 337, 257, 2212, 2316, 2744, 281, 5042, 670, 69, 2414, 885], "temperature": 0.0, "avg_logprob": -0.17077557373046875, "compression_ratio": 1.8550185873605949, "no_speech_prob": 2.2822865503258072e-05}, {"id": 690, "seek": 304716, "start": 3066.96, "end": 3071.2, "text": " a significant problem so that you can answer all kinds of questions like that.", "tokens": [257, 4776, 1154, 370, 300, 291, 393, 1867, 439, 3685, 295, 1651, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.17077557373046875, "compression_ratio": 1.8550185873605949, "no_speech_prob": 2.2822865503258072e-05}, {"id": 691, "seek": 307120, "start": 3071.2, "end": 3078.24, "text": " And I at least find that this is kind of useful and it's nice for learning things about", "tokens": [400, 286, 412, 1935, 915, 300, 341, 307, 733, 295, 4420, 293, 309, 311, 1481, 337, 2539, 721, 466], "temperature": 0.0, "avg_logprob": -0.15447020741690576, "compression_ratio": 1.9272727272727272, "no_speech_prob": 4.9840964493341744e-05}, {"id": 692, "seek": 307120, "start": 3078.24, "end": 3079.24, "text": " behavior.", "tokens": [5223, 13], "temperature": 0.0, "avg_logprob": -0.15447020741690576, "compression_ratio": 1.9272727272727272, "no_speech_prob": 4.9840964493341744e-05}, {"id": 693, "seek": 307120, "start": 3079.24, "end": 3084.3599999999997, "text": " And I think alongside that, I think like this is sort of a joke.", "tokens": [400, 286, 519, 12385, 300, 11, 286, 519, 411, 341, 307, 1333, 295, 257, 7647, 13], "temperature": 0.0, "avg_logprob": -0.15447020741690576, "compression_ratio": 1.9272727272727272, "no_speech_prob": 4.9840964493341744e-05}, {"id": 694, "seek": 307120, "start": 3084.3599999999997, "end": 3085.3599999999997, "text": " This isn't real.", "tokens": [639, 1943, 380, 957, 13], "temperature": 0.0, "avg_logprob": -0.15447020741690576, "compression_ratio": 1.9272727272727272, "no_speech_prob": 4.9840964493341744e-05}, {"id": 695, "seek": 307120, "start": 3085.3599999999997, "end": 3089.4399999999996, "text": " This is sort of making fun of a large number of machine learning papers that you might", "tokens": [639, 307, 1333, 295, 1455, 1019, 295, 257, 2416, 1230, 295, 3479, 2539, 10577, 300, 291, 1062], "temperature": 0.0, "avg_logprob": -0.15447020741690576, "compression_ratio": 1.9272727272727272, "no_speech_prob": 4.9840964493341744e-05}, {"id": 696, "seek": 307120, "start": 3089.4399999999996, "end": 3090.4399999999996, "text": " see.", "tokens": [536, 13], "temperature": 0.0, "avg_logprob": -0.15447020741690576, "compression_ratio": 1.9272727272727272, "no_speech_prob": 4.9840964493341744e-05}, {"id": 697, "seek": 307120, "start": 3090.4399999999996, "end": 3094.08, "text": " I think a lot of machine learning papers have tables like this.", "tokens": [286, 519, 257, 688, 295, 3479, 2539, 10577, 362, 8020, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.15447020741690576, "compression_ratio": 1.9272727272727272, "no_speech_prob": 4.9840964493341744e-05}, {"id": 698, "seek": 307120, "start": 3094.08, "end": 3097.6, "text": " And it's sort of hard to tell from like this kind of table obviously I'm making fun, but", "tokens": [400, 309, 311, 1333, 295, 1152, 281, 980, 490, 411, 341, 733, 295, 3199, 2745, 286, 478, 1455, 1019, 11, 457], "temperature": 0.0, "avg_logprob": -0.15447020741690576, "compression_ratio": 1.9272727272727272, "no_speech_prob": 4.9840964493341744e-05}, {"id": 699, "seek": 309760, "start": 3097.6, "end": 3102.12, "text": " I think it's not so unrealistic like did the technique that went into our model really", "tokens": [286, 519, 309, 311, 406, 370, 42867, 411, 630, 264, 6532, 300, 1437, 666, 527, 2316, 534], "temperature": 0.0, "avg_logprob": -0.15580292337948515, "compression_ratio": 1.6583333333333334, "no_speech_prob": 5.472226257552393e-05}, {"id": 700, "seek": 309760, "start": 3102.12, "end": 3105.12, "text": " improve on other things that happened.", "tokens": [3470, 322, 661, 721, 300, 2011, 13], "temperature": 0.0, "avg_logprob": -0.15580292337948515, "compression_ratio": 1.6583333333333334, "no_speech_prob": 5.472226257552393e-05}, {"id": 701, "seek": 309760, "start": 3105.12, "end": 3109.16, "text": " And I think that this kind of plot at least for me is a much more convincing statement", "tokens": [400, 286, 519, 300, 341, 733, 295, 7542, 412, 1935, 337, 385, 307, 257, 709, 544, 24823, 5629], "temperature": 0.0, "avg_logprob": -0.15580292337948515, "compression_ratio": 1.6583333333333334, "no_speech_prob": 5.472226257552393e-05}, {"id": 702, "seek": 309760, "start": 3109.16, "end": 3113.4, "text": " that like will clearly transformers are just better than LSTMs.", "tokens": [300, 411, 486, 4448, 4088, 433, 366, 445, 1101, 813, 441, 6840, 26386, 13], "temperature": 0.0, "avg_logprob": -0.15580292337948515, "compression_ratio": 1.6583333333333334, "no_speech_prob": 5.472226257552393e-05}, {"id": 703, "seek": 309760, "start": 3113.4, "end": 3119.8399999999997, "text": " So the slogan here is sort of success for new techniques if your goal is to sort of", "tokens": [407, 264, 33052, 510, 307, 1333, 295, 2245, 337, 777, 7512, 498, 428, 3387, 307, 281, 1333, 295], "temperature": 0.0, "avg_logprob": -0.15580292337948515, "compression_ratio": 1.6583333333333334, "no_speech_prob": 5.472226257552393e-05}, {"id": 704, "seek": 309760, "start": 3119.8399999999997, "end": 3124.36, "text": " improve a model if that is your goal.", "tokens": [3470, 257, 2316, 498, 300, 307, 428, 3387, 13], "temperature": 0.0, "avg_logprob": -0.15580292337948515, "compression_ratio": 1.6583333333333334, "no_speech_prob": 5.472226257552393e-05}, {"id": 705, "seek": 312436, "start": 3124.36, "end": 3128.6800000000003, "text": " And I think it's at least to me much more convincing and kind of clear what's going on", "tokens": [400, 286, 519, 309, 311, 412, 1935, 281, 385, 709, 544, 24823, 293, 733, 295, 1850, 437, 311, 516, 322], "temperature": 0.0, "avg_logprob": -0.21776348969032025, "compression_ratio": 1.6386861313868613, "no_speech_prob": 9.010236681206152e-05}, {"id": 706, "seek": 312436, "start": 3128.6800000000003, "end": 3130.88, "text": " if you see these trends.", "tokens": [498, 291, 536, 613, 13892, 13], "temperature": 0.0, "avg_logprob": -0.21776348969032025, "compression_ratio": 1.6386861313868613, "no_speech_prob": 9.010236681206152e-05}, {"id": 707, "seek": 312436, "start": 3130.88, "end": 3133.1600000000003, "text": " Maybe I have another slide making fun of the CS.", "tokens": [2704, 286, 362, 1071, 4137, 1455, 1019, 295, 264, 9460, 13], "temperature": 0.0, "avg_logprob": -0.21776348969032025, "compression_ratio": 1.6386861313868613, "no_speech_prob": 9.010236681206152e-05}, {"id": 708, "seek": 312436, "start": 3133.1600000000003, "end": 3138.76, "text": " So I mean, I think this is a thing that I actually see very often in research is that", "tokens": [407, 286, 914, 11, 286, 519, 341, 307, 257, 551, 300, 286, 767, 536, 588, 2049, 294, 2132, 307, 300], "temperature": 0.0, "avg_logprob": -0.21776348969032025, "compression_ratio": 1.6386861313868613, "no_speech_prob": 9.010236681206152e-05}, {"id": 709, "seek": 312436, "start": 3138.76, "end": 3146.8, "text": " you come up with some new idea and you see like you first do the cheapest easiest experiment", "tokens": [291, 808, 493, 365, 512, 777, 1558, 293, 291, 536, 411, 291, 700, 360, 264, 29167, 12889, 5120], "temperature": 0.0, "avg_logprob": -0.21776348969032025, "compression_ratio": 1.6386861313868613, "no_speech_prob": 9.010236681206152e-05}, {"id": 710, "seek": 312436, "start": 3146.8, "end": 3149.32, "text": " and you see, well my new idea improved performance.", "tokens": [293, 291, 536, 11, 731, 452, 777, 1558, 9689, 3389, 13], "temperature": 0.0, "avg_logprob": -0.21776348969032025, "compression_ratio": 1.6386861313868613, "no_speech_prob": 9.010236681206152e-05}, {"id": 711, "seek": 312436, "start": 3149.32, "end": 3151.6400000000003, "text": " I'm really excited.", "tokens": [286, 478, 534, 2919, 13], "temperature": 0.0, "avg_logprob": -0.21776348969032025, "compression_ratio": 1.6386861313868613, "no_speech_prob": 9.010236681206152e-05}, {"id": 712, "seek": 312436, "start": 3151.6400000000003, "end": 3153.76, "text": " Everyone should it should adopt this.", "tokens": [5198, 820, 309, 820, 6878, 341, 13], "temperature": 0.0, "avg_logprob": -0.21776348969032025, "compression_ratio": 1.6386861313868613, "no_speech_prob": 9.010236681206152e-05}, {"id": 713, "seek": 315376, "start": 3153.76, "end": 3158.36, "text": " But then you make some plot like this and you sort of say, oh, okay, I guess it doesn't", "tokens": [583, 550, 291, 652, 512, 7542, 411, 341, 293, 291, 1333, 295, 584, 11, 1954, 11, 1392, 11, 286, 2041, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.1816905105815214, "compression_ratio": 1.8195488721804511, "no_speech_prob": 9.602353384252638e-05}, {"id": 714, "seek": 315376, "start": 3158.36, "end": 3160.6800000000003, "text": " really matter that much at all.", "tokens": [534, 1871, 300, 709, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.1816905105815214, "compression_ratio": 1.8195488721804511, "no_speech_prob": 9.602353384252638e-05}, {"id": 715, "seek": 315376, "start": 3160.6800000000003, "end": 3162.0, "text": " And I think this is actually a comment.", "tokens": [400, 286, 519, 341, 307, 767, 257, 2871, 13], "temperature": 0.0, "avg_logprob": -0.1816905105815214, "compression_ratio": 1.8195488721804511, "no_speech_prob": 9.602353384252638e-05}, {"id": 716, "seek": 315376, "start": 3162.0, "end": 3164.8, "text": " I mean, I think we all have all sorts of ideas.", "tokens": [286, 914, 11, 286, 519, 321, 439, 362, 439, 7527, 295, 3487, 13], "temperature": 0.0, "avg_logprob": -0.1816905105815214, "compression_ratio": 1.8195488721804511, "no_speech_prob": 9.602353384252638e-05}, {"id": 717, "seek": 315376, "start": 3164.8, "end": 3170.2000000000003, "text": " I mean, people fall asleep at night and they can't sleep and then they wake up and they", "tokens": [286, 914, 11, 561, 2100, 11039, 412, 1818, 293, 436, 393, 380, 2817, 293, 550, 436, 6634, 493, 293, 436], "temperature": 0.0, "avg_logprob": -0.1816905105815214, "compression_ratio": 1.8195488721804511, "no_speech_prob": 9.602353384252638e-05}, {"id": 718, "seek": 315376, "start": 3170.2000000000003, "end": 3172.4, "text": " have ideas and like, oh, I'm going to go try this.", "tokens": [362, 3487, 293, 411, 11, 1954, 11, 286, 478, 516, 281, 352, 853, 341, 13], "temperature": 0.0, "avg_logprob": -0.1816905105815214, "compression_ratio": 1.8195488721804511, "no_speech_prob": 9.602353384252638e-05}, {"id": 719, "seek": 315376, "start": 3172.4, "end": 3173.8, "text": " We all do it.", "tokens": [492, 439, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.1816905105815214, "compression_ratio": 1.8195488721804511, "no_speech_prob": 9.602353384252638e-05}, {"id": 720, "seek": 315376, "start": 3173.8, "end": 3177.84, "text": " But oftentimes they don't work and I think this is sort of useful for understanding whether", "tokens": [583, 18349, 436, 500, 380, 589, 293, 286, 519, 341, 307, 1333, 295, 4420, 337, 3701, 1968], "temperature": 0.0, "avg_logprob": -0.1816905105815214, "compression_ratio": 1.8195488721804511, "no_speech_prob": 9.602353384252638e-05}, {"id": 721, "seek": 315376, "start": 3177.84, "end": 3179.76, "text": " your idea really, really works.", "tokens": [428, 1558, 534, 11, 534, 1985, 13], "temperature": 0.0, "avg_logprob": -0.1816905105815214, "compression_ratio": 1.8195488721804511, "no_speech_prob": 9.602353384252638e-05}, {"id": 722, "seek": 317976, "start": 3179.76, "end": 3185.5600000000004, "text": " And I mean, if all you're ever going to do is train this model, then your idea did work.", "tokens": [400, 286, 914, 11, 498, 439, 291, 434, 1562, 516, 281, 360, 307, 3847, 341, 2316, 11, 550, 428, 1558, 630, 589, 13], "temperature": 0.0, "avg_logprob": -0.18859065206427322, "compression_ratio": 1.6937269372693726, "no_speech_prob": 6.0106005548732355e-05}, {"id": 723, "seek": 317976, "start": 3185.5600000000004, "end": 3193.2400000000002, "text": " But I think that like there's sort of an expectation that probably people will be using bigger", "tokens": [583, 286, 519, 300, 411, 456, 311, 1333, 295, 364, 14334, 300, 1391, 561, 486, 312, 1228, 3801], "temperature": 0.0, "avg_logprob": -0.18859065206427322, "compression_ratio": 1.6937269372693726, "no_speech_prob": 6.0106005548732355e-05}, {"id": 724, "seek": 317976, "start": 3193.2400000000002, "end": 3195.48, "text": " computers to train larger models in the future.", "tokens": [10807, 281, 3847, 4833, 5245, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.18859065206427322, "compression_ratio": 1.6937269372693726, "no_speech_prob": 6.0106005548732355e-05}, {"id": 725, "seek": 317976, "start": 3195.48, "end": 3198.76, "text": " And so the ideas that are really going to have a huge impact are ones that sort of point", "tokens": [400, 370, 264, 3487, 300, 366, 534, 516, 281, 362, 257, 2603, 2712, 366, 2306, 300, 1333, 295, 935], "temperature": 0.0, "avg_logprob": -0.18859065206427322, "compression_ratio": 1.6937269372693726, "no_speech_prob": 6.0106005548732355e-05}, {"id": 726, "seek": 317976, "start": 3198.76, "end": 3200.5600000000004, "text": " in the opposite direction.", "tokens": [294, 264, 6182, 3513, 13], "temperature": 0.0, "avg_logprob": -0.18859065206427322, "compression_ratio": 1.6937269372693726, "no_speech_prob": 6.0106005548732355e-05}, {"id": 727, "seek": 317976, "start": 3200.5600000000004, "end": 3204.1600000000003, "text": " I've even seen ideas where on small models they make no difference at all, but on larger", "tokens": [286, 600, 754, 1612, 3487, 689, 322, 1359, 5245, 436, 652, 572, 2649, 412, 439, 11, 457, 322, 4833], "temperature": 0.0, "avg_logprob": -0.18859065206427322, "compression_ratio": 1.6937269372693726, "no_speech_prob": 6.0106005548732355e-05}, {"id": 728, "seek": 317976, "start": 3204.1600000000003, "end": 3208.6400000000003, "text": " models they do better.", "tokens": [5245, 436, 360, 1101, 13], "temperature": 0.0, "avg_logprob": -0.18859065206427322, "compression_ratio": 1.6937269372693726, "no_speech_prob": 6.0106005548732355e-05}, {"id": 729, "seek": 320864, "start": 3208.64, "end": 3211.6, "text": " And so these kinds of trends I think are useful.", "tokens": [400, 370, 613, 3685, 295, 13892, 286, 519, 366, 4420, 13], "temperature": 0.0, "avg_logprob": -0.18916371663411458, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.009123934636591e-05}, {"id": 730, "seek": 320864, "start": 3211.6, "end": 3214.2, "text": " And they're certainly useful to think about.", "tokens": [400, 436, 434, 3297, 4420, 281, 519, 466, 13], "temperature": 0.0, "avg_logprob": -0.18916371663411458, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.009123934636591e-05}, {"id": 731, "seek": 320864, "start": 3214.2, "end": 3222.4, "text": " Another point that I find useful, I think it's not sort of obvious and maybe you shouldn't", "tokens": [3996, 935, 300, 286, 915, 4420, 11, 286, 519, 309, 311, 406, 1333, 295, 6322, 293, 1310, 291, 4659, 380], "temperature": 0.0, "avg_logprob": -0.18916371663411458, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.009123934636591e-05}, {"id": 732, "seek": 320864, "start": 3222.4, "end": 3226.7599999999998, "text": " trust it completely, is that I tend to think, I mean, because I've sort of swallowed", "tokens": [3361, 309, 2584, 11, 307, 300, 286, 3928, 281, 519, 11, 286, 914, 11, 570, 286, 600, 1333, 295, 41769], "temperature": 0.0, "avg_logprob": -0.18916371663411458, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.009123934636591e-05}, {"id": 733, "seek": 320864, "start": 3226.7599999999998, "end": 3233.7999999999997, "text": " my own coolade, that if something works, then it should scale fairly predictably.", "tokens": [452, 1065, 1627, 762, 11, 300, 498, 746, 1985, 11, 550, 309, 820, 4373, 6457, 6069, 1188, 13], "temperature": 0.0, "avg_logprob": -0.18916371663411458, "compression_ratio": 1.647887323943662, "no_speech_prob": 1.009123934636591e-05}, {"id": 734, "seek": 323380, "start": 3233.8, "end": 3238.7200000000003, "text": " It's not always true, but for things that you can measure that are very close to your", "tokens": [467, 311, 406, 1009, 2074, 11, 457, 337, 721, 300, 291, 393, 3481, 300, 366, 588, 1998, 281, 428], "temperature": 0.0, "avg_logprob": -0.15839849699527844, "compression_ratio": 1.7007874015748032, "no_speech_prob": 4.126427666051313e-05}, {"id": 735, "seek": 323380, "start": 3238.7200000000003, "end": 3241.2000000000003, "text": " optimization target.", "tokens": [19618, 3779, 13], "temperature": 0.0, "avg_logprob": -0.15839849699527844, "compression_ratio": 1.7007874015748032, "no_speech_prob": 4.126427666051313e-05}, {"id": 736, "seek": 323380, "start": 3241.2000000000003, "end": 3248.28, "text": " If sort of your training process, your hyper parameters, etc., are all kind of set up well,", "tokens": [759, 1333, 295, 428, 3097, 1399, 11, 428, 9848, 9834, 11, 5183, 7933, 366, 439, 733, 295, 992, 493, 731, 11], "temperature": 0.0, "avg_logprob": -0.15839849699527844, "compression_ratio": 1.7007874015748032, "no_speech_prob": 4.126427666051313e-05}, {"id": 737, "seek": 323380, "start": 3248.28, "end": 3252.1200000000003, "text": " then I tend to think that you should see some kind of predictable trend.", "tokens": [550, 286, 3928, 281, 519, 300, 291, 820, 536, 512, 733, 295, 27737, 6028, 13], "temperature": 0.0, "avg_logprob": -0.15839849699527844, "compression_ratio": 1.7007874015748032, "no_speech_prob": 4.126427666051313e-05}, {"id": 738, "seek": 323380, "start": 3252.1200000000003, "end": 3258.7200000000003, "text": " And if that trend goes away, then I mean, maybe that's just exactly what's true.", "tokens": [400, 498, 300, 6028, 1709, 1314, 11, 550, 286, 914, 11, 1310, 300, 311, 445, 2293, 437, 311, 2074, 13], "temperature": 0.0, "avg_logprob": -0.15839849699527844, "compression_ratio": 1.7007874015748032, "no_speech_prob": 4.126427666051313e-05}, {"id": 739, "seek": 323380, "start": 3258.7200000000003, "end": 3261.96, "text": " But I think often it means that there's something broken about what's going on.", "tokens": [583, 286, 519, 2049, 309, 1355, 300, 456, 311, 746, 5463, 466, 437, 311, 516, 322, 13], "temperature": 0.0, "avg_logprob": -0.15839849699527844, "compression_ratio": 1.7007874015748032, "no_speech_prob": 4.126427666051313e-05}, {"id": 740, "seek": 326196, "start": 3261.96, "end": 3266.0, "text": " Maybe your numerics are broken and you need higher precision in some part of your model,", "tokens": [2704, 428, 7866, 1167, 366, 5463, 293, 291, 643, 2946, 18356, 294, 512, 644, 295, 428, 2316, 11], "temperature": 0.0, "avg_logprob": -0.20777290285998629, "compression_ratio": 1.7569444444444444, "no_speech_prob": 9.909193613566458e-05}, {"id": 741, "seek": 326196, "start": 3266.0, "end": 3269.28, "text": " maybe there's some bottleneck you hadn't thought of.", "tokens": [1310, 456, 311, 512, 44641, 547, 291, 8782, 380, 1194, 295, 13], "temperature": 0.0, "avg_logprob": -0.20777290285998629, "compression_ratio": 1.7569444444444444, "no_speech_prob": 9.909193613566458e-05}, {"id": 742, "seek": 326196, "start": 3269.28, "end": 3274.2, "text": " So I mean, this is also an example that kind of scaling, predictable scaling can be found", "tokens": [407, 286, 914, 11, 341, 307, 611, 364, 1365, 300, 733, 295, 21589, 11, 27737, 21589, 393, 312, 1352], "temperature": 0.0, "avg_logprob": -0.20777290285998629, "compression_ratio": 1.7569444444444444, "no_speech_prob": 9.909193613566458e-05}, {"id": 743, "seek": 326196, "start": 3274.2, "end": 3275.4, "text": " all over the place.", "tokens": [439, 670, 264, 1081, 13], "temperature": 0.0, "avg_logprob": -0.20777290285998629, "compression_ratio": 1.7569444444444444, "no_speech_prob": 9.909193613566458e-05}, {"id": 744, "seek": 326196, "start": 3275.4, "end": 3279.2400000000002, "text": " So I just think this is sort of neat.", "tokens": [407, 286, 445, 519, 341, 307, 1333, 295, 10654, 13], "temperature": 0.0, "avg_logprob": -0.20777290285998629, "compression_ratio": 1.7569444444444444, "no_speech_prob": 9.909193613566458e-05}, {"id": 745, "seek": 326196, "start": 3279.2400000000002, "end": 3283.84, "text": " So if you just train these extremely naive, very stupid multimodal models, or you use", "tokens": [407, 498, 291, 445, 3847, 613, 4664, 29052, 11, 588, 6631, 32972, 378, 304, 5245, 11, 420, 291, 764], "temperature": 0.0, "avg_logprob": -0.20777290285998629, "compression_ratio": 1.7569444444444444, "no_speech_prob": 9.909193613566458e-05}, {"id": 746, "seek": 326196, "start": 3283.84, "end": 3289.2400000000002, "text": " a decoder-only transformer to either model the text based on the image or model the image", "tokens": [257, 979, 19866, 12, 25202, 31782, 281, 2139, 2316, 264, 2487, 2361, 322, 264, 3256, 420, 2316, 264, 3256], "temperature": 0.0, "avg_logprob": -0.20777290285998629, "compression_ratio": 1.7569444444444444, "no_speech_prob": 9.909193613566458e-05}, {"id": 747, "seek": 326196, "start": 3289.2400000000002, "end": 3291.92, "text": " based on the text, then you can do that.", "tokens": [2361, 322, 264, 2487, 11, 550, 291, 393, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.20777290285998629, "compression_ratio": 1.7569444444444444, "no_speech_prob": 9.909193613566458e-05}, {"id": 748, "seek": 329192, "start": 3291.92, "end": 3296.44, "text": " And measure a sort of empirical mutual information between the image and the text.", "tokens": [400, 3481, 257, 1333, 295, 31886, 16917, 1589, 1296, 264, 3256, 293, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.14339419542732884, "compression_ratio": 1.8467153284671534, "no_speech_prob": 3.11834410240408e-05}, {"id": 749, "seek": 329192, "start": 3296.44, "end": 3302.48, "text": " How much information did the image give you about the words in the sense of sort of Shannon", "tokens": [1012, 709, 1589, 630, 264, 3256, 976, 291, 466, 264, 2283, 294, 264, 2020, 295, 1333, 295, 28974], "temperature": 0.0, "avg_logprob": -0.14339419542732884, "compression_ratio": 1.8467153284671534, "no_speech_prob": 3.11834410240408e-05}, {"id": 750, "seek": 329192, "start": 3302.48, "end": 3304.52, "text": " information?", "tokens": [1589, 30], "temperature": 0.0, "avg_logprob": -0.14339419542732884, "compression_ratio": 1.8467153284671534, "no_speech_prob": 3.11834410240408e-05}, {"id": 751, "seek": 329192, "start": 3304.52, "end": 3308.16, "text": " And or conversely, how much information did the text give you about the image?", "tokens": [400, 420, 2615, 736, 11, 577, 709, 1589, 630, 264, 2487, 976, 291, 466, 264, 3256, 30], "temperature": 0.0, "avg_logprob": -0.14339419542732884, "compression_ratio": 1.8467153284671534, "no_speech_prob": 3.11834410240408e-05}, {"id": 752, "seek": 329192, "start": 3308.16, "end": 3312.36, "text": " And this is also a place where, I mean, this is very close to the optimization target.", "tokens": [400, 341, 307, 611, 257, 1081, 689, 11, 286, 914, 11, 341, 307, 588, 1998, 281, 264, 19618, 3779, 13], "temperature": 0.0, "avg_logprob": -0.14339419542732884, "compression_ratio": 1.8467153284671534, "no_speech_prob": 3.11834410240408e-05}, {"id": 753, "seek": 329192, "start": 3312.36, "end": 3316.0, "text": " The whole point of the multimodal is to get this information.", "tokens": [440, 1379, 935, 295, 264, 32972, 378, 304, 307, 281, 483, 341, 1589, 13], "temperature": 0.0, "avg_logprob": -0.14339419542732884, "compression_ratio": 1.8467153284671534, "no_speech_prob": 3.11834410240408e-05}, {"id": 754, "seek": 329192, "start": 3316.0, "end": 3321.28, "text": " And you see that there's some predictable scaling going on where larger models are getting", "tokens": [400, 291, 536, 300, 456, 311, 512, 27737, 21589, 516, 322, 689, 4833, 5245, 366, 1242], "temperature": 0.0, "avg_logprob": -0.14339419542732884, "compression_ratio": 1.8467153284671534, "no_speech_prob": 3.11834410240408e-05}, {"id": 755, "seek": 332128, "start": 3321.28, "end": 3328.1600000000003, "text": " more information about one data, just one part of the distribution for the other.", "tokens": [544, 1589, 466, 472, 1412, 11, 445, 472, 644, 295, 264, 7316, 337, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.13972212138928866, "compression_ratio": 1.625615763546798, "no_speech_prob": 1.5439709386555478e-05}, {"id": 756, "seek": 332128, "start": 3328.1600000000003, "end": 3336.92, "text": " But I think this is sort of a general thing that you should expect in model training.", "tokens": [583, 286, 519, 341, 307, 1333, 295, 257, 2674, 551, 300, 291, 820, 2066, 294, 2316, 3097, 13], "temperature": 0.0, "avg_logprob": -0.13972212138928866, "compression_ratio": 1.625615763546798, "no_speech_prob": 1.5439709386555478e-05}, {"id": 757, "seek": 332128, "start": 3336.92, "end": 3342.96, "text": " And so maybe to sort of summarize, maybe even bigger picture implications.", "tokens": [400, 370, 1310, 281, 1333, 295, 20858, 11, 1310, 754, 3801, 3036, 16602, 13], "temperature": 0.0, "avg_logprob": -0.13972212138928866, "compression_ratio": 1.625615763546798, "no_speech_prob": 1.5439709386555478e-05}, {"id": 758, "seek": 332128, "start": 3342.96, "end": 3349.92, "text": " I think that these kinds of results suggest that it may not be the best or the smartest", "tokens": [286, 519, 300, 613, 3685, 295, 3542, 3402, 300, 309, 815, 406, 312, 264, 1151, 420, 264, 41491], "temperature": 0.0, "avg_logprob": -0.13972212138928866, "compression_ratio": 1.625615763546798, "no_speech_prob": 1.5439709386555478e-05}, {"id": 759, "seek": 334992, "start": 3349.92, "end": 3353.0, "text": " or the most interesting way to make better ML models.", "tokens": [420, 264, 881, 1880, 636, 281, 652, 1101, 21601, 5245, 13], "temperature": 0.0, "avg_logprob": -0.21783849080403647, "compression_ratio": 1.5294117647058822, "no_speech_prob": 7.239606202347204e-05}, {"id": 760, "seek": 334992, "start": 3353.0, "end": 3356.52, "text": " Maybe it won't be the way that happens in the future.", "tokens": [2704, 309, 1582, 380, 312, 264, 636, 300, 2314, 294, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.21783849080403647, "compression_ratio": 1.5294117647058822, "no_speech_prob": 7.239606202347204e-05}, {"id": 761, "seek": 334992, "start": 3356.52, "end": 3360.84, "text": " But at least I think these results suggest that there aren't any really hard conceptual", "tokens": [583, 412, 1935, 286, 519, 613, 3542, 3402, 300, 456, 3212, 380, 604, 534, 1152, 24106], "temperature": 0.0, "avg_logprob": -0.21783849080403647, "compression_ratio": 1.5294117647058822, "no_speech_prob": 7.239606202347204e-05}, {"id": 762, "seek": 334992, "start": 3360.84, "end": 3368.56, "text": " barriers preventing people from training significantly more powerful models of all kinds, including", "tokens": [13565, 19965, 561, 490, 3097, 10591, 544, 4005, 5245, 295, 439, 3685, 11, 3009], "temperature": 0.0, "avg_logprob": -0.21783849080403647, "compression_ratio": 1.5294117647058822, "no_speech_prob": 7.239606202347204e-05}, {"id": 763, "seek": 334992, "start": 3368.56, "end": 3373.7200000000003, "text": " of course, language models in AI research.", "tokens": [295, 1164, 11, 2856, 5245, 294, 7318, 2132, 13], "temperature": 0.0, "avg_logprob": -0.21783849080403647, "compression_ratio": 1.5294117647058822, "no_speech_prob": 7.239606202347204e-05}, {"id": 764, "seek": 337372, "start": 3373.72, "end": 3380.08, "text": " I think certainly my perspective, originally as a physicist, sort of coming to machine", "tokens": [286, 519, 3297, 452, 4585, 11, 7993, 382, 257, 42466, 11, 1333, 295, 1348, 281, 3479], "temperature": 0.0, "avg_logprob": -0.16455394920261426, "compression_ratio": 1.591743119266055, "no_speech_prob": 3.700704110087827e-05}, {"id": 765, "seek": 337372, "start": 3380.08, "end": 3388.2799999999997, "text": " learning, kind of fresh new way five years ago, is that, I mean, this is sort of one set", "tokens": [2539, 11, 733, 295, 4451, 777, 636, 1732, 924, 2057, 11, 307, 300, 11, 286, 914, 11, 341, 307, 1333, 295, 472, 992], "temperature": 0.0, "avg_logprob": -0.16455394920261426, "compression_ratio": 1.591743119266055, "no_speech_prob": 3.700704110087827e-05}, {"id": 766, "seek": 337372, "start": 3388.2799999999997, "end": 3395.9599999999996, "text": " of abstractions for thinking about kind of what's going on in AI research that you, if", "tokens": [295, 12649, 626, 337, 1953, 466, 733, 295, 437, 311, 516, 322, 294, 7318, 2132, 300, 291, 11, 498], "temperature": 0.0, "avg_logprob": -0.16455394920261426, "compression_ratio": 1.591743119266055, "no_speech_prob": 3.700704110087827e-05}, {"id": 767, "seek": 337372, "start": 3395.9599999999996, "end": 3399.6, "text": " you're going to be training fairly large models and you want them to do well, that's", "tokens": [291, 434, 516, 281, 312, 3097, 6457, 2416, 5245, 293, 291, 528, 552, 281, 360, 731, 11, 300, 311], "temperature": 0.0, "avg_logprob": -0.16455394920261426, "compression_ratio": 1.591743119266055, "no_speech_prob": 3.700704110087827e-05}, {"id": 768, "seek": 339960, "start": 3399.6, "end": 3404.44, "text": " a thing that you're going to do, then you probably want your models to sort of be scaling", "tokens": [257, 551, 300, 291, 434, 516, 281, 360, 11, 550, 291, 1391, 528, 428, 5245, 281, 1333, 295, 312, 21589], "temperature": 0.0, "avg_logprob": -0.18021480726159136, "compression_ratio": 1.776, "no_speech_prob": 6.189636042108759e-05}, {"id": 769, "seek": 339960, "start": 3404.44, "end": 3406.7999999999997, "text": " well in terms of their performance.", "tokens": [731, 294, 2115, 295, 641, 3389, 13], "temperature": 0.0, "avg_logprob": -0.18021480726159136, "compression_ratio": 1.776, "no_speech_prob": 6.189636042108759e-05}, {"id": 770, "seek": 339960, "start": 3406.7999999999997, "end": 3410.56, "text": " And I think this framework of maybe there's a bottleneck, but if you remove the bottleneck,", "tokens": [400, 286, 519, 341, 8388, 295, 1310, 456, 311, 257, 44641, 547, 11, 457, 498, 291, 4159, 264, 44641, 547, 11], "temperature": 0.0, "avg_logprob": -0.18021480726159136, "compression_ratio": 1.776, "no_speech_prob": 6.189636042108759e-05}, {"id": 771, "seek": 339960, "start": 3410.56, "end": 3413.6, "text": " then you'll just continue to see further progress.", "tokens": [550, 291, 603, 445, 2354, 281, 536, 3052, 4205, 13], "temperature": 0.0, "avg_logprob": -0.18021480726159136, "compression_ratio": 1.776, "no_speech_prob": 6.189636042108759e-05}, {"id": 772, "seek": 339960, "start": 3413.6, "end": 3417.24, "text": " I found useful.", "tokens": [286, 1352, 4420, 13], "temperature": 0.0, "avg_logprob": -0.18021480726159136, "compression_ratio": 1.776, "no_speech_prob": 6.189636042108759e-05}, {"id": 773, "seek": 339960, "start": 3417.24, "end": 3422.64, "text": " I think another point that, well, maybe I'll make this point at the end.", "tokens": [286, 519, 1071, 935, 300, 11, 731, 11, 1310, 286, 603, 652, 341, 935, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.18021480726159136, "compression_ratio": 1.776, "no_speech_prob": 6.189636042108759e-05}, {"id": 774, "seek": 339960, "start": 3422.64, "end": 3426.52, "text": " Another point is that, yeah, scaling laws are just sort of all over the place and they", "tokens": [3996, 935, 307, 300, 11, 1338, 11, 21589, 6064, 366, 445, 1333, 295, 439, 670, 264, 1081, 293, 436], "temperature": 0.0, "avg_logprob": -0.18021480726159136, "compression_ratio": 1.776, "no_speech_prob": 6.189636042108759e-05}, {"id": 775, "seek": 342652, "start": 3426.52, "end": 3431.7599999999998, "text": " can help you to sort of maybe organize your research a bit.", "tokens": [393, 854, 291, 281, 1333, 295, 1310, 13859, 428, 2132, 257, 857, 13], "temperature": 0.0, "avg_logprob": -0.11884783798793577, "compression_ratio": 1.6781609195402298, "no_speech_prob": 3.02178377751261e-05}, {"id": 776, "seek": 342652, "start": 3431.7599999999998, "end": 3435.92, "text": " And then, I mean, maybe the most interesting point conceptually, though, is that it seems", "tokens": [400, 550, 11, 286, 914, 11, 1310, 264, 881, 1880, 935, 3410, 671, 11, 1673, 11, 307, 300, 309, 2544], "temperature": 0.0, "avg_logprob": -0.11884783798793577, "compression_ratio": 1.6781609195402298, "no_speech_prob": 3.02178377751261e-05}, {"id": 777, "seek": 342652, "start": 3435.92, "end": 3442.56, "text": " like, if you believe this kind of story, that it seems like many domains of ML are kind", "tokens": [411, 11, 498, 291, 1697, 341, 733, 295, 1657, 11, 300, 309, 2544, 411, 867, 25514, 295, 21601, 366, 733], "temperature": 0.0, "avg_logprob": -0.11884783798793577, "compression_ratio": 1.6781609195402298, "no_speech_prob": 3.02178377751261e-05}, {"id": 778, "seek": 342652, "start": 3442.56, "end": 3447.56, "text": " of surprisingly simple and universal, things that you might not have thought are the same", "tokens": [295, 17600, 2199, 293, 11455, 11, 721, 300, 291, 1062, 406, 362, 1194, 366, 264, 912], "temperature": 0.0, "avg_logprob": -0.11884783798793577, "compression_ratio": 1.6781609195402298, "no_speech_prob": 3.02178377751261e-05}, {"id": 779, "seek": 342652, "start": 3447.56, "end": 3451.04, "text": " or more similar than they are different.", "tokens": [420, 544, 2531, 813, 436, 366, 819, 13], "temperature": 0.0, "avg_logprob": -0.11884783798793577, "compression_ratio": 1.6781609195402298, "no_speech_prob": 3.02178377751261e-05}, {"id": 780, "seek": 342652, "start": 3451.04, "end": 3454.7599999999998, "text": " And of course, this is also a fascinating thing to try to understand.", "tokens": [400, 295, 1164, 11, 341, 307, 611, 257, 10343, 551, 281, 853, 281, 1223, 13], "temperature": 0.0, "avg_logprob": -0.11884783798793577, "compression_ratio": 1.6781609195402298, "no_speech_prob": 3.02178377751261e-05}, {"id": 781, "seek": 345476, "start": 3454.76, "end": 3461.5200000000004, "text": " So I mean, I was a theoretical physicist for most of my life, so I mostly tried to understand", "tokens": [407, 286, 914, 11, 286, 390, 257, 20864, 42466, 337, 881, 295, 452, 993, 11, 370, 286, 5240, 3031, 281, 1223], "temperature": 0.0, "avg_logprob": -0.1549986187774356, "compression_ratio": 1.6907630522088353, "no_speech_prob": 2.836851308529731e-05}, {"id": 782, "seek": 345476, "start": 3461.5200000000004, "end": 3466.1600000000003, "text": " things that seem extremely esoteric and weird and why would anyone care about them.", "tokens": [721, 300, 1643, 4664, 785, 21585, 299, 293, 3657, 293, 983, 576, 2878, 1127, 466, 552, 13], "temperature": 0.0, "avg_logprob": -0.1549986187774356, "compression_ratio": 1.6907630522088353, "no_speech_prob": 2.836851308529731e-05}, {"id": 783, "seek": 345476, "start": 3466.1600000000003, "end": 3471.2000000000003, "text": " This is a thing that I think probably, probably everyone in this room kind of cares about,", "tokens": [639, 307, 257, 551, 300, 286, 519, 1391, 11, 1391, 1518, 294, 341, 1808, 733, 295, 12310, 466, 11], "temperature": 0.0, "avg_logprob": -0.1549986187774356, "compression_ratio": 1.6907630522088353, "no_speech_prob": 2.836851308529731e-05}, {"id": 784, "seek": 345476, "start": 3471.2000000000003, "end": 3475.2000000000003, "text": " like, can AI models write, can they communicate in language?", "tokens": [411, 11, 393, 7318, 5245, 2464, 11, 393, 436, 7890, 294, 2856, 30], "temperature": 0.0, "avg_logprob": -0.1549986187774356, "compression_ratio": 1.6907630522088353, "no_speech_prob": 2.836851308529731e-05}, {"id": 785, "seek": 345476, "start": 3475.2000000000003, "end": 3480.0, "text": " And these kinds of trends are really, really nice, though the kind of trends that you might", "tokens": [400, 613, 3685, 295, 13892, 366, 534, 11, 534, 1481, 11, 1673, 264, 733, 295, 13892, 300, 291, 1062], "temperature": 0.0, "avg_logprob": -0.1549986187774356, "compression_ratio": 1.6907630522088353, "no_speech_prob": 2.836851308529731e-05}, {"id": 786, "seek": 348000, "start": 3480.0, "end": 3485.32, "text": " see in a very controlled physics experiment or something, and yet they're coming out of", "tokens": [536, 294, 257, 588, 10164, 10649, 5120, 420, 746, 11, 293, 1939, 436, 434, 1348, 484, 295], "temperature": 0.0, "avg_logprob": -0.15751158224569783, "compression_ratio": 1.7713178294573644, "no_speech_prob": 7.598862430313602e-05}, {"id": 787, "seek": 348000, "start": 3485.32, "end": 3490.0, "text": " something very, very noisy and random, like, predicting language data on the internet.", "tokens": [746, 588, 11, 588, 24518, 293, 4974, 11, 411, 11, 32884, 2856, 1412, 322, 264, 4705, 13], "temperature": 0.0, "avg_logprob": -0.15751158224569783, "compression_ratio": 1.7713178294573644, "no_speech_prob": 7.598862430313602e-05}, {"id": 788, "seek": 348000, "start": 3490.0, "end": 3496.52, "text": " So I think it's very interesting to think about, like, why are these kinds of trends true?", "tokens": [407, 286, 519, 309, 311, 588, 1880, 281, 519, 466, 11, 411, 11, 983, 366, 613, 3685, 295, 13892, 2074, 30], "temperature": 0.0, "avg_logprob": -0.15751158224569783, "compression_ratio": 1.7713178294573644, "no_speech_prob": 7.598862430313602e-05}, {"id": 789, "seek": 348000, "start": 3496.52, "end": 3501.88, "text": " What is the underlying kind of theory or science here that makes these trends true?", "tokens": [708, 307, 264, 14217, 733, 295, 5261, 420, 3497, 510, 300, 1669, 613, 13892, 2074, 30], "temperature": 0.0, "avg_logprob": -0.15751158224569783, "compression_ratio": 1.7713178294573644, "no_speech_prob": 7.598862430313602e-05}, {"id": 790, "seek": 348000, "start": 3501.88, "end": 3503.6, "text": " Can we predict it?", "tokens": [1664, 321, 6069, 309, 30], "temperature": 0.0, "avg_logprob": -0.15751158224569783, "compression_ratio": 1.7713178294573644, "no_speech_prob": 7.598862430313602e-05}, {"id": 791, "seek": 348000, "start": 3503.6, "end": 3504.92, "text": " Can we refine those predictions?", "tokens": [1664, 321, 33906, 729, 21264, 30], "temperature": 0.0, "avg_logprob": -0.15751158224569783, "compression_ratio": 1.7713178294573644, "no_speech_prob": 7.598862430313602e-05}, {"id": 792, "seek": 348000, "start": 3504.92, "end": 3508.04, "text": " Can we understand why when this doesn't, doesn't occur?", "tokens": [1664, 321, 1223, 983, 562, 341, 1177, 380, 11, 1177, 380, 5160, 30], "temperature": 0.0, "avg_logprob": -0.15751158224569783, "compression_ratio": 1.7713178294573644, "no_speech_prob": 7.598862430313602e-05}, {"id": 793, "seek": 350804, "start": 3508.04, "end": 3510.7599999999998, "text": " Another question is sort of, there are some exponents here.", "tokens": [3996, 1168, 307, 1333, 295, 11, 456, 366, 512, 12680, 791, 510, 13], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 794, "seek": 350804, "start": 3510.7599999999998, "end": 3514.2, "text": " This is a straight line, but the straight line represents a power law with a particular", "tokens": [639, 307, 257, 2997, 1622, 11, 457, 264, 2997, 1622, 8855, 257, 1347, 2101, 365, 257, 1729], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 795, "seek": 350804, "start": 3514.2, "end": 3515.2, "text": " exponent.", "tokens": [37871, 13], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 796, "seek": 350804, "start": 3515.2, "end": 3516.64, "text": " Why that exponent?", "tokens": [1545, 300, 37871, 30], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 797, "seek": 350804, "start": 3516.64, "end": 3519.72, "text": " For language, it's like 0.0 H or so.", "tokens": [1171, 2856, 11, 309, 311, 411, 1958, 13, 15, 389, 420, 370, 13], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 798, "seek": 350804, "start": 3519.72, "end": 3522.88, "text": " Why 0.08 and not 0.2 or 0.4 or 0.001?", "tokens": [1545, 1958, 13, 16133, 293, 406, 1958, 13, 17, 420, 1958, 13, 19, 420, 1958, 13, 628, 16, 30], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 799, "seek": 350804, "start": 3522.88, "end": 3525.68, "text": " I think there are all sorts of questions here.", "tokens": [286, 519, 456, 366, 439, 7527, 295, 1651, 510, 13], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 800, "seek": 350804, "start": 3525.68, "end": 3529.52, "text": " When you see data that has a very clear trend, it's very interesting to understand, to try", "tokens": [1133, 291, 536, 1412, 300, 575, 257, 588, 1850, 6028, 11, 309, 311, 588, 1880, 281, 1223, 11, 281, 853], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 801, "seek": 350804, "start": 3529.52, "end": 3534.6, "text": " to think about why is something so simple happening.", "tokens": [281, 519, 466, 983, 307, 746, 370, 2199, 2737, 13], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 802, "seek": 350804, "start": 3534.6, "end": 3536.6, "text": " And I'll sort of leave you with that.", "tokens": [400, 286, 603, 1333, 295, 1856, 291, 365, 300, 13], "temperature": 0.0, "avg_logprob": -0.24868534633091519, "compression_ratio": 1.702127659574468, "no_speech_prob": 4.830359102925286e-05}, {"id": 803, "seek": 353660, "start": 3536.6, "end": 3538.6, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.6374103122287327, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0007492070435546339}, {"id": 804, "seek": 353660, "start": 3538.6, "end": 3556.16, "text": " Have you thought any about how this is scaling laws, what are providing human beings?", "tokens": [3560, 291, 1194, 604, 466, 577, 341, 307, 21589, 6064, 11, 437, 366, 6530, 1952, 8958, 30], "temperature": 0.0, "avg_logprob": -0.6374103122287327, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0007492070435546339}, {"id": 805, "seek": 353660, "start": 3556.16, "end": 3563.16, "text": " So your picture is essentially making everything bigger, avoid bottlenecks, and all the thought.", "tokens": [407, 428, 3036, 307, 4476, 1455, 1203, 3801, 11, 5042, 44641, 2761, 11, 293, 439, 264, 1194, 13], "temperature": 0.0, "avg_logprob": -0.6374103122287327, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0007492070435546339}, {"id": 806, "seek": 356316, "start": 3563.16, "end": 3570.16, "text": " Whereas I guess human beings, so you're good on the number of parameters, because they're", "tokens": [13813, 286, 2041, 1952, 8958, 11, 370, 291, 434, 665, 322, 264, 1230, 295, 9834, 11, 570, 436, 434], "temperature": 0.0, "avg_logprob": -0.7362330627441406, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0008844336261972785}, {"id": 807, "seek": 356316, "start": 3570.16, "end": 3573.16, "text": " still several orders based on two in rooms there.", "tokens": [920, 2940, 9470, 2361, 322, 732, 294, 9396, 456, 13], "temperature": 0.0, "avg_logprob": -0.7362330627441406, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0008844336261972785}, {"id": 808, "seek": 356316, "start": 3573.16, "end": 3580.16, "text": " But it seems like you're not very good on the bottom of the map to use.", "tokens": [583, 309, 2544, 411, 291, 434, 406, 588, 665, 322, 264, 2767, 295, 264, 4471, 281, 764, 13], "temperature": 0.0, "avg_logprob": -0.7362330627441406, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0008844336261972785}, {"id": 809, "seek": 356316, "start": 3580.16, "end": 3585.6, "text": " So human to use is very constrained, but it's only an empty map, so it's because of the", "tokens": [407, 1952, 281, 764, 307, 588, 38901, 11, 457, 309, 311, 787, 364, 6707, 4471, 11, 370, 309, 311, 570, 295, 264], "temperature": 0.0, "avg_logprob": -0.7362330627441406, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0008844336261972785}, {"id": 810, "seek": 356316, "start": 3585.6, "end": 3590.6, "text": " slow processing, because it's also because of the empty demands, try to have it be using", "tokens": [2964, 9007, 11, 570, 309, 311, 611, 570, 295, 264, 6707, 15107, 11, 853, 281, 362, 309, 312, 1228], "temperature": 0.0, "avg_logprob": -0.7362330627441406, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.0008844336261972785}, {"id": 811, "seek": 359060, "start": 3590.6, "end": 3594.08, "text": " most of their parameters first of the time.", "tokens": [881, 295, 641, 9834, 700, 295, 264, 565, 13], "temperature": 0.0, "avg_logprob": -0.542474889755249, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.0005705484072677791}, {"id": 812, "seek": 359060, "start": 3594.08, "end": 3602.6, "text": " And data's a little bit complex, because I guess we get a ton of data, so if you are thinking", "tokens": [400, 1412, 311, 257, 707, 857, 3997, 11, 570, 286, 2041, 321, 483, 257, 2952, 295, 1412, 11, 370, 498, 291, 366, 1953], "temperature": 0.0, "avg_logprob": -0.542474889755249, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.0005705484072677791}, {"id": 813, "seek": 359060, "start": 3602.6, "end": 3611.08, "text": " of that on the amount of language data we get, you know, sort of fully complex language", "tokens": [295, 300, 322, 264, 2372, 295, 2856, 1412, 321, 483, 11, 291, 458, 11, 1333, 295, 4498, 3997, 2856], "temperature": 0.0, "avg_logprob": -0.542474889755249, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.0005705484072677791}, {"id": 814, "seek": 359060, "start": 3611.08, "end": 3618.52, "text": " uses, sort of three orders and make the shoot down, where GPT3 is now.", "tokens": [4960, 11, 1333, 295, 1045, 9470, 293, 652, 264, 3076, 760, 11, 689, 26039, 51, 18, 307, 586, 13], "temperature": 0.0, "avg_logprob": -0.542474889755249, "compression_ratio": 1.5578947368421052, "no_speech_prob": 0.0005705484072677791}, {"id": 815, "seek": 361852, "start": 3618.52, "end": 3622.52, "text": " And get something good seems to happen, if you look at what these are all those.", "tokens": [400, 483, 746, 665, 2544, 281, 1051, 11, 498, 291, 574, 412, 437, 613, 366, 439, 729, 13], "temperature": 0.0, "avg_logprob": -0.2830531758473332, "compression_ratio": 1.8357664233576643, "no_speech_prob": 0.00015249759599100798}, {"id": 816, "seek": 361852, "start": 3622.52, "end": 3624.52, "text": " Any thoughts on that?", "tokens": [2639, 4598, 322, 300, 30], "temperature": 0.0, "avg_logprob": -0.2830531758473332, "compression_ratio": 1.8357664233576643, "no_speech_prob": 0.00015249759599100798}, {"id": 817, "seek": 361852, "start": 3624.52, "end": 3627.04, "text": " I mean, I think it's a fantastic question.", "tokens": [286, 914, 11, 286, 519, 309, 311, 257, 5456, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2830531758473332, "compression_ratio": 1.8357664233576643, "no_speech_prob": 0.00015249759599100798}, {"id": 818, "seek": 361852, "start": 3627.04, "end": 3631.04, "text": " I don't have anything to say that isn't quite speculative.", "tokens": [286, 500, 380, 362, 1340, 281, 584, 300, 1943, 380, 1596, 49415, 13], "temperature": 0.0, "avg_logprob": -0.2830531758473332, "compression_ratio": 1.8357664233576643, "no_speech_prob": 0.00015249759599100798}, {"id": 819, "seek": 361852, "start": 3631.04, "end": 3632.84, "text": " So I mean, I don't have any good answer to the question.", "tokens": [407, 286, 914, 11, 286, 500, 380, 362, 604, 665, 1867, 281, 264, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2830531758473332, "compression_ratio": 1.8357664233576643, "no_speech_prob": 0.00015249759599100798}, {"id": 820, "seek": 361852, "start": 3632.84, "end": 3634.36, "text": " I think it's a great question.", "tokens": [286, 519, 309, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.2830531758473332, "compression_ratio": 1.8357664233576643, "no_speech_prob": 0.00015249759599100798}, {"id": 821, "seek": 361852, "start": 3634.36, "end": 3637.4, "text": " I guess one thing that seems like it's true is that sort of the factor of a thousand", "tokens": [286, 2041, 472, 551, 300, 2544, 411, 309, 311, 2074, 307, 300, 1333, 295, 264, 5952, 295, 257, 4714], "temperature": 0.0, "avg_logprob": -0.2830531758473332, "compression_ratio": 1.8357664233576643, "no_speech_prob": 0.00015249759599100798}, {"id": 822, "seek": 361852, "start": 3637.4, "end": 3640.64, "text": " you mentioned seems pretty common.", "tokens": [291, 2835, 2544, 1238, 2689, 13], "temperature": 0.0, "avg_logprob": -0.2830531758473332, "compression_ratio": 1.8357664233576643, "no_speech_prob": 0.00015249759599100798}, {"id": 823, "seek": 361852, "start": 3640.64, "end": 3644.6, "text": " I mean, my impression is that AlphaGo probably plays like a thousand times more games when", "tokens": [286, 914, 11, 452, 9995, 307, 300, 20588, 12104, 1391, 5749, 411, 257, 4714, 1413, 544, 2813, 562], "temperature": 0.0, "avg_logprob": -0.2830531758473332, "compression_ratio": 1.8357664233576643, "no_speech_prob": 0.00015249759599100798}, {"id": 824, "seek": 364460, "start": 3644.6, "end": 3649.48, "text": " it trains than like a go master does.", "tokens": [309, 16329, 813, 411, 257, 352, 4505, 775, 13], "temperature": 0.0, "avg_logprob": -0.18175686106962316, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.00010882582137128338}, {"id": 825, "seek": 364460, "start": 3649.48, "end": 3655.08, "text": " I think this is a pretty common factor to see in a lot of like ML contexts.", "tokens": [286, 519, 341, 307, 257, 1238, 2689, 5952, 281, 536, 294, 257, 688, 295, 411, 21601, 30628, 13], "temperature": 0.0, "avg_logprob": -0.18175686106962316, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.00010882582137128338}, {"id": 826, "seek": 364460, "start": 3655.08, "end": 3656.24, "text": " But I have no idea why it is.", "tokens": [583, 286, 362, 572, 1558, 983, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.18175686106962316, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.00010882582137128338}, {"id": 827, "seek": 364460, "start": 3656.24, "end": 3659.24, "text": " I don't know if it's that evolution optimized us to learn fast.", "tokens": [286, 500, 380, 458, 498, 309, 311, 300, 9303, 26941, 505, 281, 1466, 2370, 13], "temperature": 0.0, "avg_logprob": -0.18175686106962316, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.00010882582137128338}, {"id": 828, "seek": 364460, "start": 3659.24, "end": 3663.2799999999997, "text": " If we have some hard coded information, if this sort of multimodal inputs that we have", "tokens": [759, 321, 362, 512, 1152, 34874, 1589, 11, 498, 341, 1333, 295, 32972, 378, 304, 15743, 300, 321, 362], "temperature": 0.0, "avg_logprob": -0.18175686106962316, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.00010882582137128338}, {"id": 829, "seek": 364460, "start": 3663.2799999999997, "end": 3668.44, "text": " help a lot, you might imagine that when you have a system that's already pretty smart,", "tokens": [854, 257, 688, 11, 291, 1062, 3811, 300, 562, 291, 362, 257, 1185, 300, 311, 1217, 1238, 4069, 11], "temperature": 0.0, "avg_logprob": -0.18175686106962316, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.00010882582137128338}, {"id": 830, "seek": 364460, "start": 3668.44, "end": 3672.44, "text": " reinforcement learning or active learning of some form becomes more and more important,", "tokens": [29280, 2539, 420, 4967, 2539, 295, 512, 1254, 3643, 544, 293, 544, 1021, 11], "temperature": 0.0, "avg_logprob": -0.18175686106962316, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.00010882582137128338}, {"id": 831, "seek": 367244, "start": 3672.44, "end": 3678.88, "text": " because like when these language models or a person, like if I read a physics textbook,", "tokens": [570, 411, 562, 613, 2856, 5245, 420, 257, 954, 11, 411, 498, 286, 1401, 257, 10649, 25591, 11], "temperature": 0.0, "avg_logprob": -0.19777878125508627, "compression_ratio": 1.75, "no_speech_prob": 9.163329377770424e-05}, {"id": 832, "seek": 367244, "start": 3678.88, "end": 3682.56, "text": " I don't really learn a lot in a certain sense because I already learned physics.", "tokens": [286, 500, 380, 534, 1466, 257, 688, 294, 257, 1629, 2020, 570, 286, 1217, 3264, 10649, 13], "temperature": 0.0, "avg_logprob": -0.19777878125508627, "compression_ratio": 1.75, "no_speech_prob": 9.163329377770424e-05}, {"id": 833, "seek": 367244, "start": 3682.56, "end": 3684.76, "text": " And I think the same is probably true for these models.", "tokens": [400, 286, 519, 264, 912, 307, 1391, 2074, 337, 613, 5245, 13], "temperature": 0.0, "avg_logprob": -0.19777878125508627, "compression_ratio": 1.75, "no_speech_prob": 9.163329377770424e-05}, {"id": 834, "seek": 367244, "start": 3684.76, "end": 3689.92, "text": " So as the models get smarter, this sort of very dumb next word prediction task is giving", "tokens": [407, 382, 264, 5245, 483, 20294, 11, 341, 1333, 295, 588, 10316, 958, 1349, 17630, 5633, 307, 2902], "temperature": 0.0, "avg_logprob": -0.19777878125508627, "compression_ratio": 1.75, "no_speech_prob": 9.163329377770424e-05}, {"id": 835, "seek": 367244, "start": 3689.92, "end": 3694.76, "text": " you less and less information, but you might expect to get more and more information if", "tokens": [291, 1570, 293, 1570, 1589, 11, 457, 291, 1062, 2066, 281, 483, 544, 293, 544, 1589, 498], "temperature": 0.0, "avg_logprob": -0.19777878125508627, "compression_ratio": 1.75, "no_speech_prob": 9.163329377770424e-05}, {"id": 836, "seek": 367244, "start": 3694.76, "end": 3697.16, "text": " you did something more active.", "tokens": [291, 630, 746, 544, 4967, 13], "temperature": 0.0, "avg_logprob": -0.19777878125508627, "compression_ratio": 1.75, "no_speech_prob": 9.163329377770424e-05}, {"id": 837, "seek": 367244, "start": 3697.16, "end": 3701.56, "text": " I can continue to speculate, but I don't really know anything about it.", "tokens": [286, 393, 2354, 281, 40775, 11, 457, 286, 500, 380, 534, 458, 1340, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.19777878125508627, "compression_ratio": 1.75, "no_speech_prob": 9.163329377770424e-05}, {"id": 838, "seek": 370156, "start": 3701.56, "end": 3707.0, "text": " I don't have anything well established to tell you.", "tokens": [286, 500, 380, 362, 1340, 731, 7545, 281, 980, 291, 13], "temperature": 0.0, "avg_logprob": -0.789254150390625, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0002412421308690682}, {"id": 839, "seek": 370156, "start": 3707.0, "end": 3708.2, "text": " It's a great question.", "tokens": [467, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.789254150390625, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0002412421308690682}, {"id": 840, "seek": 370156, "start": 3708.2, "end": 3712.7999999999997, "text": " So if you can't have the transformers, the LFGM, the LFGM, the LFGM, the transformers", "tokens": [407, 498, 291, 393, 380, 362, 264, 4088, 433, 11, 264, 441, 37, 25152, 11, 264, 441, 37, 25152, 11, 264, 441, 37, 25152, 11, 264, 4088, 433], "temperature": 0.0, "avg_logprob": -0.789254150390625, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0002412421308690682}, {"id": 841, "seek": 370156, "start": 3712.7999999999997, "end": 3722.56, "text": " are a bit better, but they're a smaller, similar mode that seems like human abilities", "tokens": [366, 257, 857, 1101, 11, 457, 436, 434, 257, 4356, 11, 2531, 4391, 300, 2544, 411, 1952, 11582], "temperature": 0.0, "avg_logprob": -0.789254150390625, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0002412421308690682}, {"id": 842, "seek": 370156, "start": 3722.56, "end": 3728.12, "text": " are a little bit more developed to a more standard, a few things on a very different place", "tokens": [366, 257, 707, 857, 544, 4743, 281, 257, 544, 3832, 11, 257, 1326, 721, 322, 257, 588, 819, 1081], "temperature": 0.0, "avg_logprob": -0.789254150390625, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0002412421308690682}, {"id": 843, "seek": 370156, "start": 3728.12, "end": 3729.12, "text": " on the graph.", "tokens": [322, 264, 4295, 13], "temperature": 0.0, "avg_logprob": -0.789254150390625, "compression_ratio": 1.6401869158878504, "no_speech_prob": 0.0002412421308690682}, {"id": 844, "seek": 372912, "start": 3729.12, "end": 3732.04, "text": " Yeah, I think that's absolutely, I think it just true.", "tokens": [865, 11, 286, 519, 300, 311, 3122, 11, 286, 519, 309, 445, 2074, 13], "temperature": 0.0, "avg_logprob": -0.2723739848417394, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.00020307744853198528}, {"id": 845, "seek": 372912, "start": 3732.04, "end": 3734.7999999999997, "text": " The simple efficiency of these models is not similar.", "tokens": [440, 2199, 10493, 295, 613, 5245, 307, 406, 2531, 13], "temperature": 0.0, "avg_logprob": -0.2723739848417394, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.00020307744853198528}, {"id": 846, "seek": 372912, "start": 3734.7999999999997, "end": 3739.7599999999998, "text": " Another way of saying is that if you got into AI research to understand the human brain,", "tokens": [3996, 636, 295, 1566, 307, 300, 498, 291, 658, 666, 7318, 2132, 281, 1223, 264, 1952, 3567, 11], "temperature": 0.0, "avg_logprob": -0.2723739848417394, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.00020307744853198528}, {"id": 847, "seek": 372912, "start": 3739.7599999999998, "end": 3743.68, "text": " it's very unclear whether we're making any progress on that.", "tokens": [309, 311, 588, 25636, 1968, 321, 434, 1455, 604, 4205, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.2723739848417394, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.00020307744853198528}, {"id": 848, "seek": 372912, "start": 3743.68, "end": 3749.2, "text": " But if we just want to sort of, yeah, for a lot of these tasks, we don't seem to have", "tokens": [583, 498, 321, 445, 528, 281, 1333, 295, 11, 1338, 11, 337, 257, 688, 295, 613, 9608, 11, 321, 500, 380, 1643, 281, 362], "temperature": 0.0, "avg_logprob": -0.2723739848417394, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.00020307744853198528}, {"id": 849, "seek": 372912, "start": 3749.2, "end": 3752.08, "text": " to solve the brain to solve AI surprisingly.", "tokens": [281, 5039, 264, 3567, 281, 5039, 7318, 17600, 13], "temperature": 0.0, "avg_logprob": -0.2723739848417394, "compression_ratio": 1.6074380165289257, "no_speech_prob": 0.00020307744853198528}, {"id": 850, "seek": 375208, "start": 3752.08, "end": 3762.2799999999997, "text": " I think they have a question.", "tokens": [286, 519, 436, 362, 257, 1168, 13], "temperature": 0.2, "avg_logprob": -0.9679478963216146, "compression_ratio": 0.8536585365853658, "no_speech_prob": 0.000661113765090704}, {"id": 851, "seek": 375208, "start": 3762.2799999999997, "end": 3763.2799999999997, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.2, "avg_logprob": -0.9679478963216146, "compression_ratio": 0.8536585365853658, "no_speech_prob": 0.000661113765090704}, {"id": 852, "seek": 376328, "start": 3763.28, "end": 3790.0, "text": " See it's deixar, and now I can wonder what you're going to get in this car and push", "tokens": [3008, 309, 311, 19701, 11, 293, 586, 286, 393, 2441, 437, 291, 434, 516, 281, 483, 294, 341, 1032, 293, 2944], "temperature": 1.0, "avg_logprob": -3.540516357421875, "compression_ratio": 1.064102564102564, "no_speech_prob": 0.004854376893490553}, {"id": 853, "seek": 379000, "start": 3790.0, "end": 3800.0, "text": " There's also like things that will probably come,", "tokens": [821, 311, 611, 411, 721, 300, 486, 1391, 808, 11], "temperature": 0.0, "avg_logprob": -0.5657488141741072, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.151471808552742}, {"id": 854, "seek": 379000, "start": 3800.0, "end": 3803.0, "text": " remember, really, like, not just acting in the past,", "tokens": [1604, 11, 534, 11, 411, 11, 406, 445, 6577, 294, 264, 1791, 11], "temperature": 0.0, "avg_logprob": -0.5657488141741072, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.151471808552742}, {"id": 855, "seek": 379000, "start": 3803.0, "end": 3806.0, "text": " that probably aren't necessarily written for,", "tokens": [300, 1391, 3212, 380, 4725, 3720, 337, 11], "temperature": 0.0, "avg_logprob": -0.5657488141741072, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.151471808552742}, {"id": 856, "seek": 379000, "start": 3806.0, "end": 3808.0, "text": " and by great science.", "tokens": [293, 538, 869, 3497, 13], "temperature": 0.0, "avg_logprob": -0.5657488141741072, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.151471808552742}, {"id": 857, "seek": 379000, "start": 3808.0, "end": 3809.0, "text": " Okay, so, Bob and Bob.", "tokens": [1033, 11, 370, 11, 6085, 293, 6085, 13], "temperature": 0.0, "avg_logprob": -0.5657488141741072, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.151471808552742}, {"id": 858, "seek": 379000, "start": 3809.0, "end": 3811.0, "text": " Sure, sure, no, these are all great questions.", "tokens": [4894, 11, 988, 11, 572, 11, 613, 366, 439, 869, 1651, 13], "temperature": 0.0, "avg_logprob": -0.5657488141741072, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.151471808552742}, {"id": 859, "seek": 379000, "start": 3811.0, "end": 3815.0, "text": " So, I mean, sort of early on,", "tokens": [407, 11, 286, 914, 11, 1333, 295, 2440, 322, 11], "temperature": 0.0, "avg_logprob": -0.5657488141741072, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.151471808552742}, {"id": 860, "seek": 379000, "start": 3815.0, "end": 3817.0, "text": " I commented on some sources of data,", "tokens": [286, 26940, 322, 512, 7139, 295, 1412, 11], "temperature": 0.0, "avg_logprob": -0.5657488141741072, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.151471808552742}, {"id": 861, "seek": 379000, "start": 3817.0, "end": 3819.0, "text": " and I mean, you're certainly correct about quality.", "tokens": [293, 286, 914, 11, 291, 434, 3297, 3006, 466, 3125, 13], "temperature": 0.0, "avg_logprob": -0.5657488141741072, "compression_ratio": 1.5745614035087718, "no_speech_prob": 0.151471808552742}, {"id": 862, "seek": 381900, "start": 3819.0, "end": 3822.0, "text": " I think in terms of quantity, I mean,", "tokens": [286, 519, 294, 2115, 295, 11275, 11, 286, 914, 11], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 863, "seek": 381900, "start": 3822.0, "end": 3826.0, "text": " I don't think anyone has, like, a digitized library of Congress,", "tokens": [286, 500, 380, 519, 2878, 575, 11, 411, 11, 257, 14293, 1602, 6405, 295, 6426, 11], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 864, "seek": 381900, "start": 3826.0, "end": 3828.0, "text": " but I think if you did, that would be like,", "tokens": [457, 286, 519, 498, 291, 630, 11, 300, 576, 312, 411, 11], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 865, "seek": 381900, "start": 3828.0, "end": 3831.0, "text": " I don't know, maybe 10x bigger than the training set for GPD3.", "tokens": [286, 500, 380, 458, 11, 1310, 1266, 87, 3801, 813, 264, 3097, 992, 337, 26039, 35, 18, 13], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 866, "seek": 381900, "start": 3831.0, "end": 3834.0, "text": " So, there's a sense in which there's probably quite a lot of,", "tokens": [407, 11, 456, 311, 257, 2020, 294, 597, 456, 311, 1391, 1596, 257, 688, 295, 11], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 867, "seek": 381900, "start": 3834.0, "end": 3837.0, "text": " still quite high quality data that isn't in use.", "tokens": [920, 1596, 1090, 3125, 1412, 300, 1943, 380, 294, 764, 13], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 868, "seek": 381900, "start": 3837.0, "end": 3839.0, "text": " I don't know whether it will ever be in use,", "tokens": [286, 500, 380, 458, 1968, 309, 486, 1562, 312, 294, 764, 11], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 869, "seek": 381900, "start": 3839.0, "end": 3841.0, "text": " so it's a complicated question.", "tokens": [370, 309, 311, 257, 6179, 1168, 13], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 870, "seek": 381900, "start": 3841.0, "end": 3844.0, "text": " And then, if you are willing to sort of take all of this garbage", "tokens": [400, 550, 11, 498, 291, 366, 4950, 281, 1333, 295, 747, 439, 295, 341, 14150], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 871, "seek": 381900, "start": 3844.0, "end": 3848.0, "text": " on the internet, or try to filter that garbage down,", "tokens": [322, 264, 4705, 11, 420, 853, 281, 6608, 300, 14150, 760, 11], "temperature": 0.0, "avg_logprob": -0.1269984971608547, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00012132764823036268}, {"id": 872, "seek": 384800, "start": 3848.0, "end": 3851.0, "text": " I think, I don't know how accurate this estimate is,", "tokens": [286, 519, 11, 286, 500, 380, 458, 577, 8559, 341, 12539, 307, 11], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 873, "seek": 384800, "start": 3851.0, "end": 3853.0, "text": " but in order of magnitude level,", "tokens": [457, 294, 1668, 295, 15668, 1496, 11], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 874, "seek": 384800, "start": 3853.0, "end": 3855.0, "text": " you can get something like 10 to the 15 words,", "tokens": [291, 393, 483, 746, 411, 1266, 281, 264, 2119, 2283, 11], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 875, "seek": 384800, "start": 3855.0, "end": 3857.0, "text": " which is a thousand times bigger.", "tokens": [597, 307, 257, 4714, 1413, 3801, 13], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 876, "seek": 384800, "start": 3857.0, "end": 3860.0, "text": " And of course, if you find any kind of intelligent way of filtering,", "tokens": [400, 295, 1164, 11, 498, 291, 915, 604, 733, 295, 13232, 636, 295, 30822, 11], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 877, "seek": 384800, "start": 3860.0, "end": 3863.0, "text": " then if you can filter down to 0.1% of that,", "tokens": [550, 498, 291, 393, 6608, 760, 281, 1958, 13, 16, 4, 295, 300, 11], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 878, "seek": 384800, "start": 3863.0, "end": 3865.0, "text": " and take the 0.1% that's best,", "tokens": [293, 747, 264, 1958, 13, 16, 4, 300, 311, 1151, 11], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 879, "seek": 384800, "start": 3865.0, "end": 3867.0, "text": " then you do still have a lot of data.", "tokens": [550, 291, 360, 920, 362, 257, 688, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 880, "seek": 384800, "start": 3867.0, "end": 3868.0, "text": " So, I think for language modeling,", "tokens": [407, 11, 286, 519, 337, 2856, 15983, 11], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 881, "seek": 384800, "start": 3868.0, "end": 3870.0, "text": " there's definitely still some headroom,", "tokens": [456, 311, 2138, 920, 512, 1378, 2861, 11], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 882, "seek": 384800, "start": 3870.0, "end": 3874.0, "text": " but this is certainly a constraint,", "tokens": [457, 341, 307, 3297, 257, 25534, 11], "temperature": 0.0, "avg_logprob": -0.10106615137170863, "compression_ratio": 1.6254416961130742, "no_speech_prob": 0.0001397442101733759}, {"id": 883, "seek": 387400, "start": 3874.0, "end": 3878.0, "text": " and there are other kinds of data distributions", "tokens": [293, 456, 366, 661, 3685, 295, 1412, 37870], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 884, "seek": 387400, "start": 3878.0, "end": 3880.0, "text": " where you'll run out sooner.", "tokens": [689, 291, 603, 1190, 484, 15324, 13], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 885, "seek": 387400, "start": 3880.0, "end": 3884.0, "text": " I mean, in terms of, yeah, I mean,", "tokens": [286, 914, 11, 294, 2115, 295, 11, 1338, 11, 286, 914, 11], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 886, "seek": 387400, "start": 3884.0, "end": 3886.0, "text": " of course, there are all sorts of other things you can explore,", "tokens": [295, 1164, 11, 456, 366, 439, 7527, 295, 661, 721, 291, 393, 6839, 11], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 887, "seek": 387400, "start": 3886.0, "end": 3888.0, "text": " one you can explore, multi-modal models,", "tokens": [472, 291, 393, 6839, 11, 4825, 12, 8014, 304, 5245, 11], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 888, "seek": 387400, "start": 3888.0, "end": 3891.0, "text": " one can switch to a different kind of loss function", "tokens": [472, 393, 3679, 281, 257, 819, 733, 295, 4470, 2445], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 889, "seek": 387400, "start": 3891.0, "end": 3895.0, "text": " that is more interactive, or actually accomplishing a task.", "tokens": [300, 307, 544, 15141, 11, 420, 767, 6548, 3807, 257, 5633, 13], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 890, "seek": 387400, "start": 3895.0, "end": 3898.0, "text": " But I think, for pure language modeling,", "tokens": [583, 286, 519, 11, 337, 6075, 2856, 15983, 11], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 891, "seek": 387400, "start": 3898.0, "end": 3900.0, "text": " it seems like there's at least some room left.", "tokens": [309, 2544, 411, 456, 311, 412, 1935, 512, 1808, 1411, 13], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 892, "seek": 387400, "start": 3900.0, "end": 3903.0, "text": " And if you think that your model size increases,", "tokens": [400, 498, 291, 519, 300, 428, 2316, 2744, 8637, 11], "temperature": 0.0, "avg_logprob": -0.10452642138042147, "compression_ratio": 1.7158671586715868, "no_speech_prob": 8.744032675167546e-05}, {"id": 893, "seek": 390300, "start": 3903.0, "end": 3906.0, "text": " sort of, if you think you can increase your model size by a factor of 100", "tokens": [1333, 295, 11, 498, 291, 519, 291, 393, 3488, 428, 2316, 2744, 538, 257, 5952, 295, 2319], "temperature": 0.0, "avg_logprob": -0.17742209341011794, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.00014191435184329748}, {"id": 894, "seek": 390300, "start": 3906.0, "end": 3909.0, "text": " and increase your data set size by a factor of 10,", "tokens": [293, 3488, 428, 1412, 992, 2744, 538, 257, 5952, 295, 1266, 11], "temperature": 0.0, "avg_logprob": -0.17742209341011794, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.00014191435184329748}, {"id": 895, "seek": 390300, "start": 3909.0, "end": 3914.0, "text": " which is sort of like roughly what this is saying.", "tokens": [597, 307, 1333, 295, 411, 9810, 437, 341, 307, 1566, 13], "temperature": 0.0, "avg_logprob": -0.17742209341011794, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.00014191435184329748}, {"id": 896, "seek": 390300, "start": 3914.0, "end": 3918.0, "text": " If you believe that, then you can still scale up your model size a lot", "tokens": [759, 291, 1697, 300, 11, 550, 291, 393, 920, 4373, 493, 428, 2316, 2744, 257, 688], "temperature": 0.0, "avg_logprob": -0.17742209341011794, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.00014191435184329748}, {"id": 897, "seek": 390300, "start": 3918.0, "end": 3921.0, "text": " and have probably plenty of data.", "tokens": [293, 362, 1391, 7140, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.17742209341011794, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.00014191435184329748}, {"id": 898, "seek": 390300, "start": 3921.0, "end": 3927.0, "text": " But, yeah, you couldn't sort of do this stuff without the internet.", "tokens": [583, 11, 1338, 11, 291, 2809, 380, 1333, 295, 360, 341, 1507, 1553, 264, 4705, 13], "temperature": 0.0, "avg_logprob": -0.17742209341011794, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.00014191435184329748}, {"id": 899, "seek": 390300, "start": 3927.0, "end": 3929.0, "text": " Yeah, or, you know,", "tokens": [865, 11, 420, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.17742209341011794, "compression_ratio": 1.727699530516432, "no_speech_prob": 0.00014191435184329748}, {"id": 900, "seek": 392900, "start": 3929.0, "end": 3933.0, "text": " do you want to share it?", "tokens": [360, 291, 528, 281, 2073, 309, 30], "temperature": 0.0, "avg_logprob": -0.6554728984832764, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0002244129282189533}, {"id": 901, "seek": 392900, "start": 3933.0, "end": 3934.0, "text": " Sure, yeah.", "tokens": [4894, 11, 1338, 13], "temperature": 0.0, "avg_logprob": -0.6554728984832764, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0002244129282189533}, {"id": 902, "seek": 392900, "start": 3934.0, "end": 3939.0, "text": " In terms of bottlenecks for improving a task of a time,", "tokens": [682, 2115, 295, 44641, 2761, 337, 11470, 257, 5633, 295, 257, 565, 11], "temperature": 0.0, "avg_logprob": -0.6554728984832764, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0002244129282189533}, {"id": 903, "seek": 392900, "start": 3939.0, "end": 3942.0, "text": " are you more optimistic about", "tokens": [366, 291, 544, 19397, 466], "temperature": 0.0, "avg_logprob": -0.6554728984832764, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0002244129282189533}, {"id": 904, "seek": 392900, "start": 3942.0, "end": 3947.0, "text": " how much larger models on the same level", "tokens": [577, 709, 4833, 5245, 322, 264, 912, 1496], "temperature": 0.0, "avg_logprob": -0.6554728984832764, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0002244129282189533}, {"id": 905, "seek": 392900, "start": 3947.0, "end": 3950.0, "text": " as a cheese water improvement,", "tokens": [382, 257, 5399, 1281, 10444, 11], "temperature": 0.0, "avg_logprob": -0.6554728984832764, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0002244129282189533}, {"id": 906, "seek": 392900, "start": 3950.0, "end": 3951.0, "text": " or petrol improvements,", "tokens": [420, 32377, 13797, 11], "temperature": 0.0, "avg_logprob": -0.6554728984832764, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0002244129282189533}, {"id": 907, "seek": 392900, "start": 3951.0, "end": 3953.0, "text": " like the LSDF transform?", "tokens": [411, 264, 441, 23969, 37, 4088, 30], "temperature": 0.0, "avg_logprob": -0.6554728984832764, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0002244129282189533}, {"id": 908, "seek": 392900, "start": 3953.0, "end": 3956.0, "text": " I guess, I mean,", "tokens": [286, 2041, 11, 286, 914, 11], "temperature": 0.0, "avg_logprob": -0.6554728984832764, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0002244129282189533}, {"id": 909, "seek": 395600, "start": 3956.0, "end": 3959.0, "text": " I think I'm sort of optimistic about both.", "tokens": [286, 519, 286, 478, 1333, 295, 19397, 466, 1293, 13], "temperature": 0.0, "avg_logprob": -0.15521907806396484, "compression_ratio": 1.7410358565737052, "no_speech_prob": 8.209537918446586e-05}, {"id": 910, "seek": 395600, "start": 3959.0, "end": 3963.0, "text": " I think that my understanding, sort of the zero-thorder understanding", "tokens": [286, 519, 300, 452, 3701, 11, 1333, 295, 264, 4018, 12, 392, 4687, 3701], "temperature": 0.0, "avg_logprob": -0.15521907806396484, "compression_ratio": 1.7410358565737052, "no_speech_prob": 8.209537918446586e-05}, {"id": 911, "seek": 395600, "start": 3963.0, "end": 3965.0, "text": " of the hardware situation is that, like,", "tokens": [295, 264, 8837, 2590, 307, 300, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.15521907806396484, "compression_ratio": 1.7410358565737052, "no_speech_prob": 8.209537918446586e-05}, {"id": 912, "seek": 395600, "start": 3965.0, "end": 3968.0, "text": " connecting together GPUs and GPU,", "tokens": [11015, 1214, 18407, 82, 293, 18407, 11], "temperature": 0.0, "avg_logprob": -0.15521907806396484, "compression_ratio": 1.7410358565737052, "no_speech_prob": 8.209537918446586e-05}, {"id": 913, "seek": 395600, "start": 3968.0, "end": 3970.0, "text": " like objects works pretty well,", "tokens": [411, 6565, 1985, 1238, 731, 11], "temperature": 0.0, "avg_logprob": -0.15521907806396484, "compression_ratio": 1.7410358565737052, "no_speech_prob": 8.209537918446586e-05}, {"id": 914, "seek": 395600, "start": 3970.0, "end": 3972.0, "text": " and that, like,", "tokens": [293, 300, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.15521907806396484, "compression_ratio": 1.7410358565737052, "no_speech_prob": 8.209537918446586e-05}, {"id": 915, "seek": 395600, "start": 3972.0, "end": 3976.0, "text": " interconnection speeds are increasing and can increase pretty easily.", "tokens": [26253, 313, 16411, 366, 5662, 293, 393, 3488, 1238, 3612, 13], "temperature": 0.0, "avg_logprob": -0.15521907806396484, "compression_ratio": 1.7410358565737052, "no_speech_prob": 8.209537918446586e-05}, {"id": 916, "seek": 395600, "start": 3976.0, "end": 3980.0, "text": " So I think that you don't need one chip to run your entire model.", "tokens": [407, 286, 519, 300, 291, 500, 380, 643, 472, 11409, 281, 1190, 428, 2302, 2316, 13], "temperature": 0.0, "avg_logprob": -0.15521907806396484, "compression_ratio": 1.7410358565737052, "no_speech_prob": 8.209537918446586e-05}, {"id": 917, "seek": 395600, "start": 3980.0, "end": 3985.0, "text": " You can distribute your model over many, many, many accelerators.", "tokens": [509, 393, 20594, 428, 2316, 670, 867, 11, 867, 11, 867, 10172, 3391, 13], "temperature": 0.0, "avg_logprob": -0.15521907806396484, "compression_ratio": 1.7410358565737052, "no_speech_prob": 8.209537918446586e-05}, {"id": 918, "seek": 398500, "start": 3985.0, "end": 3989.0, "text": " And I think you can do that if you're willing to pay for those accelerators,", "tokens": [400, 286, 519, 291, 393, 360, 300, 498, 291, 434, 4950, 281, 1689, 337, 729, 10172, 3391, 11], "temperature": 0.0, "avg_logprob": -0.13792340457439423, "compression_ratio": 1.972972972972973, "no_speech_prob": 7.248438487295061e-05}, {"id": 919, "seek": 398500, "start": 3989.0, "end": 3992.0, "text": " et cetera, then I think you can do that.", "tokens": [1030, 11458, 11, 550, 286, 519, 291, 393, 360, 300, 13], "temperature": 0.0, "avg_logprob": -0.13792340457439423, "compression_ratio": 1.972972972972973, "no_speech_prob": 7.248438487295061e-05}, {"id": 920, "seek": 398500, "start": 3992.0, "end": 3995.0, "text": " Architectural improvements, I think,", "tokens": [29306, 1807, 13797, 11, 286, 519, 11], "temperature": 0.0, "avg_logprob": -0.13792340457439423, "compression_ratio": 1.972972972972973, "no_speech_prob": 7.248438487295061e-05}, {"id": 921, "seek": 398500, "start": 3995.0, "end": 4000.0, "text": " I would say it sort of typically haven't been super excited about architectural improvements,", "tokens": [286, 576, 584, 309, 1333, 295, 5850, 2378, 380, 668, 1687, 2919, 466, 26621, 13797, 11], "temperature": 0.0, "avg_logprob": -0.13792340457439423, "compression_ratio": 1.972972972972973, "no_speech_prob": 7.248438487295061e-05}, {"id": 922, "seek": 398500, "start": 4000.0, "end": 4005.0, "text": " but I think there will continue to be architectural improvements.", "tokens": [457, 286, 519, 456, 486, 2354, 281, 312, 26621, 13797, 13], "temperature": 0.0, "avg_logprob": -0.13792340457439423, "compression_ratio": 1.972972972972973, "no_speech_prob": 7.248438487295061e-05}, {"id": 923, "seek": 398500, "start": 4005.0, "end": 4009.0, "text": " I think that, sort of, whenever you do something for the first time,", "tokens": [286, 519, 300, 11, 1333, 295, 11, 5699, 291, 360, 746, 337, 264, 700, 565, 11], "temperature": 0.0, "avg_logprob": -0.13792340457439423, "compression_ratio": 1.972972972972973, "no_speech_prob": 7.248438487295061e-05}, {"id": 924, "seek": 398500, "start": 4009.0, "end": 4012.0, "text": " or even just, like, whenever you train a really big model for the first time,", "tokens": [420, 754, 445, 11, 411, 11, 5699, 291, 3847, 257, 534, 955, 2316, 337, 264, 700, 565, 11], "temperature": 0.0, "avg_logprob": -0.13792340457439423, "compression_ratio": 1.972972972972973, "no_speech_prob": 7.248438487295061e-05}, {"id": 925, "seek": 398500, "start": 4012.0, "end": 4014.0, "text": " you sort of don't do it in the best possible way,", "tokens": [291, 1333, 295, 500, 380, 360, 309, 294, 264, 1151, 1944, 636, 11], "temperature": 0.0, "avg_logprob": -0.13792340457439423, "compression_ratio": 1.972972972972973, "no_speech_prob": 7.248438487295061e-05}, {"id": 926, "seek": 401400, "start": 4014.0, "end": 4018.0, "text": " and there's a lot of, like, all sorts of different kinds of improvements.", "tokens": [293, 456, 311, 257, 688, 295, 11, 411, 11, 439, 7527, 295, 819, 3685, 295, 13797, 13], "temperature": 0.0, "avg_logprob": -0.11132165590922037, "compression_ratio": 1.758490566037736, "no_speech_prob": 3.816535900114104e-05}, {"id": 927, "seek": 401400, "start": 4018.0, "end": 4022.0, "text": " Maybe there are, sort of, non-incremental improvements that will look like big jumps.", "tokens": [2704, 456, 366, 11, 1333, 295, 11, 2107, 12, 4647, 265, 15875, 13797, 300, 486, 574, 411, 955, 16704, 13], "temperature": 0.0, "avg_logprob": -0.11132165590922037, "compression_ratio": 1.758490566037736, "no_speech_prob": 3.816535900114104e-05}, {"id": 928, "seek": 401400, "start": 4022.0, "end": 4024.0, "text": " So yeah, I think that'll be both.", "tokens": [407, 1338, 11, 286, 519, 300, 603, 312, 1293, 13], "temperature": 0.0, "avg_logprob": -0.11132165590922037, "compression_ratio": 1.758490566037736, "no_speech_prob": 3.816535900114104e-05}, {"id": 929, "seek": 401400, "start": 4024.0, "end": 4027.0, "text": " So yeah, I mean, there's a sense in which,", "tokens": [407, 1338, 11, 286, 914, 11, 456, 311, 257, 2020, 294, 597, 11], "temperature": 0.0, "avg_logprob": -0.11132165590922037, "compression_ratio": 1.758490566037736, "no_speech_prob": 3.816535900114104e-05}, {"id": 930, "seek": 401400, "start": 4027.0, "end": 4030.0, "text": " if all you did was look at this plot and just try to continue it,", "tokens": [498, 439, 291, 630, 390, 574, 412, 341, 7542, 293, 445, 853, 281, 2354, 309, 11], "temperature": 0.0, "avg_logprob": -0.11132165590922037, "compression_ratio": 1.758490566037736, "no_speech_prob": 3.816535900114104e-05}, {"id": 931, "seek": 401400, "start": 4030.0, "end": 4034.0, "text": " that might be an underestimate of progress that the field is going to make,", "tokens": [300, 1062, 312, 364, 35826, 295, 4205, 300, 264, 2519, 307, 516, 281, 652, 11], "temperature": 0.0, "avg_logprob": -0.11132165590922037, "compression_ratio": 1.758490566037736, "no_speech_prob": 3.816535900114104e-05}, {"id": 932, "seek": 403400, "start": 4034.0, "end": 4045.0, "text": " because there will be improvements in architecture and algorithms and things like that.", "tokens": [570, 456, 486, 312, 13797, 294, 9482, 293, 14642, 293, 721, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.23018225034077963, "compression_ratio": 1.1012658227848102, "no_speech_prob": 4.118083234061487e-05}, {"id": 933, "seek": 404500, "start": 4045.0, "end": 4061.0, "text": " So this related input was looking very good at the testing process,", "tokens": [407, 341, 4077, 4846, 390, 1237, 588, 665, 412, 264, 4997, 1399, 11], "temperature": 1.0, "avg_logprob": -4.215841293334961, "compression_ratio": 1.2277227722772277, "no_speech_prob": 0.00011972043284913525}, {"id": 934, "seek": 404500, "start": 4061.0, "end": 4066.0, "text": " and it looked by some computer and new things were inBS.", "tokens": [293, 309, 2956, 538, 512, 3820, 293, 777, 721, 645, 294, 8176, 13], "temperature": 1.0, "avg_logprob": -4.215841293334961, "compression_ratio": 1.2277227722772277, "no_speech_prob": 0.00011972043284913525}, {"id": 935, "seek": 406600, "start": 4066.0, "end": 4069.0, "text": " And then he can do everything he's going to do", "tokens": [400, 550, 415, 393, 360, 1203, 415, 311, 516, 281, 360], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 936, "seek": 406600, "start": 4069.0, "end": 4072.0, "text": " with doing that this scaling long,", "tokens": [365, 884, 300, 341, 21589, 938, 11], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 937, "seek": 406600, "start": 4072.0, "end": 4074.0, "text": " like, in the fall part, and he'll force back", "tokens": [411, 11, 294, 264, 2100, 644, 11, 293, 415, 603, 3464, 646], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 938, "seek": 406600, "start": 4074.0, "end": 4077.0, "text": " this type of almost the territory.", "tokens": [341, 2010, 295, 1920, 264, 11360, 13], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 939, "seek": 406600, "start": 4077.0, "end": 4080.0, "text": " I think that's a great question.", "tokens": [286, 519, 300, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 940, "seek": 406600, "start": 4080.0, "end": 4082.0, "text": " I think the simplest version of this,", "tokens": [286, 519, 264, 22811, 3037, 295, 341, 11], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 941, "seek": 406600, "start": 4082.0, "end": 4085.0, "text": " well, a simple version of it that I think is probably", "tokens": [731, 11, 257, 2199, 3037, 295, 309, 300, 286, 519, 307, 1391], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 942, "seek": 406600, "start": 4085.0, "end": 4088.0, "text": " important and increasingly important to sort of", "tokens": [1021, 293, 12980, 1021, 281, 1333, 295], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 943, "seek": 406600, "start": 4088.0, "end": 4090.0, "text": " just reinforcement learning, reinforcement learning", "tokens": [445, 29280, 2539, 11, 29280, 2539], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 944, "seek": 406600, "start": 4090.0, "end": 4092.0, "text": " in a certain sense as a situation where you generate your own data,", "tokens": [294, 257, 1629, 2020, 382, 257, 2590, 689, 291, 8460, 428, 1065, 1412, 11], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 945, "seek": 406600, "start": 4092.0, "end": 4094.0, "text": " because if you have a language model doing RL,", "tokens": [570, 498, 291, 362, 257, 2856, 2316, 884, 497, 43, 11], "temperature": 0.0, "avg_logprob": -0.45839195551834705, "compression_ratio": 1.782918149466192, "no_speech_prob": 0.13974088430404663}, {"id": 946, "seek": 409400, "start": 4094.0, "end": 4097.0, "text": " then it writes something and then you're training on that data.", "tokens": [550, 309, 13657, 746, 293, 550, 291, 434, 3097, 322, 300, 1412, 13], "temperature": 0.0, "avg_logprob": -0.15902929867015164, "compression_ratio": 1.6376811594202898, "no_speech_prob": 8.338339830515906e-05}, {"id": 947, "seek": 409400, "start": 4097.0, "end": 4101.0, "text": " So I definitely do think that that will sort of augment data", "tokens": [407, 286, 2138, 360, 519, 300, 300, 486, 1333, 295, 29919, 1412], "temperature": 0.0, "avg_logprob": -0.15902929867015164, "compression_ratio": 1.6376811594202898, "no_speech_prob": 8.338339830515906e-05}, {"id": 948, "seek": 409400, "start": 4101.0, "end": 4105.0, "text": " and mean that there'll be other avenues for improvement.", "tokens": [293, 914, 300, 456, 603, 312, 661, 43039, 337, 10444, 13], "temperature": 0.0, "avg_logprob": -0.15902929867015164, "compression_ratio": 1.6376811594202898, "no_speech_prob": 8.338339830515906e-05}, {"id": 949, "seek": 409400, "start": 4105.0, "end": 4108.0, "text": " Literal data augmentation itself seems also seem plausible to me.", "tokens": [16090, 304, 1412, 14501, 19631, 2564, 2544, 611, 1643, 39925, 281, 385, 13], "temperature": 0.0, "avg_logprob": -0.15902929867015164, "compression_ratio": 1.6376811594202898, "no_speech_prob": 8.338339830515906e-05}, {"id": 950, "seek": 409400, "start": 4108.0, "end": 4111.0, "text": " I think it's not happening a lot because there still", "tokens": [286, 519, 309, 311, 406, 2737, 257, 688, 570, 456, 920], "temperature": 0.0, "avg_logprob": -0.15902929867015164, "compression_ratio": 1.6376811594202898, "no_speech_prob": 8.338339830515906e-05}, {"id": 951, "seek": 409400, "start": 4111.0, "end": 4114.0, "text": " is more language data out there.", "tokens": [307, 544, 2856, 1412, 484, 456, 13], "temperature": 0.0, "avg_logprob": -0.15902929867015164, "compression_ratio": 1.6376811594202898, "no_speech_prob": 8.338339830515906e-05}, {"id": 952, "seek": 409400, "start": 4117.0, "end": 4118.0, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.15902929867015164, "compression_ratio": 1.6376811594202898, "no_speech_prob": 8.338339830515906e-05}, {"id": 953, "seek": 411800, "start": 4118.0, "end": 4127.0, "text": " I think I've got two versions of this one's more clear,", "tokens": [286, 519, 286, 600, 658, 732, 9606, 295, 341, 472, 311, 544, 1850, 11], "temperature": 0.4, "avg_logprob": -0.75276524969872, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.012535548768937588}, {"id": 954, "seek": 411800, "start": 4127.0, "end": 4131.0, "text": " just coming from a nine, and a few versions back on this one", "tokens": [445, 1348, 490, 257, 4949, 11, 293, 257, 1326, 9606, 646, 322, 341, 472], "temperature": 0.4, "avg_logprob": -0.75276524969872, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.012535548768937588}, {"id": 955, "seek": 411800, "start": 4131.0, "end": 4135.0, "text": " is just like how, what about associated with the language", "tokens": [307, 445, 411, 577, 11, 437, 466, 6615, 365, 264, 2856], "temperature": 0.4, "avg_logprob": -0.75276524969872, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.012535548768937588}, {"id": 956, "seek": 411800, "start": 4135.0, "end": 4138.0, "text": " field and how I can't really explain about that in physics", "tokens": [2519, 293, 577, 286, 393, 380, 534, 2903, 466, 300, 294, 10649], "temperature": 0.4, "avg_logprob": -0.75276524969872, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.012535548768937588}, {"id": 957, "seek": 411800, "start": 4138.0, "end": 4141.0, "text": " in the second part of this.", "tokens": [294, 264, 1150, 644, 295, 341, 13], "temperature": 0.4, "avg_logprob": -0.75276524969872, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.012535548768937588}, {"id": 958, "seek": 411800, "start": 4141.0, "end": 4144.0, "text": " In your research and for this, I'm sure you dealt a lot with", "tokens": [682, 428, 2132, 293, 337, 341, 11, 286, 478, 988, 291, 15991, 257, 688, 365], "temperature": 0.4, "avg_logprob": -0.75276524969872, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.012535548768937588}, {"id": 959, "seek": 411800, "start": 4144.0, "end": 4146.0, "text": " different things going on.", "tokens": [819, 721, 516, 322, 13], "temperature": 0.4, "avg_logprob": -0.75276524969872, "compression_ratio": 1.5936073059360731, "no_speech_prob": 0.012535548768937588}, {"id": 960, "seek": 414600, "start": 4146.0, "end": 4149.0, "text": " But I kind of understand to this type of stuff,", "tokens": [583, 286, 733, 295, 1223, 281, 341, 2010, 295, 1507, 11], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 961, "seek": 414600, "start": 4149.0, "end": 4152.0, "text": " other than finding that you found particularly", "tokens": [661, 813, 5006, 300, 291, 1352, 4098], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 962, "seek": 414600, "start": 4152.0, "end": 4154.0, "text": " surprising we're going to expect you,", "tokens": [8830, 321, 434, 516, 281, 2066, 291, 11], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 963, "seek": 414600, "start": 4154.0, "end": 4157.0, "text": " I'm just with your past experience,", "tokens": [286, 478, 445, 365, 428, 1791, 1752, 11], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 964, "seek": 414600, "start": 4157.0, "end": 4160.0, "text": " because I always get this a lot of different types of data.", "tokens": [570, 286, 1009, 483, 341, 257, 688, 295, 819, 3467, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 965, "seek": 414600, "start": 4160.0, "end": 4163.0, "text": " Also since now, two of you are going to understand,", "tokens": [2743, 1670, 586, 11, 732, 295, 291, 366, 516, 281, 1223, 11], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 966, "seek": 414600, "start": 4163.0, "end": 4166.0, "text": " but for you like under this or like other stuff", "tokens": [457, 337, 291, 411, 833, 341, 420, 411, 661, 1507], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 967, "seek": 414600, "start": 4166.0, "end": 4168.0, "text": " that you're doing by phenolic,", "tokens": [300, 291, 434, 884, 538, 7279, 7940, 11], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 968, "seek": 414600, "start": 4168.0, "end": 4171.0, "text": " you're using stuff right now, like if you're anything that", "tokens": [291, 434, 1228, 1507, 558, 586, 11, 411, 498, 291, 434, 1340, 300], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 969, "seek": 414600, "start": 4171.0, "end": 4174.0, "text": " is coming through like this and going to call us about that", "tokens": [307, 1348, 807, 411, 341, 293, 516, 281, 818, 505, 466, 300], "temperature": 0.0, "avg_logprob": -0.6511432893814579, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.00016833790868986398}, {"id": 970, "seek": 417400, "start": 4174.0, "end": 4178.0, "text": " or just like particularly, so, supposing.", "tokens": [420, 445, 411, 4098, 11, 370, 11, 1003, 6110, 13], "temperature": 0.0, "avg_logprob": -0.22554395816944264, "compression_ratio": 1.7476190476190476, "no_speech_prob": 6.150112312752753e-05}, {"id": 971, "seek": 417400, "start": 4178.0, "end": 4184.0, "text": " I think to me the most surprising thing is of these sorts of", "tokens": [286, 519, 281, 385, 264, 881, 8830, 551, 307, 295, 613, 7527, 295], "temperature": 0.0, "avg_logprob": -0.22554395816944264, "compression_ratio": 1.7476190476190476, "no_speech_prob": 6.150112312752753e-05}, {"id": 972, "seek": 417400, "start": 4184.0, "end": 4189.0, "text": " results was probably that there is a very, very precise trend.", "tokens": [3542, 390, 1391, 300, 456, 307, 257, 588, 11, 588, 13600, 6028, 13], "temperature": 0.0, "avg_logprob": -0.22554395816944264, "compression_ratio": 1.7476190476190476, "no_speech_prob": 6.150112312752753e-05}, {"id": 973, "seek": 417400, "start": 4189.0, "end": 4193.0, "text": " It seems like, I mean, yeah, I mean, like, I think this is", "tokens": [467, 2544, 411, 11, 286, 914, 11, 1338, 11, 286, 914, 11, 411, 11, 286, 519, 341, 307], "temperature": 0.0, "avg_logprob": -0.22554395816944264, "compression_ratio": 1.7476190476190476, "no_speech_prob": 6.150112312752753e-05}, {"id": 974, "seek": 417400, "start": 4193.0, "end": 4196.0, "text": " sort of an unusual thing, and I think when I saw that,", "tokens": [1333, 295, 364, 10901, 551, 11, 293, 286, 519, 562, 286, 1866, 300, 11], "temperature": 0.0, "avg_logprob": -0.22554395816944264, "compression_ratio": 1.7476190476190476, "no_speech_prob": 6.150112312752753e-05}, {"id": 975, "seek": 417400, "start": 4196.0, "end": 4199.0, "text": " I thought it was a really big deal.", "tokens": [286, 1194, 309, 390, 257, 534, 955, 2028, 13], "temperature": 0.0, "avg_logprob": -0.22554395816944264, "compression_ratio": 1.7476190476190476, "no_speech_prob": 6.150112312752753e-05}, {"id": 976, "seek": 417400, "start": 4199.0, "end": 4202.0, "text": " I think that like, usually like, I mean, it's just,", "tokens": [286, 519, 300, 411, 11, 2673, 411, 11, 286, 914, 11, 309, 311, 445, 11], "temperature": 0.0, "avg_logprob": -0.22554395816944264, "compression_ratio": 1.7476190476190476, "no_speech_prob": 6.150112312752753e-05}, {"id": 977, "seek": 420200, "start": 4202.0, "end": 4205.0, "text": " it's not true in most many things you plot.", "tokens": [309, 311, 406, 2074, 294, 881, 867, 721, 291, 7542, 13], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 978, "seek": 420200, "start": 4205.0, "end": 4207.0, "text": " I mean, obviously there are other plots that don't show", "tokens": [286, 914, 11, 2745, 456, 366, 661, 28609, 300, 500, 380, 855], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 979, "seek": 420200, "start": 4207.0, "end": 4209.0, "text": " this kind of trend, even if they're reasonable.", "tokens": [341, 733, 295, 6028, 11, 754, 498, 436, 434, 10585, 13], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 980, "seek": 420200, "start": 4209.0, "end": 4211.0, "text": " I mean, like, I don't know, I mean, there's sort of a trend", "tokens": [286, 914, 11, 411, 11, 286, 500, 380, 458, 11, 286, 914, 11, 456, 311, 1333, 295, 257, 6028], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 981, "seek": 420200, "start": 4211.0, "end": 4214.0, "text": " to interview a QA, but I don't really know what that means.", "tokens": [281, 4049, 257, 1249, 32, 11, 457, 286, 500, 380, 534, 458, 437, 300, 1355, 13], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 982, "seek": 420200, "start": 4214.0, "end": 4217.0, "text": " But I think the fact that there's something seemingly", "tokens": [583, 286, 519, 264, 1186, 300, 456, 311, 746, 18709], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 983, "seek": 420200, "start": 4217.0, "end": 4222.0, "text": " very precise is, I view that as like a very intriguing", "tokens": [588, 13600, 307, 11, 286, 1910, 300, 382, 411, 257, 588, 32503], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 984, "seek": 420200, "start": 4222.0, "end": 4225.0, "text": " entry point to like try to dig into something,", "tokens": [8729, 935, 281, 411, 853, 281, 2528, 666, 746, 11], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 985, "seek": 420200, "start": 4225.0, "end": 4227.0, "text": " because it means that there's probably some deeper reason.", "tokens": [570, 309, 1355, 300, 456, 311, 1391, 512, 7731, 1778, 13], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 986, "seek": 420200, "start": 4227.0, "end": 4230.0, "text": " And then the fact that it seems fairly universal", "tokens": [400, 550, 264, 1186, 300, 309, 2544, 6457, 11455], "temperature": 0.0, "avg_logprob": -0.10883201252330434, "compression_ratio": 1.8631578947368421, "no_speech_prob": 2.1762096366728656e-05}, {"id": 987, "seek": 423000, "start": 4230.0, "end": 4233.0, "text": " across data distributions, again, suggests something like that.", "tokens": [2108, 1412, 37870, 11, 797, 11, 13409, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.16528724221622243, "compression_ratio": 1.7755905511811023, "no_speech_prob": 6.089982343837619e-05}, {"id": 988, "seek": 423000, "start": 4233.0, "end": 4237.0, "text": " Yeah, the main difference between data distributions is", "tokens": [865, 11, 264, 2135, 2649, 1296, 1412, 37870, 307], "temperature": 0.0, "avg_logprob": -0.16528724221622243, "compression_ratio": 1.7755905511811023, "no_speech_prob": 6.089982343837619e-05}, {"id": 989, "seek": 423000, "start": 4237.0, "end": 4240.0, "text": " these exponents in a scaling browser different.", "tokens": [613, 12680, 791, 294, 257, 21589, 11185, 819, 13], "temperature": 0.0, "avg_logprob": -0.16528724221622243, "compression_ratio": 1.7755905511811023, "no_speech_prob": 6.089982343837619e-05}, {"id": 990, "seek": 423000, "start": 4240.0, "end": 4243.0, "text": " I mean, in terms of like coming from physics,", "tokens": [286, 914, 11, 294, 2115, 295, 411, 1348, 490, 10649, 11], "temperature": 0.0, "avg_logprob": -0.16528724221622243, "compression_ratio": 1.7755905511811023, "no_speech_prob": 6.089982343837619e-05}, {"id": 991, "seek": 423000, "start": 4243.0, "end": 4246.0, "text": " I mean, I think I got into like a lot of this stuff partly", "tokens": [286, 914, 11, 286, 519, 286, 658, 666, 411, 257, 688, 295, 341, 1507, 17031], "temperature": 0.0, "avg_logprob": -0.16528724221622243, "compression_ratio": 1.7755905511811023, "no_speech_prob": 6.089982343837619e-05}, {"id": 992, "seek": 423000, "start": 4246.0, "end": 4250.0, "text": " because I'm fairly mercurial, and I was interested,", "tokens": [570, 286, 478, 6457, 10811, 374, 831, 11, 293, 286, 390, 3102, 11], "temperature": 0.0, "avg_logprob": -0.16528724221622243, "compression_ratio": 1.7755905511811023, "no_speech_prob": 6.089982343837619e-05}, {"id": 993, "seek": 423000, "start": 4250.0, "end": 4252.0, "text": " and a lot of other friends I had were interested,", "tokens": [293, 257, 688, 295, 661, 1855, 286, 632, 645, 3102, 11], "temperature": 0.0, "avg_logprob": -0.16528724221622243, "compression_ratio": 1.7755905511811023, "no_speech_prob": 6.089982343837619e-05}, {"id": 994, "seek": 423000, "start": 4252.0, "end": 4255.0, "text": " and so we sort of studied it, and went from there,", "tokens": [293, 370, 321, 1333, 295, 9454, 309, 11, 293, 1437, 490, 456, 11], "temperature": 0.0, "avg_logprob": -0.16528724221622243, "compression_ratio": 1.7755905511811023, "no_speech_prob": 6.089982343837619e-05}, {"id": 995, "seek": 423000, "start": 4255.0, "end": 4258.0, "text": " I had friends, et cetera.", "tokens": [286, 632, 1855, 11, 1030, 11458, 13], "temperature": 0.0, "avg_logprob": -0.16528724221622243, "compression_ratio": 1.7755905511811023, "no_speech_prob": 6.089982343837619e-05}, {"id": 996, "seek": 425800, "start": 4258.0, "end": 4261.0, "text": " But I mean, from another point of view, I think I got involved", "tokens": [583, 286, 914, 11, 490, 1071, 935, 295, 1910, 11, 286, 519, 286, 658, 3288], "temperature": 0.0, "avg_logprob": -0.12369471437790815, "compression_ratio": 1.711340206185567, "no_speech_prob": 2.706668237806298e-05}, {"id": 997, "seek": 425800, "start": 4261.0, "end": 4264.0, "text": " in it for really weird reasons, perhaps, in the sense that like,", "tokens": [294, 309, 337, 534, 3657, 4112, 11, 4317, 11, 294, 264, 2020, 300, 411, 11], "temperature": 0.0, "avg_logprob": -0.12369471437790815, "compression_ratio": 1.711340206185567, "no_speech_prob": 2.706668237806298e-05}, {"id": 998, "seek": 425800, "start": 4264.0, "end": 4268.0, "text": " I just know a lot of people who are already, and sort of,", "tokens": [286, 445, 458, 257, 688, 295, 561, 567, 366, 1217, 11, 293, 1333, 295, 11], "temperature": 0.0, "avg_logprob": -0.12369471437790815, "compression_ratio": 1.711340206185567, "no_speech_prob": 2.706668237806298e-05}, {"id": 999, "seek": 425800, "start": 4268.0, "end": 4271.0, "text": " I don't know, 2015, talking about things like, wow,", "tokens": [286, 500, 380, 458, 11, 7546, 11, 1417, 466, 721, 411, 11, 6076, 11], "temperature": 0.0, "avg_logprob": -0.12369471437790815, "compression_ratio": 1.711340206185567, "no_speech_prob": 2.706668237806298e-05}, {"id": 1000, "seek": 425800, "start": 4271.0, "end": 4274.0, "text": " is like, how much better is AI going to get?", "tokens": [307, 411, 11, 577, 709, 1101, 307, 7318, 516, 281, 483, 30], "temperature": 0.0, "avg_logprob": -0.12369471437790815, "compression_ratio": 1.711340206185567, "no_speech_prob": 2.706668237806298e-05}, {"id": 1001, "seek": 425800, "start": 4274.0, "end": 4277.0, "text": " What are the implications going to be for the world?", "tokens": [708, 366, 264, 16602, 516, 281, 312, 337, 264, 1002, 30], "temperature": 0.0, "avg_logprob": -0.12369471437790815, "compression_ratio": 1.711340206185567, "no_speech_prob": 2.706668237806298e-05}, {"id": 1002, "seek": 425800, "start": 4277.0, "end": 4280.0, "text": " Is this going to keep improving at an addressed clip?", "tokens": [1119, 341, 516, 281, 1066, 11470, 412, 364, 13847, 7353, 30], "temperature": 0.0, "avg_logprob": -0.12369471437790815, "compression_ratio": 1.711340206185567, "no_speech_prob": 2.706668237806298e-05}, {"id": 1003, "seek": 425800, "start": 4280.0, "end": 4283.0, "text": " What are we going to do to sort of make sure that these", "tokens": [708, 366, 321, 516, 281, 360, 281, 1333, 295, 652, 988, 300, 613], "temperature": 0.0, "avg_logprob": -0.12369471437790815, "compression_ratio": 1.711340206185567, "no_speech_prob": 2.706668237806298e-05}, {"id": 1004, "seek": 425800, "start": 4283.0, "end": 4286.0, "text": " models are aligned with human values to use the kind", "tokens": [5245, 366, 17962, 365, 1952, 4190, 281, 764, 264, 733], "temperature": 0.0, "avg_logprob": -0.12369471437790815, "compression_ratio": 1.711340206185567, "no_speech_prob": 2.706668237806298e-05}, {"id": 1005, "seek": 428600, "start": 4286.0, "end": 4289.0, "text": " of usual sort of phrase that's now used?", "tokens": [295, 7713, 1333, 295, 9535, 300, 311, 586, 1143, 30], "temperature": 0.0, "avg_logprob": -0.11272783279418945, "compression_ratio": 1.8164794007490637, "no_speech_prob": 1.91632589121582e-05}, {"id": 1006, "seek": 428600, "start": 4289.0, "end": 4293.0, "text": " And I sort of thought these people were weird and crazy,", "tokens": [400, 286, 1333, 295, 1194, 613, 561, 645, 3657, 293, 3219, 11], "temperature": 0.0, "avg_logprob": -0.11272783279418945, "compression_ratio": 1.8164794007490637, "no_speech_prob": 1.91632589121582e-05}, {"id": 1007, "seek": 428600, "start": 4293.0, "end": 4295.0, "text": " even though they were friends of mine, and I sort of said,", "tokens": [754, 1673, 436, 645, 1855, 295, 3892, 11, 293, 286, 1333, 295, 848, 11], "temperature": 0.0, "avg_logprob": -0.11272783279418945, "compression_ratio": 1.8164794007490637, "no_speech_prob": 1.91632589121582e-05}, {"id": 1008, "seek": 428600, "start": 4295.0, "end": 4298.0, "text": " oh, like this is really dumb, like I don't think that these", "tokens": [1954, 11, 411, 341, 307, 534, 10316, 11, 411, 286, 500, 380, 519, 300, 613], "temperature": 0.0, "avg_logprob": -0.11272783279418945, "compression_ratio": 1.8164794007490637, "no_speech_prob": 1.91632589121582e-05}, {"id": 1009, "seek": 428600, "start": 4298.0, "end": 4300.0, "text": " AI models are really something to worry about.", "tokens": [7318, 5245, 366, 534, 746, 281, 3292, 466, 13], "temperature": 0.0, "avg_logprob": -0.11272783279418945, "compression_ratio": 1.8164794007490637, "no_speech_prob": 1.91632589121582e-05}, {"id": 1010, "seek": 428600, "start": 4300.0, "end": 4305.0, "text": " But like, I was still interested, and sort of was like,", "tokens": [583, 411, 11, 286, 390, 920, 3102, 11, 293, 1333, 295, 390, 411, 11], "temperature": 0.0, "avg_logprob": -0.11272783279418945, "compression_ratio": 1.8164794007490637, "no_speech_prob": 1.91632589121582e-05}, {"id": 1011, "seek": 428600, "start": 4305.0, "end": 4307.0, "text": " well, like smart people I know think that AI is improving", "tokens": [731, 11, 411, 4069, 561, 286, 458, 519, 300, 7318, 307, 11470], "temperature": 0.0, "avg_logprob": -0.11272783279418945, "compression_ratio": 1.8164794007490637, "no_speech_prob": 1.91632589121582e-05}, {"id": 1012, "seek": 428600, "start": 4307.0, "end": 4312.0, "text": " very rapidly, and that might have a lot of impacts,", "tokens": [588, 12910, 11, 293, 300, 1062, 362, 257, 688, 295, 11606, 11], "temperature": 0.0, "avg_logprob": -0.11272783279418945, "compression_ratio": 1.8164794007490637, "no_speech_prob": 1.91632589121582e-05}, {"id": 1013, "seek": 428600, "start": 4312.0, "end": 4315.0, "text": " and might require a lot of sort of caution and thought,", "tokens": [293, 1062, 3651, 257, 688, 295, 1333, 295, 23585, 293, 1194, 11], "temperature": 0.0, "avg_logprob": -0.11272783279418945, "compression_ratio": 1.8164794007490637, "no_speech_prob": 1.91632589121582e-05}, {"id": 1014, "seek": 431500, "start": 4315.0, "end": 4318.0, "text": " and work to sort of make it safe.", "tokens": [293, 589, 281, 1333, 295, 652, 309, 3273, 13], "temperature": 0.0, "avg_logprob": -0.1210762693526897, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.168160664150491e-05}, {"id": 1015, "seek": 431500, "start": 4318.0, "end": 4321.0, "text": " And so that was actually a significant motivation for me", "tokens": [400, 370, 300, 390, 767, 257, 4776, 12335, 337, 385], "temperature": 0.0, "avg_logprob": -0.1210762693526897, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.168160664150491e-05}, {"id": 1016, "seek": 431500, "start": 4321.0, "end": 4322.0, "text": " getting involved.", "tokens": [1242, 3288, 13], "temperature": 0.0, "avg_logprob": -0.1210762693526897, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.168160664150491e-05}, {"id": 1017, "seek": 431500, "start": 4322.0, "end": 4326.0, "text": " It was a mixture of sort of, there being a lot of potentially", "tokens": [467, 390, 257, 9925, 295, 1333, 295, 11, 456, 885, 257, 688, 295, 7263], "temperature": 0.0, "avg_logprob": -0.1210762693526897, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.168160664150491e-05}, {"id": 1018, "seek": 431500, "start": 4326.0, "end": 4329.0, "text": " really intellectually interesting questions,", "tokens": [534, 46481, 1880, 1651, 11], "temperature": 0.0, "avg_logprob": -0.1210762693526897, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.168160664150491e-05}, {"id": 1019, "seek": 431500, "start": 4329.0, "end": 4332.0, "text": " liking to sort of switch fields every few years,", "tokens": [16933, 281, 1333, 295, 3679, 7909, 633, 1326, 924, 11], "temperature": 0.0, "avg_logprob": -0.1210762693526897, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.168160664150491e-05}, {"id": 1020, "seek": 431500, "start": 4332.0, "end": 4336.0, "text": " and friends of mine being very kind of concerned about this", "tokens": [293, 1855, 295, 3892, 885, 588, 733, 295, 5922, 466, 341], "temperature": 0.0, "avg_logprob": -0.1210762693526897, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.168160664150491e-05}, {"id": 1021, "seek": 431500, "start": 4336.0, "end": 4341.0, "text": " question, and yeah, that was sort of what brought me in.", "tokens": [1168, 11, 293, 1338, 11, 300, 390, 1333, 295, 437, 3038, 385, 294, 13], "temperature": 0.0, "avg_logprob": -0.1210762693526897, "compression_ratio": 1.7162162162162162, "no_speech_prob": 7.168160664150491e-05}, {"id": 1022, "seek": 434100, "start": 4341.0, "end": 4345.0, "text": " Are there everything you've seen in this picture, you know,", "tokens": [2014, 456, 1203, 291, 600, 1612, 294, 341, 3036, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.3940354860745944, "compression_ratio": 1.6311111111111112, "no_speech_prob": 9.403676085639745e-05}, {"id": 1023, "seek": 434100, "start": 4345.0, "end": 4348.0, "text": " how do model scale, the one operator,", "tokens": [577, 360, 2316, 4373, 11, 264, 472, 12973, 11], "temperature": 0.0, "avg_logprob": -0.3940354860745944, "compression_ratio": 1.6311111111111112, "no_speech_prob": 9.403676085639745e-05}, {"id": 1024, "seek": 434100, "start": 4348.0, "end": 4352.0, "text": " you know, one factor that you found has the most potential", "tokens": [291, 458, 11, 472, 5952, 300, 291, 1352, 575, 264, 881, 3995], "temperature": 0.0, "avg_logprob": -0.3940354860745944, "compression_ratio": 1.6311111111111112, "no_speech_prob": 9.403676085639745e-05}, {"id": 1025, "seek": 434100, "start": 4352.0, "end": 4358.0, "text": " to work on, the scale we've got for this kind of question.", "tokens": [281, 589, 322, 11, 264, 4373, 321, 600, 658, 337, 341, 733, 295, 1168, 13], "temperature": 0.0, "avg_logprob": -0.3940354860745944, "compression_ratio": 1.6311111111111112, "no_speech_prob": 9.403676085639745e-05}, {"id": 1026, "seek": 434100, "start": 4358.0, "end": 4363.0, "text": " I mean, if we go back to sort of very basic ML ingredients,", "tokens": [286, 914, 11, 498, 321, 352, 646, 281, 1333, 295, 588, 3875, 21601, 6952, 11], "temperature": 0.0, "avg_logprob": -0.3940354860745944, "compression_ratio": 1.6311111111111112, "no_speech_prob": 9.403676085639745e-05}, {"id": 1027, "seek": 434100, "start": 4363.0, "end": 4367.0, "text": " of like, what are these things, like,", "tokens": [295, 411, 11, 437, 366, 613, 721, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.3940354860745944, "compression_ratio": 1.6311111111111112, "no_speech_prob": 9.403676085639745e-05}, {"id": 1028, "seek": 434100, "start": 4367.0, "end": 4369.0, "text": " so there's a sense in which this is all you're doing,", "tokens": [370, 456, 311, 257, 2020, 294, 597, 341, 307, 439, 291, 434, 884, 11], "temperature": 0.0, "avg_logprob": -0.3940354860745944, "compression_ratio": 1.6311111111111112, "no_speech_prob": 9.403676085639745e-05}, {"id": 1029, "seek": 436900, "start": 4369.0, "end": 4372.0, "text": " you choose one of each of these five things.", "tokens": [291, 2826, 472, 295, 1184, 295, 613, 1732, 721, 13], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1030, "seek": 436900, "start": 4372.0, "end": 4375.0, "text": " I would guess that what the objective is,", "tokens": [286, 576, 2041, 300, 437, 264, 10024, 307, 11], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1031, "seek": 436900, "start": 4375.0, "end": 4378.0, "text": " is most likely to sort of change things, in the sense that", "tokens": [307, 881, 3700, 281, 1333, 295, 1319, 721, 11, 294, 264, 2020, 300], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1032, "seek": 436900, "start": 4378.0, "end": 4380.0, "text": " predicting the next word is really sort of one of the", "tokens": [32884, 264, 958, 1349, 307, 534, 1333, 295, 472, 295, 264], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1033, "seek": 436900, "start": 4380.0, "end": 4383.0, "text": " laziest sort of dumbest things you can do.", "tokens": [635, 37567, 1333, 295, 10316, 377, 721, 291, 393, 360, 13], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1034, "seek": 436900, "start": 4383.0, "end": 4387.0, "text": " And, and, I mean, there are all sorts of things,", "tokens": [400, 11, 293, 11, 286, 914, 11, 456, 366, 439, 7527, 295, 721, 11], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1035, "seek": 436900, "start": 4387.0, "end": 4390.0, "text": " so it's really just chosen because you want to be able to", "tokens": [370, 309, 311, 534, 445, 8614, 570, 291, 528, 281, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1036, "seek": 436900, "start": 4390.0, "end": 4392.0, "text": " compute, you want to be able to do back prop,", "tokens": [14722, 11, 291, 528, 281, 312, 1075, 281, 360, 646, 2365, 11], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1037, "seek": 436900, "start": 4392.0, "end": 4395.0, "text": " and so you want to be able to get some differentiable thing,", "tokens": [293, 370, 291, 528, 281, 312, 1075, 281, 483, 512, 819, 9364, 551, 11], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1038, "seek": 436900, "start": 4395.0, "end": 4397.0, "text": " you want to be able to get a lot of data for which you can", "tokens": [291, 528, 281, 312, 1075, 281, 483, 257, 688, 295, 1412, 337, 597, 291, 393], "temperature": 0.0, "avg_logprob": -0.10743429925706652, "compression_ratio": 1.965648854961832, "no_speech_prob": 6.180035416036844e-05}, {"id": 1039, "seek": 439700, "start": 4397.0, "end": 4403.0, "text": " compute this differentiable thing, and so that's the game that you're playing.", "tokens": [14722, 341, 819, 9364, 551, 11, 293, 370, 300, 311, 264, 1216, 300, 291, 434, 2433, 13], "temperature": 0.0, "avg_logprob": -0.1260671278016757, "compression_ratio": 1.7715355805243447, "no_speech_prob": 1.7202242815983482e-05}, {"id": 1040, "seek": 439700, "start": 4403.0, "end": 4406.0, "text": " But, I think that you can have other objectives,", "tokens": [583, 11, 286, 519, 300, 291, 393, 362, 661, 15961, 11], "temperature": 0.0, "avg_logprob": -0.1260671278016757, "compression_ratio": 1.7715355805243447, "no_speech_prob": 1.7202242815983482e-05}, {"id": 1041, "seek": 439700, "start": 4406.0, "end": 4411.0, "text": " like through reinforcement learning, or some other kind of active learning,", "tokens": [411, 807, 29280, 2539, 11, 420, 512, 661, 733, 295, 4967, 2539, 11], "temperature": 0.0, "avg_logprob": -0.1260671278016757, "compression_ratio": 1.7715355805243447, "no_speech_prob": 1.7202242815983482e-05}, {"id": 1042, "seek": 439700, "start": 4411.0, "end": 4414.0, "text": " whatever, I mean, some combination of such things.", "tokens": [2035, 11, 286, 914, 11, 512, 6562, 295, 1270, 721, 13], "temperature": 0.0, "avg_logprob": -0.1260671278016757, "compression_ratio": 1.7715355805243447, "no_speech_prob": 1.7202242815983482e-05}, {"id": 1043, "seek": 439700, "start": 4414.0, "end": 4420.0, "text": " And, I sort of would just guess that generally performance will change a lot more.", "tokens": [400, 11, 286, 1333, 295, 576, 445, 2041, 300, 5101, 3389, 486, 1319, 257, 688, 544, 13], "temperature": 0.0, "avg_logprob": -0.1260671278016757, "compression_ratio": 1.7715355805243447, "no_speech_prob": 1.7202242815983482e-05}, {"id": 1044, "seek": 439700, "start": 4420.0, "end": 4423.0, "text": " Like, if you're expecting sort of these trends to be very different,", "tokens": [1743, 11, 498, 291, 434, 9650, 1333, 295, 613, 13892, 281, 312, 588, 819, 11], "temperature": 0.0, "avg_logprob": -0.1260671278016757, "compression_ratio": 1.7715355805243447, "no_speech_prob": 1.7202242815983482e-05}, {"id": 1045, "seek": 439700, "start": 4423.0, "end": 4425.0, "text": " I would guess they're different if you have a different objective.", "tokens": [286, 576, 2041, 436, 434, 819, 498, 291, 362, 257, 819, 10024, 13], "temperature": 0.0, "avg_logprob": -0.1260671278016757, "compression_ratio": 1.7715355805243447, "no_speech_prob": 1.7202242815983482e-05}, {"id": 1046, "seek": 442500, "start": 4425.0, "end": 4430.0, "text": " I think changing the data distribution, or the model might also change things,", "tokens": [286, 519, 4473, 264, 1412, 7316, 11, 420, 264, 2316, 1062, 611, 1319, 721, 11], "temperature": 0.0, "avg_logprob": -0.1355518830561005, "compression_ratio": 1.7607843137254902, "no_speech_prob": 6.59942816128023e-05}, {"id": 1047, "seek": 442500, "start": 4430.0, "end": 4434.0, "text": " but I think that, like, the lesson that I personally draw from something like this,", "tokens": [457, 286, 519, 300, 11, 411, 11, 264, 6898, 300, 286, 5665, 2642, 490, 746, 411, 341, 11], "temperature": 0.0, "avg_logprob": -0.1355518830561005, "compression_ratio": 1.7607843137254902, "no_speech_prob": 6.59942816128023e-05}, {"id": 1048, "seek": 442500, "start": 4434.0, "end": 4438.0, "text": " is that even if you found a, like, really revolutionary change,", "tokens": [307, 300, 754, 498, 291, 1352, 257, 11, 411, 11, 534, 22687, 1319, 11], "temperature": 0.0, "avg_logprob": -0.1355518830561005, "compression_ratio": 1.7607843137254902, "no_speech_prob": 6.59942816128023e-05}, {"id": 1049, "seek": 442500, "start": 4438.0, "end": 4441.0, "text": " that was, like, much better than transformers,", "tokens": [300, 390, 11, 411, 11, 709, 1101, 813, 4088, 433, 11], "temperature": 0.0, "avg_logprob": -0.1355518830561005, "compression_ratio": 1.7607843137254902, "no_speech_prob": 6.59942816128023e-05}, {"id": 1050, "seek": 442500, "start": 4441.0, "end": 4445.0, "text": " it might be kind of equivalent to making transformers 10 times bigger,", "tokens": [309, 1062, 312, 733, 295, 10344, 281, 1455, 4088, 433, 1266, 1413, 3801, 11], "temperature": 0.0, "avg_logprob": -0.1355518830561005, "compression_ratio": 1.7607843137254902, "no_speech_prob": 6.59942816128023e-05}, {"id": 1051, "seek": 442500, "start": 4445.0, "end": 4450.0, "text": " but I'm not sure if that would be as big of a deal as changing the loss.", "tokens": [457, 286, 478, 406, 988, 498, 300, 576, 312, 382, 955, 295, 257, 2028, 382, 4473, 264, 4470, 13], "temperature": 0.0, "avg_logprob": -0.1355518830561005, "compression_ratio": 1.7607843137254902, "no_speech_prob": 6.59942816128023e-05}, {"id": 1052, "seek": 442500, "start": 4450.0, "end": 4452.0, "text": " Changing what the objective is.", "tokens": [45773, 437, 264, 10024, 307, 13], "temperature": 0.0, "avg_logprob": -0.1355518830561005, "compression_ratio": 1.7607843137254902, "no_speech_prob": 6.59942816128023e-05}, {"id": 1053, "seek": 445200, "start": 4452.0, "end": 4455.0, "text": " But that's just my guess, I have no idea.", "tokens": [583, 300, 311, 445, 452, 2041, 11, 286, 362, 572, 1558, 13], "temperature": 0.0, "avg_logprob": -0.09400294324476942, "compression_ratio": 1.7457627118644068, "no_speech_prob": 4.457445538719185e-05}, {"id": 1054, "seek": 445200, "start": 4455.0, "end": 4459.0, "text": " And, of course, this paradigm, I mean, I think I was trying to be polite.", "tokens": [400, 11, 295, 1164, 11, 341, 24709, 11, 286, 914, 11, 286, 519, 286, 390, 1382, 281, 312, 25171, 13], "temperature": 0.0, "avg_logprob": -0.09400294324476942, "compression_ratio": 1.7457627118644068, "no_speech_prob": 4.457445538719185e-05}, {"id": 1055, "seek": 445200, "start": 4459.0, "end": 4463.0, "text": " I usually have, like, a picture of a grilled cheese here to emphasize,", "tokens": [286, 2673, 362, 11, 411, 11, 257, 3036, 295, 257, 25183, 5399, 510, 281, 16078, 11], "temperature": 0.0, "avg_logprob": -0.09400294324476942, "compression_ratio": 1.7457627118644068, "no_speech_prob": 4.457445538719185e-05}, {"id": 1056, "seek": 445200, "start": 4463.0, "end": 4466.0, "text": " like, sort of, how simple and sort of silly this is,", "tokens": [411, 11, 1333, 295, 11, 577, 2199, 293, 1333, 295, 11774, 341, 307, 11], "temperature": 0.0, "avg_logprob": -0.09400294324476942, "compression_ratio": 1.7457627118644068, "no_speech_prob": 4.457445538719185e-05}, {"id": 1057, "seek": 445200, "start": 4466.0, "end": 4470.0, "text": " rather than this sort of very sophisticated palette of spices.", "tokens": [2831, 813, 341, 1333, 295, 588, 16950, 15851, 295, 19608, 13], "temperature": 0.0, "avg_logprob": -0.09400294324476942, "compression_ratio": 1.7457627118644068, "no_speech_prob": 4.457445538719185e-05}, {"id": 1058, "seek": 445200, "start": 4470.0, "end": 4475.0, "text": " And, I mean, maybe someone will say, like, this isn't the right set of ingredients", "tokens": [400, 11, 286, 914, 11, 1310, 1580, 486, 584, 11, 411, 11, 341, 1943, 380, 264, 558, 992, 295, 6952], "temperature": 0.0, "avg_logprob": -0.09400294324476942, "compression_ratio": 1.7457627118644068, "no_speech_prob": 4.457445538719185e-05}, {"id": 1059, "seek": 445200, "start": 4475.0, "end": 4478.0, "text": " from which to think about things, and there's a different thing you should do,", "tokens": [490, 597, 281, 519, 466, 721, 11, 293, 456, 311, 257, 819, 551, 291, 820, 360, 11], "temperature": 0.0, "avg_logprob": -0.09400294324476942, "compression_ratio": 1.7457627118644068, "no_speech_prob": 4.457445538719185e-05}, {"id": 1060, "seek": 445200, "start": 4478.0, "end": 4480.0, "text": " and maybe that will make a big difference as well.", "tokens": [293, 1310, 300, 486, 652, 257, 955, 2649, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.09400294324476942, "compression_ratio": 1.7457627118644068, "no_speech_prob": 4.457445538719185e-05}, {"id": 1061, "seek": 448000, "start": 4480.0, "end": 4483.0, "text": " But I, that's sort of an unknown, unknown.", "tokens": [50364, 583, 286, 11, 300, 311, 1333, 295, 364, 9841, 11, 9841, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2963933308919271, "compression_ratio": 0.9767441860465116, "no_speech_prob": 0.0008955694502219558}], "language": "en"}