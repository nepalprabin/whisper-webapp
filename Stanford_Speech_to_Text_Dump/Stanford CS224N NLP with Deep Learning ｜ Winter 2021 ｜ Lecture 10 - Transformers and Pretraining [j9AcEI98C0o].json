{"text": " Hello, everybody. Welcome to CS224N lecture 10. This is going to be primarily on pre-training, but we will also discuss sub-word models a little bit and review transformers. Okay, so we have a lot of exciting things to get into today, but some reminders about the class. Assignment 5 is being released today. Assignment 4 was due a minute ago, so if you are done with that, congratulations. If not, I hope that the late days go well. Assignment 5 is on pre-training and transformers, so these lectures are going to be very useful to you for that and I just don't cover anything after these lectures. All right. So today, let's kind of take a little peek through what the outline will be. We haven't talked about sub-word modeling yet and sort of we should have. And so we're going to talk a little bit about sub-words. You saw these in assignment 4, all just, you know, as the data that we provided to you with your machine translation system, but we're going to talk a little bit about why they're so ubiquitous in NLP because they are used in pre-trained models. I mean, they're used in a number of different models, but when we discuss pre-training, it's important to know that sub-words are part of it. Then we'll sort of motivate, we'll go on another journey of motivation of motivating model pre-training from word embedding. So we've already seen pre-training in some sense in the very first lecture of this course because we pre-trained individual word embeddings that don't take into account their contexts on very large text corporate and saw that they were able to encode a lot of useful things about language. So after we do that motivation, we'll go through model pre-training three ways. And we're going to, you know, reference actually the lecture on Tuesday. So this is why we're going to review a little bit of the transformer stuff. We'll talk about model pre-training in decoters, like a transformer decoder that we saw last week in encoters, and then encoder decoters. And each of these three cases, we're going to talk a little bit about sort of what things you could be doing and then popular models that are in use across research and in industry. And we're going to talk a little bit about, you know, what do we think pre-training is teaching? This is going to be very brief. Actually, a lot of the interpretability and analysis lecture in two weeks is going to talk more about sort of the mystery and the scientific problem of figuring out what these models are learning about language through pre-training objectives, but we'll sort of get a peak. And then we'll talk about very large models and in context learning. So if you've heard of GPT-3, for example, we're going to just briefly touch on that here and I think we'll discuss more about it in the course later on as well. Okay, so we've got a lot to do. Let's jump right in. So word structure and sub-broad models. Let's think about sort of the assumptions we've been making in this course so far. When we give you an assignment, when we talk about training word to veck, for example, we made this assumption about a language's vocabulary. In particular, we've made this assumption that has a fixed vocabulary of something like tens of thousands, maybe a hundred thousand, I don't know, a number of... But some relatively large, it seems, number of words, and that seems sort of like pretty good so far, at least, and what we've done. And we build this vocabulary from the set that we train, say, word to veck on. And then here's a crucial thing, any novel word, any word that you did not see at training time, is sort of mapped to a single unctocon. There are other ways to handle this, but you sort of have to do something and a frequent method is to map them all to unct. So let's walk through what this sort of means in English. You learn embeddings, you map them, it all works. Then you have a variation on a word like tase, with a bunch of a's. And your model isn't smart enough to know that that sort of means like very tasty, maybe. And so it maps it to unct, because it's just a dictionary look-up mess. And then you have a typo like lern, and that maps to unct as well, potentially, if it wasn't in your training set, some people make typos, but not all of them will be seen at training time. And then you'll have novel items. So this could be the first time that you've ever seen US, the students, and 224N have seen the word transformer if I. But I get the feeling you sort of have a notion of what it's supposed to mean, like maybe add transformers to or turn into using transformers, or turn into a transformer or something like that. And this is also going to be mapped to unct, even though you've seen transformer and if I. And so somehow the conclusion we have to come to is that looking at words as just like the individual sequence of characters uniquely identifies that word, and that's sort of how we should parameterize things is just wrong. And so not only is this true in English, but in many languages, this finite vocabulary assumption makes even less sense. So already it doesn't make sense in English, but English is, it's not the worst for English. So morphology is the study of the structure of words. And English is known to have pretty simple morphology in kind of specific ways. And when languages have complex morphology, it means you have longer words, more complex words that get modified more, and each one of them occurs less frequently. That should sound like a problem, right? If a word occurs less frequently, it will be less likely to show up in your training set. And maybe it'll show up in your test set, never in your training set. Now it's mapped to unct, and you don't know what to do. So an example, Swahili verbs can have hundreds of conjugations. So each conjugation encodes important information about the sentence that in English might be represented through, say, more words. And Swahili it's mapped onto the verb as prefixes and suffixes, and the like, this is called inflectional morphology. And so you can have hundreds of conjugations. I've just sort of pasted this wick-shenary block just to give you a small sample of just the huge number of conjugations there are. And so trying to memorize independently a meaning of each one of these words is just not the right answer. So this is going to be a very brief overview. And so what we're going to do is take one, let's say, class of algorithms for sub-word modeling that have been kind of developed to try to take a middle ground between two options. One option is saying everything is just like individual words. Either I know the word and I saw it at training time, or I don't know the word, and it's like unct. And then sort of another extreme option is to say it's just characters. Right? So like I get a sequence of characters, and then my neural network on top of my sequence of characters has to learn everything, has to learn how to combine words and stuff. So sub-word models in general just means looking at the sort of internal structure of words somehow, looking below the word level. But this group of models is going to try to meet a middle ground. So byte parent coding. What we're going to do is we're going to learn a vocabulary from a training data set again. So now we have a training data set. Instead of just saying, oh, everything that was split by my heuristic word splitter, like spaces in English, for example, is going to be a word in my vocabulary, we're going to learn the vocabulary using a greedy algorithm in this case. So here's what we're going to do. We start with the vocabulary containing only characters. So that's our extreme, right? So at the very least, if you've seen all the characters, then you know that you can never have an unque, right? Because you see a word, you've never seen it before, you just split it into its characters, and then you try to see, you know, deal with it that way. And then also an end of word symbol. And then we'll iterate over this algorithm. We'll say, use the corpus of text, find common adjacent letters. So maybe A and B are very frequently adjacent. And the pair of them together as a single sub word into your vocabulary. Now replace instances of that character pair with a new sub word repeat until you're desired vocabulary size. So maybe you start with a small character vocabulary, and then you end up with that same small character vocabulary plus a bunch of sort of entire words or parts of words. So notice how Apple, an entire word, looks like Apple. But then app, maybe this is sort of the first part, the first sub word of application, or up. Yeah. And then Lee, I guess I should have not put the hash there, but you know, maybe you learned Lee as like the end of a word, for example. And so what you end up with is, you know, a vocabulary where common things you get to map to themselves and then rare sequences of characters. You kind of split as little as possible. And it doesn't always end up so nicely that you learn like morphologically relevant suffixes like Lee. But you can, you know, try to split things somewhat reasonably. And if you have enough data, the sub word vocabulary you learn tends to be okay. So this is originally used in machine translation. And now a similar method, word piece, which we won't go over in this lecture is used in pre-trained models. But you know, the idea is effectively the same. And you end up with vocabularies that look a lot like this. So if we go back to our, if we go back to our examples of where, you know, word level NLP was failing us, then you have hat mapping to hat. Okay, that's good. You have hat mapping to hat because that was a common enough sequence of characters that it was actually incorporated into our sub word vocabulary. Right? And then you have learned mapping to learn. So common words good. And that means that the model, the neural network that you're going to process this text with does not need to, say, combine the letters of learn and hat in order to try to like derive the meaning of these words from the letters, because you can imagine that might be difficult. But then when you get a word that you have not seen before, you are able to decompose it. And so if you've seen tasty with varying numbers of A's at, at training time, you know, maybe you actually get some of the same sub words or similar sub words that you're splitting it into at evaluation time. So we never saw tasty enough to like, you know, however many A's in order to add it into a sub word vocabulary. But we're still able to split it into things. And then the neural network that runs on top of these sub word embeddings could be able to sort of induce that, oh, yeah, this is one of those things where people like, you know, chain letters together, chain vowels together in English for emphasis. So misspellings still pretty much mess you up. So now learn with this misspelling might be mapped to two sub words. But if you saw misspellings like this frequently enough, maybe you could learn sort of to handle it. It still messes up the model though. And, but at the very least, it's not just an umk, right? It seems clearly better than that. And then transformer, if I, maybe in the best, this is sort of optimistic, but maybe in the best case, right, you were able to say, ah, yes, this is transformer. And if I, again, the sub words that you learn don't actually tend to be this well morphologically motivated, I think. So if I is like a clear, like suffix in English that has a very common and replicable meaning when you apply it to nouns, that's derivational morphology. But you know, you're able to sort of compose the word of the meaning of transformer if I possibly from its two sub word constituents. And so when we talk about words being input to transformer models, pre-trained transformer models, throughout the entirety of this lecture, we will be talking about sub words. So I might say word, and what I mean is, you know, possibly a full word, also possibly a sub word. Okay, so when we say a sequence of words, the transformer, the pre-trained transformer has no idea, sort of whether it's dealing with words or sub words, when it's doing itself attention operations. And so this can be a problem. You can imagine if you have really weird sequences of characters, you can actually have an individual single word mapped to as many sub words as it has characters. That can be a problem because suddenly, you know, you have a ten-word sentence, but one of the words is mapped to, you know, twenty sub words. Now you have a thirty-word sentence, where twenty of the thirty words are just one real word. So keep this in mind. But, you know, I think it's important for sort of this open vocabulary assumption, it's important in English, and it's even more important in many other languages. And the actual algorithm, and you can go into the actual algorithms that are done for this, byte per encoding is sort of my favorite for going over briefly, word piece you can also take a look at. Okay. Any questions on sub words? I guess John, let me look after what does the hashtag mean? Oh, great, great point. So this means that you should be combining this sub word, so this sub word is not the end of a word. TAA, hash hash, is sort of telling the model. So if I had TAA with no hashes, that's a separate sub word. That means there's an entire word that is ta, or at the very least it's not the end of the word. See how here? I don't have the hashes at the end. It's because this is indicating that this is at the end of the word. Different sub word schemes differ on whether you should put something at the beginning of the word, if it does begin a word, or if you should put something at the end of the word, if it doesn't end the word. So when the tokenizer is running over your data, so you've got something that's tokenizing this sentence in the worst case. In the worst case, it says, in, that's a whole word, give it just the word in, no hashes, that's a whole word, give it just the word the, no hashes, and then maybe over here at sub words. We've got this weird word sub words, and it splits it into sub and words. And so sub, it's going to give it the sub word with sub hash hash to indicate that it's part of this larger word, sub words, as opposed to the word sub, like submarine, which would be different. Yeah, that's a great question. Okay, great. So that was our note on sub word modeling, and you can, you know, sub words are important, for example, in, you know, a lot of translation applications, that's why we gave you sub words on the machine translation assignment. Now let's talk about model pre-training and word embeddings. So I love, I love being able to go to this slide. So, so we saw this quote at the beginning of the class, you shall know a word by the company it keeps, and this was sort of one of the things that we used to summarize distributional semantics. This idea that word to veck was sort of well motivated in some way, because the meaning of a word can be thought of as being derived from the kind of co-occurrent statistics of words that co-occur around it, and that was just fascinatingly effective, I think. But there's this other quote actually from the same person. So we have J.R. Firth, 1935, compared to our quote before from 1957, and the second quote says, the complete meaning of a word is always contextual, and no study of meaning apart from a complete context can be taken seriously. Now again, these are just things that we can sort of think about and chew on, but it comes to mind, right, when you, when you embed words with word to veck, one of the issues is that you don't actually look at its neighbors as you're giving it an embedding. So if I have the sentence I record the record, you know, the two instances of REC, ORD, mean different things, but they're given the same word to veck embedding, right, because in word to veck you take the string, you map it to, oh, I've seen the word record before, you get that sort of vector from your learned matrix, and you give it the same thing in both cases. And so what we're going to be doing today is actually not conceptually all that different from training word to veck. Word to veck training you can think of as pre-training just a very simple model that only assigns an individual vector to each unique word type, each unique element in your vocabulary. Today we'll be going a lot farther than that, but the idea is very similar. So back in, you know, 2017, we would start with pre-trained word embeddings, and again, remember no context there, so you give a word and embedding independent of the context that it shows up in. And then you learn how to incorporate the context. It's not like our NLP models never used context, right? Instead, you would learn to incorporate the context using your LSTM, or it's later in 2017, you know, your transformer. And you would learn to incorporate context while training on the task. So you have some supervision. Maybe it's machine translation supervision, maybe sentiment, maybe question answering. And you would learn how to incorporate context in your LSTM or otherwise through the signal of the training instead of say through the word to veck signal. And so, you know, sort of pictographically, you have these word embeddings here, so the red are sort of your word to veck embeddings, and those are pre-trained. Those take up some of the parameters of your network. And then you've got your contextualization. Now this looks like an LSTM, but it could be whatever. So this maybe bidirectional encoder thing here is not pre-trained. And now that's a lot of parameters that are not pre-trained. And then maybe you have some sort of readout function at the end, right, to predict whatever thing you're trying to predict. Again, maybe it's sentiment, maybe you're doing, I don't know, topic labeling, whatever you want to do. This is sort of the paradigm. Like you set some sort of architecture and you only pre-trained the word embeddings. And so this isn't actually the conceptually, necessarily the biggest problem, because, you know, we like to think in deep learning stuff that we have a lot of training data for our objectives. I mean, one of the things that we motivated, you know, big, deep neural networks for is that they can take a lot of data and they can learn patterns from it. But it does put the onus on our downstream data to be sort of sufficient to teach the contextual aspects of language. So you can imagine if you only have a little bit of, you know, labeled data for fine tuning, you're putting a pretty big role on that data to say, hey, maybe here's some pre-trained embeddings, but like how you handle like sentences and how they compose and all that stuff, that's up to you. So if you don't have a lot of labeled data for your downstream task, you're asking it to do a lot with, you know, a large number of parameters that have been initialized randomly. Okay, so like a small portion of the parameters have been pre-trained. Okay, so where we're going is pre-training whole models. I mean, conceptually, you know, we're pretty close to there. So nowadays, almost all parameters in your neural network and let's say a lot of research settings and increasingly in industry are initialized via pre-training, just like word to vac parameters were initialized and pre-training methods in general hide parts of the input from the model itself and then train the model to reconstruct those parts. How does this connect to word to vac? In word to vac, you know, people don't usually make this connection, but it's the following. You have an individual word and it knows itself, right, because you have the embedding for the center word, right, from assignment two. You have the embedding for the center word and knows itself and you've masked out all of its neighbors. You've hidden all of its neighbors from it, right, every all of its window neighbors, you've hidden from it. You ask the center word to predict its neighbors, right? And so this is, this falls under the category of pre-training. All of these methods look similar. You hide parts of the input from the model and train the model to reconstruct those parts. The differences with full model pre-training is that you don't give the model just the individual word and have it learn an embedding of that word. You give it much more of the sequence and have it predict, you know, held out parts of the sequence. And we'll get into the details there. But, you know, the takeaway is that everything here is pre-trained jointly, possibly with the exception of the very last layer that predicts the label. Okay, and this has just been exceptionally effective at building representations of language that just map similar things in language, similar representations in these encoders, just like how word-to-vec map similar words to similar vectors. It's been exceptionally effective at making parameter initializations where you start with these parameters that have been pre-trained and then you fine-tune them on your label data. And then third, they have an exceptionally effective at defining probability distributions over language, like in language modeling, that are actually really useful to sample from in certain cases. So these are three ways in which we interact with pre-trained models. We use their representations just to compute similarities. We use them for parameter initializations. And we actually just use them as probability distributions, sort of how we train to them. Okay. So let's get into some technical parts here. I sort of want to think broad thoughts about what we could do with pre-training and what kind of things we could expect to potentially learn from this general method of hide part of the input and then see other parts of the input and then try to predict the parts that you hid. Okay. So Stanford University is located in Blank California. If we gave a model, everything that was not blanked out here and asked to predict the middle, the loss function would train the model to predict Palo Alto here, I expect. Okay. So this is an instance of something that you could imagine being a pre-training objective. You take in a sentence, you remove part of it and you say recreate the part that I removed. And in this case, if I just gave a bunch of examples that looked like this, it might learn sort of trivia thing here. Okay. Here's another one. I put blank fork down on the table. This one is under specified. So this could be the fork, my fork, his fork, her fork, some fork, yeah, a fork. So this is, you know, specifying the kinds of syntactic categories of things that can sort of appear in this context. So this is another thing that you might be able to learn from such an objective. So you have the woman walked across the street, checking for traffic over blank shoulder. One of the things that could go over here is her. That's a co-reference statement. So you could learn sort of connections between entities in a text where one word woman can also co-refer to the same entity in the world as this word, this pronoun her. Here you could think about, you know, I went to the ocean to see the fish, turtles, seals, and blank. Here I don't think there's a single correct answer as to what we could see going into that blank. But a model could learn a distribution of the kinds of things that people might be talking about when they, one, go to the ocean and two, are excited to see marine life. Right? So this is sort of a semantic category, a lexical semantic category of things that might sort of be in the same set of interest as fish, turtles, and seals in the context of I went to the ocean. Okay? So, and, you know, man, I expect that there would be examples of this in a large corpus of text. Maybe it may be a book. Okay. Here's another example. Overall, the value I got from the two hours watching it was the sum total of the popcorn and the drink. The movie was blank. Right? And this is when I'd sort of like look out into the audience and say, was the movie better good, but the movie was bad. It's my prediction here. Right? And so this is teaching you something about sentiment, about how people express sentiment in language. And so this is, even, it looks like a task itself, like do sentiment analysis is sort of what you need to do in order to figure out whether the movie was bad or good, or maybe maybe the word is neither bad or good. The movie was over or something like that. But like, if you had to choose between is bad or good more likely, right? You sort of had to figure out the sentiment of the text. Now, that's really fascinating. Okay. Here's another one. Iro went into the kitchen to make some tea. Standing next to Iro, Zuko pondered his destiny. Zuko left the blank. Okay. So this is a little easy because we really only show one place. I guess we have another now in destiny. But this is sort of talking reasoning about spatial location and the movement of sort of agents in an imagined world. We could imagine text that has lines like this. Person went into the place and was next to so and so who left and did that and sort of you have these like relationships. So here, Zuko left the kitchen. It's the most likely thing that I think would go here. And it sort of indicates that in order for a model to learn to perform this fill in the missing part task, it might need to, in general, figure out sort of where things are and whether statements mean or imply that locality. So standing next to Iro went into the kitchen. Now Iro is in the kitchen and then standing next to Iro means Zuko is now in the kitchen. And then Zuko now leaves where? Well, he was in the kitchen before. So this is sort of a very basic sense of reasoning. Now this one. Here's a sentence. I was thinking about the sequence that goes 1, 1, 2, 3, 5, 8, 13, 21, blank. So I don't know. I can imagine people writing stuff. So this is the Fibonacci sequence. And sort of you know you use some of these two to get the next one, some of these two to get the next one, some of these two. And so you have this running sum. It's a famous sequence. It shows up in a lot of text on the internet. And in general you have to learn the algorithm or just the formula, I guess, that defines the Fibonacci sequence in order to keep going. Do models in this in practice? Wait and find out. But you would have to learn it in order to get the sequence to keep going and going and going. OK, so we're going to get into specific pre-trained models, specific methods of pre-training now. So I'm going to go over a brief review of transformer encoders, decoders, and encoder decoders. Because we're going to get into the sort of technical bits now. So before I do that, I'm going to pause. Are there any questions? Yeah, there's an interesting question asked about, do these co-opening our model on our input training data and the link to training? And we need to also add some questions in the light of the huge between models that we think nowadays. Sorry, the first part of that question, was it, are we overfitting our models to what? Yes, so the risk of almost getting our model on our input training data when they're doing training? Got it. Yeah, so that's a good point. So we're using very large models. And we might imagine that there's a risk of overfitting. And in practice, yeah, it's actually one of the more crucial things to do to make pre-training work. So that turns out that you need to have a lot, a lot of data, like a lot of data. And in fact, we'll show results later on where people built a pre-trained model, pre-trained it on a lot of data. And then like six months later, someone else came along and was like, hey, if you pre-trained it on 10 months later and changed almost nothing else, it would have gone even better. Now was it overfitting? I mean, you can sort of like hold out some text during pre-training, right, and sort of evaluate the perplexity, right, the language modeling performance on that held out text. And it tends to be the case that actually these models are underfitting, right, that we need even larger and larger models to express the complex interactions that allow us to fit these datasets better. And so we'll talk about that when we talk about BERT. And one of the really interesting results is that BERT is underfit, not overfit, but in principle, yes, it's a problem to, this potentially a problem to overfit. But we end up having a ton of text in English at least, although not in every language. And so, yeah, it's important to scale them, but currently our models don't seem overfit to the pre-training text. Okay. Any other questions? All right. So we saw this figure before, right here. We saw this figure of a transformer encoder to coder from this paper attention is all you need. And so we have a couple of things. We're not going to go over the form of attention again today because we have a lot to go over, but I'm happy to chat about it more on Ed. But so in our encoder, we have some input sequence. Remember, this is a sequence of sub words now. Each sub word gets a word embedding. And each index in the transformer gets a position embedding. Now remember that we have a finite length that our sequence can possibly be like 512. That's tokens. That was that capital T from last lecture. So you have some finite length. So you have one embedding of a position for every index for all 512 indices. And then you have all your word embeddings. And then the transformer encoder, right, was this combination of sort of sub-modules that we walked through line by line on Tuesday, right. Multi-headed attention was sort of the core building block. And then we had residual and layer norm, right, to help with passing gradients and to help make training go better and faster. We had that feed forward layer to, yeah, process sort of the result of the multi-headed attention, another residual and layer norm, and then pass to an identical transformer encoder block here. And these would be stacked. We'll see a number of different configurations here, but I think, you know, 612 of these sort of stacked together. Okay. So that's a transformer encoder. And we're actually going to see whole models today that are just transformer encoders. Okay. So when we talked about machine translation, when we talked about the transformer itself, the transformer encoder decoder, we talked about this whole thing. But you could actually just have this left column, and you could actually just have this right column as well. Although the right column changes a little bit if you just have it. So remember, the right column, we had this masked multi-head self-attention, right, so where you can't look at the future. And someone asked actually about how we decode from transformers, given that you have this sort of big chunking operation. It's a great question. I won't be able to get into it in detail today, but you have to run it once during the decoding process for every time that you decode to sort of predict the next word. I'll write out something on Ed for this. So in the masked multi-head self-attention, you're not allowed to look at the future so that you sort of have this well-defined objective of trying to do language modeling. Then we have residual and layer norm. The multi-head cross-attention, remember, goes back to the last layer of the transformer encoder, or the last transformer encoder block. And then more residual and layer norm, another feed-forward layer, more residual and layer norm. Now, if we don't have an encoder here, then we get rid of the cross-attention and residual and layer norm here. So if we didn't have this stack of encoders, the decoders get simpler because you don't have to attend to them. But then again, you also have these word embeddings at the bottom and position representations for the output sequence. Okay, so that's been review. Let's talk about pre-training through language modeling. So we've actually talked maybe a little bit about this before, and we've seen language modeling in the context of maybe just wanting to do it our priori. So language models were useful, for example, in automatic speech recognition systems. They were useful in statistical machine translation systems. So let's recall the language modeling task. You can say it's defined as modeling the probability of a word at a given index t, of any word at any given index, given all the words before it. And this probability distribution is a distribution of words given their past contexts. And so this is just saying, for any prefix here, IRO goes to make. I want a probability of whatever the next word should be. So the observed next word is tasty, but maybe there's goes to make t, goes to make hot water, etc. You can have a distribution over what the next word should be in this decoder. And remember that because of the masked self-attention, make can look back to the word two, or goes, or IRO, but it can't look forward to tasty. So there's a lot of data for this, right? You just have text. And like voila, you have language modeling data. It's free. No. Once you have the text, it's freely available. You don't need to label it. And in English, you have a lot of it, right? This is not true of every language by any means, but in English, you have a lot of pre-training data. And so the simple thing about pre-training is, well, what we're going to do is we're going to train a neural network to do language modeling on a large amount of text, and we'll just save the parameters of our train network to disk. So conceptually, it's not actually different from the things that we've done before. It's just sort of the intent, right? We're training these parameters to start using them for something else later down the line. But the language modeling itself doesn't change. The decoder here doesn't change, right? It's a transformer in tree-trained models in a modern, because this is sort of a newly popular concept. Although back in 2015 was sort of when this, I think, was first effectively tried out and got some interesting results. But this could be anything here. Today, it's most going to be transformers in the models that we actually observe. Okay. So once you have your pre-trained network, what's the sort of default thing you do to take to use it? Right? And if you take anything away from this lecture in terms of just like engineering practices that will be broadly useful to you as you go off and build things and study things, maybe as a machine learning engineer or a computational social scientist, et cetera, what people tend to do is you pre-traine your network on just a lot of data, lots of text, learn very general things. And then you adapt the network to whatever you wanted to do. So we had a bunch of pre-training data, and then maybe this is a movie review that we're taking as input here, and we just apply the decoder that we sort of pre-trained, start the parameters there, and then fine tune it on whatever we were sort of wanting to do. Maybe this is a sentiment analysis task. So we run the whole sequence through the decoder, get a hidden state at the end at the very last thing, and then we predict maybe plus or minus sentiment. And this is sort of adapting the pre-trained network to the task. Because pre-trained fine-tune paradigm is wildly successful, and you should really try it whenever you're doing any NLP task nowadays effectively. Because this tends to be what some variant of this tends to be what works best. Okay, so we've got a technical note now. So if you don't like to think about optimization or gradient descent, maybe take a pass on this slide, but I encourage you to just think for a second about why should this help? Training neural nets, we're using gradient descent to try to find some global minimum of this loss function. And we're sort of doing this in two steps. The first step is we get some parameters theta hat by approximating min over our, sorry, theta is the parameters of the neural network. So all of the KQV vectors in our transformer, the word embeddings, the position embeddings, it's just all of the parameters of our neural network. And so we're doing min over all the parameters of our theta, we're trying to approximate min over the parameters of our neural network of our pre-training loss, which here was language modeling of our parameters. And this is, we just get this sort of estimate of some parameters theta hat. And then we fine tune by approximating this min over theta of the fine tune loss, maybe that's sentiment, right? Starting at theta hat. So we initialize our gradient descent at theta hat, and then we just sort of let it do what it wants. And it's just like, it just works. And in part, it has to be because something about where we start is so important, not just in terms of sort of gradient flow, although that is a big part of it. But also, it seems like, you know, stochastic gradient descent sticks relatively close to that pre-training initialization during fine tuning. This is something that we seem to observe in practice, right, that somehow the locality of stochastic gradient descent, finding local minima that are close to this theta hat, that was good for such a general problem of language modeling, it seems like, yeah, the local minima of the fine tuning loss, because we don't find, or yeah, the local minima of the fine tuning loss tend to generalize well when they're near to this theta hat that we pre-trained. And this is sort of a mystery that we're still trying to figure out more about. And then also, yeah, maybe the gradients, right, the gradients of the fine tuning loss near theta propagate nicely, so our network training goes really well as well. Okay, so this is something to chew on, but in practice, it works. I think it's just still fascinating that it works. Okay, so we talked about mainly the transformer encoder to coder, and in fact, right, I said that we could have just sort of the left-hand side encoders, you know, that to be pre-trained or just decoders to be pre-trained or encoder decoders. And there are actually really popular sort of famous models in each of these three categories. The kinds of pre-training you can do, and the kinds of applications or uses of those pre-trained models that are most natural actually depend strongly on whether you choose to pre-traine and encoder a decoder or an encoder decoder. So I think it's useful as we go through some of these popular sort of model names that you need to know and what they sort of, what their innovations were to actually split it up into these categories. So we've all, so here's the thing. We're going to go through these three, and they all have sort of benefits and in some sense, drawbacks. So the decoders, right, really what we're talking about here mainly is language models, and we've seen this so far, we've talked about pre-trained decoders, and these are nice to generate from. So you can just sample from your pre-trained language model and get things that look like the text that you were pre-training on. But one problem is that you can't condition on future words, right? So we mentioned in our modeling with LSTMs that just like, instead, if you could, when you can do it, we said that having a bi-directional LSTM was actually just way more useful than having a one-directional LSTM. Well, it's sort of true for transformers as well. So if you can see how the arrows are pointing here, the arrows are pointing up into the, you know, to the right. So this word is sort of looking back at its past history, but, you know, this word can't see, can't contextualize with the future. Whereas in the encoder block here in blue, just below it, you sort of have all pairs of interactions. And so, you know, when you're building your representations, it can actually be super useful to know what the future words are. So that's what encoders get you, right? You get bi-directional context. So you can condition on the future, maybe that helps you build up better representations of language. But the question that we'll actually go through here is, well, how do you pre-train them? You can't pre-train them as language models because you have access to the future. So if you try to do that, the loss will just immediately be zero because you can just see what the future is. That's not useful. And then we'll talk about pre-trained encoder decoders, which like maybe the best of both worlds, but also maybe unclear what's the best way to pre-train them. They definitely have benefits for both. So let's get into some general top, like a more, yeah, let's get into the decoders first, we'll go through all three. Okay. When we're pre-training a language model, right, we're pre-training it on this objective, we're trying to make it approximate this probability of a word given all of its previous words. What we end up doing, and I showed this sort of pictographically, but I'll add some math, right, we get a hidden state, h1 to ht for each of the words in the input w1 to wt. And I remember words again, mean sub words here. Okay. And we're fine tuning this, right, we can take the representation, this should be ht, a, ht plus b. And then the picture here is, right, here's ht. It's the very last encoder state. And now this has sort of the, it's seen all of its history, right, and so you can apply a linear layer here, maybe multiplying it by some parameters a and b that were not pre-trained, and then you're predicting sentiment maybe, you know, plus or minus sentiment, perhaps. And so, you know, look at the red and the gray, so most of the parameters of my neural network have now been pre-trained, the very last layer that's learning, the sentiment, say, decision, has not been pre-trained. So those have been randomly initialized. And when you, when you take the loss of the sentiment loss, right, you train not just the linear layer here, but you actually back propagate the gradients all the way through the entire pre-trained network and fine tune all of those parameters, right? So it's not like you're just training this, fine tuning time, this linear layer, you're training the whole network as a function of this fine tuning loss. And you know, maybe it's bad that like the linear layer wasn't pre-trained. In the grand scheme of things, it's not that many parameters also. So this is you, so this is just one way to interact with pre-trained models, right? And so what I want you to take away from this is that there was a contract that we had with the original model, right? The contract was that it was defining probability distributions. But when we're fine tuning, when we're interacting with the pre-trained model, what we also have are just like the trained weights and the network architecture. We don't need to use it as a language model, we don't need to use it as a probability distribution. When we're actually fine tuning it, we're really just using it for its initialization of its parameters and saying, oh, this is just a transformer decoder that was pre-trained by, oh, and it happens to be really great in that when you find tuna on some sentiment data, it does a really good job. Okay, but there's a second way to interact with pre-trained decoders, which is in some sense even more natural. It actually is closer to the contract that we started with. So we don't have to just ignore the fact that it was a probability distribution entirely, we can make use of it while still fine tuning it. So here's what we're going to do. So we can use them as a generator at fine tuning time. By generator, I mean, it's going to define this distribution of words given their context. And then we'll actually just fine tune that probability distribution. So in a task like some kind of turn-based dialogue, we might encode the dialogue history as your past context. So you have a dialogue history of some things that people are saying back and forth to each other, you encode it as words, and you try to predict the next words in the dialogue. Right, and maybe you're pre-training objective, you looked at very general purpose text from, I don't know, Wikipedia or books or something, and you're fine tuning it as a language model, but you're fine tuning it as a language model on this sort of domain-specific distribution of text like dialogue or maybe summarization where you paste in the whole document and then say a specific word and then the summary and say predict the summary. And so what this looks like is, again, at fine tuning time here, you have your h1 to ht is equal to the decoder of the words, and then you have this distribution that you're fine tuning of wt is a h is the type again, ht minus 1 plus b. So now every time I have this, I'm predicting these words from word 1, I predict word 2, we're 2, I predict word 3, etc., right, the actual last layer of the network unlike before, the last layer of the network has been pre-trained, but I'm still fine tuning the whole thing. Right, so a and b here are mapping to sort of a probability distribution over my vocabulary or the logits of a probability distribution, and I guess get this sort of like tweak them now, in order to have the distribution that I'm going to use, reflect the thing like dialogue that I wanted to reflect. Okay, so those are two ways of interacting with a pre-trained decoder. Now here's an example of what is ended up being the first, that be a line of wildly successful or at least talked about pre-trained decoders. So the generative pre-trained decoder, or GPC, was a huge success in some sense, or at least it got a lot of buzz, so it's a transformer decoder, no encoder, with 12 layers, I'm giving you the details so you can start to get a feeling for how the size of things changes. Over the years, as we'll continue to progress here, had each of our, each of the hidden states was dimensionality, 70, had 768, so if you remember back to last lecture, we had a term D, which was our dimensionality, so D is 768, and then an interesting statement that you should keep in mind for the engineering-minded folks is that the actual feed-forward layers, right, you've got a hidden layer in the feed-forward layer, and this was actually very large, so you had these sort of like position-wise feed-forward layers, right, and the feed-forward layer would take the 768-dimensional vector, sort of like project it to 3,000-dimensional space through the sort of non-linearity, and then project it back to 768. This ends up being because you can squash a lot more parameters in, for not too much more compute in this way, but that's curious. Okay, and then, byte-parent coding, it's actually, was this one byte-parent coding? Well, it was a sub-word vocabulary with 40,000 merges, so 40,000 merges, so that's not the size of the vocabulary because you started with a bunch of characters, and I don't remember how many characters they started with, but so it's a relatively small vocabulary you can see, right? And compared to, if you tried to say, have every word, have a unique representation, now it's going to be trained on books, corporates, it's got 7,000 unique books, and it contains long spans of contiguous texts, so you have, instead of, say, training it on individual sentences, just small short sentences, right? The model is able to learn long distance dependencies because you haven't split, like, a book into random sentences and shuffled them all around. You've sort of kept it contiguous, so we can have that sort of consistency. And then, a little treat here, yeah, so GPC never showed up in the original paper, or the original blog post, like as an acronym, and it could actually sort of refer to, like, generative pre-training, sort of what, like, the title of the paper would suggest, or generative pre-trained transformer. And I sort of decided to say generative pre-trained transformer because this seemed like way too general. So GPC. Okay, so they pre-trained this huge language model transformer, this huge transformer decoder, just on 7,000 books. And they fine-tuned it on a number of different tasks, and I want to talk a little bit about the details about how they fine-tuned it. And so they fine-tuned it on one particular task, or family tasks, called natural language inference. So in natural language inference, we're labeling pairs of sentences as entailing or contradictory to each other in neutral. So you have a premise, and you hold the premise as sort of true, the man is in the doorway. And you have a hypothesis, the person is near the door. If this person is referring to that man, then, you know, it's sort of like, oh, yeah, so this is sort of entailed because there's a person, because the man is a person, and they're in the doorway, then they are near the door. So you have this sort of logical reasoning that you're doing, or you're supposed to be able to be doing, and you're labeling these sentences. So it's a labeled task. You've got sort of an input that's cut into two parts, and then one of three outputs. Okay, so the GPT paper evaluates on this task. But what they've got is a transformer decoder. So what do they do? This is sort of one of the earlier examples of, you know, taking, instead of changing your neural network architecture to adapt to the kind of task you're doing, you're going to just format the task as like a bunch of tokens and not change your architecture. Because the pre-training was so useful, it's probably better to keep the architecture fixed, pre-training it, and then change the task specification to sort of fit the pre-trained architecture. So what they did, right, they put this token start, this is a special token, the man is in the doorway, some delimiter token, right. So this is just a linear sequence of tokens that we're giving as one big prefix to GPT. And then the person is near the door, and then some extra token here, right, extract. And then, you know, the linear classifier that we talked about, and sort of the first way to interact with models, with decoder models, it's applied to the representation of the extract token, right. So you have the last hidden state on top of extract, and then you fine tune the whole network to predict these labels, right. And so this sort of input formatting is increasingly, increasingly used to keep the model architecture the same and allow for a variety of different problems to be solved with it. Okay, and so did it work? Unnatural language inference, the answer is yes. So there's a number of different numbers here. I wouldn't worry too much about it. The fine tune transformer language model is sort of what you should pay attention to. There's a lot of effort that went into the other models, right. And so this is the story of pre-training. People put a lot of effort into models that do various sort of careful things. And then you take a single transformer and you say, I'm going to pre-training it on a ton of text and not worry too much about anything else and just fine tune it, and you end up doing super, super well. Sometimes not too much better in the GPT case than sort of the best known state of the art methods, but usually a little bit better. And again, the amount of effort, the amount of tasks, specific effort that you have to put into it, it's very low. Okay, and so what about the other way of interacting with decoters, right. So we had, we said that we can interact with decoters just by sampling from them, just by saying, well, there are probability distributions. So we can use them in their capacities as language models. And so GPT 2, this is just really just a bigger GPT, and we're too much about it, with larger hidden units, more layers. When it was trained on more data, it was shown to produce sort of relatively convincing samples of natural language. So this is something that went around Twitter a lot, right. So you have this sort of contrived example that probably didn't show up in the training data that has a scientist discovering a herd of unicorns. And then they sort of sample from a, almost the distribution of the model. They sort of give the model some extra credit here. They do something called truncating the distribution of the language models, sort of cut out noise at GPT 2. So it's not exactly a perfect sample, but more or less GPT 2 generated this. And so you have the scientist discovering unicorns, and then, you know, you have this consistency, okay, there's the scientist. You know, you have them giving you the name. You have, you refer back to this, well, yeah, you refer back to the scientist's name. You sort of have these like topic consistency things. Also the syntax is really good. It looks, you know, vaguely like English. And so this is sort of continued to be a trend. As we get larger and larger language models, we actually sample from them, even when we give them prompts that look sort of odd, and they seem to be increasingly convincing. Okay. So pre-training encoders, okay. Pre-training encoders. So let's take another second because I need some more water here. If there's another question, let me know. All right. So the benefit of encoders that we talked about was that they get this bidirectional context. So you can, while you're building representations of your sentence, of your parts of sentences, you can look to the future and that can help you build a better representation of the word that you're looking at right now. But the big problem is that we can't do language modeling now. So we've pretty much only said, we like, we've relied on this task that we already knew about language modeling to do our pre-training. But now we want to pre-training coders. And so we can't, we can't use it. So what are we going to do? Here's the solution that was come up with a paper that introduced the language model of the model called Bert. It's called masked language modeling. So here's the idea. We get the sentence and then we just take a fraction of the words and we replace them with a sort of a mask token. A token that's, that means you don't know what this is right now. And then you predict these words. Some details we'll get into in the next slide. But so here's what it looks like. We have the sentence, I mask to the mask. We get some hidden states for all of them, right? So we haven't changed the transformer encoder at all. We've just said, okay, here's like this sequence. You get to see everything, right? Look at all the arrows going everywhere. But then, right, we have this prediction layer that we're, that we're, that we're pre-training, right? And we're using it. We only have loss on the words where we had masks here. So I had this masked and then I have to predict that it was went that went here and store that went here. And now this is a lot like language modeling you might say. But now you don't need to have this sort of left to right decomposition. You're saying, I'm going to remove some of the words and you have to predict what they are. This is called masked language modeling. And it's been very, very, very effective with a quick caveat. It gets a little more complicated. So, so what did they actually do? They, they proposed masked language modeling. And they released the weights of this, of this pre-trained transformer. So the little bit more complexity to get masked language modeling to work. So you are going to take a random 15% of the sub word tokens. That was, that was true. But you're not always going to replace them with mask. You can think of it like, if the model sees a mask token, it gets a guarantee that it needs to predict something. And if the model doesn't see a mask token, it gets a guarantee that it doesn't need to predict anything. So why should it bother building strong representations of the words that aren't masked? And I want my model to build strong representations of everything. So we're going to add some sort of uncertainty to the model. So what we're going to do is, for those 15% of tokens, 80% of the time, we're going to replace it with a mask. That was our original idea of mask language modeling. Then 10% of the time, we're actually going to replace the word with just a random token. Just a random vocabulary item can be anything. And then the other 10% of the time, we're going to leave the word unchanged. So now, it sees a word. It could be a random token, or it could be unchanged. And if I see a mask, I know I need to predict it. So what these two things do here is say, you have to sort of be doing this, you have to be on your toes for every word in your representation. So here, I pizza to the mask. And it turns out, and the model didn't know this, but it's getting three lost terms for this sentence. It only has one mask, but it's going to be penalized for predicting three different things. And it needs to predict that this word is actually went. So I replaced this one. It needs to predict that this word is two, is in fact the word two. And then it needs to predict that this word is in fact store. Now as a short interlude, you might be thinking, you might be thinking, John, there's no way the model could know this. It's so under specified. I pizza is a little weird, I admit. But there's just no way to know that this is store or in went into. I mean, the same thing is true of language modeling. So it's going to end up learning these average statistics about what things tend to be in the given context. And it's going to sort of hedge its bets and try to build a distribution of what things could appear there. So for the people who are thinking that, if there wasn't, that's what you should be thinking. It has to sort of know what kinds of things will end up in these slots. It has other uncertainty, because it can't be sure that any of the other words are necessarily right. And then it is, it's predicting these three words. And so you can see why it's important to not just have masks potentially, to have these sort of token randomization things, because again, we don't actually care about its ability to predict the masks. I'm not going to usually, I'm not going to actually sample from the model's distribution over what should go here. Instead, I am going to use the parameters of the neural network and expect that it built strong representations of language. So I don't want it to think it's got a free pass for representing something if it doesn't have a mask there. So there was one extra thing with the BERT pre-training, which is a next sentence prediction objective. So the input to BERT looks like this. This is straight from the BERT paper. You have a label here before your first sentence, and then a separation, and then a second sentence. So you had always two contiguous chunks of text. You had a first chunk of text here. My dog is cute. And then a second chunk of text, he likes playing. You can see the sub words there. And now these would actually be both be much longer. So these whole thing would be 512 words, and it would be about half, and that would be about half, and they'd be contiguous chunks of text. But here was the deal. What they wanted to do was they wanted to try to teach the system to understand sort of relationships between different whole pieces of text. In order to better pre-trained for downstream applications like question answering, where you have two pretty different pieces of text, and you need to know how they relate to each other. So the objective they came up with was you should sometimes have the second chunk of text be the actual chunk of text that directly follows the first in your data set, and sometimes have the second chunk of text be randomly sampled from somewhere else, so unrelated. And the model should predict whether it's the first case or the second. In order, again, to sort of have to reason about the relationships between the two chunks of text. So this is next sentence prediction. I think it's important to think about because it's a very different idea of pre-training objective than language modeling and masked language modeling. Even though later we're sort of argued that in the case of BERT, it's not necessary or useful. And one of the arguments is actually because it's actually way better to have a single context that's twice as long, so you can learn even longer distance dependencies and things. And so whether the objective itself would be useful if you could always just double the context size, I'm not sure if anyone's done research on that. But again, it's like a different kind of objective, and it's still noisy something about the input, right? The input was this big chunk of text, and you've noise it to say like, now you don't know whether it really was that or whether you sort of replaced it with a bunch of garbage, this sort of second portion here, whether the second portion has been replaced with something that didn't actually come from the same sequence. Okay, so let's talk some details about BERT. So BERT had 12 or 24 layers, depending on BERT base or BERT large. You'll probably use one of these models or one of the sort of descendants of these models if you choose to do something with the custom final project potentially, or if you choose the version of the default final project. And you had a 600 or a 1000 dimension hidden states, a bunch of attention heads, so this is that multi-headed attention, remember, about a bunch of them. So you're splitting all your dimensions into those 16 heads, and we're talking on the order of a couple hundred million parameters. At the time, right in 2018, we were like, whoa, that's a lot of parameters. How do you, that's a lot of parameters. And now, models are way, way, way, way bigger. So let's keep track of sort of the model sizes as we're going through this. And let's come back now to the corpus sizes as well. So we have books corpus. And this is the number of words there. This is the same thing that GPT-1 was trained on, 800 million words. Now we're going to train on also English Wikipedia, it's 250, sorry, that's 2,500 million, so that's 2,500,000,000 words. And again, to give you an idea of what is done in practice, right, pre-training is expensive and impractical for most users, let's say. So if you are a researcher with a GPU or five GPUs or something like that, you tend to not really be pre-training your whole own BERT model unless you're willing to spend a long time doing it. BERT itself was pre-trained with 64 TPU chips. A TPU is a special kind of hardware accelerator that accelerates the tensor operations effectively is developed by Google. So TPUs are just fast and can hold a lot. And for four days they had 64 chips. So if you have one GPU which you can think of as less than a single TPU, you're going to be waiting a long time to pre-training. But fine-tuning is so fast, it's so fast and impractical, it's common on a single GPU, you'll see how much faster fine-tuning is than pre-training in assignment five. And so this becomes, I think, a refrain of the field, you pre-trained once or handful of times, right, like a couple of people released big pre-trained models and then you fine-tune many times, right, so you save those parameters from pre-training and you fine-tune on all kinds of different problems. And that paradigm, right, taking something like Bert or whatever the best descendant of Bert is and taking it pre-trained and then fine-tuning it on what you want is pretty close to, you know, it's a very, very strong baseline in NLP right now, right? So and the simplicity is pretty fascinating. And there's one code base called Transformers from a company called Hugging Face that makes this just really just a couple of lines of Python to try out as well. So it sort of opened up very strong baselines without too, too much effort for a lot of tasks. Okay, so let's talk about evaluation. So pre-training is pitched as requiring all this different kind of language understanding. And the field is, the field of NLP has a hard time doing evaluation. But we try our best and we build datasets that we think are hard for various reasons because they require you to know stuff about language and about the world and about reasoning. And so when we evaluate whether pre-training is getting you a lot of sort of general knowledge, we evaluate on a lot of these tasks. So we evaluate on things like paraphrase detection on core questions. Natural language inference we saw. We have hard sentiment analysis datasets or what we're hard sentiment analysis datasets a couple of years ago. And actually, figuring out if sentences are grammatical tends to be hard. Determining the semantic similarity of text can be hard. Paraphrasing again. Natural language inference on a very, very small dataset. So this is this pre-training help you train on smaller datasets. The answer is yes, sort of thing. And so the birth folks released their paper after GPT was released. And there were a lot of sort of state of the art results that came from various things that you were supposed to be doing. And the results that you get sort of with pre-training, so here's open AI, GPT, here's birth base and large. The last three rows are all pre-trained. Elmo is sort of in the middle between pre-training the whole model and just having word embeddings. That's what this is. And the numbers you get are just, I think, to the field where quite astounding actually. We were all surprised that there was that much left to even be gotten on some of these datasets. And taking here, so this line in the table is unmarked when it's actually the number of training examples. This dataset has 2.5,000 training examples. And before sort of the big transformers came around, we had 60% accuracy on it. We run transformers on it. We get 10 points just by pre-training. And this has been a trend that has just continued. So why do anything but pre-trained encoders? We know encoders are good. We like the fact that you have bidirectional context. We also saw that BERT did better than GPT. But if you want to actually get it to do things, you can't just generate sequences from it the same way that you would from a model like GPT, a pre-trained decoder. You can sort of sample what things should go in a mask. So here's a mask. You can put a mask somewhere, sample the words that should go there. But if you want to sample whole context, right, if you want to get that story about the unicorns, for example, the encoder is not what you want to do. So they have sort of different contracts, and they can be used naturally at least in different ways. Okay, so let's talk very briefly about extensions of BERT. So they're BERT variants like Roberta and Spanbert. And there's just a bunch of papers with the word BERT in the title that did various things. Two very strong takeaways. Roberta, train BERT longer. BERT is underfit. Train it on more data. Train it for more steps. Spanbert, mask, contiguous spans of sub words. Words makes a harder, more useful pre-training task. So this is the idea that we can come up with better ways of noisy the input, of hiding stuff in the input, or breaking stuff in the input for our model to correct. So for example, if you have the sentence mask, ear, razz, razz, good, it's just not that hard to know that this is irresistibly, right, because like what could this possibly be after these sub words? So this is irresist, you know, something's about to come here and it's probably the end of that word. Whereas if you mask a long sequence of things, right now this is much harder, and actually you're getting a useful signal that is irresistibly good, and you sort of needed to mask all of them to make the task interesting. So Spanbert was like, oh, you should do this. This was super useful as well. So Roberta, just to point you at the fact that Roberta showed that BERT was underfit, you know, he said, BERT was trained on about 13 gigabytes of text, it got some accuracies, you can get above the amazing results of BERT, four extra points or so here, right, just by taking the identical model and training it on more data, the larger batch size for a long time. And if you train it, yeah, even longer without sort of more data, you don't get any better. Very briefly, okay, so very briefly on the encoder decoders. So we've seen decoders can be good because we get to play with the contracts that they give us, we get to play with them as language models, encoders give us that bidirectional context. So encoder decoders, maybe we get both. In practice, they're actually, yeah, pretty strong. So there was a, right, we could, so I guess one of the questions is like, what do we do to pre-train them? So we could do something like language modeling, right, where we take a sequence of words, one to word two t instead of t, right, and so as I have word one here, dot, dot, dot, word t, we provide those all to our encoder and we predict on none of them. And then we have word t plus one to word two t here in our decoder, right, and we predict on these. So we're doing language modeling on half the sequence and we've taken the other half to have our bidirectional encoder, right, so we're building strong representations on the encoder side, not predicting language modeling on any of this. And then we, on the other half of the tokens, we predict, you know, as a language model would do. And the hope is that you sort of pre-trained both of these well through the one language modeling loss up here. And this is actually, so this works pretty well. The encoder benefits from bidirectionality, the decoder, you can use to train the model. But what this paper showed that introduced the Model T5, roughly at all, found to work best was actually a very, or at least a somewhat different objective. And this should keep in your mind sort of that we have different ways of specifying the pre-training objectives and they will really work differently from each other. So what they said, let's say you have an original text like this. Thank you for inviting me to your party last week. We're going to define variable length spans in the text to replace with a unique symbol that says something is missing here. And then we'll replace and then we'll remove that. So now our input to our encoder is thank you symbol one, me to your party symbol to week. So we've noise the input, we've hidden stuff in the input. Also really interestingly, this doesn't say how long this is supposed to be. That's different from BERT. BERT said, oh, you masked this many sub words. This says, well, I got some token that says something's missing here. And I don't know what it is. I don't even know how many sub words it is. And then so you have this in your encoder and then your decoder predicts the first special word, this x here. And then what was missing for inviting. So thank you x for inviting. And then it predicts y. Here's this y here. And then what was missing from the y last week. This is called span corruption. And it's really interesting to me because in terms of the actual encoder decoder, we don't have to change it compared to whether we, if we were just doing language modeling pre-training. Because I just do language modeling on all these things. I just predict these words as if I'm a language model. I've just done a text pre-processing step. So the actual, I've just pre-processed the text to look like, oh, yeah, take the input, make it look like this, then make an output that looks like that up there. And the model gets to do what is effectively language modeling, but it actually works better. So there's a lot of numbers I realize. But look at the star here. This encoder decoder with a denoising objective that tends to work the best. And they tried similar models like a prefix language model that was sort of the first try that we had at defining a pre-training objective for language models, sorry, for encoder decoders. And then they had another, a number of other options, but what worked best for the encoder decoders. And one of the fascinating things about T5 is that you could pre-train it and fine tune it on questions like when was Franklin D. Roosevelt born and fine tune it to produce the answer. And then you could ask it new questions at test time. And then it would retrieve the answer from its parameters with some accuracy. And it would do so relatively well actually. And it would do so maybe 25% of the time on some of these data sets with 220 million parameters. And then at 11 billion parameters, this is way bigger than Bert large. It would do so even better, sometimes even doing as well as systems that were allowed to look at stuff other than their own parameters. So again, this is just making this answer come from its parameters. Yeah, I'm going to have to skip this. So if you look back at this slide after class, I have each of the examples of the things that we could imagine learning from pre-training with a label of what you might be learning. So this example is 10 for universities located in blank. You might learn trivia. In all these cases, there's all these things you can learn. One thing I will say is that models also learn and can make even worse racism, sexism, all manner of bad biases that are encoded in our text. When I say, yeah, they do this. And so we'll learn more about this in our later lectures, but it's important to keep in mind that when you're doing pre-training, you're learning a lot of stuff, and not all of it is good. So with GPT-3, the last thing here is that there's this third way of interacting with models that's related to treating them as language models. So GPT-3 is this very, very large model that was released by OpenAI. But it seems to be able to learn from examples in their context, their decoder context, without gradient steps, simply by looking sort of within their history. And now GPT-3 has 175 billion parameters, right? The last T5 model we saw was 11 billion parameters. And it seems to be sort of the canonical example of this working. And so what it looks like is you give it as part of its prefix. This goes to Merci, hello, goes to Mint, goes to writes, you've got these translation examples, you ask it for the last one, and it comes up with the correct translation. Seemingly because it's learned something about the task that you're sort of telling it to do through its prefix. And so you might do the same thing with addition. So something, if I plus eight is 13, give it addition examples, you might do the next addition example for you. Or maybe trying to figure out grammatical or spelling errors, for example. And here's the French case. So again, you're learning just to do pre-training. But when you're evaluating it, you don't even fine tune the model, you just provide prefixes. And so this especially is not well understood. And so a lot of research is going into sort of what the limitations of this so-called in-context learning are. But it's a fascinating direction for future work. In total, these models are not well understood. However, small, small, in-air growth models like Bert have become general tools in a wide range of settings. They do have these issues about learning all these biases about the world. They'll go into and further lectures in this course. And so, yeah, what you've learned this week, transformers and pre-training form the basis or at least the base lines for much of a natural language processing today. And assignment five is out and you'll be able to look more into it. And I'm over time. All right. Yeah. I guess I can take a question if there is any, but people can keep going as well. So I think that I think there's a question about P5, which was how does the D-toder know that I'm currently predicting X for Y? Could you repeat that? Yeah. So about P5, there's a question that was asking how does the D-toder know it's currently predicting X for Y? It's hierarchy of predicting X for Y? I guess it doesn't specify it's going to change how does it know that it's currently predicting X for Y? OK. Yeah. That makes sense. So what it does, right? So it knows from the encoder that it has to at some point predict X and at some point predict Y because the encoder can just like remember that, oh, yeah, there's two things missing. And if there were more spans replaced, there would be a Z and then an A and then a B and you know whatever, just a bunch of unique identifiers. And then up here, it gets to say, OK, I have attention, I suppose. I can look and I know that first I have to predict this first master thing. So I'm going to generate that in my D-coder and then it gets that symbol, right? So we're doing training by giving it the right symbol. Now it gets that X and it says, OK, I'm predicting X now. And now it can predict, predict, predict, predict. Then it gets Y. So we're doing this teacher forcing training where we give it the right answer after penalizing it if it's wrong. Now it gets this Y, right? And it says, OK, now I have to predict what should go and why. And it can attend, you know, into the natural parts of this as well as what it's already predicted here because the decoder has attention within itself and it can see what should go there. So what's fascinating here is you're doing something like language modeling. But when you're predicting Y, right, you get to see what came after it. And that's I think one of the benefits of span corruption. So you're doing this thing where you don't know how long you should be predicting for like language modeling, but you get to know what came after the thing that's missing.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.24, "text": " Hello, everybody.", "tokens": [2425, 11, 2201, 13], "temperature": 0.0, "avg_logprob": -0.283055161776608, "compression_ratio": 1.3419689119170986, "no_speech_prob": 0.10152037441730499}, {"id": 1, "seek": 0, "start": 7.24, "end": 10.8, "text": " Welcome to CS224N lecture 10.", "tokens": [4027, 281, 9460, 7490, 19, 45, 7991, 1266, 13], "temperature": 0.0, "avg_logprob": -0.283055161776608, "compression_ratio": 1.3419689119170986, "no_speech_prob": 0.10152037441730499}, {"id": 2, "seek": 0, "start": 10.8, "end": 16.54, "text": " This is going to be primarily on pre-training, but we will also discuss sub-word models a", "tokens": [639, 307, 516, 281, 312, 10029, 322, 659, 12, 17227, 1760, 11, 457, 321, 486, 611, 2248, 1422, 12, 7462, 5245, 257], "temperature": 0.0, "avg_logprob": -0.283055161776608, "compression_ratio": 1.3419689119170986, "no_speech_prob": 0.10152037441730499}, {"id": 3, "seek": 0, "start": 16.54, "end": 18.8, "text": " little bit and review transformers.", "tokens": [707, 857, 293, 3131, 4088, 433, 13], "temperature": 0.0, "avg_logprob": -0.283055161776608, "compression_ratio": 1.3419689119170986, "no_speech_prob": 0.10152037441730499}, {"id": 4, "seek": 0, "start": 18.8, "end": 26.84, "text": " Okay, so we have a lot of exciting things to get into today, but some reminders about", "tokens": [1033, 11, 370, 321, 362, 257, 688, 295, 4670, 721, 281, 483, 666, 965, 11, 457, 512, 43458, 466], "temperature": 0.0, "avg_logprob": -0.283055161776608, "compression_ratio": 1.3419689119170986, "no_speech_prob": 0.10152037441730499}, {"id": 5, "seek": 2684, "start": 26.84, "end": 30.64, "text": " the class.", "tokens": [264, 1508, 13], "temperature": 0.0, "avg_logprob": -0.291318737730688, "compression_ratio": 1.650943396226415, "no_speech_prob": 7.707396434852853e-05}, {"id": 6, "seek": 2684, "start": 30.64, "end": 32.8, "text": " Assignment 5 is being released today.", "tokens": [6281, 41134, 1025, 307, 885, 4736, 965, 13], "temperature": 0.0, "avg_logprob": -0.291318737730688, "compression_ratio": 1.650943396226415, "no_speech_prob": 7.707396434852853e-05}, {"id": 7, "seek": 2684, "start": 32.8, "end": 37.0, "text": " Assignment 4 was due a minute ago, so if you are done with that, congratulations.", "tokens": [6281, 41134, 1017, 390, 3462, 257, 3456, 2057, 11, 370, 498, 291, 366, 1096, 365, 300, 11, 13568, 13], "temperature": 0.0, "avg_logprob": -0.291318737730688, "compression_ratio": 1.650943396226415, "no_speech_prob": 7.707396434852853e-05}, {"id": 8, "seek": 2684, "start": 37.0, "end": 41.24, "text": " If not, I hope that the late days go well.", "tokens": [759, 406, 11, 286, 1454, 300, 264, 3469, 1708, 352, 731, 13], "temperature": 0.0, "avg_logprob": -0.291318737730688, "compression_ratio": 1.650943396226415, "no_speech_prob": 7.707396434852853e-05}, {"id": 9, "seek": 2684, "start": 41.24, "end": 48.24, "text": " Assignment 5 is on pre-training and transformers, so these lectures are going to be very useful", "tokens": [6281, 41134, 1025, 307, 322, 659, 12, 17227, 1760, 293, 4088, 433, 11, 370, 613, 16564, 366, 516, 281, 312, 588, 4420], "temperature": 0.0, "avg_logprob": -0.291318737730688, "compression_ratio": 1.650943396226415, "no_speech_prob": 7.707396434852853e-05}, {"id": 10, "seek": 2684, "start": 48.24, "end": 52.32, "text": " to you for that and I just don't cover anything after these lectures.", "tokens": [281, 291, 337, 300, 293, 286, 445, 500, 380, 2060, 1340, 934, 613, 16564, 13], "temperature": 0.0, "avg_logprob": -0.291318737730688, "compression_ratio": 1.650943396226415, "no_speech_prob": 7.707396434852853e-05}, {"id": 11, "seek": 2684, "start": 52.32, "end": 54.0, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.291318737730688, "compression_ratio": 1.650943396226415, "no_speech_prob": 7.707396434852853e-05}, {"id": 12, "seek": 5400, "start": 54.0, "end": 60.2, "text": " So today, let's kind of take a little peek through what the outline will be.", "tokens": [407, 965, 11, 718, 311, 733, 295, 747, 257, 707, 19604, 807, 437, 264, 16387, 486, 312, 13], "temperature": 0.0, "avg_logprob": -0.16395002948351142, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.0002130172069882974}, {"id": 13, "seek": 5400, "start": 60.2, "end": 64.4, "text": " We haven't talked about sub-word modeling yet and sort of we should have.", "tokens": [492, 2378, 380, 2825, 466, 1422, 12, 7462, 15983, 1939, 293, 1333, 295, 321, 820, 362, 13], "temperature": 0.0, "avg_logprob": -0.16395002948351142, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.0002130172069882974}, {"id": 14, "seek": 5400, "start": 64.4, "end": 66.28, "text": " And so we're going to talk a little bit about sub-words.", "tokens": [400, 370, 321, 434, 516, 281, 751, 257, 707, 857, 466, 1422, 12, 13832, 13], "temperature": 0.0, "avg_logprob": -0.16395002948351142, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.0002130172069882974}, {"id": 15, "seek": 5400, "start": 66.28, "end": 71.88, "text": " You saw these in assignment 4, all just, you know, as the data that we provided to you", "tokens": [509, 1866, 613, 294, 15187, 1017, 11, 439, 445, 11, 291, 458, 11, 382, 264, 1412, 300, 321, 5649, 281, 291], "temperature": 0.0, "avg_logprob": -0.16395002948351142, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.0002130172069882974}, {"id": 16, "seek": 5400, "start": 71.88, "end": 75.72, "text": " with your machine translation system, but we're going to talk a little bit about why they're", "tokens": [365, 428, 3479, 12853, 1185, 11, 457, 321, 434, 516, 281, 751, 257, 707, 857, 466, 983, 436, 434], "temperature": 0.0, "avg_logprob": -0.16395002948351142, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.0002130172069882974}, {"id": 17, "seek": 5400, "start": 75.72, "end": 81.44, "text": " so ubiquitous in NLP because they are used in pre-trained models.", "tokens": [370, 43868, 39831, 294, 426, 45196, 570, 436, 366, 1143, 294, 659, 12, 17227, 2001, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16395002948351142, "compression_ratio": 1.7030075187969924, "no_speech_prob": 0.0002130172069882974}, {"id": 18, "seek": 8144, "start": 81.44, "end": 86.39999999999999, "text": " I mean, they're used in a number of different models, but when we discuss pre-training,", "tokens": [286, 914, 11, 436, 434, 1143, 294, 257, 1230, 295, 819, 5245, 11, 457, 562, 321, 2248, 659, 12, 17227, 1760, 11], "temperature": 0.0, "avg_logprob": -0.13144716094521916, "compression_ratio": 1.7763157894736843, "no_speech_prob": 6.811918137827888e-05}, {"id": 19, "seek": 8144, "start": 86.39999999999999, "end": 89.96, "text": " it's important to know that sub-words are part of it.", "tokens": [309, 311, 1021, 281, 458, 300, 1422, 12, 13832, 366, 644, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.13144716094521916, "compression_ratio": 1.7763157894736843, "no_speech_prob": 6.811918137827888e-05}, {"id": 20, "seek": 8144, "start": 89.96, "end": 94.24, "text": " Then we'll sort of motivate, we'll go on another journey of motivation of motivating", "tokens": [1396, 321, 603, 1333, 295, 28497, 11, 321, 603, 352, 322, 1071, 4671, 295, 12335, 295, 41066], "temperature": 0.0, "avg_logprob": -0.13144716094521916, "compression_ratio": 1.7763157894736843, "no_speech_prob": 6.811918137827888e-05}, {"id": 21, "seek": 8144, "start": 94.24, "end": 96.16, "text": " model pre-training from word embedding.", "tokens": [2316, 659, 12, 17227, 1760, 490, 1349, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.13144716094521916, "compression_ratio": 1.7763157894736843, "no_speech_prob": 6.811918137827888e-05}, {"id": 22, "seek": 8144, "start": 96.16, "end": 101.28, "text": " So we've already seen pre-training in some sense in the very first lecture of this course", "tokens": [407, 321, 600, 1217, 1612, 659, 12, 17227, 1760, 294, 512, 2020, 294, 264, 588, 700, 7991, 295, 341, 1164], "temperature": 0.0, "avg_logprob": -0.13144716094521916, "compression_ratio": 1.7763157894736843, "no_speech_prob": 6.811918137827888e-05}, {"id": 23, "seek": 8144, "start": 101.28, "end": 106.12, "text": " because we pre-trained individual word embeddings that don't take into account their contexts", "tokens": [570, 321, 659, 12, 17227, 2001, 2609, 1349, 12240, 29432, 300, 500, 380, 747, 666, 2696, 641, 30628], "temperature": 0.0, "avg_logprob": -0.13144716094521916, "compression_ratio": 1.7763157894736843, "no_speech_prob": 6.811918137827888e-05}, {"id": 24, "seek": 8144, "start": 106.12, "end": 110.92, "text": " on very large text corporate and saw that they were able to encode a lot of useful things", "tokens": [322, 588, 2416, 2487, 10896, 293, 1866, 300, 436, 645, 1075, 281, 2058, 1429, 257, 688, 295, 4420, 721], "temperature": 0.0, "avg_logprob": -0.13144716094521916, "compression_ratio": 1.7763157894736843, "no_speech_prob": 6.811918137827888e-05}, {"id": 25, "seek": 11092, "start": 110.92, "end": 113.32000000000001, "text": " about language.", "tokens": [466, 2856, 13], "temperature": 0.0, "avg_logprob": -0.1915213224050161, "compression_ratio": 1.8675496688741722, "no_speech_prob": 5.64641050004866e-05}, {"id": 26, "seek": 11092, "start": 113.32000000000001, "end": 117.56, "text": " So after we do that motivation, we'll go through model pre-training three ways.", "tokens": [407, 934, 321, 360, 300, 12335, 11, 321, 603, 352, 807, 2316, 659, 12, 17227, 1760, 1045, 2098, 13], "temperature": 0.0, "avg_logprob": -0.1915213224050161, "compression_ratio": 1.8675496688741722, "no_speech_prob": 5.64641050004866e-05}, {"id": 27, "seek": 11092, "start": 117.56, "end": 120.16, "text": " And we're going to, you know, reference actually the lecture on Tuesday.", "tokens": [400, 321, 434, 516, 281, 11, 291, 458, 11, 6408, 767, 264, 7991, 322, 10017, 13], "temperature": 0.0, "avg_logprob": -0.1915213224050161, "compression_ratio": 1.8675496688741722, "no_speech_prob": 5.64641050004866e-05}, {"id": 28, "seek": 11092, "start": 120.16, "end": 122.68, "text": " So this is why we're going to review a little bit of the transformer stuff.", "tokens": [407, 341, 307, 983, 321, 434, 516, 281, 3131, 257, 707, 857, 295, 264, 31782, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1915213224050161, "compression_ratio": 1.8675496688741722, "no_speech_prob": 5.64641050004866e-05}, {"id": 29, "seek": 11092, "start": 122.68, "end": 127.08, "text": " We'll talk about model pre-training in decoters, like a transformer decoder that we saw", "tokens": [492, 603, 751, 466, 2316, 659, 12, 17227, 1760, 294, 979, 310, 433, 11, 411, 257, 31782, 979, 19866, 300, 321, 1866], "temperature": 0.0, "avg_logprob": -0.1915213224050161, "compression_ratio": 1.8675496688741722, "no_speech_prob": 5.64641050004866e-05}, {"id": 30, "seek": 11092, "start": 127.08, "end": 130.24, "text": " last week in encoters, and then encoder decoters.", "tokens": [1036, 1243, 294, 2058, 310, 433, 11, 293, 550, 2058, 19866, 979, 310, 433, 13], "temperature": 0.0, "avg_logprob": -0.1915213224050161, "compression_ratio": 1.8675496688741722, "no_speech_prob": 5.64641050004866e-05}, {"id": 31, "seek": 11092, "start": 130.24, "end": 134.08, "text": " And each of these three cases, we're going to talk a little bit about sort of what things", "tokens": [400, 1184, 295, 613, 1045, 3331, 11, 321, 434, 516, 281, 751, 257, 707, 857, 466, 1333, 295, 437, 721], "temperature": 0.0, "avg_logprob": -0.1915213224050161, "compression_ratio": 1.8675496688741722, "no_speech_prob": 5.64641050004866e-05}, {"id": 32, "seek": 11092, "start": 134.08, "end": 140.48000000000002, "text": " you could be doing and then popular models that are in use across research and in industry.", "tokens": [291, 727, 312, 884, 293, 550, 3743, 5245, 300, 366, 294, 764, 2108, 2132, 293, 294, 3518, 13], "temperature": 0.0, "avg_logprob": -0.1915213224050161, "compression_ratio": 1.8675496688741722, "no_speech_prob": 5.64641050004866e-05}, {"id": 33, "seek": 14048, "start": 140.48, "end": 143.48, "text": " And we're going to talk a little bit about, you know, what do we think pre-training is", "tokens": [400, 321, 434, 516, 281, 751, 257, 707, 857, 466, 11, 291, 458, 11, 437, 360, 321, 519, 659, 12, 17227, 1760, 307], "temperature": 0.0, "avg_logprob": -0.1320831035745555, "compression_ratio": 1.75, "no_speech_prob": 0.00010548504360485822}, {"id": 34, "seek": 14048, "start": 143.48, "end": 144.48, "text": " teaching?", "tokens": [4571, 30], "temperature": 0.0, "avg_logprob": -0.1320831035745555, "compression_ratio": 1.75, "no_speech_prob": 0.00010548504360485822}, {"id": 35, "seek": 14048, "start": 144.48, "end": 145.88, "text": " This is going to be very brief.", "tokens": [639, 307, 516, 281, 312, 588, 5353, 13], "temperature": 0.0, "avg_logprob": -0.1320831035745555, "compression_ratio": 1.75, "no_speech_prob": 0.00010548504360485822}, {"id": 36, "seek": 14048, "start": 145.88, "end": 149.6, "text": " Actually, a lot of the interpretability and analysis lecture in two weeks is going", "tokens": [5135, 11, 257, 688, 295, 264, 7302, 2310, 293, 5215, 7991, 294, 732, 3259, 307, 516], "temperature": 0.0, "avg_logprob": -0.1320831035745555, "compression_ratio": 1.75, "no_speech_prob": 0.00010548504360485822}, {"id": 37, "seek": 14048, "start": 149.6, "end": 155.0, "text": " to talk more about sort of the mystery and the scientific problem of figuring out what", "tokens": [281, 751, 544, 466, 1333, 295, 264, 11422, 293, 264, 8134, 1154, 295, 15213, 484, 437], "temperature": 0.0, "avg_logprob": -0.1320831035745555, "compression_ratio": 1.75, "no_speech_prob": 0.00010548504360485822}, {"id": 38, "seek": 14048, "start": 155.0, "end": 159.72, "text": " these models are learning about language through pre-training objectives, but we'll sort", "tokens": [613, 5245, 366, 2539, 466, 2856, 807, 659, 12, 17227, 1760, 15961, 11, 457, 321, 603, 1333], "temperature": 0.0, "avg_logprob": -0.1320831035745555, "compression_ratio": 1.75, "no_speech_prob": 0.00010548504360485822}, {"id": 39, "seek": 14048, "start": 159.72, "end": 160.72, "text": " of get a peak.", "tokens": [295, 483, 257, 10651, 13], "temperature": 0.0, "avg_logprob": -0.1320831035745555, "compression_ratio": 1.75, "no_speech_prob": 0.00010548504360485822}, {"id": 40, "seek": 14048, "start": 160.72, "end": 163.67999999999998, "text": " And then we'll talk about very large models and in context learning.", "tokens": [400, 550, 321, 603, 751, 466, 588, 2416, 5245, 293, 294, 4319, 2539, 13], "temperature": 0.0, "avg_logprob": -0.1320831035745555, "compression_ratio": 1.75, "no_speech_prob": 0.00010548504360485822}, {"id": 41, "seek": 14048, "start": 163.67999999999998, "end": 169.35999999999999, "text": " So if you've heard of GPT-3, for example, we're going to just briefly touch on that here", "tokens": [407, 498, 291, 600, 2198, 295, 26039, 51, 12, 18, 11, 337, 1365, 11, 321, 434, 516, 281, 445, 10515, 2557, 322, 300, 510], "temperature": 0.0, "avg_logprob": -0.1320831035745555, "compression_ratio": 1.75, "no_speech_prob": 0.00010548504360485822}, {"id": 42, "seek": 16936, "start": 169.36, "end": 173.68, "text": " and I think we'll discuss more about it in the course later on as well.", "tokens": [293, 286, 519, 321, 603, 2248, 544, 466, 309, 294, 264, 1164, 1780, 322, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.23447650752655447, "compression_ratio": 1.721875, "no_speech_prob": 2.2109163182904013e-05}, {"id": 43, "seek": 16936, "start": 173.68, "end": 175.32000000000002, "text": " Okay, so we've got a lot to do.", "tokens": [1033, 11, 370, 321, 600, 658, 257, 688, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.23447650752655447, "compression_ratio": 1.721875, "no_speech_prob": 2.2109163182904013e-05}, {"id": 44, "seek": 16936, "start": 175.32000000000002, "end": 177.56, "text": " Let's jump right in.", "tokens": [961, 311, 3012, 558, 294, 13], "temperature": 0.0, "avg_logprob": -0.23447650752655447, "compression_ratio": 1.721875, "no_speech_prob": 2.2109163182904013e-05}, {"id": 45, "seek": 16936, "start": 177.56, "end": 179.8, "text": " So word structure and sub-broad models.", "tokens": [407, 1349, 3877, 293, 1422, 12, 65, 8417, 5245, 13], "temperature": 0.0, "avg_logprob": -0.23447650752655447, "compression_ratio": 1.721875, "no_speech_prob": 2.2109163182904013e-05}, {"id": 46, "seek": 16936, "start": 179.8, "end": 184.08, "text": " Let's think about sort of the assumptions we've been making in this course so far.", "tokens": [961, 311, 519, 466, 1333, 295, 264, 17695, 321, 600, 668, 1455, 294, 341, 1164, 370, 1400, 13], "temperature": 0.0, "avg_logprob": -0.23447650752655447, "compression_ratio": 1.721875, "no_speech_prob": 2.2109163182904013e-05}, {"id": 47, "seek": 16936, "start": 184.08, "end": 189.12, "text": " When we give you an assignment, when we talk about training word to veck, for example,", "tokens": [1133, 321, 976, 291, 364, 15187, 11, 562, 321, 751, 466, 3097, 1349, 281, 1241, 547, 11, 337, 1365, 11], "temperature": 0.0, "avg_logprob": -0.23447650752655447, "compression_ratio": 1.721875, "no_speech_prob": 2.2109163182904013e-05}, {"id": 48, "seek": 16936, "start": 189.12, "end": 191.68, "text": " we made this assumption about a language's vocabulary.", "tokens": [321, 1027, 341, 15302, 466, 257, 2856, 311, 19864, 13], "temperature": 0.0, "avg_logprob": -0.23447650752655447, "compression_ratio": 1.721875, "no_speech_prob": 2.2109163182904013e-05}, {"id": 49, "seek": 16936, "start": 191.68, "end": 195.32000000000002, "text": " In particular, we've made this assumption that has a fixed vocabulary of something like", "tokens": [682, 1729, 11, 321, 600, 1027, 341, 15302, 300, 575, 257, 6806, 19864, 295, 746, 411], "temperature": 0.0, "avg_logprob": -0.23447650752655447, "compression_ratio": 1.721875, "no_speech_prob": 2.2109163182904013e-05}, {"id": 50, "seek": 16936, "start": 195.32000000000002, "end": 198.8, "text": " tens of thousands, maybe a hundred thousand, I don't know, a number of...", "tokens": [10688, 295, 5383, 11, 1310, 257, 3262, 4714, 11, 286, 500, 380, 458, 11, 257, 1230, 295, 485], "temperature": 0.0, "avg_logprob": -0.23447650752655447, "compression_ratio": 1.721875, "no_speech_prob": 2.2109163182904013e-05}, {"id": 51, "seek": 19880, "start": 198.8, "end": 203.92000000000002, "text": " But some relatively large, it seems, number of words, and that seems sort of like pretty", "tokens": [583, 512, 7226, 2416, 11, 309, 2544, 11, 1230, 295, 2283, 11, 293, 300, 2544, 1333, 295, 411, 1238], "temperature": 0.0, "avg_logprob": -0.20246127446492512, "compression_ratio": 1.70703125, "no_speech_prob": 0.00013327998749446124}, {"id": 52, "seek": 19880, "start": 203.92000000000002, "end": 207.68, "text": " good so far, at least, and what we've done.", "tokens": [665, 370, 1400, 11, 412, 1935, 11, 293, 437, 321, 600, 1096, 13], "temperature": 0.0, "avg_logprob": -0.20246127446492512, "compression_ratio": 1.70703125, "no_speech_prob": 0.00013327998749446124}, {"id": 53, "seek": 19880, "start": 207.68, "end": 212.4, "text": " And we build this vocabulary from the set that we train, say, word to veck on.", "tokens": [400, 321, 1322, 341, 19864, 490, 264, 992, 300, 321, 3847, 11, 584, 11, 1349, 281, 1241, 547, 322, 13], "temperature": 0.0, "avg_logprob": -0.20246127446492512, "compression_ratio": 1.70703125, "no_speech_prob": 0.00013327998749446124}, {"id": 54, "seek": 19880, "start": 212.4, "end": 217.08, "text": " And then here's a crucial thing, any novel word, any word that you did not see at training", "tokens": [400, 550, 510, 311, 257, 11462, 551, 11, 604, 7613, 1349, 11, 604, 1349, 300, 291, 630, 406, 536, 412, 3097], "temperature": 0.0, "avg_logprob": -0.20246127446492512, "compression_ratio": 1.70703125, "no_speech_prob": 0.00013327998749446124}, {"id": 55, "seek": 19880, "start": 217.08, "end": 222.72000000000003, "text": " time, is sort of mapped to a single unctocon.", "tokens": [565, 11, 307, 1333, 295, 33318, 281, 257, 2167, 517, 349, 905, 266, 13], "temperature": 0.0, "avg_logprob": -0.20246127446492512, "compression_ratio": 1.70703125, "no_speech_prob": 0.00013327998749446124}, {"id": 56, "seek": 19880, "start": 222.72000000000003, "end": 226.92000000000002, "text": " There are other ways to handle this, but you sort of have to do something and a frequent", "tokens": [821, 366, 661, 2098, 281, 4813, 341, 11, 457, 291, 1333, 295, 362, 281, 360, 746, 293, 257, 18004], "temperature": 0.0, "avg_logprob": -0.20246127446492512, "compression_ratio": 1.70703125, "no_speech_prob": 0.00013327998749446124}, {"id": 57, "seek": 22692, "start": 226.92, "end": 229.6, "text": " method is to map them all to unct.", "tokens": [3170, 307, 281, 4471, 552, 439, 281, 517, 349, 13], "temperature": 0.0, "avg_logprob": -0.22912287930829808, "compression_ratio": 1.5630252100840336, "no_speech_prob": 3.368705802131444e-05}, {"id": 58, "seek": 22692, "start": 229.6, "end": 233.28, "text": " So let's walk through what this sort of means in English.", "tokens": [407, 718, 311, 1792, 807, 437, 341, 1333, 295, 1355, 294, 3669, 13], "temperature": 0.0, "avg_logprob": -0.22912287930829808, "compression_ratio": 1.5630252100840336, "no_speech_prob": 3.368705802131444e-05}, {"id": 59, "seek": 22692, "start": 233.28, "end": 235.88, "text": " You learn embeddings, you map them, it all works.", "tokens": [509, 1466, 12240, 29432, 11, 291, 4471, 552, 11, 309, 439, 1985, 13], "temperature": 0.0, "avg_logprob": -0.22912287930829808, "compression_ratio": 1.5630252100840336, "no_speech_prob": 3.368705802131444e-05}, {"id": 60, "seek": 22692, "start": 235.88, "end": 242.0, "text": " Then you have a variation on a word like tase, with a bunch of a's.", "tokens": [1396, 291, 362, 257, 12990, 322, 257, 1349, 411, 256, 651, 11, 365, 257, 3840, 295, 257, 311, 13], "temperature": 0.0, "avg_logprob": -0.22912287930829808, "compression_ratio": 1.5630252100840336, "no_speech_prob": 3.368705802131444e-05}, {"id": 61, "seek": 22692, "start": 242.0, "end": 247.92, "text": " And your model isn't smart enough to know that that sort of means like very tasty, maybe.", "tokens": [400, 428, 2316, 1943, 380, 4069, 1547, 281, 458, 300, 300, 1333, 295, 1355, 411, 588, 11535, 11, 1310, 13], "temperature": 0.0, "avg_logprob": -0.22912287930829808, "compression_ratio": 1.5630252100840336, "no_speech_prob": 3.368705802131444e-05}, {"id": 62, "seek": 22692, "start": 247.92, "end": 252.16, "text": " And so it maps it to unct, because it's just a dictionary look-up mess.", "tokens": [400, 370, 309, 11317, 309, 281, 517, 349, 11, 570, 309, 311, 445, 257, 25890, 574, 12, 1010, 2082, 13], "temperature": 0.0, "avg_logprob": -0.22912287930829808, "compression_ratio": 1.5630252100840336, "no_speech_prob": 3.368705802131444e-05}, {"id": 63, "seek": 25216, "start": 252.16, "end": 258.0, "text": " And then you have a typo like lern, and that maps to unct as well, potentially, if it", "tokens": [400, 550, 291, 362, 257, 2125, 78, 411, 287, 1248, 11, 293, 300, 11317, 281, 517, 349, 382, 731, 11, 7263, 11, 498, 309], "temperature": 0.0, "avg_logprob": -0.21222896440654782, "compression_ratio": 1.7804878048780488, "no_speech_prob": 1.9515367966960184e-05}, {"id": 64, "seek": 25216, "start": 258.0, "end": 262.15999999999997, "text": " wasn't in your training set, some people make typos, but not all of them will be seen", "tokens": [2067, 380, 294, 428, 3097, 992, 11, 512, 561, 652, 2125, 329, 11, 457, 406, 439, 295, 552, 486, 312, 1612], "temperature": 0.0, "avg_logprob": -0.21222896440654782, "compression_ratio": 1.7804878048780488, "no_speech_prob": 1.9515367966960184e-05}, {"id": 65, "seek": 25216, "start": 262.15999999999997, "end": 263.15999999999997, "text": " at training time.", "tokens": [412, 3097, 565, 13], "temperature": 0.0, "avg_logprob": -0.21222896440654782, "compression_ratio": 1.7804878048780488, "no_speech_prob": 1.9515367966960184e-05}, {"id": 66, "seek": 25216, "start": 263.15999999999997, "end": 264.88, "text": " And then you'll have novel items.", "tokens": [400, 550, 291, 603, 362, 7613, 4754, 13], "temperature": 0.0, "avg_logprob": -0.21222896440654782, "compression_ratio": 1.7804878048780488, "no_speech_prob": 1.9515367966960184e-05}, {"id": 67, "seek": 25216, "start": 264.88, "end": 269.15999999999997, "text": " So this could be the first time that you've ever seen US, the students, and 224N have", "tokens": [407, 341, 727, 312, 264, 700, 565, 300, 291, 600, 1562, 1612, 2546, 11, 264, 1731, 11, 293, 5853, 19, 45, 362], "temperature": 0.0, "avg_logprob": -0.21222896440654782, "compression_ratio": 1.7804878048780488, "no_speech_prob": 1.9515367966960184e-05}, {"id": 68, "seek": 25216, "start": 269.15999999999997, "end": 272.12, "text": " seen the word transformer if I.", "tokens": [1612, 264, 1349, 31782, 498, 286, 13], "temperature": 0.0, "avg_logprob": -0.21222896440654782, "compression_ratio": 1.7804878048780488, "no_speech_prob": 1.9515367966960184e-05}, {"id": 69, "seek": 25216, "start": 272.12, "end": 276.28, "text": " But I get the feeling you sort of have a notion of what it's supposed to mean, like", "tokens": [583, 286, 483, 264, 2633, 291, 1333, 295, 362, 257, 10710, 295, 437, 309, 311, 3442, 281, 914, 11, 411], "temperature": 0.0, "avg_logprob": -0.21222896440654782, "compression_ratio": 1.7804878048780488, "no_speech_prob": 1.9515367966960184e-05}, {"id": 70, "seek": 25216, "start": 276.28, "end": 281.08, "text": " maybe add transformers to or turn into using transformers, or turn into a transformer", "tokens": [1310, 909, 4088, 433, 281, 420, 1261, 666, 1228, 4088, 433, 11, 420, 1261, 666, 257, 31782], "temperature": 0.0, "avg_logprob": -0.21222896440654782, "compression_ratio": 1.7804878048780488, "no_speech_prob": 1.9515367966960184e-05}, {"id": 71, "seek": 28108, "start": 281.08, "end": 282.35999999999996, "text": " or something like that.", "tokens": [420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.16644225801740373, "compression_ratio": 1.6392857142857142, "no_speech_prob": 3.0228875402826816e-05}, {"id": 72, "seek": 28108, "start": 282.35999999999996, "end": 286.03999999999996, "text": " And this is also going to be mapped to unct, even though you've seen transformer and", "tokens": [400, 341, 307, 611, 516, 281, 312, 33318, 281, 517, 349, 11, 754, 1673, 291, 600, 1612, 31782, 293], "temperature": 0.0, "avg_logprob": -0.16644225801740373, "compression_ratio": 1.6392857142857142, "no_speech_prob": 3.0228875402826816e-05}, {"id": 73, "seek": 28108, "start": 286.03999999999996, "end": 288.64, "text": " if I.", "tokens": [498, 286, 13], "temperature": 0.0, "avg_logprob": -0.16644225801740373, "compression_ratio": 1.6392857142857142, "no_speech_prob": 3.0228875402826816e-05}, {"id": 74, "seek": 28108, "start": 288.64, "end": 294.76, "text": " And so somehow the conclusion we have to come to is that looking at words as just like", "tokens": [400, 370, 6063, 264, 10063, 321, 362, 281, 808, 281, 307, 300, 1237, 412, 2283, 382, 445, 411], "temperature": 0.0, "avg_logprob": -0.16644225801740373, "compression_ratio": 1.6392857142857142, "no_speech_prob": 3.0228875402826816e-05}, {"id": 75, "seek": 28108, "start": 294.76, "end": 300.15999999999997, "text": " the individual sequence of characters uniquely identifies that word, and that's sort of", "tokens": [264, 2609, 8310, 295, 4342, 31474, 34597, 300, 1349, 11, 293, 300, 311, 1333, 295], "temperature": 0.0, "avg_logprob": -0.16644225801740373, "compression_ratio": 1.6392857142857142, "no_speech_prob": 3.0228875402826816e-05}, {"id": 76, "seek": 28108, "start": 300.15999999999997, "end": 304.47999999999996, "text": " how we should parameterize things is just wrong.", "tokens": [577, 321, 820, 13075, 1125, 721, 307, 445, 2085, 13], "temperature": 0.0, "avg_logprob": -0.16644225801740373, "compression_ratio": 1.6392857142857142, "no_speech_prob": 3.0228875402826816e-05}, {"id": 77, "seek": 28108, "start": 304.47999999999996, "end": 309.32, "text": " And so not only is this true in English, but in many languages, this finite vocabulary", "tokens": [400, 370, 406, 787, 307, 341, 2074, 294, 3669, 11, 457, 294, 867, 8650, 11, 341, 19362, 19864], "temperature": 0.0, "avg_logprob": -0.16644225801740373, "compression_ratio": 1.6392857142857142, "no_speech_prob": 3.0228875402826816e-05}, {"id": 78, "seek": 28108, "start": 309.32, "end": 310.96, "text": " assumption makes even less sense.", "tokens": [15302, 1669, 754, 1570, 2020, 13], "temperature": 0.0, "avg_logprob": -0.16644225801740373, "compression_ratio": 1.6392857142857142, "no_speech_prob": 3.0228875402826816e-05}, {"id": 79, "seek": 31096, "start": 310.96, "end": 316.64, "text": " So already it doesn't make sense in English, but English is, it's not the worst for English.", "tokens": [407, 1217, 309, 1177, 380, 652, 2020, 294, 3669, 11, 457, 3669, 307, 11, 309, 311, 406, 264, 5855, 337, 3669, 13], "temperature": 0.0, "avg_logprob": -0.1436161404564267, "compression_ratio": 1.7035573122529644, "no_speech_prob": 1.8912209270638414e-05}, {"id": 80, "seek": 31096, "start": 316.64, "end": 322.12, "text": " So morphology is the study of the structure of words.", "tokens": [407, 25778, 1793, 307, 264, 2979, 295, 264, 3877, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1436161404564267, "compression_ratio": 1.7035573122529644, "no_speech_prob": 1.8912209270638414e-05}, {"id": 81, "seek": 31096, "start": 322.12, "end": 328.44, "text": " And English is known to have pretty simple morphology in kind of specific ways.", "tokens": [400, 3669, 307, 2570, 281, 362, 1238, 2199, 25778, 1793, 294, 733, 295, 2685, 2098, 13], "temperature": 0.0, "avg_logprob": -0.1436161404564267, "compression_ratio": 1.7035573122529644, "no_speech_prob": 1.8912209270638414e-05}, {"id": 82, "seek": 31096, "start": 328.44, "end": 334.0, "text": " And when languages have complex morphology, it means you have longer words, more complex", "tokens": [400, 562, 8650, 362, 3997, 25778, 1793, 11, 309, 1355, 291, 362, 2854, 2283, 11, 544, 3997], "temperature": 0.0, "avg_logprob": -0.1436161404564267, "compression_ratio": 1.7035573122529644, "no_speech_prob": 1.8912209270638414e-05}, {"id": 83, "seek": 31096, "start": 334.0, "end": 339.15999999999997, "text": " words that get modified more, and each one of them occurs less frequently.", "tokens": [2283, 300, 483, 15873, 544, 11, 293, 1184, 472, 295, 552, 11843, 1570, 10374, 13], "temperature": 0.0, "avg_logprob": -0.1436161404564267, "compression_ratio": 1.7035573122529644, "no_speech_prob": 1.8912209270638414e-05}, {"id": 84, "seek": 31096, "start": 339.15999999999997, "end": 340.15999999999997, "text": " That should sound like a problem, right?", "tokens": [663, 820, 1626, 411, 257, 1154, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1436161404564267, "compression_ratio": 1.7035573122529644, "no_speech_prob": 1.8912209270638414e-05}, {"id": 85, "seek": 34016, "start": 340.16, "end": 344.68, "text": " If a word occurs less frequently, it will be less likely to show up in your training", "tokens": [759, 257, 1349, 11843, 1570, 10374, 11, 309, 486, 312, 1570, 3700, 281, 855, 493, 294, 428, 3097], "temperature": 0.0, "avg_logprob": -0.14646210801710777, "compression_ratio": 1.6374501992031874, "no_speech_prob": 7.243670552270487e-05}, {"id": 86, "seek": 34016, "start": 344.68, "end": 346.68, "text": " set.", "tokens": [992, 13], "temperature": 0.0, "avg_logprob": -0.14646210801710777, "compression_ratio": 1.6374501992031874, "no_speech_prob": 7.243670552270487e-05}, {"id": 87, "seek": 34016, "start": 346.68, "end": 349.0, "text": " And maybe it'll show up in your test set, never in your training set.", "tokens": [400, 1310, 309, 603, 855, 493, 294, 428, 1500, 992, 11, 1128, 294, 428, 3097, 992, 13], "temperature": 0.0, "avg_logprob": -0.14646210801710777, "compression_ratio": 1.6374501992031874, "no_speech_prob": 7.243670552270487e-05}, {"id": 88, "seek": 34016, "start": 349.0, "end": 351.84000000000003, "text": " Now it's mapped to unct, and you don't know what to do.", "tokens": [823, 309, 311, 33318, 281, 517, 349, 11, 293, 291, 500, 380, 458, 437, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.14646210801710777, "compression_ratio": 1.6374501992031874, "no_speech_prob": 7.243670552270487e-05}, {"id": 89, "seek": 34016, "start": 351.84000000000003, "end": 356.44000000000005, "text": " So an example, Swahili verbs can have hundreds of conjugations.", "tokens": [407, 364, 1365, 11, 3926, 545, 2312, 30051, 393, 362, 6779, 295, 29456, 763, 13], "temperature": 0.0, "avg_logprob": -0.14646210801710777, "compression_ratio": 1.6374501992031874, "no_speech_prob": 7.243670552270487e-05}, {"id": 90, "seek": 34016, "start": 356.44000000000005, "end": 363.16, "text": " So each conjugation encodes important information about the sentence that in English might be", "tokens": [407, 1184, 29456, 399, 2058, 4789, 1021, 1589, 466, 264, 8174, 300, 294, 3669, 1062, 312], "temperature": 0.0, "avg_logprob": -0.14646210801710777, "compression_ratio": 1.6374501992031874, "no_speech_prob": 7.243670552270487e-05}, {"id": 91, "seek": 34016, "start": 363.16, "end": 365.76000000000005, "text": " represented through, say, more words.", "tokens": [10379, 807, 11, 584, 11, 544, 2283, 13], "temperature": 0.0, "avg_logprob": -0.14646210801710777, "compression_ratio": 1.6374501992031874, "no_speech_prob": 7.243670552270487e-05}, {"id": 92, "seek": 36576, "start": 365.76, "end": 371.36, "text": " And Swahili it's mapped onto the verb as prefixes and suffixes, and the like, this is called", "tokens": [400, 3926, 545, 2312, 309, 311, 33318, 3911, 264, 9595, 382, 18417, 36005, 293, 3889, 36005, 11, 293, 264, 411, 11, 341, 307, 1219], "temperature": 0.0, "avg_logprob": -0.18913307810217383, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.1655817615974229e-05}, {"id": 93, "seek": 36576, "start": 371.36, "end": 373.12, "text": " inflectional morphology.", "tokens": [1536, 5450, 304, 25778, 1793, 13], "temperature": 0.0, "avg_logprob": -0.18913307810217383, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.1655817615974229e-05}, {"id": 94, "seek": 36576, "start": 373.12, "end": 374.59999999999997, "text": " And so you can have hundreds of conjugations.", "tokens": [400, 370, 291, 393, 362, 6779, 295, 29456, 763, 13], "temperature": 0.0, "avg_logprob": -0.18913307810217383, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.1655817615974229e-05}, {"id": 95, "seek": 36576, "start": 374.59999999999997, "end": 380.52, "text": " I've just sort of pasted this wick-shenary block just to give you a small sample of just", "tokens": [286, 600, 445, 1333, 295, 1791, 292, 341, 261, 618, 12, 82, 2932, 822, 3461, 445, 281, 976, 291, 257, 1359, 6889, 295, 445], "temperature": 0.0, "avg_logprob": -0.18913307810217383, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.1655817615974229e-05}, {"id": 96, "seek": 36576, "start": 380.52, "end": 382.44, "text": " the huge number of conjugations there are.", "tokens": [264, 2603, 1230, 295, 29456, 763, 456, 366, 13], "temperature": 0.0, "avg_logprob": -0.18913307810217383, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.1655817615974229e-05}, {"id": 97, "seek": 36576, "start": 382.44, "end": 386.84, "text": " And so trying to memorize independently a meaning of each one of these words is just not", "tokens": [400, 370, 1382, 281, 27478, 21761, 257, 3620, 295, 1184, 472, 295, 613, 2283, 307, 445, 406], "temperature": 0.0, "avg_logprob": -0.18913307810217383, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.1655817615974229e-05}, {"id": 98, "seek": 36576, "start": 386.84, "end": 392.03999999999996, "text": " the right answer.", "tokens": [264, 558, 1867, 13], "temperature": 0.0, "avg_logprob": -0.18913307810217383, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.1655817615974229e-05}, {"id": 99, "seek": 36576, "start": 392.03999999999996, "end": 394.76, "text": " So this is going to be a very brief overview.", "tokens": [407, 341, 307, 516, 281, 312, 257, 588, 5353, 12492, 13], "temperature": 0.0, "avg_logprob": -0.18913307810217383, "compression_ratio": 1.641025641025641, "no_speech_prob": 1.1655817615974229e-05}, {"id": 100, "seek": 39476, "start": 394.76, "end": 403.76, "text": " And so what we're going to do is take one, let's say, class of algorithms for sub-word modeling", "tokens": [400, 370, 437, 321, 434, 516, 281, 360, 307, 747, 472, 11, 718, 311, 584, 11, 1508, 295, 14642, 337, 1422, 12, 7462, 15983], "temperature": 0.0, "avg_logprob": -0.21609645808508637, "compression_ratio": 1.6468253968253967, "no_speech_prob": 2.391120324318763e-05}, {"id": 101, "seek": 39476, "start": 403.76, "end": 410.8, "text": " that have been kind of developed to try to take a middle ground between two options.", "tokens": [300, 362, 668, 733, 295, 4743, 281, 853, 281, 747, 257, 2808, 2727, 1296, 732, 3956, 13], "temperature": 0.0, "avg_logprob": -0.21609645808508637, "compression_ratio": 1.6468253968253967, "no_speech_prob": 2.391120324318763e-05}, {"id": 102, "seek": 39476, "start": 410.8, "end": 414.84, "text": " One option is saying everything is just like individual words.", "tokens": [1485, 3614, 307, 1566, 1203, 307, 445, 411, 2609, 2283, 13], "temperature": 0.0, "avg_logprob": -0.21609645808508637, "compression_ratio": 1.6468253968253967, "no_speech_prob": 2.391120324318763e-05}, {"id": 103, "seek": 39476, "start": 414.84, "end": 418.64, "text": " Either I know the word and I saw it at training time, or I don't know the word, and it's like", "tokens": [13746, 286, 458, 264, 1349, 293, 286, 1866, 309, 412, 3097, 565, 11, 420, 286, 500, 380, 458, 264, 1349, 11, 293, 309, 311, 411], "temperature": 0.0, "avg_logprob": -0.21609645808508637, "compression_ratio": 1.6468253968253967, "no_speech_prob": 2.391120324318763e-05}, {"id": 104, "seek": 39476, "start": 418.64, "end": 419.64, "text": " unct.", "tokens": [517, 349, 13], "temperature": 0.0, "avg_logprob": -0.21609645808508637, "compression_ratio": 1.6468253968253967, "no_speech_prob": 2.391120324318763e-05}, {"id": 105, "seek": 39476, "start": 419.64, "end": 423.28, "text": " And then sort of another extreme option is to say it's just characters.", "tokens": [400, 550, 1333, 295, 1071, 8084, 3614, 307, 281, 584, 309, 311, 445, 4342, 13], "temperature": 0.0, "avg_logprob": -0.21609645808508637, "compression_ratio": 1.6468253968253967, "no_speech_prob": 2.391120324318763e-05}, {"id": 106, "seek": 42328, "start": 423.28, "end": 428.15999999999997, "text": " Right? So like I get a sequence of characters, and then my neural network on top of my sequence", "tokens": [1779, 30, 407, 411, 286, 483, 257, 8310, 295, 4342, 11, 293, 550, 452, 18161, 3209, 322, 1192, 295, 452, 8310], "temperature": 0.0, "avg_logprob": -0.1419254127813845, "compression_ratio": 1.6835443037974684, "no_speech_prob": 3.269176522735506e-05}, {"id": 107, "seek": 42328, "start": 428.15999999999997, "end": 433.84, "text": " of characters has to learn everything, has to learn how to combine words and stuff.", "tokens": [295, 4342, 575, 281, 1466, 1203, 11, 575, 281, 1466, 577, 281, 10432, 2283, 293, 1507, 13], "temperature": 0.0, "avg_logprob": -0.1419254127813845, "compression_ratio": 1.6835443037974684, "no_speech_prob": 3.269176522735506e-05}, {"id": 108, "seek": 42328, "start": 433.84, "end": 438.2, "text": " So sub-word models in general just means looking at the sort of internal structure of words", "tokens": [407, 1422, 12, 7462, 5245, 294, 2674, 445, 1355, 1237, 412, 264, 1333, 295, 6920, 3877, 295, 2283], "temperature": 0.0, "avg_logprob": -0.1419254127813845, "compression_ratio": 1.6835443037974684, "no_speech_prob": 3.269176522735506e-05}, {"id": 109, "seek": 42328, "start": 438.2, "end": 440.52, "text": " somehow, looking below the word level.", "tokens": [6063, 11, 1237, 2507, 264, 1349, 1496, 13], "temperature": 0.0, "avg_logprob": -0.1419254127813845, "compression_ratio": 1.6835443037974684, "no_speech_prob": 3.269176522735506e-05}, {"id": 110, "seek": 42328, "start": 440.52, "end": 445.08, "text": " But this group of models is going to try to meet a middle ground.", "tokens": [583, 341, 1594, 295, 5245, 307, 516, 281, 853, 281, 1677, 257, 2808, 2727, 13], "temperature": 0.0, "avg_logprob": -0.1419254127813845, "compression_ratio": 1.6835443037974684, "no_speech_prob": 3.269176522735506e-05}, {"id": 111, "seek": 42328, "start": 445.08, "end": 448.47999999999996, "text": " So byte parent coding.", "tokens": [407, 40846, 2596, 17720, 13], "temperature": 0.0, "avg_logprob": -0.1419254127813845, "compression_ratio": 1.6835443037974684, "no_speech_prob": 3.269176522735506e-05}, {"id": 112, "seek": 44848, "start": 448.48, "end": 453.48, "text": " What we're going to do is we're going to learn a vocabulary from a training data set", "tokens": [708, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1466, 257, 19864, 490, 257, 3097, 1412, 992], "temperature": 0.0, "avg_logprob": -0.14427425677959735, "compression_ratio": 1.87984496124031, "no_speech_prob": 5.062219497631304e-05}, {"id": 113, "seek": 44848, "start": 453.48, "end": 454.48, "text": " again.", "tokens": [797, 13], "temperature": 0.0, "avg_logprob": -0.14427425677959735, "compression_ratio": 1.87984496124031, "no_speech_prob": 5.062219497631304e-05}, {"id": 114, "seek": 44848, "start": 454.48, "end": 455.6, "text": " So now we have a training data set.", "tokens": [407, 586, 321, 362, 257, 3097, 1412, 992, 13], "temperature": 0.0, "avg_logprob": -0.14427425677959735, "compression_ratio": 1.87984496124031, "no_speech_prob": 5.062219497631304e-05}, {"id": 115, "seek": 44848, "start": 455.6, "end": 459.84000000000003, "text": " Instead of just saying, oh, everything that was split by my heuristic word splitter,", "tokens": [7156, 295, 445, 1566, 11, 1954, 11, 1203, 300, 390, 7472, 538, 452, 415, 374, 3142, 1349, 4732, 3904, 11], "temperature": 0.0, "avg_logprob": -0.14427425677959735, "compression_ratio": 1.87984496124031, "no_speech_prob": 5.062219497631304e-05}, {"id": 116, "seek": 44848, "start": 459.84000000000003, "end": 465.84000000000003, "text": " like spaces in English, for example, is going to be a word in my vocabulary, we're going", "tokens": [411, 7673, 294, 3669, 11, 337, 1365, 11, 307, 516, 281, 312, 257, 1349, 294, 452, 19864, 11, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.14427425677959735, "compression_ratio": 1.87984496124031, "no_speech_prob": 5.062219497631304e-05}, {"id": 117, "seek": 44848, "start": 465.84000000000003, "end": 470.48, "text": " to learn the vocabulary using a greedy algorithm in this case.", "tokens": [281, 1466, 264, 19864, 1228, 257, 28228, 9284, 294, 341, 1389, 13], "temperature": 0.0, "avg_logprob": -0.14427425677959735, "compression_ratio": 1.87984496124031, "no_speech_prob": 5.062219497631304e-05}, {"id": 118, "seek": 44848, "start": 470.48, "end": 472.72, "text": " So here's what we're going to do.", "tokens": [407, 510, 311, 437, 321, 434, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.14427425677959735, "compression_ratio": 1.87984496124031, "no_speech_prob": 5.062219497631304e-05}, {"id": 119, "seek": 44848, "start": 472.72, "end": 475.44, "text": " We start with the vocabulary containing only characters.", "tokens": [492, 722, 365, 264, 19864, 19273, 787, 4342, 13], "temperature": 0.0, "avg_logprob": -0.14427425677959735, "compression_ratio": 1.87984496124031, "no_speech_prob": 5.062219497631304e-05}, {"id": 120, "seek": 44848, "start": 475.44, "end": 476.76, "text": " So that's our extreme, right?", "tokens": [407, 300, 311, 527, 8084, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14427425677959735, "compression_ratio": 1.87984496124031, "no_speech_prob": 5.062219497631304e-05}, {"id": 121, "seek": 47676, "start": 476.76, "end": 482.92, "text": " So at the very least, if you've seen all the characters, then you know that you can never", "tokens": [407, 412, 264, 588, 1935, 11, 498, 291, 600, 1612, 439, 264, 4342, 11, 550, 291, 458, 300, 291, 393, 1128], "temperature": 0.0, "avg_logprob": -0.21695700217419722, "compression_ratio": 1.7196969696969697, "no_speech_prob": 3.761075276997872e-05}, {"id": 122, "seek": 47676, "start": 482.92, "end": 483.92, "text": " have an unque, right?", "tokens": [362, 364, 517, 1077, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.21695700217419722, "compression_ratio": 1.7196969696969697, "no_speech_prob": 3.761075276997872e-05}, {"id": 123, "seek": 47676, "start": 483.92, "end": 487.71999999999997, "text": " Because you see a word, you've never seen it before, you just split it into its characters,", "tokens": [1436, 291, 536, 257, 1349, 11, 291, 600, 1128, 1612, 309, 949, 11, 291, 445, 7472, 309, 666, 1080, 4342, 11], "temperature": 0.0, "avg_logprob": -0.21695700217419722, "compression_ratio": 1.7196969696969697, "no_speech_prob": 3.761075276997872e-05}, {"id": 124, "seek": 47676, "start": 487.71999999999997, "end": 491.68, "text": " and then you try to see, you know, deal with it that way.", "tokens": [293, 550, 291, 853, 281, 536, 11, 291, 458, 11, 2028, 365, 309, 300, 636, 13], "temperature": 0.0, "avg_logprob": -0.21695700217419722, "compression_ratio": 1.7196969696969697, "no_speech_prob": 3.761075276997872e-05}, {"id": 125, "seek": 47676, "start": 491.68, "end": 494.08, "text": " And then also an end of word symbol.", "tokens": [400, 550, 611, 364, 917, 295, 1349, 5986, 13], "temperature": 0.0, "avg_logprob": -0.21695700217419722, "compression_ratio": 1.7196969696969697, "no_speech_prob": 3.761075276997872e-05}, {"id": 126, "seek": 47676, "start": 494.08, "end": 495.59999999999997, "text": " And then we'll iterate over this algorithm.", "tokens": [400, 550, 321, 603, 44497, 670, 341, 9284, 13], "temperature": 0.0, "avg_logprob": -0.21695700217419722, "compression_ratio": 1.7196969696969697, "no_speech_prob": 3.761075276997872e-05}, {"id": 127, "seek": 47676, "start": 495.59999999999997, "end": 500.52, "text": " We'll say, use the corpus of text, find common adjacent letters.", "tokens": [492, 603, 584, 11, 764, 264, 1181, 31624, 295, 2487, 11, 915, 2689, 24441, 7825, 13], "temperature": 0.0, "avg_logprob": -0.21695700217419722, "compression_ratio": 1.7196969696969697, "no_speech_prob": 3.761075276997872e-05}, {"id": 128, "seek": 47676, "start": 500.52, "end": 504.32, "text": " So maybe A and B are very frequently adjacent.", "tokens": [407, 1310, 316, 293, 363, 366, 588, 10374, 24441, 13], "temperature": 0.0, "avg_logprob": -0.21695700217419722, "compression_ratio": 1.7196969696969697, "no_speech_prob": 3.761075276997872e-05}, {"id": 129, "seek": 50432, "start": 504.32, "end": 510.52, "text": " And the pair of them together as a single sub word into your vocabulary.", "tokens": [400, 264, 6119, 295, 552, 1214, 382, 257, 2167, 1422, 1349, 666, 428, 19864, 13], "temperature": 0.0, "avg_logprob": -0.16929803291956583, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.885068897739984e-05}, {"id": 130, "seek": 50432, "start": 510.52, "end": 514.4399999999999, "text": " Now replace instances of that character pair with a new sub word repeat until you're desired", "tokens": [823, 7406, 14519, 295, 300, 2517, 6119, 365, 257, 777, 1422, 1349, 7149, 1826, 291, 434, 14721], "temperature": 0.0, "avg_logprob": -0.16929803291956583, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.885068897739984e-05}, {"id": 131, "seek": 50432, "start": 514.4399999999999, "end": 515.4399999999999, "text": " vocabulary size.", "tokens": [19864, 2744, 13], "temperature": 0.0, "avg_logprob": -0.16929803291956583, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.885068897739984e-05}, {"id": 132, "seek": 50432, "start": 515.4399999999999, "end": 521.48, "text": " So maybe you start with a small character vocabulary, and then you end up with that same small", "tokens": [407, 1310, 291, 722, 365, 257, 1359, 2517, 19864, 11, 293, 550, 291, 917, 493, 365, 300, 912, 1359], "temperature": 0.0, "avg_logprob": -0.16929803291956583, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.885068897739984e-05}, {"id": 133, "seek": 50432, "start": 521.48, "end": 527.16, "text": " character vocabulary plus a bunch of sort of entire words or parts of words.", "tokens": [2517, 19864, 1804, 257, 3840, 295, 1333, 295, 2302, 2283, 420, 3166, 295, 2283, 13], "temperature": 0.0, "avg_logprob": -0.16929803291956583, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.885068897739984e-05}, {"id": 134, "seek": 50432, "start": 527.16, "end": 530.4399999999999, "text": " So notice how Apple, an entire word, looks like Apple.", "tokens": [407, 3449, 577, 6373, 11, 364, 2302, 1349, 11, 1542, 411, 6373, 13], "temperature": 0.0, "avg_logprob": -0.16929803291956583, "compression_ratio": 1.8258928571428572, "no_speech_prob": 2.885068897739984e-05}, {"id": 135, "seek": 53044, "start": 530.44, "end": 536.0, "text": " But then app, maybe this is sort of the first part, the first sub word of application, or", "tokens": [583, 550, 724, 11, 1310, 341, 307, 1333, 295, 264, 700, 644, 11, 264, 700, 1422, 1349, 295, 3861, 11, 420], "temperature": 0.0, "avg_logprob": -0.1996019272577195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.3625792234961409e-05}, {"id": 136, "seek": 53044, "start": 536.0, "end": 537.0, "text": " up.", "tokens": [493, 13], "temperature": 0.0, "avg_logprob": -0.1996019272577195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.3625792234961409e-05}, {"id": 137, "seek": 53044, "start": 537.0, "end": 539.32, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.1996019272577195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.3625792234961409e-05}, {"id": 138, "seek": 53044, "start": 539.32, "end": 547.0400000000001, "text": " And then Lee, I guess I should have not put the hash there, but you know, maybe you learned", "tokens": [400, 550, 6957, 11, 286, 2041, 286, 820, 362, 406, 829, 264, 22019, 456, 11, 457, 291, 458, 11, 1310, 291, 3264], "temperature": 0.0, "avg_logprob": -0.1996019272577195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.3625792234961409e-05}, {"id": 139, "seek": 53044, "start": 547.0400000000001, "end": 550.72, "text": " Lee as like the end of a word, for example.", "tokens": [6957, 382, 411, 264, 917, 295, 257, 1349, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1996019272577195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.3625792234961409e-05}, {"id": 140, "seek": 53044, "start": 550.72, "end": 556.48, "text": " And so what you end up with is, you know, a vocabulary where common things you get to", "tokens": [400, 370, 437, 291, 917, 493, 365, 307, 11, 291, 458, 11, 257, 19864, 689, 2689, 721, 291, 483, 281], "temperature": 0.0, "avg_logprob": -0.1996019272577195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.3625792234961409e-05}, {"id": 141, "seek": 53044, "start": 556.48, "end": 560.12, "text": " map to themselves and then rare sequences of characters.", "tokens": [4471, 281, 2969, 293, 550, 5892, 22978, 295, 4342, 13], "temperature": 0.0, "avg_logprob": -0.1996019272577195, "compression_ratio": 1.6363636363636365, "no_speech_prob": 1.3625792234961409e-05}, {"id": 142, "seek": 56012, "start": 560.12, "end": 563.12, "text": " You kind of split as little as possible.", "tokens": [509, 733, 295, 7472, 382, 707, 382, 1944, 13], "temperature": 0.0, "avg_logprob": -0.17483155302299086, "compression_ratio": 1.6722972972972974, "no_speech_prob": 5.0627968448679894e-05}, {"id": 143, "seek": 56012, "start": 563.12, "end": 567.44, "text": " And it doesn't always end up so nicely that you learn like morphologically relevant suffixes", "tokens": [400, 309, 1177, 380, 1009, 917, 493, 370, 9594, 300, 291, 1466, 411, 25778, 17157, 7340, 3889, 36005], "temperature": 0.0, "avg_logprob": -0.17483155302299086, "compression_ratio": 1.6722972972972974, "no_speech_prob": 5.0627968448679894e-05}, {"id": 144, "seek": 56012, "start": 567.44, "end": 569.0, "text": " like Lee.", "tokens": [411, 6957, 13], "temperature": 0.0, "avg_logprob": -0.17483155302299086, "compression_ratio": 1.6722972972972974, "no_speech_prob": 5.0627968448679894e-05}, {"id": 145, "seek": 56012, "start": 569.0, "end": 572.76, "text": " But you can, you know, try to split things somewhat reasonably.", "tokens": [583, 291, 393, 11, 291, 458, 11, 853, 281, 7472, 721, 8344, 23551, 13], "temperature": 0.0, "avg_logprob": -0.17483155302299086, "compression_ratio": 1.6722972972972974, "no_speech_prob": 5.0627968448679894e-05}, {"id": 146, "seek": 56012, "start": 572.76, "end": 577.24, "text": " And if you have enough data, the sub word vocabulary you learn tends to be okay.", "tokens": [400, 498, 291, 362, 1547, 1412, 11, 264, 1422, 1349, 19864, 291, 1466, 12258, 281, 312, 1392, 13], "temperature": 0.0, "avg_logprob": -0.17483155302299086, "compression_ratio": 1.6722972972972974, "no_speech_prob": 5.0627968448679894e-05}, {"id": 147, "seek": 56012, "start": 577.24, "end": 580.4, "text": " So this is originally used in machine translation.", "tokens": [407, 341, 307, 7993, 1143, 294, 3479, 12853, 13], "temperature": 0.0, "avg_logprob": -0.17483155302299086, "compression_ratio": 1.6722972972972974, "no_speech_prob": 5.0627968448679894e-05}, {"id": 148, "seek": 56012, "start": 580.4, "end": 585.08, "text": " And now a similar method, word piece, which we won't go over in this lecture is used", "tokens": [400, 586, 257, 2531, 3170, 11, 1349, 2522, 11, 597, 321, 1582, 380, 352, 670, 294, 341, 7991, 307, 1143], "temperature": 0.0, "avg_logprob": -0.17483155302299086, "compression_ratio": 1.6722972972972974, "no_speech_prob": 5.0627968448679894e-05}, {"id": 149, "seek": 56012, "start": 585.08, "end": 586.28, "text": " in pre-trained models.", "tokens": [294, 659, 12, 17227, 2001, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17483155302299086, "compression_ratio": 1.6722972972972974, "no_speech_prob": 5.0627968448679894e-05}, {"id": 150, "seek": 56012, "start": 586.28, "end": 588.2, "text": " But you know, the idea is effectively the same.", "tokens": [583, 291, 458, 11, 264, 1558, 307, 8659, 264, 912, 13], "temperature": 0.0, "avg_logprob": -0.17483155302299086, "compression_ratio": 1.6722972972972974, "no_speech_prob": 5.0627968448679894e-05}, {"id": 151, "seek": 58820, "start": 588.2, "end": 590.8000000000001, "text": " And you end up with vocabularies that look a lot like this.", "tokens": [400, 291, 917, 493, 365, 2329, 455, 1040, 530, 300, 574, 257, 688, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17677571950865187, "compression_ratio": 1.7918367346938775, "no_speech_prob": 1.3209089047450107e-05}, {"id": 152, "seek": 58820, "start": 590.8000000000001, "end": 598.12, "text": " So if we go back to our, if we go back to our examples of where, you know, word level", "tokens": [407, 498, 321, 352, 646, 281, 527, 11, 498, 321, 352, 646, 281, 527, 5110, 295, 689, 11, 291, 458, 11, 1349, 1496], "temperature": 0.0, "avg_logprob": -0.17677571950865187, "compression_ratio": 1.7918367346938775, "no_speech_prob": 1.3209089047450107e-05}, {"id": 153, "seek": 58820, "start": 598.12, "end": 604.6400000000001, "text": " NLP was failing us, then you have hat mapping to hat.", "tokens": [426, 45196, 390, 18223, 505, 11, 550, 291, 362, 2385, 18350, 281, 2385, 13], "temperature": 0.0, "avg_logprob": -0.17677571950865187, "compression_ratio": 1.7918367346938775, "no_speech_prob": 1.3209089047450107e-05}, {"id": 154, "seek": 58820, "start": 604.6400000000001, "end": 605.6400000000001, "text": " Okay, that's good.", "tokens": [1033, 11, 300, 311, 665, 13], "temperature": 0.0, "avg_logprob": -0.17677571950865187, "compression_ratio": 1.7918367346938775, "no_speech_prob": 1.3209089047450107e-05}, {"id": 155, "seek": 58820, "start": 605.6400000000001, "end": 609.9200000000001, "text": " You have hat mapping to hat because that was a common enough sequence of characters that", "tokens": [509, 362, 2385, 18350, 281, 2385, 570, 300, 390, 257, 2689, 1547, 8310, 295, 4342, 300], "temperature": 0.0, "avg_logprob": -0.17677571950865187, "compression_ratio": 1.7918367346938775, "no_speech_prob": 1.3209089047450107e-05}, {"id": 156, "seek": 58820, "start": 609.9200000000001, "end": 612.72, "text": " it was actually incorporated into our sub word vocabulary.", "tokens": [309, 390, 767, 21654, 666, 527, 1422, 1349, 19864, 13], "temperature": 0.0, "avg_logprob": -0.17677571950865187, "compression_ratio": 1.7918367346938775, "no_speech_prob": 1.3209089047450107e-05}, {"id": 157, "seek": 58820, "start": 612.72, "end": 613.72, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.17677571950865187, "compression_ratio": 1.7918367346938775, "no_speech_prob": 1.3209089047450107e-05}, {"id": 158, "seek": 58820, "start": 613.72, "end": 615.0, "text": " And then you have learned mapping to learn.", "tokens": [400, 550, 291, 362, 3264, 18350, 281, 1466, 13], "temperature": 0.0, "avg_logprob": -0.17677571950865187, "compression_ratio": 1.7918367346938775, "no_speech_prob": 1.3209089047450107e-05}, {"id": 159, "seek": 58820, "start": 615.0, "end": 616.9200000000001, "text": " So common words good.", "tokens": [407, 2689, 2283, 665, 13], "temperature": 0.0, "avg_logprob": -0.17677571950865187, "compression_ratio": 1.7918367346938775, "no_speech_prob": 1.3209089047450107e-05}, {"id": 160, "seek": 61692, "start": 616.92, "end": 620.28, "text": " And that means that the model, the neural network that you're going to process this text", "tokens": [400, 300, 1355, 300, 264, 2316, 11, 264, 18161, 3209, 300, 291, 434, 516, 281, 1399, 341, 2487], "temperature": 0.0, "avg_logprob": -0.1574129007630429, "compression_ratio": 1.7207547169811321, "no_speech_prob": 4.6831231884425506e-05}, {"id": 161, "seek": 61692, "start": 620.28, "end": 627.0799999999999, "text": " with does not need to, say, combine the letters of learn and hat in order to try to like", "tokens": [365, 775, 406, 643, 281, 11, 584, 11, 10432, 264, 7825, 295, 1466, 293, 2385, 294, 1668, 281, 853, 281, 411], "temperature": 0.0, "avg_logprob": -0.1574129007630429, "compression_ratio": 1.7207547169811321, "no_speech_prob": 4.6831231884425506e-05}, {"id": 162, "seek": 61692, "start": 627.0799999999999, "end": 631.24, "text": " derive the meaning of these words from the letters, because you can imagine that might", "tokens": [28446, 264, 3620, 295, 613, 2283, 490, 264, 7825, 11, 570, 291, 393, 3811, 300, 1062], "temperature": 0.0, "avg_logprob": -0.1574129007630429, "compression_ratio": 1.7207547169811321, "no_speech_prob": 4.6831231884425506e-05}, {"id": 163, "seek": 61692, "start": 631.24, "end": 632.24, "text": " be difficult.", "tokens": [312, 2252, 13], "temperature": 0.0, "avg_logprob": -0.1574129007630429, "compression_ratio": 1.7207547169811321, "no_speech_prob": 4.6831231884425506e-05}, {"id": 164, "seek": 61692, "start": 632.24, "end": 638.0, "text": " But then when you get a word that you have not seen before, you are able to decompose", "tokens": [583, 550, 562, 291, 483, 257, 1349, 300, 291, 362, 406, 1612, 949, 11, 291, 366, 1075, 281, 22867, 541], "temperature": 0.0, "avg_logprob": -0.1574129007630429, "compression_ratio": 1.7207547169811321, "no_speech_prob": 4.6831231884425506e-05}, {"id": 165, "seek": 61692, "start": 638.0, "end": 639.4, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.1574129007630429, "compression_ratio": 1.7207547169811321, "no_speech_prob": 4.6831231884425506e-05}, {"id": 166, "seek": 61692, "start": 639.4, "end": 645.88, "text": " And so if you've seen tasty with varying numbers of A's at, at training time, you know,", "tokens": [400, 370, 498, 291, 600, 1612, 11535, 365, 22984, 3547, 295, 316, 311, 412, 11, 412, 3097, 565, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.1574129007630429, "compression_ratio": 1.7207547169811321, "no_speech_prob": 4.6831231884425506e-05}, {"id": 167, "seek": 64588, "start": 645.88, "end": 650.4, "text": " maybe you actually get some of the same sub words or similar sub words that you're splitting", "tokens": [1310, 291, 767, 483, 512, 295, 264, 912, 1422, 2283, 420, 2531, 1422, 2283, 300, 291, 434, 30348], "temperature": 0.0, "avg_logprob": -0.15118317855031868, "compression_ratio": 1.7718120805369129, "no_speech_prob": 4.756013004225679e-05}, {"id": 168, "seek": 64588, "start": 650.4, "end": 652.28, "text": " it into at evaluation time.", "tokens": [309, 666, 412, 13344, 565, 13], "temperature": 0.0, "avg_logprob": -0.15118317855031868, "compression_ratio": 1.7718120805369129, "no_speech_prob": 4.756013004225679e-05}, {"id": 169, "seek": 64588, "start": 652.28, "end": 656.72, "text": " So we never saw tasty enough to like, you know, however many A's in order to add it into", "tokens": [407, 321, 1128, 1866, 11535, 1547, 281, 411, 11, 291, 458, 11, 4461, 867, 316, 311, 294, 1668, 281, 909, 309, 666], "temperature": 0.0, "avg_logprob": -0.15118317855031868, "compression_ratio": 1.7718120805369129, "no_speech_prob": 4.756013004225679e-05}, {"id": 170, "seek": 64588, "start": 656.72, "end": 658.88, "text": " a sub word vocabulary.", "tokens": [257, 1422, 1349, 19864, 13], "temperature": 0.0, "avg_logprob": -0.15118317855031868, "compression_ratio": 1.7718120805369129, "no_speech_prob": 4.756013004225679e-05}, {"id": 171, "seek": 64588, "start": 658.88, "end": 661.04, "text": " But we're still able to split it into things.", "tokens": [583, 321, 434, 920, 1075, 281, 7472, 309, 666, 721, 13], "temperature": 0.0, "avg_logprob": -0.15118317855031868, "compression_ratio": 1.7718120805369129, "no_speech_prob": 4.756013004225679e-05}, {"id": 172, "seek": 64588, "start": 661.04, "end": 665.2, "text": " And then the neural network that runs on top of these sub word embeddings could be able", "tokens": [400, 550, 264, 18161, 3209, 300, 6676, 322, 1192, 295, 613, 1422, 1349, 12240, 29432, 727, 312, 1075], "temperature": 0.0, "avg_logprob": -0.15118317855031868, "compression_ratio": 1.7718120805369129, "no_speech_prob": 4.756013004225679e-05}, {"id": 173, "seek": 64588, "start": 665.2, "end": 670.16, "text": " to sort of induce that, oh, yeah, this is one of those things where people like, you know,", "tokens": [281, 1333, 295, 41263, 300, 11, 1954, 11, 1338, 11, 341, 307, 472, 295, 729, 721, 689, 561, 411, 11, 291, 458, 11], "temperature": 0.0, "avg_logprob": -0.15118317855031868, "compression_ratio": 1.7718120805369129, "no_speech_prob": 4.756013004225679e-05}, {"id": 174, "seek": 64588, "start": 670.16, "end": 675.28, "text": " chain letters together, chain vowels together in English for emphasis.", "tokens": [5021, 7825, 1214, 11, 5021, 44972, 1214, 294, 3669, 337, 16271, 13], "temperature": 0.0, "avg_logprob": -0.15118317855031868, "compression_ratio": 1.7718120805369129, "no_speech_prob": 4.756013004225679e-05}, {"id": 175, "seek": 67528, "start": 675.28, "end": 678.36, "text": " So misspellings still pretty much mess you up.", "tokens": [407, 1713, 49241, 1109, 920, 1238, 709, 2082, 291, 493, 13], "temperature": 0.0, "avg_logprob": -0.1694472763273451, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.8222195144044235e-05}, {"id": 176, "seek": 67528, "start": 678.36, "end": 682.88, "text": " So now learn with this misspelling might be mapped to two sub words.", "tokens": [407, 586, 1466, 365, 341, 1713, 494, 2669, 1062, 312, 33318, 281, 732, 1422, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1694472763273451, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.8222195144044235e-05}, {"id": 177, "seek": 67528, "start": 682.88, "end": 687.76, "text": " But if you saw misspellings like this frequently enough, maybe you could learn sort of to handle", "tokens": [583, 498, 291, 1866, 1713, 49241, 1109, 411, 341, 10374, 1547, 11, 1310, 291, 727, 1466, 1333, 295, 281, 4813], "temperature": 0.0, "avg_logprob": -0.1694472763273451, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.8222195144044235e-05}, {"id": 178, "seek": 67528, "start": 687.76, "end": 688.76, "text": " it.", "tokens": [309, 13], "temperature": 0.0, "avg_logprob": -0.1694472763273451, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.8222195144044235e-05}, {"id": 179, "seek": 67528, "start": 688.76, "end": 691.12, "text": " It still messes up the model though.", "tokens": [467, 920, 2082, 279, 493, 264, 2316, 1673, 13], "temperature": 0.0, "avg_logprob": -0.1694472763273451, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.8222195144044235e-05}, {"id": 180, "seek": 67528, "start": 691.12, "end": 693.8399999999999, "text": " And, but at the very least, it's not just an umk, right?", "tokens": [400, 11, 457, 412, 264, 588, 1935, 11, 309, 311, 406, 445, 364, 1105, 74, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1694472763273451, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.8222195144044235e-05}, {"id": 181, "seek": 67528, "start": 693.8399999999999, "end": 695.9599999999999, "text": " It seems clearly better than that.", "tokens": [467, 2544, 4448, 1101, 813, 300, 13], "temperature": 0.0, "avg_logprob": -0.1694472763273451, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.8222195144044235e-05}, {"id": 182, "seek": 67528, "start": 695.9599999999999, "end": 700.4399999999999, "text": " And then transformer, if I, maybe in the best, this is sort of optimistic, but maybe", "tokens": [400, 550, 31782, 11, 498, 286, 11, 1310, 294, 264, 1151, 11, 341, 307, 1333, 295, 19397, 11, 457, 1310], "temperature": 0.0, "avg_logprob": -0.1694472763273451, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.8222195144044235e-05}, {"id": 183, "seek": 67528, "start": 700.4399999999999, "end": 704.88, "text": " in the best case, right, you were able to say, ah, yes, this is transformer.", "tokens": [294, 264, 1151, 1389, 11, 558, 11, 291, 645, 1075, 281, 584, 11, 3716, 11, 2086, 11, 341, 307, 31782, 13], "temperature": 0.0, "avg_logprob": -0.1694472763273451, "compression_ratio": 1.7727272727272727, "no_speech_prob": 3.8222195144044235e-05}, {"id": 184, "seek": 70488, "start": 704.88, "end": 711.32, "text": " And if I, again, the sub words that you learn don't actually tend to be this well morphologically", "tokens": [400, 498, 286, 11, 797, 11, 264, 1422, 2283, 300, 291, 1466, 500, 380, 767, 3928, 281, 312, 341, 731, 25778, 17157], "temperature": 0.0, "avg_logprob": -0.14859329405285063, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.8337097571929917e-05}, {"id": 185, "seek": 70488, "start": 711.32, "end": 712.6, "text": " motivated, I think.", "tokens": [14515, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.14859329405285063, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.8337097571929917e-05}, {"id": 186, "seek": 70488, "start": 712.6, "end": 718.4, "text": " So if I is like a clear, like suffix in English that has a very common and replicable meaning", "tokens": [407, 498, 286, 307, 411, 257, 1850, 11, 411, 3889, 970, 294, 3669, 300, 575, 257, 588, 2689, 293, 3248, 43023, 3620], "temperature": 0.0, "avg_logprob": -0.14859329405285063, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.8337097571929917e-05}, {"id": 187, "seek": 70488, "start": 718.4, "end": 722.92, "text": " when you apply it to nouns, that's derivational morphology.", "tokens": [562, 291, 3079, 309, 281, 48184, 11, 300, 311, 10151, 1478, 25778, 1793, 13], "temperature": 0.0, "avg_logprob": -0.14859329405285063, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.8337097571929917e-05}, {"id": 188, "seek": 70488, "start": 722.92, "end": 727.2, "text": " But you know, you're able to sort of compose the word of the meaning of transformer if I", "tokens": [583, 291, 458, 11, 291, 434, 1075, 281, 1333, 295, 35925, 264, 1349, 295, 264, 3620, 295, 31782, 498, 286], "temperature": 0.0, "avg_logprob": -0.14859329405285063, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.8337097571929917e-05}, {"id": 189, "seek": 70488, "start": 727.2, "end": 731.04, "text": " possibly from its two sub word constituents.", "tokens": [6264, 490, 1080, 732, 1422, 1349, 30847, 13], "temperature": 0.0, "avg_logprob": -0.14859329405285063, "compression_ratio": 1.6071428571428572, "no_speech_prob": 1.8337097571929917e-05}, {"id": 190, "seek": 73104, "start": 731.04, "end": 735.56, "text": " And so when we talk about words being input to transformer models, pre-trained transformer", "tokens": [400, 370, 562, 321, 751, 466, 2283, 885, 4846, 281, 31782, 5245, 11, 659, 12, 17227, 2001, 31782], "temperature": 0.0, "avg_logprob": -0.1381183340529765, "compression_ratio": 1.876984126984127, "no_speech_prob": 7.766355338389985e-06}, {"id": 191, "seek": 73104, "start": 735.56, "end": 740.52, "text": " models, throughout the entirety of this lecture, we will be talking about sub words.", "tokens": [5245, 11, 3710, 264, 31557, 295, 341, 7991, 11, 321, 486, 312, 1417, 466, 1422, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1381183340529765, "compression_ratio": 1.876984126984127, "no_speech_prob": 7.766355338389985e-06}, {"id": 192, "seek": 73104, "start": 740.52, "end": 746.76, "text": " So I might say word, and what I mean is, you know, possibly a full word, also possibly", "tokens": [407, 286, 1062, 584, 1349, 11, 293, 437, 286, 914, 307, 11, 291, 458, 11, 6264, 257, 1577, 1349, 11, 611, 6264], "temperature": 0.0, "avg_logprob": -0.1381183340529765, "compression_ratio": 1.876984126984127, "no_speech_prob": 7.766355338389985e-06}, {"id": 193, "seek": 73104, "start": 746.76, "end": 747.76, "text": " a sub word.", "tokens": [257, 1422, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1381183340529765, "compression_ratio": 1.876984126984127, "no_speech_prob": 7.766355338389985e-06}, {"id": 194, "seek": 73104, "start": 747.76, "end": 751.28, "text": " Okay, so when we say a sequence of words, the transformer, the pre-trained transformer", "tokens": [1033, 11, 370, 562, 321, 584, 257, 8310, 295, 2283, 11, 264, 31782, 11, 264, 659, 12, 17227, 2001, 31782], "temperature": 0.0, "avg_logprob": -0.1381183340529765, "compression_ratio": 1.876984126984127, "no_speech_prob": 7.766355338389985e-06}, {"id": 195, "seek": 73104, "start": 751.28, "end": 757.88, "text": " has no idea, sort of whether it's dealing with words or sub words, when it's doing itself", "tokens": [575, 572, 1558, 11, 1333, 295, 1968, 309, 311, 6260, 365, 2283, 420, 1422, 2283, 11, 562, 309, 311, 884, 2564], "temperature": 0.0, "avg_logprob": -0.1381183340529765, "compression_ratio": 1.876984126984127, "no_speech_prob": 7.766355338389985e-06}, {"id": 196, "seek": 73104, "start": 757.88, "end": 760.36, "text": " attention operations.", "tokens": [3202, 7705, 13], "temperature": 0.0, "avg_logprob": -0.1381183340529765, "compression_ratio": 1.876984126984127, "no_speech_prob": 7.766355338389985e-06}, {"id": 197, "seek": 76036, "start": 760.36, "end": 761.84, "text": " And so this can be a problem.", "tokens": [400, 370, 341, 393, 312, 257, 1154, 13], "temperature": 0.0, "avg_logprob": -0.1652362769377147, "compression_ratio": 1.9014084507042253, "no_speech_prob": 1.0949786883429624e-05}, {"id": 198, "seek": 76036, "start": 761.84, "end": 766.8000000000001, "text": " You can imagine if you have really weird sequences of characters, you can actually have an individual", "tokens": [509, 393, 3811, 498, 291, 362, 534, 3657, 22978, 295, 4342, 11, 291, 393, 767, 362, 364, 2609], "temperature": 0.0, "avg_logprob": -0.1652362769377147, "compression_ratio": 1.9014084507042253, "no_speech_prob": 1.0949786883429624e-05}, {"id": 199, "seek": 76036, "start": 766.8000000000001, "end": 771.52, "text": " single word mapped to as many sub words as it has characters.", "tokens": [2167, 1349, 33318, 281, 382, 867, 1422, 2283, 382, 309, 575, 4342, 13], "temperature": 0.0, "avg_logprob": -0.1652362769377147, "compression_ratio": 1.9014084507042253, "no_speech_prob": 1.0949786883429624e-05}, {"id": 200, "seek": 76036, "start": 771.52, "end": 775.32, "text": " That can be a problem because suddenly, you know, you have a ten-word sentence, but one", "tokens": [663, 393, 312, 257, 1154, 570, 5800, 11, 291, 458, 11, 291, 362, 257, 2064, 12, 7462, 8174, 11, 457, 472], "temperature": 0.0, "avg_logprob": -0.1652362769377147, "compression_ratio": 1.9014084507042253, "no_speech_prob": 1.0949786883429624e-05}, {"id": 201, "seek": 76036, "start": 775.32, "end": 778.84, "text": " of the words is mapped to, you know, twenty sub words.", "tokens": [295, 264, 2283, 307, 33318, 281, 11, 291, 458, 11, 7699, 1422, 2283, 13], "temperature": 0.0, "avg_logprob": -0.1652362769377147, "compression_ratio": 1.9014084507042253, "no_speech_prob": 1.0949786883429624e-05}, {"id": 202, "seek": 76036, "start": 778.84, "end": 782.8000000000001, "text": " Now you have a thirty-word sentence, where twenty of the thirty words are just one real", "tokens": [823, 291, 362, 257, 11790, 12, 7462, 8174, 11, 689, 7699, 295, 264, 11790, 2283, 366, 445, 472, 957], "temperature": 0.0, "avg_logprob": -0.1652362769377147, "compression_ratio": 1.9014084507042253, "no_speech_prob": 1.0949786883429624e-05}, {"id": 203, "seek": 76036, "start": 782.8000000000001, "end": 783.96, "text": " word.", "tokens": [1349, 13], "temperature": 0.0, "avg_logprob": -0.1652362769377147, "compression_ratio": 1.9014084507042253, "no_speech_prob": 1.0949786883429624e-05}, {"id": 204, "seek": 76036, "start": 783.96, "end": 785.24, "text": " So keep this in mind.", "tokens": [407, 1066, 341, 294, 1575, 13], "temperature": 0.0, "avg_logprob": -0.1652362769377147, "compression_ratio": 1.9014084507042253, "no_speech_prob": 1.0949786883429624e-05}, {"id": 205, "seek": 76036, "start": 785.24, "end": 789.72, "text": " But, you know, I think it's important for sort of this open vocabulary assumption, it's", "tokens": [583, 11, 291, 458, 11, 286, 519, 309, 311, 1021, 337, 1333, 295, 341, 1269, 19864, 15302, 11, 309, 311], "temperature": 0.0, "avg_logprob": -0.1652362769377147, "compression_ratio": 1.9014084507042253, "no_speech_prob": 1.0949786883429624e-05}, {"id": 206, "seek": 78972, "start": 789.72, "end": 795.24, "text": " important in English, and it's even more important in many other languages.", "tokens": [1021, 294, 3669, 11, 293, 309, 311, 754, 544, 1021, 294, 867, 661, 8650, 13], "temperature": 0.0, "avg_logprob": -0.3225518740140475, "compression_ratio": 1.6317991631799162, "no_speech_prob": 4.329052899265662e-05}, {"id": 207, "seek": 78972, "start": 795.24, "end": 798.0, "text": " And the actual algorithm, and you can go into the actual algorithms that are done for", "tokens": [400, 264, 3539, 9284, 11, 293, 291, 393, 352, 666, 264, 3539, 14642, 300, 366, 1096, 337], "temperature": 0.0, "avg_logprob": -0.3225518740140475, "compression_ratio": 1.6317991631799162, "no_speech_prob": 4.329052899265662e-05}, {"id": 208, "seek": 78972, "start": 798.0, "end": 803.96, "text": " this, byte per encoding is sort of my favorite for going over briefly, word piece you can", "tokens": [341, 11, 40846, 680, 43430, 307, 1333, 295, 452, 2954, 337, 516, 670, 10515, 11, 1349, 2522, 291, 393], "temperature": 0.0, "avg_logprob": -0.3225518740140475, "compression_ratio": 1.6317991631799162, "no_speech_prob": 4.329052899265662e-05}, {"id": 209, "seek": 78972, "start": 803.96, "end": 806.12, "text": " also take a look at.", "tokens": [611, 747, 257, 574, 412, 13], "temperature": 0.0, "avg_logprob": -0.3225518740140475, "compression_ratio": 1.6317991631799162, "no_speech_prob": 4.329052899265662e-05}, {"id": 210, "seek": 78972, "start": 806.12, "end": 807.4, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.3225518740140475, "compression_ratio": 1.6317991631799162, "no_speech_prob": 4.329052899265662e-05}, {"id": 211, "seek": 78972, "start": 807.4, "end": 809.4, "text": " Any questions on sub words?", "tokens": [2639, 1651, 322, 1422, 2283, 30], "temperature": 0.0, "avg_logprob": -0.3225518740140475, "compression_ratio": 1.6317991631799162, "no_speech_prob": 4.329052899265662e-05}, {"id": 212, "seek": 78972, "start": 809.4, "end": 814.2, "text": " I guess John, let me look after what does the hashtag mean?", "tokens": [286, 2041, 2619, 11, 718, 385, 574, 934, 437, 775, 264, 20379, 914, 30], "temperature": 0.0, "avg_logprob": -0.3225518740140475, "compression_ratio": 1.6317991631799162, "no_speech_prob": 4.329052899265662e-05}, {"id": 213, "seek": 78972, "start": 814.2, "end": 816.08, "text": " Oh, great, great point.", "tokens": [876, 11, 869, 11, 869, 935, 13], "temperature": 0.0, "avg_logprob": -0.3225518740140475, "compression_ratio": 1.6317991631799162, "no_speech_prob": 4.329052899265662e-05}, {"id": 214, "seek": 81608, "start": 816.08, "end": 820.4000000000001, "text": " So this means that you should be combining this sub word, so this sub word is not the", "tokens": [407, 341, 1355, 300, 291, 820, 312, 21928, 341, 1422, 1349, 11, 370, 341, 1422, 1349, 307, 406, 264], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 215, "seek": 81608, "start": 820.4000000000001, "end": 821.4000000000001, "text": " end of a word.", "tokens": [917, 295, 257, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 216, "seek": 81608, "start": 821.4000000000001, "end": 825.2800000000001, "text": " TAA, hash hash, is sort of telling the model.", "tokens": [314, 5265, 11, 22019, 22019, 11, 307, 1333, 295, 3585, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 217, "seek": 81608, "start": 825.2800000000001, "end": 830.2, "text": " So if I had TAA with no hashes, that's a separate sub word.", "tokens": [407, 498, 286, 632, 314, 5265, 365, 572, 575, 8076, 11, 300, 311, 257, 4994, 1422, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 218, "seek": 81608, "start": 830.2, "end": 834.44, "text": " That means there's an entire word that is ta, or at the very least it's not the end of", "tokens": [663, 1355, 456, 311, 364, 2302, 1349, 300, 307, 1846, 11, 420, 412, 264, 588, 1935, 309, 311, 406, 264, 917, 295], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 219, "seek": 81608, "start": 834.44, "end": 835.44, "text": " the word.", "tokens": [264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 220, "seek": 81608, "start": 835.44, "end": 836.44, "text": " See how here?", "tokens": [3008, 577, 510, 30], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 221, "seek": 81608, "start": 836.44, "end": 837.9200000000001, "text": " I don't have the hashes at the end.", "tokens": [286, 500, 380, 362, 264, 575, 8076, 412, 264, 917, 13], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 222, "seek": 81608, "start": 837.9200000000001, "end": 840.84, "text": " It's because this is indicating that this is at the end of the word.", "tokens": [467, 311, 570, 341, 307, 25604, 300, 341, 307, 412, 264, 917, 295, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 223, "seek": 81608, "start": 840.84, "end": 844.76, "text": " Different sub word schemes differ on whether you should put something at the beginning of", "tokens": [20825, 1422, 1349, 26954, 743, 322, 1968, 291, 820, 829, 746, 412, 264, 2863, 295], "temperature": 0.0, "avg_logprob": -0.14848323770471522, "compression_ratio": 1.9176029962546817, "no_speech_prob": 4.9082718760473654e-05}, {"id": 224, "seek": 84476, "start": 844.76, "end": 848.68, "text": " the word, if it does begin a word, or if you should put something at the end of the", "tokens": [264, 1349, 11, 498, 309, 775, 1841, 257, 1349, 11, 420, 498, 291, 820, 829, 746, 412, 264, 917, 295, 264], "temperature": 0.0, "avg_logprob": -0.2211498200424074, "compression_ratio": 2.072463768115942, "no_speech_prob": 2.8403708711266518e-05}, {"id": 225, "seek": 84476, "start": 848.68, "end": 851.24, "text": " word, if it doesn't end the word.", "tokens": [1349, 11, 498, 309, 1177, 380, 917, 264, 1349, 13], "temperature": 0.0, "avg_logprob": -0.2211498200424074, "compression_ratio": 2.072463768115942, "no_speech_prob": 2.8403708711266518e-05}, {"id": 226, "seek": 84476, "start": 851.24, "end": 855.8, "text": " So when the tokenizer is running over your data, so you've got something that's tokenizing", "tokens": [407, 562, 264, 14862, 6545, 307, 2614, 670, 428, 1412, 11, 370, 291, 600, 658, 746, 300, 311, 14862, 3319], "temperature": 0.0, "avg_logprob": -0.2211498200424074, "compression_ratio": 2.072463768115942, "no_speech_prob": 2.8403708711266518e-05}, {"id": 227, "seek": 84476, "start": 855.8, "end": 860.76, "text": " this sentence in the worst case.", "tokens": [341, 8174, 294, 264, 5855, 1389, 13], "temperature": 0.0, "avg_logprob": -0.2211498200424074, "compression_ratio": 2.072463768115942, "no_speech_prob": 2.8403708711266518e-05}, {"id": 228, "seek": 84476, "start": 860.76, "end": 867.04, "text": " In the worst case, it says, in, that's a whole word, give it just the word in, no hashes,", "tokens": [682, 264, 5855, 1389, 11, 309, 1619, 11, 294, 11, 300, 311, 257, 1379, 1349, 11, 976, 309, 445, 264, 1349, 294, 11, 572, 575, 8076, 11], "temperature": 0.0, "avg_logprob": -0.2211498200424074, "compression_ratio": 2.072463768115942, "no_speech_prob": 2.8403708711266518e-05}, {"id": 229, "seek": 84476, "start": 867.04, "end": 872.72, "text": " that's a whole word, give it just the word the, no hashes, and then maybe over here at", "tokens": [300, 311, 257, 1379, 1349, 11, 976, 309, 445, 264, 1349, 264, 11, 572, 575, 8076, 11, 293, 550, 1310, 670, 510, 412], "temperature": 0.0, "avg_logprob": -0.2211498200424074, "compression_ratio": 2.072463768115942, "no_speech_prob": 2.8403708711266518e-05}, {"id": 230, "seek": 84476, "start": 872.72, "end": 874.24, "text": " sub words.", "tokens": [1422, 2283, 13], "temperature": 0.0, "avg_logprob": -0.2211498200424074, "compression_ratio": 2.072463768115942, "no_speech_prob": 2.8403708711266518e-05}, {"id": 231, "seek": 87424, "start": 874.24, "end": 879.16, "text": " We've got this weird word sub words, and it splits it into sub and words.", "tokens": [492, 600, 658, 341, 3657, 1349, 1422, 2283, 11, 293, 309, 37741, 309, 666, 1422, 293, 2283, 13], "temperature": 0.0, "avg_logprob": -0.24081820319680608, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.0226059607230127e-05}, {"id": 232, "seek": 87424, "start": 879.16, "end": 885.76, "text": " And so sub, it's going to give it the sub word with sub hash hash to indicate that it's", "tokens": [400, 370, 1422, 11, 309, 311, 516, 281, 976, 309, 264, 1422, 1349, 365, 459, 65, 22019, 22019, 281, 13330, 300, 309, 311], "temperature": 0.0, "avg_logprob": -0.24081820319680608, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.0226059607230127e-05}, {"id": 233, "seek": 87424, "start": 885.76, "end": 892.92, "text": " part of this larger word, sub words, as opposed to the word sub, like submarine, which would", "tokens": [644, 295, 341, 4833, 1349, 11, 1422, 2283, 11, 382, 8851, 281, 264, 1349, 1422, 11, 411, 33995, 11, 597, 576], "temperature": 0.0, "avg_logprob": -0.24081820319680608, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.0226059607230127e-05}, {"id": 234, "seek": 87424, "start": 892.92, "end": 893.92, "text": " be different.", "tokens": [312, 819, 13], "temperature": 0.0, "avg_logprob": -0.24081820319680608, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.0226059607230127e-05}, {"id": 235, "seek": 87424, "start": 893.92, "end": 902.28, "text": " Yeah, that's a great question.", "tokens": [865, 11, 300, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.24081820319680608, "compression_ratio": 1.6428571428571428, "no_speech_prob": 3.0226059607230127e-05}, {"id": 236, "seek": 90228, "start": 902.28, "end": 906.4399999999999, "text": " Okay, great.", "tokens": [1033, 11, 869, 13], "temperature": 0.0, "avg_logprob": -0.22329919988458807, "compression_ratio": 1.715953307392996, "no_speech_prob": 4.5384553231997415e-05}, {"id": 237, "seek": 90228, "start": 906.4399999999999, "end": 911.0799999999999, "text": " So that was our note on sub word modeling, and you can, you know, sub words are important,", "tokens": [407, 300, 390, 527, 3637, 322, 1422, 1349, 15983, 11, 293, 291, 393, 11, 291, 458, 11, 1422, 2283, 366, 1021, 11], "temperature": 0.0, "avg_logprob": -0.22329919988458807, "compression_ratio": 1.715953307392996, "no_speech_prob": 4.5384553231997415e-05}, {"id": 238, "seek": 90228, "start": 911.0799999999999, "end": 917.0799999999999, "text": " for example, in, you know, a lot of translation applications, that's why we gave you sub words", "tokens": [337, 1365, 11, 294, 11, 291, 458, 11, 257, 688, 295, 12853, 5821, 11, 300, 311, 983, 321, 2729, 291, 1422, 2283], "temperature": 0.0, "avg_logprob": -0.22329919988458807, "compression_ratio": 1.715953307392996, "no_speech_prob": 4.5384553231997415e-05}, {"id": 239, "seek": 90228, "start": 917.0799999999999, "end": 919.12, "text": " on the machine translation assignment.", "tokens": [322, 264, 3479, 12853, 15187, 13], "temperature": 0.0, "avg_logprob": -0.22329919988458807, "compression_ratio": 1.715953307392996, "no_speech_prob": 4.5384553231997415e-05}, {"id": 240, "seek": 90228, "start": 919.12, "end": 922.28, "text": " Now let's talk about model pre-training and word embeddings.", "tokens": [823, 718, 311, 751, 466, 2316, 659, 12, 17227, 1760, 293, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.22329919988458807, "compression_ratio": 1.715953307392996, "no_speech_prob": 4.5384553231997415e-05}, {"id": 241, "seek": 90228, "start": 922.28, "end": 925.48, "text": " So I love, I love being able to go to this slide.", "tokens": [407, 286, 959, 11, 286, 959, 885, 1075, 281, 352, 281, 341, 4137, 13], "temperature": 0.0, "avg_logprob": -0.22329919988458807, "compression_ratio": 1.715953307392996, "no_speech_prob": 4.5384553231997415e-05}, {"id": 242, "seek": 90228, "start": 925.48, "end": 929.4, "text": " So, so we saw this quote at the beginning of the class, you shall know a word by the company", "tokens": [407, 11, 370, 321, 1866, 341, 6513, 412, 264, 2863, 295, 264, 1508, 11, 291, 4393, 458, 257, 1349, 538, 264, 2237], "temperature": 0.0, "avg_logprob": -0.22329919988458807, "compression_ratio": 1.715953307392996, "no_speech_prob": 4.5384553231997415e-05}, {"id": 243, "seek": 92940, "start": 929.4, "end": 934.12, "text": " it keeps, and this was sort of one of the things that we used to summarize distributional", "tokens": [309, 5965, 11, 293, 341, 390, 1333, 295, 472, 295, 264, 721, 300, 321, 1143, 281, 20858, 7316, 304], "temperature": 0.0, "avg_logprob": -0.1421407417014793, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1187268177745864e-05}, {"id": 244, "seek": 92940, "start": 934.12, "end": 935.12, "text": " semantics.", "tokens": [4361, 45298, 13], "temperature": 0.0, "avg_logprob": -0.1421407417014793, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1187268177745864e-05}, {"id": 245, "seek": 92940, "start": 935.12, "end": 939.3199999999999, "text": " This idea that word to veck was sort of well motivated in some way, because the meaning", "tokens": [639, 1558, 300, 1349, 281, 1241, 547, 390, 1333, 295, 731, 14515, 294, 512, 636, 11, 570, 264, 3620], "temperature": 0.0, "avg_logprob": -0.1421407417014793, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1187268177745864e-05}, {"id": 246, "seek": 92940, "start": 939.3199999999999, "end": 944.92, "text": " of a word can be thought of as being derived from the kind of co-occurrent statistics of", "tokens": [295, 257, 1349, 393, 312, 1194, 295, 382, 885, 18949, 490, 264, 733, 295, 598, 12, 905, 14112, 1753, 12523, 295], "temperature": 0.0, "avg_logprob": -0.1421407417014793, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1187268177745864e-05}, {"id": 247, "seek": 92940, "start": 944.92, "end": 952.16, "text": " words that co-occur around it, and that was just fascinatingly effective, I think.", "tokens": [2283, 300, 598, 12, 905, 14112, 926, 309, 11, 293, 300, 390, 445, 10343, 356, 4942, 11, 286, 519, 13], "temperature": 0.0, "avg_logprob": -0.1421407417014793, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1187268177745864e-05}, {"id": 248, "seek": 92940, "start": 952.16, "end": 954.76, "text": " But there's this other quote actually from the same person.", "tokens": [583, 456, 311, 341, 661, 6513, 767, 490, 264, 912, 954, 13], "temperature": 0.0, "avg_logprob": -0.1421407417014793, "compression_ratio": 1.6666666666666667, "no_speech_prob": 3.1187268177745864e-05}, {"id": 249, "seek": 95476, "start": 954.76, "end": 961.3199999999999, "text": " So we have J.R. Firth, 1935, compared to our quote before from 1957, and the second", "tokens": [407, 321, 362, 508, 13, 49, 13, 28164, 392, 11, 1294, 8794, 11, 5347, 281, 527, 6513, 949, 490, 46256, 11, 293, 264, 1150], "temperature": 0.0, "avg_logprob": -0.16449145735981308, "compression_ratio": 1.597609561752988, "no_speech_prob": 1.3206257790443487e-05}, {"id": 250, "seek": 95476, "start": 961.3199999999999, "end": 966.56, "text": " quote says, the complete meaning of a word is always contextual, and no study of meaning", "tokens": [6513, 1619, 11, 264, 3566, 3620, 295, 257, 1349, 307, 1009, 35526, 11, 293, 572, 2979, 295, 3620], "temperature": 0.0, "avg_logprob": -0.16449145735981308, "compression_ratio": 1.597609561752988, "no_speech_prob": 1.3206257790443487e-05}, {"id": 251, "seek": 95476, "start": 966.56, "end": 970.48, "text": " apart from a complete context can be taken seriously.", "tokens": [4936, 490, 257, 3566, 4319, 393, 312, 2726, 6638, 13], "temperature": 0.0, "avg_logprob": -0.16449145735981308, "compression_ratio": 1.597609561752988, "no_speech_prob": 1.3206257790443487e-05}, {"id": 252, "seek": 95476, "start": 970.48, "end": 975.48, "text": " Now again, these are just things that we can sort of think about and chew on, but it", "tokens": [823, 797, 11, 613, 366, 445, 721, 300, 321, 393, 1333, 295, 519, 466, 293, 21200, 322, 11, 457, 309], "temperature": 0.0, "avg_logprob": -0.16449145735981308, "compression_ratio": 1.597609561752988, "no_speech_prob": 1.3206257790443487e-05}, {"id": 253, "seek": 95476, "start": 975.48, "end": 980.4399999999999, "text": " comes to mind, right, when you, when you embed words with word to veck, one of the issues", "tokens": [1487, 281, 1575, 11, 558, 11, 562, 291, 11, 562, 291, 12240, 2283, 365, 1349, 281, 1241, 547, 11, 472, 295, 264, 2663], "temperature": 0.0, "avg_logprob": -0.16449145735981308, "compression_ratio": 1.597609561752988, "no_speech_prob": 1.3206257790443487e-05}, {"id": 254, "seek": 98044, "start": 980.44, "end": 986.0, "text": " is that you don't actually look at its neighbors as you're giving it an embedding.", "tokens": [307, 300, 291, 500, 380, 767, 574, 412, 1080, 12512, 382, 291, 434, 2902, 309, 364, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.14509565599503055, "compression_ratio": 1.7072243346007605, "no_speech_prob": 2.884700552385766e-05}, {"id": 255, "seek": 98044, "start": 986.0, "end": 993.44, "text": " So if I have the sentence I record the record, you know, the two instances of REC, ORD,", "tokens": [407, 498, 286, 362, 264, 8174, 286, 2136, 264, 2136, 11, 291, 458, 11, 264, 732, 14519, 295, 497, 8140, 11, 19654, 35, 11], "temperature": 0.0, "avg_logprob": -0.14509565599503055, "compression_ratio": 1.7072243346007605, "no_speech_prob": 2.884700552385766e-05}, {"id": 256, "seek": 98044, "start": 993.44, "end": 997.9200000000001, "text": " mean different things, but they're given the same word to veck embedding, right, because", "tokens": [914, 819, 721, 11, 457, 436, 434, 2212, 264, 912, 1349, 281, 1241, 547, 12240, 3584, 11, 558, 11, 570], "temperature": 0.0, "avg_logprob": -0.14509565599503055, "compression_ratio": 1.7072243346007605, "no_speech_prob": 2.884700552385766e-05}, {"id": 257, "seek": 98044, "start": 997.9200000000001, "end": 1002.2, "text": " in word to veck you take the string, you map it to, oh, I've seen the word record before,", "tokens": [294, 1349, 281, 1241, 547, 291, 747, 264, 6798, 11, 291, 4471, 309, 281, 11, 1954, 11, 286, 600, 1612, 264, 1349, 2136, 949, 11], "temperature": 0.0, "avg_logprob": -0.14509565599503055, "compression_ratio": 1.7072243346007605, "no_speech_prob": 2.884700552385766e-05}, {"id": 258, "seek": 98044, "start": 1002.2, "end": 1007.2800000000001, "text": " you get that sort of vector from your learned matrix, and you give it the same thing in both", "tokens": [291, 483, 300, 1333, 295, 8062, 490, 428, 3264, 8141, 11, 293, 291, 976, 309, 264, 912, 551, 294, 1293], "temperature": 0.0, "avg_logprob": -0.14509565599503055, "compression_ratio": 1.7072243346007605, "no_speech_prob": 2.884700552385766e-05}, {"id": 259, "seek": 98044, "start": 1007.2800000000001, "end": 1009.5600000000001, "text": " cases.", "tokens": [3331, 13], "temperature": 0.0, "avg_logprob": -0.14509565599503055, "compression_ratio": 1.7072243346007605, "no_speech_prob": 2.884700552385766e-05}, {"id": 260, "seek": 100956, "start": 1009.56, "end": 1014.5999999999999, "text": " And so what we're going to be doing today is actually not conceptually all that different", "tokens": [400, 370, 437, 321, 434, 516, 281, 312, 884, 965, 307, 767, 406, 3410, 671, 439, 300, 819], "temperature": 0.0, "avg_logprob": -0.13576286633809406, "compression_ratio": 1.6823104693140793, "no_speech_prob": 3.942812327295542e-05}, {"id": 261, "seek": 100956, "start": 1014.5999999999999, "end": 1016.64, "text": " from training word to veck.", "tokens": [490, 3097, 1349, 281, 1241, 547, 13], "temperature": 0.0, "avg_logprob": -0.13576286633809406, "compression_ratio": 1.6823104693140793, "no_speech_prob": 3.942812327295542e-05}, {"id": 262, "seek": 100956, "start": 1016.64, "end": 1021.8, "text": " Word to veck training you can think of as pre-training just a very simple model that only assigns", "tokens": [8725, 281, 1241, 547, 3097, 291, 393, 519, 295, 382, 659, 12, 17227, 1760, 445, 257, 588, 2199, 2316, 300, 787, 6269, 82], "temperature": 0.0, "avg_logprob": -0.13576286633809406, "compression_ratio": 1.6823104693140793, "no_speech_prob": 3.942812327295542e-05}, {"id": 263, "seek": 100956, "start": 1021.8, "end": 1027.48, "text": " an individual vector to each unique word type, each unique element in your vocabulary.", "tokens": [364, 2609, 8062, 281, 1184, 3845, 1349, 2010, 11, 1184, 3845, 4478, 294, 428, 19864, 13], "temperature": 0.0, "avg_logprob": -0.13576286633809406, "compression_ratio": 1.6823104693140793, "no_speech_prob": 3.942812327295542e-05}, {"id": 264, "seek": 100956, "start": 1027.48, "end": 1032.04, "text": " Today we'll be going a lot farther than that, but the idea is very similar.", "tokens": [2692, 321, 603, 312, 516, 257, 688, 20344, 813, 300, 11, 457, 264, 1558, 307, 588, 2531, 13], "temperature": 0.0, "avg_logprob": -0.13576286633809406, "compression_ratio": 1.6823104693140793, "no_speech_prob": 3.942812327295542e-05}, {"id": 265, "seek": 100956, "start": 1032.04, "end": 1037.32, "text": " So back in, you know, 2017, we would start with pre-trained word embeddings, and again,", "tokens": [407, 646, 294, 11, 291, 458, 11, 6591, 11, 321, 576, 722, 365, 659, 12, 17227, 2001, 1349, 12240, 29432, 11, 293, 797, 11], "temperature": 0.0, "avg_logprob": -0.13576286633809406, "compression_ratio": 1.6823104693140793, "no_speech_prob": 3.942812327295542e-05}, {"id": 266, "seek": 103732, "start": 1037.32, "end": 1041.6399999999999, "text": " remember no context there, so you give a word and embedding independent of the context", "tokens": [1604, 572, 4319, 456, 11, 370, 291, 976, 257, 1349, 293, 12240, 3584, 6695, 295, 264, 4319], "temperature": 0.0, "avg_logprob": -0.1887037754058838, "compression_ratio": 1.7540322580645162, "no_speech_prob": 6.707102875225246e-05}, {"id": 267, "seek": 103732, "start": 1041.6399999999999, "end": 1043.1599999999999, "text": " that it shows up in.", "tokens": [300, 309, 3110, 493, 294, 13], "temperature": 0.0, "avg_logprob": -0.1887037754058838, "compression_ratio": 1.7540322580645162, "no_speech_prob": 6.707102875225246e-05}, {"id": 268, "seek": 103732, "start": 1043.1599999999999, "end": 1045.4399999999998, "text": " And then you learn how to incorporate the context.", "tokens": [400, 550, 291, 1466, 577, 281, 16091, 264, 4319, 13], "temperature": 0.0, "avg_logprob": -0.1887037754058838, "compression_ratio": 1.7540322580645162, "no_speech_prob": 6.707102875225246e-05}, {"id": 269, "seek": 103732, "start": 1045.4399999999998, "end": 1048.72, "text": " It's not like our NLP models never used context, right?", "tokens": [467, 311, 406, 411, 527, 426, 45196, 5245, 1128, 1143, 4319, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1887037754058838, "compression_ratio": 1.7540322580645162, "no_speech_prob": 6.707102875225246e-05}, {"id": 270, "seek": 103732, "start": 1048.72, "end": 1054.48, "text": " Instead, you would learn to incorporate the context using your LSTM, or it's later in 2017,", "tokens": [7156, 11, 291, 576, 1466, 281, 16091, 264, 4319, 1228, 428, 441, 6840, 44, 11, 420, 309, 311, 1780, 294, 6591, 11], "temperature": 0.0, "avg_logprob": -0.1887037754058838, "compression_ratio": 1.7540322580645162, "no_speech_prob": 6.707102875225246e-05}, {"id": 271, "seek": 103732, "start": 1054.48, "end": 1057.1599999999999, "text": " you know, your transformer.", "tokens": [291, 458, 11, 428, 31782, 13], "temperature": 0.0, "avg_logprob": -0.1887037754058838, "compression_ratio": 1.7540322580645162, "no_speech_prob": 6.707102875225246e-05}, {"id": 272, "seek": 103732, "start": 1057.1599999999999, "end": 1061.76, "text": " And you would learn to incorporate context while training on the task.", "tokens": [400, 291, 576, 1466, 281, 16091, 4319, 1339, 3097, 322, 264, 5633, 13], "temperature": 0.0, "avg_logprob": -0.1887037754058838, "compression_ratio": 1.7540322580645162, "no_speech_prob": 6.707102875225246e-05}, {"id": 273, "seek": 103732, "start": 1061.76, "end": 1063.72, "text": " So you have some supervision.", "tokens": [407, 291, 362, 512, 32675, 13], "temperature": 0.0, "avg_logprob": -0.1887037754058838, "compression_ratio": 1.7540322580645162, "no_speech_prob": 6.707102875225246e-05}, {"id": 274, "seek": 106372, "start": 1063.72, "end": 1067.64, "text": " Maybe it's machine translation supervision, maybe sentiment, maybe question answering.", "tokens": [2704, 309, 311, 3479, 12853, 32675, 11, 1310, 16149, 11, 1310, 1168, 13430, 13], "temperature": 0.0, "avg_logprob": -0.15454385635700632, "compression_ratio": 1.7707006369426752, "no_speech_prob": 3.071264654863626e-05}, {"id": 275, "seek": 106372, "start": 1067.64, "end": 1073.28, "text": " And you would learn how to incorporate context in your LSTM or otherwise through the signal", "tokens": [400, 291, 576, 1466, 577, 281, 16091, 4319, 294, 428, 441, 6840, 44, 420, 5911, 807, 264, 6358], "temperature": 0.0, "avg_logprob": -0.15454385635700632, "compression_ratio": 1.7707006369426752, "no_speech_prob": 3.071264654863626e-05}, {"id": 276, "seek": 106372, "start": 1073.28, "end": 1076.68, "text": " of the training instead of say through the word to veck signal.", "tokens": [295, 264, 3097, 2602, 295, 584, 807, 264, 1349, 281, 1241, 547, 6358, 13], "temperature": 0.0, "avg_logprob": -0.15454385635700632, "compression_ratio": 1.7707006369426752, "no_speech_prob": 3.071264654863626e-05}, {"id": 277, "seek": 106372, "start": 1076.68, "end": 1081.08, "text": " And so, you know, sort of pictographically, you have these word embeddings here, so the", "tokens": [400, 370, 11, 291, 458, 11, 1333, 295, 2317, 3108, 984, 11, 291, 362, 613, 1349, 12240, 29432, 510, 11, 370, 264], "temperature": 0.0, "avg_logprob": -0.15454385635700632, "compression_ratio": 1.7707006369426752, "no_speech_prob": 3.071264654863626e-05}, {"id": 278, "seek": 106372, "start": 1081.08, "end": 1084.96, "text": " red are sort of your word to veck embeddings, and those are pre-trained.", "tokens": [2182, 366, 1333, 295, 428, 1349, 281, 1241, 547, 12240, 29432, 11, 293, 729, 366, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.15454385635700632, "compression_ratio": 1.7707006369426752, "no_speech_prob": 3.071264654863626e-05}, {"id": 279, "seek": 106372, "start": 1084.96, "end": 1088.24, "text": " Those take up some of the parameters of your network.", "tokens": [3950, 747, 493, 512, 295, 264, 9834, 295, 428, 3209, 13], "temperature": 0.0, "avg_logprob": -0.15454385635700632, "compression_ratio": 1.7707006369426752, "no_speech_prob": 3.071264654863626e-05}, {"id": 280, "seek": 106372, "start": 1088.24, "end": 1089.76, "text": " And then you've got your contextualization.", "tokens": [400, 550, 291, 600, 658, 428, 35526, 2144, 13], "temperature": 0.0, "avg_logprob": -0.15454385635700632, "compression_ratio": 1.7707006369426752, "no_speech_prob": 3.071264654863626e-05}, {"id": 281, "seek": 106372, "start": 1089.76, "end": 1092.24, "text": " Now this looks like an LSTM, but it could be whatever.", "tokens": [823, 341, 1542, 411, 364, 441, 6840, 44, 11, 457, 309, 727, 312, 2035, 13], "temperature": 0.0, "avg_logprob": -0.15454385635700632, "compression_ratio": 1.7707006369426752, "no_speech_prob": 3.071264654863626e-05}, {"id": 282, "seek": 109224, "start": 1092.24, "end": 1096.64, "text": " So this maybe bidirectional encoder thing here is not pre-trained.", "tokens": [407, 341, 1310, 12957, 621, 41048, 2058, 19866, 551, 510, 307, 406, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.15164520557110125, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.0458757717278786e-05}, {"id": 283, "seek": 109224, "start": 1096.64, "end": 1099.56, "text": " And now that's a lot of parameters that are not pre-trained.", "tokens": [400, 586, 300, 311, 257, 688, 295, 9834, 300, 366, 406, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.15164520557110125, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.0458757717278786e-05}, {"id": 284, "seek": 109224, "start": 1099.56, "end": 1104.2, "text": " And then maybe you have some sort of readout function at the end, right, to predict whatever", "tokens": [400, 550, 1310, 291, 362, 512, 1333, 295, 1401, 346, 2445, 412, 264, 917, 11, 558, 11, 281, 6069, 2035], "temperature": 0.0, "avg_logprob": -0.15164520557110125, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.0458757717278786e-05}, {"id": 285, "seek": 109224, "start": 1104.2, "end": 1105.2, "text": " thing you're trying to predict.", "tokens": [551, 291, 434, 1382, 281, 6069, 13], "temperature": 0.0, "avg_logprob": -0.15164520557110125, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.0458757717278786e-05}, {"id": 286, "seek": 109224, "start": 1105.2, "end": 1110.2, "text": " Again, maybe it's sentiment, maybe you're doing, I don't know, topic labeling, whatever", "tokens": [3764, 11, 1310, 309, 311, 16149, 11, 1310, 291, 434, 884, 11, 286, 500, 380, 458, 11, 4829, 40244, 11, 2035], "temperature": 0.0, "avg_logprob": -0.15164520557110125, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.0458757717278786e-05}, {"id": 287, "seek": 109224, "start": 1110.2, "end": 1111.2, "text": " you want to do.", "tokens": [291, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.15164520557110125, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.0458757717278786e-05}, {"id": 288, "seek": 109224, "start": 1111.2, "end": 1112.2, "text": " This is sort of the paradigm.", "tokens": [639, 307, 1333, 295, 264, 24709, 13], "temperature": 0.0, "avg_logprob": -0.15164520557110125, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.0458757717278786e-05}, {"id": 289, "seek": 109224, "start": 1112.2, "end": 1116.76, "text": " Like you set some sort of architecture and you only pre-trained the word embeddings.", "tokens": [1743, 291, 992, 512, 1333, 295, 9482, 293, 291, 787, 659, 12, 17227, 2001, 264, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.15164520557110125, "compression_ratio": 1.797709923664122, "no_speech_prob": 2.0458757717278786e-05}, {"id": 290, "seek": 111676, "start": 1116.76, "end": 1124.16, "text": " And so this isn't actually the conceptually, necessarily the biggest problem, because,", "tokens": [400, 370, 341, 1943, 380, 767, 264, 3410, 671, 11, 4725, 264, 3880, 1154, 11, 570, 11], "temperature": 0.0, "avg_logprob": -0.12736621336503462, "compression_ratio": 1.7165354330708662, "no_speech_prob": 2.668307388375979e-05}, {"id": 291, "seek": 111676, "start": 1124.16, "end": 1129.28, "text": " you know, we like to think in deep learning stuff that we have a lot of training data", "tokens": [291, 458, 11, 321, 411, 281, 519, 294, 2452, 2539, 1507, 300, 321, 362, 257, 688, 295, 3097, 1412], "temperature": 0.0, "avg_logprob": -0.12736621336503462, "compression_ratio": 1.7165354330708662, "no_speech_prob": 2.668307388375979e-05}, {"id": 292, "seek": 111676, "start": 1129.28, "end": 1130.28, "text": " for our objectives.", "tokens": [337, 527, 15961, 13], "temperature": 0.0, "avg_logprob": -0.12736621336503462, "compression_ratio": 1.7165354330708662, "no_speech_prob": 2.668307388375979e-05}, {"id": 293, "seek": 111676, "start": 1130.28, "end": 1136.2, "text": " I mean, one of the things that we motivated, you know, big, deep neural networks for is", "tokens": [286, 914, 11, 472, 295, 264, 721, 300, 321, 14515, 11, 291, 458, 11, 955, 11, 2452, 18161, 9590, 337, 307], "temperature": 0.0, "avg_logprob": -0.12736621336503462, "compression_ratio": 1.7165354330708662, "no_speech_prob": 2.668307388375979e-05}, {"id": 294, "seek": 111676, "start": 1136.2, "end": 1139.24, "text": " that they can take a lot of data and they can learn patterns from it.", "tokens": [300, 436, 393, 747, 257, 688, 295, 1412, 293, 436, 393, 1466, 8294, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.12736621336503462, "compression_ratio": 1.7165354330708662, "no_speech_prob": 2.668307388375979e-05}, {"id": 295, "seek": 111676, "start": 1139.24, "end": 1145.8, "text": " But it does put the onus on our downstream data to be sort of sufficient to teach the", "tokens": [583, 309, 775, 829, 264, 322, 301, 322, 527, 30621, 1412, 281, 312, 1333, 295, 11563, 281, 2924, 264], "temperature": 0.0, "avg_logprob": -0.12736621336503462, "compression_ratio": 1.7165354330708662, "no_speech_prob": 2.668307388375979e-05}, {"id": 296, "seek": 114580, "start": 1145.8, "end": 1147.9199999999998, "text": " contextual aspects of language.", "tokens": [35526, 7270, 295, 2856, 13], "temperature": 0.0, "avg_logprob": -0.15148464838663736, "compression_ratio": 1.7465277777777777, "no_speech_prob": 1.3005167602386791e-05}, {"id": 297, "seek": 114580, "start": 1147.9199999999998, "end": 1153.0, "text": " So you can imagine if you only have a little bit of, you know, labeled data for fine tuning,", "tokens": [407, 291, 393, 3811, 498, 291, 787, 362, 257, 707, 857, 295, 11, 291, 458, 11, 21335, 1412, 337, 2489, 15164, 11], "temperature": 0.0, "avg_logprob": -0.15148464838663736, "compression_ratio": 1.7465277777777777, "no_speech_prob": 1.3005167602386791e-05}, {"id": 298, "seek": 114580, "start": 1153.0, "end": 1157.3999999999999, "text": " you're putting a pretty big role on that data to say, hey, maybe here's some pre-trained", "tokens": [291, 434, 3372, 257, 1238, 955, 3090, 322, 300, 1412, 281, 584, 11, 4177, 11, 1310, 510, 311, 512, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.15148464838663736, "compression_ratio": 1.7465277777777777, "no_speech_prob": 1.3005167602386791e-05}, {"id": 299, "seek": 114580, "start": 1157.3999999999999, "end": 1161.72, "text": " embeddings, but like how you handle like sentences and how they compose and all that stuff,", "tokens": [12240, 29432, 11, 457, 411, 577, 291, 4813, 411, 16579, 293, 577, 436, 35925, 293, 439, 300, 1507, 11], "temperature": 0.0, "avg_logprob": -0.15148464838663736, "compression_ratio": 1.7465277777777777, "no_speech_prob": 1.3005167602386791e-05}, {"id": 300, "seek": 114580, "start": 1161.72, "end": 1163.48, "text": " that's up to you.", "tokens": [300, 311, 493, 281, 291, 13], "temperature": 0.0, "avg_logprob": -0.15148464838663736, "compression_ratio": 1.7465277777777777, "no_speech_prob": 1.3005167602386791e-05}, {"id": 301, "seek": 114580, "start": 1163.48, "end": 1167.08, "text": " So if you don't have a lot of labeled data for your downstream task, you're asking", "tokens": [407, 498, 291, 500, 380, 362, 257, 688, 295, 21335, 1412, 337, 428, 30621, 5633, 11, 291, 434, 3365], "temperature": 0.0, "avg_logprob": -0.15148464838663736, "compression_ratio": 1.7465277777777777, "no_speech_prob": 1.3005167602386791e-05}, {"id": 302, "seek": 114580, "start": 1167.08, "end": 1172.8, "text": " it to do a lot with, you know, a large number of parameters that have been initialized randomly.", "tokens": [309, 281, 360, 257, 688, 365, 11, 291, 458, 11, 257, 2416, 1230, 295, 9834, 300, 362, 668, 5883, 1602, 16979, 13], "temperature": 0.0, "avg_logprob": -0.15148464838663736, "compression_ratio": 1.7465277777777777, "no_speech_prob": 1.3005167602386791e-05}, {"id": 303, "seek": 117280, "start": 1172.8, "end": 1178.24, "text": " Okay, so like a small portion of the parameters have been pre-trained.", "tokens": [1033, 11, 370, 411, 257, 1359, 8044, 295, 264, 9834, 362, 668, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.16735044780530428, "compression_ratio": 1.5938864628820961, "no_speech_prob": 2.7965048502665013e-05}, {"id": 304, "seek": 117280, "start": 1178.24, "end": 1183.44, "text": " Okay, so where we're going is pre-training whole models.", "tokens": [1033, 11, 370, 689, 321, 434, 516, 307, 659, 12, 17227, 1760, 1379, 5245, 13], "temperature": 0.0, "avg_logprob": -0.16735044780530428, "compression_ratio": 1.5938864628820961, "no_speech_prob": 2.7965048502665013e-05}, {"id": 305, "seek": 117280, "start": 1183.44, "end": 1187.1599999999999, "text": " I mean, conceptually, you know, we're pretty close to there.", "tokens": [286, 914, 11, 3410, 671, 11, 291, 458, 11, 321, 434, 1238, 1998, 281, 456, 13], "temperature": 0.0, "avg_logprob": -0.16735044780530428, "compression_ratio": 1.5938864628820961, "no_speech_prob": 2.7965048502665013e-05}, {"id": 306, "seek": 117280, "start": 1187.1599999999999, "end": 1193.6399999999999, "text": " So nowadays, almost all parameters in your neural network and let's say a lot of research", "tokens": [407, 13434, 11, 1920, 439, 9834, 294, 428, 18161, 3209, 293, 718, 311, 584, 257, 688, 295, 2132], "temperature": 0.0, "avg_logprob": -0.16735044780530428, "compression_ratio": 1.5938864628820961, "no_speech_prob": 2.7965048502665013e-05}, {"id": 307, "seek": 117280, "start": 1193.6399999999999, "end": 1198.8, "text": " settings and increasingly in industry are initialized via pre-training, just like word", "tokens": [6257, 293, 12980, 294, 3518, 366, 5883, 1602, 5766, 659, 12, 17227, 1760, 11, 445, 411, 1349], "temperature": 0.0, "avg_logprob": -0.16735044780530428, "compression_ratio": 1.5938864628820961, "no_speech_prob": 2.7965048502665013e-05}, {"id": 308, "seek": 119880, "start": 1198.8, "end": 1207.2, "text": " to vac parameters were initialized and pre-training methods in general hide parts of the input", "tokens": [281, 2842, 9834, 645, 5883, 1602, 293, 659, 12, 17227, 1760, 7150, 294, 2674, 6479, 3166, 295, 264, 4846], "temperature": 0.0, "avg_logprob": -0.14735101539397907, "compression_ratio": 1.703125, "no_speech_prob": 4.0058279410004616e-05}, {"id": 309, "seek": 119880, "start": 1207.2, "end": 1211.76, "text": " from the model itself and then train the model to reconstruct those parts.", "tokens": [490, 264, 2316, 2564, 293, 550, 3847, 264, 2316, 281, 31499, 729, 3166, 13], "temperature": 0.0, "avg_logprob": -0.14735101539397907, "compression_ratio": 1.703125, "no_speech_prob": 4.0058279410004616e-05}, {"id": 310, "seek": 119880, "start": 1211.76, "end": 1214.8799999999999, "text": " How does this connect to word to vac?", "tokens": [1012, 775, 341, 1745, 281, 1349, 281, 2842, 30], "temperature": 0.0, "avg_logprob": -0.14735101539397907, "compression_ratio": 1.703125, "no_speech_prob": 4.0058279410004616e-05}, {"id": 311, "seek": 119880, "start": 1214.8799999999999, "end": 1219.04, "text": " In word to vac, you know, people don't usually make this connection, but it's the following.", "tokens": [682, 1349, 281, 2842, 11, 291, 458, 11, 561, 500, 380, 2673, 652, 341, 4984, 11, 457, 309, 311, 264, 3480, 13], "temperature": 0.0, "avg_logprob": -0.14735101539397907, "compression_ratio": 1.703125, "no_speech_prob": 4.0058279410004616e-05}, {"id": 312, "seek": 119880, "start": 1219.04, "end": 1225.24, "text": " You have an individual word and it knows itself, right, because you have the embedding for", "tokens": [509, 362, 364, 2609, 1349, 293, 309, 3255, 2564, 11, 558, 11, 570, 291, 362, 264, 12240, 3584, 337], "temperature": 0.0, "avg_logprob": -0.14735101539397907, "compression_ratio": 1.703125, "no_speech_prob": 4.0058279410004616e-05}, {"id": 313, "seek": 119880, "start": 1225.24, "end": 1227.84, "text": " the center word, right, from assignment two.", "tokens": [264, 3056, 1349, 11, 558, 11, 490, 15187, 732, 13], "temperature": 0.0, "avg_logprob": -0.14735101539397907, "compression_ratio": 1.703125, "no_speech_prob": 4.0058279410004616e-05}, {"id": 314, "seek": 122784, "start": 1227.84, "end": 1231.9199999999998, "text": " You have the embedding for the center word and knows itself and you've masked out all", "tokens": [509, 362, 264, 12240, 3584, 337, 264, 3056, 1349, 293, 3255, 2564, 293, 291, 600, 45249, 484, 439], "temperature": 0.0, "avg_logprob": -0.1390362270807816, "compression_ratio": 1.900414937759336, "no_speech_prob": 1.749948387441691e-05}, {"id": 315, "seek": 122784, "start": 1231.9199999999998, "end": 1233.56, "text": " of its neighbors.", "tokens": [295, 1080, 12512, 13], "temperature": 0.0, "avg_logprob": -0.1390362270807816, "compression_ratio": 1.900414937759336, "no_speech_prob": 1.749948387441691e-05}, {"id": 316, "seek": 122784, "start": 1233.56, "end": 1236.6399999999999, "text": " You've hidden all of its neighbors from it, right, every all of its window neighbors, you've", "tokens": [509, 600, 7633, 439, 295, 1080, 12512, 490, 309, 11, 558, 11, 633, 439, 295, 1080, 4910, 12512, 11, 291, 600], "temperature": 0.0, "avg_logprob": -0.1390362270807816, "compression_ratio": 1.900414937759336, "no_speech_prob": 1.749948387441691e-05}, {"id": 317, "seek": 122784, "start": 1236.6399999999999, "end": 1237.6399999999999, "text": " hidden from it.", "tokens": [7633, 490, 309, 13], "temperature": 0.0, "avg_logprob": -0.1390362270807816, "compression_ratio": 1.900414937759336, "no_speech_prob": 1.749948387441691e-05}, {"id": 318, "seek": 122784, "start": 1237.6399999999999, "end": 1241.84, "text": " You ask the center word to predict its neighbors, right?", "tokens": [509, 1029, 264, 3056, 1349, 281, 6069, 1080, 12512, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1390362270807816, "compression_ratio": 1.900414937759336, "no_speech_prob": 1.749948387441691e-05}, {"id": 319, "seek": 122784, "start": 1241.84, "end": 1247.36, "text": " And so this is, this falls under the category of pre-training.", "tokens": [400, 370, 341, 307, 11, 341, 8804, 833, 264, 7719, 295, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.1390362270807816, "compression_ratio": 1.900414937759336, "no_speech_prob": 1.749948387441691e-05}, {"id": 320, "seek": 122784, "start": 1247.36, "end": 1248.6, "text": " All of these methods look similar.", "tokens": [1057, 295, 613, 7150, 574, 2531, 13], "temperature": 0.0, "avg_logprob": -0.1390362270807816, "compression_ratio": 1.900414937759336, "no_speech_prob": 1.749948387441691e-05}, {"id": 321, "seek": 122784, "start": 1248.6, "end": 1254.12, "text": " You hide parts of the input from the model and train the model to reconstruct those parts.", "tokens": [509, 6479, 3166, 295, 264, 4846, 490, 264, 2316, 293, 3847, 264, 2316, 281, 31499, 729, 3166, 13], "temperature": 0.0, "avg_logprob": -0.1390362270807816, "compression_ratio": 1.900414937759336, "no_speech_prob": 1.749948387441691e-05}, {"id": 322, "seek": 125412, "start": 1254.12, "end": 1258.4399999999998, "text": " The differences with full model pre-training is that you don't give the model just the", "tokens": [440, 7300, 365, 1577, 2316, 659, 12, 17227, 1760, 307, 300, 291, 500, 380, 976, 264, 2316, 445, 264], "temperature": 0.0, "avg_logprob": -0.1609150202928391, "compression_ratio": 1.752, "no_speech_prob": 8.091128256637603e-05}, {"id": 323, "seek": 125412, "start": 1258.4399999999998, "end": 1261.08, "text": " individual word and have it learn an embedding of that word.", "tokens": [2609, 1349, 293, 362, 309, 1466, 364, 12240, 3584, 295, 300, 1349, 13], "temperature": 0.0, "avg_logprob": -0.1609150202928391, "compression_ratio": 1.752, "no_speech_prob": 8.091128256637603e-05}, {"id": 324, "seek": 125412, "start": 1261.08, "end": 1266.1999999999998, "text": " You give it much more of the sequence and have it predict, you know, held out parts of", "tokens": [509, 976, 309, 709, 544, 295, 264, 8310, 293, 362, 309, 6069, 11, 291, 458, 11, 5167, 484, 3166, 295], "temperature": 0.0, "avg_logprob": -0.1609150202928391, "compression_ratio": 1.752, "no_speech_prob": 8.091128256637603e-05}, {"id": 325, "seek": 125412, "start": 1266.1999999999998, "end": 1267.1999999999998, "text": " the sequence.", "tokens": [264, 8310, 13], "temperature": 0.0, "avg_logprob": -0.1609150202928391, "compression_ratio": 1.752, "no_speech_prob": 8.091128256637603e-05}, {"id": 326, "seek": 125412, "start": 1267.1999999999998, "end": 1268.36, "text": " And we'll get into the details there.", "tokens": [400, 321, 603, 483, 666, 264, 4365, 456, 13], "temperature": 0.0, "avg_logprob": -0.1609150202928391, "compression_ratio": 1.752, "no_speech_prob": 8.091128256637603e-05}, {"id": 327, "seek": 125412, "start": 1268.36, "end": 1274.0, "text": " But, you know, the takeaway is that everything here is pre-trained jointly, possibly with", "tokens": [583, 11, 291, 458, 11, 264, 30681, 307, 300, 1203, 510, 307, 659, 12, 17227, 2001, 46557, 11, 6264, 365], "temperature": 0.0, "avg_logprob": -0.1609150202928391, "compression_ratio": 1.752, "no_speech_prob": 8.091128256637603e-05}, {"id": 328, "seek": 125412, "start": 1274.0, "end": 1278.8, "text": " the exception of the very last layer that predicts the label.", "tokens": [264, 11183, 295, 264, 588, 1036, 4583, 300, 6069, 82, 264, 7645, 13], "temperature": 0.0, "avg_logprob": -0.1609150202928391, "compression_ratio": 1.752, "no_speech_prob": 8.091128256637603e-05}, {"id": 329, "seek": 127880, "start": 1278.8, "end": 1286.12, "text": " Okay, and this has just been exceptionally effective at building representations of language", "tokens": [1033, 11, 293, 341, 575, 445, 668, 37807, 4942, 412, 2390, 33358, 295, 2856], "temperature": 0.0, "avg_logprob": -0.16289419823504508, "compression_ratio": 1.8596491228070176, "no_speech_prob": 7.366790669038892e-05}, {"id": 330, "seek": 127880, "start": 1286.12, "end": 1291.32, "text": " that just map similar things in language, similar representations in these encoders, just", "tokens": [300, 445, 4471, 2531, 721, 294, 2856, 11, 2531, 33358, 294, 613, 2058, 378, 433, 11, 445], "temperature": 0.0, "avg_logprob": -0.16289419823504508, "compression_ratio": 1.8596491228070176, "no_speech_prob": 7.366790669038892e-05}, {"id": 331, "seek": 127880, "start": 1291.32, "end": 1295.52, "text": " like how word-to-vec map similar words to similar vectors.", "tokens": [411, 577, 1349, 12, 1353, 12, 303, 66, 4471, 2531, 2283, 281, 2531, 18875, 13], "temperature": 0.0, "avg_logprob": -0.16289419823504508, "compression_ratio": 1.8596491228070176, "no_speech_prob": 7.366790669038892e-05}, {"id": 332, "seek": 127880, "start": 1295.52, "end": 1300.0, "text": " It's been exceptionally effective at making parameter initializations where you start with", "tokens": [467, 311, 668, 37807, 4942, 412, 1455, 13075, 5883, 14455, 689, 291, 722, 365], "temperature": 0.0, "avg_logprob": -0.16289419823504508, "compression_ratio": 1.8596491228070176, "no_speech_prob": 7.366790669038892e-05}, {"id": 333, "seek": 127880, "start": 1300.0, "end": 1305.84, "text": " these parameters that have been pre-trained and then you fine-tune them on your label data.", "tokens": [613, 9834, 300, 362, 668, 659, 12, 17227, 2001, 293, 550, 291, 2489, 12, 83, 2613, 552, 322, 428, 7645, 1412, 13], "temperature": 0.0, "avg_logprob": -0.16289419823504508, "compression_ratio": 1.8596491228070176, "no_speech_prob": 7.366790669038892e-05}, {"id": 334, "seek": 130584, "start": 1305.84, "end": 1309.6799999999998, "text": " And then third, they have an exceptionally effective at defining probability distributions", "tokens": [400, 550, 2636, 11, 436, 362, 364, 37807, 4942, 412, 17827, 8482, 37870], "temperature": 0.0, "avg_logprob": -0.19287345959590033, "compression_ratio": 1.7415730337078652, "no_speech_prob": 7.645716323168017e-06}, {"id": 335, "seek": 130584, "start": 1309.6799999999998, "end": 1314.72, "text": " over language, like in language modeling, that are actually really useful to sample from", "tokens": [670, 2856, 11, 411, 294, 2856, 15983, 11, 300, 366, 767, 534, 4420, 281, 6889, 490], "temperature": 0.0, "avg_logprob": -0.19287345959590033, "compression_ratio": 1.7415730337078652, "no_speech_prob": 7.645716323168017e-06}, {"id": 336, "seek": 130584, "start": 1314.72, "end": 1316.56, "text": " in certain cases.", "tokens": [294, 1629, 3331, 13], "temperature": 0.0, "avg_logprob": -0.19287345959590033, "compression_ratio": 1.7415730337078652, "no_speech_prob": 7.645716323168017e-06}, {"id": 337, "seek": 130584, "start": 1316.56, "end": 1319.72, "text": " So these are three ways in which we interact with pre-trained models.", "tokens": [407, 613, 366, 1045, 2098, 294, 597, 321, 4648, 365, 659, 12, 17227, 2001, 5245, 13], "temperature": 0.0, "avg_logprob": -0.19287345959590033, "compression_ratio": 1.7415730337078652, "no_speech_prob": 7.645716323168017e-06}, {"id": 338, "seek": 130584, "start": 1319.72, "end": 1322.52, "text": " We use their representations just to compute similarities.", "tokens": [492, 764, 641, 33358, 445, 281, 14722, 24197, 13], "temperature": 0.0, "avg_logprob": -0.19287345959590033, "compression_ratio": 1.7415730337078652, "no_speech_prob": 7.645716323168017e-06}, {"id": 339, "seek": 130584, "start": 1322.52, "end": 1324.52, "text": " We use them for parameter initializations.", "tokens": [492, 764, 552, 337, 13075, 5883, 14455, 13], "temperature": 0.0, "avg_logprob": -0.19287345959590033, "compression_ratio": 1.7415730337078652, "no_speech_prob": 7.645716323168017e-06}, {"id": 340, "seek": 130584, "start": 1324.52, "end": 1330.52, "text": " And we actually just use them as probability distributions, sort of how we train to them.", "tokens": [400, 321, 767, 445, 764, 552, 382, 8482, 37870, 11, 1333, 295, 577, 321, 3847, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.19287345959590033, "compression_ratio": 1.7415730337078652, "no_speech_prob": 7.645716323168017e-06}, {"id": 341, "seek": 130584, "start": 1330.52, "end": 1332.8, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19287345959590033, "compression_ratio": 1.7415730337078652, "no_speech_prob": 7.645716323168017e-06}, {"id": 342, "seek": 133280, "start": 1332.8, "end": 1336.28, "text": " So let's get into some technical parts here.", "tokens": [407, 718, 311, 483, 666, 512, 6191, 3166, 510, 13], "temperature": 0.0, "avg_logprob": -0.17221188341450488, "compression_ratio": 1.7054545454545456, "no_speech_prob": 7.720124267507344e-05}, {"id": 343, "seek": 133280, "start": 1336.28, "end": 1341.6, "text": " I sort of want to think broad thoughts about what we could do with pre-training and what", "tokens": [286, 1333, 295, 528, 281, 519, 4152, 4598, 466, 437, 321, 727, 360, 365, 659, 12, 17227, 1760, 293, 437], "temperature": 0.0, "avg_logprob": -0.17221188341450488, "compression_ratio": 1.7054545454545456, "no_speech_prob": 7.720124267507344e-05}, {"id": 344, "seek": 133280, "start": 1341.6, "end": 1346.8, "text": " kind of things we could expect to potentially learn from this general method of hide part", "tokens": [733, 295, 721, 321, 727, 2066, 281, 7263, 1466, 490, 341, 2674, 3170, 295, 6479, 644], "temperature": 0.0, "avg_logprob": -0.17221188341450488, "compression_ratio": 1.7054545454545456, "no_speech_prob": 7.720124267507344e-05}, {"id": 345, "seek": 133280, "start": 1346.8, "end": 1350.68, "text": " of the input and then see other parts of the input and then try to predict the parts", "tokens": [295, 264, 4846, 293, 550, 536, 661, 3166, 295, 264, 4846, 293, 550, 853, 281, 6069, 264, 3166], "temperature": 0.0, "avg_logprob": -0.17221188341450488, "compression_ratio": 1.7054545454545456, "no_speech_prob": 7.720124267507344e-05}, {"id": 346, "seek": 133280, "start": 1350.68, "end": 1351.84, "text": " that you hid.", "tokens": [300, 291, 16253, 13], "temperature": 0.0, "avg_logprob": -0.17221188341450488, "compression_ratio": 1.7054545454545456, "no_speech_prob": 7.720124267507344e-05}, {"id": 347, "seek": 133280, "start": 1351.84, "end": 1352.84, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.17221188341450488, "compression_ratio": 1.7054545454545456, "no_speech_prob": 7.720124267507344e-05}, {"id": 348, "seek": 133280, "start": 1352.84, "end": 1356.84, "text": " So Stanford University is located in Blank California.", "tokens": [407, 20374, 3535, 307, 6870, 294, 2177, 657, 5384, 13], "temperature": 0.0, "avg_logprob": -0.17221188341450488, "compression_ratio": 1.7054545454545456, "no_speech_prob": 7.720124267507344e-05}, {"id": 349, "seek": 133280, "start": 1356.84, "end": 1359.96, "text": " If we gave a model, everything that was not blanked out here and asked to predict the", "tokens": [759, 321, 2729, 257, 2316, 11, 1203, 300, 390, 406, 8247, 292, 484, 510, 293, 2351, 281, 6069, 264], "temperature": 0.0, "avg_logprob": -0.17221188341450488, "compression_ratio": 1.7054545454545456, "no_speech_prob": 7.720124267507344e-05}, {"id": 350, "seek": 135996, "start": 1359.96, "end": 1369.28, "text": " middle, the loss function would train the model to predict Palo Alto here, I expect.", "tokens": [2808, 11, 264, 4470, 2445, 576, 3847, 264, 2316, 281, 6069, 6116, 78, 50066, 510, 11, 286, 2066, 13], "temperature": 0.0, "avg_logprob": -0.19031310292471826, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.2605791198438965e-05}, {"id": 351, "seek": 135996, "start": 1369.28, "end": 1370.28, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19031310292471826, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.2605791198438965e-05}, {"id": 352, "seek": 135996, "start": 1370.28, "end": 1374.72, "text": " So this is an instance of something that you could imagine being a pre-training objective.", "tokens": [407, 341, 307, 364, 5197, 295, 746, 300, 291, 727, 3811, 885, 257, 659, 12, 17227, 1760, 10024, 13], "temperature": 0.0, "avg_logprob": -0.19031310292471826, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.2605791198438965e-05}, {"id": 353, "seek": 135996, "start": 1374.72, "end": 1379.72, "text": " You take in a sentence, you remove part of it and you say recreate the part that I removed.", "tokens": [509, 747, 294, 257, 8174, 11, 291, 4159, 644, 295, 309, 293, 291, 584, 25833, 264, 644, 300, 286, 7261, 13], "temperature": 0.0, "avg_logprob": -0.19031310292471826, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.2605791198438965e-05}, {"id": 354, "seek": 135996, "start": 1379.72, "end": 1383.1200000000001, "text": " And in this case, if I just gave a bunch of examples that looked like this, it might", "tokens": [400, 294, 341, 1389, 11, 498, 286, 445, 2729, 257, 3840, 295, 5110, 300, 2956, 411, 341, 11, 309, 1062], "temperature": 0.0, "avg_logprob": -0.19031310292471826, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.2605791198438965e-05}, {"id": 355, "seek": 135996, "start": 1383.1200000000001, "end": 1387.44, "text": " learn sort of trivia thing here.", "tokens": [1466, 1333, 295, 48770, 551, 510, 13], "temperature": 0.0, "avg_logprob": -0.19031310292471826, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.2605791198438965e-05}, {"id": 356, "seek": 135996, "start": 1387.44, "end": 1388.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.19031310292471826, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.2605791198438965e-05}, {"id": 357, "seek": 135996, "start": 1388.44, "end": 1389.44, "text": " Here's another one.", "tokens": [1692, 311, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.19031310292471826, "compression_ratio": 1.5977011494252873, "no_speech_prob": 1.2605791198438965e-05}, {"id": 358, "seek": 138944, "start": 1389.44, "end": 1392.52, "text": " I put blank fork down on the table.", "tokens": [286, 829, 8247, 17716, 760, 322, 264, 3199, 13], "temperature": 0.0, "avg_logprob": -0.20617598295211792, "compression_ratio": 1.6862745098039216, "no_speech_prob": 4.330721276346594e-05}, {"id": 359, "seek": 138944, "start": 1392.52, "end": 1394.72, "text": " This one is under specified.", "tokens": [639, 472, 307, 833, 22206, 13], "temperature": 0.0, "avg_logprob": -0.20617598295211792, "compression_ratio": 1.6862745098039216, "no_speech_prob": 4.330721276346594e-05}, {"id": 360, "seek": 138944, "start": 1394.72, "end": 1404.64, "text": " So this could be the fork, my fork, his fork, her fork, some fork, yeah, a fork.", "tokens": [407, 341, 727, 312, 264, 17716, 11, 452, 17716, 11, 702, 17716, 11, 720, 17716, 11, 512, 17716, 11, 1338, 11, 257, 17716, 13], "temperature": 0.0, "avg_logprob": -0.20617598295211792, "compression_ratio": 1.6862745098039216, "no_speech_prob": 4.330721276346594e-05}, {"id": 361, "seek": 138944, "start": 1404.64, "end": 1409.8400000000001, "text": " So this is, you know, specifying the kinds of syntactic categories of things that can", "tokens": [407, 341, 307, 11, 291, 458, 11, 1608, 5489, 264, 3685, 295, 23980, 19892, 10479, 295, 721, 300, 393], "temperature": 0.0, "avg_logprob": -0.20617598295211792, "compression_ratio": 1.6862745098039216, "no_speech_prob": 4.330721276346594e-05}, {"id": 362, "seek": 138944, "start": 1409.8400000000001, "end": 1412.56, "text": " sort of appear in this context.", "tokens": [1333, 295, 4204, 294, 341, 4319, 13], "temperature": 0.0, "avg_logprob": -0.20617598295211792, "compression_ratio": 1.6862745098039216, "no_speech_prob": 4.330721276346594e-05}, {"id": 363, "seek": 138944, "start": 1412.56, "end": 1417.48, "text": " So this is another thing that you might be able to learn from such an objective.", "tokens": [407, 341, 307, 1071, 551, 300, 291, 1062, 312, 1075, 281, 1466, 490, 1270, 364, 10024, 13], "temperature": 0.0, "avg_logprob": -0.20617598295211792, "compression_ratio": 1.6862745098039216, "no_speech_prob": 4.330721276346594e-05}, {"id": 364, "seek": 141748, "start": 1417.48, "end": 1422.4, "text": " So you have the woman walked across the street, checking for traffic over blank shoulder.", "tokens": [407, 291, 362, 264, 3059, 7628, 2108, 264, 4838, 11, 8568, 337, 6419, 670, 8247, 7948, 13], "temperature": 0.0, "avg_logprob": -0.17287482563247028, "compression_ratio": 1.7170542635658914, "no_speech_prob": 4.330768570071086e-05}, {"id": 365, "seek": 141748, "start": 1422.4, "end": 1424.68, "text": " One of the things that could go over here is her.", "tokens": [1485, 295, 264, 721, 300, 727, 352, 670, 510, 307, 720, 13], "temperature": 0.0, "avg_logprob": -0.17287482563247028, "compression_ratio": 1.7170542635658914, "no_speech_prob": 4.330768570071086e-05}, {"id": 366, "seek": 141748, "start": 1424.68, "end": 1427.52, "text": " That's a co-reference statement.", "tokens": [663, 311, 257, 598, 12, 265, 5158, 5629, 13], "temperature": 0.0, "avg_logprob": -0.17287482563247028, "compression_ratio": 1.7170542635658914, "no_speech_prob": 4.330768570071086e-05}, {"id": 367, "seek": 141748, "start": 1427.52, "end": 1433.76, "text": " So you could learn sort of connections between entities in a text where one word woman can", "tokens": [407, 291, 727, 1466, 1333, 295, 9271, 1296, 16667, 294, 257, 2487, 689, 472, 1349, 3059, 393], "temperature": 0.0, "avg_logprob": -0.17287482563247028, "compression_ratio": 1.7170542635658914, "no_speech_prob": 4.330768570071086e-05}, {"id": 368, "seek": 141748, "start": 1433.76, "end": 1441.2, "text": " also co-refer to the same entity in the world as this word, this pronoun her.", "tokens": [611, 598, 12, 265, 612, 281, 264, 912, 13977, 294, 264, 1002, 382, 341, 1349, 11, 341, 14144, 720, 13], "temperature": 0.0, "avg_logprob": -0.17287482563247028, "compression_ratio": 1.7170542635658914, "no_speech_prob": 4.330768570071086e-05}, {"id": 369, "seek": 141748, "start": 1441.2, "end": 1446.1200000000001, "text": " Here you could think about, you know, I went to the ocean to see the fish, turtles, seals,", "tokens": [1692, 291, 727, 519, 466, 11, 291, 458, 11, 286, 1437, 281, 264, 7810, 281, 536, 264, 3506, 11, 32422, 11, 32031, 11], "temperature": 0.0, "avg_logprob": -0.17287482563247028, "compression_ratio": 1.7170542635658914, "no_speech_prob": 4.330768570071086e-05}, {"id": 370, "seek": 141748, "start": 1446.1200000000001, "end": 1447.1200000000001, "text": " and blank.", "tokens": [293, 8247, 13], "temperature": 0.0, "avg_logprob": -0.17287482563247028, "compression_ratio": 1.7170542635658914, "no_speech_prob": 4.330768570071086e-05}, {"id": 371, "seek": 144712, "start": 1447.12, "end": 1450.4399999999998, "text": " Here I don't think there's a single correct answer as to what we could see going into that", "tokens": [1692, 286, 500, 380, 519, 456, 311, 257, 2167, 3006, 1867, 382, 281, 437, 321, 727, 536, 516, 666, 300], "temperature": 0.0, "avg_logprob": -0.16800618171691895, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.3996842578053474e-05}, {"id": 372, "seek": 144712, "start": 1450.4399999999998, "end": 1451.8, "text": " blank.", "tokens": [8247, 13], "temperature": 0.0, "avg_logprob": -0.16800618171691895, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.3996842578053474e-05}, {"id": 373, "seek": 144712, "start": 1451.8, "end": 1455.3999999999999, "text": " But a model could learn a distribution of the kinds of things that people might be talking", "tokens": [583, 257, 2316, 727, 1466, 257, 7316, 295, 264, 3685, 295, 721, 300, 561, 1062, 312, 1417], "temperature": 0.0, "avg_logprob": -0.16800618171691895, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.3996842578053474e-05}, {"id": 374, "seek": 144712, "start": 1455.3999999999999, "end": 1460.3999999999999, "text": " about when they, one, go to the ocean and two, are excited to see marine life.", "tokens": [466, 562, 436, 11, 472, 11, 352, 281, 264, 7810, 293, 732, 11, 366, 2919, 281, 536, 20246, 993, 13], "temperature": 0.0, "avg_logprob": -0.16800618171691895, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.3996842578053474e-05}, {"id": 375, "seek": 144712, "start": 1460.3999999999999, "end": 1461.3999999999999, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.16800618171691895, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.3996842578053474e-05}, {"id": 376, "seek": 144712, "start": 1461.3999999999999, "end": 1465.56, "text": " So this is sort of a semantic category, a lexical semantic category of things that might", "tokens": [407, 341, 307, 1333, 295, 257, 47982, 7719, 11, 257, 476, 87, 804, 47982, 7719, 295, 721, 300, 1062], "temperature": 0.0, "avg_logprob": -0.16800618171691895, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.3996842578053474e-05}, {"id": 377, "seek": 144712, "start": 1465.56, "end": 1472.12, "text": " sort of be in the same set of interest as fish, turtles, and seals in the context of", "tokens": [1333, 295, 312, 294, 264, 912, 992, 295, 1179, 382, 3506, 11, 32422, 11, 293, 32031, 294, 264, 4319, 295], "temperature": 0.0, "avg_logprob": -0.16800618171691895, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.3996842578053474e-05}, {"id": 378, "seek": 144712, "start": 1472.12, "end": 1474.4399999999998, "text": " I went to the ocean.", "tokens": [286, 1437, 281, 264, 7810, 13], "temperature": 0.0, "avg_logprob": -0.16800618171691895, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.3996842578053474e-05}, {"id": 379, "seek": 144712, "start": 1474.4399999999998, "end": 1476.2399999999998, "text": " Okay?", "tokens": [1033, 30], "temperature": 0.0, "avg_logprob": -0.16800618171691895, "compression_ratio": 1.7272727272727273, "no_speech_prob": 4.3996842578053474e-05}, {"id": 380, "seek": 147624, "start": 1476.24, "end": 1480.48, "text": " So, and, you know, man, I expect that there would be examples of this in a large corpus", "tokens": [407, 11, 293, 11, 291, 458, 11, 587, 11, 286, 2066, 300, 456, 576, 312, 5110, 295, 341, 294, 257, 2416, 1181, 31624], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 381, "seek": 147624, "start": 1480.48, "end": 1481.48, "text": " of text.", "tokens": [295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 382, "seek": 147624, "start": 1481.48, "end": 1483.48, "text": " Maybe it may be a book.", "tokens": [2704, 309, 815, 312, 257, 1446, 13], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 383, "seek": 147624, "start": 1483.48, "end": 1484.48, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 384, "seek": 147624, "start": 1484.48, "end": 1486.72, "text": " Here's another example.", "tokens": [1692, 311, 1071, 1365, 13], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 385, "seek": 147624, "start": 1486.72, "end": 1492.56, "text": " Overall, the value I got from the two hours watching it was the sum total of the popcorn", "tokens": [18420, 11, 264, 2158, 286, 658, 490, 264, 732, 2496, 1976, 309, 390, 264, 2408, 3217, 295, 264, 25334], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 386, "seek": 147624, "start": 1492.56, "end": 1493.64, "text": " and the drink.", "tokens": [293, 264, 2822, 13], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 387, "seek": 147624, "start": 1493.64, "end": 1495.04, "text": " The movie was blank.", "tokens": [440, 3169, 390, 8247, 13], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 388, "seek": 147624, "start": 1495.04, "end": 1496.04, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 389, "seek": 147624, "start": 1496.04, "end": 1500.2, "text": " And this is when I'd sort of like look out into the audience and say, was the movie better", "tokens": [400, 341, 307, 562, 286, 1116, 1333, 295, 411, 574, 484, 666, 264, 4034, 293, 584, 11, 390, 264, 3169, 1101], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 390, "seek": 147624, "start": 1500.2, "end": 1502.64, "text": " good, but the movie was bad.", "tokens": [665, 11, 457, 264, 3169, 390, 1578, 13], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 391, "seek": 147624, "start": 1502.64, "end": 1504.88, "text": " It's my prediction here.", "tokens": [467, 311, 452, 17630, 510, 13], "temperature": 0.0, "avg_logprob": -0.27325969433966485, "compression_ratio": 1.599250936329588, "no_speech_prob": 7.0271133154165e-05}, {"id": 392, "seek": 150488, "start": 1504.88, "end": 1506.88, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.19473052391639123, "compression_ratio": 1.8863636363636365, "no_speech_prob": 5.304917795001529e-05}, {"id": 393, "seek": 150488, "start": 1506.88, "end": 1509.8400000000001, "text": " And so this is teaching you something about sentiment, about how people express sentiment", "tokens": [400, 370, 341, 307, 4571, 291, 746, 466, 16149, 11, 466, 577, 561, 5109, 16149], "temperature": 0.0, "avg_logprob": -0.19473052391639123, "compression_ratio": 1.8863636363636365, "no_speech_prob": 5.304917795001529e-05}, {"id": 394, "seek": 150488, "start": 1509.8400000000001, "end": 1511.0800000000002, "text": " in language.", "tokens": [294, 2856, 13], "temperature": 0.0, "avg_logprob": -0.19473052391639123, "compression_ratio": 1.8863636363636365, "no_speech_prob": 5.304917795001529e-05}, {"id": 395, "seek": 150488, "start": 1511.0800000000002, "end": 1517.8400000000001, "text": " And so this is, even, it looks like a task itself, like do sentiment analysis is sort", "tokens": [400, 370, 341, 307, 11, 754, 11, 309, 1542, 411, 257, 5633, 2564, 11, 411, 360, 16149, 5215, 307, 1333], "temperature": 0.0, "avg_logprob": -0.19473052391639123, "compression_ratio": 1.8863636363636365, "no_speech_prob": 5.304917795001529e-05}, {"id": 396, "seek": 150488, "start": 1517.8400000000001, "end": 1522.92, "text": " of what you need to do in order to figure out whether the movie was bad or good, or maybe", "tokens": [295, 437, 291, 643, 281, 360, 294, 1668, 281, 2573, 484, 1968, 264, 3169, 390, 1578, 420, 665, 11, 420, 1310], "temperature": 0.0, "avg_logprob": -0.19473052391639123, "compression_ratio": 1.8863636363636365, "no_speech_prob": 5.304917795001529e-05}, {"id": 397, "seek": 150488, "start": 1522.92, "end": 1524.16, "text": " maybe the word is neither bad or good.", "tokens": [1310, 264, 1349, 307, 9662, 1578, 420, 665, 13], "temperature": 0.0, "avg_logprob": -0.19473052391639123, "compression_ratio": 1.8863636363636365, "no_speech_prob": 5.304917795001529e-05}, {"id": 398, "seek": 150488, "start": 1524.16, "end": 1525.96, "text": " The movie was over or something like that.", "tokens": [440, 3169, 390, 670, 420, 746, 411, 300, 13], "temperature": 0.0, "avg_logprob": -0.19473052391639123, "compression_ratio": 1.8863636363636365, "no_speech_prob": 5.304917795001529e-05}, {"id": 399, "seek": 150488, "start": 1525.96, "end": 1530.44, "text": " But like, if you had to choose between is bad or good more likely, right?", "tokens": [583, 411, 11, 498, 291, 632, 281, 2826, 1296, 307, 1578, 420, 665, 544, 3700, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19473052391639123, "compression_ratio": 1.8863636363636365, "no_speech_prob": 5.304917795001529e-05}, {"id": 400, "seek": 150488, "start": 1530.44, "end": 1533.44, "text": " You sort of had to figure out the sentiment of the text.", "tokens": [509, 1333, 295, 632, 281, 2573, 484, 264, 16149, 295, 264, 2487, 13], "temperature": 0.0, "avg_logprob": -0.19473052391639123, "compression_ratio": 1.8863636363636365, "no_speech_prob": 5.304917795001529e-05}, {"id": 401, "seek": 153344, "start": 1533.44, "end": 1536.44, "text": " Now, that's really fascinating.", "tokens": [823, 11, 300, 311, 534, 10343, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 402, "seek": 153344, "start": 1536.44, "end": 1537.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 403, "seek": 153344, "start": 1537.44, "end": 1539.44, "text": " Here's another one.", "tokens": [1692, 311, 1071, 472, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 404, "seek": 153344, "start": 1539.44, "end": 1542.92, "text": " Iro went into the kitchen to make some tea.", "tokens": [286, 340, 1437, 666, 264, 6525, 281, 652, 512, 5817, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 405, "seek": 153344, "start": 1542.92, "end": 1546.44, "text": " Standing next to Iro, Zuko pondered his destiny.", "tokens": [33655, 958, 281, 286, 340, 11, 20991, 78, 17384, 4073, 702, 17893, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 406, "seek": 153344, "start": 1546.44, "end": 1548.44, "text": " Zuko left the blank.", "tokens": [20991, 78, 1411, 264, 8247, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 407, "seek": 153344, "start": 1548.44, "end": 1549.44, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 408, "seek": 153344, "start": 1549.44, "end": 1553.6000000000001, "text": " So this is a little easy because we really only show one place.", "tokens": [407, 341, 307, 257, 707, 1858, 570, 321, 534, 787, 855, 472, 1081, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 409, "seek": 153344, "start": 1553.6000000000001, "end": 1556.24, "text": " I guess we have another now in destiny.", "tokens": [286, 2041, 321, 362, 1071, 586, 294, 17893, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 410, "seek": 153344, "start": 1556.24, "end": 1560.1200000000001, "text": " But this is sort of talking reasoning about spatial location and the movement of sort of", "tokens": [583, 341, 307, 1333, 295, 1417, 21577, 466, 23598, 4914, 293, 264, 3963, 295, 1333, 295], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 411, "seek": 153344, "start": 1560.1200000000001, "end": 1562.56, "text": " agents in an imagined world.", "tokens": [12554, 294, 364, 16590, 1002, 13], "temperature": 0.0, "avg_logprob": -0.2678661346435547, "compression_ratio": 1.6153846153846154, "no_speech_prob": 7.841271144570783e-05}, {"id": 412, "seek": 156256, "start": 1562.56, "end": 1566.04, "text": " We could imagine text that has lines like this.", "tokens": [492, 727, 3811, 2487, 300, 575, 3876, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.20499474179428234, "compression_ratio": 1.6980392156862745, "no_speech_prob": 1.8624346921569668e-05}, {"id": 413, "seek": 156256, "start": 1566.04, "end": 1570.24, "text": " Person went into the place and was next to so and so who left and did that and sort of", "tokens": [8443, 1437, 666, 264, 1081, 293, 390, 958, 281, 370, 293, 370, 567, 1411, 293, 630, 300, 293, 1333, 295], "temperature": 0.0, "avg_logprob": -0.20499474179428234, "compression_ratio": 1.6980392156862745, "no_speech_prob": 1.8624346921569668e-05}, {"id": 414, "seek": 156256, "start": 1570.24, "end": 1572.24, "text": " you have these like relationships.", "tokens": [291, 362, 613, 411, 6159, 13], "temperature": 0.0, "avg_logprob": -0.20499474179428234, "compression_ratio": 1.6980392156862745, "no_speech_prob": 1.8624346921569668e-05}, {"id": 415, "seek": 156256, "start": 1572.24, "end": 1574.28, "text": " So here, Zuko left the kitchen.", "tokens": [407, 510, 11, 20991, 78, 1411, 264, 6525, 13], "temperature": 0.0, "avg_logprob": -0.20499474179428234, "compression_ratio": 1.6980392156862745, "no_speech_prob": 1.8624346921569668e-05}, {"id": 416, "seek": 156256, "start": 1574.28, "end": 1577.6799999999998, "text": " It's the most likely thing that I think would go here.", "tokens": [467, 311, 264, 881, 3700, 551, 300, 286, 519, 576, 352, 510, 13], "temperature": 0.0, "avg_logprob": -0.20499474179428234, "compression_ratio": 1.6980392156862745, "no_speech_prob": 1.8624346921569668e-05}, {"id": 417, "seek": 156256, "start": 1577.6799999999998, "end": 1584.0, "text": " And it sort of indicates that in order for a model to learn to perform this fill in the", "tokens": [400, 309, 1333, 295, 16203, 300, 294, 1668, 337, 257, 2316, 281, 1466, 281, 2042, 341, 2836, 294, 264], "temperature": 0.0, "avg_logprob": -0.20499474179428234, "compression_ratio": 1.6980392156862745, "no_speech_prob": 1.8624346921569668e-05}, {"id": 418, "seek": 156256, "start": 1584.0, "end": 1592.36, "text": " missing part task, it might need to, in general, figure out sort of where things are and", "tokens": [5361, 644, 5633, 11, 309, 1062, 643, 281, 11, 294, 2674, 11, 2573, 484, 1333, 295, 689, 721, 366, 293], "temperature": 0.0, "avg_logprob": -0.20499474179428234, "compression_ratio": 1.6980392156862745, "no_speech_prob": 1.8624346921569668e-05}, {"id": 419, "seek": 159236, "start": 1592.36, "end": 1596.76, "text": " whether statements mean or imply that locality.", "tokens": [1968, 12363, 914, 420, 33616, 300, 1628, 1860, 13], "temperature": 0.0, "avg_logprob": -0.23127548140708845, "compression_ratio": 1.719387755102041, "no_speech_prob": 4.0285872273670975e-06}, {"id": 420, "seek": 159236, "start": 1596.76, "end": 1600.1599999999999, "text": " So standing next to Iro went into the kitchen.", "tokens": [407, 4877, 958, 281, 286, 340, 1437, 666, 264, 6525, 13], "temperature": 0.0, "avg_logprob": -0.23127548140708845, "compression_ratio": 1.719387755102041, "no_speech_prob": 4.0285872273670975e-06}, {"id": 421, "seek": 159236, "start": 1600.1599999999999, "end": 1605.6, "text": " Now Iro is in the kitchen and then standing next to Iro means Zuko is now in the kitchen.", "tokens": [823, 286, 340, 307, 294, 264, 6525, 293, 550, 4877, 958, 281, 286, 340, 1355, 20991, 78, 307, 586, 294, 264, 6525, 13], "temperature": 0.0, "avg_logprob": -0.23127548140708845, "compression_ratio": 1.719387755102041, "no_speech_prob": 4.0285872273670975e-06}, {"id": 422, "seek": 159236, "start": 1605.6, "end": 1608.28, "text": " And then Zuko now leaves where?", "tokens": [400, 550, 20991, 78, 586, 5510, 689, 30], "temperature": 0.0, "avg_logprob": -0.23127548140708845, "compression_ratio": 1.719387755102041, "no_speech_prob": 4.0285872273670975e-06}, {"id": 423, "seek": 159236, "start": 1608.28, "end": 1610.1999999999998, "text": " Well, he was in the kitchen before.", "tokens": [1042, 11, 415, 390, 294, 264, 6525, 949, 13], "temperature": 0.0, "avg_logprob": -0.23127548140708845, "compression_ratio": 1.719387755102041, "no_speech_prob": 4.0285872273670975e-06}, {"id": 424, "seek": 159236, "start": 1610.1999999999998, "end": 1613.08, "text": " So this is sort of a very basic sense of reasoning.", "tokens": [407, 341, 307, 1333, 295, 257, 588, 3875, 2020, 295, 21577, 13], "temperature": 0.0, "avg_logprob": -0.23127548140708845, "compression_ratio": 1.719387755102041, "no_speech_prob": 4.0285872273670975e-06}, {"id": 425, "seek": 159236, "start": 1613.08, "end": 1615.08, "text": " Now this one.", "tokens": [823, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.23127548140708845, "compression_ratio": 1.719387755102041, "no_speech_prob": 4.0285872273670975e-06}, {"id": 426, "seek": 159236, "start": 1615.08, "end": 1616.6799999999998, "text": " Here's a sentence.", "tokens": [1692, 311, 257, 8174, 13], "temperature": 0.0, "avg_logprob": -0.23127548140708845, "compression_ratio": 1.719387755102041, "no_speech_prob": 4.0285872273670975e-06}, {"id": 427, "seek": 161668, "start": 1616.68, "end": 1623.44, "text": " I was thinking about the sequence that goes 1, 1, 2, 3, 5, 8, 13, 21, blank.", "tokens": [286, 390, 1953, 466, 264, 8310, 300, 1709, 502, 11, 502, 11, 568, 11, 805, 11, 1025, 11, 1649, 11, 3705, 11, 5080, 11, 8247, 13], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 428, "seek": 161668, "start": 1623.44, "end": 1625.28, "text": " So I don't know.", "tokens": [407, 286, 500, 380, 458, 13], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 429, "seek": 161668, "start": 1625.28, "end": 1627.0, "text": " I can imagine people writing stuff.", "tokens": [286, 393, 3811, 561, 3579, 1507, 13], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 430, "seek": 161668, "start": 1627.0, "end": 1629.04, "text": " So this is the Fibonacci sequence.", "tokens": [407, 341, 307, 264, 479, 897, 266, 43870, 8310, 13], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 431, "seek": 161668, "start": 1629.04, "end": 1632.0, "text": " And sort of you know you use some of these two to get the next one, some of these two", "tokens": [400, 1333, 295, 291, 458, 291, 764, 512, 295, 613, 732, 281, 483, 264, 958, 472, 11, 512, 295, 613, 732], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 432, "seek": 161668, "start": 1632.0, "end": 1634.1200000000001, "text": " to get the next one, some of these two.", "tokens": [281, 483, 264, 958, 472, 11, 512, 295, 613, 732, 13], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 433, "seek": 161668, "start": 1634.1200000000001, "end": 1636.52, "text": " And so you have this running sum.", "tokens": [400, 370, 291, 362, 341, 2614, 2408, 13], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 434, "seek": 161668, "start": 1636.52, "end": 1637.52, "text": " It's a famous sequence.", "tokens": [467, 311, 257, 4618, 8310, 13], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 435, "seek": 161668, "start": 1637.52, "end": 1640.64, "text": " It shows up in a lot of text on the internet.", "tokens": [467, 3110, 493, 294, 257, 688, 295, 2487, 322, 264, 4705, 13], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 436, "seek": 161668, "start": 1640.64, "end": 1646.24, "text": " And in general you have to learn the algorithm or just the formula, I guess, that defines the", "tokens": [400, 294, 2674, 291, 362, 281, 1466, 264, 9284, 420, 445, 264, 8513, 11, 286, 2041, 11, 300, 23122, 264], "temperature": 0.0, "avg_logprob": -0.2180424888662044, "compression_ratio": 1.8345864661654134, "no_speech_prob": 7.250050111906603e-05}, {"id": 437, "seek": 164624, "start": 1646.24, "end": 1649.64, "text": " Fibonacci sequence in order to keep going.", "tokens": [479, 897, 266, 43870, 8310, 294, 1668, 281, 1066, 516, 13], "temperature": 0.0, "avg_logprob": -0.2225874850624486, "compression_ratio": 1.7638888888888888, "no_speech_prob": 4.756365524372086e-05}, {"id": 438, "seek": 164624, "start": 1649.64, "end": 1652.16, "text": " Do models in this in practice?", "tokens": [1144, 5245, 294, 341, 294, 3124, 30], "temperature": 0.0, "avg_logprob": -0.2225874850624486, "compression_ratio": 1.7638888888888888, "no_speech_prob": 4.756365524372086e-05}, {"id": 439, "seek": 164624, "start": 1652.16, "end": 1653.16, "text": " Wait and find out.", "tokens": [3802, 293, 915, 484, 13], "temperature": 0.0, "avg_logprob": -0.2225874850624486, "compression_ratio": 1.7638888888888888, "no_speech_prob": 4.756365524372086e-05}, {"id": 440, "seek": 164624, "start": 1653.16, "end": 1657.96, "text": " But you would have to learn it in order to get the sequence to keep going and going and", "tokens": [583, 291, 576, 362, 281, 1466, 309, 294, 1668, 281, 483, 264, 8310, 281, 1066, 516, 293, 516, 293], "temperature": 0.0, "avg_logprob": -0.2225874850624486, "compression_ratio": 1.7638888888888888, "no_speech_prob": 4.756365524372086e-05}, {"id": 441, "seek": 164624, "start": 1657.96, "end": 1659.96, "text": " going.", "tokens": [516, 13], "temperature": 0.0, "avg_logprob": -0.2225874850624486, "compression_ratio": 1.7638888888888888, "no_speech_prob": 4.756365524372086e-05}, {"id": 442, "seek": 164624, "start": 1659.96, "end": 1666.52, "text": " OK, so we're going to get into specific pre-trained models, specific methods of pre-training", "tokens": [2264, 11, 370, 321, 434, 516, 281, 483, 666, 2685, 659, 12, 17227, 2001, 5245, 11, 2685, 7150, 295, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.2225874850624486, "compression_ratio": 1.7638888888888888, "no_speech_prob": 4.756365524372086e-05}, {"id": 443, "seek": 164624, "start": 1666.52, "end": 1667.52, "text": " now.", "tokens": [586, 13], "temperature": 0.0, "avg_logprob": -0.2225874850624486, "compression_ratio": 1.7638888888888888, "no_speech_prob": 4.756365524372086e-05}, {"id": 444, "seek": 164624, "start": 1667.52, "end": 1676.1200000000001, "text": " So I'm going to go over a brief review of transformer encoders, decoders, and encoder decoders.", "tokens": [407, 286, 478, 516, 281, 352, 670, 257, 5353, 3131, 295, 31782, 2058, 378, 433, 11, 979, 378, 433, 11, 293, 2058, 19866, 979, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.2225874850624486, "compression_ratio": 1.7638888888888888, "no_speech_prob": 4.756365524372086e-05}, {"id": 445, "seek": 167612, "start": 1676.12, "end": 1678.9199999999998, "text": " Because we're going to get into the sort of technical bits now.", "tokens": [1436, 321, 434, 516, 281, 483, 666, 264, 1333, 295, 6191, 9239, 586, 13], "temperature": 0.0, "avg_logprob": -0.4345701233414579, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00013330875663086772}, {"id": 446, "seek": 167612, "start": 1678.9199999999998, "end": 1681.9199999999998, "text": " So before I do that, I'm going to pause.", "tokens": [407, 949, 286, 360, 300, 11, 286, 478, 516, 281, 10465, 13], "temperature": 0.0, "avg_logprob": -0.4345701233414579, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00013330875663086772}, {"id": 447, "seek": 167612, "start": 1681.9199999999998, "end": 1682.9199999999998, "text": " Are there any questions?", "tokens": [2014, 456, 604, 1651, 30], "temperature": 0.0, "avg_logprob": -0.4345701233414579, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00013330875663086772}, {"id": 448, "seek": 167612, "start": 1682.9199999999998, "end": 1691.12, "text": " Yeah, there's an interesting question asked about, do these co-opening our model on our", "tokens": [865, 11, 456, 311, 364, 1880, 1168, 2351, 466, 11, 360, 613, 598, 12, 404, 4559, 527, 2316, 322, 527], "temperature": 0.0, "avg_logprob": -0.4345701233414579, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00013330875663086772}, {"id": 449, "seek": 167612, "start": 1691.12, "end": 1693.12, "text": " input training data and the link to training?", "tokens": [4846, 3097, 1412, 293, 264, 2113, 281, 3097, 30], "temperature": 0.0, "avg_logprob": -0.4345701233414579, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00013330875663086772}, {"id": 450, "seek": 167612, "start": 1693.12, "end": 1697.8799999999999, "text": " And we need to also add some questions in the light of the huge between models that we", "tokens": [400, 321, 643, 281, 611, 909, 512, 1651, 294, 264, 1442, 295, 264, 2603, 1296, 5245, 300, 321], "temperature": 0.0, "avg_logprob": -0.4345701233414579, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00013330875663086772}, {"id": 451, "seek": 167612, "start": 1697.8799999999999, "end": 1699.8799999999999, "text": " think nowadays.", "tokens": [519, 13434, 13], "temperature": 0.0, "avg_logprob": -0.4345701233414579, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00013330875663086772}, {"id": 452, "seek": 167612, "start": 1699.8799999999999, "end": 1705.4399999999998, "text": " Sorry, the first part of that question, was it, are we overfitting our models to what?", "tokens": [4919, 11, 264, 700, 644, 295, 300, 1168, 11, 390, 309, 11, 366, 321, 670, 69, 2414, 527, 5245, 281, 437, 30], "temperature": 0.0, "avg_logprob": -0.4345701233414579, "compression_ratio": 1.7224334600760456, "no_speech_prob": 0.00013330875663086772}, {"id": 453, "seek": 170544, "start": 1705.44, "end": 1709.8, "text": " Yes, so the risk of almost getting our model on our input training data when they're", "tokens": [1079, 11, 370, 264, 3148, 295, 1920, 1242, 527, 2316, 322, 527, 4846, 3097, 1412, 562, 436, 434], "temperature": 0.0, "avg_logprob": -0.19685349813321742, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00010716608085203916}, {"id": 454, "seek": 170544, "start": 1709.8, "end": 1710.8, "text": " doing training?", "tokens": [884, 3097, 30], "temperature": 0.0, "avg_logprob": -0.19685349813321742, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00010716608085203916}, {"id": 455, "seek": 170544, "start": 1710.8, "end": 1711.8, "text": " Got it.", "tokens": [5803, 309, 13], "temperature": 0.0, "avg_logprob": -0.19685349813321742, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00010716608085203916}, {"id": 456, "seek": 170544, "start": 1711.8, "end": 1714.3600000000001, "text": " Yeah, so that's a good point.", "tokens": [865, 11, 370, 300, 311, 257, 665, 935, 13], "temperature": 0.0, "avg_logprob": -0.19685349813321742, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00010716608085203916}, {"id": 457, "seek": 170544, "start": 1714.3600000000001, "end": 1716.68, "text": " So we're using very large models.", "tokens": [407, 321, 434, 1228, 588, 2416, 5245, 13], "temperature": 0.0, "avg_logprob": -0.19685349813321742, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00010716608085203916}, {"id": 458, "seek": 170544, "start": 1716.68, "end": 1720.28, "text": " And we might imagine that there's a risk of overfitting.", "tokens": [400, 321, 1062, 3811, 300, 456, 311, 257, 3148, 295, 670, 69, 2414, 13], "temperature": 0.0, "avg_logprob": -0.19685349813321742, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00010716608085203916}, {"id": 459, "seek": 170544, "start": 1720.28, "end": 1725.28, "text": " And in practice, yeah, it's actually one of the more crucial things to do to make pre-training", "tokens": [400, 294, 3124, 11, 1338, 11, 309, 311, 767, 472, 295, 264, 544, 11462, 721, 281, 360, 281, 652, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.19685349813321742, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00010716608085203916}, {"id": 460, "seek": 170544, "start": 1725.28, "end": 1726.28, "text": " work.", "tokens": [589, 13], "temperature": 0.0, "avg_logprob": -0.19685349813321742, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00010716608085203916}, {"id": 461, "seek": 170544, "start": 1726.28, "end": 1731.3200000000002, "text": " So that turns out that you need to have a lot, a lot of data, like a lot of data.", "tokens": [407, 300, 4523, 484, 300, 291, 643, 281, 362, 257, 688, 11, 257, 688, 295, 1412, 11, 411, 257, 688, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.19685349813321742, "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.00010716608085203916}, {"id": 462, "seek": 173132, "start": 1731.32, "end": 1736.4399999999998, "text": " And in fact, we'll show results later on where people built a pre-trained model, pre-trained", "tokens": [400, 294, 1186, 11, 321, 603, 855, 3542, 1780, 322, 689, 561, 3094, 257, 659, 12, 17227, 2001, 2316, 11, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.11091847980723661, "compression_ratio": 1.7491166077738516, "no_speech_prob": 2.9301831091288477e-05}, {"id": 463, "seek": 173132, "start": 1736.4399999999998, "end": 1738.36, "text": " it on a lot of data.", "tokens": [309, 322, 257, 688, 295, 1412, 13], "temperature": 0.0, "avg_logprob": -0.11091847980723661, "compression_ratio": 1.7491166077738516, "no_speech_prob": 2.9301831091288477e-05}, {"id": 464, "seek": 173132, "start": 1738.36, "end": 1742.1599999999999, "text": " And then like six months later, someone else came along and was like, hey, if you pre-trained", "tokens": [400, 550, 411, 2309, 2493, 1780, 11, 1580, 1646, 1361, 2051, 293, 390, 411, 11, 4177, 11, 498, 291, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.11091847980723661, "compression_ratio": 1.7491166077738516, "no_speech_prob": 2.9301831091288477e-05}, {"id": 465, "seek": 173132, "start": 1742.1599999999999, "end": 1746.28, "text": " it on 10 months later and changed almost nothing else, it would have gone even better.", "tokens": [309, 322, 1266, 2493, 1780, 293, 3105, 1920, 1825, 1646, 11, 309, 576, 362, 2780, 754, 1101, 13], "temperature": 0.0, "avg_logprob": -0.11091847980723661, "compression_ratio": 1.7491166077738516, "no_speech_prob": 2.9301831091288477e-05}, {"id": 466, "seek": 173132, "start": 1746.28, "end": 1747.6, "text": " Now was it overfitting?", "tokens": [823, 390, 309, 670, 69, 2414, 30], "temperature": 0.0, "avg_logprob": -0.11091847980723661, "compression_ratio": 1.7491166077738516, "no_speech_prob": 2.9301831091288477e-05}, {"id": 467, "seek": 173132, "start": 1747.6, "end": 1753.12, "text": " I mean, you can sort of like hold out some text during pre-training, right, and sort", "tokens": [286, 914, 11, 291, 393, 1333, 295, 411, 1797, 484, 512, 2487, 1830, 659, 12, 17227, 1760, 11, 558, 11, 293, 1333], "temperature": 0.0, "avg_logprob": -0.11091847980723661, "compression_ratio": 1.7491166077738516, "no_speech_prob": 2.9301831091288477e-05}, {"id": 468, "seek": 173132, "start": 1753.12, "end": 1758.2, "text": " of evaluate the perplexity, right, the language modeling performance on that held out text.", "tokens": [295, 13059, 264, 680, 18945, 507, 11, 558, 11, 264, 2856, 15983, 3389, 322, 300, 5167, 484, 2487, 13], "temperature": 0.0, "avg_logprob": -0.11091847980723661, "compression_ratio": 1.7491166077738516, "no_speech_prob": 2.9301831091288477e-05}, {"id": 469, "seek": 175820, "start": 1758.2, "end": 1762.6000000000001, "text": " And it tends to be the case that actually these models are underfitting, right, that we", "tokens": [400, 309, 12258, 281, 312, 264, 1389, 300, 767, 613, 5245, 366, 833, 69, 2414, 11, 558, 11, 300, 321], "temperature": 0.0, "avg_logprob": -0.13912669632786004, "compression_ratio": 1.7508650519031141, "no_speech_prob": 5.918353417655453e-05}, {"id": 470, "seek": 175820, "start": 1762.6000000000001, "end": 1768.16, "text": " need even larger and larger models to express the complex interactions that allow us to", "tokens": [643, 754, 4833, 293, 4833, 5245, 281, 5109, 264, 3997, 13280, 300, 2089, 505, 281], "temperature": 0.0, "avg_logprob": -0.13912669632786004, "compression_ratio": 1.7508650519031141, "no_speech_prob": 5.918353417655453e-05}, {"id": 471, "seek": 175820, "start": 1768.16, "end": 1770.92, "text": " fit these datasets better.", "tokens": [3318, 613, 42856, 1101, 13], "temperature": 0.0, "avg_logprob": -0.13912669632786004, "compression_ratio": 1.7508650519031141, "no_speech_prob": 5.918353417655453e-05}, {"id": 472, "seek": 175820, "start": 1770.92, "end": 1773.6000000000001, "text": " And so we'll talk about that when we talk about BERT.", "tokens": [400, 370, 321, 603, 751, 466, 300, 562, 321, 751, 466, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.13912669632786004, "compression_ratio": 1.7508650519031141, "no_speech_prob": 5.918353417655453e-05}, {"id": 473, "seek": 175820, "start": 1773.6000000000001, "end": 1777.68, "text": " And one of the really interesting results is that BERT is underfit, not overfit, but", "tokens": [400, 472, 295, 264, 534, 1880, 3542, 307, 300, 363, 31479, 307, 833, 6845, 11, 406, 670, 6845, 11, 457], "temperature": 0.0, "avg_logprob": -0.13912669632786004, "compression_ratio": 1.7508650519031141, "no_speech_prob": 5.918353417655453e-05}, {"id": 474, "seek": 175820, "start": 1777.68, "end": 1782.2, "text": " in principle, yes, it's a problem to, this potentially a problem to overfit.", "tokens": [294, 8665, 11, 2086, 11, 309, 311, 257, 1154, 281, 11, 341, 7263, 257, 1154, 281, 670, 6845, 13], "temperature": 0.0, "avg_logprob": -0.13912669632786004, "compression_ratio": 1.7508650519031141, "no_speech_prob": 5.918353417655453e-05}, {"id": 475, "seek": 175820, "start": 1782.2, "end": 1786.96, "text": " But we end up having a ton of text in English at least, although not in every language.", "tokens": [583, 321, 917, 493, 1419, 257, 2952, 295, 2487, 294, 3669, 412, 1935, 11, 4878, 406, 294, 633, 2856, 13], "temperature": 0.0, "avg_logprob": -0.13912669632786004, "compression_ratio": 1.7508650519031141, "no_speech_prob": 5.918353417655453e-05}, {"id": 476, "seek": 178696, "start": 1786.96, "end": 1791.68, "text": " And so, yeah, it's important to scale them, but currently our models don't seem overfit", "tokens": [400, 370, 11, 1338, 11, 309, 311, 1021, 281, 4373, 552, 11, 457, 4362, 527, 5245, 500, 380, 1643, 670, 6845], "temperature": 0.0, "avg_logprob": -0.2919519577903309, "compression_ratio": 1.4974093264248705, "no_speech_prob": 4.068468842888251e-05}, {"id": 477, "seek": 178696, "start": 1791.68, "end": 1793.96, "text": " to the pre-training text.", "tokens": [281, 264, 659, 12, 17227, 1760, 2487, 13], "temperature": 0.0, "avg_logprob": -0.2919519577903309, "compression_ratio": 1.4974093264248705, "no_speech_prob": 4.068468842888251e-05}, {"id": 478, "seek": 178696, "start": 1793.96, "end": 1794.96, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2919519577903309, "compression_ratio": 1.4974093264248705, "no_speech_prob": 4.068468842888251e-05}, {"id": 479, "seek": 178696, "start": 1794.96, "end": 1802.44, "text": " Any other questions?", "tokens": [2639, 661, 1651, 30], "temperature": 0.0, "avg_logprob": -0.2919519577903309, "compression_ratio": 1.4974093264248705, "no_speech_prob": 4.068468842888251e-05}, {"id": 480, "seek": 178696, "start": 1802.44, "end": 1805.68, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.2919519577903309, "compression_ratio": 1.4974093264248705, "no_speech_prob": 4.068468842888251e-05}, {"id": 481, "seek": 178696, "start": 1805.68, "end": 1807.88, "text": " So we saw this figure before, right here.", "tokens": [407, 321, 1866, 341, 2573, 949, 11, 558, 510, 13], "temperature": 0.0, "avg_logprob": -0.2919519577903309, "compression_ratio": 1.4974093264248705, "no_speech_prob": 4.068468842888251e-05}, {"id": 482, "seek": 178696, "start": 1807.88, "end": 1812.44, "text": " We saw this figure of a transformer encoder to coder from this paper attention is all you", "tokens": [492, 1866, 341, 2573, 295, 257, 31782, 2058, 19866, 281, 17656, 260, 490, 341, 3035, 3202, 307, 439, 291], "temperature": 0.0, "avg_logprob": -0.2919519577903309, "compression_ratio": 1.4974093264248705, "no_speech_prob": 4.068468842888251e-05}, {"id": 483, "seek": 178696, "start": 1812.44, "end": 1814.28, "text": " need.", "tokens": [643, 13], "temperature": 0.0, "avg_logprob": -0.2919519577903309, "compression_ratio": 1.4974093264248705, "no_speech_prob": 4.068468842888251e-05}, {"id": 484, "seek": 181428, "start": 1814.28, "end": 1817.44, "text": " And so we have a couple of things.", "tokens": [400, 370, 321, 362, 257, 1916, 295, 721, 13], "temperature": 0.0, "avg_logprob": -0.2138237645549159, "compression_ratio": 1.6977611940298507, "no_speech_prob": 6.012740414007567e-05}, {"id": 485, "seek": 181428, "start": 1817.44, "end": 1822.44, "text": " We're not going to go over the form of attention again today because we have a lot to go over,", "tokens": [492, 434, 406, 516, 281, 352, 670, 264, 1254, 295, 3202, 797, 965, 570, 321, 362, 257, 688, 281, 352, 670, 11], "temperature": 0.0, "avg_logprob": -0.2138237645549159, "compression_ratio": 1.6977611940298507, "no_speech_prob": 6.012740414007567e-05}, {"id": 486, "seek": 181428, "start": 1822.44, "end": 1825.56, "text": " but I'm happy to chat about it more on Ed.", "tokens": [457, 286, 478, 2055, 281, 5081, 466, 309, 544, 322, 3977, 13], "temperature": 0.0, "avg_logprob": -0.2138237645549159, "compression_ratio": 1.6977611940298507, "no_speech_prob": 6.012740414007567e-05}, {"id": 487, "seek": 181428, "start": 1825.56, "end": 1828.6, "text": " But so in our encoder, we have some input sequence.", "tokens": [583, 370, 294, 527, 2058, 19866, 11, 321, 362, 512, 4846, 8310, 13], "temperature": 0.0, "avg_logprob": -0.2138237645549159, "compression_ratio": 1.6977611940298507, "no_speech_prob": 6.012740414007567e-05}, {"id": 488, "seek": 181428, "start": 1828.6, "end": 1831.8, "text": " Remember, this is a sequence of sub words now.", "tokens": [5459, 11, 341, 307, 257, 8310, 295, 1422, 2283, 586, 13], "temperature": 0.0, "avg_logprob": -0.2138237645549159, "compression_ratio": 1.6977611940298507, "no_speech_prob": 6.012740414007567e-05}, {"id": 489, "seek": 181428, "start": 1831.8, "end": 1834.56, "text": " Each sub word gets a word embedding.", "tokens": [6947, 1422, 1349, 2170, 257, 1349, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.2138237645549159, "compression_ratio": 1.6977611940298507, "no_speech_prob": 6.012740414007567e-05}, {"id": 490, "seek": 181428, "start": 1834.56, "end": 1838.28, "text": " And each index in the transformer gets a position embedding.", "tokens": [400, 1184, 8186, 294, 264, 31782, 2170, 257, 2535, 12240, 3584, 13], "temperature": 0.0, "avg_logprob": -0.2138237645549159, "compression_ratio": 1.6977611940298507, "no_speech_prob": 6.012740414007567e-05}, {"id": 491, "seek": 181428, "start": 1838.28, "end": 1844.08, "text": " Now remember that we have a finite length that our sequence can possibly be like 512.", "tokens": [823, 1604, 300, 321, 362, 257, 19362, 4641, 300, 527, 8310, 393, 6264, 312, 411, 1025, 4762, 13], "temperature": 0.0, "avg_logprob": -0.2138237645549159, "compression_ratio": 1.6977611940298507, "no_speech_prob": 6.012740414007567e-05}, {"id": 492, "seek": 184408, "start": 1844.08, "end": 1845.08, "text": " That's tokens.", "tokens": [663, 311, 22667, 13], "temperature": 0.0, "avg_logprob": -0.18789128644750752, "compression_ratio": 1.653225806451613, "no_speech_prob": 3.590901178540662e-05}, {"id": 493, "seek": 184408, "start": 1845.08, "end": 1847.52, "text": " That was that capital T from last lecture.", "tokens": [663, 390, 300, 4238, 314, 490, 1036, 7991, 13], "temperature": 0.0, "avg_logprob": -0.18789128644750752, "compression_ratio": 1.653225806451613, "no_speech_prob": 3.590901178540662e-05}, {"id": 494, "seek": 184408, "start": 1847.52, "end": 1848.6799999999998, "text": " So you have some finite length.", "tokens": [407, 291, 362, 512, 19362, 4641, 13], "temperature": 0.0, "avg_logprob": -0.18789128644750752, "compression_ratio": 1.653225806451613, "no_speech_prob": 3.590901178540662e-05}, {"id": 495, "seek": 184408, "start": 1848.6799999999998, "end": 1855.12, "text": " So you have one embedding of a position for every index for all 512 indices.", "tokens": [407, 291, 362, 472, 12240, 3584, 295, 257, 2535, 337, 633, 8186, 337, 439, 1025, 4762, 43840, 13], "temperature": 0.0, "avg_logprob": -0.18789128644750752, "compression_ratio": 1.653225806451613, "no_speech_prob": 3.590901178540662e-05}, {"id": 496, "seek": 184408, "start": 1855.12, "end": 1857.6399999999999, "text": " And then you have all your word embeddings.", "tokens": [400, 550, 291, 362, 439, 428, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.18789128644750752, "compression_ratio": 1.653225806451613, "no_speech_prob": 3.590901178540662e-05}, {"id": 497, "seek": 184408, "start": 1857.6399999999999, "end": 1863.8, "text": " And then the transformer encoder, right, was this combination of sort of sub-modules that", "tokens": [400, 550, 264, 31782, 2058, 19866, 11, 558, 11, 390, 341, 6562, 295, 1333, 295, 1422, 12, 8014, 3473, 300], "temperature": 0.0, "avg_logprob": -0.18789128644750752, "compression_ratio": 1.653225806451613, "no_speech_prob": 3.590901178540662e-05}, {"id": 498, "seek": 184408, "start": 1863.8, "end": 1868.3999999999999, "text": " we walked through line by line on Tuesday, right.", "tokens": [321, 7628, 807, 1622, 538, 1622, 322, 10017, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.18789128644750752, "compression_ratio": 1.653225806451613, "no_speech_prob": 3.590901178540662e-05}, {"id": 499, "seek": 184408, "start": 1868.3999999999999, "end": 1872.3999999999999, "text": " Multi-headed attention was sort of the core building block.", "tokens": [29238, 12, 28409, 3202, 390, 1333, 295, 264, 4965, 2390, 3461, 13], "temperature": 0.0, "avg_logprob": -0.18789128644750752, "compression_ratio": 1.653225806451613, "no_speech_prob": 3.590901178540662e-05}, {"id": 500, "seek": 187240, "start": 1872.4, "end": 1876.64, "text": " And then we had residual and layer norm, right, to help with passing gradients and to help", "tokens": [400, 550, 321, 632, 27980, 293, 4583, 2026, 11, 558, 11, 281, 854, 365, 8437, 2771, 2448, 293, 281, 854], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 501, "seek": 187240, "start": 1876.64, "end": 1879.0, "text": " make training go better and faster.", "tokens": [652, 3097, 352, 1101, 293, 4663, 13], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 502, "seek": 187240, "start": 1879.0, "end": 1885.4, "text": " We had that feed forward layer to, yeah, process sort of the result of the multi-headed", "tokens": [492, 632, 300, 3154, 2128, 4583, 281, 11, 1338, 11, 1399, 1333, 295, 264, 1874, 295, 264, 4825, 12, 28409], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 503, "seek": 187240, "start": 1885.4, "end": 1890.2, "text": " attention, another residual and layer norm, and then pass to an identical transformer", "tokens": [3202, 11, 1071, 27980, 293, 4583, 2026, 11, 293, 550, 1320, 281, 364, 14800, 31782], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 504, "seek": 187240, "start": 1890.2, "end": 1891.2, "text": " encoder block here.", "tokens": [2058, 19866, 3461, 510, 13], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 505, "seek": 187240, "start": 1891.2, "end": 1892.48, "text": " And these would be stacked.", "tokens": [400, 613, 576, 312, 28867, 13], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 506, "seek": 187240, "start": 1892.48, "end": 1898.24, "text": " We'll see a number of different configurations here, but I think, you know, 612 of these", "tokens": [492, 603, 536, 257, 1230, 295, 819, 31493, 510, 11, 457, 286, 519, 11, 291, 458, 11, 1386, 4762, 295, 613], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 507, "seek": 187240, "start": 1898.24, "end": 1899.88, "text": " sort of stacked together.", "tokens": [1333, 295, 28867, 1214, 13], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 508, "seek": 187240, "start": 1899.88, "end": 1900.88, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 509, "seek": 187240, "start": 1900.88, "end": 1902.2, "text": " So that's a transformer encoder.", "tokens": [407, 300, 311, 257, 31782, 2058, 19866, 13], "temperature": 0.0, "avg_logprob": -0.18860964010690004, "compression_ratio": 1.7491289198606272, "no_speech_prob": 1.4969385119911749e-05}, {"id": 510, "seek": 190220, "start": 1902.2, "end": 1907.88, "text": " And we're actually going to see whole models today that are just transformer encoders.", "tokens": [400, 321, 434, 767, 516, 281, 536, 1379, 5245, 965, 300, 366, 445, 31782, 2058, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.15574798886738125, "compression_ratio": 2.051792828685259, "no_speech_prob": 1.3210974429966882e-05}, {"id": 511, "seek": 190220, "start": 1907.88, "end": 1908.88, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.15574798886738125, "compression_ratio": 2.051792828685259, "no_speech_prob": 1.3210974429966882e-05}, {"id": 512, "seek": 190220, "start": 1908.88, "end": 1912.76, "text": " So when we talked about machine translation, when we talked about the transformer itself,", "tokens": [407, 562, 321, 2825, 466, 3479, 12853, 11, 562, 321, 2825, 466, 264, 31782, 2564, 11], "temperature": 0.0, "avg_logprob": -0.15574798886738125, "compression_ratio": 2.051792828685259, "no_speech_prob": 1.3210974429966882e-05}, {"id": 513, "seek": 190220, "start": 1912.76, "end": 1916.0, "text": " the transformer encoder decoder, we talked about this whole thing.", "tokens": [264, 31782, 2058, 19866, 979, 19866, 11, 321, 2825, 466, 341, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.15574798886738125, "compression_ratio": 2.051792828685259, "no_speech_prob": 1.3210974429966882e-05}, {"id": 514, "seek": 190220, "start": 1916.0, "end": 1919.32, "text": " But you could actually just have this left column, and you could actually just have this", "tokens": [583, 291, 727, 767, 445, 362, 341, 1411, 7738, 11, 293, 291, 727, 767, 445, 362, 341], "temperature": 0.0, "avg_logprob": -0.15574798886738125, "compression_ratio": 2.051792828685259, "no_speech_prob": 1.3210974429966882e-05}, {"id": 515, "seek": 190220, "start": 1919.32, "end": 1922.2, "text": " right column as well.", "tokens": [558, 7738, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15574798886738125, "compression_ratio": 2.051792828685259, "no_speech_prob": 1.3210974429966882e-05}, {"id": 516, "seek": 190220, "start": 1922.2, "end": 1924.68, "text": " Although the right column changes a little bit if you just have it.", "tokens": [5780, 264, 558, 7738, 2962, 257, 707, 857, 498, 291, 445, 362, 309, 13], "temperature": 0.0, "avg_logprob": -0.15574798886738125, "compression_ratio": 2.051792828685259, "no_speech_prob": 1.3210974429966882e-05}, {"id": 517, "seek": 190220, "start": 1924.68, "end": 1930.28, "text": " So remember, the right column, we had this masked multi-head self-attention, right, so", "tokens": [407, 1604, 11, 264, 558, 7738, 11, 321, 632, 341, 45249, 4825, 12, 1934, 2698, 12, 1591, 1251, 11, 558, 11, 370], "temperature": 0.0, "avg_logprob": -0.15574798886738125, "compression_ratio": 2.051792828685259, "no_speech_prob": 1.3210974429966882e-05}, {"id": 518, "seek": 193028, "start": 1930.28, "end": 1934.08, "text": " where you can't look at the future.", "tokens": [689, 291, 393, 380, 574, 412, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.14892289088322566, "compression_ratio": 1.6748251748251748, "no_speech_prob": 1.2217587936902419e-05}, {"id": 519, "seek": 193028, "start": 1934.08, "end": 1938.6, "text": " And someone asked actually about how we decode from transformers, given that you have this", "tokens": [400, 1580, 2351, 767, 466, 577, 321, 979, 1429, 490, 4088, 433, 11, 2212, 300, 291, 362, 341], "temperature": 0.0, "avg_logprob": -0.14892289088322566, "compression_ratio": 1.6748251748251748, "no_speech_prob": 1.2217587936902419e-05}, {"id": 520, "seek": 193028, "start": 1938.6, "end": 1940.0, "text": " sort of big chunking operation.", "tokens": [1333, 295, 955, 16635, 278, 6916, 13], "temperature": 0.0, "avg_logprob": -0.14892289088322566, "compression_ratio": 1.6748251748251748, "no_speech_prob": 1.2217587936902419e-05}, {"id": 521, "seek": 193028, "start": 1940.0, "end": 1941.0, "text": " It's a great question.", "tokens": [467, 311, 257, 869, 1168, 13], "temperature": 0.0, "avg_logprob": -0.14892289088322566, "compression_ratio": 1.6748251748251748, "no_speech_prob": 1.2217587936902419e-05}, {"id": 522, "seek": 193028, "start": 1941.0, "end": 1945.96, "text": " I won't be able to get into it in detail today, but you have to run it once during the decoding", "tokens": [286, 1582, 380, 312, 1075, 281, 483, 666, 309, 294, 2607, 965, 11, 457, 291, 362, 281, 1190, 309, 1564, 1830, 264, 979, 8616], "temperature": 0.0, "avg_logprob": -0.14892289088322566, "compression_ratio": 1.6748251748251748, "no_speech_prob": 1.2217587936902419e-05}, {"id": 523, "seek": 193028, "start": 1945.96, "end": 1951.84, "text": " process for every time that you decode to sort of predict the next word.", "tokens": [1399, 337, 633, 565, 300, 291, 979, 1429, 281, 1333, 295, 6069, 264, 958, 1349, 13], "temperature": 0.0, "avg_logprob": -0.14892289088322566, "compression_ratio": 1.6748251748251748, "no_speech_prob": 1.2217587936902419e-05}, {"id": 524, "seek": 193028, "start": 1951.84, "end": 1954.2, "text": " I'll write out something on Ed for this.", "tokens": [286, 603, 2464, 484, 746, 322, 3977, 337, 341, 13], "temperature": 0.0, "avg_logprob": -0.14892289088322566, "compression_ratio": 1.6748251748251748, "no_speech_prob": 1.2217587936902419e-05}, {"id": 525, "seek": 193028, "start": 1954.2, "end": 1958.3999999999999, "text": " So in the masked multi-head self-attention, you're not allowed to look at the future so", "tokens": [407, 294, 264, 45249, 4825, 12, 1934, 2698, 12, 1591, 1251, 11, 291, 434, 406, 4350, 281, 574, 412, 264, 2027, 370], "temperature": 0.0, "avg_logprob": -0.14892289088322566, "compression_ratio": 1.6748251748251748, "no_speech_prob": 1.2217587936902419e-05}, {"id": 526, "seek": 195840, "start": 1958.4, "end": 1964.2800000000002, "text": " that you sort of have this well-defined objective of trying to do language modeling.", "tokens": [300, 291, 1333, 295, 362, 341, 731, 12, 37716, 10024, 295, 1382, 281, 360, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.15937226768431625, "compression_ratio": 1.9746835443037976, "no_speech_prob": 1.593533124832902e-05}, {"id": 527, "seek": 195840, "start": 1964.2800000000002, "end": 1966.0400000000002, "text": " Then we have residual and layer norm.", "tokens": [1396, 321, 362, 27980, 293, 4583, 2026, 13], "temperature": 0.0, "avg_logprob": -0.15937226768431625, "compression_ratio": 1.9746835443037976, "no_speech_prob": 1.593533124832902e-05}, {"id": 528, "seek": 195840, "start": 1966.0400000000002, "end": 1970.0, "text": " The multi-head cross-attention, remember, goes back to the last layer of the transformer", "tokens": [440, 4825, 12, 1934, 3278, 12, 1591, 1251, 11, 1604, 11, 1709, 646, 281, 264, 1036, 4583, 295, 264, 31782], "temperature": 0.0, "avg_logprob": -0.15937226768431625, "compression_ratio": 1.9746835443037976, "no_speech_prob": 1.593533124832902e-05}, {"id": 529, "seek": 195840, "start": 1970.0, "end": 1973.5600000000002, "text": " encoder, or the last transformer encoder block.", "tokens": [2058, 19866, 11, 420, 264, 1036, 31782, 2058, 19866, 3461, 13], "temperature": 0.0, "avg_logprob": -0.15937226768431625, "compression_ratio": 1.9746835443037976, "no_speech_prob": 1.593533124832902e-05}, {"id": 530, "seek": 195840, "start": 1973.5600000000002, "end": 1977.2800000000002, "text": " And then more residual and layer norm, another feed-forward layer, more residual and layer", "tokens": [400, 550, 544, 27980, 293, 4583, 2026, 11, 1071, 3154, 12, 13305, 4583, 11, 544, 27980, 293, 4583], "temperature": 0.0, "avg_logprob": -0.15937226768431625, "compression_ratio": 1.9746835443037976, "no_speech_prob": 1.593533124832902e-05}, {"id": 531, "seek": 195840, "start": 1977.2800000000002, "end": 1978.2800000000002, "text": " norm.", "tokens": [2026, 13], "temperature": 0.0, "avg_logprob": -0.15937226768431625, "compression_ratio": 1.9746835443037976, "no_speech_prob": 1.593533124832902e-05}, {"id": 532, "seek": 195840, "start": 1978.2800000000002, "end": 1984.16, "text": " Now, if we don't have an encoder here, then we get rid of the cross-attention and residual", "tokens": [823, 11, 498, 321, 500, 380, 362, 364, 2058, 19866, 510, 11, 550, 321, 483, 3973, 295, 264, 3278, 12, 1591, 1251, 293, 27980], "temperature": 0.0, "avg_logprob": -0.15937226768431625, "compression_ratio": 1.9746835443037976, "no_speech_prob": 1.593533124832902e-05}, {"id": 533, "seek": 195840, "start": 1984.16, "end": 1985.5600000000002, "text": " and layer norm here.", "tokens": [293, 4583, 2026, 510, 13], "temperature": 0.0, "avg_logprob": -0.15937226768431625, "compression_ratio": 1.9746835443037976, "no_speech_prob": 1.593533124832902e-05}, {"id": 534, "seek": 198556, "start": 1985.56, "end": 1989.28, "text": " So if we didn't have this stack of encoders, the decoders get simpler because you don't", "tokens": [407, 498, 321, 994, 380, 362, 341, 8630, 295, 2058, 378, 433, 11, 264, 979, 378, 433, 483, 18587, 570, 291, 500, 380], "temperature": 0.0, "avg_logprob": -0.16069538473225325, "compression_ratio": 1.7244582043343653, "no_speech_prob": 1.8922171875601634e-05}, {"id": 535, "seek": 198556, "start": 1989.28, "end": 1990.9199999999998, "text": " have to attend to them.", "tokens": [362, 281, 6888, 281, 552, 13], "temperature": 0.0, "avg_logprob": -0.16069538473225325, "compression_ratio": 1.7244582043343653, "no_speech_prob": 1.8922171875601634e-05}, {"id": 536, "seek": 198556, "start": 1990.9199999999998, "end": 1994.9199999999998, "text": " But then again, you also have these word embeddings at the bottom and position representations", "tokens": [583, 550, 797, 11, 291, 611, 362, 613, 1349, 12240, 29432, 412, 264, 2767, 293, 2535, 33358], "temperature": 0.0, "avg_logprob": -0.16069538473225325, "compression_ratio": 1.7244582043343653, "no_speech_prob": 1.8922171875601634e-05}, {"id": 537, "seek": 198556, "start": 1994.9199999999998, "end": 1997.6399999999999, "text": " for the output sequence.", "tokens": [337, 264, 5598, 8310, 13], "temperature": 0.0, "avg_logprob": -0.16069538473225325, "compression_ratio": 1.7244582043343653, "no_speech_prob": 1.8922171875601634e-05}, {"id": 538, "seek": 198556, "start": 1997.6399999999999, "end": 2000.8799999999999, "text": " Okay, so that's been review.", "tokens": [1033, 11, 370, 300, 311, 668, 3131, 13], "temperature": 0.0, "avg_logprob": -0.16069538473225325, "compression_ratio": 1.7244582043343653, "no_speech_prob": 1.8922171875601634e-05}, {"id": 539, "seek": 198556, "start": 2000.8799999999999, "end": 2002.9199999999998, "text": " Let's talk about pre-training through language modeling.", "tokens": [961, 311, 751, 466, 659, 12, 17227, 1760, 807, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.16069538473225325, "compression_ratio": 1.7244582043343653, "no_speech_prob": 1.8922171875601634e-05}, {"id": 540, "seek": 198556, "start": 2002.9199999999998, "end": 2006.52, "text": " So we've actually talked maybe a little bit about this before, and we've seen language", "tokens": [407, 321, 600, 767, 2825, 1310, 257, 707, 857, 466, 341, 949, 11, 293, 321, 600, 1612, 2856], "temperature": 0.0, "avg_logprob": -0.16069538473225325, "compression_ratio": 1.7244582043343653, "no_speech_prob": 1.8922171875601634e-05}, {"id": 541, "seek": 198556, "start": 2006.52, "end": 2010.96, "text": " modeling in the context of maybe just wanting to do it our priori.", "tokens": [15983, 294, 264, 4319, 295, 1310, 445, 7935, 281, 360, 309, 527, 4059, 72, 13], "temperature": 0.0, "avg_logprob": -0.16069538473225325, "compression_ratio": 1.7244582043343653, "no_speech_prob": 1.8922171875601634e-05}, {"id": 542, "seek": 198556, "start": 2010.96, "end": 2015.28, "text": " So language models were useful, for example, in automatic speech recognition systems.", "tokens": [407, 2856, 5245, 645, 4420, 11, 337, 1365, 11, 294, 12509, 6218, 11150, 3652, 13], "temperature": 0.0, "avg_logprob": -0.16069538473225325, "compression_ratio": 1.7244582043343653, "no_speech_prob": 1.8922171875601634e-05}, {"id": 543, "seek": 201528, "start": 2015.28, "end": 2018.36, "text": " They were useful in statistical machine translation systems.", "tokens": [814, 645, 4420, 294, 22820, 3479, 12853, 3652, 13], "temperature": 0.0, "avg_logprob": -0.1677320246793786, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.805594547477085e-05}, {"id": 544, "seek": 201528, "start": 2018.36, "end": 2022.12, "text": " So let's recall the language modeling task.", "tokens": [407, 718, 311, 9901, 264, 2856, 15983, 5633, 13], "temperature": 0.0, "avg_logprob": -0.1677320246793786, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.805594547477085e-05}, {"id": 545, "seek": 201528, "start": 2022.12, "end": 2027.8, "text": " You can say it's defined as modeling the probability of a word at a given index t, of any word", "tokens": [509, 393, 584, 309, 311, 7642, 382, 15983, 264, 8482, 295, 257, 1349, 412, 257, 2212, 8186, 256, 11, 295, 604, 1349], "temperature": 0.0, "avg_logprob": -0.1677320246793786, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.805594547477085e-05}, {"id": 546, "seek": 201528, "start": 2027.8, "end": 2031.92, "text": " at any given index, given all the words before it.", "tokens": [412, 604, 2212, 8186, 11, 2212, 439, 264, 2283, 949, 309, 13], "temperature": 0.0, "avg_logprob": -0.1677320246793786, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.805594547477085e-05}, {"id": 547, "seek": 201528, "start": 2031.92, "end": 2039.8, "text": " And this probability distribution is a distribution of words given their past contexts.", "tokens": [400, 341, 8482, 7316, 307, 257, 7316, 295, 2283, 2212, 641, 1791, 30628, 13], "temperature": 0.0, "avg_logprob": -0.1677320246793786, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.805594547477085e-05}, {"id": 548, "seek": 201528, "start": 2039.8, "end": 2045.16, "text": " And so this is just saying, for any prefix here, IRO goes to make.", "tokens": [400, 370, 341, 307, 445, 1566, 11, 337, 604, 46969, 510, 11, 286, 7142, 1709, 281, 652, 13], "temperature": 0.0, "avg_logprob": -0.1677320246793786, "compression_ratio": 1.7016806722689075, "no_speech_prob": 1.805594547477085e-05}, {"id": 549, "seek": 204516, "start": 2045.16, "end": 2047.48, "text": " I want a probability of whatever the next word should be.", "tokens": [286, 528, 257, 8482, 295, 2035, 264, 958, 1349, 820, 312, 13], "temperature": 0.0, "avg_logprob": -0.1744085947672526, "compression_ratio": 1.7198443579766538, "no_speech_prob": 4.682504368247464e-05}, {"id": 550, "seek": 204516, "start": 2047.48, "end": 2054.12, "text": " So the observed next word is tasty, but maybe there's goes to make t, goes to make hot", "tokens": [407, 264, 13095, 958, 1349, 307, 11535, 11, 457, 1310, 456, 311, 1709, 281, 652, 256, 11, 1709, 281, 652, 2368], "temperature": 0.0, "avg_logprob": -0.1744085947672526, "compression_ratio": 1.7198443579766538, "no_speech_prob": 4.682504368247464e-05}, {"id": 551, "seek": 204516, "start": 2054.12, "end": 2055.6800000000003, "text": " water, etc.", "tokens": [1281, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.1744085947672526, "compression_ratio": 1.7198443579766538, "no_speech_prob": 4.682504368247464e-05}, {"id": 552, "seek": 204516, "start": 2055.6800000000003, "end": 2059.7200000000003, "text": " You can have a distribution over what the next word should be in this decoder.", "tokens": [509, 393, 362, 257, 7316, 670, 437, 264, 958, 1349, 820, 312, 294, 341, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.1744085947672526, "compression_ratio": 1.7198443579766538, "no_speech_prob": 4.682504368247464e-05}, {"id": 553, "seek": 204516, "start": 2059.7200000000003, "end": 2064.48, "text": " And remember that because of the masked self-attention, make can look back to the word", "tokens": [400, 1604, 300, 570, 295, 264, 45249, 2698, 12, 1591, 1251, 11, 652, 393, 574, 646, 281, 264, 1349], "temperature": 0.0, "avg_logprob": -0.1744085947672526, "compression_ratio": 1.7198443579766538, "no_speech_prob": 4.682504368247464e-05}, {"id": 554, "seek": 204516, "start": 2064.48, "end": 2070.84, "text": " two, or goes, or IRO, but it can't look forward to tasty.", "tokens": [732, 11, 420, 1709, 11, 420, 286, 7142, 11, 457, 309, 393, 380, 574, 2128, 281, 11535, 13], "temperature": 0.0, "avg_logprob": -0.1744085947672526, "compression_ratio": 1.7198443579766538, "no_speech_prob": 4.682504368247464e-05}, {"id": 555, "seek": 204516, "start": 2070.84, "end": 2071.92, "text": " So there's a lot of data for this, right?", "tokens": [407, 456, 311, 257, 688, 295, 1412, 337, 341, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1744085947672526, "compression_ratio": 1.7198443579766538, "no_speech_prob": 4.682504368247464e-05}, {"id": 556, "seek": 204516, "start": 2071.92, "end": 2073.44, "text": " You just have text.", "tokens": [509, 445, 362, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1744085947672526, "compression_ratio": 1.7198443579766538, "no_speech_prob": 4.682504368247464e-05}, {"id": 557, "seek": 207344, "start": 2073.44, "end": 2076.36, "text": " And like voila, you have language modeling data.", "tokens": [400, 411, 45565, 11, 291, 362, 2856, 15983, 1412, 13], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 558, "seek": 207344, "start": 2076.36, "end": 2077.36, "text": " It's free.", "tokens": [467, 311, 1737, 13], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 559, "seek": 207344, "start": 2077.36, "end": 2078.36, "text": " No.", "tokens": [883, 13], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 560, "seek": 207344, "start": 2078.36, "end": 2080.84, "text": " Once you have the text, it's freely available.", "tokens": [3443, 291, 362, 264, 2487, 11, 309, 311, 16433, 2435, 13], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 561, "seek": 207344, "start": 2080.84, "end": 2082.56, "text": " You don't need to label it.", "tokens": [509, 500, 380, 643, 281, 7645, 309, 13], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 562, "seek": 207344, "start": 2082.56, "end": 2084.16, "text": " And in English, you have a lot of it, right?", "tokens": [400, 294, 3669, 11, 291, 362, 257, 688, 295, 309, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 563, "seek": 207344, "start": 2084.16, "end": 2090.28, "text": " This is not true of every language by any means, but in English, you have a lot of pre-training", "tokens": [639, 307, 406, 2074, 295, 633, 2856, 538, 604, 1355, 11, 457, 294, 3669, 11, 291, 362, 257, 688, 295, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 564, "seek": 207344, "start": 2090.28, "end": 2092.08, "text": " data.", "tokens": [1412, 13], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 565, "seek": 207344, "start": 2092.08, "end": 2098.0, "text": " And so the simple thing about pre-training is, well, what we're going to do is we're", "tokens": [400, 370, 264, 2199, 551, 466, 659, 12, 17227, 1760, 307, 11, 731, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 566, "seek": 207344, "start": 2098.0, "end": 2101.76, "text": " going to train a neural network to do language modeling on a large amount of text, and we'll", "tokens": [516, 281, 3847, 257, 18161, 3209, 281, 360, 2856, 15983, 322, 257, 2416, 2372, 295, 2487, 11, 293, 321, 603], "temperature": 0.0, "avg_logprob": -0.1716199092727771, "compression_ratio": 1.8373015873015872, "no_speech_prob": 6.91827735863626e-05}, {"id": 567, "seek": 210176, "start": 2101.76, "end": 2106.7200000000003, "text": " just save the parameters of our train network to disk.", "tokens": [445, 3155, 264, 9834, 295, 527, 3847, 3209, 281, 12355, 13], "temperature": 0.0, "avg_logprob": -0.14068491030962038, "compression_ratio": 1.7265917602996255, "no_speech_prob": 5.560333011089824e-05}, {"id": 568, "seek": 210176, "start": 2106.7200000000003, "end": 2110.5600000000004, "text": " So conceptually, it's not actually different from the things that we've done before.", "tokens": [407, 3410, 671, 11, 309, 311, 406, 767, 819, 490, 264, 721, 300, 321, 600, 1096, 949, 13], "temperature": 0.0, "avg_logprob": -0.14068491030962038, "compression_ratio": 1.7265917602996255, "no_speech_prob": 5.560333011089824e-05}, {"id": 569, "seek": 210176, "start": 2110.5600000000004, "end": 2112.48, "text": " It's just sort of the intent, right?", "tokens": [467, 311, 445, 1333, 295, 264, 8446, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14068491030962038, "compression_ratio": 1.7265917602996255, "no_speech_prob": 5.560333011089824e-05}, {"id": 570, "seek": 210176, "start": 2112.48, "end": 2117.0800000000004, "text": " We're training these parameters to start using them for something else later down the line.", "tokens": [492, 434, 3097, 613, 9834, 281, 722, 1228, 552, 337, 746, 1646, 1780, 760, 264, 1622, 13], "temperature": 0.0, "avg_logprob": -0.14068491030962038, "compression_ratio": 1.7265917602996255, "no_speech_prob": 5.560333011089824e-05}, {"id": 571, "seek": 210176, "start": 2117.0800000000004, "end": 2120.1600000000003, "text": " But the language modeling itself doesn't change.", "tokens": [583, 264, 2856, 15983, 2564, 1177, 380, 1319, 13], "temperature": 0.0, "avg_logprob": -0.14068491030962038, "compression_ratio": 1.7265917602996255, "no_speech_prob": 5.560333011089824e-05}, {"id": 572, "seek": 210176, "start": 2120.1600000000003, "end": 2122.32, "text": " The decoder here doesn't change, right?", "tokens": [440, 979, 19866, 510, 1177, 380, 1319, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14068491030962038, "compression_ratio": 1.7265917602996255, "no_speech_prob": 5.560333011089824e-05}, {"id": 573, "seek": 210176, "start": 2122.32, "end": 2127.84, "text": " It's a transformer in tree-trained models in a modern, because this is sort of a newly", "tokens": [467, 311, 257, 31782, 294, 4230, 12, 17227, 2001, 5245, 294, 257, 4363, 11, 570, 341, 307, 1333, 295, 257, 15109], "temperature": 0.0, "avg_logprob": -0.14068491030962038, "compression_ratio": 1.7265917602996255, "no_speech_prob": 5.560333011089824e-05}, {"id": 574, "seek": 210176, "start": 2127.84, "end": 2130.4, "text": " popular concept.", "tokens": [3743, 3410, 13], "temperature": 0.0, "avg_logprob": -0.14068491030962038, "compression_ratio": 1.7265917602996255, "no_speech_prob": 5.560333011089824e-05}, {"id": 575, "seek": 213040, "start": 2130.4, "end": 2136.04, "text": " Although back in 2015 was sort of when this, I think, was first effectively tried out and", "tokens": [5780, 646, 294, 7546, 390, 1333, 295, 562, 341, 11, 286, 519, 11, 390, 700, 8659, 3031, 484, 293], "temperature": 0.0, "avg_logprob": -0.2244718136994735, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.6479020713595673e-05}, {"id": 576, "seek": 213040, "start": 2136.04, "end": 2139.84, "text": " got some interesting results.", "tokens": [658, 512, 1880, 3542, 13], "temperature": 0.0, "avg_logprob": -0.2244718136994735, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.6479020713595673e-05}, {"id": 577, "seek": 213040, "start": 2139.84, "end": 2142.32, "text": " But this could be anything here.", "tokens": [583, 341, 727, 312, 1340, 510, 13], "temperature": 0.0, "avg_logprob": -0.2244718136994735, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.6479020713595673e-05}, {"id": 578, "seek": 213040, "start": 2142.32, "end": 2147.56, "text": " Today, it's most going to be transformers in the models that we actually observe.", "tokens": [2692, 11, 309, 311, 881, 516, 281, 312, 4088, 433, 294, 264, 5245, 300, 321, 767, 11441, 13], "temperature": 0.0, "avg_logprob": -0.2244718136994735, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.6479020713595673e-05}, {"id": 579, "seek": 213040, "start": 2147.56, "end": 2149.32, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2244718136994735, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.6479020713595673e-05}, {"id": 580, "seek": 213040, "start": 2149.32, "end": 2152.6800000000003, "text": " So once you have your pre-trained network, what's the sort of default thing you do to", "tokens": [407, 1564, 291, 362, 428, 659, 12, 17227, 2001, 3209, 11, 437, 311, 264, 1333, 295, 7576, 551, 291, 360, 281], "temperature": 0.0, "avg_logprob": -0.2244718136994735, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.6479020713595673e-05}, {"id": 581, "seek": 213040, "start": 2152.6800000000003, "end": 2154.12, "text": " take to use it?", "tokens": [747, 281, 764, 309, 30], "temperature": 0.0, "avg_logprob": -0.2244718136994735, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.6479020713595673e-05}, {"id": 582, "seek": 213040, "start": 2154.12, "end": 2155.12, "text": " Right?", "tokens": [1779, 30], "temperature": 0.0, "avg_logprob": -0.2244718136994735, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.6479020713595673e-05}, {"id": 583, "seek": 213040, "start": 2155.12, "end": 2158.88, "text": " And if you take anything away from this lecture in terms of just like engineering practices", "tokens": [400, 498, 291, 747, 1340, 1314, 490, 341, 7991, 294, 2115, 295, 445, 411, 7043, 7525], "temperature": 0.0, "avg_logprob": -0.2244718136994735, "compression_ratio": 1.5638297872340425, "no_speech_prob": 5.6479020713595673e-05}, {"id": 584, "seek": 215888, "start": 2158.88, "end": 2165.6800000000003, "text": " that will be broadly useful to you as you go off and build things and study things, maybe", "tokens": [300, 486, 312, 19511, 4420, 281, 291, 382, 291, 352, 766, 293, 1322, 721, 293, 2979, 721, 11, 1310], "temperature": 0.0, "avg_logprob": -0.1487088290127841, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.00010550223669270054}, {"id": 585, "seek": 215888, "start": 2165.6800000000003, "end": 2171.96, "text": " as a machine learning engineer or a computational social scientist, et cetera, what people tend", "tokens": [382, 257, 3479, 2539, 11403, 420, 257, 28270, 2093, 12662, 11, 1030, 11458, 11, 437, 561, 3928], "temperature": 0.0, "avg_logprob": -0.1487088290127841, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.00010550223669270054}, {"id": 586, "seek": 215888, "start": 2171.96, "end": 2176.84, "text": " to do is you pre-traine your network on just a lot of data, lots of text, learn very", "tokens": [281, 360, 307, 291, 659, 12, 17227, 533, 428, 3209, 322, 445, 257, 688, 295, 1412, 11, 3195, 295, 2487, 11, 1466, 588], "temperature": 0.0, "avg_logprob": -0.1487088290127841, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.00010550223669270054}, {"id": 587, "seek": 215888, "start": 2176.84, "end": 2178.36, "text": " general things.", "tokens": [2674, 721, 13], "temperature": 0.0, "avg_logprob": -0.1487088290127841, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.00010550223669270054}, {"id": 588, "seek": 215888, "start": 2178.36, "end": 2182.84, "text": " And then you adapt the network to whatever you wanted to do.", "tokens": [400, 550, 291, 6231, 264, 3209, 281, 2035, 291, 1415, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1487088290127841, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.00010550223669270054}, {"id": 589, "seek": 215888, "start": 2182.84, "end": 2186.76, "text": " So we had a bunch of pre-training data, and then maybe this is a movie review that", "tokens": [407, 321, 632, 257, 3840, 295, 659, 12, 17227, 1760, 1412, 11, 293, 550, 1310, 341, 307, 257, 3169, 3131, 300], "temperature": 0.0, "avg_logprob": -0.1487088290127841, "compression_ratio": 1.7269076305220883, "no_speech_prob": 0.00010550223669270054}, {"id": 590, "seek": 218676, "start": 2186.76, "end": 2194.0400000000004, "text": " we're taking as input here, and we just apply the decoder that we sort of pre-trained,", "tokens": [321, 434, 1940, 382, 4846, 510, 11, 293, 321, 445, 3079, 264, 979, 19866, 300, 321, 1333, 295, 659, 12, 17227, 2001, 11], "temperature": 0.0, "avg_logprob": -0.1331728747767261, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.0143905203440227e-05}, {"id": 591, "seek": 218676, "start": 2194.0400000000004, "end": 2201.1600000000003, "text": " start the parameters there, and then fine tune it on whatever we were sort of wanting", "tokens": [722, 264, 9834, 456, 11, 293, 550, 2489, 10864, 309, 322, 2035, 321, 645, 1333, 295, 7935], "temperature": 0.0, "avg_logprob": -0.1331728747767261, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.0143905203440227e-05}, {"id": 592, "seek": 218676, "start": 2201.1600000000003, "end": 2202.1600000000003, "text": " to do.", "tokens": [281, 360, 13], "temperature": 0.0, "avg_logprob": -0.1331728747767261, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.0143905203440227e-05}, {"id": 593, "seek": 218676, "start": 2202.1600000000003, "end": 2203.92, "text": " Maybe this is a sentiment analysis task.", "tokens": [2704, 341, 307, 257, 16149, 5215, 5633, 13], "temperature": 0.0, "avg_logprob": -0.1331728747767261, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.0143905203440227e-05}, {"id": 594, "seek": 218676, "start": 2203.92, "end": 2208.92, "text": " So we run the whole sequence through the decoder, get a hidden state at the end at the", "tokens": [407, 321, 1190, 264, 1379, 8310, 807, 264, 979, 19866, 11, 483, 257, 7633, 1785, 412, 264, 917, 412, 264], "temperature": 0.0, "avg_logprob": -0.1331728747767261, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.0143905203440227e-05}, {"id": 595, "seek": 218676, "start": 2208.92, "end": 2213.6000000000004, "text": " very last thing, and then we predict maybe plus or minus sentiment.", "tokens": [588, 1036, 551, 11, 293, 550, 321, 6069, 1310, 1804, 420, 3175, 16149, 13], "temperature": 0.0, "avg_logprob": -0.1331728747767261, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.0143905203440227e-05}, {"id": 596, "seek": 218676, "start": 2213.6000000000004, "end": 2216.0400000000004, "text": " And this is sort of adapting the pre-trained network to the task.", "tokens": [400, 341, 307, 1333, 295, 34942, 264, 659, 12, 17227, 2001, 3209, 281, 264, 5633, 13], "temperature": 0.0, "avg_logprob": -0.1331728747767261, "compression_ratio": 1.7569721115537849, "no_speech_prob": 2.0143905203440227e-05}, {"id": 597, "seek": 221604, "start": 2216.04, "end": 2222.48, "text": " Because pre-trained fine-tune paradigm is wildly successful, and you should really try", "tokens": [1436, 659, 12, 17227, 2001, 2489, 12, 83, 2613, 24709, 307, 34731, 4406, 11, 293, 291, 820, 534, 853], "temperature": 0.0, "avg_logprob": -0.20879611416139465, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.907719630864449e-05}, {"id": 598, "seek": 221604, "start": 2222.48, "end": 2229.2, "text": " it whenever you're doing any NLP task nowadays effectively.", "tokens": [309, 5699, 291, 434, 884, 604, 426, 45196, 5633, 13434, 8659, 13], "temperature": 0.0, "avg_logprob": -0.20879611416139465, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.907719630864449e-05}, {"id": 599, "seek": 221604, "start": 2229.2, "end": 2234.08, "text": " Because this tends to be what some variant of this tends to be what works best.", "tokens": [1436, 341, 12258, 281, 312, 437, 512, 17501, 295, 341, 12258, 281, 312, 437, 1985, 1151, 13], "temperature": 0.0, "avg_logprob": -0.20879611416139465, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.907719630864449e-05}, {"id": 600, "seek": 221604, "start": 2234.08, "end": 2239.96, "text": " Okay, so we've got a technical note now.", "tokens": [1033, 11, 370, 321, 600, 658, 257, 6191, 3637, 586, 13], "temperature": 0.0, "avg_logprob": -0.20879611416139465, "compression_ratio": 1.467032967032967, "no_speech_prob": 4.907719630864449e-05}, {"id": 601, "seek": 223996, "start": 2239.96, "end": 2247.16, "text": " So if you don't like to think about optimization or gradient descent, maybe take a pass on", "tokens": [407, 498, 291, 500, 380, 411, 281, 519, 466, 19618, 420, 16235, 23475, 11, 1310, 747, 257, 1320, 322], "temperature": 0.0, "avg_logprob": -0.15895301539723466, "compression_ratio": 1.5666666666666667, "no_speech_prob": 9.457523992750794e-05}, {"id": 602, "seek": 223996, "start": 2247.16, "end": 2254.68, "text": " this slide, but I encourage you to just think for a second about why should this help?", "tokens": [341, 4137, 11, 457, 286, 5373, 291, 281, 445, 519, 337, 257, 1150, 466, 983, 820, 341, 854, 30], "temperature": 0.0, "avg_logprob": -0.15895301539723466, "compression_ratio": 1.5666666666666667, "no_speech_prob": 9.457523992750794e-05}, {"id": 603, "seek": 223996, "start": 2254.68, "end": 2260.2400000000002, "text": " Training neural nets, we're using gradient descent to try to find some global minimum", "tokens": [20620, 18161, 36170, 11, 321, 434, 1228, 16235, 23475, 281, 853, 281, 915, 512, 4338, 7285], "temperature": 0.0, "avg_logprob": -0.15895301539723466, "compression_ratio": 1.5666666666666667, "no_speech_prob": 9.457523992750794e-05}, {"id": 604, "seek": 223996, "start": 2260.2400000000002, "end": 2263.88, "text": " of this loss function.", "tokens": [295, 341, 4470, 2445, 13], "temperature": 0.0, "avg_logprob": -0.15895301539723466, "compression_ratio": 1.5666666666666667, "no_speech_prob": 9.457523992750794e-05}, {"id": 605, "seek": 223996, "start": 2263.88, "end": 2267.2, "text": " And we're sort of doing this in two steps.", "tokens": [400, 321, 434, 1333, 295, 884, 341, 294, 732, 4439, 13], "temperature": 0.0, "avg_logprob": -0.15895301539723466, "compression_ratio": 1.5666666666666667, "no_speech_prob": 9.457523992750794e-05}, {"id": 606, "seek": 226720, "start": 2267.2, "end": 2275.64, "text": " The first step is we get some parameters theta hat by approximating min over our, sorry,", "tokens": [440, 700, 1823, 307, 321, 483, 512, 9834, 9725, 2385, 538, 8542, 990, 923, 670, 527, 11, 2597, 11], "temperature": 0.0, "avg_logprob": -0.17282827626103942, "compression_ratio": 2.0216450216450217, "no_speech_prob": 2.467759077262599e-05}, {"id": 607, "seek": 226720, "start": 2275.64, "end": 2277.8799999999997, "text": " theta is the parameters of the neural network.", "tokens": [9725, 307, 264, 9834, 295, 264, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.17282827626103942, "compression_ratio": 2.0216450216450217, "no_speech_prob": 2.467759077262599e-05}, {"id": 608, "seek": 226720, "start": 2277.8799999999997, "end": 2283.2, "text": " So all of the KQV vectors in our transformer, the word embeddings, the position embeddings,", "tokens": [407, 439, 295, 264, 591, 48, 53, 18875, 294, 527, 31782, 11, 264, 1349, 12240, 29432, 11, 264, 2535, 12240, 29432, 11], "temperature": 0.0, "avg_logprob": -0.17282827626103942, "compression_ratio": 2.0216450216450217, "no_speech_prob": 2.467759077262599e-05}, {"id": 609, "seek": 226720, "start": 2283.2, "end": 2287.24, "text": " it's just all of the parameters of our neural network.", "tokens": [309, 311, 445, 439, 295, 264, 9834, 295, 527, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.17282827626103942, "compression_ratio": 2.0216450216450217, "no_speech_prob": 2.467759077262599e-05}, {"id": 610, "seek": 226720, "start": 2287.24, "end": 2291.2799999999997, "text": " And so we're doing min over all the parameters of our theta, we're trying to approximate min", "tokens": [400, 370, 321, 434, 884, 923, 670, 439, 264, 9834, 295, 527, 9725, 11, 321, 434, 1382, 281, 30874, 923], "temperature": 0.0, "avg_logprob": -0.17282827626103942, "compression_ratio": 2.0216450216450217, "no_speech_prob": 2.467759077262599e-05}, {"id": 611, "seek": 226720, "start": 2291.2799999999997, "end": 2294.72, "text": " over the parameters of our neural network of our pre-training loss, which here was language", "tokens": [670, 264, 9834, 295, 527, 18161, 3209, 295, 527, 659, 12, 17227, 1760, 4470, 11, 597, 510, 390, 2856], "temperature": 0.0, "avg_logprob": -0.17282827626103942, "compression_ratio": 2.0216450216450217, "no_speech_prob": 2.467759077262599e-05}, {"id": 612, "seek": 229472, "start": 2294.72, "end": 2298.12, "text": " modeling of our parameters.", "tokens": [15983, 295, 527, 9834, 13], "temperature": 0.0, "avg_logprob": -0.19546233690701997, "compression_ratio": 1.778301886792453, "no_speech_prob": 1.3630696230393369e-05}, {"id": 613, "seek": 229472, "start": 2298.12, "end": 2303.9599999999996, "text": " And this is, we just get this sort of estimate of some parameters theta hat.", "tokens": [400, 341, 307, 11, 321, 445, 483, 341, 1333, 295, 12539, 295, 512, 9834, 9725, 2385, 13], "temperature": 0.0, "avg_logprob": -0.19546233690701997, "compression_ratio": 1.778301886792453, "no_speech_prob": 1.3630696230393369e-05}, {"id": 614, "seek": 229472, "start": 2303.9599999999996, "end": 2311.2, "text": " And then we fine tune by approximating this min over theta of the fine tune loss, maybe", "tokens": [400, 550, 321, 2489, 10864, 538, 8542, 990, 341, 923, 670, 9725, 295, 264, 2489, 10864, 4470, 11, 1310], "temperature": 0.0, "avg_logprob": -0.19546233690701997, "compression_ratio": 1.778301886792453, "no_speech_prob": 1.3630696230393369e-05}, {"id": 615, "seek": 229472, "start": 2311.2, "end": 2313.2, "text": " that's sentiment, right?", "tokens": [300, 311, 16149, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.19546233690701997, "compression_ratio": 1.778301886792453, "no_speech_prob": 1.3630696230393369e-05}, {"id": 616, "seek": 229472, "start": 2313.2, "end": 2314.2, "text": " Starting at theta hat.", "tokens": [16217, 412, 9725, 2385, 13], "temperature": 0.0, "avg_logprob": -0.19546233690701997, "compression_ratio": 1.778301886792453, "no_speech_prob": 1.3630696230393369e-05}, {"id": 617, "seek": 229472, "start": 2314.2, "end": 2317.48, "text": " So we initialize our gradient descent at theta hat, and then we just sort of let it do", "tokens": [407, 321, 5883, 1125, 527, 16235, 23475, 412, 9725, 2385, 11, 293, 550, 321, 445, 1333, 295, 718, 309, 360], "temperature": 0.0, "avg_logprob": -0.19546233690701997, "compression_ratio": 1.778301886792453, "no_speech_prob": 1.3630696230393369e-05}, {"id": 618, "seek": 229472, "start": 2317.48, "end": 2318.8399999999997, "text": " what it wants.", "tokens": [437, 309, 2738, 13], "temperature": 0.0, "avg_logprob": -0.19546233690701997, "compression_ratio": 1.778301886792453, "no_speech_prob": 1.3630696230393369e-05}, {"id": 619, "seek": 229472, "start": 2318.8399999999997, "end": 2322.12, "text": " And it's just like, it just works.", "tokens": [400, 309, 311, 445, 411, 11, 309, 445, 1985, 13], "temperature": 0.0, "avg_logprob": -0.19546233690701997, "compression_ratio": 1.778301886792453, "no_speech_prob": 1.3630696230393369e-05}, {"id": 620, "seek": 232212, "start": 2322.12, "end": 2329.56, "text": " And in part, it has to be because something about where we start is so important, not just", "tokens": [400, 294, 644, 11, 309, 575, 281, 312, 570, 746, 466, 689, 321, 722, 307, 370, 1021, 11, 406, 445], "temperature": 0.0, "avg_logprob": -0.13282606773769734, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.119747634627856e-05}, {"id": 621, "seek": 232212, "start": 2329.56, "end": 2333.3599999999997, "text": " in terms of sort of gradient flow, although that is a big part of it.", "tokens": [294, 2115, 295, 1333, 295, 16235, 3095, 11, 4878, 300, 307, 257, 955, 644, 295, 309, 13], "temperature": 0.0, "avg_logprob": -0.13282606773769734, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.119747634627856e-05}, {"id": 622, "seek": 232212, "start": 2333.3599999999997, "end": 2340.72, "text": " But also, it seems like, you know, stochastic gradient descent sticks relatively close to", "tokens": [583, 611, 11, 309, 2544, 411, 11, 291, 458, 11, 342, 8997, 2750, 16235, 23475, 12518, 7226, 1998, 281], "temperature": 0.0, "avg_logprob": -0.13282606773769734, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.119747634627856e-05}, {"id": 623, "seek": 232212, "start": 2340.72, "end": 2343.96, "text": " that pre-training initialization during fine tuning.", "tokens": [300, 659, 12, 17227, 1760, 5883, 2144, 1830, 2489, 15164, 13], "temperature": 0.0, "avg_logprob": -0.13282606773769734, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.119747634627856e-05}, {"id": 624, "seek": 232212, "start": 2343.96, "end": 2349.8399999999997, "text": " This is something that we seem to observe in practice, right, that somehow the locality", "tokens": [639, 307, 746, 300, 321, 1643, 281, 11441, 294, 3124, 11, 558, 11, 300, 6063, 264, 1628, 1860], "temperature": 0.0, "avg_logprob": -0.13282606773769734, "compression_ratio": 1.615702479338843, "no_speech_prob": 3.119747634627856e-05}, {"id": 625, "seek": 234984, "start": 2349.84, "end": 2353.92, "text": " of stochastic gradient descent, finding local minima that are close to this theta hat,", "tokens": [295, 342, 8997, 2750, 16235, 23475, 11, 5006, 2654, 4464, 64, 300, 366, 1998, 281, 341, 9725, 2385, 11], "temperature": 0.0, "avg_logprob": -0.10664685794285365, "compression_ratio": 1.9888059701492538, "no_speech_prob": 2.0783383661182597e-05}, {"id": 626, "seek": 234984, "start": 2353.92, "end": 2359.04, "text": " that was good for such a general problem of language modeling, it seems like, yeah, the", "tokens": [300, 390, 665, 337, 1270, 257, 2674, 1154, 295, 2856, 15983, 11, 309, 2544, 411, 11, 1338, 11, 264], "temperature": 0.0, "avg_logprob": -0.10664685794285365, "compression_ratio": 1.9888059701492538, "no_speech_prob": 2.0783383661182597e-05}, {"id": 627, "seek": 234984, "start": 2359.04, "end": 2363.36, "text": " local minima of the fine tuning loss, because we don't find, or yeah, the local minima", "tokens": [2654, 4464, 64, 295, 264, 2489, 15164, 4470, 11, 570, 321, 500, 380, 915, 11, 420, 1338, 11, 264, 2654, 4464, 64], "temperature": 0.0, "avg_logprob": -0.10664685794285365, "compression_ratio": 1.9888059701492538, "no_speech_prob": 2.0783383661182597e-05}, {"id": 628, "seek": 234984, "start": 2363.36, "end": 2368.04, "text": " of the fine tuning loss tend to generalize well when they're near to this theta hat that", "tokens": [295, 264, 2489, 15164, 4470, 3928, 281, 2674, 1125, 731, 562, 436, 434, 2651, 281, 341, 9725, 2385, 300], "temperature": 0.0, "avg_logprob": -0.10664685794285365, "compression_ratio": 1.9888059701492538, "no_speech_prob": 2.0783383661182597e-05}, {"id": 629, "seek": 234984, "start": 2368.04, "end": 2369.04, "text": " we pre-trained.", "tokens": [321, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.10664685794285365, "compression_ratio": 1.9888059701492538, "no_speech_prob": 2.0783383661182597e-05}, {"id": 630, "seek": 234984, "start": 2369.04, "end": 2372.6000000000004, "text": " And this is sort of a mystery that we're still trying to figure out more about.", "tokens": [400, 341, 307, 1333, 295, 257, 11422, 300, 321, 434, 920, 1382, 281, 2573, 484, 544, 466, 13], "temperature": 0.0, "avg_logprob": -0.10664685794285365, "compression_ratio": 1.9888059701492538, "no_speech_prob": 2.0783383661182597e-05}, {"id": 631, "seek": 234984, "start": 2372.6000000000004, "end": 2375.8, "text": " And then also, yeah, maybe the gradients, right, the gradients of the fine tuning loss", "tokens": [400, 550, 611, 11, 1338, 11, 1310, 264, 2771, 2448, 11, 558, 11, 264, 2771, 2448, 295, 264, 2489, 15164, 4470], "temperature": 0.0, "avg_logprob": -0.10664685794285365, "compression_ratio": 1.9888059701492538, "no_speech_prob": 2.0783383661182597e-05}, {"id": 632, "seek": 237580, "start": 2375.8, "end": 2380.4, "text": " near theta propagate nicely, so our network training goes really well as well.", "tokens": [2651, 9725, 48256, 9594, 11, 370, 527, 3209, 3097, 1709, 534, 731, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.19664991818941557, "compression_ratio": 1.6324786324786325, "no_speech_prob": 4.264108429197222e-05}, {"id": 633, "seek": 237580, "start": 2380.4, "end": 2385.8, "text": " Okay, so this is something to chew on, but in practice, it works.", "tokens": [1033, 11, 370, 341, 307, 746, 281, 21200, 322, 11, 457, 294, 3124, 11, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.19664991818941557, "compression_ratio": 1.6324786324786325, "no_speech_prob": 4.264108429197222e-05}, {"id": 634, "seek": 237580, "start": 2385.8, "end": 2389.6400000000003, "text": " I think it's just still fascinating that it works.", "tokens": [286, 519, 309, 311, 445, 920, 10343, 300, 309, 1985, 13], "temperature": 0.0, "avg_logprob": -0.19664991818941557, "compression_ratio": 1.6324786324786325, "no_speech_prob": 4.264108429197222e-05}, {"id": 635, "seek": 237580, "start": 2389.6400000000003, "end": 2399.6800000000003, "text": " Okay, so we talked about mainly the transformer encoder to coder, and in fact, right, I said", "tokens": [1033, 11, 370, 321, 2825, 466, 8704, 264, 31782, 2058, 19866, 281, 17656, 260, 11, 293, 294, 1186, 11, 558, 11, 286, 848], "temperature": 0.0, "avg_logprob": -0.19664991818941557, "compression_ratio": 1.6324786324786325, "no_speech_prob": 4.264108429197222e-05}, {"id": 636, "seek": 237580, "start": 2399.6800000000003, "end": 2404.8, "text": " that we could have just sort of the left-hand side encoders, you know, that to be pre-trained", "tokens": [300, 321, 727, 362, 445, 1333, 295, 264, 1411, 12, 5543, 1252, 2058, 378, 433, 11, 291, 458, 11, 300, 281, 312, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.19664991818941557, "compression_ratio": 1.6324786324786325, "no_speech_prob": 4.264108429197222e-05}, {"id": 637, "seek": 240480, "start": 2404.8, "end": 2408.6400000000003, "text": " or just decoders to be pre-trained or encoder decoders.", "tokens": [420, 445, 979, 378, 433, 281, 312, 659, 12, 17227, 2001, 420, 2058, 19866, 979, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.11644743928814878, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.012578291527461e-06}, {"id": 638, "seek": 240480, "start": 2408.6400000000003, "end": 2413.48, "text": " And there are actually really popular sort of famous models in each of these three categories.", "tokens": [400, 456, 366, 767, 534, 3743, 1333, 295, 4618, 5245, 294, 1184, 295, 613, 1045, 10479, 13], "temperature": 0.0, "avg_logprob": -0.11644743928814878, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.012578291527461e-06}, {"id": 639, "seek": 240480, "start": 2413.48, "end": 2420.6400000000003, "text": " The kinds of pre-training you can do, and the kinds of applications or uses of those", "tokens": [440, 3685, 295, 659, 12, 17227, 1760, 291, 393, 360, 11, 293, 264, 3685, 295, 5821, 420, 4960, 295, 729], "temperature": 0.0, "avg_logprob": -0.11644743928814878, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.012578291527461e-06}, {"id": 640, "seek": 240480, "start": 2420.6400000000003, "end": 2425.48, "text": " pre-trained models that are most natural actually depend strongly on whether you choose", "tokens": [659, 12, 17227, 2001, 5245, 300, 366, 881, 3303, 767, 5672, 10613, 322, 1968, 291, 2826], "temperature": 0.0, "avg_logprob": -0.11644743928814878, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.012578291527461e-06}, {"id": 641, "seek": 240480, "start": 2425.48, "end": 2431.6000000000004, "text": " to pre-traine and encoder a decoder or an encoder decoder.", "tokens": [281, 659, 12, 17227, 533, 293, 2058, 19866, 257, 979, 19866, 420, 364, 2058, 19866, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.11644743928814878, "compression_ratio": 1.8277511961722488, "no_speech_prob": 8.012578291527461e-06}, {"id": 642, "seek": 243160, "start": 2431.6, "end": 2436.92, "text": " So I think it's useful as we go through some of these popular sort of model names that", "tokens": [407, 286, 519, 309, 311, 4420, 382, 321, 352, 807, 512, 295, 613, 3743, 1333, 295, 2316, 5288, 300], "temperature": 0.0, "avg_logprob": -0.17812499324832343, "compression_ratio": 1.6984126984126984, "no_speech_prob": 1.6184707419597544e-05}, {"id": 643, "seek": 243160, "start": 2436.92, "end": 2441.4, "text": " you need to know and what they sort of, what their innovations were to actually split", "tokens": [291, 643, 281, 458, 293, 437, 436, 1333, 295, 11, 437, 641, 24283, 645, 281, 767, 7472], "temperature": 0.0, "avg_logprob": -0.17812499324832343, "compression_ratio": 1.6984126984126984, "no_speech_prob": 1.6184707419597544e-05}, {"id": 644, "seek": 243160, "start": 2441.4, "end": 2444.3199999999997, "text": " it up into these categories.", "tokens": [309, 493, 666, 613, 10479, 13], "temperature": 0.0, "avg_logprob": -0.17812499324832343, "compression_ratio": 1.6984126984126984, "no_speech_prob": 1.6184707419597544e-05}, {"id": 645, "seek": 243160, "start": 2444.3199999999997, "end": 2446.3199999999997, "text": " So we've all, so here's the thing.", "tokens": [407, 321, 600, 439, 11, 370, 510, 311, 264, 551, 13], "temperature": 0.0, "avg_logprob": -0.17812499324832343, "compression_ratio": 1.6984126984126984, "no_speech_prob": 1.6184707419597544e-05}, {"id": 646, "seek": 243160, "start": 2446.3199999999997, "end": 2451.12, "text": " We're going to go through these three, and they all have sort of benefits and in some", "tokens": [492, 434, 516, 281, 352, 807, 613, 1045, 11, 293, 436, 439, 362, 1333, 295, 5311, 293, 294, 512], "temperature": 0.0, "avg_logprob": -0.17812499324832343, "compression_ratio": 1.6984126984126984, "no_speech_prob": 1.6184707419597544e-05}, {"id": 647, "seek": 243160, "start": 2451.12, "end": 2452.68, "text": " sense, drawbacks.", "tokens": [2020, 11, 2642, 17758, 13], "temperature": 0.0, "avg_logprob": -0.17812499324832343, "compression_ratio": 1.6984126984126984, "no_speech_prob": 1.6184707419597544e-05}, {"id": 648, "seek": 243160, "start": 2452.68, "end": 2458.7599999999998, "text": " So the decoders, right, really what we're talking about here mainly is language models,", "tokens": [407, 264, 979, 378, 433, 11, 558, 11, 534, 437, 321, 434, 1417, 466, 510, 8704, 307, 2856, 5245, 11], "temperature": 0.0, "avg_logprob": -0.17812499324832343, "compression_ratio": 1.6984126984126984, "no_speech_prob": 1.6184707419597544e-05}, {"id": 649, "seek": 245876, "start": 2458.76, "end": 2463.1600000000003, "text": " and we've seen this so far, we've talked about pre-trained decoders, and these are nice", "tokens": [293, 321, 600, 1612, 341, 370, 1400, 11, 321, 600, 2825, 466, 659, 12, 17227, 2001, 979, 378, 433, 11, 293, 613, 366, 1481], "temperature": 0.0, "avg_logprob": -0.14311498670435663, "compression_ratio": 1.6875, "no_speech_prob": 1.833639180404134e-05}, {"id": 650, "seek": 245876, "start": 2463.1600000000003, "end": 2464.96, "text": " to generate from.", "tokens": [281, 8460, 490, 13], "temperature": 0.0, "avg_logprob": -0.14311498670435663, "compression_ratio": 1.6875, "no_speech_prob": 1.833639180404134e-05}, {"id": 651, "seek": 245876, "start": 2464.96, "end": 2468.84, "text": " So you can just sample from your pre-trained language model and get things that look", "tokens": [407, 291, 393, 445, 6889, 490, 428, 659, 12, 17227, 2001, 2856, 2316, 293, 483, 721, 300, 574], "temperature": 0.0, "avg_logprob": -0.14311498670435663, "compression_ratio": 1.6875, "no_speech_prob": 1.833639180404134e-05}, {"id": 652, "seek": 245876, "start": 2468.84, "end": 2471.96, "text": " like the text that you were pre-training on.", "tokens": [411, 264, 2487, 300, 291, 645, 659, 12, 17227, 1760, 322, 13], "temperature": 0.0, "avg_logprob": -0.14311498670435663, "compression_ratio": 1.6875, "no_speech_prob": 1.833639180404134e-05}, {"id": 653, "seek": 245876, "start": 2471.96, "end": 2475.28, "text": " But one problem is that you can't condition on future words, right?", "tokens": [583, 472, 1154, 307, 300, 291, 393, 380, 4188, 322, 2027, 2283, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.14311498670435663, "compression_ratio": 1.6875, "no_speech_prob": 1.833639180404134e-05}, {"id": 654, "seek": 245876, "start": 2475.28, "end": 2481.2000000000003, "text": " So we mentioned in our modeling with LSTMs that just like, instead, if you could, when", "tokens": [407, 321, 2835, 294, 527, 15983, 365, 441, 6840, 26386, 300, 445, 411, 11, 2602, 11, 498, 291, 727, 11, 562], "temperature": 0.0, "avg_logprob": -0.14311498670435663, "compression_ratio": 1.6875, "no_speech_prob": 1.833639180404134e-05}, {"id": 655, "seek": 245876, "start": 2481.2000000000003, "end": 2487.48, "text": " you can do it, we said that having a bi-directional LSTM was actually just way more useful than", "tokens": [291, 393, 360, 309, 11, 321, 848, 300, 1419, 257, 3228, 12, 18267, 41048, 441, 6840, 44, 390, 767, 445, 636, 544, 4420, 813], "temperature": 0.0, "avg_logprob": -0.14311498670435663, "compression_ratio": 1.6875, "no_speech_prob": 1.833639180404134e-05}, {"id": 656, "seek": 248748, "start": 2487.48, "end": 2489.12, "text": " having a one-directional LSTM.", "tokens": [1419, 257, 472, 12, 18267, 41048, 441, 6840, 44, 13], "temperature": 0.0, "avg_logprob": -0.18213789613096865, "compression_ratio": 1.7594501718213058, "no_speech_prob": 1.0287887562299147e-05}, {"id": 657, "seek": 248748, "start": 2489.12, "end": 2491.88, "text": " Well, it's sort of true for transformers as well.", "tokens": [1042, 11, 309, 311, 1333, 295, 2074, 337, 4088, 433, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18213789613096865, "compression_ratio": 1.7594501718213058, "no_speech_prob": 1.0287887562299147e-05}, {"id": 658, "seek": 248748, "start": 2491.88, "end": 2497.44, "text": " So if you can see how the arrows are pointing here, the arrows are pointing up into the,", "tokens": [407, 498, 291, 393, 536, 577, 264, 19669, 366, 12166, 510, 11, 264, 19669, 366, 12166, 493, 666, 264, 11], "temperature": 0.0, "avg_logprob": -0.18213789613096865, "compression_ratio": 1.7594501718213058, "no_speech_prob": 1.0287887562299147e-05}, {"id": 659, "seek": 248748, "start": 2497.44, "end": 2498.44, "text": " you know, to the right.", "tokens": [291, 458, 11, 281, 264, 558, 13], "temperature": 0.0, "avg_logprob": -0.18213789613096865, "compression_ratio": 1.7594501718213058, "no_speech_prob": 1.0287887562299147e-05}, {"id": 660, "seek": 248748, "start": 2498.44, "end": 2505.36, "text": " So this word is sort of looking back at its past history, but, you know, this word can't", "tokens": [407, 341, 1349, 307, 1333, 295, 1237, 646, 412, 1080, 1791, 2503, 11, 457, 11, 291, 458, 11, 341, 1349, 393, 380], "temperature": 0.0, "avg_logprob": -0.18213789613096865, "compression_ratio": 1.7594501718213058, "no_speech_prob": 1.0287887562299147e-05}, {"id": 661, "seek": 248748, "start": 2505.36, "end": 2508.56, "text": " see, can't contextualize with the future.", "tokens": [536, 11, 393, 380, 35526, 1125, 365, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.18213789613096865, "compression_ratio": 1.7594501718213058, "no_speech_prob": 1.0287887562299147e-05}, {"id": 662, "seek": 248748, "start": 2508.56, "end": 2512.84, "text": " Whereas in the encoder block here in blue, just below it, you sort of have all pairs of", "tokens": [13813, 294, 264, 2058, 19866, 3461, 510, 294, 3344, 11, 445, 2507, 309, 11, 291, 1333, 295, 362, 439, 15494, 295], "temperature": 0.0, "avg_logprob": -0.18213789613096865, "compression_ratio": 1.7594501718213058, "no_speech_prob": 1.0287887562299147e-05}, {"id": 663, "seek": 248748, "start": 2512.84, "end": 2514.2400000000002, "text": " interactions.", "tokens": [13280, 13], "temperature": 0.0, "avg_logprob": -0.18213789613096865, "compression_ratio": 1.7594501718213058, "no_speech_prob": 1.0287887562299147e-05}, {"id": 664, "seek": 248748, "start": 2514.2400000000002, "end": 2516.88, "text": " And so, you know, when you're building your representations, it can actually be super", "tokens": [400, 370, 11, 291, 458, 11, 562, 291, 434, 2390, 428, 33358, 11, 309, 393, 767, 312, 1687], "temperature": 0.0, "avg_logprob": -0.18213789613096865, "compression_ratio": 1.7594501718213058, "no_speech_prob": 1.0287887562299147e-05}, {"id": 665, "seek": 251688, "start": 2516.88, "end": 2518.92, "text": " useful to know what the future words are.", "tokens": [4420, 281, 458, 437, 264, 2027, 2283, 366, 13], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 666, "seek": 251688, "start": 2518.92, "end": 2520.7200000000003, "text": " So that's what encoders get you, right?", "tokens": [407, 300, 311, 437, 2058, 378, 433, 483, 291, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 667, "seek": 251688, "start": 2520.7200000000003, "end": 2522.88, "text": " You get bi-directional context.", "tokens": [509, 483, 3228, 12, 18267, 41048, 4319, 13], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 668, "seek": 251688, "start": 2522.88, "end": 2525.96, "text": " So you can condition on the future, maybe that helps you build up better representations", "tokens": [407, 291, 393, 4188, 322, 264, 2027, 11, 1310, 300, 3665, 291, 1322, 493, 1101, 33358], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 669, "seek": 251688, "start": 2525.96, "end": 2526.96, "text": " of language.", "tokens": [295, 2856, 13], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 670, "seek": 251688, "start": 2526.96, "end": 2532.08, "text": " But the question that we'll actually go through here is, well, how do you pre-train them?", "tokens": [583, 264, 1168, 300, 321, 603, 767, 352, 807, 510, 307, 11, 731, 11, 577, 360, 291, 659, 12, 17227, 259, 552, 30], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 671, "seek": 251688, "start": 2532.08, "end": 2535.76, "text": " You can't pre-train them as language models because you have access to the future.", "tokens": [509, 393, 380, 659, 12, 17227, 259, 552, 382, 2856, 5245, 570, 291, 362, 2105, 281, 264, 2027, 13], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 672, "seek": 251688, "start": 2535.76, "end": 2540.0, "text": " So if you try to do that, the loss will just immediately be zero because you can just", "tokens": [407, 498, 291, 853, 281, 360, 300, 11, 264, 4470, 486, 445, 4258, 312, 4018, 570, 291, 393, 445], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 673, "seek": 251688, "start": 2540.0, "end": 2541.0, "text": " see what the future is.", "tokens": [536, 437, 264, 2027, 307, 13], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 674, "seek": 251688, "start": 2541.0, "end": 2542.8, "text": " That's not useful.", "tokens": [663, 311, 406, 4420, 13], "temperature": 0.0, "avg_logprob": -0.13094863085679606, "compression_ratio": 1.7889273356401385, "no_speech_prob": 3.534088682499714e-05}, {"id": 675, "seek": 254280, "start": 2542.8, "end": 2548.0800000000004, "text": " And then we'll talk about pre-trained encoder decoders, which like maybe the best of both", "tokens": [400, 550, 321, 603, 751, 466, 659, 12, 17227, 2001, 2058, 19866, 979, 378, 433, 11, 597, 411, 1310, 264, 1151, 295, 1293], "temperature": 0.0, "avg_logprob": -0.24907457828521729, "compression_ratio": 1.5911330049261083, "no_speech_prob": 1.9521894500940107e-05}, {"id": 676, "seek": 254280, "start": 2548.0800000000004, "end": 2553.2400000000002, "text": " worlds, but also maybe unclear what's the best way to pre-train them.", "tokens": [13401, 11, 457, 611, 1310, 25636, 437, 311, 264, 1151, 636, 281, 659, 12, 17227, 259, 552, 13], "temperature": 0.0, "avg_logprob": -0.24907457828521729, "compression_ratio": 1.5911330049261083, "no_speech_prob": 1.9521894500940107e-05}, {"id": 677, "seek": 254280, "start": 2553.2400000000002, "end": 2556.6800000000003, "text": " They definitely have benefits for both.", "tokens": [814, 2138, 362, 5311, 337, 1293, 13], "temperature": 0.0, "avg_logprob": -0.24907457828521729, "compression_ratio": 1.5911330049261083, "no_speech_prob": 1.9521894500940107e-05}, {"id": 678, "seek": 254280, "start": 2556.6800000000003, "end": 2563.52, "text": " So let's get into some general top, like a more, yeah, let's get into the decoders first,", "tokens": [407, 718, 311, 483, 666, 512, 2674, 1192, 11, 411, 257, 544, 11, 1338, 11, 718, 311, 483, 666, 264, 979, 378, 433, 700, 11], "temperature": 0.0, "avg_logprob": -0.24907457828521729, "compression_ratio": 1.5911330049261083, "no_speech_prob": 1.9521894500940107e-05}, {"id": 679, "seek": 254280, "start": 2563.52, "end": 2566.0800000000004, "text": " we'll go through all three.", "tokens": [321, 603, 352, 807, 439, 1045, 13], "temperature": 0.0, "avg_logprob": -0.24907457828521729, "compression_ratio": 1.5911330049261083, "no_speech_prob": 1.9521894500940107e-05}, {"id": 680, "seek": 254280, "start": 2566.0800000000004, "end": 2567.0800000000004, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.24907457828521729, "compression_ratio": 1.5911330049261083, "no_speech_prob": 1.9521894500940107e-05}, {"id": 681, "seek": 256708, "start": 2567.08, "end": 2574.52, "text": " When we're pre-training a language model, right, we're pre-training it on this objective,", "tokens": [1133, 321, 434, 659, 12, 17227, 1760, 257, 2856, 2316, 11, 558, 11, 321, 434, 659, 12, 17227, 1760, 309, 322, 341, 10024, 11], "temperature": 0.0, "avg_logprob": -0.2500421180099737, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.0687384171178564e-05}, {"id": 682, "seek": 256708, "start": 2574.52, "end": 2579.24, "text": " we're trying to make it approximate this probability of a word given all of its previous", "tokens": [321, 434, 1382, 281, 652, 309, 30874, 341, 8482, 295, 257, 1349, 2212, 439, 295, 1080, 3894], "temperature": 0.0, "avg_logprob": -0.2500421180099737, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.0687384171178564e-05}, {"id": 683, "seek": 256708, "start": 2579.24, "end": 2581.4, "text": " words.", "tokens": [2283, 13], "temperature": 0.0, "avg_logprob": -0.2500421180099737, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.0687384171178564e-05}, {"id": 684, "seek": 256708, "start": 2581.4, "end": 2584.84, "text": " What we end up doing, and I showed this sort of pictographically, but I'll add some math,", "tokens": [708, 321, 917, 493, 884, 11, 293, 286, 4712, 341, 1333, 295, 2317, 3108, 984, 11, 457, 286, 603, 909, 512, 5221, 11], "temperature": 0.0, "avg_logprob": -0.2500421180099737, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.0687384171178564e-05}, {"id": 685, "seek": 256708, "start": 2584.84, "end": 2591.6, "text": " right, we get a hidden state, h1 to ht for each of the words in the input w1 to wt.", "tokens": [558, 11, 321, 483, 257, 7633, 1785, 11, 276, 16, 281, 276, 83, 337, 1184, 295, 264, 2283, 294, 264, 4846, 261, 16, 281, 261, 83, 13], "temperature": 0.0, "avg_logprob": -0.2500421180099737, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.0687384171178564e-05}, {"id": 686, "seek": 256708, "start": 2591.6, "end": 2594.64, "text": " And I remember words again, mean sub words here.", "tokens": [400, 286, 1604, 2283, 797, 11, 914, 1422, 2283, 510, 13], "temperature": 0.0, "avg_logprob": -0.2500421180099737, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.0687384171178564e-05}, {"id": 687, "seek": 256708, "start": 2594.64, "end": 2595.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2500421180099737, "compression_ratio": 1.6428571428571428, "no_speech_prob": 4.0687384171178564e-05}, {"id": 688, "seek": 259564, "start": 2595.64, "end": 2600.2799999999997, "text": " And we're fine tuning this, right, we can take the representation, this should be ht,", "tokens": [400, 321, 434, 2489, 15164, 341, 11, 558, 11, 321, 393, 747, 264, 10290, 11, 341, 820, 312, 276, 83, 11], "temperature": 0.0, "avg_logprob": -0.21139223007928756, "compression_ratio": 1.6, "no_speech_prob": 1.6699776097084396e-05}, {"id": 689, "seek": 259564, "start": 2600.2799999999997, "end": 2602.72, "text": " a, ht plus b.", "tokens": [257, 11, 276, 83, 1804, 272, 13], "temperature": 0.0, "avg_logprob": -0.21139223007928756, "compression_ratio": 1.6, "no_speech_prob": 1.6699776097084396e-05}, {"id": 690, "seek": 259564, "start": 2602.72, "end": 2605.24, "text": " And then the picture here is, right, here's ht.", "tokens": [400, 550, 264, 3036, 510, 307, 11, 558, 11, 510, 311, 276, 83, 13], "temperature": 0.0, "avg_logprob": -0.21139223007928756, "compression_ratio": 1.6, "no_speech_prob": 1.6699776097084396e-05}, {"id": 691, "seek": 259564, "start": 2605.24, "end": 2609.72, "text": " It's the very last encoder state.", "tokens": [467, 311, 264, 588, 1036, 2058, 19866, 1785, 13], "temperature": 0.0, "avg_logprob": -0.21139223007928756, "compression_ratio": 1.6, "no_speech_prob": 1.6699776097084396e-05}, {"id": 692, "seek": 259564, "start": 2609.72, "end": 2616.24, "text": " And now this has sort of the, it's seen all of its history, right, and so you can apply", "tokens": [400, 586, 341, 575, 1333, 295, 264, 11, 309, 311, 1612, 439, 295, 1080, 2503, 11, 558, 11, 293, 370, 291, 393, 3079], "temperature": 0.0, "avg_logprob": -0.21139223007928756, "compression_ratio": 1.6, "no_speech_prob": 1.6699776097084396e-05}, {"id": 693, "seek": 259564, "start": 2616.24, "end": 2621.64, "text": " a linear layer here, maybe multiplying it by some parameters a and b that were not", "tokens": [257, 8213, 4583, 510, 11, 1310, 30955, 309, 538, 512, 9834, 257, 293, 272, 300, 645, 406], "temperature": 0.0, "avg_logprob": -0.21139223007928756, "compression_ratio": 1.6, "no_speech_prob": 1.6699776097084396e-05}, {"id": 694, "seek": 262164, "start": 2621.64, "end": 2626.8799999999997, "text": " pre-trained, and then you're predicting sentiment maybe, you know, plus or minus sentiment,", "tokens": [659, 12, 17227, 2001, 11, 293, 550, 291, 434, 32884, 16149, 1310, 11, 291, 458, 11, 1804, 420, 3175, 16149, 11], "temperature": 0.0, "avg_logprob": -0.1491753033229283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 4.4687476474791765e-05}, {"id": 695, "seek": 262164, "start": 2626.8799999999997, "end": 2627.8799999999997, "text": " perhaps.", "tokens": [4317, 13], "temperature": 0.0, "avg_logprob": -0.1491753033229283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 4.4687476474791765e-05}, {"id": 696, "seek": 262164, "start": 2627.8799999999997, "end": 2631.14, "text": " And so, you know, look at the red and the gray, so most of the parameters of my neural", "tokens": [400, 370, 11, 291, 458, 11, 574, 412, 264, 2182, 293, 264, 10855, 11, 370, 881, 295, 264, 9834, 295, 452, 18161], "temperature": 0.0, "avg_logprob": -0.1491753033229283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 4.4687476474791765e-05}, {"id": 697, "seek": 262164, "start": 2631.14, "end": 2636.72, "text": " network have now been pre-trained, the very last layer that's learning, the sentiment,", "tokens": [3209, 362, 586, 668, 659, 12, 17227, 2001, 11, 264, 588, 1036, 4583, 300, 311, 2539, 11, 264, 16149, 11], "temperature": 0.0, "avg_logprob": -0.1491753033229283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 4.4687476474791765e-05}, {"id": 698, "seek": 262164, "start": 2636.72, "end": 2640.3199999999997, "text": " say, decision, has not been pre-trained.", "tokens": [584, 11, 3537, 11, 575, 406, 668, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.1491753033229283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 4.4687476474791765e-05}, {"id": 699, "seek": 262164, "start": 2640.3199999999997, "end": 2642.12, "text": " So those have been randomly initialized.", "tokens": [407, 729, 362, 668, 16979, 5883, 1602, 13], "temperature": 0.0, "avg_logprob": -0.1491753033229283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 4.4687476474791765e-05}, {"id": 700, "seek": 262164, "start": 2642.12, "end": 2646.48, "text": " And when you, when you take the loss of the sentiment loss, right, you train not just", "tokens": [400, 562, 291, 11, 562, 291, 747, 264, 4470, 295, 264, 16149, 4470, 11, 558, 11, 291, 3847, 406, 445], "temperature": 0.0, "avg_logprob": -0.1491753033229283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 4.4687476474791765e-05}, {"id": 701, "seek": 262164, "start": 2646.48, "end": 2651.2, "text": " the linear layer here, but you actually back propagate the gradients all the way through", "tokens": [264, 8213, 4583, 510, 11, 457, 291, 767, 646, 48256, 264, 2771, 2448, 439, 264, 636, 807], "temperature": 0.0, "avg_logprob": -0.1491753033229283, "compression_ratio": 1.8501742160278745, "no_speech_prob": 4.4687476474791765e-05}, {"id": 702, "seek": 265120, "start": 2651.2, "end": 2656.3999999999996, "text": " the entire pre-trained network and fine tune all of those parameters, right?", "tokens": [264, 2302, 659, 12, 17227, 2001, 3209, 293, 2489, 10864, 439, 295, 729, 9834, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15770293063804752, "compression_ratio": 1.8693877551020408, "no_speech_prob": 2.796671469695866e-05}, {"id": 703, "seek": 265120, "start": 2656.3999999999996, "end": 2660.3599999999997, "text": " So it's not like you're just training this, fine tuning time, this linear layer, you're", "tokens": [407, 309, 311, 406, 411, 291, 434, 445, 3097, 341, 11, 2489, 15164, 565, 11, 341, 8213, 4583, 11, 291, 434], "temperature": 0.0, "avg_logprob": -0.15770293063804752, "compression_ratio": 1.8693877551020408, "no_speech_prob": 2.796671469695866e-05}, {"id": 704, "seek": 265120, "start": 2660.3599999999997, "end": 2665.8399999999997, "text": " training the whole network as a function of this fine tuning loss.", "tokens": [3097, 264, 1379, 3209, 382, 257, 2445, 295, 341, 2489, 15164, 4470, 13], "temperature": 0.0, "avg_logprob": -0.15770293063804752, "compression_ratio": 1.8693877551020408, "no_speech_prob": 2.796671469695866e-05}, {"id": 705, "seek": 265120, "start": 2665.8399999999997, "end": 2670.16, "text": " And you know, maybe it's bad that like the linear layer wasn't pre-trained.", "tokens": [400, 291, 458, 11, 1310, 309, 311, 1578, 300, 411, 264, 8213, 4583, 2067, 380, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.15770293063804752, "compression_ratio": 1.8693877551020408, "no_speech_prob": 2.796671469695866e-05}, {"id": 706, "seek": 265120, "start": 2670.16, "end": 2674.8799999999997, "text": " In the grand scheme of things, it's not that many parameters also.", "tokens": [682, 264, 2697, 12232, 295, 721, 11, 309, 311, 406, 300, 867, 9834, 611, 13], "temperature": 0.0, "avg_logprob": -0.15770293063804752, "compression_ratio": 1.8693877551020408, "no_speech_prob": 2.796671469695866e-05}, {"id": 707, "seek": 265120, "start": 2674.8799999999997, "end": 2678.68, "text": " So this is you, so this is just one way to interact with pre-trained models, right?", "tokens": [407, 341, 307, 291, 11, 370, 341, 307, 445, 472, 636, 281, 4648, 365, 659, 12, 17227, 2001, 5245, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.15770293063804752, "compression_ratio": 1.8693877551020408, "no_speech_prob": 2.796671469695866e-05}, {"id": 708, "seek": 267868, "start": 2678.68, "end": 2682.72, "text": " And so what I want you to take away from this is that there was a contract that we had", "tokens": [400, 370, 437, 286, 528, 291, 281, 747, 1314, 490, 341, 307, 300, 456, 390, 257, 4364, 300, 321, 632], "temperature": 0.0, "avg_logprob": -0.099697691599528, "compression_ratio": 1.9240506329113924, "no_speech_prob": 1.6962867448455654e-05}, {"id": 709, "seek": 267868, "start": 2682.72, "end": 2684.6, "text": " with the original model, right?", "tokens": [365, 264, 3380, 2316, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.099697691599528, "compression_ratio": 1.9240506329113924, "no_speech_prob": 1.6962867448455654e-05}, {"id": 710, "seek": 267868, "start": 2684.6, "end": 2688.68, "text": " The contract was that it was defining probability distributions.", "tokens": [440, 4364, 390, 300, 309, 390, 17827, 8482, 37870, 13], "temperature": 0.0, "avg_logprob": -0.099697691599528, "compression_ratio": 1.9240506329113924, "no_speech_prob": 1.6962867448455654e-05}, {"id": 711, "seek": 267868, "start": 2688.68, "end": 2692.04, "text": " But when we're fine tuning, when we're interacting with the pre-trained model, what we also", "tokens": [583, 562, 321, 434, 2489, 15164, 11, 562, 321, 434, 18017, 365, 264, 659, 12, 17227, 2001, 2316, 11, 437, 321, 611], "temperature": 0.0, "avg_logprob": -0.099697691599528, "compression_ratio": 1.9240506329113924, "no_speech_prob": 1.6962867448455654e-05}, {"id": 712, "seek": 267868, "start": 2692.04, "end": 2695.52, "text": " have are just like the trained weights and the network architecture.", "tokens": [362, 366, 445, 411, 264, 8895, 17443, 293, 264, 3209, 9482, 13], "temperature": 0.0, "avg_logprob": -0.099697691599528, "compression_ratio": 1.9240506329113924, "no_speech_prob": 1.6962867448455654e-05}, {"id": 713, "seek": 267868, "start": 2695.52, "end": 2698.72, "text": " We don't need to use it as a language model, we don't need to use it as a probability", "tokens": [492, 500, 380, 643, 281, 764, 309, 382, 257, 2856, 2316, 11, 321, 500, 380, 643, 281, 764, 309, 382, 257, 8482], "temperature": 0.0, "avg_logprob": -0.099697691599528, "compression_ratio": 1.9240506329113924, "no_speech_prob": 1.6962867448455654e-05}, {"id": 714, "seek": 267868, "start": 2698.72, "end": 2699.72, "text": " distribution.", "tokens": [7316, 13], "temperature": 0.0, "avg_logprob": -0.099697691599528, "compression_ratio": 1.9240506329113924, "no_speech_prob": 1.6962867448455654e-05}, {"id": 715, "seek": 267868, "start": 2699.72, "end": 2704.04, "text": " When we're actually fine tuning it, we're really just using it for its initialization", "tokens": [1133, 321, 434, 767, 2489, 15164, 309, 11, 321, 434, 534, 445, 1228, 309, 337, 1080, 5883, 2144], "temperature": 0.0, "avg_logprob": -0.099697691599528, "compression_ratio": 1.9240506329113924, "no_speech_prob": 1.6962867448455654e-05}, {"id": 716, "seek": 267868, "start": 2704.04, "end": 2708.64, "text": " of its parameters and saying, oh, this is just a transformer decoder that was", "tokens": [295, 1080, 9834, 293, 1566, 11, 1954, 11, 341, 307, 445, 257, 31782, 979, 19866, 300, 390], "temperature": 0.0, "avg_logprob": -0.099697691599528, "compression_ratio": 1.9240506329113924, "no_speech_prob": 1.6962867448455654e-05}, {"id": 717, "seek": 270864, "start": 2708.64, "end": 2714.56, "text": " pre-trained by, oh, and it happens to be really great in that when you find tuna on some", "tokens": [659, 12, 17227, 2001, 538, 11, 1954, 11, 293, 309, 2314, 281, 312, 534, 869, 294, 300, 562, 291, 915, 26670, 322, 512], "temperature": 0.0, "avg_logprob": -0.16717657424111404, "compression_ratio": 1.6795774647887325, "no_speech_prob": 2.753314583969768e-05}, {"id": 718, "seek": 270864, "start": 2714.56, "end": 2717.2, "text": " sentiment data, it does a really good job.", "tokens": [16149, 1412, 11, 309, 775, 257, 534, 665, 1691, 13], "temperature": 0.0, "avg_logprob": -0.16717657424111404, "compression_ratio": 1.6795774647887325, "no_speech_prob": 2.753314583969768e-05}, {"id": 719, "seek": 270864, "start": 2717.2, "end": 2722.3599999999997, "text": " Okay, but there's a second way to interact with pre-trained decoders, which is in some", "tokens": [1033, 11, 457, 456, 311, 257, 1150, 636, 281, 4648, 365, 659, 12, 17227, 2001, 979, 378, 433, 11, 597, 307, 294, 512], "temperature": 0.0, "avg_logprob": -0.16717657424111404, "compression_ratio": 1.6795774647887325, "no_speech_prob": 2.753314583969768e-05}, {"id": 720, "seek": 270864, "start": 2722.3599999999997, "end": 2724.12, "text": " sense even more natural.", "tokens": [2020, 754, 544, 3303, 13], "temperature": 0.0, "avg_logprob": -0.16717657424111404, "compression_ratio": 1.6795774647887325, "no_speech_prob": 2.753314583969768e-05}, {"id": 721, "seek": 270864, "start": 2724.12, "end": 2728.12, "text": " It actually is closer to the contract that we started with.", "tokens": [467, 767, 307, 4966, 281, 264, 4364, 300, 321, 1409, 365, 13], "temperature": 0.0, "avg_logprob": -0.16717657424111404, "compression_ratio": 1.6795774647887325, "no_speech_prob": 2.753314583969768e-05}, {"id": 722, "seek": 270864, "start": 2728.12, "end": 2732.7999999999997, "text": " So we don't have to just ignore the fact that it was a probability distribution entirely,", "tokens": [407, 321, 500, 380, 362, 281, 445, 11200, 264, 1186, 300, 309, 390, 257, 8482, 7316, 7696, 11], "temperature": 0.0, "avg_logprob": -0.16717657424111404, "compression_ratio": 1.6795774647887325, "no_speech_prob": 2.753314583969768e-05}, {"id": 723, "seek": 270864, "start": 2732.7999999999997, "end": 2735.3599999999997, "text": " we can make use of it while still fine tuning it.", "tokens": [321, 393, 652, 764, 295, 309, 1339, 920, 2489, 15164, 309, 13], "temperature": 0.0, "avg_logprob": -0.16717657424111404, "compression_ratio": 1.6795774647887325, "no_speech_prob": 2.753314583969768e-05}, {"id": 724, "seek": 270864, "start": 2735.3599999999997, "end": 2737.3199999999997, "text": " So here's what we're going to do.", "tokens": [407, 510, 311, 437, 321, 434, 516, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.16717657424111404, "compression_ratio": 1.6795774647887325, "no_speech_prob": 2.753314583969768e-05}, {"id": 725, "seek": 273732, "start": 2737.32, "end": 2740.36, "text": " So we can use them as a generator at fine tuning time.", "tokens": [407, 321, 393, 764, 552, 382, 257, 19265, 412, 2489, 15164, 565, 13], "temperature": 0.0, "avg_logprob": -0.13268498700074474, "compression_ratio": 1.680327868852459, "no_speech_prob": 2.078361649182625e-05}, {"id": 726, "seek": 273732, "start": 2740.36, "end": 2747.52, "text": " By generator, I mean, it's going to define this distribution of words given their context.", "tokens": [3146, 19265, 11, 286, 914, 11, 309, 311, 516, 281, 6964, 341, 7316, 295, 2283, 2212, 641, 4319, 13], "temperature": 0.0, "avg_logprob": -0.13268498700074474, "compression_ratio": 1.680327868852459, "no_speech_prob": 2.078361649182625e-05}, {"id": 727, "seek": 273732, "start": 2747.52, "end": 2751.96, "text": " And then we'll actually just fine tune that probability distribution.", "tokens": [400, 550, 321, 603, 767, 445, 2489, 10864, 300, 8482, 7316, 13], "temperature": 0.0, "avg_logprob": -0.13268498700074474, "compression_ratio": 1.680327868852459, "no_speech_prob": 2.078361649182625e-05}, {"id": 728, "seek": 273732, "start": 2751.96, "end": 2758.56, "text": " So in a task like some kind of turn-based dialogue, we might encode the dialogue history", "tokens": [407, 294, 257, 5633, 411, 512, 733, 295, 1261, 12, 6032, 10221, 11, 321, 1062, 2058, 1429, 264, 10221, 2503], "temperature": 0.0, "avg_logprob": -0.13268498700074474, "compression_ratio": 1.680327868852459, "no_speech_prob": 2.078361649182625e-05}, {"id": 729, "seek": 273732, "start": 2758.56, "end": 2761.88, "text": " as your past context.", "tokens": [382, 428, 1791, 4319, 13], "temperature": 0.0, "avg_logprob": -0.13268498700074474, "compression_ratio": 1.680327868852459, "no_speech_prob": 2.078361649182625e-05}, {"id": 730, "seek": 273732, "start": 2761.88, "end": 2766.76, "text": " So you have a dialogue history of some things that people are saying back and forth", "tokens": [407, 291, 362, 257, 10221, 2503, 295, 512, 721, 300, 561, 366, 1566, 646, 293, 5220], "temperature": 0.0, "avg_logprob": -0.13268498700074474, "compression_ratio": 1.680327868852459, "no_speech_prob": 2.078361649182625e-05}, {"id": 731, "seek": 276676, "start": 2766.76, "end": 2770.5600000000004, "text": " to each other, you encode it as words, and you try to predict the next words in the", "tokens": [281, 1184, 661, 11, 291, 2058, 1429, 309, 382, 2283, 11, 293, 291, 853, 281, 6069, 264, 958, 2283, 294, 264], "temperature": 0.0, "avg_logprob": -0.17578274863106863, "compression_ratio": 1.7559055118110236, "no_speech_prob": 3.070970342378132e-05}, {"id": 732, "seek": 276676, "start": 2770.5600000000004, "end": 2771.5600000000004, "text": " dialogue.", "tokens": [10221, 13], "temperature": 0.0, "avg_logprob": -0.17578274863106863, "compression_ratio": 1.7559055118110236, "no_speech_prob": 3.070970342378132e-05}, {"id": 733, "seek": 276676, "start": 2771.5600000000004, "end": 2775.6400000000003, "text": " Right, and maybe you're pre-training objective, you looked at very general purpose text", "tokens": [1779, 11, 293, 1310, 291, 434, 659, 12, 17227, 1760, 10024, 11, 291, 2956, 412, 588, 2674, 4334, 2487], "temperature": 0.0, "avg_logprob": -0.17578274863106863, "compression_ratio": 1.7559055118110236, "no_speech_prob": 3.070970342378132e-05}, {"id": 734, "seek": 276676, "start": 2775.6400000000003, "end": 2779.7200000000003, "text": " from, I don't know, Wikipedia or books or something, and you're fine tuning it as a", "tokens": [490, 11, 286, 500, 380, 458, 11, 28999, 420, 3642, 420, 746, 11, 293, 291, 434, 2489, 15164, 309, 382, 257], "temperature": 0.0, "avg_logprob": -0.17578274863106863, "compression_ratio": 1.7559055118110236, "no_speech_prob": 3.070970342378132e-05}, {"id": 735, "seek": 276676, "start": 2779.7200000000003, "end": 2785.84, "text": " language model, but you're fine tuning it as a language model on this sort of domain-specific", "tokens": [2856, 2316, 11, 457, 291, 434, 2489, 15164, 309, 382, 257, 2856, 2316, 322, 341, 1333, 295, 9274, 12, 29258], "temperature": 0.0, "avg_logprob": -0.17578274863106863, "compression_ratio": 1.7559055118110236, "no_speech_prob": 3.070970342378132e-05}, {"id": 736, "seek": 276676, "start": 2785.84, "end": 2790.2000000000003, "text": " distribution of text like dialogue or maybe summarization where you paste in the whole", "tokens": [7316, 295, 2487, 411, 10221, 420, 1310, 14611, 2144, 689, 291, 9163, 294, 264, 1379], "temperature": 0.0, "avg_logprob": -0.17578274863106863, "compression_ratio": 1.7559055118110236, "no_speech_prob": 3.070970342378132e-05}, {"id": 737, "seek": 279020, "start": 2790.2, "end": 2797.8799999999997, "text": " document and then say a specific word and then the summary and say predict the summary.", "tokens": [4166, 293, 550, 584, 257, 2685, 1349, 293, 550, 264, 12691, 293, 584, 6069, 264, 12691, 13], "temperature": 0.0, "avg_logprob": -0.20468457539876303, "compression_ratio": 1.6875, "no_speech_prob": 1.777633406163659e-05}, {"id": 738, "seek": 279020, "start": 2797.8799999999997, "end": 2803.56, "text": " And so what this looks like is, again, at fine tuning time here, you have your h1 to", "tokens": [400, 370, 437, 341, 1542, 411, 307, 11, 797, 11, 412, 2489, 15164, 565, 510, 11, 291, 362, 428, 276, 16, 281], "temperature": 0.0, "avg_logprob": -0.20468457539876303, "compression_ratio": 1.6875, "no_speech_prob": 1.777633406163659e-05}, {"id": 739, "seek": 279020, "start": 2803.56, "end": 2809.04, "text": " ht is equal to the decoder of the words, and then you have this distribution that you're", "tokens": [276, 83, 307, 2681, 281, 264, 979, 19866, 295, 264, 2283, 11, 293, 550, 291, 362, 341, 7316, 300, 291, 434], "temperature": 0.0, "avg_logprob": -0.20468457539876303, "compression_ratio": 1.6875, "no_speech_prob": 1.777633406163659e-05}, {"id": 740, "seek": 279020, "start": 2809.04, "end": 2815.3199999999997, "text": " fine tuning of wt is a h is the type again, ht minus 1 plus b.", "tokens": [2489, 15164, 295, 261, 83, 307, 257, 276, 307, 264, 2010, 797, 11, 276, 83, 3175, 502, 1804, 272, 13], "temperature": 0.0, "avg_logprob": -0.20468457539876303, "compression_ratio": 1.6875, "no_speech_prob": 1.777633406163659e-05}, {"id": 741, "seek": 281532, "start": 2815.32, "end": 2821.48, "text": " So now every time I have this, I'm predicting these words from word 1, I predict word 2,", "tokens": [407, 586, 633, 565, 286, 362, 341, 11, 286, 478, 32884, 613, 2283, 490, 1349, 502, 11, 286, 6069, 1349, 568, 11], "temperature": 0.0, "avg_logprob": -0.24675837133684728, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.0449654837429989e-05}, {"id": 742, "seek": 281532, "start": 2821.48, "end": 2827.56, "text": " we're 2, I predict word 3, etc., right, the actual last layer of the network unlike before,", "tokens": [321, 434, 568, 11, 286, 6069, 1349, 805, 11, 5183, 7933, 558, 11, 264, 3539, 1036, 4583, 295, 264, 3209, 8343, 949, 11], "temperature": 0.0, "avg_logprob": -0.24675837133684728, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.0449654837429989e-05}, {"id": 743, "seek": 281532, "start": 2827.56, "end": 2832.0800000000004, "text": " the last layer of the network has been pre-trained, but I'm still fine tuning the whole thing.", "tokens": [264, 1036, 4583, 295, 264, 3209, 575, 668, 659, 12, 17227, 2001, 11, 457, 286, 478, 920, 2489, 15164, 264, 1379, 551, 13], "temperature": 0.0, "avg_logprob": -0.24675837133684728, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.0449654837429989e-05}, {"id": 744, "seek": 281532, "start": 2832.0800000000004, "end": 2837.6800000000003, "text": " Right, so a and b here are mapping to sort of a probability distribution over my vocabulary", "tokens": [1779, 11, 370, 257, 293, 272, 510, 366, 18350, 281, 1333, 295, 257, 8482, 7316, 670, 452, 19864], "temperature": 0.0, "avg_logprob": -0.24675837133684728, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.0449654837429989e-05}, {"id": 745, "seek": 281532, "start": 2837.6800000000003, "end": 2843.04, "text": " or the logits of a probability distribution, and I guess get this sort of like tweak them", "tokens": [420, 264, 3565, 1208, 295, 257, 8482, 7316, 11, 293, 286, 2041, 483, 341, 1333, 295, 411, 29879, 552], "temperature": 0.0, "avg_logprob": -0.24675837133684728, "compression_ratio": 1.8207171314741035, "no_speech_prob": 1.0449654837429989e-05}, {"id": 746, "seek": 284304, "start": 2843.04, "end": 2848.2799999999997, "text": " now, in order to have the distribution that I'm going to use, reflect the thing like dialogue", "tokens": [586, 11, 294, 1668, 281, 362, 264, 7316, 300, 286, 478, 516, 281, 764, 11, 5031, 264, 551, 411, 10221], "temperature": 0.0, "avg_logprob": -0.2155136956108941, "compression_ratio": 1.5961538461538463, "no_speech_prob": 2.0461018721107394e-05}, {"id": 747, "seek": 284304, "start": 2848.2799999999997, "end": 2850.04, "text": " that I wanted to reflect.", "tokens": [300, 286, 1415, 281, 5031, 13], "temperature": 0.0, "avg_logprob": -0.2155136956108941, "compression_ratio": 1.5961538461538463, "no_speech_prob": 2.0461018721107394e-05}, {"id": 748, "seek": 284304, "start": 2850.04, "end": 2856.96, "text": " Okay, so those are two ways of interacting with a pre-trained decoder.", "tokens": [1033, 11, 370, 729, 366, 732, 2098, 295, 18017, 365, 257, 659, 12, 17227, 2001, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.2155136956108941, "compression_ratio": 1.5961538461538463, "no_speech_prob": 2.0461018721107394e-05}, {"id": 749, "seek": 284304, "start": 2856.96, "end": 2864.12, "text": " Now here's an example of what is ended up being the first, that be a line of wildly successful", "tokens": [823, 510, 311, 364, 1365, 295, 437, 307, 4590, 493, 885, 264, 700, 11, 300, 312, 257, 1622, 295, 34731, 4406], "temperature": 0.0, "avg_logprob": -0.2155136956108941, "compression_ratio": 1.5961538461538463, "no_speech_prob": 2.0461018721107394e-05}, {"id": 750, "seek": 284304, "start": 2864.12, "end": 2869.16, "text": " or at least talked about pre-trained decoders.", "tokens": [420, 412, 1935, 2825, 466, 659, 12, 17227, 2001, 979, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.2155136956108941, "compression_ratio": 1.5961538461538463, "no_speech_prob": 2.0461018721107394e-05}, {"id": 751, "seek": 286916, "start": 2869.16, "end": 2877.8399999999997, "text": " So the generative pre-trained decoder, or GPC, was a huge success in some sense, or at", "tokens": [407, 264, 1337, 1166, 659, 12, 17227, 2001, 979, 19866, 11, 420, 460, 12986, 11, 390, 257, 2603, 2245, 294, 512, 2020, 11, 420, 412], "temperature": 0.0, "avg_logprob": -0.16685167948404947, "compression_ratio": 1.543103448275862, "no_speech_prob": 3.426241164561361e-05}, {"id": 752, "seek": 286916, "start": 2877.8399999999997, "end": 2884.92, "text": " least it got a lot of buzz, so it's a transformer decoder, no encoder, with 12 layers, I'm giving", "tokens": [1935, 309, 658, 257, 688, 295, 13036, 11, 370, 309, 311, 257, 31782, 979, 19866, 11, 572, 2058, 19866, 11, 365, 2272, 7914, 11, 286, 478, 2902], "temperature": 0.0, "avg_logprob": -0.16685167948404947, "compression_ratio": 1.543103448275862, "no_speech_prob": 3.426241164561361e-05}, {"id": 753, "seek": 286916, "start": 2884.92, "end": 2890.64, "text": " you the details so you can start to get a feeling for how the size of things changes.", "tokens": [291, 264, 4365, 370, 291, 393, 722, 281, 483, 257, 2633, 337, 577, 264, 2744, 295, 721, 2962, 13], "temperature": 0.0, "avg_logprob": -0.16685167948404947, "compression_ratio": 1.543103448275862, "no_speech_prob": 3.426241164561361e-05}, {"id": 754, "seek": 286916, "start": 2890.64, "end": 2895.08, "text": " Over the years, as we'll continue to progress here, had each of our, each of the hidden", "tokens": [4886, 264, 924, 11, 382, 321, 603, 2354, 281, 4205, 510, 11, 632, 1184, 295, 527, 11, 1184, 295, 264, 7633], "temperature": 0.0, "avg_logprob": -0.16685167948404947, "compression_ratio": 1.543103448275862, "no_speech_prob": 3.426241164561361e-05}, {"id": 755, "seek": 289508, "start": 2895.08, "end": 2900.36, "text": " states was dimensionality, 70, had 768, so if you remember back to last lecture, we had", "tokens": [4368, 390, 10139, 1860, 11, 5285, 11, 632, 24733, 23, 11, 370, 498, 291, 1604, 646, 281, 1036, 7991, 11, 321, 632], "temperature": 0.0, "avg_logprob": -0.17836727240146735, "compression_ratio": 1.8072289156626506, "no_speech_prob": 9.758485975908116e-05}, {"id": 756, "seek": 289508, "start": 2900.36, "end": 2906.92, "text": " a term D, which was our dimensionality, so D is 768, and then an interesting statement", "tokens": [257, 1433, 413, 11, 597, 390, 527, 10139, 1860, 11, 370, 413, 307, 24733, 23, 11, 293, 550, 364, 1880, 5629], "temperature": 0.0, "avg_logprob": -0.17836727240146735, "compression_ratio": 1.8072289156626506, "no_speech_prob": 9.758485975908116e-05}, {"id": 757, "seek": 289508, "start": 2906.92, "end": 2911.04, "text": " that you should keep in mind for the engineering-minded folks is that the actual feed-forward", "tokens": [300, 291, 820, 1066, 294, 1575, 337, 264, 7043, 12, 23310, 4024, 307, 300, 264, 3539, 3154, 12, 13305], "temperature": 0.0, "avg_logprob": -0.17836727240146735, "compression_ratio": 1.8072289156626506, "no_speech_prob": 9.758485975908116e-05}, {"id": 758, "seek": 289508, "start": 2911.04, "end": 2915.52, "text": " layers, right, you've got a hidden layer in the feed-forward layer, and this was actually", "tokens": [7914, 11, 558, 11, 291, 600, 658, 257, 7633, 4583, 294, 264, 3154, 12, 13305, 4583, 11, 293, 341, 390, 767], "temperature": 0.0, "avg_logprob": -0.17836727240146735, "compression_ratio": 1.8072289156626506, "no_speech_prob": 9.758485975908116e-05}, {"id": 759, "seek": 289508, "start": 2915.52, "end": 2921.96, "text": " very large, so you had these sort of like position-wise feed-forward layers, right, and the", "tokens": [588, 2416, 11, 370, 291, 632, 613, 1333, 295, 411, 2535, 12, 3711, 3154, 12, 13305, 7914, 11, 558, 11, 293, 264], "temperature": 0.0, "avg_logprob": -0.17836727240146735, "compression_ratio": 1.8072289156626506, "no_speech_prob": 9.758485975908116e-05}, {"id": 760, "seek": 292196, "start": 2921.96, "end": 2927.4, "text": " feed-forward layer would take the 768-dimensional vector, sort of like project it to 3,000-dimensional", "tokens": [3154, 12, 13305, 4583, 576, 747, 264, 24733, 23, 12, 18759, 8062, 11, 1333, 295, 411, 1716, 309, 281, 805, 11, 1360, 12, 18759], "temperature": 0.0, "avg_logprob": -0.23226699829101563, "compression_ratio": 1.6771929824561405, "no_speech_prob": 2.429416053928435e-05}, {"id": 761, "seek": 292196, "start": 2927.4, "end": 2932.4, "text": " space through the sort of non-linearity, and then project it back to 768.", "tokens": [1901, 807, 264, 1333, 295, 2107, 12, 1889, 17409, 11, 293, 550, 1716, 309, 646, 281, 24733, 23, 13], "temperature": 0.0, "avg_logprob": -0.23226699829101563, "compression_ratio": 1.6771929824561405, "no_speech_prob": 2.429416053928435e-05}, {"id": 762, "seek": 292196, "start": 2932.4, "end": 2936.96, "text": " This ends up being because you can squash a lot more parameters in, for not too much", "tokens": [639, 5314, 493, 885, 570, 291, 393, 30725, 257, 688, 544, 9834, 294, 11, 337, 406, 886, 709], "temperature": 0.0, "avg_logprob": -0.23226699829101563, "compression_ratio": 1.6771929824561405, "no_speech_prob": 2.429416053928435e-05}, {"id": 763, "seek": 292196, "start": 2936.96, "end": 2941.2400000000002, "text": " more compute in this way, but that's curious.", "tokens": [544, 14722, 294, 341, 636, 11, 457, 300, 311, 6369, 13], "temperature": 0.0, "avg_logprob": -0.23226699829101563, "compression_ratio": 1.6771929824561405, "no_speech_prob": 2.429416053928435e-05}, {"id": 764, "seek": 292196, "start": 2941.2400000000002, "end": 2946.28, "text": " Okay, and then, byte-parent coding, it's actually, was this one byte-parent coding?", "tokens": [1033, 11, 293, 550, 11, 40846, 12, 38321, 17720, 11, 309, 311, 767, 11, 390, 341, 472, 40846, 12, 38321, 17720, 30], "temperature": 0.0, "avg_logprob": -0.23226699829101563, "compression_ratio": 1.6771929824561405, "no_speech_prob": 2.429416053928435e-05}, {"id": 765, "seek": 292196, "start": 2946.28, "end": 2950.6, "text": " Well, it was a sub-word vocabulary with 40,000 merges, so 40,000 merges, so that's not", "tokens": [1042, 11, 309, 390, 257, 1422, 12, 7462, 19864, 365, 3356, 11, 1360, 3551, 2880, 11, 370, 3356, 11, 1360, 3551, 2880, 11, 370, 300, 311, 406], "temperature": 0.0, "avg_logprob": -0.23226699829101563, "compression_ratio": 1.6771929824561405, "no_speech_prob": 2.429416053928435e-05}, {"id": 766, "seek": 295060, "start": 2950.6, "end": 2955.12, "text": " the size of the vocabulary because you started with a bunch of characters, and I don't remember", "tokens": [264, 2744, 295, 264, 19864, 570, 291, 1409, 365, 257, 3840, 295, 4342, 11, 293, 286, 500, 380, 1604], "temperature": 0.0, "avg_logprob": -0.18401724605237024, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.6469962146366015e-05}, {"id": 767, "seek": 295060, "start": 2955.12, "end": 2959.68, "text": " how many characters they started with, but so it's a relatively small vocabulary you can", "tokens": [577, 867, 4342, 436, 1409, 365, 11, 457, 370, 309, 311, 257, 7226, 1359, 19864, 291, 393], "temperature": 0.0, "avg_logprob": -0.18401724605237024, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.6469962146366015e-05}, {"id": 768, "seek": 295060, "start": 2959.68, "end": 2961.6, "text": " see, right?", "tokens": [536, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.18401724605237024, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.6469962146366015e-05}, {"id": 769, "seek": 295060, "start": 2961.6, "end": 2967.24, "text": " And compared to, if you tried to say, have every word, have a unique representation, now", "tokens": [400, 5347, 281, 11, 498, 291, 3031, 281, 584, 11, 362, 633, 1349, 11, 362, 257, 3845, 10290, 11, 586], "temperature": 0.0, "avg_logprob": -0.18401724605237024, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.6469962146366015e-05}, {"id": 770, "seek": 295060, "start": 2967.24, "end": 2972.6, "text": " it's going to be trained on books, corporates, it's got 7,000 unique books, and it contains", "tokens": [309, 311, 516, 281, 312, 8895, 322, 3642, 11, 6804, 1024, 11, 309, 311, 658, 1614, 11, 1360, 3845, 3642, 11, 293, 309, 8306], "temperature": 0.0, "avg_logprob": -0.18401724605237024, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.6469962146366015e-05}, {"id": 771, "seek": 295060, "start": 2972.6, "end": 2977.92, "text": " long spans of contiguous texts, so you have, instead of, say, training it on individual", "tokens": [938, 44086, 295, 660, 30525, 15765, 11, 370, 291, 362, 11, 2602, 295, 11, 584, 11, 3097, 309, 322, 2609], "temperature": 0.0, "avg_logprob": -0.18401724605237024, "compression_ratio": 1.684782608695652, "no_speech_prob": 5.6469962146366015e-05}, {"id": 772, "seek": 297792, "start": 2977.92, "end": 2980.7200000000003, "text": " sentences, just small short sentences, right?", "tokens": [16579, 11, 445, 1359, 2099, 16579, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2441474190593636, "compression_ratio": 1.7175324675324675, "no_speech_prob": 8.217010326916352e-05}, {"id": 773, "seek": 297792, "start": 2980.7200000000003, "end": 2986.0, "text": " The model is able to learn long distance dependencies because you haven't split, like, a book", "tokens": [440, 2316, 307, 1075, 281, 1466, 938, 4560, 36606, 570, 291, 2378, 380, 7472, 11, 411, 11, 257, 1446], "temperature": 0.0, "avg_logprob": -0.2441474190593636, "compression_ratio": 1.7175324675324675, "no_speech_prob": 8.217010326916352e-05}, {"id": 774, "seek": 297792, "start": 2986.0, "end": 2988.8, "text": " into random sentences and shuffled them all around.", "tokens": [666, 4974, 16579, 293, 402, 33974, 552, 439, 926, 13], "temperature": 0.0, "avg_logprob": -0.2441474190593636, "compression_ratio": 1.7175324675324675, "no_speech_prob": 8.217010326916352e-05}, {"id": 775, "seek": 297792, "start": 2988.8, "end": 2993.04, "text": " You've sort of kept it contiguous, so we can have that sort of consistency.", "tokens": [509, 600, 1333, 295, 4305, 309, 660, 30525, 11, 370, 321, 393, 362, 300, 1333, 295, 14416, 13], "temperature": 0.0, "avg_logprob": -0.2441474190593636, "compression_ratio": 1.7175324675324675, "no_speech_prob": 8.217010326916352e-05}, {"id": 776, "seek": 297792, "start": 2993.04, "end": 2998.6800000000003, "text": " And then, a little treat here, yeah, so GPC never showed up in the original paper, or", "tokens": [400, 550, 11, 257, 707, 2387, 510, 11, 1338, 11, 370, 460, 12986, 1128, 4712, 493, 294, 264, 3380, 3035, 11, 420], "temperature": 0.0, "avg_logprob": -0.2441474190593636, "compression_ratio": 1.7175324675324675, "no_speech_prob": 8.217010326916352e-05}, {"id": 777, "seek": 297792, "start": 2998.6800000000003, "end": 3003.12, "text": " the original blog post, like as an acronym, and it could actually sort of refer to, like,", "tokens": [264, 3380, 6968, 2183, 11, 411, 382, 364, 39195, 11, 293, 309, 727, 767, 1333, 295, 2864, 281, 11, 411, 11], "temperature": 0.0, "avg_logprob": -0.2441474190593636, "compression_ratio": 1.7175324675324675, "no_speech_prob": 8.217010326916352e-05}, {"id": 778, "seek": 297792, "start": 3003.12, "end": 3007.12, "text": " generative pre-training, sort of what, like, the title of the paper would suggest, or", "tokens": [1337, 1166, 659, 12, 17227, 1760, 11, 1333, 295, 437, 11, 411, 11, 264, 4876, 295, 264, 3035, 576, 3402, 11, 420], "temperature": 0.0, "avg_logprob": -0.2441474190593636, "compression_ratio": 1.7175324675324675, "no_speech_prob": 8.217010326916352e-05}, {"id": 779, "seek": 300712, "start": 3007.12, "end": 3009.68, "text": " generative pre-trained transformer.", "tokens": [1337, 1166, 659, 12, 17227, 2001, 31782, 13], "temperature": 0.0, "avg_logprob": -0.20058766117802374, "compression_ratio": 1.8957528957528957, "no_speech_prob": 4.9846730689750984e-05}, {"id": 780, "seek": 300712, "start": 3009.68, "end": 3013.52, "text": " And I sort of decided to say generative pre-trained transformer because this seemed like way", "tokens": [400, 286, 1333, 295, 3047, 281, 584, 1337, 1166, 659, 12, 17227, 2001, 31782, 570, 341, 6576, 411, 636], "temperature": 0.0, "avg_logprob": -0.20058766117802374, "compression_ratio": 1.8957528957528957, "no_speech_prob": 4.9846730689750984e-05}, {"id": 781, "seek": 300712, "start": 3013.52, "end": 3015.3599999999997, "text": " too general.", "tokens": [886, 2674, 13], "temperature": 0.0, "avg_logprob": -0.20058766117802374, "compression_ratio": 1.8957528957528957, "no_speech_prob": 4.9846730689750984e-05}, {"id": 782, "seek": 300712, "start": 3015.3599999999997, "end": 3017.3599999999997, "text": " So GPC.", "tokens": [407, 460, 12986, 13], "temperature": 0.0, "avg_logprob": -0.20058766117802374, "compression_ratio": 1.8957528957528957, "no_speech_prob": 4.9846730689750984e-05}, {"id": 783, "seek": 300712, "start": 3017.3599999999997, "end": 3022.68, "text": " Okay, so they pre-trained this huge language model transformer, this huge transformer", "tokens": [1033, 11, 370, 436, 659, 12, 17227, 2001, 341, 2603, 2856, 2316, 31782, 11, 341, 2603, 31782], "temperature": 0.0, "avg_logprob": -0.20058766117802374, "compression_ratio": 1.8957528957528957, "no_speech_prob": 4.9846730689750984e-05}, {"id": 784, "seek": 300712, "start": 3022.68, "end": 3025.64, "text": " decoder, just on 7,000 books.", "tokens": [979, 19866, 11, 445, 322, 1614, 11, 1360, 3642, 13], "temperature": 0.0, "avg_logprob": -0.20058766117802374, "compression_ratio": 1.8957528957528957, "no_speech_prob": 4.9846730689750984e-05}, {"id": 785, "seek": 300712, "start": 3025.64, "end": 3028.7999999999997, "text": " And they fine-tuned it on a number of different tasks, and I want to talk a little bit about", "tokens": [400, 436, 2489, 12, 83, 43703, 309, 322, 257, 1230, 295, 819, 9608, 11, 293, 286, 528, 281, 751, 257, 707, 857, 466], "temperature": 0.0, "avg_logprob": -0.20058766117802374, "compression_ratio": 1.8957528957528957, "no_speech_prob": 4.9846730689750984e-05}, {"id": 786, "seek": 300712, "start": 3028.7999999999997, "end": 3031.7999999999997, "text": " the details about how they fine-tuned it.", "tokens": [264, 4365, 466, 577, 436, 2489, 12, 83, 43703, 309, 13], "temperature": 0.0, "avg_logprob": -0.20058766117802374, "compression_ratio": 1.8957528957528957, "no_speech_prob": 4.9846730689750984e-05}, {"id": 787, "seek": 300712, "start": 3031.7999999999997, "end": 3036.68, "text": " And so they fine-tuned it on one particular task, or family tasks, called natural language", "tokens": [400, 370, 436, 2489, 12, 83, 43703, 309, 322, 472, 1729, 5633, 11, 420, 1605, 9608, 11, 1219, 3303, 2856], "temperature": 0.0, "avg_logprob": -0.20058766117802374, "compression_ratio": 1.8957528957528957, "no_speech_prob": 4.9846730689750984e-05}, {"id": 788, "seek": 303668, "start": 3036.68, "end": 3038.24, "text": " inference.", "tokens": [38253, 13], "temperature": 0.0, "avg_logprob": -0.1787784882705577, "compression_ratio": 1.9613899613899615, "no_speech_prob": 1.7228590877493843e-05}, {"id": 789, "seek": 303668, "start": 3038.24, "end": 3043.3199999999997, "text": " So in natural language inference, we're labeling pairs of sentences as entailing or contradictory", "tokens": [407, 294, 3303, 2856, 38253, 11, 321, 434, 40244, 15494, 295, 16579, 382, 948, 23315, 420, 49555], "temperature": 0.0, "avg_logprob": -0.1787784882705577, "compression_ratio": 1.9613899613899615, "no_speech_prob": 1.7228590877493843e-05}, {"id": 790, "seek": 303668, "start": 3043.3199999999997, "end": 3044.3199999999997, "text": " to each other in neutral.", "tokens": [281, 1184, 661, 294, 10598, 13], "temperature": 0.0, "avg_logprob": -0.1787784882705577, "compression_ratio": 1.9613899613899615, "no_speech_prob": 1.7228590877493843e-05}, {"id": 791, "seek": 303668, "start": 3044.3199999999997, "end": 3050.2799999999997, "text": " So you have a premise, and you hold the premise as sort of true, the man is in the doorway.", "tokens": [407, 291, 362, 257, 22045, 11, 293, 291, 1797, 264, 22045, 382, 1333, 295, 2074, 11, 264, 587, 307, 294, 264, 41992, 13], "temperature": 0.0, "avg_logprob": -0.1787784882705577, "compression_ratio": 1.9613899613899615, "no_speech_prob": 1.7228590877493843e-05}, {"id": 792, "seek": 303668, "start": 3050.2799999999997, "end": 3054.0, "text": " And you have a hypothesis, the person is near the door.", "tokens": [400, 291, 362, 257, 17291, 11, 264, 954, 307, 2651, 264, 2853, 13], "temperature": 0.0, "avg_logprob": -0.1787784882705577, "compression_ratio": 1.9613899613899615, "no_speech_prob": 1.7228590877493843e-05}, {"id": 793, "seek": 303668, "start": 3054.0, "end": 3059.68, "text": " If this person is referring to that man, then, you know, it's sort of like, oh, yeah,", "tokens": [759, 341, 954, 307, 13761, 281, 300, 587, 11, 550, 11, 291, 458, 11, 309, 311, 1333, 295, 411, 11, 1954, 11, 1338, 11], "temperature": 0.0, "avg_logprob": -0.1787784882705577, "compression_ratio": 1.9613899613899615, "no_speech_prob": 1.7228590877493843e-05}, {"id": 794, "seek": 303668, "start": 3059.68, "end": 3064.0, "text": " so this is sort of entailed because there's a person, because the man is a person, and", "tokens": [370, 341, 307, 1333, 295, 948, 24731, 570, 456, 311, 257, 954, 11, 570, 264, 587, 307, 257, 954, 11, 293], "temperature": 0.0, "avg_logprob": -0.1787784882705577, "compression_ratio": 1.9613899613899615, "no_speech_prob": 1.7228590877493843e-05}, {"id": 795, "seek": 303668, "start": 3064.0, "end": 3066.56, "text": " they're in the doorway, then they are near the door.", "tokens": [436, 434, 294, 264, 41992, 11, 550, 436, 366, 2651, 264, 2853, 13], "temperature": 0.0, "avg_logprob": -0.1787784882705577, "compression_ratio": 1.9613899613899615, "no_speech_prob": 1.7228590877493843e-05}, {"id": 796, "seek": 306656, "start": 3066.56, "end": 3071.24, "text": " So you have this sort of logical reasoning that you're doing, or you're supposed to be", "tokens": [407, 291, 362, 341, 1333, 295, 14978, 21577, 300, 291, 434, 884, 11, 420, 291, 434, 3442, 281, 312], "temperature": 0.0, "avg_logprob": -0.17832447233654203, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.0127116183866747e-05}, {"id": 797, "seek": 306656, "start": 3071.24, "end": 3074.12, "text": " able to be doing, and you're labeling these sentences.", "tokens": [1075, 281, 312, 884, 11, 293, 291, 434, 40244, 613, 16579, 13], "temperature": 0.0, "avg_logprob": -0.17832447233654203, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.0127116183866747e-05}, {"id": 798, "seek": 306656, "start": 3074.12, "end": 3075.92, "text": " So it's a labeled task.", "tokens": [407, 309, 311, 257, 21335, 5633, 13], "temperature": 0.0, "avg_logprob": -0.17832447233654203, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.0127116183866747e-05}, {"id": 799, "seek": 306656, "start": 3075.92, "end": 3081.2799999999997, "text": " You've got sort of an input that's cut into two parts, and then one of three outputs.", "tokens": [509, 600, 658, 1333, 295, 364, 4846, 300, 311, 1723, 666, 732, 3166, 11, 293, 550, 472, 295, 1045, 23930, 13], "temperature": 0.0, "avg_logprob": -0.17832447233654203, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.0127116183866747e-05}, {"id": 800, "seek": 306656, "start": 3081.2799999999997, "end": 3085.16, "text": " Okay, so the GPT paper evaluates on this task.", "tokens": [1033, 11, 370, 264, 26039, 51, 3035, 6133, 1024, 322, 341, 5633, 13], "temperature": 0.0, "avg_logprob": -0.17832447233654203, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.0127116183866747e-05}, {"id": 801, "seek": 306656, "start": 3085.16, "end": 3088.12, "text": " But what they've got is a transformer decoder.", "tokens": [583, 437, 436, 600, 658, 307, 257, 31782, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.17832447233654203, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.0127116183866747e-05}, {"id": 802, "seek": 306656, "start": 3088.12, "end": 3090.6, "text": " So what do they do?", "tokens": [407, 437, 360, 436, 360, 30], "temperature": 0.0, "avg_logprob": -0.17832447233654203, "compression_ratio": 1.6367713004484306, "no_speech_prob": 1.0127116183866747e-05}, {"id": 803, "seek": 309060, "start": 3090.6, "end": 3097.04, "text": " This is sort of one of the earlier examples of, you know, taking, instead of changing your", "tokens": [639, 307, 1333, 295, 472, 295, 264, 3071, 5110, 295, 11, 291, 458, 11, 1940, 11, 2602, 295, 4473, 428], "temperature": 0.0, "avg_logprob": -0.1323670004015771, "compression_ratio": 1.7950819672131149, "no_speech_prob": 6.399335688911378e-05}, {"id": 804, "seek": 309060, "start": 3097.04, "end": 3102.96, "text": " neural network architecture to adapt to the kind of task you're doing, you're going to", "tokens": [18161, 3209, 9482, 281, 6231, 281, 264, 733, 295, 5633, 291, 434, 884, 11, 291, 434, 516, 281], "temperature": 0.0, "avg_logprob": -0.1323670004015771, "compression_ratio": 1.7950819672131149, "no_speech_prob": 6.399335688911378e-05}, {"id": 805, "seek": 309060, "start": 3102.96, "end": 3109.92, "text": " just format the task as like a bunch of tokens and not change your architecture.", "tokens": [445, 7877, 264, 5633, 382, 411, 257, 3840, 295, 22667, 293, 406, 1319, 428, 9482, 13], "temperature": 0.0, "avg_logprob": -0.1323670004015771, "compression_ratio": 1.7950819672131149, "no_speech_prob": 6.399335688911378e-05}, {"id": 806, "seek": 309060, "start": 3109.92, "end": 3113.68, "text": " Because the pre-training was so useful, it's probably better to keep the architecture", "tokens": [1436, 264, 659, 12, 17227, 1760, 390, 370, 4420, 11, 309, 311, 1391, 1101, 281, 1066, 264, 9482], "temperature": 0.0, "avg_logprob": -0.1323670004015771, "compression_ratio": 1.7950819672131149, "no_speech_prob": 6.399335688911378e-05}, {"id": 807, "seek": 309060, "start": 3113.68, "end": 3119.7599999999998, "text": " fixed, pre-training it, and then change the task specification to sort of fit the pre-trained", "tokens": [6806, 11, 659, 12, 17227, 1760, 309, 11, 293, 550, 1319, 264, 5633, 31256, 281, 1333, 295, 3318, 264, 659, 12, 17227, 2001], "temperature": 0.0, "avg_logprob": -0.1323670004015771, "compression_ratio": 1.7950819672131149, "no_speech_prob": 6.399335688911378e-05}, {"id": 808, "seek": 311976, "start": 3119.76, "end": 3120.76, "text": " architecture.", "tokens": [9482, 13], "temperature": 0.0, "avg_logprob": -0.1136584153046479, "compression_ratio": 1.75, "no_speech_prob": 2.1437253963085823e-05}, {"id": 809, "seek": 311976, "start": 3120.76, "end": 3125.48, "text": " So what they did, right, they put this token start, this is a special token, the man is", "tokens": [407, 437, 436, 630, 11, 558, 11, 436, 829, 341, 14862, 722, 11, 341, 307, 257, 2121, 14862, 11, 264, 587, 307], "temperature": 0.0, "avg_logprob": -0.1136584153046479, "compression_ratio": 1.75, "no_speech_prob": 2.1437253963085823e-05}, {"id": 810, "seek": 311976, "start": 3125.48, "end": 3129.0800000000004, "text": " in the doorway, some delimiter token, right.", "tokens": [294, 264, 41992, 11, 512, 1103, 332, 1681, 14862, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.1136584153046479, "compression_ratio": 1.75, "no_speech_prob": 2.1437253963085823e-05}, {"id": 811, "seek": 311976, "start": 3129.0800000000004, "end": 3135.1600000000003, "text": " So this is just a linear sequence of tokens that we're giving as one big prefix to GPT.", "tokens": [407, 341, 307, 445, 257, 8213, 8310, 295, 22667, 300, 321, 434, 2902, 382, 472, 955, 46969, 281, 26039, 51, 13], "temperature": 0.0, "avg_logprob": -0.1136584153046479, "compression_ratio": 1.75, "no_speech_prob": 2.1437253963085823e-05}, {"id": 812, "seek": 311976, "start": 3135.1600000000003, "end": 3141.6800000000003, "text": " And then the person is near the door, and then some extra token here, right, extract.", "tokens": [400, 550, 264, 954, 307, 2651, 264, 2853, 11, 293, 550, 512, 2857, 14862, 510, 11, 558, 11, 8947, 13], "temperature": 0.0, "avg_logprob": -0.1136584153046479, "compression_ratio": 1.75, "no_speech_prob": 2.1437253963085823e-05}, {"id": 813, "seek": 311976, "start": 3141.6800000000003, "end": 3145.6800000000003, "text": " And then, you know, the linear classifier that we talked about, and sort of the first", "tokens": [400, 550, 11, 291, 458, 11, 264, 8213, 1508, 9902, 300, 321, 2825, 466, 11, 293, 1333, 295, 264, 700], "temperature": 0.0, "avg_logprob": -0.1136584153046479, "compression_ratio": 1.75, "no_speech_prob": 2.1437253963085823e-05}, {"id": 814, "seek": 314568, "start": 3145.68, "end": 3152.6, "text": " way to interact with models, with decoder models, it's applied to the representation of the", "tokens": [636, 281, 4648, 365, 5245, 11, 365, 979, 19866, 5245, 11, 309, 311, 6456, 281, 264, 10290, 295, 264], "temperature": 0.0, "avg_logprob": -0.17834019174381177, "compression_ratio": 1.6963562753036436, "no_speech_prob": 2.6270494345226325e-05}, {"id": 815, "seek": 314568, "start": 3152.6, "end": 3154.52, "text": " extract token, right.", "tokens": [8947, 14862, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.17834019174381177, "compression_ratio": 1.6963562753036436, "no_speech_prob": 2.6270494345226325e-05}, {"id": 816, "seek": 314568, "start": 3154.52, "end": 3159.0, "text": " So you have the last hidden state on top of extract, and then you fine tune the whole", "tokens": [407, 291, 362, 264, 1036, 7633, 1785, 322, 1192, 295, 8947, 11, 293, 550, 291, 2489, 10864, 264, 1379], "temperature": 0.0, "avg_logprob": -0.17834019174381177, "compression_ratio": 1.6963562753036436, "no_speech_prob": 2.6270494345226325e-05}, {"id": 817, "seek": 314568, "start": 3159.0, "end": 3161.7999999999997, "text": " network to predict these labels, right.", "tokens": [3209, 281, 6069, 613, 16949, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.17834019174381177, "compression_ratio": 1.6963562753036436, "no_speech_prob": 2.6270494345226325e-05}, {"id": 818, "seek": 314568, "start": 3161.7999999999997, "end": 3168.52, "text": " And so this sort of input formatting is increasingly, increasingly used to keep the model architecture", "tokens": [400, 370, 341, 1333, 295, 4846, 39366, 307, 12980, 11, 12980, 1143, 281, 1066, 264, 2316, 9482], "temperature": 0.0, "avg_logprob": -0.17834019174381177, "compression_ratio": 1.6963562753036436, "no_speech_prob": 2.6270494345226325e-05}, {"id": 819, "seek": 314568, "start": 3168.52, "end": 3173.72, "text": " the same and allow for a variety of different problems to be solved with it.", "tokens": [264, 912, 293, 2089, 337, 257, 5673, 295, 819, 2740, 281, 312, 13041, 365, 309, 13], "temperature": 0.0, "avg_logprob": -0.17834019174381177, "compression_ratio": 1.6963562753036436, "no_speech_prob": 2.6270494345226325e-05}, {"id": 820, "seek": 317372, "start": 3173.72, "end": 3175.64, "text": " Okay, and so did it work?", "tokens": [1033, 11, 293, 370, 630, 309, 589, 30], "temperature": 0.0, "avg_logprob": -0.17250174370364865, "compression_ratio": 1.7602739726027397, "no_speech_prob": 4.609167081071064e-05}, {"id": 821, "seek": 317372, "start": 3175.64, "end": 3178.4399999999996, "text": " Unnatural language inference, the answer is yes.", "tokens": [1156, 16296, 2856, 38253, 11, 264, 1867, 307, 2086, 13], "temperature": 0.0, "avg_logprob": -0.17250174370364865, "compression_ratio": 1.7602739726027397, "no_speech_prob": 4.609167081071064e-05}, {"id": 822, "seek": 317372, "start": 3178.4399999999996, "end": 3180.2799999999997, "text": " So there's a number of different numbers here.", "tokens": [407, 456, 311, 257, 1230, 295, 819, 3547, 510, 13], "temperature": 0.0, "avg_logprob": -0.17250174370364865, "compression_ratio": 1.7602739726027397, "no_speech_prob": 4.609167081071064e-05}, {"id": 823, "seek": 317372, "start": 3180.2799999999997, "end": 3181.72, "text": " I wouldn't worry too much about it.", "tokens": [286, 2759, 380, 3292, 886, 709, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.17250174370364865, "compression_ratio": 1.7602739726027397, "no_speech_prob": 4.609167081071064e-05}, {"id": 824, "seek": 317372, "start": 3181.72, "end": 3186.8399999999997, "text": " The fine tune transformer language model is sort of what you should pay attention to.", "tokens": [440, 2489, 10864, 31782, 2856, 2316, 307, 1333, 295, 437, 291, 820, 1689, 3202, 281, 13], "temperature": 0.0, "avg_logprob": -0.17250174370364865, "compression_ratio": 1.7602739726027397, "no_speech_prob": 4.609167081071064e-05}, {"id": 825, "seek": 317372, "start": 3186.8399999999997, "end": 3189.68, "text": " There's a lot of effort that went into the other models, right.", "tokens": [821, 311, 257, 688, 295, 4630, 300, 1437, 666, 264, 661, 5245, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.17250174370364865, "compression_ratio": 1.7602739726027397, "no_speech_prob": 4.609167081071064e-05}, {"id": 826, "seek": 317372, "start": 3189.68, "end": 3191.7599999999998, "text": " And so this is the story of pre-training.", "tokens": [400, 370, 341, 307, 264, 1657, 295, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.17250174370364865, "compression_ratio": 1.7602739726027397, "no_speech_prob": 4.609167081071064e-05}, {"id": 827, "seek": 317372, "start": 3191.7599999999998, "end": 3195.4399999999996, "text": " People put a lot of effort into models that do various sort of careful things.", "tokens": [3432, 829, 257, 688, 295, 4630, 666, 5245, 300, 360, 3683, 1333, 295, 5026, 721, 13], "temperature": 0.0, "avg_logprob": -0.17250174370364865, "compression_ratio": 1.7602739726027397, "no_speech_prob": 4.609167081071064e-05}, {"id": 828, "seek": 317372, "start": 3195.4399999999996, "end": 3200.08, "text": " And then you take a single transformer and you say, I'm going to pre-training it on a", "tokens": [400, 550, 291, 747, 257, 2167, 31782, 293, 291, 584, 11, 286, 478, 516, 281, 659, 12, 17227, 1760, 309, 322, 257], "temperature": 0.0, "avg_logprob": -0.17250174370364865, "compression_ratio": 1.7602739726027397, "no_speech_prob": 4.609167081071064e-05}, {"id": 829, "seek": 320008, "start": 3200.08, "end": 3204.6, "text": " ton of text and not worry too much about anything else and just fine tune it, and you end up", "tokens": [2952, 295, 2487, 293, 406, 3292, 886, 709, 466, 1340, 1646, 293, 445, 2489, 10864, 309, 11, 293, 291, 917, 493], "temperature": 0.0, "avg_logprob": -0.17889260907545157, "compression_ratio": 1.7566666666666666, "no_speech_prob": 3.321107942610979e-05}, {"id": 830, "seek": 320008, "start": 3204.6, "end": 3208.3199999999997, "text": " doing super, super well.", "tokens": [884, 1687, 11, 1687, 731, 13], "temperature": 0.0, "avg_logprob": -0.17889260907545157, "compression_ratio": 1.7566666666666666, "no_speech_prob": 3.321107942610979e-05}, {"id": 831, "seek": 320008, "start": 3208.3199999999997, "end": 3212.96, "text": " Sometimes not too much better in the GPT case than sort of the best known state of the", "tokens": [4803, 406, 886, 709, 1101, 294, 264, 26039, 51, 1389, 813, 1333, 295, 264, 1151, 2570, 1785, 295, 264], "temperature": 0.0, "avg_logprob": -0.17889260907545157, "compression_ratio": 1.7566666666666666, "no_speech_prob": 3.321107942610979e-05}, {"id": 832, "seek": 320008, "start": 3212.96, "end": 3216.16, "text": " art methods, but usually a little bit better.", "tokens": [1523, 7150, 11, 457, 2673, 257, 707, 857, 1101, 13], "temperature": 0.0, "avg_logprob": -0.17889260907545157, "compression_ratio": 1.7566666666666666, "no_speech_prob": 3.321107942610979e-05}, {"id": 833, "seek": 320008, "start": 3216.16, "end": 3219.72, "text": " And again, the amount of effort, the amount of tasks, specific effort that you have to put", "tokens": [400, 797, 11, 264, 2372, 295, 4630, 11, 264, 2372, 295, 9608, 11, 2685, 4630, 300, 291, 362, 281, 829], "temperature": 0.0, "avg_logprob": -0.17889260907545157, "compression_ratio": 1.7566666666666666, "no_speech_prob": 3.321107942610979e-05}, {"id": 834, "seek": 320008, "start": 3219.72, "end": 3221.7999999999997, "text": " into it, it's very low.", "tokens": [666, 309, 11, 309, 311, 588, 2295, 13], "temperature": 0.0, "avg_logprob": -0.17889260907545157, "compression_ratio": 1.7566666666666666, "no_speech_prob": 3.321107942610979e-05}, {"id": 835, "seek": 320008, "start": 3221.7999999999997, "end": 3226.4, "text": " Okay, and so what about the other way of interacting with decoters, right.", "tokens": [1033, 11, 293, 370, 437, 466, 264, 661, 636, 295, 18017, 365, 979, 310, 433, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.17889260907545157, "compression_ratio": 1.7566666666666666, "no_speech_prob": 3.321107942610979e-05}, {"id": 836, "seek": 320008, "start": 3226.4, "end": 3229.72, "text": " So we had, we said that we can interact with decoters just by sampling from them, just", "tokens": [407, 321, 632, 11, 321, 848, 300, 321, 393, 4648, 365, 979, 310, 433, 445, 538, 21179, 490, 552, 11, 445], "temperature": 0.0, "avg_logprob": -0.17889260907545157, "compression_ratio": 1.7566666666666666, "no_speech_prob": 3.321107942610979e-05}, {"id": 837, "seek": 322972, "start": 3229.72, "end": 3232.04, "text": " by saying, well, there are probability distributions.", "tokens": [538, 1566, 11, 731, 11, 456, 366, 8482, 37870, 13], "temperature": 0.0, "avg_logprob": -0.17760247730073475, "compression_ratio": 1.5648854961832062, "no_speech_prob": 2.177530768676661e-05}, {"id": 838, "seek": 322972, "start": 3232.04, "end": 3235.6, "text": " So we can use them in their capacities as language models.", "tokens": [407, 321, 393, 764, 552, 294, 641, 39396, 382, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.17760247730073475, "compression_ratio": 1.5648854961832062, "no_speech_prob": 2.177530768676661e-05}, {"id": 839, "seek": 322972, "start": 3235.6, "end": 3242.3599999999997, "text": " And so GPT 2, this is just really just a bigger GPT, and we're too much about it, with", "tokens": [400, 370, 26039, 51, 568, 11, 341, 307, 445, 534, 445, 257, 3801, 26039, 51, 11, 293, 321, 434, 886, 709, 466, 309, 11, 365], "temperature": 0.0, "avg_logprob": -0.17760247730073475, "compression_ratio": 1.5648854961832062, "no_speech_prob": 2.177530768676661e-05}, {"id": 840, "seek": 322972, "start": 3242.3599999999997, "end": 3245.2, "text": " larger hidden units, more layers.", "tokens": [4833, 7633, 6815, 11, 544, 7914, 13], "temperature": 0.0, "avg_logprob": -0.17760247730073475, "compression_ratio": 1.5648854961832062, "no_speech_prob": 2.177530768676661e-05}, {"id": 841, "seek": 322972, "start": 3245.2, "end": 3250.3599999999997, "text": " When it was trained on more data, it was shown to produce sort of relatively convincing", "tokens": [1133, 309, 390, 8895, 322, 544, 1412, 11, 309, 390, 4898, 281, 5258, 1333, 295, 7226, 24823], "temperature": 0.0, "avg_logprob": -0.17760247730073475, "compression_ratio": 1.5648854961832062, "no_speech_prob": 2.177530768676661e-05}, {"id": 842, "seek": 322972, "start": 3250.3599999999997, "end": 3251.9199999999996, "text": " samples of natural language.", "tokens": [10938, 295, 3303, 2856, 13], "temperature": 0.0, "avg_logprob": -0.17760247730073475, "compression_ratio": 1.5648854961832062, "no_speech_prob": 2.177530768676661e-05}, {"id": 843, "seek": 322972, "start": 3251.9199999999996, "end": 3254.7999999999997, "text": " So this is something that went around Twitter a lot, right.", "tokens": [407, 341, 307, 746, 300, 1437, 926, 5794, 257, 688, 11, 558, 13], "temperature": 0.0, "avg_logprob": -0.17760247730073475, "compression_ratio": 1.5648854961832062, "no_speech_prob": 2.177530768676661e-05}, {"id": 844, "seek": 325480, "start": 3254.8, "end": 3260.04, "text": " So you have this sort of contrived example that probably didn't show up in the training", "tokens": [407, 291, 362, 341, 1333, 295, 660, 470, 937, 1365, 300, 1391, 994, 380, 855, 493, 294, 264, 3097], "temperature": 0.0, "avg_logprob": -0.17391963534884983, "compression_ratio": 1.7255813953488373, "no_speech_prob": 1.260399312741356e-05}, {"id": 845, "seek": 325480, "start": 3260.04, "end": 3264.92, "text": " data that has a scientist discovering a herd of unicorns.", "tokens": [1412, 300, 575, 257, 12662, 24773, 257, 29484, 295, 28122, 82, 13], "temperature": 0.0, "avg_logprob": -0.17391963534884983, "compression_ratio": 1.7255813953488373, "no_speech_prob": 1.260399312741356e-05}, {"id": 846, "seek": 325480, "start": 3264.92, "end": 3271.6400000000003, "text": " And then they sort of sample from a, almost the distribution of the model.", "tokens": [400, 550, 436, 1333, 295, 6889, 490, 257, 11, 1920, 264, 7316, 295, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.17391963534884983, "compression_ratio": 1.7255813953488373, "no_speech_prob": 1.260399312741356e-05}, {"id": 847, "seek": 325480, "start": 3271.6400000000003, "end": 3276.96, "text": " They sort of give the model some extra credit here.", "tokens": [814, 1333, 295, 976, 264, 2316, 512, 2857, 5397, 510, 13], "temperature": 0.0, "avg_logprob": -0.17391963534884983, "compression_ratio": 1.7255813953488373, "no_speech_prob": 1.260399312741356e-05}, {"id": 848, "seek": 325480, "start": 3276.96, "end": 3282.2000000000003, "text": " They do something called truncating the distribution of the language models, sort of cut out noise", "tokens": [814, 360, 746, 1219, 504, 409, 66, 990, 264, 7316, 295, 264, 2856, 5245, 11, 1333, 295, 1723, 484, 5658], "temperature": 0.0, "avg_logprob": -0.17391963534884983, "compression_ratio": 1.7255813953488373, "no_speech_prob": 1.260399312741356e-05}, {"id": 849, "seek": 328220, "start": 3282.2, "end": 3284.6, "text": " at GPT 2.", "tokens": [412, 26039, 51, 568, 13], "temperature": 0.0, "avg_logprob": -0.20231428050031566, "compression_ratio": 1.725, "no_speech_prob": 2.282375680806581e-05}, {"id": 850, "seek": 328220, "start": 3284.6, "end": 3292.12, "text": " So it's not exactly a perfect sample, but more or less GPT 2 generated this.", "tokens": [407, 309, 311, 406, 2293, 257, 2176, 6889, 11, 457, 544, 420, 1570, 26039, 51, 568, 10833, 341, 13], "temperature": 0.0, "avg_logprob": -0.20231428050031566, "compression_ratio": 1.725, "no_speech_prob": 2.282375680806581e-05}, {"id": 851, "seek": 328220, "start": 3292.12, "end": 3296.2, "text": " And so you have the scientist discovering unicorns, and then, you know, you have this", "tokens": [400, 370, 291, 362, 264, 12662, 24773, 28122, 82, 11, 293, 550, 11, 291, 458, 11, 291, 362, 341], "temperature": 0.0, "avg_logprob": -0.20231428050031566, "compression_ratio": 1.725, "no_speech_prob": 2.282375680806581e-05}, {"id": 852, "seek": 328220, "start": 3296.2, "end": 3300.16, "text": " consistency, okay, there's the scientist.", "tokens": [14416, 11, 1392, 11, 456, 311, 264, 12662, 13], "temperature": 0.0, "avg_logprob": -0.20231428050031566, "compression_ratio": 1.725, "no_speech_prob": 2.282375680806581e-05}, {"id": 853, "seek": 328220, "start": 3300.16, "end": 3303.6, "text": " You know, you have them giving you the name.", "tokens": [509, 458, 11, 291, 362, 552, 2902, 291, 264, 1315, 13], "temperature": 0.0, "avg_logprob": -0.20231428050031566, "compression_ratio": 1.725, "no_speech_prob": 2.282375680806581e-05}, {"id": 854, "seek": 328220, "start": 3303.6, "end": 3311.2, "text": " You have, you refer back to this, well, yeah, you refer back to the scientist's name.", "tokens": [509, 362, 11, 291, 2864, 646, 281, 341, 11, 731, 11, 1338, 11, 291, 2864, 646, 281, 264, 12662, 311, 1315, 13], "temperature": 0.0, "avg_logprob": -0.20231428050031566, "compression_ratio": 1.725, "no_speech_prob": 2.282375680806581e-05}, {"id": 855, "seek": 331120, "start": 3311.2, "end": 3313.4399999999996, "text": " You sort of have these like topic consistency things.", "tokens": [509, 1333, 295, 362, 613, 411, 4829, 14416, 721, 13], "temperature": 0.0, "avg_logprob": -0.2401006632837756, "compression_ratio": 1.6334661354581674, "no_speech_prob": 9.309170854976401e-05}, {"id": 856, "seek": 331120, "start": 3313.4399999999996, "end": 3315.8399999999997, "text": " Also the syntax is really good.", "tokens": [2743, 264, 28431, 307, 534, 665, 13], "temperature": 0.0, "avg_logprob": -0.2401006632837756, "compression_ratio": 1.6334661354581674, "no_speech_prob": 9.309170854976401e-05}, {"id": 857, "seek": 331120, "start": 3315.8399999999997, "end": 3318.7999999999997, "text": " It looks, you know, vaguely like English.", "tokens": [467, 1542, 11, 291, 458, 11, 13501, 48863, 411, 3669, 13], "temperature": 0.0, "avg_logprob": -0.2401006632837756, "compression_ratio": 1.6334661354581674, "no_speech_prob": 9.309170854976401e-05}, {"id": 858, "seek": 331120, "start": 3318.7999999999997, "end": 3320.52, "text": " And so this is sort of continued to be a trend.", "tokens": [400, 370, 341, 307, 1333, 295, 7014, 281, 312, 257, 6028, 13], "temperature": 0.0, "avg_logprob": -0.2401006632837756, "compression_ratio": 1.6334661354581674, "no_speech_prob": 9.309170854976401e-05}, {"id": 859, "seek": 331120, "start": 3320.52, "end": 3323.8399999999997, "text": " As we get larger and larger language models, we actually sample from them, even when we", "tokens": [1018, 321, 483, 4833, 293, 4833, 2856, 5245, 11, 321, 767, 6889, 490, 552, 11, 754, 562, 321], "temperature": 0.0, "avg_logprob": -0.2401006632837756, "compression_ratio": 1.6334661354581674, "no_speech_prob": 9.309170854976401e-05}, {"id": 860, "seek": 331120, "start": 3323.8399999999997, "end": 3329.72, "text": " give them prompts that look sort of odd, and they seem to be increasingly convincing.", "tokens": [976, 552, 41095, 300, 574, 1333, 295, 7401, 11, 293, 436, 1643, 281, 312, 12980, 24823, 13], "temperature": 0.0, "avg_logprob": -0.2401006632837756, "compression_ratio": 1.6334661354581674, "no_speech_prob": 9.309170854976401e-05}, {"id": 861, "seek": 331120, "start": 3329.72, "end": 3331.64, "text": " Okay.", "tokens": [1033, 13], "temperature": 0.0, "avg_logprob": -0.2401006632837756, "compression_ratio": 1.6334661354581674, "no_speech_prob": 9.309170854976401e-05}, {"id": 862, "seek": 331120, "start": 3331.64, "end": 3336.72, "text": " So pre-training encoders, okay.", "tokens": [407, 659, 12, 17227, 1760, 2058, 378, 433, 11, 1392, 13], "temperature": 0.0, "avg_logprob": -0.2401006632837756, "compression_ratio": 1.6334661354581674, "no_speech_prob": 9.309170854976401e-05}, {"id": 863, "seek": 331120, "start": 3336.72, "end": 3337.72, "text": " Pre-training encoders.", "tokens": [6001, 12, 17227, 1760, 2058, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.2401006632837756, "compression_ratio": 1.6334661354581674, "no_speech_prob": 9.309170854976401e-05}, {"id": 864, "seek": 333772, "start": 3337.72, "end": 3342.16, "text": " So let's take another second because I need some more water here.", "tokens": [407, 718, 311, 747, 1071, 1150, 570, 286, 643, 512, 544, 1281, 510, 13], "temperature": 0.0, "avg_logprob": -0.18728242946576468, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.568775587657001e-05}, {"id": 865, "seek": 333772, "start": 3342.16, "end": 3344.72, "text": " If there's another question, let me know.", "tokens": [759, 456, 311, 1071, 1168, 11, 718, 385, 458, 13], "temperature": 0.0, "avg_logprob": -0.18728242946576468, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.568775587657001e-05}, {"id": 866, "seek": 333772, "start": 3344.72, "end": 3353.3999999999996, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.18728242946576468, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.568775587657001e-05}, {"id": 867, "seek": 333772, "start": 3353.3999999999996, "end": 3359.04, "text": " So the benefit of encoders that we talked about was that they get this bidirectional context.", "tokens": [407, 264, 5121, 295, 2058, 378, 433, 300, 321, 2825, 466, 390, 300, 436, 483, 341, 12957, 621, 41048, 4319, 13], "temperature": 0.0, "avg_logprob": -0.18728242946576468, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.568775587657001e-05}, {"id": 868, "seek": 333772, "start": 3359.04, "end": 3365.0, "text": " So you can, while you're building representations of your sentence, of your parts of sentences,", "tokens": [407, 291, 393, 11, 1339, 291, 434, 2390, 33358, 295, 428, 8174, 11, 295, 428, 3166, 295, 16579, 11], "temperature": 0.0, "avg_logprob": -0.18728242946576468, "compression_ratio": 1.5555555555555556, "no_speech_prob": 1.568775587657001e-05}, {"id": 869, "seek": 336500, "start": 3365.0, "end": 3368.84, "text": " you can look to the future and that can help you build a better representation of the word", "tokens": [291, 393, 574, 281, 264, 2027, 293, 300, 393, 854, 291, 1322, 257, 1101, 10290, 295, 264, 1349], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 870, "seek": 336500, "start": 3368.84, "end": 3370.36, "text": " that you're looking at right now.", "tokens": [300, 291, 434, 1237, 412, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 871, "seek": 336500, "start": 3370.36, "end": 3373.0, "text": " But the big problem is that we can't do language modeling now.", "tokens": [583, 264, 955, 1154, 307, 300, 321, 393, 380, 360, 2856, 15983, 586, 13], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 872, "seek": 336500, "start": 3373.0, "end": 3377.4, "text": " So we've pretty much only said, we like, we've relied on this task that we already knew about", "tokens": [407, 321, 600, 1238, 709, 787, 848, 11, 321, 411, 11, 321, 600, 35463, 322, 341, 5633, 300, 321, 1217, 2586, 466], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 873, "seek": 336500, "start": 3377.4, "end": 3379.56, "text": " language modeling to do our pre-training.", "tokens": [2856, 15983, 281, 360, 527, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 874, "seek": 336500, "start": 3379.56, "end": 3381.76, "text": " But now we want to pre-training coders.", "tokens": [583, 586, 321, 528, 281, 659, 12, 17227, 1760, 17656, 433, 13], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 875, "seek": 336500, "start": 3381.76, "end": 3383.76, "text": " And so we can't, we can't use it.", "tokens": [400, 370, 321, 393, 380, 11, 321, 393, 380, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 876, "seek": 336500, "start": 3383.76, "end": 3387.28, "text": " So what are we going to do?", "tokens": [407, 437, 366, 321, 516, 281, 360, 30], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 877, "seek": 336500, "start": 3387.28, "end": 3392.36, "text": " Here's the solution that was come up with a paper that introduced the language model of", "tokens": [1692, 311, 264, 3827, 300, 390, 808, 493, 365, 257, 3035, 300, 7268, 264, 2856, 2316, 295], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 878, "seek": 336500, "start": 3392.36, "end": 3394.92, "text": " the model called Bert.", "tokens": [264, 2316, 1219, 29594, 13], "temperature": 0.0, "avg_logprob": -0.16922066662762617, "compression_ratio": 1.8356164383561644, "no_speech_prob": 4.609199822880328e-05}, {"id": 879, "seek": 339492, "start": 3394.92, "end": 3397.04, "text": " It's called masked language modeling.", "tokens": [467, 311, 1219, 45249, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 880, "seek": 339492, "start": 3397.04, "end": 3399.52, "text": " So here's the idea.", "tokens": [407, 510, 311, 264, 1558, 13], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 881, "seek": 339492, "start": 3399.52, "end": 3403.88, "text": " We get the sentence and then we just take a fraction of the words and we replace them", "tokens": [492, 483, 264, 8174, 293, 550, 321, 445, 747, 257, 14135, 295, 264, 2283, 293, 321, 7406, 552], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 882, "seek": 339492, "start": 3403.88, "end": 3406.2400000000002, "text": " with a sort of a mask token.", "tokens": [365, 257, 1333, 295, 257, 6094, 14862, 13], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 883, "seek": 339492, "start": 3406.2400000000002, "end": 3410.76, "text": " A token that's, that means you don't know what this is right now.", "tokens": [316, 14862, 300, 311, 11, 300, 1355, 291, 500, 380, 458, 437, 341, 307, 558, 586, 13], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 884, "seek": 339492, "start": 3410.76, "end": 3413.0, "text": " And then you predict these words.", "tokens": [400, 550, 291, 6069, 613, 2283, 13], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 885, "seek": 339492, "start": 3413.0, "end": 3414.84, "text": " Some details we'll get into in the next slide.", "tokens": [2188, 4365, 321, 603, 483, 666, 294, 264, 958, 4137, 13], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 886, "seek": 339492, "start": 3414.84, "end": 3416.64, "text": " But so here's what it looks like.", "tokens": [583, 370, 510, 311, 437, 309, 1542, 411, 13], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 887, "seek": 339492, "start": 3416.64, "end": 3421.32, "text": " We have the sentence, I mask to the mask.", "tokens": [492, 362, 264, 8174, 11, 286, 6094, 281, 264, 6094, 13], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 888, "seek": 339492, "start": 3421.32, "end": 3423.2000000000003, "text": " We get some hidden states for all of them, right?", "tokens": [492, 483, 512, 7633, 4368, 337, 439, 295, 552, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2057517021894455, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.7034838896943256e-05}, {"id": 889, "seek": 342320, "start": 3423.2, "end": 3428.52, "text": " So we haven't changed the transformer encoder at all.", "tokens": [407, 321, 2378, 380, 3105, 264, 31782, 2058, 19866, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 890, "seek": 342320, "start": 3428.52, "end": 3430.8399999999997, "text": " We've just said, okay, here's like this sequence.", "tokens": [492, 600, 445, 848, 11, 1392, 11, 510, 311, 411, 341, 8310, 13], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 891, "seek": 342320, "start": 3430.8399999999997, "end": 3432.0, "text": " You get to see everything, right?", "tokens": [509, 483, 281, 536, 1203, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 892, "seek": 342320, "start": 3432.0, "end": 3433.8399999999997, "text": " Look at all the arrows going everywhere.", "tokens": [2053, 412, 439, 264, 19669, 516, 5315, 13], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 893, "seek": 342320, "start": 3433.8399999999997, "end": 3439.8399999999997, "text": " But then, right, we have this prediction layer that we're, that we're, that we're pre-training,", "tokens": [583, 550, 11, 558, 11, 321, 362, 341, 17630, 4583, 300, 321, 434, 11, 300, 321, 434, 11, 300, 321, 434, 659, 12, 17227, 1760, 11], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 894, "seek": 342320, "start": 3439.8399999999997, "end": 3440.8399999999997, "text": " right?", "tokens": [558, 30], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 895, "seek": 342320, "start": 3440.8399999999997, "end": 3441.8399999999997, "text": " And we're using it.", "tokens": [400, 321, 434, 1228, 309, 13], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 896, "seek": 342320, "start": 3441.8399999999997, "end": 3446.3599999999997, "text": " We only have loss on the words where we had masks here.", "tokens": [492, 787, 362, 4470, 322, 264, 2283, 689, 321, 632, 11830, 510, 13], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 897, "seek": 342320, "start": 3446.3599999999997, "end": 3451.16, "text": " So I had this masked and then I have to predict that it was went that went here and store", "tokens": [407, 286, 632, 341, 45249, 293, 550, 286, 362, 281, 6069, 300, 309, 390, 1437, 300, 1437, 510, 293, 3531], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 898, "seek": 342320, "start": 3451.16, "end": 3452.8399999999997, "text": " that went here.", "tokens": [300, 1437, 510, 13], "temperature": 0.0, "avg_logprob": -0.16871532041635087, "compression_ratio": 1.801556420233463, "no_speech_prob": 3.372937862877734e-05}, {"id": 899, "seek": 345284, "start": 3452.84, "end": 3456.2400000000002, "text": " And now this is a lot like language modeling you might say.", "tokens": [400, 586, 341, 307, 257, 688, 411, 2856, 15983, 291, 1062, 584, 13], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 900, "seek": 345284, "start": 3456.2400000000002, "end": 3459.7200000000003, "text": " But now you don't need to have this sort of left to right decomposition.", "tokens": [583, 586, 291, 500, 380, 643, 281, 362, 341, 1333, 295, 1411, 281, 558, 48356, 13], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 901, "seek": 345284, "start": 3459.7200000000003, "end": 3463.08, "text": " You're saying, I'm going to remove some of the words and you have to predict what they", "tokens": [509, 434, 1566, 11, 286, 478, 516, 281, 4159, 512, 295, 264, 2283, 293, 291, 362, 281, 6069, 437, 436], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 902, "seek": 345284, "start": 3463.08, "end": 3464.2400000000002, "text": " are.", "tokens": [366, 13], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 903, "seek": 345284, "start": 3464.2400000000002, "end": 3466.04, "text": " This is called masked language modeling.", "tokens": [639, 307, 1219, 45249, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 904, "seek": 345284, "start": 3466.04, "end": 3469.7200000000003, "text": " And it's been very, very, very effective with a quick caveat.", "tokens": [400, 309, 311, 668, 588, 11, 588, 11, 588, 4942, 365, 257, 1702, 43012, 13], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 905, "seek": 345284, "start": 3469.7200000000003, "end": 3471.36, "text": " It gets a little more complicated.", "tokens": [467, 2170, 257, 707, 544, 6179, 13], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 906, "seek": 345284, "start": 3471.36, "end": 3474.36, "text": " So, so what did they actually do?", "tokens": [407, 11, 370, 437, 630, 436, 767, 360, 30], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 907, "seek": 345284, "start": 3474.36, "end": 3476.56, "text": " They, they proposed masked language modeling.", "tokens": [814, 11, 436, 10348, 45249, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 908, "seek": 345284, "start": 3476.56, "end": 3479.4, "text": " And they released the weights of this, of this pre-trained transformer.", "tokens": [400, 436, 4736, 264, 17443, 295, 341, 11, 295, 341, 659, 12, 17227, 2001, 31782, 13], "temperature": 0.0, "avg_logprob": -0.20320502387152778, "compression_ratio": 1.7663230240549828, "no_speech_prob": 6.202574877534062e-05}, {"id": 909, "seek": 347940, "start": 3479.4, "end": 3483.6, "text": " So the little bit more complexity to get masked language modeling to work.", "tokens": [407, 264, 707, 857, 544, 14024, 281, 483, 45249, 2856, 15983, 281, 589, 13], "temperature": 0.0, "avg_logprob": -0.15031589605869392, "compression_ratio": 1.8200836820083681, "no_speech_prob": 2.7101208615931682e-05}, {"id": 910, "seek": 347940, "start": 3483.6, "end": 3489.32, "text": " So you are going to take a random 15% of the sub word tokens.", "tokens": [407, 291, 366, 516, 281, 747, 257, 4974, 2119, 4, 295, 264, 1422, 1349, 22667, 13], "temperature": 0.0, "avg_logprob": -0.15031589605869392, "compression_ratio": 1.8200836820083681, "no_speech_prob": 2.7101208615931682e-05}, {"id": 911, "seek": 347940, "start": 3489.32, "end": 3490.8, "text": " That was, that was true.", "tokens": [663, 390, 11, 300, 390, 2074, 13], "temperature": 0.0, "avg_logprob": -0.15031589605869392, "compression_ratio": 1.8200836820083681, "no_speech_prob": 2.7101208615931682e-05}, {"id": 912, "seek": 347940, "start": 3490.8, "end": 3494.44, "text": " But you're not always going to replace them with mask.", "tokens": [583, 291, 434, 406, 1009, 516, 281, 7406, 552, 365, 6094, 13], "temperature": 0.0, "avg_logprob": -0.15031589605869392, "compression_ratio": 1.8200836820083681, "no_speech_prob": 2.7101208615931682e-05}, {"id": 913, "seek": 347940, "start": 3494.44, "end": 3499.6, "text": " You can think of it like, if the model sees a mask token, it gets a guarantee that it", "tokens": [509, 393, 519, 295, 309, 411, 11, 498, 264, 2316, 8194, 257, 6094, 14862, 11, 309, 2170, 257, 10815, 300, 309], "temperature": 0.0, "avg_logprob": -0.15031589605869392, "compression_ratio": 1.8200836820083681, "no_speech_prob": 2.7101208615931682e-05}, {"id": 914, "seek": 347940, "start": 3499.6, "end": 3501.6800000000003, "text": " needs to predict something.", "tokens": [2203, 281, 6069, 746, 13], "temperature": 0.0, "avg_logprob": -0.15031589605869392, "compression_ratio": 1.8200836820083681, "no_speech_prob": 2.7101208615931682e-05}, {"id": 915, "seek": 347940, "start": 3501.6800000000003, "end": 3506.0, "text": " And if the model doesn't see a mask token, it gets a guarantee that it doesn't need to", "tokens": [400, 498, 264, 2316, 1177, 380, 536, 257, 6094, 14862, 11, 309, 2170, 257, 10815, 300, 309, 1177, 380, 643, 281], "temperature": 0.0, "avg_logprob": -0.15031589605869392, "compression_ratio": 1.8200836820083681, "no_speech_prob": 2.7101208615931682e-05}, {"id": 916, "seek": 347940, "start": 3506.0, "end": 3507.1600000000003, "text": " predict anything.", "tokens": [6069, 1340, 13], "temperature": 0.0, "avg_logprob": -0.15031589605869392, "compression_ratio": 1.8200836820083681, "no_speech_prob": 2.7101208615931682e-05}, {"id": 917, "seek": 350716, "start": 3507.16, "end": 3513.0, "text": " So why should it bother building strong representations of the words that aren't masked?", "tokens": [407, 983, 820, 309, 8677, 2390, 2068, 33358, 295, 264, 2283, 300, 3212, 380, 45249, 30], "temperature": 0.0, "avg_logprob": -0.12794305339004053, "compression_ratio": 1.8731884057971016, "no_speech_prob": 1.0449290130054578e-05}, {"id": 918, "seek": 350716, "start": 3513.0, "end": 3516.48, "text": " And I want my model to build strong representations of everything.", "tokens": [400, 286, 528, 452, 2316, 281, 1322, 2068, 33358, 295, 1203, 13], "temperature": 0.0, "avg_logprob": -0.12794305339004053, "compression_ratio": 1.8731884057971016, "no_speech_prob": 1.0449290130054578e-05}, {"id": 919, "seek": 350716, "start": 3516.48, "end": 3518.96, "text": " So we're going to add some sort of uncertainty to the model.", "tokens": [407, 321, 434, 516, 281, 909, 512, 1333, 295, 15697, 281, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.12794305339004053, "compression_ratio": 1.8731884057971016, "no_speech_prob": 1.0449290130054578e-05}, {"id": 920, "seek": 350716, "start": 3518.96, "end": 3523.0, "text": " So what we're going to do is, for those 15% of tokens, 80% of the time, we're going", "tokens": [407, 437, 321, 434, 516, 281, 360, 307, 11, 337, 729, 2119, 4, 295, 22667, 11, 4688, 4, 295, 264, 565, 11, 321, 434, 516], "temperature": 0.0, "avg_logprob": -0.12794305339004053, "compression_ratio": 1.8731884057971016, "no_speech_prob": 1.0449290130054578e-05}, {"id": 921, "seek": 350716, "start": 3523.0, "end": 3524.8399999999997, "text": " to replace it with a mask.", "tokens": [281, 7406, 309, 365, 257, 6094, 13], "temperature": 0.0, "avg_logprob": -0.12794305339004053, "compression_ratio": 1.8731884057971016, "no_speech_prob": 1.0449290130054578e-05}, {"id": 922, "seek": 350716, "start": 3524.8399999999997, "end": 3528.3999999999996, "text": " That was our original idea of mask language modeling.", "tokens": [663, 390, 527, 3380, 1558, 295, 6094, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.12794305339004053, "compression_ratio": 1.8731884057971016, "no_speech_prob": 1.0449290130054578e-05}, {"id": 923, "seek": 350716, "start": 3528.3999999999996, "end": 3532.6, "text": " Then 10% of the time, we're actually going to replace the word with just a random token.", "tokens": [1396, 1266, 4, 295, 264, 565, 11, 321, 434, 767, 516, 281, 7406, 264, 1349, 365, 445, 257, 4974, 14862, 13], "temperature": 0.0, "avg_logprob": -0.12794305339004053, "compression_ratio": 1.8731884057971016, "no_speech_prob": 1.0449290130054578e-05}, {"id": 924, "seek": 350716, "start": 3532.6, "end": 3536.3199999999997, "text": " Just a random vocabulary item can be anything.", "tokens": [1449, 257, 4974, 19864, 3174, 393, 312, 1340, 13], "temperature": 0.0, "avg_logprob": -0.12794305339004053, "compression_ratio": 1.8731884057971016, "no_speech_prob": 1.0449290130054578e-05}, {"id": 925, "seek": 353632, "start": 3536.32, "end": 3539.7200000000003, "text": " And then the other 10% of the time, we're going to leave the word unchanged.", "tokens": [400, 550, 264, 661, 1266, 4, 295, 264, 565, 11, 321, 434, 516, 281, 1856, 264, 1349, 44553, 13], "temperature": 0.0, "avg_logprob": -0.16020678629917381, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.765644113533199e-06}, {"id": 926, "seek": 353632, "start": 3539.7200000000003, "end": 3543.1200000000003, "text": " So now, it sees a word.", "tokens": [407, 586, 11, 309, 8194, 257, 1349, 13], "temperature": 0.0, "avg_logprob": -0.16020678629917381, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.765644113533199e-06}, {"id": 927, "seek": 353632, "start": 3543.1200000000003, "end": 3546.8, "text": " It could be a random token, or it could be unchanged.", "tokens": [467, 727, 312, 257, 4974, 14862, 11, 420, 309, 727, 312, 44553, 13], "temperature": 0.0, "avg_logprob": -0.16020678629917381, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.765644113533199e-06}, {"id": 928, "seek": 353632, "start": 3546.8, "end": 3550.92, "text": " And if I see a mask, I know I need to predict it.", "tokens": [400, 498, 286, 536, 257, 6094, 11, 286, 458, 286, 643, 281, 6069, 309, 13], "temperature": 0.0, "avg_logprob": -0.16020678629917381, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.765644113533199e-06}, {"id": 929, "seek": 353632, "start": 3550.92, "end": 3555.6400000000003, "text": " So what these two things do here is say, you have to sort of be doing this, you have to", "tokens": [407, 437, 613, 732, 721, 360, 510, 307, 584, 11, 291, 362, 281, 1333, 295, 312, 884, 341, 11, 291, 362, 281], "temperature": 0.0, "avg_logprob": -0.16020678629917381, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.765644113533199e-06}, {"id": 930, "seek": 353632, "start": 3555.6400000000003, "end": 3558.8, "text": " be on your toes for every word in your representation.", "tokens": [312, 322, 428, 14681, 337, 633, 1349, 294, 428, 10290, 13], "temperature": 0.0, "avg_logprob": -0.16020678629917381, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.765644113533199e-06}, {"id": 931, "seek": 353632, "start": 3558.8, "end": 3562.0, "text": " So here, I pizza to the mask.", "tokens": [407, 510, 11, 286, 8298, 281, 264, 6094, 13], "temperature": 0.0, "avg_logprob": -0.16020678629917381, "compression_ratio": 1.6391304347826088, "no_speech_prob": 7.765644113533199e-06}, {"id": 932, "seek": 356200, "start": 3562.0, "end": 3567.28, "text": " And it turns out, and the model didn't know this, but it's getting three lost terms for", "tokens": [400, 309, 4523, 484, 11, 293, 264, 2316, 994, 380, 458, 341, 11, 457, 309, 311, 1242, 1045, 2731, 2115, 337], "temperature": 0.0, "avg_logprob": -0.1772102242085471, "compression_ratio": 1.9683794466403162, "no_speech_prob": 1.34156152853393e-05}, {"id": 933, "seek": 356200, "start": 3567.28, "end": 3568.28, "text": " this sentence.", "tokens": [341, 8174, 13], "temperature": 0.0, "avg_logprob": -0.1772102242085471, "compression_ratio": 1.9683794466403162, "no_speech_prob": 1.34156152853393e-05}, {"id": 934, "seek": 356200, "start": 3568.28, "end": 3572.72, "text": " It only has one mask, but it's going to be penalized for predicting three different things.", "tokens": [467, 787, 575, 472, 6094, 11, 457, 309, 311, 516, 281, 312, 13661, 1602, 337, 32884, 1045, 819, 721, 13], "temperature": 0.0, "avg_logprob": -0.1772102242085471, "compression_ratio": 1.9683794466403162, "no_speech_prob": 1.34156152853393e-05}, {"id": 935, "seek": 356200, "start": 3572.72, "end": 3575.64, "text": " And it needs to predict that this word is actually went.", "tokens": [400, 309, 2203, 281, 6069, 300, 341, 1349, 307, 767, 1437, 13], "temperature": 0.0, "avg_logprob": -0.1772102242085471, "compression_ratio": 1.9683794466403162, "no_speech_prob": 1.34156152853393e-05}, {"id": 936, "seek": 356200, "start": 3575.64, "end": 3577.8, "text": " So I replaced this one.", "tokens": [407, 286, 10772, 341, 472, 13], "temperature": 0.0, "avg_logprob": -0.1772102242085471, "compression_ratio": 1.9683794466403162, "no_speech_prob": 1.34156152853393e-05}, {"id": 937, "seek": 356200, "start": 3577.8, "end": 3581.92, "text": " It needs to predict that this word is two, is in fact the word two.", "tokens": [467, 2203, 281, 6069, 300, 341, 1349, 307, 732, 11, 307, 294, 1186, 264, 1349, 732, 13], "temperature": 0.0, "avg_logprob": -0.1772102242085471, "compression_ratio": 1.9683794466403162, "no_speech_prob": 1.34156152853393e-05}, {"id": 938, "seek": 356200, "start": 3581.92, "end": 3586.48, "text": " And then it needs to predict that this word is in fact store.", "tokens": [400, 550, 309, 2203, 281, 6069, 300, 341, 1349, 307, 294, 1186, 3531, 13], "temperature": 0.0, "avg_logprob": -0.1772102242085471, "compression_ratio": 1.9683794466403162, "no_speech_prob": 1.34156152853393e-05}, {"id": 939, "seek": 356200, "start": 3586.48, "end": 3589.8, "text": " Now as a short interlude, you might be thinking, you might be thinking, John, there's no way", "tokens": [823, 382, 257, 2099, 728, 32334, 11, 291, 1062, 312, 1953, 11, 291, 1062, 312, 1953, 11, 2619, 11, 456, 311, 572, 636], "temperature": 0.0, "avg_logprob": -0.1772102242085471, "compression_ratio": 1.9683794466403162, "no_speech_prob": 1.34156152853393e-05}, {"id": 940, "seek": 358980, "start": 3589.8, "end": 3592.6000000000004, "text": " the model could know this.", "tokens": [264, 2316, 727, 458, 341, 13], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 941, "seek": 358980, "start": 3592.6000000000004, "end": 3594.4, "text": " It's so under specified.", "tokens": [467, 311, 370, 833, 22206, 13], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 942, "seek": 358980, "start": 3594.4, "end": 3596.48, "text": " I pizza is a little weird, I admit.", "tokens": [286, 8298, 307, 257, 707, 3657, 11, 286, 9796, 13], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 943, "seek": 358980, "start": 3596.48, "end": 3598.96, "text": " But there's just no way to know that this is store or in went into.", "tokens": [583, 456, 311, 445, 572, 636, 281, 458, 300, 341, 307, 3531, 420, 294, 1437, 666, 13], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 944, "seek": 358980, "start": 3598.96, "end": 3601.2000000000003, "text": " I mean, the same thing is true of language modeling.", "tokens": [286, 914, 11, 264, 912, 551, 307, 2074, 295, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 945, "seek": 358980, "start": 3601.2000000000003, "end": 3605.2000000000003, "text": " So it's going to end up learning these average statistics about what things tend to be in", "tokens": [407, 309, 311, 516, 281, 917, 493, 2539, 613, 4274, 12523, 466, 437, 721, 3928, 281, 312, 294], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 946, "seek": 358980, "start": 3605.2000000000003, "end": 3606.88, "text": " the given context.", "tokens": [264, 2212, 4319, 13], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 947, "seek": 358980, "start": 3606.88, "end": 3610.84, "text": " And it's going to sort of hedge its bets and try to build a distribution of what things", "tokens": [400, 309, 311, 516, 281, 1333, 295, 25304, 1080, 39922, 293, 853, 281, 1322, 257, 7316, 295, 437, 721], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 948, "seek": 358980, "start": 3610.84, "end": 3612.36, "text": " could appear there.", "tokens": [727, 4204, 456, 13], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 949, "seek": 358980, "start": 3612.36, "end": 3614.96, "text": " So for the people who are thinking that, if there wasn't, that's what you should be", "tokens": [407, 337, 264, 561, 567, 366, 1953, 300, 11, 498, 456, 2067, 380, 11, 300, 311, 437, 291, 820, 312], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 950, "seek": 358980, "start": 3614.96, "end": 3615.96, "text": " thinking.", "tokens": [1953, 13], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 951, "seek": 358980, "start": 3615.96, "end": 3618.96, "text": " It has to sort of know what kinds of things will end up in these slots.", "tokens": [467, 575, 281, 1333, 295, 458, 437, 3685, 295, 721, 486, 917, 493, 294, 613, 24266, 13], "temperature": 0.0, "avg_logprob": -0.22182803420546632, "compression_ratio": 1.8354037267080745, "no_speech_prob": 7.963524694787338e-05}, {"id": 952, "seek": 361896, "start": 3618.96, "end": 3623.96, "text": " It has other uncertainty, because it can't be sure that any of the other words are necessarily", "tokens": [467, 575, 661, 15697, 11, 570, 309, 393, 380, 312, 988, 300, 604, 295, 264, 661, 2283, 366, 4725], "temperature": 0.0, "avg_logprob": -0.20832453641024504, "compression_ratio": 1.73046875, "no_speech_prob": 4.538978464552201e-05}, {"id": 953, "seek": 361896, "start": 3623.96, "end": 3625.64, "text": " right.", "tokens": [558, 13], "temperature": 0.0, "avg_logprob": -0.20832453641024504, "compression_ratio": 1.73046875, "no_speech_prob": 4.538978464552201e-05}, {"id": 954, "seek": 361896, "start": 3625.64, "end": 3630.44, "text": " And then it is, it's predicting these three words.", "tokens": [400, 550, 309, 307, 11, 309, 311, 32884, 613, 1045, 2283, 13], "temperature": 0.0, "avg_logprob": -0.20832453641024504, "compression_ratio": 1.73046875, "no_speech_prob": 4.538978464552201e-05}, {"id": 955, "seek": 361896, "start": 3630.44, "end": 3636.84, "text": " And so you can see why it's important to not just have masks potentially, to have these", "tokens": [400, 370, 291, 393, 536, 983, 309, 311, 1021, 281, 406, 445, 362, 11830, 7263, 11, 281, 362, 613], "temperature": 0.0, "avg_logprob": -0.20832453641024504, "compression_ratio": 1.73046875, "no_speech_prob": 4.538978464552201e-05}, {"id": 956, "seek": 361896, "start": 3636.84, "end": 3641.2, "text": " sort of token randomization things, because again, we don't actually care about its ability", "tokens": [1333, 295, 14862, 4974, 2144, 721, 11, 570, 797, 11, 321, 500, 380, 767, 1127, 466, 1080, 3485], "temperature": 0.0, "avg_logprob": -0.20832453641024504, "compression_ratio": 1.73046875, "no_speech_prob": 4.538978464552201e-05}, {"id": 957, "seek": 361896, "start": 3641.2, "end": 3643.2400000000002, "text": " to predict the masks.", "tokens": [281, 6069, 264, 11830, 13], "temperature": 0.0, "avg_logprob": -0.20832453641024504, "compression_ratio": 1.73046875, "no_speech_prob": 4.538978464552201e-05}, {"id": 958, "seek": 361896, "start": 3643.2400000000002, "end": 3648.16, "text": " I'm not going to usually, I'm not going to actually sample from the model's distribution", "tokens": [286, 478, 406, 516, 281, 2673, 11, 286, 478, 406, 516, 281, 767, 6889, 490, 264, 2316, 311, 7316], "temperature": 0.0, "avg_logprob": -0.20832453641024504, "compression_ratio": 1.73046875, "no_speech_prob": 4.538978464552201e-05}, {"id": 959, "seek": 364816, "start": 3648.16, "end": 3650.92, "text": " over what should go here.", "tokens": [670, 437, 820, 352, 510, 13], "temperature": 0.0, "avg_logprob": -0.17878762731012307, "compression_ratio": 1.604, "no_speech_prob": 6.010391007293947e-05}, {"id": 960, "seek": 364816, "start": 3650.92, "end": 3656.56, "text": " Instead, I am going to use the parameters of the neural network and expect that it built", "tokens": [7156, 11, 286, 669, 516, 281, 764, 264, 9834, 295, 264, 18161, 3209, 293, 2066, 300, 309, 3094], "temperature": 0.0, "avg_logprob": -0.17878762731012307, "compression_ratio": 1.604, "no_speech_prob": 6.010391007293947e-05}, {"id": 961, "seek": 364816, "start": 3656.56, "end": 3658.68, "text": " strong representations of language.", "tokens": [2068, 33358, 295, 2856, 13], "temperature": 0.0, "avg_logprob": -0.17878762731012307, "compression_ratio": 1.604, "no_speech_prob": 6.010391007293947e-05}, {"id": 962, "seek": 364816, "start": 3658.68, "end": 3662.44, "text": " So I don't want it to think it's got a free pass for representing something if it doesn't", "tokens": [407, 286, 500, 380, 528, 309, 281, 519, 309, 311, 658, 257, 1737, 1320, 337, 13460, 746, 498, 309, 1177, 380], "temperature": 0.0, "avg_logprob": -0.17878762731012307, "compression_ratio": 1.604, "no_speech_prob": 6.010391007293947e-05}, {"id": 963, "seek": 364816, "start": 3662.44, "end": 3666.2799999999997, "text": " have a mask there.", "tokens": [362, 257, 6094, 456, 13], "temperature": 0.0, "avg_logprob": -0.17878762731012307, "compression_ratio": 1.604, "no_speech_prob": 6.010391007293947e-05}, {"id": 964, "seek": 364816, "start": 3666.2799999999997, "end": 3674.92, "text": " So there was one extra thing with the BERT pre-training, which is a next sentence prediction", "tokens": [407, 456, 390, 472, 2857, 551, 365, 264, 363, 31479, 659, 12, 17227, 1760, 11, 597, 307, 257, 958, 8174, 17630], "temperature": 0.0, "avg_logprob": -0.17878762731012307, "compression_ratio": 1.604, "no_speech_prob": 6.010391007293947e-05}, {"id": 965, "seek": 364816, "start": 3674.92, "end": 3675.92, "text": " objective.", "tokens": [10024, 13], "temperature": 0.0, "avg_logprob": -0.17878762731012307, "compression_ratio": 1.604, "no_speech_prob": 6.010391007293947e-05}, {"id": 966, "seek": 364816, "start": 3675.92, "end": 3677.48, "text": " So the input to BERT looks like this.", "tokens": [407, 264, 4846, 281, 363, 31479, 1542, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.17878762731012307, "compression_ratio": 1.604, "no_speech_prob": 6.010391007293947e-05}, {"id": 967, "seek": 367748, "start": 3677.48, "end": 3679.6, "text": " This is straight from the BERT paper.", "tokens": [639, 307, 2997, 490, 264, 363, 31479, 3035, 13], "temperature": 0.0, "avg_logprob": -0.22572198903785562, "compression_ratio": 1.6860986547085202, "no_speech_prob": 5.647425496135838e-05}, {"id": 968, "seek": 367748, "start": 3679.6, "end": 3684.92, "text": " You have a label here before your first sentence, and then a separation, and then a second", "tokens": [509, 362, 257, 7645, 510, 949, 428, 700, 8174, 11, 293, 550, 257, 14634, 11, 293, 550, 257, 1150], "temperature": 0.0, "avg_logprob": -0.22572198903785562, "compression_ratio": 1.6860986547085202, "no_speech_prob": 5.647425496135838e-05}, {"id": 969, "seek": 367748, "start": 3684.92, "end": 3685.92, "text": " sentence.", "tokens": [8174, 13], "temperature": 0.0, "avg_logprob": -0.22572198903785562, "compression_ratio": 1.6860986547085202, "no_speech_prob": 5.647425496135838e-05}, {"id": 970, "seek": 367748, "start": 3685.92, "end": 3689.88, "text": " So you had always two contiguous chunks of text.", "tokens": [407, 291, 632, 1009, 732, 660, 30525, 24004, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.22572198903785562, "compression_ratio": 1.6860986547085202, "no_speech_prob": 5.647425496135838e-05}, {"id": 971, "seek": 367748, "start": 3689.88, "end": 3691.72, "text": " You had a first chunk of text here.", "tokens": [509, 632, 257, 700, 16635, 295, 2487, 510, 13], "temperature": 0.0, "avg_logprob": -0.22572198903785562, "compression_ratio": 1.6860986547085202, "no_speech_prob": 5.647425496135838e-05}, {"id": 972, "seek": 367748, "start": 3691.72, "end": 3693.44, "text": " My dog is cute.", "tokens": [1222, 3000, 307, 4052, 13], "temperature": 0.0, "avg_logprob": -0.22572198903785562, "compression_ratio": 1.6860986547085202, "no_speech_prob": 5.647425496135838e-05}, {"id": 973, "seek": 367748, "start": 3693.44, "end": 3695.72, "text": " And then a second chunk of text, he likes playing.", "tokens": [400, 550, 257, 1150, 16635, 295, 2487, 11, 415, 5902, 2433, 13], "temperature": 0.0, "avg_logprob": -0.22572198903785562, "compression_ratio": 1.6860986547085202, "no_speech_prob": 5.647425496135838e-05}, {"id": 974, "seek": 367748, "start": 3695.72, "end": 3698.12, "text": " You can see the sub words there.", "tokens": [509, 393, 536, 264, 1422, 2283, 456, 13], "temperature": 0.0, "avg_logprob": -0.22572198903785562, "compression_ratio": 1.6860986547085202, "no_speech_prob": 5.647425496135838e-05}, {"id": 975, "seek": 367748, "start": 3698.12, "end": 3702.4, "text": " And now these would actually be both be much longer.", "tokens": [400, 586, 613, 576, 767, 312, 1293, 312, 709, 2854, 13], "temperature": 0.0, "avg_logprob": -0.22572198903785562, "compression_ratio": 1.6860986547085202, "no_speech_prob": 5.647425496135838e-05}, {"id": 976, "seek": 370240, "start": 3702.4, "end": 3707.32, "text": " So these whole thing would be 512 words, and it would be about half, and that would be", "tokens": [407, 613, 1379, 551, 576, 312, 1025, 4762, 2283, 11, 293, 309, 576, 312, 466, 1922, 11, 293, 300, 576, 312], "temperature": 0.0, "avg_logprob": -0.16377023170734273, "compression_ratio": 1.8045112781954886, "no_speech_prob": 1.8625167285790667e-05}, {"id": 977, "seek": 370240, "start": 3707.32, "end": 3711.88, "text": " about half, and they'd be contiguous chunks of text.", "tokens": [466, 1922, 11, 293, 436, 1116, 312, 660, 30525, 24004, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.16377023170734273, "compression_ratio": 1.8045112781954886, "no_speech_prob": 1.8625167285790667e-05}, {"id": 978, "seek": 370240, "start": 3711.88, "end": 3713.0, "text": " But here was the deal.", "tokens": [583, 510, 390, 264, 2028, 13], "temperature": 0.0, "avg_logprob": -0.16377023170734273, "compression_ratio": 1.8045112781954886, "no_speech_prob": 1.8625167285790667e-05}, {"id": 979, "seek": 370240, "start": 3713.0, "end": 3717.96, "text": " What they wanted to do was they wanted to try to teach the system to understand sort of", "tokens": [708, 436, 1415, 281, 360, 390, 436, 1415, 281, 853, 281, 2924, 264, 1185, 281, 1223, 1333, 295], "temperature": 0.0, "avg_logprob": -0.16377023170734273, "compression_ratio": 1.8045112781954886, "no_speech_prob": 1.8625167285790667e-05}, {"id": 980, "seek": 370240, "start": 3717.96, "end": 3721.8, "text": " relationships between different whole pieces of text.", "tokens": [6159, 1296, 819, 1379, 3755, 295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.16377023170734273, "compression_ratio": 1.8045112781954886, "no_speech_prob": 1.8625167285790667e-05}, {"id": 981, "seek": 370240, "start": 3721.8, "end": 3726.88, "text": " In order to better pre-trained for downstream applications like question answering, where", "tokens": [682, 1668, 281, 1101, 659, 12, 17227, 2001, 337, 30621, 5821, 411, 1168, 13430, 11, 689], "temperature": 0.0, "avg_logprob": -0.16377023170734273, "compression_ratio": 1.8045112781954886, "no_speech_prob": 1.8625167285790667e-05}, {"id": 982, "seek": 370240, "start": 3726.88, "end": 3731.48, "text": " you have two pretty different pieces of text, and you need to know how they relate to", "tokens": [291, 362, 732, 1238, 819, 3755, 295, 2487, 11, 293, 291, 643, 281, 458, 577, 436, 10961, 281], "temperature": 0.0, "avg_logprob": -0.16377023170734273, "compression_ratio": 1.8045112781954886, "no_speech_prob": 1.8625167285790667e-05}, {"id": 983, "seek": 373148, "start": 3731.48, "end": 3732.48, "text": " each other.", "tokens": [1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.14303861879834942, "compression_ratio": 1.8487394957983194, "no_speech_prob": 2.3177681214292534e-05}, {"id": 984, "seek": 373148, "start": 3732.48, "end": 3738.64, "text": " So the objective they came up with was you should sometimes have the second chunk of text", "tokens": [407, 264, 10024, 436, 1361, 493, 365, 390, 291, 820, 2171, 362, 264, 1150, 16635, 295, 2487], "temperature": 0.0, "avg_logprob": -0.14303861879834942, "compression_ratio": 1.8487394957983194, "no_speech_prob": 2.3177681214292534e-05}, {"id": 985, "seek": 373148, "start": 3738.64, "end": 3746.92, "text": " be the actual chunk of text that directly follows the first in your data set, and sometimes", "tokens": [312, 264, 3539, 16635, 295, 2487, 300, 3838, 10002, 264, 700, 294, 428, 1412, 992, 11, 293, 2171], "temperature": 0.0, "avg_logprob": -0.14303861879834942, "compression_ratio": 1.8487394957983194, "no_speech_prob": 2.3177681214292534e-05}, {"id": 986, "seek": 373148, "start": 3746.92, "end": 3752.92, "text": " have the second chunk of text be randomly sampled from somewhere else, so unrelated.", "tokens": [362, 264, 1150, 16635, 295, 2487, 312, 16979, 3247, 15551, 490, 4079, 1646, 11, 370, 38967, 13], "temperature": 0.0, "avg_logprob": -0.14303861879834942, "compression_ratio": 1.8487394957983194, "no_speech_prob": 2.3177681214292534e-05}, {"id": 987, "seek": 373148, "start": 3752.92, "end": 3757.28, "text": " And the model should predict whether it's the first case or the second.", "tokens": [400, 264, 2316, 820, 6069, 1968, 309, 311, 264, 700, 1389, 420, 264, 1150, 13], "temperature": 0.0, "avg_logprob": -0.14303861879834942, "compression_ratio": 1.8487394957983194, "no_speech_prob": 2.3177681214292534e-05}, {"id": 988, "seek": 373148, "start": 3757.28, "end": 3761.0, "text": " In order, again, to sort of have to reason about the relationships between the two chunks", "tokens": [682, 1668, 11, 797, 11, 281, 1333, 295, 362, 281, 1778, 466, 264, 6159, 1296, 264, 732, 24004], "temperature": 0.0, "avg_logprob": -0.14303861879834942, "compression_ratio": 1.8487394957983194, "no_speech_prob": 2.3177681214292534e-05}, {"id": 989, "seek": 376100, "start": 3761.0, "end": 3762.48, "text": " of text.", "tokens": [295, 2487, 13], "temperature": 0.0, "avg_logprob": -0.1996801075182463, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7102465537609532e-05}, {"id": 990, "seek": 376100, "start": 3762.48, "end": 3764.48, "text": " So this is next sentence prediction.", "tokens": [407, 341, 307, 958, 8174, 17630, 13], "temperature": 0.0, "avg_logprob": -0.1996801075182463, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7102465537609532e-05}, {"id": 991, "seek": 376100, "start": 3764.48, "end": 3768.68, "text": " I think it's important to think about because it's a very different idea of pre-training", "tokens": [286, 519, 309, 311, 1021, 281, 519, 466, 570, 309, 311, 257, 588, 819, 1558, 295, 659, 12, 17227, 1760], "temperature": 0.0, "avg_logprob": -0.1996801075182463, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7102465537609532e-05}, {"id": 992, "seek": 376100, "start": 3768.68, "end": 3773.88, "text": " objective than language modeling and masked language modeling.", "tokens": [10024, 813, 2856, 15983, 293, 45249, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.1996801075182463, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7102465537609532e-05}, {"id": 993, "seek": 376100, "start": 3773.88, "end": 3778.04, "text": " Even though later we're sort of argued that in the case of BERT, it's not necessary or", "tokens": [2754, 1673, 1780, 321, 434, 1333, 295, 20219, 300, 294, 264, 1389, 295, 363, 31479, 11, 309, 311, 406, 4818, 420], "temperature": 0.0, "avg_logprob": -0.1996801075182463, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7102465537609532e-05}, {"id": 994, "seek": 376100, "start": 3778.04, "end": 3779.96, "text": " useful.", "tokens": [4420, 13], "temperature": 0.0, "avg_logprob": -0.1996801075182463, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7102465537609532e-05}, {"id": 995, "seek": 376100, "start": 3779.96, "end": 3786.28, "text": " And one of the arguments is actually because it's actually way better to have a single", "tokens": [400, 472, 295, 264, 12869, 307, 767, 570, 309, 311, 767, 636, 1101, 281, 362, 257, 2167], "temperature": 0.0, "avg_logprob": -0.1996801075182463, "compression_ratio": 1.6266094420600858, "no_speech_prob": 2.7102465537609532e-05}, {"id": 996, "seek": 378628, "start": 3786.28, "end": 3792.1200000000003, "text": " context that's twice as long, so you can learn even longer distance dependencies and things.", "tokens": [4319, 300, 311, 6091, 382, 938, 11, 370, 291, 393, 1466, 754, 2854, 4560, 36606, 293, 721, 13], "temperature": 0.0, "avg_logprob": -0.16703106806828424, "compression_ratio": 1.6805111821086263, "no_speech_prob": 2.884310197259765e-05}, {"id": 997, "seek": 378628, "start": 3792.1200000000003, "end": 3795.84, "text": " And so whether the objective itself would be useful if you could always just double", "tokens": [400, 370, 1968, 264, 10024, 2564, 576, 312, 4420, 498, 291, 727, 1009, 445, 3834], "temperature": 0.0, "avg_logprob": -0.16703106806828424, "compression_ratio": 1.6805111821086263, "no_speech_prob": 2.884310197259765e-05}, {"id": 998, "seek": 378628, "start": 3795.84, "end": 3798.84, "text": " the context size, I'm not sure if anyone's done research on that.", "tokens": [264, 4319, 2744, 11, 286, 478, 406, 988, 498, 2878, 311, 1096, 2132, 322, 300, 13], "temperature": 0.0, "avg_logprob": -0.16703106806828424, "compression_ratio": 1.6805111821086263, "no_speech_prob": 2.884310197259765e-05}, {"id": 999, "seek": 378628, "start": 3798.84, "end": 3802.76, "text": " But again, it's like a different kind of objective, and it's still noisy something about", "tokens": [583, 797, 11, 309, 311, 411, 257, 819, 733, 295, 10024, 11, 293, 309, 311, 920, 24518, 746, 466], "temperature": 0.0, "avg_logprob": -0.16703106806828424, "compression_ratio": 1.6805111821086263, "no_speech_prob": 2.884310197259765e-05}, {"id": 1000, "seek": 378628, "start": 3802.76, "end": 3803.76, "text": " the input, right?", "tokens": [264, 4846, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.16703106806828424, "compression_ratio": 1.6805111821086263, "no_speech_prob": 2.884310197259765e-05}, {"id": 1001, "seek": 378628, "start": 3803.76, "end": 3808.7200000000003, "text": " The input was this big chunk of text, and you've noise it to say like, now you don't know", "tokens": [440, 4846, 390, 341, 955, 16635, 295, 2487, 11, 293, 291, 600, 5658, 309, 281, 584, 411, 11, 586, 291, 500, 380, 458], "temperature": 0.0, "avg_logprob": -0.16703106806828424, "compression_ratio": 1.6805111821086263, "no_speech_prob": 2.884310197259765e-05}, {"id": 1002, "seek": 378628, "start": 3808.7200000000003, "end": 3812.96, "text": " whether it really was that or whether you sort of replaced it with a bunch of garbage,", "tokens": [1968, 309, 534, 390, 300, 420, 1968, 291, 1333, 295, 10772, 309, 365, 257, 3840, 295, 14150, 11], "temperature": 0.0, "avg_logprob": -0.16703106806828424, "compression_ratio": 1.6805111821086263, "no_speech_prob": 2.884310197259765e-05}, {"id": 1003, "seek": 381296, "start": 3812.96, "end": 3819.96, "text": " this sort of second portion here, whether the second portion has been replaced with something", "tokens": [341, 1333, 295, 1150, 8044, 510, 11, 1968, 264, 1150, 8044, 575, 668, 10772, 365, 746], "temperature": 0.0, "avg_logprob": -0.22581424353257665, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.822002690867521e-05}, {"id": 1004, "seek": 381296, "start": 3819.96, "end": 3824.44, "text": " that didn't actually come from the same sequence.", "tokens": [300, 994, 380, 767, 808, 490, 264, 912, 8310, 13], "temperature": 0.0, "avg_logprob": -0.22581424353257665, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.822002690867521e-05}, {"id": 1005, "seek": 381296, "start": 3824.44, "end": 3829.44, "text": " Okay, so let's talk some details about BERT.", "tokens": [1033, 11, 370, 718, 311, 751, 512, 4365, 466, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.22581424353257665, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.822002690867521e-05}, {"id": 1006, "seek": 381296, "start": 3829.44, "end": 3833.52, "text": " So BERT had 12 or 24 layers, depending on BERT base or BERT large.", "tokens": [407, 363, 31479, 632, 2272, 420, 4022, 7914, 11, 5413, 322, 363, 31479, 3096, 420, 363, 31479, 2416, 13], "temperature": 0.0, "avg_logprob": -0.22581424353257665, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.822002690867521e-05}, {"id": 1007, "seek": 381296, "start": 3833.52, "end": 3837.08, "text": " You'll probably use one of these models or one of the sort of descendants of these models", "tokens": [509, 603, 1391, 764, 472, 295, 613, 5245, 420, 472, 295, 264, 1333, 295, 31693, 295, 613, 5245], "temperature": 0.0, "avg_logprob": -0.22581424353257665, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.822002690867521e-05}, {"id": 1008, "seek": 381296, "start": 3837.08, "end": 3842.2400000000002, "text": " if you choose to do something with the custom final project potentially, or if you choose", "tokens": [498, 291, 2826, 281, 360, 746, 365, 264, 2375, 2572, 1716, 7263, 11, 420, 498, 291, 2826], "temperature": 0.0, "avg_logprob": -0.22581424353257665, "compression_ratio": 1.7058823529411764, "no_speech_prob": 3.822002690867521e-05}, {"id": 1009, "seek": 384224, "start": 3842.24, "end": 3846.3199999999997, "text": " the version of the default final project.", "tokens": [264, 3037, 295, 264, 7576, 2572, 1716, 13], "temperature": 0.0, "avg_logprob": -0.23714916522686297, "compression_ratio": 1.7237354085603114, "no_speech_prob": 6.92073444952257e-05}, {"id": 1010, "seek": 384224, "start": 3846.3199999999997, "end": 3851.4799999999996, "text": " And you had a 600 or a 1000 dimension hidden states, a bunch of attention heads, so this", "tokens": [400, 291, 632, 257, 11849, 420, 257, 9714, 10139, 7633, 4368, 11, 257, 3840, 295, 3202, 8050, 11, 370, 341], "temperature": 0.0, "avg_logprob": -0.23714916522686297, "compression_ratio": 1.7237354085603114, "no_speech_prob": 6.92073444952257e-05}, {"id": 1011, "seek": 384224, "start": 3851.4799999999996, "end": 3854.7599999999998, "text": " is that multi-headed attention, remember, about a bunch of them.", "tokens": [307, 300, 4825, 12, 28409, 3202, 11, 1604, 11, 466, 257, 3840, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.23714916522686297, "compression_ratio": 1.7237354085603114, "no_speech_prob": 6.92073444952257e-05}, {"id": 1012, "seek": 384224, "start": 3854.7599999999998, "end": 3859.6, "text": " So you're splitting all your dimensions into those 16 heads, and we're talking on the", "tokens": [407, 291, 434, 30348, 439, 428, 12819, 666, 729, 3165, 8050, 11, 293, 321, 434, 1417, 322, 264], "temperature": 0.0, "avg_logprob": -0.23714916522686297, "compression_ratio": 1.7237354085603114, "no_speech_prob": 6.92073444952257e-05}, {"id": 1013, "seek": 384224, "start": 3859.6, "end": 3863.2, "text": " order of a couple hundred million parameters.", "tokens": [1668, 295, 257, 1916, 3262, 2459, 9834, 13], "temperature": 0.0, "avg_logprob": -0.23714916522686297, "compression_ratio": 1.7237354085603114, "no_speech_prob": 6.92073444952257e-05}, {"id": 1014, "seek": 384224, "start": 3863.2, "end": 3868.56, "text": " At the time, right in 2018, we were like, whoa, that's a lot of parameters.", "tokens": [1711, 264, 565, 11, 558, 294, 6096, 11, 321, 645, 411, 11, 13310, 11, 300, 311, 257, 688, 295, 9834, 13], "temperature": 0.0, "avg_logprob": -0.23714916522686297, "compression_ratio": 1.7237354085603114, "no_speech_prob": 6.92073444952257e-05}, {"id": 1015, "seek": 384224, "start": 3868.56, "end": 3872.04, "text": " How do you, that's a lot of parameters.", "tokens": [1012, 360, 291, 11, 300, 311, 257, 688, 295, 9834, 13], "temperature": 0.0, "avg_logprob": -0.23714916522686297, "compression_ratio": 1.7237354085603114, "no_speech_prob": 6.92073444952257e-05}, {"id": 1016, "seek": 387204, "start": 3872.04, "end": 3875.12, "text": " And now, models are way, way, way, way bigger.", "tokens": [400, 586, 11, 5245, 366, 636, 11, 636, 11, 636, 11, 636, 3801, 13], "temperature": 0.0, "avg_logprob": -0.21971918931647913, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.821270365733653e-05}, {"id": 1017, "seek": 387204, "start": 3875.12, "end": 3879.12, "text": " So let's keep track of sort of the model sizes as we're going through this.", "tokens": [407, 718, 311, 1066, 2837, 295, 1333, 295, 264, 2316, 11602, 382, 321, 434, 516, 807, 341, 13], "temperature": 0.0, "avg_logprob": -0.21971918931647913, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.821270365733653e-05}, {"id": 1018, "seek": 387204, "start": 3879.12, "end": 3882.12, "text": " And let's come back now to the corpus sizes as well.", "tokens": [400, 718, 311, 808, 646, 586, 281, 264, 1181, 31624, 11602, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.21971918931647913, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.821270365733653e-05}, {"id": 1019, "seek": 387204, "start": 3882.12, "end": 3883.52, "text": " So we have books corpus.", "tokens": [407, 321, 362, 3642, 1181, 31624, 13], "temperature": 0.0, "avg_logprob": -0.21971918931647913, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.821270365733653e-05}, {"id": 1020, "seek": 387204, "start": 3883.52, "end": 3885.2, "text": " And this is the number of words there.", "tokens": [400, 341, 307, 264, 1230, 295, 2283, 456, 13], "temperature": 0.0, "avg_logprob": -0.21971918931647913, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.821270365733653e-05}, {"id": 1021, "seek": 387204, "start": 3885.2, "end": 3890.2, "text": " This is the same thing that GPT-1 was trained on, 800 million words.", "tokens": [639, 307, 264, 912, 551, 300, 26039, 51, 12, 16, 390, 8895, 322, 11, 13083, 2459, 2283, 13], "temperature": 0.0, "avg_logprob": -0.21971918931647913, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.821270365733653e-05}, {"id": 1022, "seek": 387204, "start": 3890.2, "end": 3896.84, "text": " Now we're going to train on also English Wikipedia, it's 250, sorry, that's 2,500 million,", "tokens": [823, 321, 434, 516, 281, 3847, 322, 611, 3669, 28999, 11, 309, 311, 11650, 11, 2597, 11, 300, 311, 568, 11, 7526, 2459, 11], "temperature": 0.0, "avg_logprob": -0.21971918931647913, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.821270365733653e-05}, {"id": 1023, "seek": 387204, "start": 3896.84, "end": 3899.88, "text": " so that's 2,500,000,000 words.", "tokens": [370, 300, 311, 568, 11, 7526, 11, 1360, 11, 1360, 2283, 13], "temperature": 0.0, "avg_logprob": -0.21971918931647913, "compression_ratio": 1.7269076305220883, "no_speech_prob": 3.821270365733653e-05}, {"id": 1024, "seek": 389988, "start": 3899.88, "end": 3906.36, "text": " And again, to give you an idea of what is done in practice, right, pre-training is expensive", "tokens": [400, 797, 11, 281, 976, 291, 364, 1558, 295, 437, 307, 1096, 294, 3124, 11, 558, 11, 659, 12, 17227, 1760, 307, 5124], "temperature": 0.0, "avg_logprob": -0.13453326968971743, "compression_ratio": 1.5625, "no_speech_prob": 0.00011951282067457214}, {"id": 1025, "seek": 389988, "start": 3906.36, "end": 3911.08, "text": " and impractical for most users, let's say.", "tokens": [293, 704, 1897, 804, 337, 881, 5022, 11, 718, 311, 584, 13], "temperature": 0.0, "avg_logprob": -0.13453326968971743, "compression_ratio": 1.5625, "no_speech_prob": 0.00011951282067457214}, {"id": 1026, "seek": 389988, "start": 3911.08, "end": 3916.8, "text": " So if you are a researcher with a GPU or five GPUs or something like that, you tend to", "tokens": [407, 498, 291, 366, 257, 21751, 365, 257, 18407, 420, 1732, 18407, 82, 420, 746, 411, 300, 11, 291, 3928, 281], "temperature": 0.0, "avg_logprob": -0.13453326968971743, "compression_ratio": 1.5625, "no_speech_prob": 0.00011951282067457214}, {"id": 1027, "seek": 389988, "start": 3916.8, "end": 3920.48, "text": " not really be pre-training your whole own BERT model unless you're willing to spend", "tokens": [406, 534, 312, 659, 12, 17227, 1760, 428, 1379, 1065, 363, 31479, 2316, 5969, 291, 434, 4950, 281, 3496], "temperature": 0.0, "avg_logprob": -0.13453326968971743, "compression_ratio": 1.5625, "no_speech_prob": 0.00011951282067457214}, {"id": 1028, "seek": 389988, "start": 3920.48, "end": 3922.48, "text": " a long time doing it.", "tokens": [257, 938, 565, 884, 309, 13], "temperature": 0.0, "avg_logprob": -0.13453326968971743, "compression_ratio": 1.5625, "no_speech_prob": 0.00011951282067457214}, {"id": 1029, "seek": 389988, "start": 3922.48, "end": 3925.76, "text": " BERT itself was pre-trained with 64 TPU chips.", "tokens": [363, 31479, 2564, 390, 659, 12, 17227, 2001, 365, 12145, 314, 8115, 11583, 13], "temperature": 0.0, "avg_logprob": -0.13453326968971743, "compression_ratio": 1.5625, "no_speech_prob": 0.00011951282067457214}, {"id": 1030, "seek": 392576, "start": 3925.76, "end": 3931.84, "text": " A TPU is a special kind of hardware accelerator that accelerates the tensor operations effectively", "tokens": [316, 314, 8115, 307, 257, 2121, 733, 295, 8837, 39889, 300, 10172, 1024, 264, 40863, 7705, 8659], "temperature": 0.0, "avg_logprob": -0.14902143963312697, "compression_ratio": 1.609375, "no_speech_prob": 1.5932599126244895e-05}, {"id": 1031, "seek": 392576, "start": 3931.84, "end": 3935.0400000000004, "text": " is developed by Google.", "tokens": [307, 4743, 538, 3329, 13], "temperature": 0.0, "avg_logprob": -0.14902143963312697, "compression_ratio": 1.609375, "no_speech_prob": 1.5932599126244895e-05}, {"id": 1032, "seek": 392576, "start": 3935.0400000000004, "end": 3940.0, "text": " So TPUs are just fast and can hold a lot.", "tokens": [407, 314, 8115, 82, 366, 445, 2370, 293, 393, 1797, 257, 688, 13], "temperature": 0.0, "avg_logprob": -0.14902143963312697, "compression_ratio": 1.609375, "no_speech_prob": 1.5932599126244895e-05}, {"id": 1033, "seek": 392576, "start": 3940.0, "end": 3942.0, "text": " And for four days they had 64 chips.", "tokens": [400, 337, 1451, 1708, 436, 632, 12145, 11583, 13], "temperature": 0.0, "avg_logprob": -0.14902143963312697, "compression_ratio": 1.609375, "no_speech_prob": 1.5932599126244895e-05}, {"id": 1034, "seek": 392576, "start": 3942.0, "end": 3946.7200000000003, "text": " So if you have one GPU which you can think of as less than a single TPU, you're going", "tokens": [407, 498, 291, 362, 472, 18407, 597, 291, 393, 519, 295, 382, 1570, 813, 257, 2167, 314, 8115, 11, 291, 434, 516], "temperature": 0.0, "avg_logprob": -0.14902143963312697, "compression_ratio": 1.609375, "no_speech_prob": 1.5932599126244895e-05}, {"id": 1035, "seek": 392576, "start": 3946.7200000000003, "end": 3948.5600000000004, "text": " to be waiting a long time to pre-training.", "tokens": [281, 312, 3806, 257, 938, 565, 281, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.14902143963312697, "compression_ratio": 1.609375, "no_speech_prob": 1.5932599126244895e-05}, {"id": 1036, "seek": 392576, "start": 3948.5600000000004, "end": 3954.6800000000003, "text": " But fine-tuning is so fast, it's so fast and impractical, it's common on a single", "tokens": [583, 2489, 12, 83, 37726, 307, 370, 2370, 11, 309, 311, 370, 2370, 293, 704, 1897, 804, 11, 309, 311, 2689, 322, 257, 2167], "temperature": 0.0, "avg_logprob": -0.14902143963312697, "compression_ratio": 1.609375, "no_speech_prob": 1.5932599126244895e-05}, {"id": 1037, "seek": 395468, "start": 3954.68, "end": 3960.7599999999998, "text": " GPU, you'll see how much faster fine-tuning is than pre-training in assignment five.", "tokens": [18407, 11, 291, 603, 536, 577, 709, 4663, 2489, 12, 83, 37726, 307, 813, 659, 12, 17227, 1760, 294, 15187, 1732, 13], "temperature": 0.0, "avg_logprob": -0.16888215806749132, "compression_ratio": 1.7079646017699115, "no_speech_prob": 6.811845378251746e-05}, {"id": 1038, "seek": 395468, "start": 3960.7599999999998, "end": 3966.08, "text": " And so this becomes, I think, a refrain of the field, you pre-trained once or handful", "tokens": [400, 370, 341, 3643, 11, 286, 519, 11, 257, 46177, 295, 264, 2519, 11, 291, 659, 12, 17227, 2001, 1564, 420, 16458], "temperature": 0.0, "avg_logprob": -0.16888215806749132, "compression_ratio": 1.7079646017699115, "no_speech_prob": 6.811845378251746e-05}, {"id": 1039, "seek": 395468, "start": 3966.08, "end": 3971.04, "text": " of times, right, like a couple of people released big pre-trained models and then you fine-tune", "tokens": [295, 1413, 11, 558, 11, 411, 257, 1916, 295, 561, 4736, 955, 659, 12, 17227, 2001, 5245, 293, 550, 291, 2489, 12, 83, 2613], "temperature": 0.0, "avg_logprob": -0.16888215806749132, "compression_ratio": 1.7079646017699115, "no_speech_prob": 6.811845378251746e-05}, {"id": 1040, "seek": 395468, "start": 3971.04, "end": 3975.8799999999997, "text": " many times, right, so you save those parameters from pre-training and you fine-tune on all", "tokens": [867, 1413, 11, 558, 11, 370, 291, 3155, 729, 9834, 490, 659, 12, 17227, 1760, 293, 291, 2489, 12, 83, 2613, 322, 439], "temperature": 0.0, "avg_logprob": -0.16888215806749132, "compression_ratio": 1.7079646017699115, "no_speech_prob": 6.811845378251746e-05}, {"id": 1041, "seek": 395468, "start": 3975.8799999999997, "end": 3980.7599999999998, "text": " kinds of different problems.", "tokens": [3685, 295, 819, 2740, 13], "temperature": 0.0, "avg_logprob": -0.16888215806749132, "compression_ratio": 1.7079646017699115, "no_speech_prob": 6.811845378251746e-05}, {"id": 1042, "seek": 398076, "start": 3980.76, "end": 3985.0, "text": " And that paradigm, right, taking something like Bert or whatever the best descendant of", "tokens": [400, 300, 24709, 11, 558, 11, 1940, 746, 411, 29594, 420, 2035, 264, 1151, 16333, 394, 295], "temperature": 0.0, "avg_logprob": -0.17204670030243543, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.1975785279646516e-05}, {"id": 1043, "seek": 398076, "start": 3985.0, "end": 3991.36, "text": " Bert is and taking it pre-trained and then fine-tuning it on what you want is pretty", "tokens": [29594, 307, 293, 1940, 309, 659, 12, 17227, 2001, 293, 550, 2489, 12, 83, 37726, 309, 322, 437, 291, 528, 307, 1238], "temperature": 0.0, "avg_logprob": -0.17204670030243543, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.1975785279646516e-05}, {"id": 1044, "seek": 398076, "start": 3991.36, "end": 3997.5600000000004, "text": " close to, you know, it's a very, very strong baseline in NLP right now, right?", "tokens": [1998, 281, 11, 291, 458, 11, 309, 311, 257, 588, 11, 588, 2068, 20518, 294, 426, 45196, 558, 586, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17204670030243543, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.1975785279646516e-05}, {"id": 1045, "seek": 398076, "start": 3997.5600000000004, "end": 4000.88, "text": " So and the simplicity is pretty fascinating.", "tokens": [407, 293, 264, 25632, 307, 1238, 10343, 13], "temperature": 0.0, "avg_logprob": -0.17204670030243543, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.1975785279646516e-05}, {"id": 1046, "seek": 398076, "start": 4000.88, "end": 4006.48, "text": " And there's one code base called Transformers from a company called Hugging Face that", "tokens": [400, 456, 311, 472, 3089, 3096, 1219, 27938, 433, 490, 257, 2237, 1219, 46892, 3249, 4047, 300], "temperature": 0.0, "avg_logprob": -0.17204670030243543, "compression_ratio": 1.5850622406639003, "no_speech_prob": 4.1975785279646516e-05}, {"id": 1047, "seek": 400648, "start": 4006.48, "end": 4011.76, "text": " makes this just really just a couple of lines of Python to try out as well.", "tokens": [1669, 341, 445, 534, 445, 257, 1916, 295, 3876, 295, 15329, 281, 853, 484, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.15780087175040408, "compression_ratio": 1.6289752650176679, "no_speech_prob": 3.3726693800417706e-05}, {"id": 1048, "seek": 400648, "start": 4011.76, "end": 4017.2400000000002, "text": " So it sort of opened up very strong baselines without too, too much effort for a lot of", "tokens": [407, 309, 1333, 295, 5625, 493, 588, 2068, 987, 9173, 1553, 886, 11, 886, 709, 4630, 337, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.15780087175040408, "compression_ratio": 1.6289752650176679, "no_speech_prob": 3.3726693800417706e-05}, {"id": 1049, "seek": 400648, "start": 4017.2400000000002, "end": 4018.2400000000002, "text": " tasks.", "tokens": [9608, 13], "temperature": 0.0, "avg_logprob": -0.15780087175040408, "compression_ratio": 1.6289752650176679, "no_speech_prob": 3.3726693800417706e-05}, {"id": 1050, "seek": 400648, "start": 4018.2400000000002, "end": 4021.52, "text": " Okay, so let's talk about evaluation.", "tokens": [1033, 11, 370, 718, 311, 751, 466, 13344, 13], "temperature": 0.0, "avg_logprob": -0.15780087175040408, "compression_ratio": 1.6289752650176679, "no_speech_prob": 3.3726693800417706e-05}, {"id": 1051, "seek": 400648, "start": 4021.52, "end": 4026.52, "text": " So pre-training is pitched as requiring all this different kind of language understanding.", "tokens": [407, 659, 12, 17227, 1760, 307, 32994, 382, 24165, 439, 341, 819, 733, 295, 2856, 3701, 13], "temperature": 0.0, "avg_logprob": -0.15780087175040408, "compression_ratio": 1.6289752650176679, "no_speech_prob": 3.3726693800417706e-05}, {"id": 1052, "seek": 400648, "start": 4026.52, "end": 4031.76, "text": " And the field is, the field of NLP has a hard time doing evaluation.", "tokens": [400, 264, 2519, 307, 11, 264, 2519, 295, 426, 45196, 575, 257, 1152, 565, 884, 13344, 13], "temperature": 0.0, "avg_logprob": -0.15780087175040408, "compression_ratio": 1.6289752650176679, "no_speech_prob": 3.3726693800417706e-05}, {"id": 1053, "seek": 400648, "start": 4031.76, "end": 4035.48, "text": " But we try our best and we build datasets that we think are hard for various reasons because", "tokens": [583, 321, 853, 527, 1151, 293, 321, 1322, 42856, 300, 321, 519, 366, 1152, 337, 3683, 4112, 570], "temperature": 0.0, "avg_logprob": -0.15780087175040408, "compression_ratio": 1.6289752650176679, "no_speech_prob": 3.3726693800417706e-05}, {"id": 1054, "seek": 403548, "start": 4035.48, "end": 4039.92, "text": " they require you to know stuff about language and about the world and about reasoning.", "tokens": [436, 3651, 291, 281, 458, 1507, 466, 2856, 293, 466, 264, 1002, 293, 466, 21577, 13], "temperature": 0.0, "avg_logprob": -0.18595104591519224, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.2690601074136794e-05}, {"id": 1055, "seek": 403548, "start": 4039.92, "end": 4046.56, "text": " And so when we evaluate whether pre-training is getting you a lot of sort of general knowledge,", "tokens": [400, 370, 562, 321, 13059, 1968, 659, 12, 17227, 1760, 307, 1242, 291, 257, 688, 295, 1333, 295, 2674, 3601, 11], "temperature": 0.0, "avg_logprob": -0.18595104591519224, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.2690601074136794e-05}, {"id": 1056, "seek": 403548, "start": 4046.56, "end": 4050.84, "text": " we evaluate on a lot of these tasks.", "tokens": [321, 13059, 322, 257, 688, 295, 613, 9608, 13], "temperature": 0.0, "avg_logprob": -0.18595104591519224, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.2690601074136794e-05}, {"id": 1057, "seek": 403548, "start": 4050.84, "end": 4057.28, "text": " So we evaluate on things like paraphrase detection on core questions.", "tokens": [407, 321, 13059, 322, 721, 411, 36992, 1703, 651, 17784, 322, 4965, 1651, 13], "temperature": 0.0, "avg_logprob": -0.18595104591519224, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.2690601074136794e-05}, {"id": 1058, "seek": 403548, "start": 4057.28, "end": 4059.16, "text": " Natural language inference we saw.", "tokens": [20137, 2856, 38253, 321, 1866, 13], "temperature": 0.0, "avg_logprob": -0.18595104591519224, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.2690601074136794e-05}, {"id": 1059, "seek": 403548, "start": 4059.16, "end": 4063.2, "text": " We have hard sentiment analysis datasets or what we're hard sentiment analysis datasets", "tokens": [492, 362, 1152, 16149, 5215, 42856, 420, 437, 321, 434, 1152, 16149, 5215, 42856], "temperature": 0.0, "avg_logprob": -0.18595104591519224, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.2690601074136794e-05}, {"id": 1060, "seek": 403548, "start": 4063.2, "end": 4065.04, "text": " a couple of years ago.", "tokens": [257, 1916, 295, 924, 2057, 13], "temperature": 0.0, "avg_logprob": -0.18595104591519224, "compression_ratio": 1.7975206611570247, "no_speech_prob": 3.2690601074136794e-05}, {"id": 1061, "seek": 406504, "start": 4065.04, "end": 4070.0, "text": " And actually, figuring out if sentences are grammatical tends to be hard.", "tokens": [400, 767, 11, 15213, 484, 498, 16579, 366, 17570, 267, 804, 12258, 281, 312, 1152, 13], "temperature": 0.0, "avg_logprob": -0.21845442507447316, "compression_ratio": 1.673913043478261, "no_speech_prob": 4.90725324198138e-05}, {"id": 1062, "seek": 406504, "start": 4070.0, "end": 4074.0, "text": " Determining the semantic similarity of text can be hard.", "tokens": [4237, 966, 1760, 264, 47982, 32194, 295, 2487, 393, 312, 1152, 13], "temperature": 0.0, "avg_logprob": -0.21845442507447316, "compression_ratio": 1.673913043478261, "no_speech_prob": 4.90725324198138e-05}, {"id": 1063, "seek": 406504, "start": 4074.0, "end": 4075.52, "text": " Paraphrasing again.", "tokens": [3457, 569, 1703, 3349, 797, 13], "temperature": 0.0, "avg_logprob": -0.21845442507447316, "compression_ratio": 1.673913043478261, "no_speech_prob": 4.90725324198138e-05}, {"id": 1064, "seek": 406504, "start": 4075.52, "end": 4077.84, "text": " Natural language inference on a very, very small dataset.", "tokens": [20137, 2856, 38253, 322, 257, 588, 11, 588, 1359, 28872, 13], "temperature": 0.0, "avg_logprob": -0.21845442507447316, "compression_ratio": 1.673913043478261, "no_speech_prob": 4.90725324198138e-05}, {"id": 1065, "seek": 406504, "start": 4077.84, "end": 4081.16, "text": " So this is this pre-training help you train on smaller datasets.", "tokens": [407, 341, 307, 341, 659, 12, 17227, 1760, 854, 291, 3847, 322, 4356, 42856, 13], "temperature": 0.0, "avg_logprob": -0.21845442507447316, "compression_ratio": 1.673913043478261, "no_speech_prob": 4.90725324198138e-05}, {"id": 1066, "seek": 406504, "start": 4081.16, "end": 4083.2799999999997, "text": " The answer is yes, sort of thing.", "tokens": [440, 1867, 307, 2086, 11, 1333, 295, 551, 13], "temperature": 0.0, "avg_logprob": -0.21845442507447316, "compression_ratio": 1.673913043478261, "no_speech_prob": 4.90725324198138e-05}, {"id": 1067, "seek": 406504, "start": 4083.2799999999997, "end": 4089.48, "text": " And so the birth folks released their paper after GPT was released.", "tokens": [400, 370, 264, 3965, 4024, 4736, 641, 3035, 934, 26039, 51, 390, 4736, 13], "temperature": 0.0, "avg_logprob": -0.21845442507447316, "compression_ratio": 1.673913043478261, "no_speech_prob": 4.90725324198138e-05}, {"id": 1068, "seek": 406504, "start": 4089.48, "end": 4093.52, "text": " And there were a lot of sort of state of the art results that came from various things", "tokens": [400, 456, 645, 257, 688, 295, 1333, 295, 1785, 295, 264, 1523, 3542, 300, 1361, 490, 3683, 721], "temperature": 0.0, "avg_logprob": -0.21845442507447316, "compression_ratio": 1.673913043478261, "no_speech_prob": 4.90725324198138e-05}, {"id": 1069, "seek": 409352, "start": 4093.52, "end": 4096.12, "text": " that you were supposed to be doing.", "tokens": [300, 291, 645, 3442, 281, 312, 884, 13], "temperature": 0.0, "avg_logprob": -0.2333734579253615, "compression_ratio": 1.6322314049586777, "no_speech_prob": 3.0706018151249737e-05}, {"id": 1070, "seek": 409352, "start": 4096.12, "end": 4102.52, "text": " And the results that you get sort of with pre-training, so here's open AI, GPT, here's", "tokens": [400, 264, 3542, 300, 291, 483, 1333, 295, 365, 659, 12, 17227, 1760, 11, 370, 510, 311, 1269, 7318, 11, 26039, 51, 11, 510, 311], "temperature": 0.0, "avg_logprob": -0.2333734579253615, "compression_ratio": 1.6322314049586777, "no_speech_prob": 3.0706018151249737e-05}, {"id": 1071, "seek": 409352, "start": 4102.52, "end": 4103.52, "text": " birth base and large.", "tokens": [3965, 3096, 293, 2416, 13], "temperature": 0.0, "avg_logprob": -0.2333734579253615, "compression_ratio": 1.6322314049586777, "no_speech_prob": 3.0706018151249737e-05}, {"id": 1072, "seek": 409352, "start": 4103.52, "end": 4105.76, "text": " The last three rows are all pre-trained.", "tokens": [440, 1036, 1045, 13241, 366, 439, 659, 12, 17227, 2001, 13], "temperature": 0.0, "avg_logprob": -0.2333734579253615, "compression_ratio": 1.6322314049586777, "no_speech_prob": 3.0706018151249737e-05}, {"id": 1073, "seek": 409352, "start": 4105.76, "end": 4112.48, "text": " Elmo is sort of in the middle between pre-training the whole model and just having word embeddings.", "tokens": [2699, 3280, 307, 1333, 295, 294, 264, 2808, 1296, 659, 12, 17227, 1760, 264, 1379, 2316, 293, 445, 1419, 1349, 12240, 29432, 13], "temperature": 0.0, "avg_logprob": -0.2333734579253615, "compression_ratio": 1.6322314049586777, "no_speech_prob": 3.0706018151249737e-05}, {"id": 1074, "seek": 409352, "start": 4112.48, "end": 4114.56, "text": " That's what this is.", "tokens": [663, 311, 437, 341, 307, 13], "temperature": 0.0, "avg_logprob": -0.2333734579253615, "compression_ratio": 1.6322314049586777, "no_speech_prob": 3.0706018151249737e-05}, {"id": 1075, "seek": 409352, "start": 4114.56, "end": 4119.32, "text": " And the numbers you get are just, I think, to the field where quite astounding actually.", "tokens": [400, 264, 3547, 291, 483, 366, 445, 11, 286, 519, 11, 281, 264, 2519, 689, 1596, 5357, 24625, 767, 13], "temperature": 0.0, "avg_logprob": -0.2333734579253615, "compression_ratio": 1.6322314049586777, "no_speech_prob": 3.0706018151249737e-05}, {"id": 1076, "seek": 411932, "start": 4119.32, "end": 4124.16, "text": " We were all surprised that there was that much left to even be gotten on some of these datasets.", "tokens": [492, 645, 439, 6100, 300, 456, 390, 300, 709, 1411, 281, 754, 312, 5768, 322, 512, 295, 613, 42856, 13], "temperature": 0.0, "avg_logprob": -0.14530930637328093, "compression_ratio": 1.669172932330827, "no_speech_prob": 5.142260852153413e-05}, {"id": 1077, "seek": 411932, "start": 4124.16, "end": 4129.08, "text": " And taking here, so this line in the table is unmarked when it's actually the number", "tokens": [400, 1940, 510, 11, 370, 341, 1622, 294, 264, 3199, 307, 517, 5638, 292, 562, 309, 311, 767, 264, 1230], "temperature": 0.0, "avg_logprob": -0.14530930637328093, "compression_ratio": 1.669172932330827, "no_speech_prob": 5.142260852153413e-05}, {"id": 1078, "seek": 411932, "start": 4129.08, "end": 4130.4, "text": " of training examples.", "tokens": [295, 3097, 5110, 13], "temperature": 0.0, "avg_logprob": -0.14530930637328093, "compression_ratio": 1.669172932330827, "no_speech_prob": 5.142260852153413e-05}, {"id": 1079, "seek": 411932, "start": 4130.4, "end": 4133.92, "text": " This dataset has 2.5,000 training examples.", "tokens": [639, 28872, 575, 568, 13, 20, 11, 1360, 3097, 5110, 13], "temperature": 0.0, "avg_logprob": -0.14530930637328093, "compression_ratio": 1.669172932330827, "no_speech_prob": 5.142260852153413e-05}, {"id": 1080, "seek": 411932, "start": 4133.92, "end": 4139.84, "text": " And before sort of the big transformers came around, we had 60% accuracy on it.", "tokens": [400, 949, 1333, 295, 264, 955, 4088, 433, 1361, 926, 11, 321, 632, 4060, 4, 14170, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.14530930637328093, "compression_ratio": 1.669172932330827, "no_speech_prob": 5.142260852153413e-05}, {"id": 1081, "seek": 411932, "start": 4139.84, "end": 4141.0, "text": " We run transformers on it.", "tokens": [492, 1190, 4088, 433, 322, 309, 13], "temperature": 0.0, "avg_logprob": -0.14530930637328093, "compression_ratio": 1.669172932330827, "no_speech_prob": 5.142260852153413e-05}, {"id": 1082, "seek": 411932, "start": 4141.0, "end": 4143.719999999999, "text": " We get 10 points just by pre-training.", "tokens": [492, 483, 1266, 2793, 445, 538, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.14530930637328093, "compression_ratio": 1.669172932330827, "no_speech_prob": 5.142260852153413e-05}, {"id": 1083, "seek": 411932, "start": 4143.719999999999, "end": 4147.2, "text": " And this has been a trend that has just continued.", "tokens": [400, 341, 575, 668, 257, 6028, 300, 575, 445, 7014, 13], "temperature": 0.0, "avg_logprob": -0.14530930637328093, "compression_ratio": 1.669172932330827, "no_speech_prob": 5.142260852153413e-05}, {"id": 1084, "seek": 414720, "start": 4147.2, "end": 4151.4, "text": " So why do anything but pre-trained encoders?", "tokens": [407, 983, 360, 1340, 457, 659, 12, 17227, 2001, 2058, 378, 433, 30], "temperature": 0.0, "avg_logprob": -0.18377333931300951, "compression_ratio": 1.5942622950819672, "no_speech_prob": 2.0459350707824342e-05}, {"id": 1085, "seek": 414720, "start": 4151.4, "end": 4153.5199999999995, "text": " We know encoders are good.", "tokens": [492, 458, 2058, 378, 433, 366, 665, 13], "temperature": 0.0, "avg_logprob": -0.18377333931300951, "compression_ratio": 1.5942622950819672, "no_speech_prob": 2.0459350707824342e-05}, {"id": 1086, "seek": 414720, "start": 4153.5199999999995, "end": 4155.44, "text": " We like the fact that you have bidirectional context.", "tokens": [492, 411, 264, 1186, 300, 291, 362, 12957, 621, 41048, 4319, 13], "temperature": 0.0, "avg_logprob": -0.18377333931300951, "compression_ratio": 1.5942622950819672, "no_speech_prob": 2.0459350707824342e-05}, {"id": 1087, "seek": 414720, "start": 4155.44, "end": 4158.88, "text": " We also saw that BERT did better than GPT.", "tokens": [492, 611, 1866, 300, 363, 31479, 630, 1101, 813, 26039, 51, 13], "temperature": 0.0, "avg_logprob": -0.18377333931300951, "compression_ratio": 1.5942622950819672, "no_speech_prob": 2.0459350707824342e-05}, {"id": 1088, "seek": 414720, "start": 4158.88, "end": 4167.32, "text": " But if you want to actually get it to do things, you can't just generate sequences from", "tokens": [583, 498, 291, 528, 281, 767, 483, 309, 281, 360, 721, 11, 291, 393, 380, 445, 8460, 22978, 490], "temperature": 0.0, "avg_logprob": -0.18377333931300951, "compression_ratio": 1.5942622950819672, "no_speech_prob": 2.0459350707824342e-05}, {"id": 1089, "seek": 414720, "start": 4167.32, "end": 4172.44, "text": " it the same way that you would from a model like GPT, a pre-trained decoder.", "tokens": [309, 264, 912, 636, 300, 291, 576, 490, 257, 2316, 411, 26039, 51, 11, 257, 659, 12, 17227, 2001, 979, 19866, 13], "temperature": 0.0, "avg_logprob": -0.18377333931300951, "compression_ratio": 1.5942622950819672, "no_speech_prob": 2.0459350707824342e-05}, {"id": 1090, "seek": 414720, "start": 4172.44, "end": 4174.76, "text": " You can sort of sample what things should go in a mask.", "tokens": [509, 393, 1333, 295, 6889, 437, 721, 820, 352, 294, 257, 6094, 13], "temperature": 0.0, "avg_logprob": -0.18377333931300951, "compression_ratio": 1.5942622950819672, "no_speech_prob": 2.0459350707824342e-05}, {"id": 1091, "seek": 417476, "start": 4174.76, "end": 4179.64, "text": " So here's a mask. You can put a mask somewhere, sample the words that should go there.", "tokens": [407, 510, 311, 257, 6094, 13, 509, 393, 829, 257, 6094, 4079, 11, 6889, 264, 2283, 300, 820, 352, 456, 13], "temperature": 0.0, "avg_logprob": -0.2082966663798348, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.318384576938115e-05}, {"id": 1092, "seek": 417476, "start": 4179.64, "end": 4182.360000000001, "text": " But if you want to sample whole context, right, if you want to get that story about the", "tokens": [583, 498, 291, 528, 281, 6889, 1379, 4319, 11, 558, 11, 498, 291, 528, 281, 483, 300, 1657, 466, 264], "temperature": 0.0, "avg_logprob": -0.2082966663798348, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.318384576938115e-05}, {"id": 1093, "seek": 417476, "start": 4182.360000000001, "end": 4186.72, "text": " unicorns, for example, the encoder is not what you want to do.", "tokens": [28122, 82, 11, 337, 1365, 11, 264, 2058, 19866, 307, 406, 437, 291, 528, 281, 360, 13], "temperature": 0.0, "avg_logprob": -0.2082966663798348, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.318384576938115e-05}, {"id": 1094, "seek": 417476, "start": 4186.72, "end": 4191.88, "text": " So they have sort of different contracts, and they can be used naturally at least in", "tokens": [407, 436, 362, 1333, 295, 819, 13952, 11, 293, 436, 393, 312, 1143, 8195, 412, 1935, 294], "temperature": 0.0, "avg_logprob": -0.2082966663798348, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.318384576938115e-05}, {"id": 1095, "seek": 417476, "start": 4191.88, "end": 4193.280000000001, "text": " different ways.", "tokens": [819, 2098, 13], "temperature": 0.0, "avg_logprob": -0.2082966663798348, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.318384576938115e-05}, {"id": 1096, "seek": 417476, "start": 4193.280000000001, "end": 4197.52, "text": " Okay, so let's talk very briefly about extensions of BERT.", "tokens": [1033, 11, 370, 718, 311, 751, 588, 10515, 466, 25129, 295, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.2082966663798348, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.318384576938115e-05}, {"id": 1097, "seek": 417476, "start": 4197.52, "end": 4200.92, "text": " So they're BERT variants like Roberta and Spanbert.", "tokens": [407, 436, 434, 363, 31479, 21669, 411, 15800, 1328, 293, 1738, 282, 4290, 13], "temperature": 0.0, "avg_logprob": -0.2082966663798348, "compression_ratio": 1.6753731343283582, "no_speech_prob": 2.318384576938115e-05}, {"id": 1098, "seek": 420092, "start": 4200.92, "end": 4204.88, "text": " And there's just a bunch of papers with the word BERT in the title that did various things.", "tokens": [400, 456, 311, 445, 257, 3840, 295, 10577, 365, 264, 1349, 363, 31479, 294, 264, 4876, 300, 630, 3683, 721, 13], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1099, "seek": 420092, "start": 4204.88, "end": 4206.8, "text": " Two very strong takeaways.", "tokens": [4453, 588, 2068, 45584, 13], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1100, "seek": 420092, "start": 4206.8, "end": 4208.8, "text": " Roberta, train BERT longer.", "tokens": [15800, 1328, 11, 3847, 363, 31479, 2854, 13], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1101, "seek": 420092, "start": 4208.8, "end": 4210.04, "text": " BERT is underfit.", "tokens": [363, 31479, 307, 833, 6845, 13], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1102, "seek": 420092, "start": 4210.04, "end": 4211.28, "text": " Train it on more data.", "tokens": [28029, 309, 322, 544, 1412, 13], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1103, "seek": 420092, "start": 4211.28, "end": 4213.12, "text": " Train it for more steps.", "tokens": [28029, 309, 337, 544, 4439, 13], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1104, "seek": 420092, "start": 4213.12, "end": 4218.96, "text": " Spanbert, mask, contiguous spans of sub words.", "tokens": [1738, 282, 4290, 11, 6094, 11, 660, 30525, 44086, 295, 1422, 2283, 13], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1105, "seek": 420092, "start": 4218.96, "end": 4221.24, "text": " Words makes a harder, more useful pre-training task.", "tokens": [32857, 1669, 257, 6081, 11, 544, 4420, 659, 12, 17227, 1760, 5633, 13], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1106, "seek": 420092, "start": 4221.24, "end": 4225.6, "text": " So this is the idea that we can come up with better ways of noisy the input, of hiding", "tokens": [407, 341, 307, 264, 1558, 300, 321, 393, 808, 493, 365, 1101, 2098, 295, 24518, 264, 4846, 11, 295, 10596], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1107, "seek": 420092, "start": 4225.6, "end": 4230.4400000000005, "text": " stuff in the input, or breaking stuff in the input for our model to correct.", "tokens": [1507, 294, 264, 4846, 11, 420, 7697, 1507, 294, 264, 4846, 337, 527, 2316, 281, 3006, 13], "temperature": 0.0, "avg_logprob": -0.21180109734082744, "compression_ratio": 1.7122302158273381, "no_speech_prob": 2.5066054149647243e-05}, {"id": 1108, "seek": 423044, "start": 4230.44, "end": 4237.5199999999995, "text": " So for example, if you have the sentence mask, ear, razz, razz, good, it's just not that", "tokens": [407, 337, 1365, 11, 498, 291, 362, 264, 8174, 6094, 11, 1273, 11, 367, 9112, 11, 367, 9112, 11, 665, 11, 309, 311, 445, 406, 300], "temperature": 0.0, "avg_logprob": -0.2277692968195135, "compression_ratio": 1.6824034334763949, "no_speech_prob": 3.168639523210004e-05}, {"id": 1109, "seek": 423044, "start": 4237.5199999999995, "end": 4243.28, "text": " hard to know that this is irresistibly, right, because like what could this possibly", "tokens": [1152, 281, 458, 300, 341, 307, 3418, 495, 468, 3545, 11, 558, 11, 570, 411, 437, 727, 341, 6264], "temperature": 0.0, "avg_logprob": -0.2277692968195135, "compression_ratio": 1.6824034334763949, "no_speech_prob": 3.168639523210004e-05}, {"id": 1110, "seek": 423044, "start": 4243.28, "end": 4244.639999999999, "text": " be after these sub words?", "tokens": [312, 934, 613, 1422, 2283, 30], "temperature": 0.0, "avg_logprob": -0.2277692968195135, "compression_ratio": 1.6824034334763949, "no_speech_prob": 3.168639523210004e-05}, {"id": 1111, "seek": 423044, "start": 4244.639999999999, "end": 4251.12, "text": " So this is irresist, you know, something's about to come here and it's probably the end", "tokens": [407, 341, 307, 3418, 495, 468, 11, 291, 458, 11, 746, 311, 466, 281, 808, 510, 293, 309, 311, 1391, 264, 917], "temperature": 0.0, "avg_logprob": -0.2277692968195135, "compression_ratio": 1.6824034334763949, "no_speech_prob": 3.168639523210004e-05}, {"id": 1112, "seek": 423044, "start": 4251.12, "end": 4252.12, "text": " of that word.", "tokens": [295, 300, 1349, 13], "temperature": 0.0, "avg_logprob": -0.2277692968195135, "compression_ratio": 1.6824034334763949, "no_speech_prob": 3.168639523210004e-05}, {"id": 1113, "seek": 423044, "start": 4252.12, "end": 4257.36, "text": " Whereas if you mask a long sequence of things, right now this is much harder, and actually", "tokens": [13813, 498, 291, 6094, 257, 938, 8310, 295, 721, 11, 558, 586, 341, 307, 709, 6081, 11, 293, 767], "temperature": 0.0, "avg_logprob": -0.2277692968195135, "compression_ratio": 1.6824034334763949, "no_speech_prob": 3.168639523210004e-05}, {"id": 1114, "seek": 425736, "start": 4257.36, "end": 4262.2, "text": " you're getting a useful signal that is irresistibly good, and you sort of needed to mask all of", "tokens": [291, 434, 1242, 257, 4420, 6358, 300, 307, 3418, 495, 468, 3545, 665, 11, 293, 291, 1333, 295, 2978, 281, 6094, 439, 295], "temperature": 0.0, "avg_logprob": -0.18158520785245028, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.6437074009445496e-05}, {"id": 1115, "seek": 425736, "start": 4262.2, "end": 4264.28, "text": " them to make the task interesting.", "tokens": [552, 281, 652, 264, 5633, 1880, 13], "temperature": 0.0, "avg_logprob": -0.18158520785245028, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.6437074009445496e-05}, {"id": 1116, "seek": 425736, "start": 4264.28, "end": 4268.04, "text": " So Spanbert was like, oh, you should do this.", "tokens": [407, 1738, 282, 4290, 390, 411, 11, 1954, 11, 291, 820, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.18158520785245028, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.6437074009445496e-05}, {"id": 1117, "seek": 425736, "start": 4268.04, "end": 4270.16, "text": " This was super useful as well.", "tokens": [639, 390, 1687, 4420, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.18158520785245028, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.6437074009445496e-05}, {"id": 1118, "seek": 425736, "start": 4270.16, "end": 4275.48, "text": " So Roberta, just to point you at the fact that Roberta showed that BERT was underfit,", "tokens": [407, 15800, 1328, 11, 445, 281, 935, 291, 412, 264, 1186, 300, 15800, 1328, 4712, 300, 363, 31479, 390, 833, 6845, 11], "temperature": 0.0, "avg_logprob": -0.18158520785245028, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.6437074009445496e-05}, {"id": 1119, "seek": 425736, "start": 4275.48, "end": 4281.44, "text": " you know, he said, BERT was trained on about 13 gigabytes of text, it got some accuracies,", "tokens": [291, 458, 11, 415, 848, 11, 363, 31479, 390, 8895, 322, 466, 3705, 42741, 295, 2487, 11, 309, 658, 512, 5771, 20330, 11], "temperature": 0.0, "avg_logprob": -0.18158520785245028, "compression_ratio": 1.5737704918032787, "no_speech_prob": 1.6437074009445496e-05}, {"id": 1120, "seek": 428144, "start": 4281.44, "end": 4287.839999999999, "text": " you can get above the amazing results of BERT, four extra points or so here, right, just", "tokens": [291, 393, 483, 3673, 264, 2243, 3542, 295, 363, 31479, 11, 1451, 2857, 2793, 420, 370, 510, 11, 558, 11, 445], "temperature": 0.0, "avg_logprob": -0.17626541539242394, "compression_ratio": 1.552511415525114, "no_speech_prob": 3.762821143027395e-05}, {"id": 1121, "seek": 428144, "start": 4287.839999999999, "end": 4295.12, "text": " by taking the identical model and training it on more data, the larger batch size for", "tokens": [538, 1940, 264, 14800, 2316, 293, 3097, 309, 322, 544, 1412, 11, 264, 4833, 15245, 2744, 337], "temperature": 0.0, "avg_logprob": -0.17626541539242394, "compression_ratio": 1.552511415525114, "no_speech_prob": 3.762821143027395e-05}, {"id": 1122, "seek": 428144, "start": 4295.12, "end": 4296.599999999999, "text": " a long time.", "tokens": [257, 938, 565, 13], "temperature": 0.0, "avg_logprob": -0.17626541539242394, "compression_ratio": 1.552511415525114, "no_speech_prob": 3.762821143027395e-05}, {"id": 1123, "seek": 428144, "start": 4296.599999999999, "end": 4301.2, "text": " And if you train it, yeah, even longer without sort of more data, you don't get any", "tokens": [400, 498, 291, 3847, 309, 11, 1338, 11, 754, 2854, 1553, 1333, 295, 544, 1412, 11, 291, 500, 380, 483, 604], "temperature": 0.0, "avg_logprob": -0.17626541539242394, "compression_ratio": 1.552511415525114, "no_speech_prob": 3.762821143027395e-05}, {"id": 1124, "seek": 428144, "start": 4301.2, "end": 4305.24, "text": " better.", "tokens": [1101, 13], "temperature": 0.0, "avg_logprob": -0.17626541539242394, "compression_ratio": 1.552511415525114, "no_speech_prob": 3.762821143027395e-05}, {"id": 1125, "seek": 428144, "start": 4305.24, "end": 4309.32, "text": " Very briefly, okay, so very briefly on the encoder decoders.", "tokens": [4372, 10515, 11, 1392, 11, 370, 588, 10515, 322, 264, 2058, 19866, 979, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.17626541539242394, "compression_ratio": 1.552511415525114, "no_speech_prob": 3.762821143027395e-05}, {"id": 1126, "seek": 430932, "start": 4309.32, "end": 4313.639999999999, "text": " So we've seen decoders can be good because we get to play with the contracts that they", "tokens": [407, 321, 600, 1612, 979, 378, 433, 393, 312, 665, 570, 321, 483, 281, 862, 365, 264, 13952, 300, 436], "temperature": 0.0, "avg_logprob": -0.1904725467457491, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.762691994779743e-05}, {"id": 1127, "seek": 430932, "start": 4313.639999999999, "end": 4317.32, "text": " give us, we get to play with them as language models, encoders give us that bidirectional", "tokens": [976, 505, 11, 321, 483, 281, 862, 365, 552, 382, 2856, 5245, 11, 2058, 378, 433, 976, 505, 300, 12957, 621, 41048], "temperature": 0.0, "avg_logprob": -0.1904725467457491, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.762691994779743e-05}, {"id": 1128, "seek": 430932, "start": 4317.32, "end": 4318.5599999999995, "text": " context.", "tokens": [4319, 13], "temperature": 0.0, "avg_logprob": -0.1904725467457491, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.762691994779743e-05}, {"id": 1129, "seek": 430932, "start": 4318.5599999999995, "end": 4322.0, "text": " So encoder decoders, maybe we get both.", "tokens": [407, 2058, 19866, 979, 378, 433, 11, 1310, 321, 483, 1293, 13], "temperature": 0.0, "avg_logprob": -0.1904725467457491, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.762691994779743e-05}, {"id": 1130, "seek": 430932, "start": 4322.0, "end": 4324.88, "text": " In practice, they're actually, yeah, pretty strong.", "tokens": [682, 3124, 11, 436, 434, 767, 11, 1338, 11, 1238, 2068, 13], "temperature": 0.0, "avg_logprob": -0.1904725467457491, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.762691994779743e-05}, {"id": 1131, "seek": 430932, "start": 4324.88, "end": 4331.48, "text": " So there was a, right, we could, so I guess one of the questions is like, what do we do", "tokens": [407, 456, 390, 257, 11, 558, 11, 321, 727, 11, 370, 286, 2041, 472, 295, 264, 1651, 307, 411, 11, 437, 360, 321, 360], "temperature": 0.0, "avg_logprob": -0.1904725467457491, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.762691994779743e-05}, {"id": 1132, "seek": 430932, "start": 4331.48, "end": 4333.12, "text": " to pre-train them?", "tokens": [281, 659, 12, 17227, 259, 552, 30], "temperature": 0.0, "avg_logprob": -0.1904725467457491, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.762691994779743e-05}, {"id": 1133, "seek": 430932, "start": 4333.12, "end": 4338.32, "text": " So we could do something like language modeling, right, where we take a sequence of words,", "tokens": [407, 321, 727, 360, 746, 411, 2856, 15983, 11, 558, 11, 689, 321, 747, 257, 8310, 295, 2283, 11], "temperature": 0.0, "avg_logprob": -0.1904725467457491, "compression_ratio": 1.7924528301886793, "no_speech_prob": 3.762691994779743e-05}, {"id": 1134, "seek": 433832, "start": 4338.32, "end": 4347.4, "text": " one to word two t instead of t, right, and so as I have word one here, dot, dot, dot,", "tokens": [472, 281, 1349, 732, 256, 2602, 295, 256, 11, 558, 11, 293, 370, 382, 286, 362, 1349, 472, 510, 11, 5893, 11, 5893, 11, 5893, 11], "temperature": 0.0, "avg_logprob": -0.16623075803120932, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.331294621806592e-05}, {"id": 1135, "seek": 433832, "start": 4347.4, "end": 4352.24, "text": " word t, we provide those all to our encoder and we predict on none of them.", "tokens": [1349, 256, 11, 321, 2893, 729, 439, 281, 527, 2058, 19866, 293, 321, 6069, 322, 6022, 295, 552, 13], "temperature": 0.0, "avg_logprob": -0.16623075803120932, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.331294621806592e-05}, {"id": 1136, "seek": 433832, "start": 4352.24, "end": 4357.36, "text": " And then we have word t plus one to word two t here in our decoder, right, and we predict", "tokens": [400, 550, 321, 362, 1349, 256, 1804, 472, 281, 1349, 732, 256, 510, 294, 527, 979, 19866, 11, 558, 11, 293, 321, 6069], "temperature": 0.0, "avg_logprob": -0.16623075803120932, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.331294621806592e-05}, {"id": 1137, "seek": 433832, "start": 4357.36, "end": 4358.36, "text": " on these.", "tokens": [322, 613, 13], "temperature": 0.0, "avg_logprob": -0.16623075803120932, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.331294621806592e-05}, {"id": 1138, "seek": 433832, "start": 4358.36, "end": 4362.48, "text": " So we're doing language modeling on half the sequence and we've taken the other half", "tokens": [407, 321, 434, 884, 2856, 15983, 322, 1922, 264, 8310, 293, 321, 600, 2726, 264, 661, 1922], "temperature": 0.0, "avg_logprob": -0.16623075803120932, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.331294621806592e-05}, {"id": 1139, "seek": 433832, "start": 4362.48, "end": 4366.88, "text": " to have our bidirectional encoder, right, so we're building strong representations on", "tokens": [281, 362, 527, 12957, 621, 41048, 2058, 19866, 11, 558, 11, 370, 321, 434, 2390, 2068, 33358, 322], "temperature": 0.0, "avg_logprob": -0.16623075803120932, "compression_ratio": 1.894736842105263, "no_speech_prob": 4.331294621806592e-05}, {"id": 1140, "seek": 436688, "start": 4366.88, "end": 4372.04, "text": " the encoder side, not predicting language modeling on any of this.", "tokens": [264, 2058, 19866, 1252, 11, 406, 32884, 2856, 15983, 322, 604, 295, 341, 13], "temperature": 0.0, "avg_logprob": -0.13302882371750552, "compression_ratio": 1.7844827586206897, "no_speech_prob": 1.6959804270300083e-05}, {"id": 1141, "seek": 436688, "start": 4372.04, "end": 4375.52, "text": " And then we, on the other half of the tokens, we predict, you know, as a language model", "tokens": [400, 550, 321, 11, 322, 264, 661, 1922, 295, 264, 22667, 11, 321, 6069, 11, 291, 458, 11, 382, 257, 2856, 2316], "temperature": 0.0, "avg_logprob": -0.13302882371750552, "compression_ratio": 1.7844827586206897, "no_speech_prob": 1.6959804270300083e-05}, {"id": 1142, "seek": 436688, "start": 4375.52, "end": 4377.4400000000005, "text": " would do.", "tokens": [576, 360, 13], "temperature": 0.0, "avg_logprob": -0.13302882371750552, "compression_ratio": 1.7844827586206897, "no_speech_prob": 1.6959804270300083e-05}, {"id": 1143, "seek": 436688, "start": 4377.4400000000005, "end": 4381.12, "text": " And the hope is that you sort of pre-trained both of these well through the one language", "tokens": [400, 264, 1454, 307, 300, 291, 1333, 295, 659, 12, 17227, 2001, 1293, 295, 613, 731, 807, 264, 472, 2856], "temperature": 0.0, "avg_logprob": -0.13302882371750552, "compression_ratio": 1.7844827586206897, "no_speech_prob": 1.6959804270300083e-05}, {"id": 1144, "seek": 436688, "start": 4381.12, "end": 4384.28, "text": " modeling loss up here.", "tokens": [15983, 4470, 493, 510, 13], "temperature": 0.0, "avg_logprob": -0.13302882371750552, "compression_ratio": 1.7844827586206897, "no_speech_prob": 1.6959804270300083e-05}, {"id": 1145, "seek": 436688, "start": 4384.28, "end": 4386.4800000000005, "text": " And this is actually, so this works pretty well.", "tokens": [400, 341, 307, 767, 11, 370, 341, 1985, 1238, 731, 13], "temperature": 0.0, "avg_logprob": -0.13302882371750552, "compression_ratio": 1.7844827586206897, "no_speech_prob": 1.6959804270300083e-05}, {"id": 1146, "seek": 436688, "start": 4386.4800000000005, "end": 4391.0, "text": " The encoder benefits from bidirectionality, the decoder, you can use to train the model.", "tokens": [440, 2058, 19866, 5311, 490, 12957, 621, 882, 1860, 11, 264, 979, 19866, 11, 291, 393, 764, 281, 3847, 264, 2316, 13], "temperature": 0.0, "avg_logprob": -0.13302882371750552, "compression_ratio": 1.7844827586206897, "no_speech_prob": 1.6959804270300083e-05}, {"id": 1147, "seek": 439100, "start": 4391.0, "end": 4399.16, "text": " But what this paper showed that introduced the Model T5, roughly at all, found to work", "tokens": [583, 437, 341, 3035, 4712, 300, 7268, 264, 17105, 314, 20, 11, 9810, 412, 439, 11, 1352, 281, 589], "temperature": 0.0, "avg_logprob": -0.13383178176166854, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.626330388011411e-05}, {"id": 1148, "seek": 439100, "start": 4399.16, "end": 4402.92, "text": " best was actually a very, or at least a somewhat different objective.", "tokens": [1151, 390, 767, 257, 588, 11, 420, 412, 1935, 257, 8344, 819, 10024, 13], "temperature": 0.0, "avg_logprob": -0.13383178176166854, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.626330388011411e-05}, {"id": 1149, "seek": 439100, "start": 4402.92, "end": 4406.88, "text": " And this should keep in your mind sort of that we have different ways of specifying the", "tokens": [400, 341, 820, 1066, 294, 428, 1575, 1333, 295, 300, 321, 362, 819, 2098, 295, 1608, 5489, 264], "temperature": 0.0, "avg_logprob": -0.13383178176166854, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.626330388011411e-05}, {"id": 1150, "seek": 439100, "start": 4406.88, "end": 4410.4, "text": " pre-training objectives and they will really work differently from each other.", "tokens": [659, 12, 17227, 1760, 15961, 293, 436, 486, 534, 589, 7614, 490, 1184, 661, 13], "temperature": 0.0, "avg_logprob": -0.13383178176166854, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.626330388011411e-05}, {"id": 1151, "seek": 439100, "start": 4410.4, "end": 4413.56, "text": " So what they said, let's say you have an original text like this.", "tokens": [407, 437, 436, 848, 11, 718, 311, 584, 291, 362, 364, 3380, 2487, 411, 341, 13], "temperature": 0.0, "avg_logprob": -0.13383178176166854, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.626330388011411e-05}, {"id": 1152, "seek": 439100, "start": 4413.56, "end": 4417.84, "text": " Thank you for inviting me to your party last week.", "tokens": [1044, 291, 337, 18202, 385, 281, 428, 3595, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.13383178176166854, "compression_ratio": 1.6356877323420074, "no_speech_prob": 2.626330388011411e-05}, {"id": 1153, "seek": 441784, "start": 4417.84, "end": 4423.8, "text": " We're going to define variable length spans in the text to replace with a unique symbol", "tokens": [492, 434, 516, 281, 6964, 7006, 4641, 44086, 294, 264, 2487, 281, 7406, 365, 257, 3845, 5986], "temperature": 0.0, "avg_logprob": -0.19874966144561768, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.4676944526145235e-05}, {"id": 1154, "seek": 441784, "start": 4423.8, "end": 4426.360000000001, "text": " that says something is missing here.", "tokens": [300, 1619, 746, 307, 5361, 510, 13], "temperature": 0.0, "avg_logprob": -0.19874966144561768, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.4676944526145235e-05}, {"id": 1155, "seek": 441784, "start": 4426.360000000001, "end": 4428.96, "text": " And then we'll replace and then we'll remove that.", "tokens": [400, 550, 321, 603, 7406, 293, 550, 321, 603, 4159, 300, 13], "temperature": 0.0, "avg_logprob": -0.19874966144561768, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.4676944526145235e-05}, {"id": 1156, "seek": 441784, "start": 4428.96, "end": 4437.76, "text": " So now our input to our encoder is thank you symbol one, me to your party symbol to week.", "tokens": [407, 586, 527, 4846, 281, 527, 2058, 19866, 307, 1309, 291, 5986, 472, 11, 385, 281, 428, 3595, 5986, 281, 1243, 13], "temperature": 0.0, "avg_logprob": -0.19874966144561768, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.4676944526145235e-05}, {"id": 1157, "seek": 441784, "start": 4437.76, "end": 4441.2, "text": " So we've noise the input, we've hidden stuff in the input.", "tokens": [407, 321, 600, 5658, 264, 4846, 11, 321, 600, 7633, 1507, 294, 264, 4846, 13], "temperature": 0.0, "avg_logprob": -0.19874966144561768, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.4676944526145235e-05}, {"id": 1158, "seek": 441784, "start": 4441.2, "end": 4445.52, "text": " Also really interestingly, this doesn't say how long this is supposed to be.", "tokens": [2743, 534, 25873, 11, 341, 1177, 380, 584, 577, 938, 341, 307, 3442, 281, 312, 13], "temperature": 0.0, "avg_logprob": -0.19874966144561768, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.4676944526145235e-05}, {"id": 1159, "seek": 441784, "start": 4445.52, "end": 4447.52, "text": " That's different from BERT.", "tokens": [663, 311, 819, 490, 363, 31479, 13], "temperature": 0.0, "avg_logprob": -0.19874966144561768, "compression_ratio": 1.6563706563706564, "no_speech_prob": 4.4676944526145235e-05}, {"id": 1160, "seek": 444752, "start": 4447.52, "end": 4450.76, "text": " BERT said, oh, you masked this many sub words.", "tokens": [363, 31479, 848, 11, 1954, 11, 291, 45249, 341, 867, 1422, 2283, 13], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1161, "seek": 444752, "start": 4450.76, "end": 4453.92, "text": " This says, well, I got some token that says something's missing here.", "tokens": [639, 1619, 11, 731, 11, 286, 658, 512, 14862, 300, 1619, 746, 311, 5361, 510, 13], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1162, "seek": 444752, "start": 4453.92, "end": 4454.92, "text": " And I don't know what it is.", "tokens": [400, 286, 500, 380, 458, 437, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1163, "seek": 444752, "start": 4454.92, "end": 4457.72, "text": " I don't even know how many sub words it is.", "tokens": [286, 500, 380, 754, 458, 577, 867, 1422, 2283, 309, 307, 13], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1164, "seek": 444752, "start": 4457.72, "end": 4464.76, "text": " And then so you have this in your encoder and then your decoder predicts the first special", "tokens": [400, 550, 370, 291, 362, 341, 294, 428, 2058, 19866, 293, 550, 428, 979, 19866, 6069, 82, 264, 700, 2121], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1165, "seek": 444752, "start": 4464.76, "end": 4467.8, "text": " word, this x here.", "tokens": [1349, 11, 341, 2031, 510, 13], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1166, "seek": 444752, "start": 4467.8, "end": 4470.160000000001, "text": " And then what was missing for inviting.", "tokens": [400, 550, 437, 390, 5361, 337, 18202, 13], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1167, "seek": 444752, "start": 4470.160000000001, "end": 4473.200000000001, "text": " So thank you x for inviting.", "tokens": [407, 1309, 291, 2031, 337, 18202, 13], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1168, "seek": 444752, "start": 4473.200000000001, "end": 4474.64, "text": " And then it predicts y.", "tokens": [400, 550, 309, 6069, 82, 288, 13], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1169, "seek": 444752, "start": 4474.64, "end": 4475.64, "text": " Here's this y here.", "tokens": [1692, 311, 341, 288, 510, 13], "temperature": 0.0, "avg_logprob": -0.19442715720524864, "compression_ratio": 1.8070175438596492, "no_speech_prob": 0.0001054697495419532}, {"id": 1170, "seek": 447564, "start": 4475.64, "end": 4480.4400000000005, "text": " And then what was missing from the y last week.", "tokens": [400, 550, 437, 390, 5361, 490, 264, 288, 1036, 1243, 13], "temperature": 0.0, "avg_logprob": -0.16886791882214244, "compression_ratio": 1.6907630522088353, "no_speech_prob": 0.000141910306410864}, {"id": 1171, "seek": 447564, "start": 4480.4400000000005, "end": 4482.360000000001, "text": " This is called span corruption.", "tokens": [639, 307, 1219, 16174, 17959, 13], "temperature": 0.0, "avg_logprob": -0.16886791882214244, "compression_ratio": 1.6907630522088353, "no_speech_prob": 0.000141910306410864}, {"id": 1172, "seek": 447564, "start": 4482.360000000001, "end": 4487.4800000000005, "text": " And it's really interesting to me because in terms of the actual encoder decoder, we don't", "tokens": [400, 309, 311, 534, 1880, 281, 385, 570, 294, 2115, 295, 264, 3539, 2058, 19866, 979, 19866, 11, 321, 500, 380], "temperature": 0.0, "avg_logprob": -0.16886791882214244, "compression_ratio": 1.6907630522088353, "no_speech_prob": 0.000141910306410864}, {"id": 1173, "seek": 447564, "start": 4487.4800000000005, "end": 4491.64, "text": " have to change it compared to whether we, if we were just doing language modeling pre-training.", "tokens": [362, 281, 1319, 309, 5347, 281, 1968, 321, 11, 498, 321, 645, 445, 884, 2856, 15983, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.16886791882214244, "compression_ratio": 1.6907630522088353, "no_speech_prob": 0.000141910306410864}, {"id": 1174, "seek": 447564, "start": 4491.64, "end": 4494.12, "text": " Because I just do language modeling on all these things.", "tokens": [1436, 286, 445, 360, 2856, 15983, 322, 439, 613, 721, 13], "temperature": 0.0, "avg_logprob": -0.16886791882214244, "compression_ratio": 1.6907630522088353, "no_speech_prob": 0.000141910306410864}, {"id": 1175, "seek": 447564, "start": 4494.12, "end": 4497.4800000000005, "text": " I just predict these words as if I'm a language model.", "tokens": [286, 445, 6069, 613, 2283, 382, 498, 286, 478, 257, 2856, 2316, 13], "temperature": 0.0, "avg_logprob": -0.16886791882214244, "compression_ratio": 1.6907630522088353, "no_speech_prob": 0.000141910306410864}, {"id": 1176, "seek": 447564, "start": 4497.4800000000005, "end": 4501.320000000001, "text": " I've just done a text pre-processing step.", "tokens": [286, 600, 445, 1096, 257, 2487, 659, 12, 41075, 278, 1823, 13], "temperature": 0.0, "avg_logprob": -0.16886791882214244, "compression_ratio": 1.6907630522088353, "no_speech_prob": 0.000141910306410864}, {"id": 1177, "seek": 450132, "start": 4501.32, "end": 4506.92, "text": " So the actual, I've just pre-processed the text to look like, oh, yeah, take the input,", "tokens": [407, 264, 3539, 11, 286, 600, 445, 659, 12, 41075, 292, 264, 2487, 281, 574, 411, 11, 1954, 11, 1338, 11, 747, 264, 4846, 11], "temperature": 0.0, "avg_logprob": -0.16806215000903513, "compression_ratio": 1.7572463768115942, "no_speech_prob": 9.312186011811718e-05}, {"id": 1178, "seek": 450132, "start": 4506.92, "end": 4511.04, "text": " make it look like this, then make an output that looks like that up there.", "tokens": [652, 309, 574, 411, 341, 11, 550, 652, 364, 5598, 300, 1542, 411, 300, 493, 456, 13], "temperature": 0.0, "avg_logprob": -0.16806215000903513, "compression_ratio": 1.7572463768115942, "no_speech_prob": 9.312186011811718e-05}, {"id": 1179, "seek": 450132, "start": 4511.04, "end": 4515.799999999999, "text": " And the model gets to do what is effectively language modeling, but it actually works better.", "tokens": [400, 264, 2316, 2170, 281, 360, 437, 307, 8659, 2856, 15983, 11, 457, 309, 767, 1985, 1101, 13], "temperature": 0.0, "avg_logprob": -0.16806215000903513, "compression_ratio": 1.7572463768115942, "no_speech_prob": 9.312186011811718e-05}, {"id": 1180, "seek": 450132, "start": 4515.799999999999, "end": 4518.36, "text": " So there's a lot of numbers I realize.", "tokens": [407, 456, 311, 257, 688, 295, 3547, 286, 4325, 13], "temperature": 0.0, "avg_logprob": -0.16806215000903513, "compression_ratio": 1.7572463768115942, "no_speech_prob": 9.312186011811718e-05}, {"id": 1181, "seek": 450132, "start": 4518.36, "end": 4520.16, "text": " But look at the star here.", "tokens": [583, 574, 412, 264, 3543, 510, 13], "temperature": 0.0, "avg_logprob": -0.16806215000903513, "compression_ratio": 1.7572463768115942, "no_speech_prob": 9.312186011811718e-05}, {"id": 1182, "seek": 450132, "start": 4520.16, "end": 4525.2, "text": " This encoder decoder with a denoising objective that tends to work the best.", "tokens": [639, 2058, 19866, 979, 19866, 365, 257, 1441, 78, 3436, 10024, 300, 12258, 281, 589, 264, 1151, 13], "temperature": 0.0, "avg_logprob": -0.16806215000903513, "compression_ratio": 1.7572463768115942, "no_speech_prob": 9.312186011811718e-05}, {"id": 1183, "seek": 450132, "start": 4525.2, "end": 4531.2, "text": " And they tried similar models like a prefix language model that was sort of the first", "tokens": [400, 436, 3031, 2531, 5245, 411, 257, 46969, 2856, 2316, 300, 390, 1333, 295, 264, 700], "temperature": 0.0, "avg_logprob": -0.16806215000903513, "compression_ratio": 1.7572463768115942, "no_speech_prob": 9.312186011811718e-05}, {"id": 1184, "seek": 453120, "start": 4531.2, "end": 4535.5599999999995, "text": " try that we had at defining a pre-training objective for language models, sorry, for encoder", "tokens": [853, 300, 321, 632, 412, 17827, 257, 659, 12, 17227, 1760, 10024, 337, 2856, 5245, 11, 2597, 11, 337, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.19466203053792316, "compression_ratio": 1.7609561752988048, "no_speech_prob": 7.717830885667354e-05}, {"id": 1185, "seek": 453120, "start": 4535.5599999999995, "end": 4538.72, "text": " decoders.", "tokens": [979, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.19466203053792316, "compression_ratio": 1.7609561752988048, "no_speech_prob": 7.717830885667354e-05}, {"id": 1186, "seek": 453120, "start": 4538.72, "end": 4541.96, "text": " And then they had another, a number of other options, but what worked best for the encoder", "tokens": [400, 550, 436, 632, 1071, 11, 257, 1230, 295, 661, 3956, 11, 457, 437, 2732, 1151, 337, 264, 2058, 19866], "temperature": 0.0, "avg_logprob": -0.19466203053792316, "compression_ratio": 1.7609561752988048, "no_speech_prob": 7.717830885667354e-05}, {"id": 1187, "seek": 453120, "start": 4541.96, "end": 4543.5199999999995, "text": " decoders.", "tokens": [979, 378, 433, 13], "temperature": 0.0, "avg_logprob": -0.19466203053792316, "compression_ratio": 1.7609561752988048, "no_speech_prob": 7.717830885667354e-05}, {"id": 1188, "seek": 453120, "start": 4543.5199999999995, "end": 4548.04, "text": " And one of the fascinating things about T5 is that you could pre-train it and fine tune", "tokens": [400, 472, 295, 264, 10343, 721, 466, 314, 20, 307, 300, 291, 727, 659, 12, 83, 7146, 309, 293, 2489, 10864], "temperature": 0.0, "avg_logprob": -0.19466203053792316, "compression_ratio": 1.7609561752988048, "no_speech_prob": 7.717830885667354e-05}, {"id": 1189, "seek": 453120, "start": 4548.04, "end": 4554.12, "text": " it on questions like when was Franklin D. Roosevelt born and fine tune it to produce the", "tokens": [309, 322, 1651, 411, 562, 390, 22010, 413, 13, 28515, 4232, 293, 2489, 10864, 309, 281, 5258, 264], "temperature": 0.0, "avg_logprob": -0.19466203053792316, "compression_ratio": 1.7609561752988048, "no_speech_prob": 7.717830885667354e-05}, {"id": 1190, "seek": 453120, "start": 4554.12, "end": 4555.88, "text": " answer.", "tokens": [1867, 13], "temperature": 0.0, "avg_logprob": -0.19466203053792316, "compression_ratio": 1.7609561752988048, "no_speech_prob": 7.717830885667354e-05}, {"id": 1191, "seek": 453120, "start": 4555.88, "end": 4558.92, "text": " And then you could ask it new questions at test time.", "tokens": [400, 550, 291, 727, 1029, 309, 777, 1651, 412, 1500, 565, 13], "temperature": 0.0, "avg_logprob": -0.19466203053792316, "compression_ratio": 1.7609561752988048, "no_speech_prob": 7.717830885667354e-05}, {"id": 1192, "seek": 455892, "start": 4558.92, "end": 4562.68, "text": " And then it would retrieve the answer from its parameters with some accuracy.", "tokens": [400, 550, 309, 576, 30254, 264, 1867, 490, 1080, 9834, 365, 512, 14170, 13], "temperature": 0.0, "avg_logprob": -0.1985858147885619, "compression_ratio": 1.871212121212121, "no_speech_prob": 1.3415598914434668e-05}, {"id": 1193, "seek": 455892, "start": 4562.68, "end": 4565.76, "text": " And it would do so relatively well actually.", "tokens": [400, 309, 576, 360, 370, 7226, 731, 767, 13], "temperature": 0.0, "avg_logprob": -0.1985858147885619, "compression_ratio": 1.871212121212121, "no_speech_prob": 1.3415598914434668e-05}, {"id": 1194, "seek": 455892, "start": 4565.76, "end": 4570.16, "text": " And it would do so maybe 25% of the time on some of these data sets with 220 million", "tokens": [400, 309, 576, 360, 370, 1310, 3552, 4, 295, 264, 565, 322, 512, 295, 613, 1412, 6352, 365, 29387, 2459], "temperature": 0.0, "avg_logprob": -0.1985858147885619, "compression_ratio": 1.871212121212121, "no_speech_prob": 1.3415598914434668e-05}, {"id": 1195, "seek": 455892, "start": 4570.16, "end": 4571.16, "text": " parameters.", "tokens": [9834, 13], "temperature": 0.0, "avg_logprob": -0.1985858147885619, "compression_ratio": 1.871212121212121, "no_speech_prob": 1.3415598914434668e-05}, {"id": 1196, "seek": 455892, "start": 4571.16, "end": 4575.4, "text": " And then at 11 billion parameters, this is way bigger than Bert large.", "tokens": [400, 550, 412, 2975, 5218, 9834, 11, 341, 307, 636, 3801, 813, 29594, 2416, 13], "temperature": 0.0, "avg_logprob": -0.1985858147885619, "compression_ratio": 1.871212121212121, "no_speech_prob": 1.3415598914434668e-05}, {"id": 1197, "seek": 455892, "start": 4575.4, "end": 4580.72, "text": " It would do so even better, sometimes even doing as well as systems that were allowed", "tokens": [467, 576, 360, 370, 754, 1101, 11, 2171, 754, 884, 382, 731, 382, 3652, 300, 645, 4350], "temperature": 0.0, "avg_logprob": -0.1985858147885619, "compression_ratio": 1.871212121212121, "no_speech_prob": 1.3415598914434668e-05}, {"id": 1198, "seek": 455892, "start": 4580.72, "end": 4582.64, "text": " to look at stuff other than their own parameters.", "tokens": [281, 574, 412, 1507, 661, 813, 641, 1065, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1985858147885619, "compression_ratio": 1.871212121212121, "no_speech_prob": 1.3415598914434668e-05}, {"id": 1199, "seek": 455892, "start": 4582.64, "end": 4586.72, "text": " So again, this is just making this answer come from its parameters.", "tokens": [407, 797, 11, 341, 307, 445, 1455, 341, 1867, 808, 490, 1080, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1985858147885619, "compression_ratio": 1.871212121212121, "no_speech_prob": 1.3415598914434668e-05}, {"id": 1200, "seek": 458672, "start": 4586.72, "end": 4590.88, "text": " Yeah, I'm going to have to skip this.", "tokens": [865, 11, 286, 478, 516, 281, 362, 281, 10023, 341, 13], "temperature": 0.0, "avg_logprob": -0.14724270502726236, "compression_ratio": 1.7191780821917808, "no_speech_prob": 5.560696808970533e-05}, {"id": 1201, "seek": 458672, "start": 4590.88, "end": 4595.8, "text": " So if you look back at this slide after class, I have each of the examples of the things", "tokens": [407, 498, 291, 574, 646, 412, 341, 4137, 934, 1508, 11, 286, 362, 1184, 295, 264, 5110, 295, 264, 721], "temperature": 0.0, "avg_logprob": -0.14724270502726236, "compression_ratio": 1.7191780821917808, "no_speech_prob": 5.560696808970533e-05}, {"id": 1202, "seek": 458672, "start": 4595.8, "end": 4600.0, "text": " that we could imagine learning from pre-training with a label of what you might be learning.", "tokens": [300, 321, 727, 3811, 2539, 490, 659, 12, 17227, 1760, 365, 257, 7645, 295, 437, 291, 1062, 312, 2539, 13], "temperature": 0.0, "avg_logprob": -0.14724270502726236, "compression_ratio": 1.7191780821917808, "no_speech_prob": 5.560696808970533e-05}, {"id": 1203, "seek": 458672, "start": 4600.0, "end": 4603.0, "text": " So this example is 10 for universities located in blank.", "tokens": [407, 341, 1365, 307, 1266, 337, 11779, 6870, 294, 8247, 13], "temperature": 0.0, "avg_logprob": -0.14724270502726236, "compression_ratio": 1.7191780821917808, "no_speech_prob": 5.560696808970533e-05}, {"id": 1204, "seek": 458672, "start": 4603.0, "end": 4604.0, "text": " You might learn trivia.", "tokens": [509, 1062, 1466, 48770, 13], "temperature": 0.0, "avg_logprob": -0.14724270502726236, "compression_ratio": 1.7191780821917808, "no_speech_prob": 5.560696808970533e-05}, {"id": 1205, "seek": 458672, "start": 4604.0, "end": 4606.96, "text": " In all these cases, there's all these things you can learn.", "tokens": [682, 439, 613, 3331, 11, 456, 311, 439, 613, 721, 291, 393, 1466, 13], "temperature": 0.0, "avg_logprob": -0.14724270502726236, "compression_ratio": 1.7191780821917808, "no_speech_prob": 5.560696808970533e-05}, {"id": 1206, "seek": 458672, "start": 4606.96, "end": 4613.240000000001, "text": " One thing I will say is that models also learn and can make even worse racism, sexism,", "tokens": [1485, 551, 286, 486, 584, 307, 300, 5245, 611, 1466, 293, 393, 652, 754, 5324, 12664, 11, 3260, 1434, 11], "temperature": 0.0, "avg_logprob": -0.14724270502726236, "compression_ratio": 1.7191780821917808, "no_speech_prob": 5.560696808970533e-05}, {"id": 1207, "seek": 458672, "start": 4613.240000000001, "end": 4616.04, "text": " all manner of bad biases that are encoded in our text.", "tokens": [439, 9060, 295, 1578, 32152, 300, 366, 2058, 12340, 294, 527, 2487, 13], "temperature": 0.0, "avg_logprob": -0.14724270502726236, "compression_ratio": 1.7191780821917808, "no_speech_prob": 5.560696808970533e-05}, {"id": 1208, "seek": 461604, "start": 4616.04, "end": 4620.28, "text": " When I say, yeah, they do this.", "tokens": [1133, 286, 584, 11, 1338, 11, 436, 360, 341, 13], "temperature": 0.0, "avg_logprob": -0.11562888083919402, "compression_ratio": 1.643939393939394, "no_speech_prob": 8.346445247298107e-05}, {"id": 1209, "seek": 461604, "start": 4620.28, "end": 4622.96, "text": " And so we'll learn more about this in our later lectures, but it's important to keep", "tokens": [400, 370, 321, 603, 1466, 544, 466, 341, 294, 527, 1780, 16564, 11, 457, 309, 311, 1021, 281, 1066], "temperature": 0.0, "avg_logprob": -0.11562888083919402, "compression_ratio": 1.643939393939394, "no_speech_prob": 8.346445247298107e-05}, {"id": 1210, "seek": 461604, "start": 4622.96, "end": 4626.76, "text": " in mind that when you're doing pre-training, you're learning a lot of stuff, and not all", "tokens": [294, 1575, 300, 562, 291, 434, 884, 659, 12, 17227, 1760, 11, 291, 434, 2539, 257, 688, 295, 1507, 11, 293, 406, 439], "temperature": 0.0, "avg_logprob": -0.11562888083919402, "compression_ratio": 1.643939393939394, "no_speech_prob": 8.346445247298107e-05}, {"id": 1211, "seek": 461604, "start": 4626.76, "end": 4629.88, "text": " of it is good.", "tokens": [295, 309, 307, 665, 13], "temperature": 0.0, "avg_logprob": -0.11562888083919402, "compression_ratio": 1.643939393939394, "no_speech_prob": 8.346445247298107e-05}, {"id": 1212, "seek": 461604, "start": 4629.88, "end": 4636.4, "text": " So with GPT-3, the last thing here is that there's this third way of interacting with models", "tokens": [407, 365, 26039, 51, 12, 18, 11, 264, 1036, 551, 510, 307, 300, 456, 311, 341, 2636, 636, 295, 18017, 365, 5245], "temperature": 0.0, "avg_logprob": -0.11562888083919402, "compression_ratio": 1.643939393939394, "no_speech_prob": 8.346445247298107e-05}, {"id": 1213, "seek": 461604, "start": 4636.4, "end": 4639.68, "text": " that's related to treating them as language models.", "tokens": [300, 311, 4077, 281, 15083, 552, 382, 2856, 5245, 13], "temperature": 0.0, "avg_logprob": -0.11562888083919402, "compression_ratio": 1.643939393939394, "no_speech_prob": 8.346445247298107e-05}, {"id": 1214, "seek": 461604, "start": 4639.68, "end": 4645.4, "text": " So GPT-3 is this very, very large model that was released by OpenAI.", "tokens": [407, 26039, 51, 12, 18, 307, 341, 588, 11, 588, 2416, 2316, 300, 390, 4736, 538, 7238, 48698, 13], "temperature": 0.0, "avg_logprob": -0.11562888083919402, "compression_ratio": 1.643939393939394, "no_speech_prob": 8.346445247298107e-05}, {"id": 1215, "seek": 464540, "start": 4645.4, "end": 4651.4, "text": " But it seems to be able to learn from examples in their context, their decoder context,", "tokens": [583, 309, 2544, 281, 312, 1075, 281, 1466, 490, 5110, 294, 641, 4319, 11, 641, 979, 19866, 4319, 11], "temperature": 0.0, "avg_logprob": -0.1599029314399946, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.01350176212145e-05}, {"id": 1216, "seek": 464540, "start": 4651.4, "end": 4656.719999999999, "text": " without gradient steps, simply by looking sort of within their history.", "tokens": [1553, 16235, 4439, 11, 2935, 538, 1237, 1333, 295, 1951, 641, 2503, 13], "temperature": 0.0, "avg_logprob": -0.1599029314399946, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.01350176212145e-05}, {"id": 1217, "seek": 464540, "start": 4656.719999999999, "end": 4660.36, "text": " And now GPT-3 has 175 billion parameters, right?", "tokens": [400, 586, 26039, 51, 12, 18, 575, 41165, 5218, 9834, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.1599029314399946, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.01350176212145e-05}, {"id": 1218, "seek": 464540, "start": 4660.36, "end": 4664.44, "text": " The last T5 model we saw was 11 billion parameters.", "tokens": [440, 1036, 314, 20, 2316, 321, 1866, 390, 2975, 5218, 9834, 13], "temperature": 0.0, "avg_logprob": -0.1599029314399946, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.01350176212145e-05}, {"id": 1219, "seek": 464540, "start": 4664.44, "end": 4668.4, "text": " And it seems to be sort of the canonical example of this working.", "tokens": [400, 309, 2544, 281, 312, 1333, 295, 264, 46491, 1365, 295, 341, 1364, 13], "temperature": 0.0, "avg_logprob": -0.1599029314399946, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.01350176212145e-05}, {"id": 1220, "seek": 464540, "start": 4668.4, "end": 4672.5599999999995, "text": " And so what it looks like is you give it as part of its prefix.", "tokens": [400, 370, 437, 309, 1542, 411, 307, 291, 976, 309, 382, 644, 295, 1080, 46969, 13], "temperature": 0.0, "avg_logprob": -0.1599029314399946, "compression_ratio": 1.6455696202531647, "no_speech_prob": 2.01350176212145e-05}, {"id": 1221, "seek": 467256, "start": 4672.56, "end": 4676.96, "text": " This goes to Merci, hello, goes to Mint, goes to writes, you've got these translation", "tokens": [639, 1709, 281, 19856, 11, 7751, 11, 1709, 281, 36188, 11, 1709, 281, 13657, 11, 291, 600, 658, 613, 12853], "temperature": 0.0, "avg_logprob": -0.24762458801269532, "compression_ratio": 1.8016194331983806, "no_speech_prob": 3.589769403333776e-05}, {"id": 1222, "seek": 467256, "start": 4676.96, "end": 4683.280000000001, "text": " examples, you ask it for the last one, and it comes up with the correct translation.", "tokens": [5110, 11, 291, 1029, 309, 337, 264, 1036, 472, 11, 293, 309, 1487, 493, 365, 264, 3006, 12853, 13], "temperature": 0.0, "avg_logprob": -0.24762458801269532, "compression_ratio": 1.8016194331983806, "no_speech_prob": 3.589769403333776e-05}, {"id": 1223, "seek": 467256, "start": 4683.280000000001, "end": 4686.160000000001, "text": " Seemingly because it's learned something about the task that you're sort of telling", "tokens": [1100, 443, 12163, 570, 309, 311, 3264, 746, 466, 264, 5633, 300, 291, 434, 1333, 295, 3585], "temperature": 0.0, "avg_logprob": -0.24762458801269532, "compression_ratio": 1.8016194331983806, "no_speech_prob": 3.589769403333776e-05}, {"id": 1224, "seek": 467256, "start": 4686.160000000001, "end": 4688.64, "text": " it to do through its prefix.", "tokens": [309, 281, 360, 807, 1080, 46969, 13], "temperature": 0.0, "avg_logprob": -0.24762458801269532, "compression_ratio": 1.8016194331983806, "no_speech_prob": 3.589769403333776e-05}, {"id": 1225, "seek": 467256, "start": 4688.64, "end": 4690.4800000000005, "text": " And so you might do the same thing with addition.", "tokens": [400, 370, 291, 1062, 360, 264, 912, 551, 365, 4500, 13], "temperature": 0.0, "avg_logprob": -0.24762458801269532, "compression_ratio": 1.8016194331983806, "no_speech_prob": 3.589769403333776e-05}, {"id": 1226, "seek": 467256, "start": 4690.4800000000005, "end": 4695.64, "text": " So something, if I plus eight is 13, give it addition examples, you might do the next", "tokens": [407, 746, 11, 498, 286, 1804, 3180, 307, 3705, 11, 976, 309, 4500, 5110, 11, 291, 1062, 360, 264, 958], "temperature": 0.0, "avg_logprob": -0.24762458801269532, "compression_ratio": 1.8016194331983806, "no_speech_prob": 3.589769403333776e-05}, {"id": 1227, "seek": 467256, "start": 4695.64, "end": 4698.240000000001, "text": " addition example for you.", "tokens": [4500, 1365, 337, 291, 13], "temperature": 0.0, "avg_logprob": -0.24762458801269532, "compression_ratio": 1.8016194331983806, "no_speech_prob": 3.589769403333776e-05}, {"id": 1228, "seek": 469824, "start": 4698.24, "end": 4704.08, "text": " Or maybe trying to figure out grammatical or spelling errors, for example.", "tokens": [1610, 1310, 1382, 281, 2573, 484, 17570, 267, 804, 420, 22254, 13603, 11, 337, 1365, 13], "temperature": 0.0, "avg_logprob": -0.1931436538696289, "compression_ratio": 1.4554455445544554, "no_speech_prob": 4.326555790612474e-05}, {"id": 1229, "seek": 469824, "start": 4704.08, "end": 4709.76, "text": " And here's the French case.", "tokens": [400, 510, 311, 264, 5522, 1389, 13], "temperature": 0.0, "avg_logprob": -0.1931436538696289, "compression_ratio": 1.4554455445544554, "no_speech_prob": 4.326555790612474e-05}, {"id": 1230, "seek": 469824, "start": 4709.76, "end": 4713.48, "text": " So again, you're learning just to do pre-training.", "tokens": [407, 797, 11, 291, 434, 2539, 445, 281, 360, 659, 12, 17227, 1760, 13], "temperature": 0.0, "avg_logprob": -0.1931436538696289, "compression_ratio": 1.4554455445544554, "no_speech_prob": 4.326555790612474e-05}, {"id": 1231, "seek": 469824, "start": 4713.48, "end": 4720.04, "text": " But when you're evaluating it, you don't even fine tune the model, you just provide prefixes.", "tokens": [583, 562, 291, 434, 27479, 309, 11, 291, 500, 380, 754, 2489, 10864, 264, 2316, 11, 291, 445, 2893, 18417, 36005, 13], "temperature": 0.0, "avg_logprob": -0.1931436538696289, "compression_ratio": 1.4554455445544554, "no_speech_prob": 4.326555790612474e-05}, {"id": 1232, "seek": 469824, "start": 4720.04, "end": 4723.88, "text": " And so this especially is not well understood.", "tokens": [400, 370, 341, 2318, 307, 406, 731, 7320, 13], "temperature": 0.0, "avg_logprob": -0.1931436538696289, "compression_ratio": 1.4554455445544554, "no_speech_prob": 4.326555790612474e-05}, {"id": 1233, "seek": 472388, "start": 4723.88, "end": 4728.6, "text": " And so a lot of research is going into sort of what the limitations of this so-called", "tokens": [400, 370, 257, 688, 295, 2132, 307, 516, 666, 1333, 295, 437, 264, 15705, 295, 341, 370, 12, 11880], "temperature": 0.0, "avg_logprob": -0.23243352164209416, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.129188528168015e-05}, {"id": 1234, "seek": 472388, "start": 4728.6, "end": 4729.92, "text": " in-context learning are.", "tokens": [294, 12, 9000, 3828, 2539, 366, 13], "temperature": 0.0, "avg_logprob": -0.23243352164209416, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.129188528168015e-05}, {"id": 1235, "seek": 472388, "start": 4729.92, "end": 4733.72, "text": " But it's a fascinating direction for future work.", "tokens": [583, 309, 311, 257, 10343, 3513, 337, 2027, 589, 13], "temperature": 0.0, "avg_logprob": -0.23243352164209416, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.129188528168015e-05}, {"id": 1236, "seek": 472388, "start": 4733.72, "end": 4736.68, "text": " In total, these models are not well understood.", "tokens": [682, 3217, 11, 613, 5245, 366, 406, 731, 7320, 13], "temperature": 0.0, "avg_logprob": -0.23243352164209416, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.129188528168015e-05}, {"id": 1237, "seek": 472388, "start": 4736.68, "end": 4741.64, "text": " However, small, small, in-air growth models like Bert have become general tools in a wide", "tokens": [2908, 11, 1359, 11, 1359, 11, 294, 12, 1246, 4599, 5245, 411, 29594, 362, 1813, 2674, 3873, 294, 257, 4874], "temperature": 0.0, "avg_logprob": -0.23243352164209416, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.129188528168015e-05}, {"id": 1238, "seek": 472388, "start": 4741.64, "end": 4742.88, "text": " range of settings.", "tokens": [3613, 295, 6257, 13], "temperature": 0.0, "avg_logprob": -0.23243352164209416, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.129188528168015e-05}, {"id": 1239, "seek": 472388, "start": 4742.88, "end": 4746.12, "text": " They do have these issues about learning all these biases about the world.", "tokens": [814, 360, 362, 613, 2663, 466, 2539, 439, 613, 32152, 466, 264, 1002, 13], "temperature": 0.0, "avg_logprob": -0.23243352164209416, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.129188528168015e-05}, {"id": 1240, "seek": 472388, "start": 4746.12, "end": 4750.64, "text": " They'll go into and further lectures in this course.", "tokens": [814, 603, 352, 666, 293, 3052, 16564, 294, 341, 1164, 13], "temperature": 0.0, "avg_logprob": -0.23243352164209416, "compression_ratio": 1.6481481481481481, "no_speech_prob": 4.129188528168015e-05}, {"id": 1241, "seek": 475064, "start": 4750.64, "end": 4755.240000000001, "text": " And so, yeah, what you've learned this week, transformers and pre-training form the basis", "tokens": [400, 370, 11, 1338, 11, 437, 291, 600, 3264, 341, 1243, 11, 4088, 433, 293, 659, 12, 17227, 1760, 1254, 264, 5143], "temperature": 0.0, "avg_logprob": -0.23720451354980468, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.826569420401938e-05}, {"id": 1242, "seek": 475064, "start": 4755.240000000001, "end": 4760.08, "text": " or at least the base lines for much of a natural language processing today.", "tokens": [420, 412, 1935, 264, 3096, 3876, 337, 709, 295, 257, 3303, 2856, 9007, 965, 13], "temperature": 0.0, "avg_logprob": -0.23720451354980468, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.826569420401938e-05}, {"id": 1243, "seek": 475064, "start": 4760.08, "end": 4765.04, "text": " And assignment five is out and you'll be able to look more into it.", "tokens": [400, 15187, 1732, 307, 484, 293, 291, 603, 312, 1075, 281, 574, 544, 666, 309, 13], "temperature": 0.0, "avg_logprob": -0.23720451354980468, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.826569420401938e-05}, {"id": 1244, "seek": 475064, "start": 4765.04, "end": 4766.04, "text": " And I'm over time.", "tokens": [400, 286, 478, 670, 565, 13], "temperature": 0.0, "avg_logprob": -0.23720451354980468, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.826569420401938e-05}, {"id": 1245, "seek": 475064, "start": 4766.04, "end": 4767.04, "text": " All right.", "tokens": [1057, 558, 13], "temperature": 0.0, "avg_logprob": -0.23720451354980468, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.826569420401938e-05}, {"id": 1246, "seek": 475064, "start": 4767.04, "end": 4768.04, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.23720451354980468, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.826569420401938e-05}, {"id": 1247, "seek": 475064, "start": 4768.04, "end": 4779.92, "text": " I guess I can take a question if there is any, but people can keep going as well.", "tokens": [286, 2041, 286, 393, 747, 257, 1168, 498, 456, 307, 604, 11, 457, 561, 393, 1066, 516, 382, 731, 13], "temperature": 0.0, "avg_logprob": -0.23720451354980468, "compression_ratio": 1.5462555066079295, "no_speech_prob": 4.826569420401938e-05}, {"id": 1248, "seek": 477992, "start": 4779.92, "end": 4790.4, "text": " So I think that I think there's a question about P5, which was how does the D-toder know", "tokens": [407, 286, 519, 300, 286, 519, 456, 311, 257, 1168, 466, 430, 20, 11, 597, 390, 577, 775, 264, 413, 12, 83, 19866, 458], "temperature": 0.0, "avg_logprob": -0.4366758202993742, "compression_ratio": 1.8588957055214723, "no_speech_prob": 0.0004648546746466309}, {"id": 1249, "seek": 477992, "start": 4790.4, "end": 4792.92, "text": " that I'm currently predicting X for Y?", "tokens": [300, 286, 478, 4362, 32884, 1783, 337, 398, 30], "temperature": 0.0, "avg_logprob": -0.4366758202993742, "compression_ratio": 1.8588957055214723, "no_speech_prob": 0.0004648546746466309}, {"id": 1250, "seek": 477992, "start": 4792.92, "end": 4794.92, "text": " Could you repeat that?", "tokens": [7497, 291, 7149, 300, 30], "temperature": 0.0, "avg_logprob": -0.4366758202993742, "compression_ratio": 1.8588957055214723, "no_speech_prob": 0.0004648546746466309}, {"id": 1251, "seek": 477992, "start": 4794.92, "end": 4795.92, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.4366758202993742, "compression_ratio": 1.8588957055214723, "no_speech_prob": 0.0004648546746466309}, {"id": 1252, "seek": 477992, "start": 4795.92, "end": 4800.84, "text": " So about P5, there's a question that was asking how does the D-toder know it's currently", "tokens": [407, 466, 430, 20, 11, 456, 311, 257, 1168, 300, 390, 3365, 577, 775, 264, 413, 12, 83, 19866, 458, 309, 311, 4362], "temperature": 0.0, "avg_logprob": -0.4366758202993742, "compression_ratio": 1.8588957055214723, "no_speech_prob": 0.0004648546746466309}, {"id": 1253, "seek": 477992, "start": 4800.84, "end": 4804.16, "text": " predicting X for Y?", "tokens": [32884, 1783, 337, 398, 30], "temperature": 0.0, "avg_logprob": -0.4366758202993742, "compression_ratio": 1.8588957055214723, "no_speech_prob": 0.0004648546746466309}, {"id": 1254, "seek": 477992, "start": 4804.16, "end": 4809.32, "text": " It's hierarchy of predicting X for Y?", "tokens": [467, 311, 22333, 295, 32884, 1783, 337, 398, 30], "temperature": 0.0, "avg_logprob": -0.4366758202993742, "compression_ratio": 1.8588957055214723, "no_speech_prob": 0.0004648546746466309}, {"id": 1255, "seek": 480932, "start": 4809.32, "end": 4813.44, "text": " I guess it doesn't specify it's going to change how does it know that it's currently", "tokens": [286, 2041, 309, 1177, 380, 16500, 309, 311, 516, 281, 1319, 577, 775, 309, 458, 300, 309, 311, 4362], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1256, "seek": 480932, "start": 4813.44, "end": 4815.04, "text": " predicting X for Y?", "tokens": [32884, 1783, 337, 398, 30], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1257, "seek": 480932, "start": 4815.04, "end": 4816.04, "text": " OK.", "tokens": [2264, 13], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1258, "seek": 480932, "start": 4816.04, "end": 4817.04, "text": " Yeah.", "tokens": [865, 13], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1259, "seek": 480932, "start": 4817.04, "end": 4818.04, "text": " That makes sense.", "tokens": [663, 1669, 2020, 13], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1260, "seek": 480932, "start": 4818.04, "end": 4819.04, "text": " So what it does, right?", "tokens": [407, 437, 309, 775, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1261, "seek": 480932, "start": 4819.04, "end": 4823.799999999999, "text": " So it knows from the encoder that it has to at some point predict X and at some point", "tokens": [407, 309, 3255, 490, 264, 2058, 19866, 300, 309, 575, 281, 412, 512, 935, 6069, 1783, 293, 412, 512, 935], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1262, "seek": 480932, "start": 4823.799999999999, "end": 4828.28, "text": " predict Y because the encoder can just like remember that, oh, yeah, there's two things", "tokens": [6069, 398, 570, 264, 2058, 19866, 393, 445, 411, 1604, 300, 11, 1954, 11, 1338, 11, 456, 311, 732, 721], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1263, "seek": 480932, "start": 4828.28, "end": 4829.28, "text": " missing.", "tokens": [5361, 13], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1264, "seek": 480932, "start": 4829.28, "end": 4833.84, "text": " And if there were more spans replaced, there would be a Z and then an A and then a B and", "tokens": [400, 498, 456, 645, 544, 44086, 10772, 11, 456, 576, 312, 257, 1176, 293, 550, 364, 316, 293, 550, 257, 363, 293], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1265, "seek": 480932, "start": 4833.84, "end": 4838.28, "text": " you know whatever, just a bunch of unique identifiers.", "tokens": [291, 458, 2035, 11, 445, 257, 3840, 295, 3845, 2473, 23463, 13], "temperature": 0.0, "avg_logprob": -0.2525719841607183, "compression_ratio": 1.7067137809187278, "no_speech_prob": 5.3894793381914496e-05}, {"id": 1266, "seek": 483828, "start": 4838.28, "end": 4844.92, "text": " And then up here, it gets to say, OK, I have attention, I suppose.", "tokens": [400, 550, 493, 510, 11, 309, 2170, 281, 584, 11, 2264, 11, 286, 362, 3202, 11, 286, 7297, 13], "temperature": 0.0, "avg_logprob": -0.169206039325611, "compression_ratio": 1.88, "no_speech_prob": 1.862914905359503e-05}, {"id": 1267, "seek": 483828, "start": 4844.92, "end": 4848.88, "text": " I can look and I know that first I have to predict this first master thing.", "tokens": [286, 393, 574, 293, 286, 458, 300, 700, 286, 362, 281, 6069, 341, 700, 4505, 551, 13], "temperature": 0.0, "avg_logprob": -0.169206039325611, "compression_ratio": 1.88, "no_speech_prob": 1.862914905359503e-05}, {"id": 1268, "seek": 483828, "start": 4848.88, "end": 4852.8, "text": " So I'm going to generate that in my D-coder and then it gets that symbol, right?", "tokens": [407, 286, 478, 516, 281, 8460, 300, 294, 452, 413, 12, 66, 19866, 293, 550, 309, 2170, 300, 5986, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.169206039325611, "compression_ratio": 1.88, "no_speech_prob": 1.862914905359503e-05}, {"id": 1269, "seek": 483828, "start": 4852.8, "end": 4855.639999999999, "text": " So we're doing training by giving it the right symbol.", "tokens": [407, 321, 434, 884, 3097, 538, 2902, 309, 264, 558, 5986, 13], "temperature": 0.0, "avg_logprob": -0.169206039325611, "compression_ratio": 1.88, "no_speech_prob": 1.862914905359503e-05}, {"id": 1270, "seek": 483828, "start": 4855.639999999999, "end": 4859.5199999999995, "text": " Now it gets that X and it says, OK, I'm predicting X now.", "tokens": [823, 309, 2170, 300, 1783, 293, 309, 1619, 11, 2264, 11, 286, 478, 32884, 1783, 586, 13], "temperature": 0.0, "avg_logprob": -0.169206039325611, "compression_ratio": 1.88, "no_speech_prob": 1.862914905359503e-05}, {"id": 1271, "seek": 483828, "start": 4859.5199999999995, "end": 4861.84, "text": " And now it can predict, predict, predict, predict.", "tokens": [400, 586, 309, 393, 6069, 11, 6069, 11, 6069, 11, 6069, 13], "temperature": 0.0, "avg_logprob": -0.169206039325611, "compression_ratio": 1.88, "no_speech_prob": 1.862914905359503e-05}, {"id": 1272, "seek": 483828, "start": 4861.84, "end": 4862.84, "text": " Then it gets Y.", "tokens": [1396, 309, 2170, 398, 13], "temperature": 0.0, "avg_logprob": -0.169206039325611, "compression_ratio": 1.88, "no_speech_prob": 1.862914905359503e-05}, {"id": 1273, "seek": 483828, "start": 4862.84, "end": 4866.24, "text": " So we're doing this teacher forcing training where we give it the right answer after penalizing", "tokens": [407, 321, 434, 884, 341, 5027, 19030, 3097, 689, 321, 976, 309, 264, 558, 1867, 934, 13661, 3319], "temperature": 0.0, "avg_logprob": -0.169206039325611, "compression_ratio": 1.88, "no_speech_prob": 1.862914905359503e-05}, {"id": 1274, "seek": 483828, "start": 4866.24, "end": 4867.759999999999, "text": " it if it's wrong.", "tokens": [309, 498, 309, 311, 2085, 13], "temperature": 0.0, "avg_logprob": -0.169206039325611, "compression_ratio": 1.88, "no_speech_prob": 1.862914905359503e-05}, {"id": 1275, "seek": 486776, "start": 4867.76, "end": 4870.2, "text": " Now it gets this Y, right?", "tokens": [823, 309, 2170, 341, 398, 11, 558, 30], "temperature": 0.0, "avg_logprob": -0.17244207536852038, "compression_ratio": 1.8158730158730159, "no_speech_prob": 6.704647239530459e-05}, {"id": 1276, "seek": 486776, "start": 4870.2, "end": 4872.4400000000005, "text": " And it says, OK, now I have to predict what should go and why.", "tokens": [400, 309, 1619, 11, 2264, 11, 586, 286, 362, 281, 6069, 437, 820, 352, 293, 983, 13], "temperature": 0.0, "avg_logprob": -0.17244207536852038, "compression_ratio": 1.8158730158730159, "no_speech_prob": 6.704647239530459e-05}, {"id": 1277, "seek": 486776, "start": 4872.4400000000005, "end": 4876.76, "text": " And it can attend, you know, into the natural parts of this as well as what it's already", "tokens": [400, 309, 393, 6888, 11, 291, 458, 11, 666, 264, 3303, 3166, 295, 341, 382, 731, 382, 437, 309, 311, 1217], "temperature": 0.0, "avg_logprob": -0.17244207536852038, "compression_ratio": 1.8158730158730159, "no_speech_prob": 6.704647239530459e-05}, {"id": 1278, "seek": 486776, "start": 4876.76, "end": 4882.16, "text": " predicted here because the decoder has attention within itself and it can see what should go", "tokens": [19147, 510, 570, 264, 979, 19866, 575, 3202, 1951, 2564, 293, 309, 393, 536, 437, 820, 352], "temperature": 0.0, "avg_logprob": -0.17244207536852038, "compression_ratio": 1.8158730158730159, "no_speech_prob": 6.704647239530459e-05}, {"id": 1279, "seek": 486776, "start": 4882.16, "end": 4883.16, "text": " there.", "tokens": [456, 13], "temperature": 0.0, "avg_logprob": -0.17244207536852038, "compression_ratio": 1.8158730158730159, "no_speech_prob": 6.704647239530459e-05}, {"id": 1280, "seek": 486776, "start": 4883.16, "end": 4885.8, "text": " So what's fascinating here is you're doing something like language modeling.", "tokens": [407, 437, 311, 10343, 510, 307, 291, 434, 884, 746, 411, 2856, 15983, 13], "temperature": 0.0, "avg_logprob": -0.17244207536852038, "compression_ratio": 1.8158730158730159, "no_speech_prob": 6.704647239530459e-05}, {"id": 1281, "seek": 486776, "start": 4885.8, "end": 4889.4800000000005, "text": " But when you're predicting Y, right, you get to see what came after it.", "tokens": [583, 562, 291, 434, 32884, 398, 11, 558, 11, 291, 483, 281, 536, 437, 1361, 934, 309, 13], "temperature": 0.0, "avg_logprob": -0.17244207536852038, "compression_ratio": 1.8158730158730159, "no_speech_prob": 6.704647239530459e-05}, {"id": 1282, "seek": 486776, "start": 4889.4800000000005, "end": 4891.4400000000005, "text": " And that's I think one of the benefits of span corruption.", "tokens": [400, 300, 311, 286, 519, 472, 295, 264, 5311, 295, 16174, 17959, 13], "temperature": 0.0, "avg_logprob": -0.17244207536852038, "compression_ratio": 1.8158730158730159, "no_speech_prob": 6.704647239530459e-05}, {"id": 1283, "seek": 486776, "start": 4891.4400000000005, "end": 4894.6, "text": " So you're doing this thing where you don't know how long you should be predicting for", "tokens": [407, 291, 434, 884, 341, 551, 689, 291, 500, 380, 458, 577, 938, 291, 820, 312, 32884, 337], "temperature": 0.0, "avg_logprob": -0.17244207536852038, "compression_ratio": 1.8158730158730159, "no_speech_prob": 6.704647239530459e-05}, {"id": 1284, "seek": 489460, "start": 4894.6, "end": 4899.68, "text": " like language modeling, but you get to know what came after the thing that's missing.", "tokens": [50364, 411, 2856, 15983, 11, 457, 291, 483, 281, 458, 437, 1361, 934, 264, 551, 300, 311, 5361, 13, 50618], "temperature": 0.0, "avg_logprob": -0.11783964293343681, "compression_ratio": 1.0759493670886076, "no_speech_prob": 0.00028068452957086265}], "language": "en"}